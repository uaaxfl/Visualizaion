1992.tmi-1.20,J90-2002,0,0.126728,"Missing"
1992.tmi-1.20,1991.mtsummit-papers.18,0,0.0331948,"Missing"
1992.tmi-1.20,1991.mtsummit-papers.9,1,0.856286,"Missing"
1992.tmi-1.20,P91-1024,0,0.0387602,"Missing"
1992.tmi-1.20,1983.tc-1.13,0,0.0601175,"Missing"
1995.tmi-1.12,C92-2090,0,0.199555,"Missing"
1995.tmi-1.12,C94-1012,1,0.794255,"Missing"
1995.tmi-1.12,1991.mtsummit-papers.9,1,0.905149,"Missing"
1995.tmi-1.12,1993.tmi-1.28,1,0.899628,"Missing"
1995.tmi-1.12,C94-1013,1,0.885845,"Missing"
1997.mtsummit-papers.2,1994.amta-1.10,0,0.108479,"Missing"
1999.mtsummit-1.8,C94-1012,1,\N,Missing
1999.mtsummit-1.8,1991.mtsummit-papers.9,1,\N,Missing
1999.tmi-1.22,1991.mtsummit-papers.9,1,0.837321,"is issue and demonstrate a method which reduces the time and effort to build high-quality KBMT systems. A semantic model developed for a particular domain may not cover all of the structural attachments in sentences which the system will eventually encounter. Therefore, a system which relies only on a semantic model for accurate attachment will require constant update. Furthermore, it is often necessary to process new documents for new product lines not covered by the existing domain model, resulting in an ongoing need to update the domain model over time. The KANT machine translation system (Mitamura et al. 1991) queries the author to disambiguate interactively if the domain model cannot disambiguate a structural attachment automatically. This solution is not always satisfactory - interactive disambiguation is not always accurate, and it is always a time-consuming task, and hence costly in terms of overall system productivity. 218 In this paper, we present the results of an experiment which combines domainindependent heuristics with a semantic knowledge base. We explore a multiple-strategy approach which preserves a high degree of translation quality, while reducing both the need for interactive disam"
1999.tmi-1.22,C92-3168,1,0.861449,"Missing"
1999.tmi-1.22,P90-1004,0,0.0160681,"Missing"
2001.mtsummit-papers.43,C88-1021,0,0.58799,"Missing"
2001.mtsummit-papers.43,C90-3063,0,0.0994061,"Missing"
2001.mtsummit-papers.43,P98-1064,0,0.034423,"Missing"
2001.mtsummit-papers.43,C96-1021,0,0.0604466,"Missing"
2001.mtsummit-papers.43,J94-4002,0,0.159566,"Missing"
2001.mtsummit-papers.43,J90-4001,0,0.734918,"Missing"
2001.mtsummit-papers.43,P98-2143,0,0.0437377,"Missing"
2001.mtsummit-papers.43,1991.mtsummit-papers.9,1,0.724613,"-poor systems, it can be inferred that adding more linguistic knowledge reduces the need for scoring procedures to prune incorrect antecedents. If necessary, semantic knowledge can be used once syntactic rules have been exhausted. In the next sections, we explain the details of our resolution algorithm, present the results of an evaluation on texts drawn from technical manuals, and discuss some implications for current and future work. KANTOO Anaphora Resolution KANTOO is a knowledge-based, interlingual machine translation system, the most recent implementation of the original KANT MT system (Mitamura et al. 1991). KANTOO accepts Controlled English as input (Mitamura and Nyberg, 1995); the current input specification is referred to as KANT Controlled English (KCE). KCE places some restrictions on vocabulary and grammar; although some of the sentences in this study were rewritten to conform to KCE, we did not edit pronominal anaphors or any other constituents relevant to the anaphor resolution process. Identification of Possible Antecedents Possible antecedents for a given pronoun are identified according to a set of pre-defined constraints: 1. The candidate antecedent must be a noun, unit, tag, or conj"
2001.mtsummit-papers.43,C94-2189,0,\N,Missing
2001.mtsummit-papers.43,C98-2138,0,\N,Missing
2002.tmi-papers.13,C88-1021,0,0.191998,"Missing"
2002.tmi-papers.13,C90-3063,0,0.0606638,"Missing"
2002.tmi-papers.13,P98-1064,0,0.0292616,"Missing"
2002.tmi-papers.13,C96-1021,0,0.068191,"Missing"
2002.tmi-papers.13,J94-4002,0,0.0744051,"Missing"
2002.tmi-papers.13,J90-4001,0,0.105424,"Missing"
2002.tmi-papers.13,C94-2189,0,0.0488992,"Missing"
2002.tmi-papers.13,P98-2143,0,0.059915,"Missing"
2002.tmi-papers.13,1991.mtsummit-papers.9,1,0.480272,"on indicates performance comparable to that of score-based, knowledge-poor systems, it can be inferred that adding more linguistic knowledge reduces the need for scoring procedures to prune incorrect antecedents. If necessary, semantic knowledge can be used once syntactic rules have been exhausted. In the next sections, we discuss the details of our resolution algorithm, and the results of an evaluation on technical texts which were translated to Spanish and German. We conclude with a discussion of some implications for current and future work. 2 KANTOO Multilingual MT System The KANT System (Mitamura et al. 1991) is a knowledge-based, interlingual machine translation system, developed for multilingual translations of technical documents in various domains. Application domains include heavy equipment documentation, computer manuals, automotive documentation, and medical records written in controlled language (Mitamura and Nyberg 1995). KANTOO is the reimplementation of the original KANT MT system, and also accepts Controlled English as input. The current input specification is referred to as KANT Controlled English (KCE). KCE places some restrictions on vocabulary and grammar. Although some of the sent"
2003.mtsummit-systems.13,1991.mtsummit-papers.9,1,0.732638,"Missing"
2020.deelio-1.4,2020.acl-main.676,0,0.215327,"Missing"
2020.deelio-1.4,P18-1225,1,0.881941,"Missing"
2020.deelio-1.4,P98-1013,0,0.0283536,"Missing"
2020.deelio-1.4,K18-1031,0,0.0247595,"y Fluency, also known as naturalness or readability, is a measure of how fluent text is. The higher the fluency, the more it imitates grammatically and logically correct human text.13 1. P ERPLEXITY (PPL) is defined as: 1 P P L(S) = exp(− ln(pM (S))) |S| where S is a piece of text and pM (S) is the probability assigned to S by the language model. We finetune GPT-2 on a two-million review subset of YR (with a 500K additional validation split) and use this finetuned model for PPL evaluation. Outputs less likely to be seen in YR will typically have higher PPL. 2. SLOR (syntactic log-odds ratio) (Kann et al., 2018) is our main fluency metric. It modifies 2.5.3 Semantic Content Preservation (SCP) SCP assesses how closely each generated continuation (hypothesis) matches in semantic content to the ground truth distribution of continuations (reference). Since the latter is unavailable in this case, we use the prompt itself as a proxy for reference.14 We use what we call the Prompt-Continuation BertScore (BPRO). BPRO computes average BertScore (Zhang et al., 2019a) between each continuation and the prompt. BertScore computes pertoken BERT representations for both hypothesis and reference and aligns each hypo"
2020.deelio-1.4,K19-1079,0,0.0274561,"er SBLEU values represent higher inter-continuation diversity. 2. U NIQUE T RIGRAMS (UTR) (Tevet and Berant, 2020; Li et al., 2016) measures the ratio of unique to total trigrams in a population of generations. Higher UTR represents greater diversity. Since UTR is defined at the population level, it can assess the extent of crosscontinuation repetition. 3. T YPE -T OKEN R ATIO (TTR) is the ratio of unique to total tokens in a piece of text, and serves as a measure of intra-continuation diversity. The higher the TTR, the more varied the vocabulary in a continuation. 4. R ARE -W ORDS (RW ORDS) (See et al., 2019) is defined by the following: X ntrain (w) Es∼S [ − log ] Ntrain w∈s PPL by normalizing for individual tokens (e.g. “Zimbabwe” is less frequent than “France” but just as fluent), and serves as a better measure. Higher SLOR represents higher fluency. The equation for SLOR is as follows: Y 1 (ln(pM (S)) − ln( p(t))) SLOR(S) = |S| t∈S where |S |is the length of S (in tokens), pM (S) is the probability of S under language model M , and p(t) are the unconditional probabilities of individual tokens (or unigrams) t in S. We use the same finetuned GPT-2 model on YR as for PPL mentioned above for SLOR."
2020.deelio-1.4,N19-1131,0,0.035261,"e this finetuned model for PPL evaluation. Outputs less likely to be seen in YR will typically have higher PPL. 2. SLOR (syntactic log-odds ratio) (Kann et al., 2018) is our main fluency metric. It modifies 2.5.3 Semantic Content Preservation (SCP) SCP assesses how closely each generated continuation (hypothesis) matches in semantic content to the ground truth distribution of continuations (reference). Since the latter is unavailable in this case, we use the prompt itself as a proxy for reference.14 We use what we call the Prompt-Continuation BertScore (BPRO). BPRO computes average BertScore (Zhang et al., 2019a) between each continuation and the prompt. BertScore computes pertoken BERT representations for both hypothesis and reference and aligns each hypothesis token to a reference token. We prefer BertScore over symbolic measures (e.g BLEU) since it does not rely on exact string matching alone and allows soft matches between different parts of the input pair. 12 This is because we generate 100 continuations per test example. See Section §3.4 for more. 13 We evaluate perplexity and SLOR on the concatenations of the generated continuations with their corresponding prompts, and Spellcheck on the gene"
2020.deelio-1.4,D13-1170,0,0.00489758,"Missing"
2020.deelio-1.4,N03-1033,0,0.140447,"Missing"
2020.emnlp-demos.26,bethard-etal-2014-cleartk,0,0.0259715,"§2.1.2), an NER processor (Gardner et al., 2018) to find actors and movies from the retrieved reviews, a sentiment processor (Hutto and Gilbert, 2014) for sentence polarity, and an English-German translation processor. A ChatBot Workflow The case study considers the scenario where we have a corpus of movie reviews in English to answer complex queries (e.g., “movies with a positive sentiment starring by a certain actor”) by a 4 Related Work The framework shares some characteristics with UIMA (G¨otz and Suhre, 2004) backed systems, such as DKPro (Eckart de Castilho and Gurevych, 2014), ClearTK (Bethard et al., 2014) and cTakes (Khalifa and Meystre, 2015). There are NLP toolboxes like NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e"
2020.emnlp-demos.26,W16-4011,0,0.0416132,"Missing"
2020.emnlp-demos.26,N13-3004,0,0.0201171,"ike NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e Knowtator (Ogren, 2006), BRAT (Stenetorp et al., 2012), Anafora (Chen and Styler, 2013), GATE (Cunningham et al., 2013), WebAnno (Castilho, 2016), and YEDDA (Yang et al., 2018), our system provides a more flexi201 Figure 4: A system for diagnosis analysis and retrieval from clinical notes. The data-centric approach makes it easy to assemble a variety of components and UI elements. Example text was obtained from UNC School of Medicine. ble experience with customizable plug-ins, extendable data types, and full-fledged NLP support. The Prodigy tool by spaCy is not open-source and supports only pre-defined annotation tasks like NER. 5 Conclusions and Future Work We present a data-ce"
2020.emnlp-demos.26,clarke-etal-2012-nlp,0,0.0133224,"ovie reviews in English to answer complex queries (e.g., “movies with a positive sentiment starring by a certain actor”) by a 4 Related Work The framework shares some characteristics with UIMA (G¨otz and Suhre, 2004) backed systems, such as DKPro (Eckart de Castilho and Gurevych, 2014), ClearTK (Bethard et al., 2014) and cTakes (Khalifa and Meystre, 2015). There are NLP toolboxes like NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e Knowtator (Ogren, 2006), BRAT (Stenetorp et al., 2012), Anafora (Chen and Styler, 2013), GATE (Cunningham et al., 2013), WebAnno (Castilho, 2016), and YEDDA (Yang et al., 2018), our system provides a more flexi201 Figure 4: A system for diagnosis analysis and retrieval from clinical notes. The data-c"
2020.emnlp-demos.26,2020.acl-main.192,0,0.0302599,"hybrid modeling using both neural and symbolic features. Take retrieval for example, the framework offers retrieval processors (§2.2) that retrieve a coarse-grained candidate set with symbolic features (e.g., TF.IDF) first, and then refine the results with more expensive embeddingbased re-ranking (Nogueira and Cho, 2019). Likewise, fast embedding based search is facilitated with the Faiss library (Johnson et al., 2017). Shared modeling approaches. The uniform input/output representation for NLP tasks makes it easy to share the modeling approaches across diverse tasks. For example, similar to Jiang et al. (2020), all tasks involving the Span and Link data types as outputs (e.g., dependency parsing, relation extraction, coreference resolution) can potentially use the exact same neural network architecture for modeling. Further with the standardized APIs of our framework, users can spawn models for all such tasks using the same code with minimal edits. Top right of Figure 2 shows an example where the same relation extractor is implemented with dependency parser for a new task, and the only difference lies in accessing different data features. 2.2 Processors Universal data representation enables a unifo"
2020.emnlp-demos.26,D17-1018,0,0.0183671,"onents. 3 3.1 Case Studies A Clinical Information Workflow We demonstrate an information system for clinical diagnosis analysis, retrieval, and user interaction. Figure 4 shows an overview of the system. To build the workflow, we first define domain-specific data types, such as Clinical Entity Mention, via JSON config files as shown in Figure 2. We then develop processors for text processing: (1) we create an LSTM-based clinical NER processor (Boag et al., 2015), a Span-Relation model based relation extraction processor (He et al., 2018), and a coreference processor with the End-to-End model (Lee et al., 2017) to extract key information; (2) we build a report generation processor following Li et al. (2019) with extracted mentions and relations; (3) we build a simple keyword based dialogue system for user to interact using natural languages. The whole workflow is implemented with minimal engineering effort. For example, the workflow is assembled with just 20 lines of code; and the IE processors are implemented with around 50 lines of code by reusing libraries and models. 3.2 German user. The iterative workflow consists of a review retrieval processor based on the hybrid symbolic-neural feature model"
2020.emnlp-demos.26,P14-5010,0,0.295616,"tion techniques are used to produce summaries from diverse sources. To develop domain-specific NLP systems fast, it is highly desirable to have a unified open-source framework that supports: (1) seamless integration and interoperation across NLP functions ranging from text analysis to retrieval to generation; (2) rich user interaction for data visualization and annotation; (3) extensible plug-ins for customized components; and (4) highly reusable components. A wealth of NLP toolkits exist (§4), such as spaCy (Honnibal and Montani, 2017), DKPro (Eckart de Castilho and Gurevych, 2014), CoreNLP (Manning et al., 2014), for pipelining multiple NLP functions; BRAT (Stenetorp et al., 2012) and YEDDA (Yang et al., 2018) for annotating certain types of data. None of them have addressed all the desiderata uniformly. Combining them for a complete workflow requires non-trivial effort and expertise (e.g., ad-hoc gluing code), posing challenges for maintenance and upgrading. We introduce a new unified framework to support complex NLP workflows that involve text data ingestion, analysis, retrieval, generation, visualization, and annotation. The framework provides an infrastructure to simply plug in arbitrary NLP func"
2020.emnlp-demos.26,N06-4006,0,0.093763,"(Khalifa and Meystre, 2015). There are NLP toolboxes like NLTK (Bird and Loper, 2016) and AllenNLP (Gardner et al., 2018), GluonNLP (Guo et al., 2019), NLP pipelines like Stanford CoreNLP (Manning et al., 2014), SpaCy (Honnibal and Montani, 2017), and Illinois Curator (Clarke et al., 2012). As in §2.2, our system develops a convenient scaffold and provides a rich set of utilities to reconcile the benefits of symbolic data system, neural modeling, and human interaction, making it suit for building complex workflows. Compared to open-source text annotation toolkits, such as Prot´eg´e Knowtator (Ogren, 2006), BRAT (Stenetorp et al., 2012), Anafora (Chen and Styler, 2013), GATE (Cunningham et al., 2013), WebAnno (Castilho, 2016), and YEDDA (Yang et al., 2018), our system provides a more flexi201 Figure 4: A system for diagnosis analysis and retrieval from clinical notes. The data-centric approach makes it easy to assemble a variety of components and UI elements. Example text was obtained from UNC School of Medicine. ble experience with customizable plug-ins, extendable data types, and full-fledged NLP support. The Prodigy tool by spaCy is not open-source and supports only pre-defined annotation ta"
2020.emnlp-demos.26,E12-2021,0,0.310908,"To develop domain-specific NLP systems fast, it is highly desirable to have a unified open-source framework that supports: (1) seamless integration and interoperation across NLP functions ranging from text analysis to retrieval to generation; (2) rich user interaction for data visualization and annotation; (3) extensible plug-ins for customized components; and (4) highly reusable components. A wealth of NLP toolkits exist (§4), such as spaCy (Honnibal and Montani, 2017), DKPro (Eckart de Castilho and Gurevych, 2014), CoreNLP (Manning et al., 2014), for pipelining multiple NLP functions; BRAT (Stenetorp et al., 2012) and YEDDA (Yang et al., 2018) for annotating certain types of data. None of them have addressed all the desiderata uniformly. Combining them for a complete workflow requires non-trivial effort and expertise (e.g., ad-hoc gluing code), posing challenges for maintenance and upgrading. We introduce a new unified framework to support complex NLP workflows that involve text data ingestion, analysis, retrieval, generation, visualization, and annotation. The framework provides an infrastructure to simply plug in arbitrary NLP functions and offers pre-built and reusable components to build desired wo"
2020.emnlp-demos.26,P18-4006,1,0.917441,"ems fast, it is highly desirable to have a unified open-source framework that supports: (1) seamless integration and interoperation across NLP functions ranging from text analysis to retrieval to generation; (2) rich user interaction for data visualization and annotation; (3) extensible plug-ins for customized components; and (4) highly reusable components. A wealth of NLP toolkits exist (§4), such as spaCy (Honnibal and Montani, 2017), DKPro (Eckart de Castilho and Gurevych, 2014), CoreNLP (Manning et al., 2014), for pipelining multiple NLP functions; BRAT (Stenetorp et al., 2012) and YEDDA (Yang et al., 2018) for annotating certain types of data. None of them have addressed all the desiderata uniformly. Combining them for a complete workflow requires non-trivial effort and expertise (e.g., ad-hoc gluing code), posing challenges for maintenance and upgrading. We introduce a new unified framework to support complex NLP workflows that involve text data ingestion, analysis, retrieval, generation, visualization, and annotation. The framework provides an infrastructure to simply plug in arbitrary NLP functions and offers pre-built and reusable components to build desired workflows. Importantly, the fram"
2020.lrec-1.253,M95-1012,0,0.225125,"tion and answer” sessions are divided based on questioner. Then questions/answers are divided into individual question/answer. Finally, the set of a question and an answer are identified. 3. Related work The process of selecting sentences in a document is called segmentation or sentence boundary detection(Zeldes et al., 2019; Azzi et al., 2019). In this section, we discuss the studies related to the process of segmentation or sentence boundary detection. Sentence boundary detection is a widely used technique. A variety of systems use lists of hand-crafted regular expressions and abbreviations(Aberdeen et al., 1995) Gillick discussed the challenges associated with it, in addition to the relevant features, (Gillick, 2009). Azzi et. al. organized FinSBD-2019, which focused on sentence boundary detection in PDF noisy text in the financial domain(Azzi et al., 2019). Their shared task aimed at collecting systems for extracting well segmented sentences from financial prospectuses by detecting and marking their beginning and ending boundaries. Sentences are basic units of a written language, and detecting the beginning and end of sentences, or sentence boundary detection (SBD) is a foundational first step in se"
2020.lrec-1.253,W19-5512,0,0.010904,"ing steps are needed in order to determine the argument structure in the Minutes. First, The minutes are divided into “deliberation”, “questions and answers”, and so on. Then the deliberation sessions are divided into “proposition” and “discussion” and “voting”. The “question and answer” sessions are divided based on questioner. Then questions/answers are divided into individual question/answer. Finally, the set of a question and an answer are identified. 3. Related work The process of selecting sentences in a document is called segmentation or sentence boundary detection(Zeldes et al., 2019; Azzi et al., 2019). In this section, we discuss the studies related to the process of segmentation or sentence boundary detection. Sentence boundary detection is a widely used technique. A variety of systems use lists of hand-crafted regular expressions and abbreviations(Aberdeen et al., 1995) Gillick discussed the challenges associated with it, in addition to the relevant features, (Gillick, 2009). Azzi et. al. organized FinSBD-2019, which focused on sentence boundary detection in PDF noisy text in the financial domain(Azzi et al., 2019). Their shared task aimed at collecting systems for extracting well segmen"
2020.lrec-1.253,N09-2061,0,0.0140529,"ion/answer. Finally, the set of a question and an answer are identified. 3. Related work The process of selecting sentences in a document is called segmentation or sentence boundary detection(Zeldes et al., 2019; Azzi et al., 2019). In this section, we discuss the studies related to the process of segmentation or sentence boundary detection. Sentence boundary detection is a widely used technique. A variety of systems use lists of hand-crafted regular expressions and abbreviations(Aberdeen et al., 1995) Gillick discussed the challenges associated with it, in addition to the relevant features, (Gillick, 2009). Azzi et. al. organized FinSBD-2019, which focused on sentence boundary detection in PDF noisy text in the financial domain(Azzi et al., 2019). Their shared task aimed at collecting systems for extracting well segmented sentences from financial prospectuses by detecting and marking their beginning and ending boundaries. Sentences are basic units of a written language, and detecting the beginning and end of sentences, or sentence boundary detection (SBD) is a foundational first step in several natural language processing (NLP) applications, such as POS tagging; syntactic, semantic, and discour"
2020.lrec-1.253,W19-6407,0,0.0431248,"heir shared task aimed at collecting systems for extracting well segmented sentences from financial prospectuses by detecting and marking their beginning and ending boundaries. Sentences are basic units of a written language, and detecting the beginning and end of sentences, or sentence boundary detection (SBD) is a foundational first step in several natural language processing (NLP) applications, such as POS tagging; syntactic, semantic, and discourse parsing; information extraction; or machine translation. Judge et. al. proposed a Shared Task on structure extraction from financial documents(Juge et al., 2019). Systems participating in this shared task were given a sample collection of financial prospectuses with different levels of structures and lengths. The participant’s systems extract structural information and build a table of content. 4. Dataset In this study, we aim to extract question-and-answer sets (QA-Sets) for structuring and summarizing. 4.1. Annotation procedure We design a segmentation task to extract the QA-Sets in the assembly minutes. We create the QA-Sets using both Tokyo Metropolitan assembly minutes2 and Togikai-dayori (Newsletter in Japanese)3 . A newsletter can be regarded a"
2020.lrec-1.253,W19-2713,0,0.024106,"1). Therefore, following steps are needed in order to determine the argument structure in the Minutes. First, The minutes are divided into “deliberation”, “questions and answers”, and so on. Then the deliberation sessions are divided into “proposition” and “discussion” and “voting”. The “question and answer” sessions are divided based on questioner. Then questions/answers are divided into individual question/answer. Finally, the set of a question and an answer are identified. 3. Related work The process of selecting sentences in a document is called segmentation or sentence boundary detection(Zeldes et al., 2019; Azzi et al., 2019). In this section, we discuss the studies related to the process of segmentation or sentence boundary detection. Sentence boundary detection is a widely used technique. A variety of systems use lists of hand-crafted regular expressions and abbreviations(Aberdeen et al., 1995) Gillick discussed the challenges associated with it, in addition to the relevant features, (Gillick, 2009). Azzi et. al. organized FinSBD-2019, which focused on sentence boundary detection in PDF noisy text in the financial domain(Azzi et al., 2019). Their shared task aimed at collecting systems for ex"
2021.conll-1.39,P14-2082,0,0.0247002,"rning (CoNLL), pages 496–517 November 10–11, 2021. ©2021 Association for Computational Linguistics to the same event is often non-trivial. Occasionally, event mentions only share a partial identity (quasiidentity). In this work, we present a new dataset for CDEC that attempts to overcome both issues. Earlier efforts on CDEC dataset collection were limited to specific pre-defined event types, restricting the scope of event mentions that could be studied. In this work, we instead annotate mentions of all types, i.e., open-domain events (Araki and Mitamura, 2018), and provide a dense annotation (Cassidy et al., 2014) by checking for coreference relationship between every mention pair in all underlying document pairs. We compile documents from the publicly available English Wikinews.3 To facilitate our goal of dense annotation of mentions and their coreference, we develop and release a new easy-to-use annotation tool that allows linking text spans across documents. We crowdsource coreference annotations on Mechanical Turk.4 Prior work has attributed the quasi-identity behavior of events to two specific phenomena, membership and subevent (Hovy et al., 2013). However, its implications in cross-document setti"
2021.conll-1.39,2021.naacl-main.198,0,0.0290744,"ersion of the original ECB dataset (Bejan and Harabagiu, 2008). ECB+ suffers from a major limitation with coreference annotations restricted to only the first few sentences in the documents. However, CDEC is a long-range phenomenon, and there is a need for more densely annotated datasets. Many other datasets have since been curated for the task of CDEC. Some related works include, MEANTIME (Minard et al., 2016), Event hoppers (Song et al., 2018), Gun Violence Corpus (GVC) 497 (Vossen et al., 2018), Football Coreference Corpus (FCC) (Bugert et al., 2020), and Wikipedia Event Coreference (WEC) (Eirew et al., 2021). However, most CDEC systems are still evaluated primarily on ECB+. Additionally, all of these datasets do not account for the quasi-identity nature of events. Though compiled from Wikinews, CDEC annotations in the MEANTIME corpus were limited to events with participants from a pre-defined list of 44 seed entities. While the FCC corpus was also crowdsourced, the annotation unit was an entire sentence instead of a single event mention. WEC corpus uses hyperlinks from Wikipedia but primarily handles referential events. In this work, we use open-domain events and treat an event mention as our ann"
2021.conll-1.39,recasens-etal-2012-annotating,0,0.307881,". We compile documents from the publicly available English Wikinews.3 To facilitate our goal of dense annotation of mentions and their coreference, we develop and release a new easy-to-use annotation tool that allows linking text spans across documents. We crowdsource coreference annotations on Mechanical Turk.4 Prior work has attributed the quasi-identity behavior of events to two specific phenomena, membership and subevent (Hovy et al., 2013). However, its implications in cross-document settings remain unclear. In this work, we specifically focus on a cross-document setup. As highlighted by Recasens et al. (2012), a direct annotation of quasi-identity relations is hard because annotators might not be familiar with the phenomenon. Therefore, we propose a new annotation workflow that allows for easy determination of quasi-identity links. To this end, we collect evidence for time, location, and participant(s) overlap between corefering mentions. We also collect information regarding any potential inclusion relationship between the mention pair. Our workflow allowed us to empirically identify a new type of quasi-identity, spatiotemporal continuity, in addition to the existing types defined by Hovy et al."
2021.conll-1.39,W18-2501,0,0.0157965,"ble 1: An overview of the compiled CDEC dataset. work, our dataset constitutes of a single topic (Disaster and accidents) and 55 subtopics (individual storylines). We restrict CDEC annotations to subtopics that contain 3 or 4 documents. Our algorithm aims for completeness of the CDEC dataset by maximizing for intra-subtopic and minimizing inter-subtopic coreference. Event Mention Identification: To annotate the event mentions in the above-collected documents, we first run a combination of mention detection systems. Specifically, we use the OpenIE system (Stanovsky et al., 2018) from AllenNLP (Gardner et al., 2018) and an open-domain event extraction system (Araki and Mitamura, 2018). The former is effective at extracting verbal events, whereas the latter is good at nominal events. In contrast to most prior work, we do not restrict the mentions to specific event types or salient events. We believe it is important to study all underlying events to achieve a complete understanding of the corpus. Since the quality of mention identification is critical to our CDEC dataset, we ask an expert to go through the automatically identified mentions and add/edit/delete mentions using the Stave annotation tool (Liu e"
2021.conll-1.39,L18-1558,0,0.0608604,"Missing"
2021.conll-1.39,W13-1203,1,0.717553,"including the semantics of the event mention, its arguments, and the document context. Resolving coreference across documents is more challenging, as it requires modeling identity over a much longer context. To this end, we identify two major issues with existing cross-document event coreference (CDEC) datasets that limit the progress on this task. First, many prior datasets often annotate coreference only on a restricted set of event types, limiting the coverage of mentions in the dataset. Second, many datasets and models insufficiently tackle the concept of event identity. As highlighted by Hovy et al. (2013), the decision of whether two mentions refer 2 A mention is a linguistic expression in text that denotes a specific instance of an event. 496 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 496–517 November 10–11, 2021. ©2021 Association for Computational Linguistics to the same event is often non-trivial. Occasionally, event mentions only share a partial identity (quasiidentity). In this work, we present a new dataset for CDEC that attempts to overcome both issues. Earlier efforts on CDEC dataset collection were limited to specific pre-defined even"
2021.conll-1.39,N18-1081,0,0.044499,"Missing"
2021.conll-1.39,C16-1183,0,0.0499181,"Missing"
2021.conll-1.39,L18-1480,0,0.0227632,"e. For cross-document coreference, ECB+ (Cybulska and Vossen, 2014) is a widely popular benchmark and is an extended version of the original ECB dataset (Bejan and Harabagiu, 2008). ECB+ suffers from a major limitation with coreference annotations restricted to only the first few sentences in the documents. However, CDEC is a long-range phenomenon, and there is a need for more densely annotated datasets. Many other datasets have since been curated for the task of CDEC. Some related works include, MEANTIME (Minard et al., 2016), Event hoppers (Song et al., 2018), Gun Violence Corpus (GVC) 497 (Vossen et al., 2018), Football Coreference Corpus (FCC) (Bugert et al., 2020), and Wikipedia Event Coreference (WEC) (Eirew et al., 2021). However, most CDEC systems are still evaluated primarily on ECB+. Additionally, all of these datasets do not account for the quasi-identity nature of events. Though compiled from Wikinews, CDEC annotations in the MEANTIME corpus were limited to events with participants from a pre-defined list of 44 seed entities. While the FCC corpus was also crowdsourced, the annotation unit was an entire sentence instead of a single event mention. WEC corpus uses hyperlinks from Wikipedia bu"
2021.conll-1.39,D19-6201,0,0.029257,"Missing"
2021.findings-acl.84,2020.acl-main.676,0,0.0267569,"§4). Numerous previously mentioned DA techniques, e.g. (Wei and Zou, 2019; Chen et al., 2020b; Anaby-Tavor et al., 2020), have been used or can be used for text classification tasks. 5.1 Question Answering (QA) 5.4 Parsing Tasks Jia and Liang (2016) propose DATA RECOMBINA TION for injecting task-specific priors to neural semantic parsers. A synchronous context-free grammar (SCFG) is induced from training data, and new ""recombinant"" examples are sampled. Yu et al. (2020) introduce G RAPPA, a pretraining approach for table semantic parsing, and generate synthetic question-SQL pairs via an SCFG. Andreas (2020) 973 use compositionality to construct synthetic examples for downstream tasks like semantic parsing. Fragments of original examples are replaced with fragments from other examples in similar contexts. Vania et al. (2019) investigate DA for lowresource dependency parsing including dependency tree morphing from Sahin ¸ and Steedman (2018) (Figure 2) and modified nonce sentence generation from Gulordava et al. (2018), which replaces content words with other words of the same POS, morphological features, and dependency labels. 5.5 Grammatical Error Correction (GEC) Lack of parallel data is typica"
2021.findings-acl.84,2020.acl-main.499,0,0.0168343,"Longpre et al. (2019) investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang et al. (2019) propose a DA method using distant supervision to improve BERT finetuning for opendomain QA. Riabi et al. (2020) leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al. (2019) propose XLDA, or CROSS - LINGUAL DA, which substitutes a portion of the input text with its translation in another language, improving performance across multiple languages on NLI tasks including the SQuAD QA task. Asai and Hajishirzi (2020) use logical and linguistic knowledge to generate additional training data to improve the accuracy and consistency of QA responses by models. Yu et al. (2018) introduce a new QA architecture called QANet that shows improved performance on SQuAD when combined with augmented data generated using backtranslation. 5.3 Tasks Summarization Fabbri et al. (2020) investigate backtranslation as a DA method for few-shot abstractive summarization with the use of a consistency loss inspired by UDA. Parida and Motlicek (2019) propose an iterative DA approach for abstractive summarization that uses a mix of"
2021.findings-acl.84,W18-6111,0,0.0244136,"Vania et al. (2019) investigate DA for lowresource dependency parsing including dependency tree morphing from Sahin ¸ and Steedman (2018) (Figure 2) and modified nonce sentence generation from Gulordava et al. (2018), which replaces content words with other words of the same POS, morphological features, and dependency labels. 5.5 Grammatical Error Correction (GEC) Lack of parallel data is typically a barrier for GEC. Various works have thus looked at DA methods for GEC. We discuss some here, and more can be found in Table 2 in Appendix C. There is work that makes use of additional resources. Boyd (2018) use German edits from Wikipedia revision history and use those relating to GEC as augmented training data. Zhang et al. (2019b) explore multi-task transfer, or the use of annotated data from other tasks. There is also work that adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated s"
2021.findings-acl.84,P19-1175,0,0.0555398,"Missing"
2021.findings-acl.84,2020.acl-main.12,0,0.0165365,"recent work in vision (Gontijo-Lopes et al., 2020) has proposed that affinity (the distributional shift caused by DA) and diversity (the complexity of the Working in specialized domains such as those with domain-specific vocabulary and jargon (e.g. medicine) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data (Zhang et al., 2019a; Herzig et al., 2020; Campagna et al., 2020; Zhong et al., 2020). 975 Working with low-resource languages may present similar difficulties as specialized domains. Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language family or very distinctive in linguistic and typological terms. For example, those which are language isolates or lack high-resource cognates. More vision-inspired techniques: Although many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV DA techniques motivate"
2021.findings-acl.84,W17-4714,0,0.062208,"Missing"
2021.findings-acl.84,W19-4423,0,0.0135446,"re is work that makes use of additional resources. Boyd (2018) use German edits from Wikipedia revision history and use those relating to GEC as augmented training data. Zhang et al. (2019b) explore multi-task transfer, or the use of annotated data from other tasks. There is also work that adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated samples along with POS-specific noising. There have also been approaches to improve the diversity of generated errors. Wan et al. (2020) investigate noising through editing the latent representations of grammatical sentences, and Xie et al. (2018) use a neural sequence transduction model and beam search noising procedures. 5.6 Neural Machine Translation (NMT) There are many works which have investigated DA for NMT. We highlighted some in §3 and §4.1, e.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019). We discuss some further ones here"
2021.findings-acl.84,2020.coling-main.343,0,0.028145,"ation (QMDS) called Q MDS C NN and Q MD S I R by modifying CNN/DM (Hermann et al., 2015) and mining search-query logs, respectively. Sequence Tagging Tasks Ding et al. (2020) propose DAGA, a two-step DA process. First, a language model over sequences of tags and words linearized as per a certain scheme is learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. Sahin ¸ and Steedman (2018), discussed in §3.1, use dependency tree morphing (Figure 2) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a DA method based on M IX U P called S EQ M IX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection. In this section, we discuss several DA works for common NLP tasks.2 We focus on nonclassification tasks as classification is worked on by default, and well covered in earlier sections (e.g. §3 and"
2021.findings-acl.84,N19-1423,0,0.014469,"tation. Gao et al. (2019) advocate retaining the full distribution through ""soft"" augmented examples, showing gains on machine translation. Nie et al. (2020) augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders, Ng et al. (2020) use a corrupt-andreconstruct approach, with the corruption function q(x0 |x) masking an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). Their approach works well on domain-shifted test sets across 9 datasets on sentiment, NLI, and NMT. Feng et al. (2019) propose a task called S EMAN TIC T EXT E XCHANGE (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the replacement entity (RE). They do so by using a system called SMERTI and a masked LM approach. While not proposed directly for DA, it can be used as such, as investigated in Feng et al. (2020). Rather than starting from an existing example and modifying it, some model-based DA approaches directly es"
2021.findings-acl.84,2020.emnlp-main.488,0,0.0364984,"rida and Motlicek (2019) propose an iterative DA approach for abstractive summarization that uses a mix of synthetic and real data, where the former is generated from Common Crawl. Zhu et al. (2019) introduce a query-focused summarization (Dang, 2005) dataset collected using Wikipedia called W IKI R EF which can be used for DA. Pasunuru et al. (2021) use DA methods to construct two training datasets for Query-focused Multi-Document Summarization (QMDS) called Q MDS C NN and Q MD S I R by modifying CNN/DM (Hermann et al., 2015) and mining search-query logs, respectively. Sequence Tagging Tasks Ding et al. (2020) propose DAGA, a two-step DA process. First, a language model over sequences of tags and words linearized as per a certain scheme is learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. Sahin ¸ and Steedman (2018), discussed in §3.1, use dependency tree morphing (Figure 2) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show im"
2021.findings-acl.84,P17-2090,0,0.174971,"on NLP applications.2 4.1 Low-Resource Languages Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet (Miller, 1995) may be difficult to use effectively here.3 There are ways to leverage high-resource languages for low-resource languages, particularly if they have similar linguistic properties. Xia et al. (2019) use this approach to improve low-resource NMT. Li et al. (2020b) use backtranslation and selflearning to generate augmented training data. Inspired by work in CV, Fadaee et al. (2017) generate additional training examples that contain lowfrequency (rare) words in synthetically created contexts. Qin et al. (2020) present a DA framework to generate multi-lingual code-switching data to finetune multilingual-BERT. It encourages the alignment of representations from source and multiple target languages once by mixing their context information. They see improved performance across 5 tasks with 19 languages. 3 Low-resource language challenges discussed more in §6. DA Method S YNONYM R EPLACEMENT (Zhang et al., 2015) R ANDOM D ELETION (Wei and Zou, 2019) R ANDOM S WAP (Wei and Zou"
2021.findings-acl.84,2020.deelio-1.4,1,0.884715,"an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). Their approach works well on domain-shifted test sets across 9 datasets on sentiment, NLI, and NMT. Feng et al. (2019) propose a task called S EMAN TIC T EXT E XCHANGE (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the replacement entity (RE). They do so by using a system called SMERTI and a masked LM approach. While not proposed directly for DA, it can be used as such, as investigated in Feng et al. (2020). Rather than starting from an existing example and modifying it, some model-based DA approaches directly estimate a generative process from the training set and sample from it. AnabyTavor et al. (2020) learn a label-conditioned generator by finetuning GPT-2 (Radford et al., 2019) on the training data, using this to generate candidate examples per class. A classifier trained on the original training set is then used to select top k candidate examples which confidently belong to the respective class for augmentation. Quteineh et al. (2020) use a similar label-conditioned GPT-2 generation method"
2021.findings-acl.84,P19-1555,0,0.106378,"yashi (2018) and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs (Kobayashi, 2018) and transformers (Yang et al., 2020) have also been used for augmentation. Kobayashi (2018) generate augmented examples by replacing words with others randomly drawn according to the recurrent language model’s distribution based on the current context (illustration in Figure 3). Yang et al. (2020) propose GDAUGc which generates synthetic examples using pretrained transformer language models, and selects the most informative and diverse set for augmentation. Gao et al. (2019) advocate retaining the full distribution through ""soft"" augmented examples, showing gains on machine translation. Nie et al. (2020) augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders, Ng et al. (2020) use a corrupt-andreconstruct approach, with the corruption function q(x0 |x) masking an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). The"
2021.findings-acl.84,2020.acl-main.60,0,0.044451,"Missing"
2021.findings-acl.84,W17-3518,0,0.0229672,"the vocabulary. Nguyen et al. (2020) propose DATA D IVERSIFICATION which merges original training data with the predictions of several forward and backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown (2019) inject Gaussian noise into a trained decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentretrain loop helps performance on E2E-NLG. 5.8 Open-Ended & Conditional Generation There has been li"
2021.findings-acl.84,P18-2103,0,0.0136442,"ion facilitates curriculum learning for training triplet networks for few-shot text classification. Lee et al. (2021) use T5 to generate additional examples for data-scarce classes. 4.5 Adversarial Examples (AVEs) Adversarial examples can be generated using innocuous label-preserving transformations (e.g. paraphrasing) that fool state-of-the-art NLP models, as shown in Jia et al. (2019). Specifically, they add sentences with distractor spans to passages to construct AVEs for span-based QA. Zhang et al. (2019d) construct AVEs for paraphrase detection using word swapping. Kang et al. (2018) and Glockner et al. (2018) create AVEs for textual entailment using WordNet relations. 5 5.2 Longpre et al. (2019) investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang et al. (2019) propose a DA method using distant supervision to improve BERT finetuning for opendomain QA. Riabi et al. (2020) leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al. (2019) propose XLDA, or CROSS - LINGUAL DA, which substitutes a portion of the input text with its translation in another language, improving performance ac"
2021.findings-acl.84,W19-5205,0,0.0602769,"Missing"
2021.findings-acl.84,2021.tacl-1.3,0,0.0429607,"Missing"
2021.findings-acl.84,W19-4427,0,0.0680803,"u, 2019) BACKTRANSLATION (Sennrich et al., 2016) SCPN (Wieting and Gimpel, 2017) S EMANTIC T EXT E XCHANGE (Feng et al., 2019) C ONTEXTUAL AUG (Kobayashi, 2018) LAMBADA (Anaby-Tavor et al., 2020) GECA (Andreas, 2020) S EQ M IX U P (Guo et al., 2020) S WITCH O UT (Wang et al., 2018b) E MIX (Jindal et al., 2020a) S PEECH M IX (Jindal et al., 2020b) M IX T EXT (Chen et al., 2020c) S IGNED G RAPH (Chen et al., 2020b) DT REE M ORPH (Sahin ¸ and Steedman, 2018) Sub2 (Shi et al., 2021) DAGA (Ding et al., 2020) WN-H YPERS (Feng et al., 2020) S YNTHETIC N OISE (Feng et al., 2020) UE DIN -MS (DA part) (Grundkiewicz et al., 2019) N ONCE (Gulordava et al., 2018) XLDA (Singh et al., 2019) S EQ M IX (Zhang et al., 2020) S LOT-S UB -LM (Louvan and Magnini, 2020) UBT & TBT (Vaibhav et al., 2019) S OFT C ONTEXTUAL DA (Gao et al., 2019) DATA D IVERSIFICATION (Nguyen et al., 2020) D I PS (Kumar et al., 2019a) AUGMENTED SBERT (Thakur et al., 2021) Ext.Know Pretrained Preprocess × × × 3 3 3 3 3 × × × × × × × × × × × × × × 3 3 3 3 3 3 3 3 3 × × × × × × × × × × × × × × × × × 3 × 3 3 × × × × × × × × tok tok tok Depends const const tok tok tok dep dep tok const+KWE tok tok const Depends tok tok Depends tok Depends tok - Level Task-"
2021.findings-acl.84,D19-1530,0,0.037302,"Missing"
2021.findings-acl.84,D19-5601,0,0.0162949,"nts randomly chosen words in a sentence using a contextual mixture of multiple related words over the vocabulary. Nguyen et al. (2020) propose DATA D IVERSIFICATION which merges original training data with the predictions of several forward and backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown (2019) inject Gaussian noise into a trained decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentre"
2021.findings-acl.84,2020.acl-main.398,0,0.0158664,"-scale experiment. A recent work in vision (Gontijo-Lopes et al., 2020) has proposed that affinity (the distributional shift caused by DA) and diversity (the complexity of the Working in specialized domains such as those with domain-specific vocabulary and jargon (e.g. medicine) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data (Zhang et al., 2019a; Herzig et al., 2020; Campagna et al., 2020; Zhong et al., 2020). 975 Working with low-resource languages may present similar difficulties as specialized domains. Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language family or very distinctive in linguistic and typological terms. For example, those which are language isolates or lack high-resource cognates. More vision-inspired techniques: Although many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV"
2021.findings-acl.84,C18-1105,0,0.0264481,"TIC N OISE (randomly perturbing non-terminal characters in words) are useful, and the quality of generated text improves to a peak at ≈ 3x the original amount of training data. 5.9 Dialogue Most DA approaches for dialogue focus on taskoriented dialogue. We outline some below, and more can be found in Table 4 in Appendix C. Quan and Xiong (2019) present sentence and word-level DA approaches for end-to-end taskoriented dialogue. Louvan and Magnini (2020) propose LIGHTWEIGHT AUGMENTATION, a set of word-span and sentence-level DA methods for lowresource slot filling and intent classification. 974 Hou et al. (2018) present a seq2seq DA framework to augment dialogue utterances for dialogue language understanding (Young et al., 2013), including a diversity rank to produce diverse utterances. Zhang et al. (2019c) propose MADA to generate diverse responses using the property that several valid responses exist for a dialogue context. There is also DA work for spoken dialogue. Hou et al. (2018), Kim et al. (2019), Zhao et al. (2019), and Yoo et al. (2019) investigate DA methods for dialogue and spoken language understanding (SLU), including generative latent variable models. 5.10 Multimodal Tasks DA technique"
2021.findings-acl.84,P18-1225,1,0.822205,"how that data augmentation facilitates curriculum learning for training triplet networks for few-shot text classification. Lee et al. (2021) use T5 to generate additional examples for data-scarce classes. 4.5 Adversarial Examples (AVEs) Adversarial examples can be generated using innocuous label-preserving transformations (e.g. paraphrasing) that fool state-of-the-art NLP models, as shown in Jia et al. (2019). Specifically, they add sentences with distractor spans to passages to construct AVEs for span-based QA. Zhang et al. (2019d) construct AVEs for paraphrase detection using word swapping. Kang et al. (2018) and Glockner et al. (2018) create AVEs for textual entailment using WordNet relations. 5 5.2 Longpre et al. (2019) investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang et al. (2019) propose a DA method using distant supervision to improve BERT finetuning for opendomain QA. Riabi et al. (2020) leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al. (2019) propose XLDA, or CROSS - LINGUAL DA, which substitutes a portion of the input text with its translation in another languag"
2021.findings-acl.84,N19-1333,0,0.0422133,"Missing"
2021.findings-acl.84,2020.webnlg-1.9,0,0.0351998,"backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown (2019) inject Gaussian noise into a trained decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentretrain loop helps performance on E2E-NLG. 5.8 Open-Ended & Conditional Generation There has been limited work on DA for open-ended and conditional text generation. Feng et al. (2020) experiment with a suite of DA methods for finetuning GPT-2 on a"
2021.findings-acl.84,2020.emnlp-main.97,0,0.0312233,"ntext (illustration in Figure 3). Yang et al. (2020) propose GDAUGc which generates synthetic examples using pretrained transformer language models, and selects the most informative and diverse set for augmentation. Gao et al. (2019) advocate retaining the full distribution through ""soft"" augmented examples, showing gains on machine translation. Nie et al. (2020) augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders, Ng et al. (2020) use a corrupt-andreconstruct approach, with the corruption function q(x0 |x) masking an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). Their approach works well on domain-shifted test sets across 9 datasets on sentiment, NLI, and NMT. Feng et al. (2019) propose a task called S EMAN TIC T EXT E XCHANGE (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the replacement entity (RE). They do so by using a system called SMERTI and a masked LM app"
2021.findings-acl.84,P15-2070,0,0.0490698,"Missing"
2021.findings-acl.84,2020.emnlp-main.600,0,0.0356232,"directly for DA, it can be used as such, as investigated in Feng et al. (2020). Rather than starting from an existing example and modifying it, some model-based DA approaches directly estimate a generative process from the training set and sample from it. AnabyTavor et al. (2020) learn a label-conditioned generator by finetuning GPT-2 (Radford et al., 2019) on the training data, using this to generate candidate examples per class. A classifier trained on the original training set is then used to select top k candidate examples which confidently belong to the respective class for augmentation. Quteineh et al. (2020) use a similar label-conditioned GPT-2 generation method, and demonstrate its effectiveness as a DA method in an active learning setup. Other approaches include syntactic or controlled paraphrasing (Iyyer et al., 2018; Kumar et al., 2020), document or story-level paraphrasing (Gangal et al., 2021), augmenting misclassified examples (Dreossi et al., 2018), BERT cross-encoder labeling of new inputs (Thakur et al., 2021), and guided generation using large-scale generative language models (Liu et al., 2020b,c). Models can also learn to combine together simpler DA primitives (Cubuk et al., 2018; Ra"
2021.findings-acl.84,D18-1545,0,0.301879,"ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant. 3 Techniques & Methods We now discuss some methodologically representative DA techniques which are relevant to all tasks via the extensibility of their formulation.2 969 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. 3.2 Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Sahin ¸ and Steedman (2018) 3.1 Rule-Based Techniques Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model’s feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017; Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei"
2021.findings-acl.84,P16-1009,0,0.621579,"P (Guo et al., 2020) generalizes M IX U P for sequence transduction tasks in two ways - the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like S WITCH O UT (Wang et al., 2018a). 3.3 Model-Based Techniques Seq2seq and language models have also been used for DA. The popular BACKTRANSLATION method (Sennrich et al., 2016) translates a sequence into another language and then back into the original language. Kumar et al. (2019a) train seq2seq models with their proposed method DiPS which learns to generate diverse paraphrases of input text using a modified decoder with a submodular objective, 970 Figure 3: Contextual Augmentation, Kobayashi (2018) and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs (Kobayashi, 2018) and transformers (Yang et al., 2020) have also been used for augmentation. Kobayashi (2018) generate augmented examples by replacing words with o"
2021.findings-acl.84,2021.findings-acl.307,0,0.0729355,"Missing"
2021.findings-acl.84,D19-6504,0,0.0596753,"Missing"
2021.findings-acl.84,2021.naacl-main.28,0,0.0634242,"per class. A classifier trained on the original training set is then used to select top k candidate examples which confidently belong to the respective class for augmentation. Quteineh et al. (2020) use a similar label-conditioned GPT-2 generation method, and demonstrate its effectiveness as a DA method in an active learning setup. Other approaches include syntactic or controlled paraphrasing (Iyyer et al., 2018; Kumar et al., 2020), document or story-level paraphrasing (Gangal et al., 2021), augmenting misclassified examples (Dreossi et al., 2018), BERT cross-encoder labeling of new inputs (Thakur et al., 2021), and guided generation using large-scale generative language models (Liu et al., 2020b,c). Models can also learn to combine together simpler DA primitives (Cubuk et al., 2018; Ratner et al., 2017) or add human-in-the-loop (Kaushik et al., 2020, 2021). 971 4 Applications In this section, we discuss several DA methods for some common NLP applications.2 4.1 Low-Resource Languages Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet (Miller, 1995) may be difficult to use effe"
2021.findings-acl.84,N19-1190,0,0.0614498,"Missing"
2021.findings-acl.84,D19-1102,0,0.0207881,"arsing Tasks Jia and Liang (2016) propose DATA RECOMBINA TION for injecting task-specific priors to neural semantic parsers. A synchronous context-free grammar (SCFG) is induced from training data, and new ""recombinant"" examples are sampled. Yu et al. (2020) introduce G RAPPA, a pretraining approach for table semantic parsing, and generate synthetic question-SQL pairs via an SCFG. Andreas (2020) 973 use compositionality to construct synthetic examples for downstream tasks like semantic parsing. Fragments of original examples are replaced with fragments from other examples in similar contexts. Vania et al. (2019) investigate DA for lowresource dependency parsing including dependency tree morphing from Sahin ¸ and Steedman (2018) (Figure 2) and modified nonce sentence generation from Gulordava et al. (2018), which replaces content words with other words of the same POS, morphological features, and dependency labels. 5.5 Grammatical Error Correction (GEC) Lack of parallel data is typically a barrier for GEC. Various works have thus looked at DA methods for GEC. We discuss some here, and more can be found in Table 2 in Appendix C. There is work that makes use of additional resources. Boyd (2018) use Germ"
2021.findings-acl.84,2020.coling-main.200,0,0.0290317,"l. (2019b) explore multi-task transfer, or the use of annotated data from other tasks. There is also work that adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated samples along with POS-specific noising. There have also been approaches to improve the diversity of generated errors. Wan et al. (2020) investigate noising through editing the latent representations of grammatical sentences, and Xie et al. (2018) use a neural sequence transduction model and beam search noising procedures. 5.6 Neural Machine Translation (NMT) There are many works which have investigated DA for NMT. We highlighted some in §3 and §4.1, e.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019). We discuss some further ones here, and more can be found in Table 3 in Appendix C. Wang et al. (2018a) propose S WITCH O UT, a DA method that randomly replaces words in both source and target sentences with other"
2021.findings-acl.84,D18-1100,0,0.238676,"tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. S EQ 2M IX U P (Guo et al., 2020) generalizes M IX U P for sequence transduction tasks in two ways - the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like S WITCH O UT (Wang et al., 2018a). 3.3 Model-Based Techniques Seq2seq and language models have also been used for DA. The popular BACKTRANSLATION method (Sennrich et al., 2016) translates a sequence into another language and then back into the original language. Kumar et al. (2019a) train seq2seq models with their proposed method DiPS which learns to generate diverse paraphrases of input text using a modified decoder with a submodular objective, 970 Figure 3: Contextual Augmentation, Kobayashi (2018) and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs (Kobayashi, 2018)"
2021.findings-acl.84,2021.naacl-main.434,1,0.882504,"invariances is less obvious. What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two. Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize. Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance. Kashefi and Hwa (2"
2021.findings-acl.84,2021.eacl-main.252,1,0.894767,"invariances is less obvious. What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two. Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize. Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance. Kashefi and Hwa (2"
2021.findings-acl.84,D19-1670,1,0.54652,"capture the desired invariances is less obvious. What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two. Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize. Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance."
2021.findings-acl.84,2020.bea-1.21,0,0.0846676,"Missing"
2021.findings-acl.84,P17-1190,0,0.0491557,"Missing"
2021.findings-acl.84,D17-1239,0,0.023004,"h source and target sentences with other random words from their corresponding vocabularies. Gao et al. (2019) introduce S OFT C ONTEXTUAL DA that softly augments randomly chosen words in a sentence using a contextual mixture of multiple related words over the vocabulary. Nguyen et al. (2020) propose DATA D IVERSIFICATION which merges original training data with the predictions of several forward and backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequen"
2021.findings-acl.84,P19-1579,0,0.109762,"ives (Cubuk et al., 2018; Ratner et al., 2017) or add human-in-the-loop (Kaushik et al., 2020, 2021). 971 4 Applications In this section, we discuss several DA methods for some common NLP applications.2 4.1 Low-Resource Languages Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet (Miller, 1995) may be difficult to use effectively here.3 There are ways to leverage high-resource languages for low-resource languages, particularly if they have similar linguistic properties. Xia et al. (2019) use this approach to improve low-resource NMT. Li et al. (2020b) use backtranslation and selflearning to generate augmented training data. Inspired by work in CV, Fadaee et al. (2017) generate additional training examples that contain lowfrequency (rare) words in synthetically created contexts. Qin et al. (2020) present a DA framework to generate multi-lingual code-switching data to finetune multilingual-BERT. It encourages the alignment of representations from source and multiple target languages once by mixing their context information. They see improved performance across 5 tasks with 19 l"
2021.findings-acl.84,N18-1057,0,0.0237125,"adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated samples along with POS-specific noising. There have also been approaches to improve the diversity of generated errors. Wan et al. (2020) investigate noising through editing the latent representations of grammatical sentences, and Xie et al. (2018) use a neural sequence transduction model and beam search noising procedures. 5.6 Neural Machine Translation (NMT) There are many works which have investigated DA for NMT. We highlighted some in §3 and §4.1, e.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019). We discuss some further ones here, and more can be found in Table 3 in Appendix C. Wang et al. (2018a) propose S WITCH O UT, a DA method that randomly replaces words in both source and target sentences with other random words from their corresponding vocabularies. Gao et al. (2019) introduce S OFT C ONTEXTUAL DA that soft"
2021.findings-acl.84,W19-4415,0,0.0277437,"Missing"
2021.findings-acl.84,2020.findings-emnlp.90,0,0.0394786,"Missing"
2021.findings-acl.84,L18-1436,0,0.0281775,"C. Beginning with speech, Wang et al. (2020) propose a DA method to improve the robustness of downstream dialogue models to speech recognition errors. Wiesner et al. (2018) and Renduchintala et al. (2018) propose DA methods for end-to-end automatic speech recognition (ASR). Looking at images or video, Xu et al. (2020) learn a cross-modality matching network to produce synthetic image-text pairs for multimodal classifiers. Atliha and Šešok (2020) explore DA methods such as synonym replacement and contextualized word embeddings augmentation using BERT for image captioning. Kafle et al. (2017), Yokota and Nakayama (2018), and Tang et al. (2020) propose methods for visual QA including question generation and adversarial examples. 6 augmentation) can predict DA performance, but it is unclear how these results might translate to NLP. Minimal benefit for pretrained models on indomain data: With the popularization of large pretrained language models, it has recently come to light that a couple of previously effective DA techniques for certain text classification tasks in English (Wei and Zou, 2019; Sennrich et al., 2016) provide little benefit for models like BERT and RoBERTa, which already achieve high performanc"
2021.findings-acl.84,2020.emnlp-main.691,0,0.0351694,"er sequences of tags and words linearized as per a certain scheme is learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. Sahin ¸ and Steedman (2018), discussed in §3.1, use dependency tree morphing (Figure 2) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a DA method based on M IX U P called S EQ M IX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection. In this section, we discuss several DA works for common NLP tasks.2 We focus on nonclassification tasks as classification is worked on by default, and well covered in earlier sections (e.g. §3 and §4). Numerous previously mentioned DA techniques, e.g. (Wei and Zou, 2019; Chen et al., 2020b; Anaby-Tavor et al., 2020), have been used or can be used for text classification tasks. 5.1 Question Answering (QA) 5.4 Parsing Tasks Jia and L"
2021.findings-acl.84,D19-1537,0,0.0428563,"Missing"
2021.findings-acl.84,N19-1131,0,0.0338603,"Missing"
2021.findings-acl.84,N18-2003,0,0.0239995,"pplicability, dependencies, and requirements. Ext.Know, KWE, tok, const, and dep stand for External Knowledge, keyword extraction, tokenization, constituency parsing, and dependency parsing, respectively. Ext.Know refers to whether the DA method requires external knowledge (e.g. WordNet) and Pretrained if it requires a pretrained model (e.g. BERT). Preprocess denotes preprocessing required, Level denotes the depth at which data is modified by the DA, and Task-Agnostic refers to whether the DA method can be applied to different tasks. See Appendix B for further explanation. 4.2 Mitigating Bias Zhao et al. (2018) attempt to mitigate gender bias in coreference resolution by creating an augmented dataset identical to the original but biased towards the underrepresented gender (using gender swapping of entities such as replacing ""he"" with ""she"") and train on the union of the two datasets. Lu et al. (2020) formally propose COUN TERFACTUAL DA (CDA) for gender bias mitigation, which involves causal interventions that break associations between gendered and gender-neutral words. Zmigrod et al. (2019) and Hall Maudslay et al. (2019) propose further improvements to CDA. Moosavi et al. (2020) augment training s"
2021.findings-acl.84,D19-1375,0,0.0254998,"d dialogue. Louvan and Magnini (2020) propose LIGHTWEIGHT AUGMENTATION, a set of word-span and sentence-level DA methods for lowresource slot filling and intent classification. 974 Hou et al. (2018) present a seq2seq DA framework to augment dialogue utterances for dialogue language understanding (Young et al., 2013), including a diversity rank to produce diverse utterances. Zhang et al. (2019c) propose MADA to generate diverse responses using the property that several valid responses exist for a dialogue context. There is also DA work for spoken dialogue. Hou et al. (2018), Kim et al. (2019), Zhao et al. (2019), and Yoo et al. (2019) investigate DA methods for dialogue and spoken language understanding (SLU), including generative latent variable models. 5.10 Multimodal Tasks DA techniques have also been proposed for multimodal tasks where aligned data for multiple modalities is required. We look at ones that involve language or text. Some are discussed below, and more can be found in Table 5 in Appendix C. Beginning with speech, Wang et al. (2020) propose a DA method to improve the robustness of downstream dialogue models to speech recognition errors. Wiesner et al. (2018) and Renduchintala et al. ("
2021.findings-acl.84,2020.emnlp-main.558,0,0.0178819,"Gontijo-Lopes et al., 2020) has proposed that affinity (the distributional shift caused by DA) and diversity (the complexity of the Working in specialized domains such as those with domain-specific vocabulary and jargon (e.g. medicine) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data (Zhang et al., 2019a; Herzig et al., 2020; Campagna et al., 2020; Zhong et al., 2020). 975 Working with low-resource languages may present similar difficulties as specialized domains. Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language family or very distinctive in linguistic and typological terms. For example, those which are language isolates or lack high-resource cognates. More vision-inspired techniques: Although many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV DA techniques motivated by real-world invar"
2021.findings-acl.84,P19-1161,0,0.0283339,"her the DA method can be applied to different tasks. See Appendix B for further explanation. 4.2 Mitigating Bias Zhao et al. (2018) attempt to mitigate gender bias in coreference resolution by creating an augmented dataset identical to the original but biased towards the underrepresented gender (using gender swapping of entities such as replacing ""he"" with ""she"") and train on the union of the two datasets. Lu et al. (2020) formally propose COUN TERFACTUAL DA (CDA) for gender bias mitigation, which involves causal interventions that break associations between gendered and gender-neutral words. Zmigrod et al. (2019) and Hall Maudslay et al. (2019) propose further improvements to CDA. Moosavi et al. (2020) augment training sentences with their corresponding predicate-argument structures, improving the robustness of transformer models against various types of biases. 4.3 M INORITY OVERSAMPLING T ECHNIQUE (SMOTE) (Chawla et al., 2002), which generates augmented minority class examples through interpolation, still remains popular (Fernández et al., 2018). M ULTILABEL SMOTE (MLSMOTE) (Charte et al., 2015) modifies SMOTE to balance classes for multi-label classification, where classifiers predict more than one"
A00-2012,C96-1017,0,0.0778889,"to both perfect and imperfect tenses, but the subjunctive and the jussive are restricted to the imperfect tense. The imperative has a special form, and the energetic can be derived from either the imperfect or the imperative. z Diacritic marks are used in Arabic language textbooks and occasionally in regular texts to resolve ambiguous words (e.g. to mark a passive verb use). There are 15 triliteral patterns, of which at least 9 are in common use, and 4 much rarer quadriliteral patterns. All these patterns undergo some stem changes with respect to voweling in 86 (1983) two-level morphology. In Beesley (1996) the system is reworked into a finite-state lexical transducer to perform analysis and generation. In two-level systems, the lexical level includes short vowels that are typically not realized on the the surface level. Kiraz (1994) presents an analysis of Arabic morphology based on the CV-, moraic-, and affixational models. He introduces a multi-tape two-level model and a formalism where three tapes are used for the lexical level (root, pattern, and vocalization) and one tape for the surface level. To illustrate our approach, we focus on a particular type of verbs, termed hollow verbs, and sho"
A00-2012,P98-1018,0,0.335276,"ll into four classes: . Verbs of the pattern CawaC o r CawuC (e.g. [Tawut] 'to be long'), where the middle radical is 'w'. Their characteristic is a long 'uu' between the first and last radical in the imperfect. E.g., From the underlying root [zawar]: zaara 'he visited' and yazuuru 'he visits' . Verbs of the pattern CayiC, where middle radical is 'y'. E.g., From the underlying root [hayib]: haaba 'he feared' and yahaabu 'he fears' Stem allomorphs : Perfect: -bib- and-haabImperfect: -hab- and-haabStem allomorphs: Perfect: -zur- and -zaarImperfect:-zur- and-zuurIn the relevant literature (e.g., Beesley, 1998; Kiraz, 1994), verbs belonging to the above classes are all assumed to have the pattern CVCVC. The pattern does not show the verb conjugation class and makes it difficult to predict the type of stem allomorph to use. To avoid these problems, we keep information on the middle radical and vowel in the base form of the verb. In generation, classes 2 and 4 of the verb can be handled as one because they have the same perfect and imperfect stemsP . Verbs of the pattern CawiC, where the middle radical is 'w'. Their characteristic is a long 'aa' between the first and last radical in the imperfect. E."
A00-2012,1995.mtsummit-1.1,0,0.0194007,"in large part, the problem of stem changes from that of prefixes and suffixes. The gain is a significant reduction in the size number of transformational rules, as much as a factor of three for certain verb classes. This improves the space efficiency of the system and its maintainability by reducing duplication of rules, and simplifies the rules by isolating different types of changes. 3 Grammars of Arabic are not uniform in their classification of &quot;hamzated&quot; verbs, verbs containing the glottal stop as one of the radicals (e.g. [sa?a[] 'to ask'). Wright (1968) includes them as weak verbs, but Cowan (1964) doesn't. Hamzated verbs change the written 'seat' of the hamza from 'alif' to 'waaw' or 'yaa?', depending on the phonetic context. 4 In the Arabic transcription capital letters indicate emphatic consonants; 'H' is the voiceless pharyngeal fricative ; &quot; ' the voiced pharyngeal fricative ; '?' is the glottal stop 'hamza'. 87 triliteral I I strong , regular weak I I hamzated I doubled radical I weak initial radical (assimilated) [ weak middle radical (hollow) I I I ' mood I present (imperfect) I active I I I tense I reterit ffect) I I weak final radical (defective) , participle , indicative I im"
A00-2012,C94-1029,0,0.276075,"itic marks are used in Arabic language textbooks and occasionally in regular texts to resolve ambiguous words (e.g. to mark a passive verb use). There are 15 triliteral patterns, of which at least 9 are in common use, and 4 much rarer quadriliteral patterns. All these patterns undergo some stem changes with respect to voweling in 86 (1983) two-level morphology. In Beesley (1996) the system is reworked into a finite-state lexical transducer to perform analysis and generation. In two-level systems, the lexical level includes short vowels that are typically not realized on the the surface level. Kiraz (1994) presents an analysis of Arabic morphology based on the CV-, moraic-, and affixational models. He introduces a multi-tape two-level model and a formalism where three tapes are used for the lexical level (root, pattern, and vocalization) and one tape for the surface level. To illustrate our approach, we focus on a particular type of verbs, termed hollow verbs, and show how we integrate their treatment with that of more regular verbs. We also discuss how the approach can be extended to other classes of verbs and other parts of speech. 1 Arabic Verbal Morphology Verb roots in Arabic can be classi"
A00-2012,J92-1003,0,\N,Missing
A00-2012,C98-1018,0,\N,Missing
araki-etal-2014-detecting,N10-1138,0,\N,Missing
araki-etal-2014-detecting,D11-1116,1,\N,Missing
araki-etal-2014-detecting,C02-1165,0,\N,Missing
araki-etal-2014-detecting,W97-1311,0,\N,Missing
araki-etal-2014-detecting,D08-1031,0,\N,Missing
araki-etal-2014-detecting,W99-0201,0,\N,Missing
araki-etal-2014-detecting,W11-1902,0,\N,Missing
araki-etal-2014-detecting,P94-1019,0,\N,Missing
araki-etal-2014-detecting,P08-1090,0,\N,Missing
araki-etal-2014-detecting,D10-1033,0,\N,Missing
araki-etal-2014-detecting,P09-1068,0,\N,Missing
araki-etal-2014-detecting,P10-1143,0,\N,Missing
araki-etal-2014-detecting,P10-1100,0,\N,Missing
araki-etal-2014-detecting,W13-1203,1,\N,Missing
araki-etal-2014-detecting,mendes-etal-2012-dbpedia,0,\N,Missing
araki-etal-2014-detecting,P13-2083,1,\N,Missing
araki-etal-2014-detecting,D13-1178,0,\N,Missing
araki-etal-2014-detecting,W09-4303,0,\N,Missing
araki-etal-2014-detecting,W04-3205,0,\N,Missing
araki-etal-2014-detecting,bejan-harabagiu-2008-linguistic,0,\N,Missing
C16-1107,W11-1401,0,0.0605425,"ctors have to be automatically generated using these as input. Random generation of distractors (Mostow and Jang, 2012), distractors with a corpus frequency comparable to the correct answer (Hoshino and Nakagawa, 2007), morphologically, orthographically or phonetically similar words as distractors (Pino and Eskenazi, 2009), and semantically similar words selected using taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009), thesauri (Sumita et al., 2005; Smith et al., 2010) or an additional layering of semantic relatedness to the context (sentence or paragraph) (Pino and Eskenazi, 2009; Agarwal et al., 2011; Mostow and Jang, 2012) are all valid strategies for the generation of distractor items. However, most of these methods generate word-level distractors for testing grammar rather than comprehension. Moreover, distractor generation is often tied to the method of generating questions. Our method automatically generates phrase-level distractors for testing reading comprehension, and is decoupled from the method used for question generation. 3 Generating Questions and Multiple-Choice Answers Our system comprises a question generation component and a distractor generation component. We show a high"
C16-1107,I13-2002,1,0.816854,"nerated questions. In the context of computer-assisted language learning, intelligent tutoring systems have been traditionally employed for generating questions in various domains (Koedinger et al., 1997; Vanlehn et al., 2005). Some of the research in this area uses questions as a way of engaging students in ongoing dialogue (Graesser et al., 2001; Piwek and Stoyanchev, 2010). Our underlying motivation to enhance the reading comprehension ability of non-native English readers aligns best with the Reader Specific Lexical Practice (REAP) system (Heilman et al., 2006) and the SmartReader system (Azab et al., 2013). Though very promising, state-of-the-art approaches in this area have not dealt with multiple sentences to generate questions, either. Our work is aimed at addressing this challenge. 1 See Section 3.1 for details of this corpus. 1126 Prior work in distractor generation has been mostly studied on cloze (gap-fill) questions. Distractor generation for cloze questions often comprises two steps. Candidate Selection controls the type and difficulty of the items, and is intimately tied to the intent and target audience for the questions. This step may take advantage of available datasets such as the"
C16-1107,N12-1092,0,0.110713,"language processing technology to aid teachers in examining learners’ reading comprehension. Past studies in education showed that higher-level questions, in contrast to simple factoid questions, have more educational benefits for reading comprehension (Anderson and Biddle, 1975; Andre, 1979; Hamaker, 1986). However, most of existing approaches to question generation have focused on generating questions from a single sentence, relying heavily on syntax and shallow semantics with an emphasis on grammaticality (Mitkov and Ha, 2003; Chen et al., 2009; Heilman and Smith, 2010; Curto et al., 2011; Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014). A problem with this approach is that the majority of questions generated from single sentences tend to be too specific and low-level to properly measure learners’ understandings of the overall contents of text. In other words, what is assessed by such question generation systems ends up essentially being the ability to compare sentences, just requiring learners to find a single sentence that has almost the same surface form as a given interrogative sentence. Results of simple sentence comparisons do little to contribute towards the goal of as"
C16-1107,D14-1159,0,0.0168619,"learners, more specifically, students who learn English as a second language (ESL). Therefore, our ultimate goal is to generate multiple-choice questions from plain texts in an arbitrary domain. However, the state-of-the-art in extracting semantic representations of event and entity relations from text does not perform well enough to support our question generation approach. Thus, the evaluation of question generation relying on automated semantic relation extraction is not practical at this time. Therefore, in this work we use texts and expert human annotations from the ProcessBank corpus1 (Berant et al., 2014) to facilitate our semantics-oriented question generation. Our contributions are as follows: 1. This is the first work to automatically generate questions from multiple sentences, involving specific inference steps such as coreference resolution and paraphrase detection. Our experiments show that questions generated by our approach require taking a larger number of inference steps while ensuring comparable grammatical correctness and answer existence, as compared to questions generated by a traditional single-sentence approach. 2. We also present a complementary system which generates question"
C16-1107,W11-2705,0,0.146937,"xt is a key natural language processing technology to aid teachers in examining learners’ reading comprehension. Past studies in education showed that higher-level questions, in contrast to simple factoid questions, have more educational benefits for reading comprehension (Anderson and Biddle, 1975; Andre, 1979; Hamaker, 1986). However, most of existing approaches to question generation have focused on generating questions from a single sentence, relying heavily on syntax and shallow semantics with an emphasis on grammaticality (Mitkov and Ha, 2003; Chen et al., 2009; Heilman and Smith, 2010; Curto et al., 2011; Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014). A problem with this approach is that the majority of questions generated from single sentences tend to be too specific and low-level to properly measure learners’ understandings of the overall contents of text. In other words, what is assessed by such question generation systems ends up essentially being the ability to compare sentences, just requiring learners to find a single sentence that has almost the same surface form as a given interrogative sentence. Results of simple sentence comparisons do little to contribute t"
C16-1107,N10-1086,0,0.697371,"estion generation from text is a key natural language processing technology to aid teachers in examining learners’ reading comprehension. Past studies in education showed that higher-level questions, in contrast to simple factoid questions, have more educational benefits for reading comprehension (Anderson and Biddle, 1975; Andre, 1979; Hamaker, 1986). However, most of existing approaches to question generation have focused on generating questions from a single sentence, relying heavily on syntax and shallow semantics with an emphasis on grammaticality (Mitkov and Ha, 2003; Chen et al., 2009; Heilman and Smith, 2010; Curto et al., 2011; Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014). A problem with this approach is that the majority of questions generated from single sentences tend to be too specific and low-level to properly measure learners’ understandings of the overall contents of text. In other words, what is assessed by such question generation systems ends up essentially being the ability to compare sentences, just requiring learners to find a single sentence that has almost the same surface form as a given interrogative sentence. Results of simple sentence comparisons do li"
C16-1107,P06-1063,0,0.0351599,"Missing"
C16-1107,P15-1086,0,0.0276914,"ns (Wolfe, 1976; Mitkov and Ha, 2003; Judge et al., 2006; Heilman and Smith, 2010; Curto et al., 2011). In contrast, shallow semanticsbased transformation uses the output of shallow semantic parsers and additional knowledge bases, and generates questions from sentences based on their semantic structures (Chen et al., 2009; Mannem et al., 2010; Becker et al., 2012; Yao et al., 2012; Mazidi and Nielsen, 2014). Both groups generate questions based on a single sentence of text, relying heavily on its argument structures in question construction and mostly emphasizing grammaticality in evaluation. Labutov et al. (2015) have recently presented an approach to generate high-level questions using ontology-derived templates. Our approach differs from theirs in that we leverage semantic representations of text to inject specific inference steps, such as event and entity coreferences, into the process of answering system-generated questions. In the context of computer-assisted language learning, intelligent tutoring systems have been traditionally employed for generating questions in various domains (Koedinger et al., 1997; Vanlehn et al., 2005). Some of the research in this area uses questions as a way of engagin"
C16-1107,W13-2114,0,0.0600085,"echnology to aid teachers in examining learners’ reading comprehension. Past studies in education showed that higher-level questions, in contrast to simple factoid questions, have more educational benefits for reading comprehension (Anderson and Biddle, 1975; Andre, 1979; Hamaker, 1986). However, most of existing approaches to question generation have focused on generating questions from a single sentence, relying heavily on syntax and shallow semantics with an emphasis on grammaticality (Mitkov and Ha, 2003; Chen et al., 2009; Heilman and Smith, 2010; Curto et al., 2011; Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014). A problem with this approach is that the majority of questions generated from single sentences tend to be too specific and low-level to properly measure learners’ understandings of the overall contents of text. In other words, what is assessed by such question generation systems ends up essentially being the ability to compare sentences, just requiring learners to find a single sentence that has almost the same surface form as a given interrogative sentence. Results of simple sentence comparisons do little to contribute towards the goal of assessing learners’ readi"
C16-1107,P14-2053,0,0.0601659,"rs in examining learners’ reading comprehension. Past studies in education showed that higher-level questions, in contrast to simple factoid questions, have more educational benefits for reading comprehension (Anderson and Biddle, 1975; Andre, 1979; Hamaker, 1986). However, most of existing approaches to question generation have focused on generating questions from a single sentence, relying heavily on syntax and shallow semantics with an emphasis on grammaticality (Mitkov and Ha, 2003; Chen et al., 2009; Heilman and Smith, 2010; Curto et al., 2011; Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014). A problem with this approach is that the majority of questions generated from single sentences tend to be too specific and low-level to properly measure learners’ understandings of the overall contents of text. In other words, what is assessed by such question generation systems ends up essentially being the ability to compare sentences, just requiring learners to find a single sentence that has almost the same surface form as a given interrogative sentence. Results of simple sentence comparisons do little to contribute towards the goal of assessing learners’ reading comprehension. In this w"
C16-1107,W03-0203,0,0.749683,"standing of material. Thus, automatic question generation from text is a key natural language processing technology to aid teachers in examining learners’ reading comprehension. Past studies in education showed that higher-level questions, in contrast to simple factoid questions, have more educational benefits for reading comprehension (Anderson and Biddle, 1975; Andre, 1979; Hamaker, 1986). However, most of existing approaches to question generation have focused on generating questions from a single sentence, relying heavily on syntax and shallow semantics with an emphasis on grammaticality (Mitkov and Ha, 2003; Chen et al., 2009; Heilman and Smith, 2010; Curto et al., 2011; Becker et al., 2012; Lindberg et al., 2013; Mazidi and Nielsen, 2014). A problem with this approach is that the majority of questions generated from single sentences tend to be too specific and low-level to properly measure learners’ understandings of the overall contents of text. In other words, what is assessed by such question generation systems ends up essentially being the ability to compare sentences, just requiring learners to find a single sentence that has almost the same surface form as a given interrogative sentence."
C16-1107,W09-0207,0,0.0125296,"on remains solvable, i.e., there is a total of one correct answer only (Zesch and Melamud, 2014). In most cases, however, only the question and the correct answer are available, and distractors have to be automatically generated using these as input. Random generation of distractors (Mostow and Jang, 2012), distractors with a corpus frequency comparable to the correct answer (Hoshino and Nakagawa, 2007), morphologically, orthographically or phonetically similar words as distractors (Pino and Eskenazi, 2009), and semantically similar words selected using taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009), thesauri (Sumita et al., 2005; Smith et al., 2010) or an additional layering of semantic relatedness to the context (sentence or paragraph) (Pino and Eskenazi, 2009; Agarwal et al., 2011; Mostow and Jang, 2012) are all valid strategies for the generation of distractor items. However, most of these methods generate word-level distractors for testing grammar rather than comprehension. Moreover, distractor generation is often tied to the method of generating questions. Our method automatically generates phrase-level distractors for testing reading comprehension, and is decoupled from the method"
C16-1107,W12-2016,0,0.0164983,"e datasets such as the Lang-8 corpus2 to identify confusion pairs and use the most frequent learner confusions as distractors (Sakaguchi et al., 2013) or the set of English language prepositions for use as distractors in preposition testing tasks (Lee and Seneff, 2007). Reliability Checking ensures that the question remains solvable, i.e., there is a total of one correct answer only (Zesch and Melamud, 2014). In most cases, however, only the question and the correct answer are available, and distractors have to be automatically generated using these as input. Random generation of distractors (Mostow and Jang, 2012), distractors with a corpus frequency comparable to the correct answer (Hoshino and Nakagawa, 2007), morphologically, orthographically or phonetically similar words as distractors (Pino and Eskenazi, 2009), and semantically similar words selected using taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009), thesauri (Sumita et al., 2005; Smith et al., 2010) or an additional layering of semantic relatedness to the context (sentence or paragraph) (Pino and Eskenazi, 2009; Agarwal et al., 2011; Mostow and Jang, 2012) are all valid strategies for the generation of distractor items. However,"
C16-1107,P13-2043,0,0.0169877,"to generate questions, either. Our work is aimed at addressing this challenge. 1 See Section 3.1 for details of this corpus. 1126 Prior work in distractor generation has been mostly studied on cloze (gap-fill) questions. Distractor generation for cloze questions often comprises two steps. Candidate Selection controls the type and difficulty of the items, and is intimately tied to the intent and target audience for the questions. This step may take advantage of available datasets such as the Lang-8 corpus2 to identify confusion pairs and use the most frequent learner confusions as distractors (Sakaguchi et al., 2013) or the set of English language prepositions for use as distractors in preposition testing tasks (Lee and Seneff, 2007). Reliability Checking ensures that the question remains solvable, i.e., there is a total of one correct answer only (Zesch and Melamud, 2014). In most cases, however, only the question and the correct answer are available, and distractors have to be automatically generated using these as input. Random generation of distractors (Mostow and Jang, 2012), distractors with a corpus frequency comparable to the correct answer (Hoshino and Nakagawa, 2007), morphologically, orthograph"
C16-1107,W05-0210,0,0.154498,"is a total of one correct answer only (Zesch and Melamud, 2014). In most cases, however, only the question and the correct answer are available, and distractors have to be automatically generated using these as input. Random generation of distractors (Mostow and Jang, 2012), distractors with a corpus frequency comparable to the correct answer (Hoshino and Nakagawa, 2007), morphologically, orthographically or phonetically similar words as distractors (Pino and Eskenazi, 2009), and semantically similar words selected using taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009), thesauri (Sumita et al., 2005; Smith et al., 2010) or an additional layering of semantic relatedness to the context (sentence or paragraph) (Pino and Eskenazi, 2009; Agarwal et al., 2011; Mostow and Jang, 2012) are all valid strategies for the generation of distractor items. However, most of these methods generate word-level distractors for testing grammar rather than comprehension. Moreover, distractor generation is often tied to the method of generating questions. Our method automatically generates phrase-level distractors for testing reading comprehension, and is decoupled from the method used for question generation."
C16-1107,W14-1817,0,0.024503,"ns often comprises two steps. Candidate Selection controls the type and difficulty of the items, and is intimately tied to the intent and target audience for the questions. This step may take advantage of available datasets such as the Lang-8 corpus2 to identify confusion pairs and use the most frequent learner confusions as distractors (Sakaguchi et al., 2013) or the set of English language prepositions for use as distractors in preposition testing tasks (Lee and Seneff, 2007). Reliability Checking ensures that the question remains solvable, i.e., there is a total of one correct answer only (Zesch and Melamud, 2014). In most cases, however, only the question and the correct answer are available, and distractors have to be automatically generated using these as input. Random generation of distractors (Mostow and Jang, 2012), distractors with a corpus frequency comparable to the correct answer (Hoshino and Nakagawa, 2007), morphologically, orthographically or phonetically similar words as distractors (Pino and Eskenazi, 2009), and semantically similar words selected using taxonomies (Hoshino and Nakagawa, 2007; Mitkov et al., 2009), thesauri (Sumita et al., 2005; Smith et al., 2010) or an additional layeri"
C18-1007,baumann-pierrehumbert-2014-using,0,0.0298959,"anual verification to create distantly supervised data. Another is to explore typical machine learning techniques, for example adversarial training of bilingual word representations. We find that in event-type detection task—the task to classify [parts of] documents into a fixed set of labels—they give about the same performance. We explore ways in which the two methods can be complementary and also see how to best utilize a limited budget for manual annotation to maximize performance gain. 1 Introduction For most languages of the world, few or no language processing tools or resources exist (Baumann and Pierrehumbert, 2014). This hinders efforts to apply certain language technologies enjoyed by languages like English, in which much current research is done. To perform natural language processing tasks in resource-poor languages, one way to overcome data scarcity is to tap on resources from another resource-rich language. Assuming that there are already good resources and tools to solve the same tasks in the more resource-rich language (henceforth, auxiliary language), the only remaining challenge is to transfer the learning process into the resource-poor language (henceforth, target language) and adapt it to the"
C18-1007,D16-1136,0,0.0214924,"ent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this against a simple keyword-based method. Figure 1: Architecture of the neural classifier with adversarial domain (language) adaptation by Ganin and Lempitsky (2015). Arrows show the flow of gradient. 3.1 Adversarial Convolutional Network The first step is to train a bilingual word embedding as a shared feature representation space between the two languages. We trained our bilingual word embedding for English and the incident language using the method proposed in XlingualEmb (Duong et al., 2016). This method is a cross-lingual extension from word2vec model (Mikolov et al., 2013b) to bilingual text using two large monolingual corpora and a bilingual dictionary. Based on the shared representation, we then used a convolutional neural network (CNN) (Kim, 2014) to perform the classification. There are two main advantages of choosing a deep neural classifier over a shallow one. First, CNN outperforms shallow models like SVM or Logistic Regression in various text classification benchmark datasets (Kim, 2014; Lai et al., 2015; Johnson and Zhang, 2015; Xu and Yang, 2017). Second, CNN takes de"
C18-1007,D16-1105,0,0.0139784,"ant information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be u"
C18-1007,E14-1049,0,0.0237372,"view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this against a simple keyword-based method. Figure 1: Architecture of the neural classifier with adversarial domain (language) adaptation by Ganin and Lempitsky (2015). Arrows show the flo"
C18-1007,D11-1086,0,0.0641385,"Missing"
C18-1007,W15-0805,0,0.0308877,"is organized as follows: we first describe the related work on cross-lingual NLP tasks in low-resource settings, specifically how the available resources are used. Based on previous work, we then apply our proposed training data augmentation methods and run experiments to show the effectiveness of our methods. We then analyze the results, and follow with a few suggestions on how to best utilize the available annotation effort for maximum gain. 2 Related Work Keyword-based Models A keyword-based heuristic model is a simple yet effective approach to extract specific information such as events (Keane et al., 2015), because keywords often indicate a strong presence of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ"
C18-1007,D14-1181,0,0.00426904,"ky (2015). Arrows show the flow of gradient. 3.1 Adversarial Convolutional Network The first step is to train a bilingual word embedding as a shared feature representation space between the two languages. We trained our bilingual word embedding for English and the incident language using the method proposed in XlingualEmb (Duong et al., 2016). This method is a cross-lingual extension from word2vec model (Mikolov et al., 2013b) to bilingual text using two large monolingual corpora and a bilingual dictionary. Based on the shared representation, we then used a convolutional neural network (CNN) (Kim, 2014) to perform the classification. There are two main advantages of choosing a deep neural classifier over a shallow one. First, CNN outperforms shallow models like SVM or Logistic Regression in various text classification benchmark datasets (Kim, 2014; Lai et al., 2015; Johnson and Zhang, 2015; Xu and Yang, 2017). Second, CNN takes dense word vector representations as input, allowing one to incorporate the state-of-the-art bilingual word embedding methods into the pipeline. The CNN model takes a sequence of word embeddings as input and applies 1-D convolutional operation on the input to extract"
C18-1007,W11-3607,0,0.0329797,"nce of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct wo"
C18-1007,W15-1521,0,0.0236723,"Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this against a simple keyword-based method. Figure 1: Architecture of the neural classifier with adversarial domain (language) adaptation by Ganin and Lempitsky (2015). Arrows show the flow of gradient. 3.1 Ad"
C18-1007,P15-2105,0,0.0575202,"Missing"
C18-1007,P07-1123,0,0.0286022,"rage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be unsatisfactory. Some researchers utilized the bilingual dictionary to translate the models instead (Xu et al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Gu"
C18-1007,D10-1025,0,0.0271954,"al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefo"
C18-1007,D10-1103,0,0.0202148,"result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be unsatisfactory. Some researchers utilized the bilingual dictionary to translate the models instead (Xu et al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix"
C18-1007,P08-1033,0,0.0128768,"maximum gain. 2 Related Work Keyword-based Models A keyword-based heuristic model is a simple yet effective approach to extract specific information such as events (Keane et al., 2015), because keywords often indicate a strong presence of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictio"
C18-1007,I13-1088,0,0.0253113,"2 Related Work Keyword-based Models A keyword-based heuristic model is a simple yet effective approach to extract specific information such as events (Keane et al., 2015), because keywords often indicate a strong presence of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate do"
C18-1007,P09-1027,0,0.121107,"ents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be unsatisfactory. Some researchers utilized the bilingual dictionary to translate the models instead (Xu et al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing doc"
C18-1007,P17-1130,1,0.832009,"view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this again"
C18-1007,P15-1042,0,0.0204976,"chine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art meth"
C18-1007,D16-1024,0,0.0144787,"s an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work i"
C18-1007,P16-1133,0,0.0153345,"s an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work i"
C18-1075,W06-0901,0,0.403029,"s. Our training data generated from SemCor is 11 times larger than SW100 with respect to the number of event nuggets. Still, many nouns and phrases do not appear in the training data, making correct predictions difficult. As shown in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed do"
C18-1075,D15-1247,1,0.817985,"e’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Rilo"
C18-1075,P98-1013,0,0.680723,"only contributes to partial understanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-domain question answering, we need automatic event identification techniques with larger, wider, and more consistent coverage (Saur´ı et al., 2005; Pradhan et al., 2007). There are several pieces of prior work on open-domain events, but they have some limitations with respect to coverage of events. Lexical databases such as WordNet (Miller et al., 1990), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) can be viewed as a superset of events, and their subtaxonomies seem to provide an extensional definition of events. However, these databases have a narrow coverage of events because they are generally expected to cover basic terminology due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank covers current terminology and proper nouns such as ‘Hurricane Katrina’. OntoNotes (Weischedel et al., 2011) is aimed at covering an unrestricted set of events and entities, but its event annotation is limited to a small number of eventive no"
C18-1075,D14-1159,0,0.10079,"n various domains. For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. Prior studies on automatic event detection traditionally focus on limited types of events, mainly defined by several research initiatives and shared tasks in a few domains: • Newswire: TIPSTER (Onyshkevych et al., 1993) and MUC (Grishman and Sundheim, 1996) • Multi-domain: ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2016). • Biology: PASBio (Wattarujeekrit et al., 2004), GENIA (Kim et al., 2008), BioNLP (Kim et al., 2009) and ProcessBank (Berant et al., 2014). Although closed-domain event detection might be of practical use in some domain-specific scenarios, it only contributes to partial understanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-domain question answering, we need automatic event identification techniques with larger, wider, and more consistent coverage (Saur´ı et al., 2005; Pradhan et al., 2007). There are several pieces of prior work on open-domain events, but they have some limitation"
C18-1075,P15-1017,0,0.241449,"rk Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition ("
C18-1075,P17-1038,0,0.161797,"). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from the syntactic perspective, without clarifying what semantically constitutes events. The ISO-TimeML specification (Pustejovsky et al., 2010) defines events as “something that can be said to obtain or hold true, to happen or to occur.” This definit"
C18-1075,cybulska-vossen-2014-using,0,0.0450015,"e of events because they are generally expected to cover basic terminology due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank covers current terminology and proper nouns such as ‘Hurricane Katrina’. OntoNotes (Weischedel et al., 2011) is aimed at covering an unrestricted set of events and entities, but its event annotation is limited to a small number of eventive nouns. TimeML (Pustejovsky et al., 2003) annotates events and times in a domain-agnostic manner, focusing on temporal aspects of events, but it does not deal with multi-word and generic events. ECB+ (Cybulska and Vossen, 2014) augments the extended EventCorefBank corpus (Lee et al., 2012) by re-annotating events and event coreference. Our initial analysis shows that it annotates events only in a portion of each document, not all events in the text. Ritter et al. (2012) address open-domain event detection on Twitter. Their annotation follows TimeML and thus is likely to have similar problems to TimeML. Richer This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 878 Proceedings of the 27th International Conference on Compu"
C18-1075,doddington-etal-2004-automatic,0,0.263848,"r human annotation of events. 1 Introduction Events are a key semantic component integral to natural language understanding. They are a ubiquitous linguistic phenomenon appearing in various domains. For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. Prior studies on automatic event detection traditionally focus on limited types of events, mainly defined by several research initiatives and shared tasks in a few domains: • Newswire: TIPSTER (Onyshkevych et al., 1993) and MUC (Grishman and Sundheim, 1996) • Multi-domain: ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2016). • Biology: PASBio (Wattarujeekrit et al., 2004), GENIA (Kim et al., 2008), BioNLP (Kim et al., 2009) and ProcessBank (Berant et al., 2014). Although closed-domain event detection might be of practical use in some domain-specific scenarios, it only contributes to partial understanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-domain question answering, we need automatic event identification techniques with larg"
C18-1075,P16-2011,0,0.0407601,"on using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (C"
C18-1075,P16-2060,0,0.0612205,"dia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng"
C18-1075,C96-1079,0,0.54715,"on in various domains, while obviating the need for human annotation of events. 1 Introduction Events are a key semantic component integral to natural language understanding. They are a ubiquitous linguistic phenomenon appearing in various domains. For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. Prior studies on automatic event detection traditionally focus on limited types of events, mainly defined by several research initiatives and shared tasks in a few domains: • Newswire: TIPSTER (Onyshkevych et al., 1993) and MUC (Grishman and Sundheim, 1996) • Multi-domain: ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2016). • Biology: PASBio (Wattarujeekrit et al., 2004), GENIA (Kim et al., 2008), BioNLP (Kim et al., 2009) and ProcessBank (Berant et al., 2014). Although closed-domain event detection might be of practical use in some domain-specific scenarios, it only contributes to partial understanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-domain question answering, we need autom"
C18-1075,P11-1113,0,0.174679,"r than SW100 with respect to the number of event nuggets. Still, many nouns and phrases do not appear in the training data, making correct predictions difficult. As shown in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervi"
C18-1075,E12-1029,0,0.131039,"Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from the syntactic perspective"
C18-1075,N13-1005,0,0.0266974,"Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from the syntactic perspective, without clarifying what"
C18-1075,P16-1025,0,0.122835,"; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from the syntactic perspective, without clarifying what semantically constitutes events. The ISO-TimeML specification (Pustejovsky et al., 2010) defines events as “something that can be said to obtain or ho"
C18-1075,P15-1162,0,0.0677572,"Missing"
C18-1075,P08-1030,0,0.241904,"ning data generated from SemCor is 11 times larger than SW100 with respect to the number of event nuggets. Still, many nouns and phrases do not appear in the training data, making correct predictions difficult. As shown in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject"
C18-1075,W09-1401,0,0.0928885,"s linguistic phenomenon appearing in various domains. For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. Prior studies on automatic event detection traditionally focus on limited types of events, mainly defined by several research initiatives and shared tasks in a few domains: • Newswire: TIPSTER (Onyshkevych et al., 1993) and MUC (Grishman and Sundheim, 1996) • Multi-domain: ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2016). • Biology: PASBio (Wattarujeekrit et al., 2004), GENIA (Kim et al., 2008), BioNLP (Kim et al., 2009) and ProcessBank (Berant et al., 2014). Although closed-domain event detection might be of practical use in some domain-specific scenarios, it only contributes to partial understanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-domain question answering, we need automatic event identification techniques with larger, wider, and more consistent coverage (Saur´ı et al., 2005; Pradhan et al., 2007). There are several pieces of prior work on open-domain"
C18-1075,D17-1018,0,0.0199141,"ersion of the standard BIO tagging scheme, inspired by Metke-Jimenez and Karimi (2016). Besides the three tags of B, I and O, we introduce two additional tags: DB and DI. DB means the beginning of a discontinuous concept, and DI the continuation of a discontinuous concept. The BLSTM model computes a hidden representation from each input word and then predicts one of {B, I, DB, DI, O}. For the BLSTM model, we use a fixed concatenation of the GloVe embeddings and 50-dimensional word embeddings from Turian et al. (2010) under the same assumption 7 https://nlp.stanford.edu/projects/glove/ 883 as (Lee et al., 2017) that different learning objectives of the GloVe and Turian embeddings can provide orthogonal information. We additionally employ 10-dimensional part-of-speech embeddings. We train the model with the objective of minimizing cross-entropy loss. We use early stopping based on the loss on a validation set. 4 Open-Domain Event Corpus Since our target is unrestricted domains, we need gold standard data to evaluate whether systems work robustly in various domains. However, annotating events manually in all domains would be unrealistic due to annotation cost. To make the corpus creation manageable wh"
C18-1075,P13-1008,0,0.165834,"e most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a;"
C18-1075,D14-1198,0,0.0408865,"domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalko"
C18-1075,C10-1077,0,0.519947,"m SemCor is 11 times larger than SW100 with respect to the number of event nuggets. Still, many nouns and phrases do not appear in the training data, making correct predictions difficult. As shown in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supe"
C18-1075,P10-1081,0,0.115377,"m SemCor is 11 times larger than SW100 with respect to the number of event nuggets. Still, many nouns and phrases do not appear in the training data, making correct predictions difficult. As shown in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supe"
C18-1075,P11-2045,0,0.155312,"n initial learning rate of η0 = 1.0e−3. We use a minibatch of size 1. To mitigate overfitting, we apply the dropout method (Srivastava et al., 2014) to the inputs and outputs of the network. We also employ L2 regularization. We perform a small grid search over combinations of dropout rates {0.0, 0.1, 0.2} and L2 regularization penalties {0.0, 1.0e−3, 1.0e−4}. We use early stopping based on performance on the validation set. 3.3 Learning for Event Detection After generating training data, we train a supervised event detection model on the synthesized data. As seen in the self-training model by Liao and Grishman (2011), erroneously generated training data worsen system performance. Therefore, we need to generate training data as accurately as possible. On the other hand, our algorithm for generating training data comprises at least three non-trivial (errorprone) submodules: disambiguation, wikification, and gloss classification. To eliminate negative effects of disambiguation errors, we choose the SemCor corpus (Miller et al., 1993) as our base text for training data generation. SemCor has human-annotated WordNet senses on 186 documents in numerous genres. We apply our rule-based event detector to generate"
C18-1075,E12-1030,0,0.0237978,"Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from t"
C18-1075,P12-1088,0,0.0261944,"n in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and"
C18-1075,P14-5010,0,0.00719326,"suitable to event detection because the exceptionality of multi-word units in the phrase lexicon translates to the meaningfulness of textual units of (phrasal) event nuggets. From the perspective of open-domain event detection, supervised phrase detection models are likely suboptimal because they might be limited to particular domains or overfitting to small datasets. Therefore, we explore a simple dictionary-lookup approach to detect WordNet phrasal verbs as events, inspired by Yin and Sch¨utze (2015). One enhancement to their approach is that we examine dependencies using Stanford CoreNLP (Manning et al., 2014), illustrated as follows: (15) Snipers were picking them off. (16) He picked an apple off the tree. In (15), ‘picking . . . off’ forms a discontinuous phrasal verb, whereas ‘picked’ in (16) does not. Dependencies can be of help to resolve these two cases. In the former case, a dependency relation ‘picking compound:prt −−−−−−−−→ off’ is a direct signal of the phrasal verb construction. As for noun phrases, we apply our heuristics for nouns to head words of the phrases. 3.2 Enhancements with Wikipedia This subsection describes two techniques to enhance our training data generation: heuristics-ba"
C18-1075,P11-1163,0,0.0876053,"ions difficult. As shown in Table 5(a), the most difficult domain is ‘Disease’ where numerous domain-specific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for simil"
C18-1075,H93-1061,0,0.268422,". 3.3 Learning for Event Detection After generating training data, we train a supervised event detection model on the synthesized data. As seen in the self-training model by Liao and Grishman (2011), erroneously generated training data worsen system performance. Therefore, we need to generate training data as accurately as possible. On the other hand, our algorithm for generating training data comprises at least three non-trivial (errorprone) submodules: disambiguation, wikification, and gloss classification. To eliminate negative effects of disambiguation errors, we choose the SemCor corpus (Miller et al., 1993) as our base text for training data generation. SemCor has human-annotated WordNet senses on 186 documents in numerous genres. We apply our rule-based event detector to generate training data automatically from SemCor. We formalize event detection as a sequence labeling problem and employ a BLSTM for sequence modeling. One difference from traditional sequence labeling problems is that our output includes discontinuous phrases. Thus, we leverage an extended version of the standard BIO tagging scheme, inspired by Metke-Jimenez and Karimi (2016). Besides the three tags of B, I and O, we introduce"
C18-1075,W15-0809,1,0.788708,"ch (1986) and define the three classes on the basis of durativity and telicity (Moens and Steedman, 1988; Pulman, 1997): • states: notions that are durative and changeless, e.g, want, own, love, resemble • processes: notions that are durative and atelic, e.g., walking, sleeping, raining • actions2 : notions that are telic or momentaneous happenings, e.g., build, walk to Boston, recognize We define expressions as events if they denote an eventuality, i.e., a state, a process, or an action. 2.2 Syntactic Perspective: Event Nuggets From a syntactic perspective, we use the notion of event nugget (Mitamura et al., 2015), which is a semantically meaningful unit that expresses an event. We give several examples below, where we use boldface to highlight event nuggets and underlines to show units of multi-word ones. (1) He opened fire at the teller in the bank. (2) I cried when my grandpa kicked the bucket. (3) Tom was happy when he received a present. (4) Susan turned the TV on. (5) She responded his email dismissively. 1 2 https://bitbucket.org/junaraki/coling2018-event Bach (1986) uses the term ‘events’ to refer to this class. In this work, we use ‘actions’ instead for clarification. 879 Event nuggets can be"
C18-1075,J88-2003,0,0.879643,"tated events comprise verbs, nouns, adjectives, and phrases which are continuous or discontinuous (see Section 2.2). Despite this relatively wide and flexible annotation of events, we achieved high inter-annotator agreement (see Section 4). 2 Definition of Events We give our definition of events from both semantic and syntactic perspective. 2.1 Semantic Perspective: Eventualities Events are a highly ambiguous notion, and thus there are many ways to define them. In this work, we use the notion of eventualities by Bach (1986) and define the three classes on the basis of durativity and telicity (Moens and Steedman, 1988; Pulman, 1997): • states: notions that are durative and changeless, e.g, want, own, love, resemble • processes: notions that are durative and atelic, e.g., walking, sleeping, raining • actions2 : notions that are telic or momentaneous happenings, e.g., build, walk to Boston, recognize We define expressions as events if they denote an eventuality, i.e., a state, a process, or an action. 2.2 Syntactic Perspective: Event Nuggets From a syntactic perspective, we use the notion of event nugget (Mitamura et al., 2015), which is a semantically meaningful unit that expresses an event. We give several"
C18-1075,P15-2060,0,0.0762931,"has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and di"
C18-1075,N16-1034,0,0.100462,"at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensor"
C18-1075,W16-5706,0,0.0616129,"Missing"
C18-1075,X93-1013,0,0.622685,"vision enables robust event detection in various domains, while obviating the need for human annotation of events. 1 Introduction Events are a key semantic component integral to natural language understanding. They are a ubiquitous linguistic phenomenon appearing in various domains. For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. Prior studies on automatic event detection traditionally focus on limited types of events, mainly defined by several research initiatives and shared tasks in a few domains: • Newswire: TIPSTER (Onyshkevych et al., 1993) and MUC (Grishman and Sundheim, 1996) • Multi-domain: ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2016). • Biology: PASBio (Wattarujeekrit et al., 2004), GENIA (Kim et al., 2008), BioNLP (Kim et al., 2009) and ProcessBank (Berant et al., 2014). Although closed-domain event detection might be of practical use in some domain-specific scenarios, it only contributes to partial understanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-do"
C18-1075,J05-1004,0,0.1583,"tanding of events because it only addresses a subset of events by definition. On the other hand, there is an established consensus that in order to advance natural language applications such as open-domain question answering, we need automatic event identification techniques with larger, wider, and more consistent coverage (Saur´ı et al., 2005; Pradhan et al., 2007). There are several pieces of prior work on open-domain events, but they have some limitations with respect to coverage of events. Lexical databases such as WordNet (Miller et al., 1990), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) can be viewed as a superset of events, and their subtaxonomies seem to provide an extensional definition of events. However, these databases have a narrow coverage of events because they are generally expected to cover basic terminology due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank covers current terminology and proper nouns such as ‘Hurricane Katrina’. OntoNotes (Weischedel et al., 2011) is aimed at covering an unrestricted set of events and entities, but its event annotation is limited to a small number of eventive nouns. TimeML (Pustejovsky et al., 20"
C18-1075,D16-1038,0,0.0283567,"2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff, 2013). Others explore self-training (Liao and Grishman, 2011), event vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from the syntactic perspective, without clarifying what semantically constitutes events. The ISO-TimeML specification (Pustejovsky et al., 2010) defines events"
C18-1075,D14-1162,0,0.081535,"T denote a matrix comprising hidden vectors [h1 , . . . , hT ] where d is the dimensionality of a hidden vector. The selfattention mechanism computes the hidden state as follows: M = tanh(H) (1) T α = softmax(w M) r = Hα T ∗ h = tanh(r) (2) (3) (4) where a vector α ∈ RT is attention weights and w ∈ Rd is a parameter vector. We refer to the attentionbased classifier as GC-BLSTM-Attn. Implementation Details. We randomly sample 1,000 examples from each of D+ and D− to create a test set and a validation set, and use the rest of D for a training set. We use the 300-dimensional GloVe vectors7 from (Pennington et al., 2014) and do not fine-tune them during training. We map all out-of-vocabulary words to a single vector randomly initialized by uniform sampling from [−0.01, 0.01]. We use a single hidden layer of 100 dimensions, i.e., d = 100. We optimize model parameters using minibatch stochastic gradient descent (SGD) with momentum 0.9. We choose an initial learning rate of η0 = 1.0e−3. We use a minibatch of size 1. To mitigate overfitting, we apply the dropout method (Srivastava et al., 2014) to the inputs and outputs of the network. We also employ L2 regularization. We perform a small grid search over combinat"
C18-1075,pustejovsky-etal-2010-iso,0,0.0396493,"vector representation (Peng et al., 2016), tensorbased composition (Huang et al., 2016), and distant supervision (Chen et al., 2017). However, their models focus on predicate-argument structures and are validated in a few domains, mostly in ACE. It is unclear how well they scale to the open domain, particularly to phrases and proper nouns. Several lines of recent work refine the definition of events. Rich ERE (LDC, 2015) defines events under a particular event ontology mainly from the syntactic perspective, without clarifying what semantically constitutes events. The ISO-TimeML specification (Pustejovsky et al., 2010) defines events as “something that can be said to obtain or hold true, to happen or to occur.” This definition is consistent with Bach’s eventualities and the closest to ours. The studies described above have narrower coverage of events than our work in terms of domains, syntactic types, or multi-word expressions. 8 Conclusion and Future Work We have introduced open-domain event detection, a new event detection paradigm whose goal is to detect all kinds of events regardless of domains. Due to the ubiquity and ambiguities of events, human annotation of events in the open domain is substantially"
C18-1075,P11-1138,0,0.0130372,"ction 3.2.1) and classifier-based enhancement (Section 3.2.2). One disadvantage of RULE is the limited coverage of WordNet. As described in Section 1, WordNet does not cover many proper nouns that we generally see in newspaper articles, such as the following: (17) Property damage by Hurricane Katrina around $108 billion. (18) The Cultural Revolution was ... In order to achieve higher recall, we incorporate Wikipedia to capture proper nouns which are not in WordNet, motivated by the fact that Wikipedia has a much broader coverage of concepts than WordNet synsets.5 We use the Illinois Wikifier (Ratinov et al., 2011) to extract Wikipedia concepts from text. 5 WordNet 3.0 has 120K synsets, and English Wikipedia has 5.5M articles as of October 2017, as shown at https:// stats.wikimedia.org/EN/TablesWikipediaEN.htm. 881 3.2.1 Heuristics-based Enhancement For our first enhancement, we make two assumptions: (1) the first sentence of a Wikipedia article provides a high-quality gloss of its corresponding concept, and (2) the syntactic head of a gloss represents a high-level concept carrying significant information to decide eventiveness. The first assumption is supported by Wikipedia’s style manual on how to wri"
C18-1075,P16-1113,0,0.017738,"6.9 87.3 Table 4: Performance of the rule-based event detectors on SW100. Results of Training Data Generation We measure the performance of the rule-based event detectors on SW100 using precision (P), recall (R), and F1 with the two matching options described in Section 4. We use IMS (It Makes Sense) for disambiguation. Table 4 shows the results. VERB is a simple baseline that detects all single-word main verbs as events, excluding be-verbs and auxiliary verbs. PRED is another baseline that detects all predicates as events by running a state-of-the-art semantic role labeler called PathLSTM10 (Roth and Lapata, 2016). Since PathLSTM is trained on both PropBank and NomBank, it is able to detect both verbal and nominal predicates. However, semantic role labeling has a different focus on predicate-argument structures. More specifically, the combination of PropBank and NomBank has a narrower coverage of events while having non-event predicates. This difference explains PathLSTM’s relatively low performance of 58.5 strict F1, even underperforming the VERB baseline. The performance difference between RULE and VERB mostly comes from nouns, indicating that our WordNet-based heuristics is effective. We found that"
C18-1075,H05-1088,0,0.303981,"Missing"
C18-1075,Q14-1016,0,0.0147325,"man-made river in the country. (14) The tower has 20,000 sparkling lights. We convert these adjectives to verbs in the infinitive form using pattern.en (De Smedt and Daelemans, 2012). Adverbs. We develop heuristics using WordNet. We first convert adverbs to adjectives by looking up pertainyms (relational adjectives) and seeking verbs derivationally related to the adjectives in WordNet. Adverbs connect with their modifying verbs, forming a single event nugget, as illustrated in (5). Thus, we combine eventive adverbs with such verbs to detect resulting verb phrases as events. Phrases. Following Schneider et al. (2014), we define phrases to be lexicalized combinations of two or more words that are exceptional enough to be considered as single units in the lexicon. We assume that this definition is suitable to event detection because the exceptionality of multi-word units in the phrase lexicon translates to the meaningfulness of textual units of (phrasal) event nuggets. From the perspective of open-domain event detection, supervised phrase detection models are likely suboptimal because they might be limited to particular domains or overfitting to small datasets. Therefore, we explore a simple dictionary-look"
C18-1075,P10-1040,0,0.0205373,"beling problems is that our output includes discontinuous phrases. Thus, we leverage an extended version of the standard BIO tagging scheme, inspired by Metke-Jimenez and Karimi (2016). Besides the three tags of B, I and O, we introduce two additional tags: DB and DI. DB means the beginning of a discontinuous concept, and DI the continuation of a discontinuous concept. The BLSTM model computes a hidden representation from each input word and then predicts one of {B, I, DB, DI, O}. For the BLSTM model, we use a fixed concatenation of the GloVe embeddings and 50-dimensional word embeddings from Turian et al. (2010) under the same assumption 7 https://nlp.stanford.edu/projects/glove/ 883 as (Lee et al., 2017) that different learning objectives of the GloVe and Turian embeddings can provide orthogonal information. We additionally employ 10-dimensional part-of-speech embeddings. We train the model with the objective of minimizing cross-entropy loss. We use early stopping based on the loss on a validation set. 4 Open-Domain Event Corpus Since our target is unrestricted domains, we need gold standard data to evaluate whether systems work robustly in various domains. However, annotating events manually in all"
C18-1075,N16-1033,0,0.0394153,"pecific terms, such as migraine and bubonic plague, can appear even in simplified text of Simple Wikipedia, but not in SemCor at all. 7 Related Work Most prior work has addressed event detection using supervised models based on symbolic features. Some studies employ token-level classifiers (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010b; Hong et al., 2011; Berant et al., 2014), while others cast event detection as a sequence labeling problem and apply structured prediction models (McClosky et al., 2011; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014; Araki and Mitamura, 2015; Yang and Mitchell, 2016). Recent work has explored neural network models with distributional features (Ghaeini et al., 2016; Nguyen et al., 2016; Chen et al., 2015; Nguyen and Grishman, 2015; Feng et al., 2016). These models are typically trained on a small amount of human-annotated data in closed domains and thus subject to overfitting. Semi-supervised and unsupervised approaches are less studied than supervised ones in event detection. Several studies leverage bootstrapping methods to find new patterns for similar events (Liao and Grishman, 2010a; Liu and Strzalkowski, 2012; Huang and Riloff, 2012; Huang and Riloff"
C18-1075,N15-1154,0,0.0213807,"Missing"
C18-1075,P10-4014,0,0.0466094,"entence of entry Electron is: (19) The electron is a subatomic particle with a negative elementary electric charge. The gloss of Electron is the underlined text above. Our analysis shows that most Wikipedia articles follow the first-sentence format. The second assumption is illustrated by the syntactic head of the Electron gloss, which is ‘particle’. Based on the assumptions, we develop head-based heuristics, which we call HeadLookup. We find the syntactic head of a gloss using dependencies and disambiguate the head using a state-of-the-art word sense disambiguation tool IMS (It Makes Sense) (Zhong and Ng, 2010). We then check if the head’s sense is subsumed by the three synsets of state2n , process6n , and event1n . In the case of Electron, the head’s sense atom2n is not under the synsets. Thus, the model concludes that Electron is noneventive. Note that HeadLookup itself is a general technique which can be applied to any gloss. Our first enhancement applies it to Wikipedia glosses, and we refer to the enhanced model as RULE-WP-HL. 3.2.2 Classifier-based Enhancement Our second enhancement leverages a binary gloss classifier to decide the eventiveness of proper nouns. We refer to this enhanced model"
C18-1075,P16-2034,0,0.0255616,"on, and train the model using the binary cross-entropy loss. Attention. Neural networks with attention mechanisms have achieved great success in a wide variety of natural language tasks. The basic idea is to enable the model to attend to all past hidden vectors and put higher weights on important parts so that the model can encode the sequence information more effectively. This idea intuitively makes sense for gloss classification as well, because syntactic heads are likely more important, as illustrated in Section 3.2.1. As shown in Figure 2, we leverage a selfattention mechanism, following (Zhou et al., 2016; Lin et al., 2017). Let H ∈ Rd×T denote a matrix comprising hidden vectors [h1 , . . . , hT ] where d is the dimensionality of a hidden vector. The selfattention mechanism computes the hidden state as follows: M = tanh(H) (1) T α = softmax(w M) r = Hα T ∗ h = tanh(r) (2) (3) (4) where a vector α ∈ RT is attention weights and w ∈ Rd is a parameter vector. We refer to the attentionbased classifier as GC-BLSTM-Attn. Implementation Details. We randomly sample 1,000 examples from each of D+ and D− to create a test set and a validation set, and use the rest of D for a training set. We use the 300-d"
C18-1309,araki-etal-2014-detecting,1,0.88093,"u/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer t"
C18-1309,P98-1012,0,0.403292,"Missing"
C18-1309,P14-1005,0,0.0359091,"Missing"
C18-1309,P08-1090,0,0.272255,"016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014). This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank"
C18-1309,chambers-jurafsky-2010-database,0,0.0318085,"rder them. We design separate feature sets to capture these aspects: the Script Compatibility set considers whether mentions should belong to the same script; the Event Ordering set determines the relative ordering of the mentions. Our final features are the cross products of features from the following 3 sets. 1. Surface-Based Script Compatibility: these features capture whether two mentions are script compatible based on the surface information, including: • Mention headword pair. • Event type pair. • Whether two event mentions appear in the same cluster in Chambers’s event schema database (Chambers and Jurafsky, 2010). • Whether the two event mentions share arguments, and the semantic frame name of the shared argument (produced by the Semafor parser (Das and Smith, 2011)). 2. Discourse-Based Script Compatibility: these features capture whether two event mentions are related given the discourse context. • Dependency path between the two mentions. • Function words (words other than Noun, Verb, Adjective and Adverb) in between the two mentions. • The types of other event mentions between the two mentions. • The sentence distance of two event mentions. • Whether there are temporal expressions (AGM-TMP slot fro"
C18-1309,P11-1098,0,0.0299351,"tem beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these event relations. Title and Abstract in Chinese 基于图的事件共指消解以及依序关系解码算法 文本中提及的事件之间常存在着复杂的关系。在这篇文章中，我们主要研究两种关系： 事件间的共指关系以及依序关系。我们指出，常被应用在事件指代消歧上的传统的树结 构解码方式并不适用于解决事件依序问题。为了解决这个问题，我们提出了一种适用于 两种情况的新的图结构解码算法。这个算法让我们可以为这两个任务设计灵活的特征。 在实验上，我们的事件共指消解系统在TAC-KBP 2015的共指消解任务上达到了目前最佳 的水平。同时，我们的依序关系系统的结果超过了一个基于时间分析并有部分正确答案 的基线系统。最后我们分析了结果，并讨论了事件关系判别任务的一些挑战。 1 Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011), news summarization (Vossen and Caselli, 2015), story understanding (Mostafazadeh et al., 2016). Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documen"
C18-1309,W09-3208,0,0.0288666,"n the corresponding task documents2 . 2 Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-K"
C18-1309,I13-1100,0,0.0146042,"hers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small b"
C18-1309,N15-1116,0,0.013159,"event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of"
C18-1309,W09-4303,0,0.0316458,"task documents2 . 2 Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tas"
C18-1309,N13-1104,0,0.163821,"C dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014). This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky"
C18-1309,W02-1001,0,0.0876307,"ntions to represent the full event graph by taking one single mention from each event. Following the “easiest” intuition, we select the single mention that will result in the highest score given the current feature weight w. Match Criteria: We consider two graphs to match when their inferred graphs are the same. The inferred graph is defined by taking the transitive closure of the graph and propagate the links through the coreference relations. For example, in Figure 1, the mention fired will be linked to two killed mentions after propagation. Feature Delta: In structural perceptron training (Collins, 2002), the weights are updated directly by the feature delta. For all the features f˜ of the gold standard graph z˜ and features fˆ of a decoded graph zˆ, the feature delta is simply: ∆ = f˜ − fˆ. However, a decoded graph may contain links that are not directly presented but inferable from the gold standard graph. For example, in Figure 2, the prediction graph has a link from M 5 to M 1 (the orange arc), which is absent but inferable from the gold standard tree. If we keep these links when computing ∆, the model does not converge well. We thus remove the features on the inferable links from fˆ when"
C18-1309,cybulska-vossen-2014-using,0,0.119546,"s/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, This work is li"
C18-1309,P11-1144,0,0.156628,"tic Headword token and lemma pair, and whether they are the same. The pair of event types, and whether they are the same. The pair of realis types and whether they are the same. POS pair of the two mentions and whether they are the same. Whether the 5-word windows of the two mentions matches exactly. Sentence distance between the two mentions. Frame name pair of the two mentions and whether they are the same. Whether a mention is the syntactic ancestor of another. Table 1: Coreference Features. Parsing is done using Stanford CoreNLP (Manning et al., 2014); frame names are produced by Semafor (Das and Smith, 2011). by 2. For example, in Figure 2 the prediction graph (bottom right) incorrectly links m4 to Root and misses a link to m3 , which cause a total loss of 3. In addition, to be consistent with the feature delta computation, we do not compute loss for predicted links that are inferable from the gold standard. 3.2 Features 3.2.1 Event Coreference Features For event coreference, we design a simple feature set to capture syntactic and semantic similarity of arcs. The main features are summarized in Table 1. In the TAC KBP 2015 coreference task setting, the event mentions are annotated with two attrib"
C18-1309,D12-1062,0,0.118648,"se knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002). There have been several studies on building automatic temporal reasoning systems (Uzzaman and Allen, 2010; Do et al., 2012; Chambers et al., 2014). In comparison, the Event Sequencing task is motivated by the Script theory, which places more emphasis on common-sense knowledge about event chronology. 3 Model 3.1 Graph-Based Decoding Model In the Latent Antecedent Tree (LAT) model popularly used for entity coreference decoding (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014), each node represents an event mention and each arc a coreference relation, and new mentions are connected to some past mention considered most similar. Thus the LAT model represents the decoding structure as a tree. This can represent any"
C18-1309,Q14-1037,0,0.0258159,"on a single timeline; 2) temporal relations for events occurring at similar time points may be complicated. Script-based relations may alleviate the problem. For example, if a bombing kills some people, the temporal relation of the bombing and kill may be “inclusion” or “after”. This is considered an After relation in ES because bombing causes the killing. For structure prediction, decoding — recovering the complex structure from local decisions — is one of the core problems. The most successful decoding algorithm for coreference nowadays is mention ranking based (Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014; Lee et al., 2017). These models rank the antecedents (mentions that appear earlier in discourse) and recover the full coreference clusters from local decisions. However, unlike coreference relations, sequencing relations are directed. Coreference decoding algorithms cannot be directly applied to such relations (§3.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task (§4.4) and beats an informed baseline on the event sequencing task (§4.5). Fina"
C18-1309,W13-1203,1,0.88579,"finition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to"
C18-1309,W97-1311,0,0.382807,"coding algorithms cannot be directly applied to such relations (§3.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task (§4.4) and beats an informed baseline on the event sequencing task (§4.5). Finally, we analyze the results and discuss the difficult challenges for both tasks (§5). Detailed definitions of these tasks can be found in the corresponding task documents2 . 2 Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the de"
C18-1309,D12-1045,0,0.0264432,"event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised"
C18-1309,D17-1018,0,0.0145507,"temporal relations for events occurring at similar time points may be complicated. Script-based relations may alleviate the problem. For example, if a bombing kills some people, the temporal relation of the bombing and kill may be “inclusion” or “after”. This is considered an After relation in ES because bombing causes the killing. For structure prediction, decoding — recovering the complex structure from local decisions — is one of the core problems. The most successful decoding algorithm for coreference nowadays is mention ranking based (Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014; Lee et al., 2017). These models rank the antecedents (mentions that appear earlier in discourse) and recover the full coreference clusters from local decisions. However, unlike coreference relations, sequencing relations are directed. Coreference decoding algorithms cannot be directly applied to such relations (§3.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task (§4.4) and beats an informed baseline on the event sequencing task (§4.5). Finally, we analyze the"
C18-1309,liu-etal-2014-supervised,1,0.910364,"nd several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. F"
C18-1309,P17-1009,0,0.0535241,"ormally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, This work is licensed under a Creative Commons Attribution 4.0 Inter"
C18-1309,H05-1004,0,0.195691,"Missing"
C18-1309,P14-5010,0,0.0044398,"Missing"
C18-1309,N16-1098,0,0.0151667,"these event relations. Title and Abstract in Chinese 基于图的事件共指消解以及依序关系解码算法 文本中提及的事件之间常存在着复杂的关系。在这篇文章中，我们主要研究两种关系： 事件间的共指关系以及依序关系。我们指出，常被应用在事件指代消歧上的传统的树结 构解码方式并不适用于解决事件依序问题。为了解决这个问题，我们提出了一种适用于 两种情况的新的图结构解码算法。这个算法让我们可以为这两个任务设计灵活的特征。 在实验上，我们的事件共指消解系统在TAC-KBP 2015的共指消解任务上达到了目前最佳 的水平。同时，我们的依序关系系统的结果超过了一个基于时间分析并有部分正确答案 的基线系统。最后我们分析了结果，并讨论了事件关系判别任务的一些挑战。 1 Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011), news summarization (Vossen and Caselli, 2015), story understanding (Mostafazadeh et al., 2016). Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and E"
C18-1309,P02-1014,0,0.0497368,"on as the original graph. In the example above, structure 1 is a transitive reduction graph for structure 2. We call the decoding structures that corresponding to the reduction graphs as minimum decoding structures. For LAG, we further restrict Z(A) to contain only minimum decoding structures. 3.1.2 Training Details in Latent Antecedent Graph In this section, we describe the decoding details for LAG. Note that if we enforce a single antecedent for each node (as in our coreference model), it falls back to the LAT model (Bj¨orkelund and Kuhn, 2014). Decoding: We use a greedy best-first decoder (Ng and Cardie, 2002), which makes a left-to-right pass over the mentions. The decoding step is the same for line 6 and 8. The only difference is that we will ˜ at line 8. For each node mj , we keep all links that score higher than the root use gold antecedent set (A) link h0, mj , ri. Cycle and Structure Check: Incremental decoding a DAG may introduce cycles to the graph, or violate the minimum decoding structure criterion. To solve this, we maintain a set R(mi ) that is reachable from mi during the decoding process. We reject a new link (hmj , mi i if mj ∈ R(mi )) to avoid cycles. We also reject a redundant link"
C18-1309,D16-1038,0,0.0581449,"ed as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, This work is licensed under a Creative Commons Attr"
C18-1309,P16-1027,0,0.116574,"ion comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014). This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002). There have been several studies on"
C18-1309,D15-1195,0,0.0507238,"Missing"
C18-1309,D11-1116,1,0.787984,"vent mentions share arguments, and the semantic frame name of the shared argument (produced by the Semafor parser (Das and Smith, 2011)). 2. Discourse-Based Script Compatibility: these features capture whether two event mentions are related given the discourse context. • Dependency path between the two mentions. • Function words (words other than Noun, Verb, Adjective and Adverb) in between the two mentions. • The types of other event mentions between the two mentions. • The sentence distance of two event mentions. • Whether there are temporal expressions (AGM-TMP slot from a semantic parser (Tratz and Hovy, 2011)) in the sentences of the two mentions. 3. Event Ordering: this feature set tries to capture the ordering of events. We use the discourse ordering of two mentions (forward: the antecedent is the parent; backward: the antecedent is the child), and temporal ordering produced by Caevo (Chambers et al., 2014). 3650 Taking the after arc from fired to killed in Figure 1 as an example, a feature after the cross product is: Event type pair is Conflict.Attack and Life.Die, discourse ordering is backward, and sentence distance is 0. 4 Experiments 4.1 Dataset We conduct experiments on the dataset release"
C18-1309,S10-1062,0,0.0378593,"s as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002). There have been several studies on building automatic temporal reasoning systems (Uzzaman and Allen, 2010; Do et al., 2012; Chambers et al., 2014). In comparison, the Event Sequencing task is motivated by the Script theory, which places more emphasis on common-sense knowledge about event chronology. 3 Model 3.1 Graph-Based Decoding Model In the Latent Antecedent Tree (LAT) model popularly used for entity coreference decoding (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014), each node represents an event mention and each arc a coreference relation, and new mentions are connected to some past mention considered most similar. Thus the LAT model represents the decoding structure as a tree. This c"
C18-1309,S13-2001,0,0.219916,"Missing"
C18-1309,W15-4507,0,0.0234149,"baseline. We discuss the challenges of studying these event relations. Title and Abstract in Chinese 基于图的事件共指消解以及依序关系解码算法 文本中提及的事件之间常存在着复杂的关系。在这篇文章中，我们主要研究两种关系： 事件间的共指关系以及依序关系。我们指出，常被应用在事件指代消歧上的传统的树结 构解码方式并不适用于解决事件依序问题。为了解决这个问题，我们提出了一种适用于 两种情况的新的图结构解码算法。这个算法让我们可以为这两个任务设计灵活的特征。 在实验上，我们的事件共指消解系统在TAC-KBP 2015的共指消解任务上达到了目前最佳 的水平。同时，我们的依序关系系统的结果超过了一个基于时间分析并有部分正确答案 的基线系统。最后我们分析了结果，并讨论了事件关系判别任务的一些挑战。 1 Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011), news summarization (Vossen and Caselli, 2015), story understanding (Mostafazadeh et al., 2016). Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types"
C18-1309,C98-1012,0,\N,Missing
C18-1309,Q14-1022,0,\N,Missing
C18-1309,N15-1082,0,\N,Missing
C18-1309,M93-1007,0,\N,Missing
C94-1012,1993.tmi-1.28,0,\N,Missing
C94-1012,C92-2113,0,\N,Missing
C94-1012,1991.mtsummit-papers.9,1,\N,Missing
C94-1013,C92-3168,1,\N,Missing
C94-1013,1991.mtsummit-papers.9,1,\N,Missing
cavalli-sforza-etal-2000-challenges,hakkani-etal-1998-english,1,\N,Missing
cavalli-sforza-etal-2000-challenges,C92-3168,1,\N,Missing
D07-1003,J90-2002,0,0.0433493,"l. (2005) experimented with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual “dictionary” which simply encodes the identity relation. This brings us to the question that drives this work: is there a statistical translation-like model that is natura"
D07-1003,J93-2003,0,0.0563337,"ed with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual “dictionary” which simply encodes the identity relation. This brings us to the question that drives this work: is there a statistical translation-like model that is natural and accurate for qu"
D07-1003,P05-1067,0,0.0586,"pardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999"
D07-1003,P03-1003,0,0.0374351,"ast-growing research problem. Stateof-the-art QA systems are extremely complex. They usually take the form of a pipeline architecture, chaining together modules that perform tasks such as answer type analysis (identifying whether the correct answer will be a person, location, date, etc.), document retrieval, answer candidate extraction, and answer reranking. This architecture is so predominant that each task listed above has evolved into its own sub-field and is often studied and evaluated independently (Shima et al., 2006). At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). The first step, retrieval, narrows down the search space from a corpus of millions of documents to a focused set of maybe a few hundred using an IR engine, where efficiency and recall are the main focus. The second step, selection, assesses each candidate answer string proposed by the first step, and finds the one that is most likely to be an answer to the given question. The granularity of the target answer string varies depending on the type of the question. For example, answers to factoid questions (e.g., Who, When, Where) are usually single words or short phrases, while definitional ques"
D07-1003,P03-2041,0,0.0350525,"ection 3. Our version of QG, called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case"
D07-1003,P06-1121,0,0.0474339,"ethod are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999; Attardi et al., 2001). More recent atte"
D07-1003,P03-1011,0,0.0554453,"version of QG, called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performa"
D07-1003,P06-1114,0,0.0164312,"gine a generative story for QA in which the question is generated from the answer sentence through a series of syntactic and semantic transformations. The same story has been told for machine translation (Yamada and Knight, 2001, inter alia), in which a target language sentence (the desired output) has undergone semantic transformation (word to word translation) and syntactic transformation (syntax divergence across languages) to generate the source language sentence (noisy-channel model). Similar stories can also be found in paraphrasing (Quirk et al., 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005). Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner (2006) for machine translation. Unlike most synchronous formalisms, QG does not posit a strict isomorphism between the two trees, and it provides 23 an elegant description for the set of local configurations. In Section 2 we situate our contribution in the context of earlier work, and we give a brief discussion of quasi-synchronous grammars in Section 3. Our version of QG, called the Jeopardy model, and our parameter estimation method are desc"
D07-1003,N06-1006,0,0.0625658,"Missing"
D07-1003,J93-2004,0,0.0507162,"Missing"
D07-1003,P05-1012,0,0.0337334,"p(a |q) ∝ p(q | a) · p(a). Because A is known and is assumed to be generated by an external extraction system, we could use that extraction system to assign scores (and hence, probabilities p(a)) to the candidate answers. Other scores could also be used, such as reputability of the document the answer came from, grammaticality, etc. Here, aiming for simplicity, we do not aim to use such information. Hence we treat p(a) as uniform over A.3 The second adjustment adds a labeled, directed dependency tree to the question and the answer. The tree is produced by a state-of-the-art dependency parser (McDonald et al., 2005) trained on the Wall Street Journal Penn Treebank (Marcus et al., 1993). A dependency tree on a sequence w = hw1 , ..., wk i is a mapping of indices of words to indices of their syntactic parents and a label for the syntactic relation, τ : {1, ..., k} → {0, ..., k} × L. Each word wi has a single parent, denoted wτ (i).par . Cycles are not permitted. w0 is taken to be the invisible “wall” symbol at the left edge of the sentence; it has a single child (|{i : τ (i) = 0} |= 1). The label for wi is denoted τ (i).lab. The third adjustment involves a hidden variable X, the alignment between question"
D07-1003,2004.tmi-1.5,0,0.00748673,"called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually de"
D07-1003,H05-1086,0,0.0498115,"between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual “dictionary” which simply encodes the identity relation. This brings us to the question that drives this work: is there a statistical translation-like model that is natural and accurate for question answering? We propose Smith and Eisner’s (2006) qua"
D07-1003,W04-3219,0,0.00920502,"Missing"
D07-1003,P05-1034,0,0.0605414,"rameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999; Attardi et al., 20"
D07-1003,N03-2029,0,0.0120182,"e definitional questions and other more complex question types (e.g., How, Why) look for sentences or short passages. In this work, we fix the granularity of an answer to a single sentence. Earlier work on answer selection relies only on the surface-level text information. Two approaches are most common: surface pattern matching, and similarity measures on the question and answer, represented as bags of words. In the former, patterns for a certain answer type are either crafted manually (Soubbotin and Soubbotin, 2001) or acquired from training examples automatically (Ittycheriah et al., 2001; Ravichandran et al., 2003; Licuanan and Weischedel, 2003). In the latter, measures like cosine-similarity are applied to (usually) bag-of-words representations of the question and answer. Although many of these systems have achieved very good results in TREC-style evaluations, shallow methods using the bag-of-word representation clearly have their limitations. Examples of 22 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 22–32, Prague, June 2007. 2007 Association for Computational Linguistics cases where the bag-of-words a"
D07-1003,P06-1112,0,0.0329313,"he usual similarity measures can then be used on the new feature representation. For example, Punyakanok et al. (2004) used approximate tree matching and tree-edit-distance to compute a similarity score between the question and answer parse trees. Similarly, Shen et al. (2005) experimented with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in E"
D07-1003,I05-1045,0,0.0152485,"tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999; Attardi et al., 2001). More recent attempts have tried to augment the bag-ofwords representation—which, after all, is simply a real-valued feature vector—with syntactic features. The usual similarity measures can then be used on the new feature representation. For example, Punyakanok et al. (2004) used approximate tree matching and tree-edit-distance to compute a similarity score between the question and answer parse trees. Similarly, Shen et al. (2005) experimented with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et"
D07-1003,shima-etal-2006-modular,1,0.379433,"1 Introduction and Motivation Open-domain question answering (QA) is a widelystudied and fast-growing research problem. Stateof-the-art QA systems are extremely complex. They usually take the form of a pipeline architecture, chaining together modules that perform tasks such as answer type analysis (identifying whether the correct answer will be a person, location, date, etc.), document retrieval, answer candidate extraction, and answer reranking. This architecture is so predominant that each task listed above has evolved into its own sub-field and is often studied and evaluated independently (Shima et al., 2006). At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). The first step, retrieval, narrows down the search space from a corpus of millions of documents to a focused set of maybe a few hundred using an IR engine, where efficiency and recall are the main focus. The second step, selection, assesses each candidate answer string proposed by the first step, and finds the one that is most likely to be an answer to the given question. The granularity of the target answer string varies depending on the type of the question. For example, answers to factoid quest"
D07-1003,W06-3104,0,0.113674,"cience Carnegie Mellon University Pittsburgh, PA 15213 USA {mengqiu,nasmith,teruko}@cs.cmu.edu Abstract This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines. 1 Introduction and Motivation Open-domain question answering (QA) is a widelystudied and fast-growing research problem. Stateof-the-art QA systems are extremely complex. They usually take the form of a pipeline architecture, chaining together modules that perfo"
D07-1003,P98-2230,0,0.0718167,"onous grammars in Section 3. Our version of QG, called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was"
D07-1003,W05-1205,0,0.0133641,"sing. Indeed, in this work, we imagine a generative story for QA in which the question is generated from the answer sentence through a series of syntactic and semantic transformations. The same story has been told for machine translation (Yamada and Knight, 2001, inter alia), in which a target language sentence (the desired output) has undergone semantic transformation (word to word translation) and syntactic transformation (syntax divergence across languages) to generate the source language sentence (noisy-channel model). Similar stories can also be found in paraphrasing (Quirk et al., 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005). Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner (2006) for machine translation. Unlike most synchronous formalisms, QG does not posit a strict isomorphism between the two trees, and it provides 23 an elegant description for the set of local configurations. In Section 2 we situate our contribution in the context of earlier work, and we give a brief discussion of quasi-synchronous grammars in Section 3. Our version of QG, called the Jeopardy m"
D07-1003,P01-1067,0,0.0412892,"semantic variations occur in almost every question-answer pair, and typically they cannot be easily captured using shallow representations. It is also worth noting that such syntactic and semantic variations are not unique to QA; they can be found in many other closely related NLP tasks, motivating extensive community efforts in syntactic and semantic processing. Indeed, in this work, we imagine a generative story for QA in which the question is generated from the answer sentence through a series of syntactic and semantic transformations. The same story has been told for machine translation (Yamada and Knight, 2001, inter alia), in which a target language sentence (the desired output) has undergone semantic transformation (word to word translation) and syntactic transformation (syntax divergence across languages) to generate the source language sentence (noisy-channel model). Similar stories can also be found in paraphrasing (Quirk et al., 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005). Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner (2006) for machine translation. Unlike most synchr"
D07-1003,C98-2225,0,\N,Missing
D15-1247,J14-2004,0,0.185758,"enge of synchronizing the assignments of event triggers and coreference. To avoid this problem, we propose an incremental decoding algorithm that combines the segment-based decoding and best-first clustering algorithm. 3. Our experiments indicate that the joint model achieves a substantial performance gain in event coreference resolution with a corpus in the biology domain, as compared to a pipelined model. into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling ("
D15-1247,D14-1159,0,0.494702,"E3 and E4. On the other hand, previous works typically rely on a pipelined model that extracts events (e.g., E1 and E3) at the first stage, and then resolves event coreference at the second stage. Although this modularity is preferable from development perspectives, the pipelined model limits the interactions. That is, the first stage alone is unlikely to detect E2 and E4 as events due to the difficulties described above. These missing events make it impossible for the second stage to resolve event coreference E1-E2 and E3-E4. In this work, we address the problem using the ProcessBank corpus (Berant et al., 2014). Following the terminology defined in the corpus, we introduce several terms: • Event: an abstract representation of a change of state, independent from particular texts. • Event trigger: main word(s) in text, typically a verb or a noun that most clearly expresses an event. • Event arguments: participants or attributes in text, typically nouns, that are involved in an event. • Event mention: a clause in text that describes an event, and includes both a trigger and arguments. • Event coreference: a linguistic phenomenon that two event mentions refer to the same event. We aim to explore the int"
D15-1247,P08-2037,0,0.0231809,"el outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses. The Pale"
D15-1247,W12-4503,0,0.0502561,"Missing"
D15-1247,P11-1113,0,0.0560022,"Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference l"
D15-1247,N12-1015,0,0.0475719,"Missing"
D15-1247,P08-1030,0,0.149337,"n and event coreference resolution. 2 3 Related Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, an"
D15-1247,P11-1115,0,0.0393365,"ntifies event triggers and resolves event coreference. We demonstrate that the joint model outperforms a pipelined model by 6.9 BLANC F1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations"
D15-1247,D08-1008,0,0.0872067,"Missing"
D15-1247,D12-1045,0,0.0722715,"ncremental decoding algorithm that combines the segment-based decoding and best-first clustering algorithm. 3. Our experiments indicate that the joint model achieves a substantial performance gain in event coreference resolution with a corpus in the biology domain, as compared to a pipelined model. into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the"
D15-1247,P14-1005,0,0.0281297,"Missing"
D15-1247,J13-4004,0,0.103037,"nt coreference by comparing event coreferences from the complete assignment at a certain position with those from complete assignments at following positions. This makes it complicated to implement the formalization of token-level sequential labeling for joint decoding in our task. One possible way to avoid this problem is to extract event trigger candidates with a preference on high recall first, and then search event coreference from those candidates, regarding them as complete assignments of an event trigger. This recalloriented pre-filtering is often used in entity coreference resolution (Lee et al., 2013; Bj¨orkelund 2076 Algorithm 1 Joint decoding for event triggers and coreference with beam search. Input: input document x = (x1 , x2 , . . . , xn ) Input: beam width k, max length of event trigger lmax Output: best event graph yˆ for x 1: initialize empty beam history B[1..n] 2: for i ← 1..n do 3: for l ← 1..lmax do 4: for y ∈ B[i − l] do 5: e ← C REATE E VENT T RIGGER(l, i). 6: A PPEND E VENT T RIGGER(y, e) 7: B[i] ← k-B EST(B[i] ∪ y) 8: for j ← 1..i − 1 do 9: c ← C REATE E VENT C OREF(j, e). 10: A DD E VENT C OREF(y, c) 11: B[i] ← k-B EST(B[i] ∪ y) 12: return B[n][0] 4 and Farkas, 2012). In"
D15-1247,W09-1206,0,0.0807227,"Missing"
D15-1247,D12-1133,0,0.028866,", some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 2 3"
D15-1247,P14-1038,0,0.0202558,"jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 2 3 Related Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has"
D15-1247,P06-1047,0,0.0359311,"1 and 1.8 CoNLL F1 points in event coreference resolution using a corpus in the biology domain. 1 Introduction Events convey semantic information such as who did what to whom where and when. They also corefer to each other, playing a role of discourse connection points to form a coherent story. These aspects of events have been already utilized in a wide variety of natural language processing (NLP) applications, such as automated population of knowledge bases (Ji and Grishman, 2011), topic detection and tracking (Allan, 2002), question answering (Bikel and Castelli, 2008), text summarization (Li et al., 2006), and contradiction detection (de Marneffe et al., 2008). This fact illustrates the importance of event extraction and event coreference resolution. Those semantic and discourse aspects of events are not independent from each other, and in fact often work in interactive manners. We give two examples of the interactions: (1) British bank Barclays had agreed to buy(E1) Spanish rival Banco Zaragozano for 1.14 billion euros. The combination(E2) of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses. The Palestinian Authority condemned the attack"
D15-1247,P04-1015,0,0.0719526,"em specifically on event coreference, and thus their work is not comparable to ours. # of paragraphs # of event triggers # of event coreferences Train 120 823 73 Dev 30 224 28 Test 50 356 30 Total 200 1403 131 Table 1: Statistics of our dataset. Unlike previous work (Berant et al., 2014; Li et al., 2013), we explicitly allow an event trigger to have multiple tokens, such as verb phrase ‘look into’ and compound proper noun ‘World War II’. This is a more realistic setting for event trigger identification since in general there are a considerable number of multi-token event triggers1 . 3.2 date (Collins and Roark, 2004) and max-violation update (Huang et al., 2012) to our model. Our initial experiments indicated that early updates happen too early to gain sufficient feedback on weights from entire documents in training examples, ending up with a poorer performance than the standard update. This contrasts with the fact that the early-update strategy was successfully applied to other NLP tasks such as constituent parsing (Collins and Roark, 2004) and dependency parsing (Zhang and Clark, 2008b). The main reason why the early update fell short of the standard update in our setting is that joint event trigger ide"
D15-1247,P13-1008,0,0.287607,"system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between event trigger identification and event coreference resolution. 2 3 Related Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two task"
D15-1247,W02-1001,0,0.58455,"oth a trigger and arguments. • Event coreference: a linguistic phenomenon that two event mentions refer to the same event. We aim to explore the interactions between event mentions and event coreference. As a first step toward the goal, we focus on the task of identifying event triggers and resolving event coreference, and 2074 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2074–2080, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. propose a document-level joint learning model using structured perceptron (Collins, 2002) that simultaneously predicts them. Our assumption is that the joint model is able to capture the interactions between event triggers and event coreference adequately, and such comprehensive decision improves the system performance. For instance, the joint model is likely to extract E2 as well as E1 successfully via their event coreference by simultaneously looking at coreference features. Our contributions are as follows: 1. This is the first work that simultaneously predicts event triggers and event coreference using a single joint model. At the core of the model is a document-level structur"
D15-1247,P10-1081,0,0.098137,"resolution. 2 3 Related Work No previous work deals with event extraction and event coreference resolution simultaneously. We thus describe how these two tasks have been addressed separately, and how joint structured learning has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an"
D15-1247,P08-1118,0,0.061042,"Missing"
D15-1247,liu-etal-2014-supervised,1,0.59495,"ssignments of event triggers and coreference. To avoid this problem, we propose an incremental decoding algorithm that combines the segment-based decoding and best-first clustering algorithm. 3. Our experiments indicate that the joint model achieves a substantial performance gain in event coreference resolution with a corpus in the biology domain, as compared to a pipelined model. into their model. Event coreference resolution is more challenging and less explored. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugue"
D15-1247,P14-2005,0,0.0135457,"clearly reveals the effectiveness of the joint model by focusing only on the architectural difference. One could develop other baseline systems. One of them is a deterministic sieve-based approach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and 2077 MUC B3 CEAFm CEAFe BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event corefere"
D15-1247,H05-1004,0,0.395239,"gger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and 2077 MUC B3 CEAFm CEAFe BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009), which is the average of MUC F1, B3 F1, and CEAFe F1. 5 Results and Discussio"
D15-1247,P14-5010,0,0.0125291,"Missing"
D15-1247,D08-1059,0,0.213698,"red. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between even"
D15-1247,N13-1038,0,0.0201073,"r knowledge, this is the first work that solves these two tasks simultaneously. Our experiment shows that the proposed method effectively penalizes false positives in joint search, thereby outperforming a pipelined model substantially in event coreference resolution. There are a number of avenues for future work. One can further ensure the advantage of the joint model using a larger corpus. Our preliminary experiment on the ACE 2005 corpus shows that due to its larger document size and event types, one will need to reduce training time by a distributed learning algorithm such as mini-batches (Zhao and Huang, 2013). Another future work is to incorporate other components of events into the model. These include event types, event arguments, and other relations such as subevents. One could leverage them as other learning targets or constraints, and investigate further benefits of joint modeling. Acknowledgments This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the Deep Exploration and Filtering of Text (DEFT) Program, and by U.S. Army Research Office (ARO) grant W911NF-14-1-0436 under the Reading, Extraction, and Assembly of Pathways for Evidentiary Reading (REAPER) Program."
D15-1247,P11-1163,0,0.0473173,"LP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference link between two event triggers. 3.1 Corpus The ProcessBank corpus consists of 200 paragraphs from the textbook Biology (Campbell and Reece, 2005). Table 1 shows statistics of our data splits. The original corpus provides 150 paragra"
D15-1247,P02-1014,0,0.461227,"Missing"
D15-1247,N10-1123,0,0.0177668,"has been studied in other NLP tasks. Event extraction has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference link between two event triggers. 3.1 Corpus The ProcessBank corpus consists of 200 paragraphs from the textbook Biology (Campbell and Reece, 2005). Table 1 shows statistics of our data splits. The original corp"
D15-1247,P14-2006,0,0.0293867,"Missing"
D15-1247,W09-1119,0,0.148344,"Missing"
D15-1247,D11-1001,0,0.018594,"ion has been studied mainly in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference link between two event triggers. 3.1 Corpus The ProcessBank corpus consists of 200 paragraphs from the textbook Biology (Campbell and Reece, 2005). Table 1 shows statistics of our data splits. The original corpus provides 150 paragraphs as training data, and w"
D15-1247,D14-1090,0,0.0188771,"in the newswire domain and the biomedical domain as the task of detecting event triggers and determining their event types and arguments. In the former domain, most work took a pipelined approach where local classifiers identify triggers first, and then detect arguments (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). Li et al. (2013) presented a structured perceptron model to detect triggers and arguments jointly. Similarly, joint dependencies in events were also addressed in the latter domain (Poon and Vanderwende, 2010; McClosky et al., 2011; Riedel and McCallum, 2011; Venugopal et al., 2014). However, none of them incorporated event coreference Approach We formalize the extraction of event triggers and event coreference as a problem of structured prediction. The output structure is a document-level event graph where each node represents an event trigger, and each edge represents an event coreference link between two event triggers. 3.1 Corpus The ProcessBank corpus consists of 200 paragraphs from the textbook Biology (Campbell and Reece, 2005). Table 1 shows statistics of our data splits. The original corpus provides 150 paragraphs as training data, and we split them into 120 and"
D15-1247,M95-1005,0,0.544053,"ach by Lee et al. (2013). A natural extension to the approach for performing event trigger identification as well as event coreference resolution would be to develop additional sieves to classify singletons into real event triggers or spurious ones. We leave it for future work. 4.2 Evaluation We evaluate our system using a reference implementation of coreference scoring algorithms (Pradhan et al., 2014; Luo et al., 2014). As for event trigger identification, this scorer computes precision (P), recall (R), and the F1 score. With respect to event coreference resolution, the scorer computes MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), two CEAF metrics CEAFm and CEAFe (Luo, 2005), and 2077 MUC B3 CEAFm CEAFe BLANC CoNLL System R P F1 R P F1 R P F1 R P F1 R P F1 F1 Baseline 26.66 19.51 22.53 55.47 58.64 57.01 53.08 60.38 56.50 52.68 63.14 57.44 30.13 25.10 25.05 45.66 Joint 20.00 37.50 26.08 53.37 63.36 57.93 53.93 62.95 58.09 55.06 62.11 58.38 27.51 38.43 31.91 47.45 Table 2: Results of event coreference resolution. ‘Baseline’ refers to the second stage of our baseline. BLANC (Recasens and Hovy, 2011) extended by Luo et al. (2014). We also report the CoNLL average (Denis and Baldridge, 2009),"
D15-1247,P08-1101,0,0.195824,"red. To set up event triggers as a starting point of the task, some works use human annotation in a corpus (Bejan and Harabagiu, 2014; Liu et al., 2014), and others use the output of a separate event extraction system (Lee et al., 2012). Berant et al. (2014) presented a model that jointly predicts event arguments and event coreference (as well as other relations between event triggers). However, none of them tries to predict event triggers and event coreference jointly. Joint structured learning has been applied to several NLP tasks, such as word segmentation and part-of-speech (POS) tagging (Zhang and Clark, 2008a), POS tagging and dependency parsing (Bohnet and Nivre, 2012), dependency parsing and semantic role labeling (Johansson and Nugues, 2008), the extraction of event triggers and arguments (Li et al., 2013), and the extraction of entity mentions and relations (Li and Ji, 2014). Their underlying ideas are similar to ours. That is, one can train a structured learning model to globally capture the interactions between two relevant tasks via a certain kind of structure, while making predictions specifically for these respective tasks. However, no prior work has studied the interactions between even"
D18-1154,araki-etal-2014-detecting,1,0.82887,"ent salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed"
D18-1154,P98-1013,0,0.666037,"tence or across the whole document are used to capture 1226 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1226–1236 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics interactions on both local and global aspects (§4). The model significantly outperforms a strong “Frequency” baseline in our experiments. However, there are other discourse relations beyond lexical similarity. Figure 1 showcases some: the script relation (Schank and Abelson, 1977)1 between “charge” and “trial”, and the frame relation (Baker et al., 1998) between “attacks” and “trial” (“attacks” fills the “charges” role of “trial”). Since it is unclear which ones contribute more to salience, we design a Kernel based Centrality Estimation (KCE) model (§5) to capture salient specific interactions between discourse units automatically. In KCE, discourse units are projected to embeddings, which are trained end-to-end towards the salience task to capture rich semantic information. A set of soft-count kernels are trained to weigh salient specific latent relations between discourse units. With the capacity to model richer relations, KCE outperforms t"
D18-1154,D13-1178,0,0.109023,"de, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger"
D18-1154,W98-1501,0,0.106467,"ntrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most important event per document. There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018). In this work, we study the case for events. Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976). Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999). Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004). Though sharing similar intuitions, our proposed models are designed to learn richer semantic relations in the embedding space. Comparing to the traditional summarization task, we focus on events, which are at a different granularity. Our experiments also unveil interesting phenomena among events and other discourse units. 3 The Event Salience Corpus This section introduces our approach to construct a"
D18-1154,D12-1062,0,0.0326899,"n these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using s"
D18-1154,J08-1001,0,0.19003,"relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In Figure 1, the “trial” event is connected to many other events: “charge” is pressed before “trial”; “trial” is being “delayed”. We present two salience detection systems based on the o"
D18-1154,L16-1527,0,0.0181484,"rd frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most important event per document. There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018). In this work, we study the case for events. Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976). Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999). Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004). Though sharing similar intuitions, our proposed models are designed"
D18-1154,W17-2712,0,0.0246946,", we focus on events, which are at a different granularity. Our experiments also unveil interesting phenomena among events and other discourse units. 3 The Event Salience Corpus This section introduces our approach to construct a large-scale event salience corpus, including methods for finding event mentions and obtaining saliency labels. The studies are based on the Annotated New York Times corpus (Sandhaus, 2008), a newswire corpus with expert-written abstracts. 1227 3.1 Automatic Corpus Creation Event Mention Annotation: Despite many annotation attempts on events (Pustejovsky et al., 2002; Brown et al., 2017), automatic labeling of them in general domain remains an open problem. Most of the previous work follows empirical approaches. For example, Chambers and Jurafsky (2008) consider all verbs together with their subject and object as events. Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). There are two main challenges in labeling event mentions. First, we need to decide which lexical items are event triggers. Second, we have to disambiguate the word sense to correctly identify event"
D18-1154,E14-4040,0,0.504409,"he feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures). 1 Figure 1: Examples annotations. Underlying words are annotated event triggers; the red bold ones are annotated as salient. Introduction Automatic extraction of prominent information from text has always been a core problem in language research. While traditional methods mostly concentrate on the word level, researchers start to analyze higher-level discourse units in text, such as entities (Dunietz and Gillick, 2014) and events (Choubey et al., 2018). Events are important discourse units that form the backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of t"
D18-1154,P08-1090,0,0.554492,"d for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more g"
D18-1154,N18-1076,0,0.0416231,"he “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In Figure 1, the “trial” event is connected to many other events: “charge” is pressed before “trial”; “trial” is being “delayed”. We present two salience detection systems based on the observations. First is a feature based learning to rank model. Beyond basic features like frequ"
D18-1154,N13-1104,0,0.0399892,"Missing"
D18-1154,N18-2055,0,0.118711,"Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures). 1 Figure 1: Examples annotations. Underlying words are annotated event triggers; the red bold ones are annotated as salient. Introduction Automatic extraction of prominent information from text has always been a core problem in language research. While traditional methods mostly concentrate on the word level, researchers start to analyze higher-level discourse units in text, such as entities (Dunietz and Gillick, 2014) and events (Choubey et al., 2018). Events are important discourse units that form the backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events"
D18-1154,P11-1144,0,0.0601145,"with their subject and object as events. Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). There are two main challenges in labeling event mentions. First, we need to decide which lexical items are event triggers. Second, we have to disambiguate the word sense to correctly identify events. For example, the word “phone” can refer to an entity (a physical phone) or an event (a phone call event). We use FrameNet to solve these problems. We first use a FrameNet based parser: Semafor (Das and Smith, 2011), to find and disambiguate triggers into frame classes. We then use the FrameNet ontology to select event mentions. Our frame based selection method follows the Vendler classes (Vendler, 1957), a four way classification of eventuality: states, activities, accomplishments and achievements. The last three classes involve state change, and are normally considered as events. Following this, we create an “eventevoking frame” list using the following procedure: 1. We keep frames that are subframes of Event and Process in the FrameNet ontology. 2. We discard frames that are subframes of state, entity"
D18-1154,D11-1027,0,0.0640051,"orpus, including methods for finding event mentions and obtaining saliency labels. The studies are based on the Annotated New York Times corpus (Sandhaus, 2008), a newswire corpus with expert-written abstracts. 1227 3.1 Automatic Corpus Creation Event Mention Annotation: Despite many annotation attempts on events (Pustejovsky et al., 2002; Brown et al., 2017), automatic labeling of them in general domain remains an open problem. Most of the previous work follows empirical approaches. For example, Chambers and Jurafsky (2008) consider all verbs together with their subject and object as events. Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). There are two main challenges in labeling event mentions. First, we need to decide which lexical items are event triggers. Second, we have to disambiguate the word sense to correctly identify events. For example, the word “phone” can refer to an entity (a physical phone) or an event (a phone call event). We use FrameNet to solve these problems. We first use a FrameNet based parser: Semafor (Das and Smith, 2011), to find and disambiguate triggers in"
D18-1154,P13-1008,0,0.0335072,"e observe interesting connections between salience and various discourse relations (§7.1 and Table 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant ev"
D18-1154,liu-etal-2014-supervised,1,0.800884,"tions (§7.1 and Table 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and impo"
D18-1154,P17-1009,0,0.0174103,"ble 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundred"
D18-1154,P14-5010,0,0.00271144,"vi , d) + b where F (evi , d) is the features for evi in d (Table 2); Wf and b are the parameters to learn. The model is trained with pairwise loss: This section presents the feature-based model, including the features and the learning process. 4.1 X max(0, 1 − f (ev + , d) + f (ev − , d)), (2) ev + ,ev − ∈d w.r.t. y(ev + , d) = +1 & y(ev − , d) = −1. Features Our features are summarized in Table 2. Basic Discourse Features: We first use two basic features similar to Dunietz and Gillick (2014): Frequency and Sentence Location. Frequency is the lemma count of the mention’s syntactic head word (Manning et al., 2014). Sentence Location is the sentence index of the mention, since the first few sentences are normally more important. These two features are often used to estimate salience (Barzilay and Lapata, 2008; Vossen and Caselli, 2015). Content Features: We then design several lexical similarity features, to reflect Grimes’ content relatedness (Grimes, 1975). In addition to events, the relations between events and entities are also important. For example, Figure 1 shows some related entities in the legal domain, such as “prosecutors” and “court”. Ideally, they should help promote the salience status for"
D18-1154,J91-1002,0,0.431721,". There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018). In this work, we study the case for events. Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976). Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999). Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004). Though sharing similar intuitions, our proposed models are designed to learn richer semantic relations in the embedding space. Comparing to the traditional summarization task, we focus on events, which are at a different granularity. Our experiments also unveil interesting phenomena among events and other discourse units. 3 The Event Salience Corpus This section introduces our approach to construct a large-scale event salience corpus, including methods for finding event mentions and obtaining saliency labels. The studies are based on the Annotated New York Times corpu"
D18-1154,P15-2060,0,0.0241662,"ting connections between salience and various discourse relations (§7.1 and Table 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They"
D18-1154,D16-1038,0,0.0991669,"Missing"
D18-1154,P16-1027,0,0.0203681,"ripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most im"
D18-1154,W15-4507,0,0.367892,"n documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In Figure 1, the “trial” event is connect"
D18-1154,Q15-1009,0,0.175292,"e backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are formin"
D18-1154,N13-1110,0,0.0192343,"nd Process in the FrameNet ontology. 2. We discard frames that are subframes of state, entity and attribute frames, such as Entity, Attributes, Locale, etc. 3. We manually inspect frames that are not subframes of the above-mentioned ones (around 200) to keep event related ones (including subframes), such as Arson, Delivery, etc. This gives us a total of 569 frames. We parse the documents with Semafor and consider predicates that trigger a frame in the list as candidates. We finish the process by removing the light verbs3 and reporting events4 from the candidates, similar to previous research (Recasens et al., 2013). Salience Labeling: For all articles with a human written abstract (around 664,911) in the New York 3 Light verbs carry little semantic information: “appear”, “be”, “become”, “do”, “have”, “seem”, “do”, “get”, “give”, “go”, “have”, “keep”, “make”, “put”, “set”, “take”. 4 Reporting verbs are normally associated with the narrator: “argue”, “claim”, “say”, “suggest”, “tell”. Train Dev Test # Documents 526126 64000 63589 Avg. # Word 794.12 790.27 798.68 Avg. # Events 61.96 60.65 61.34 Avg. # Entities 197.63 196.95 198.40 8.77 8.79 8.90 Avg. # Salience Table 1: Dataset Statistics. Times Annotated"
D18-1154,recasens-etal-2010-typology,1,0.860753,"Missing"
hakkani-etal-1998-english,E93-1022,0,\N,Missing
hakkani-etal-1998-english,A97-1047,0,\N,Missing
hakkani-etal-1998-english,C92-3168,1,\N,Missing
hakkani-etal-1998-english,E93-1066,1,\N,Missing
hakkani-etal-1998-english,C96-1094,0,\N,Missing
hakkani-etal-1998-english,C92-4202,1,\N,Missing
hakkani-etal-1998-english,C94-1013,1,\N,Missing
hakkani-etal-1998-english,C94-1008,0,\N,Missing
I13-2002,W12-2038,0,0.126099,"eader does not have access to aids that would enable her to get over them including the problem of unknown words interpretation, unrecognized and forgotten names, difficult and hard-to-understand sentences, and lack of or forgetting the prior context in a former session of reading. There are many NLPbased tools, that offer various kinds of aids, to non-native English readers to help them in understanding a document. Many tools focus on assisting the reader in understanding of a specific word which may lead to better comprehension and vocabulary acquisition such as (Nerbonne et al., 1997) and (Eom et al., 2012). Some other tools focus on assisting the reader and second language learner with highlighting different patterns in the documents and providing the learner a visually enhanced version of the document (Meurers et al., ). SmartReader is an implementation of a NLPpowered tool to aid in reading texts in English by 2 System Overview Our system is based on client-server architecture as shown in Figure 1. The server is responsible for annotating plain text with NLP-related annotations and retrieving them based on the user’s interactions. The client is a standard web browser running on PCs or touch t"
I13-2002,P05-1045,0,0.0035597,"validate and normalize its orthography. Using Stanford CoreNLP tools, we segment the text into sentences, and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the wo"
I13-2002,W11-1902,0,0.0318399,"ext into sentences, and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the word overlapping with that position. These are which are then interpreted by the que"
I13-2002,J13-4004,0,0.0225131,", we segment the text into sentences, and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the word overlapping with that position. These are which are then inte"
I13-2002,A97-1020,0,0.780538,"blems, especially when the reader does not have access to aids that would enable her to get over them including the problem of unknown words interpretation, unrecognized and forgotten names, difficult and hard-to-understand sentences, and lack of or forgetting the prior context in a former session of reading. There are many NLPbased tools, that offer various kinds of aids, to non-native English readers to help them in understanding a document. Many tools focus on assisting the reader in understanding of a specific word which may lead to better comprehension and vocabulary acquisition such as (Nerbonne et al., 1997) and (Eom et al., 2012). Some other tools focus on assisting the reader and second language learner with highlighting different patterns in the documents and providing the learner a visually enhanced version of the document (Meurers et al., ). SmartReader is an implementation of a NLPpowered tool to aid in reading texts in English by 2 System Overview Our system is based on client-server architecture as shown in Figure 1. The server is responsible for annotating plain text with NLP-related annotations and retrieving them based on the user’s interactions. The client is a standard web browser ru"
I13-2002,D10-1048,0,0.0234154,", and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the word overlapping with that position. These are which are then interpreted by the query processing unit as descr"
I13-2002,de-marneffe-etal-2006-generating,0,\N,Missing
I13-2002,R13-1006,1,\N,Missing
I13-2002,W10-1002,0,\N,Missing
ko-etal-2006-analyzing,geutner-etal-2002-design,0,\N,Missing
kupsc-etal-2004-pronominal,J90-4001,0,\N,Missing
kupsc-etal-2004-pronominal,1995.iwpt-1.15,0,\N,Missing
kupsc-etal-2004-pronominal,W98-1119,0,\N,Missing
kupsc-etal-2004-pronominal,C88-1021,0,\N,Missing
kupsc-etal-2004-pronominal,briscoe-carroll-2002-robust,0,\N,Missing
kupsc-etal-2004-pronominal,P95-1017,0,\N,Missing
kupsc-etal-2004-pronominal,W02-1028,0,\N,Missing
kupsc-etal-2004-pronominal,J01-4004,0,\N,Missing
kupsc-etal-2004-pronominal,2001.mtsummit-papers.43,1,\N,Missing
L18-1611,W17-0101,0,0.0187796,"parser combinators Multi-output parsers It is straightforward to associate a parser with multiple different kinds of outputs (Hutton, 1992) letting us simultaneously write parsers that target different representations for different NLP tasks. Intuitive representation of morphological phenomena Some morphological phenomena that are awkward to express as finite-state transducers are more straightforwardly expressed in a recursive grammar, such as the kinds of templatic or circumfixal morphology that requires finite-state transducers to be extended with flag “memory” (Pretorius and Bosch, 2003; Bower et al., 2017). It is worth noting that there is no conceptual requirement that an atomic parser define a string truncation like “remove 'd' from the end of a string”, although because of the concatenative nature of most morphology this is the most common kind of atomic parser. The relationship between the input string and the remnant string can be any string-tostring transduction. In the Tigrinya system (§4.1.), we defined some parsers using regular expression substitutions, to handle some particularly difficult plurals that involve both reduplication and root-and-pattern morphology.4 Ease of extension We"
L18-1611,H89-2010,0,0.496414,"r rather than finite-state paradigm. This paradigm allows rapid development and ease of integration with other systems, although at the cost of non-optimal theoretical efficiency. These parsers produce multiple output representations simultaneously, including lemmatization, morphological segmentation, and an English word-for-word gloss, and we evaluate these representations as input for entity detection and linking and humanitarian need detection. Keywords: morphology, parsing, Tigrinya, Oromo 1. Introduction In this paper, we experiment with using parser combinators (Hutton and Meijer, 1988; Frost and Launchbury, 1989) for the rapid development of practical morphological parsers, as an alternative or supplement to the typical finitestate transducers (Karttunen and Beesley, 1992; Karttunen, 1993). This paradigm offered some practical advantages over a finite-state system, allowing parsers to be written very rapidly in a familiar programming language, although at a cost of runtime efficiency. We present morphological parsers for two Afroasiatic languages, the Tigrinya language of Eritrea and Ethiopia (§4.1.), and the Oromo language of Ethiopia and Kenya (§4.2.).1 These parsers were designed during the LoReHLT"
L18-1611,E09-2008,0,0.0426789,"stem, and roughly doubles Oromo performance with a ~.04 F1 point improvement (~.07 when weighted for occurrence). 6. Future research While this paper has presented parser combinators as if they were in opposition to finite-state methods, the two paradigms are compatible; the ability of parser combinators to incorporate arbitrary functions into their parsing paradigm means that there are no conceptual reasons why some parts of the grammar could not be parsed in a finitestate manner and others in a recursive-descent manner. We are therefore looking into the possibility of integrating Foma FSTs (Hulden, 2009) as parser functions, and/or compiling “safe” subgraphs of the grammar into finite-state systems, to take advantage of the linear time execution where it is possible. The other benefit of finite-state parsers is that they can be run “backwards” (that is, generating rather than parsing). Incorporating this ability into a parser-combinator framework would be valuable both for pure parser-combinator systems and for the hybrid systems proposed above. The small parser combinator library released with these parsers already supports this to a limited degree: as seen in the examples in §2., the syntax"
L18-1611,littell-etal-2014-morphological,1,0.846722,"ine of the grammar. specialized parsers for root-and-pattern morphology and reduplication, and combinators that allow parsing either from the left (for prefixes) and the right (for suffixes). Familiar programming syntax and environment The programming syntax and execution environment is familiar Python. Boilerplate and repetitive code (e.g., a class of morphemes all of which have a complex environmental restriction or cause a particular morphophonological change) can be automated within the code itself; it is unnecessary to have a separate transpilation or pre-processing step, as was done in (Littell et al., 2014), to enable a new command or syntactic sugar. Even complex functions entirely outside of the parsing paradigm (e.g., orthographic conversion and normalization, dictionary lookup, etc.) can be wrapped up as a parser object and integrated into the morphological grammar. 2.4. Disadvantages of parser combinators Multi-output parsers It is straightforward to associate a parser with multiple different kinds of outputs (Hutton, 1992) letting us simultaneously write parsers that target different representations for different NLP tasks. Intuitive representation of morphological phenomena Some morpholog"
L18-1611,P17-1178,0,0.0279448,"detection and linking (EDL) for LoReHLT17 was concerned with the recognition of named entities (a subset of proper nouns) in text, their categorization as one of four entity types (person, organization, location, geopolitical entity), and their linking to an external knowledge-base of entities (compiled from several existing databases). The primary metric for EDL in LoReHLT17 was typed_mention_ceaf_plus, an F1 measure of detecting the entity and getting both the category and the link correct. We used word-to-word translation with bilingual lexicons for linking entities to the knowledge-base (Pan et al., 2017). Adding lemmatization improved translation of the entities and resulted in F1 point gain for both Tigrinya and Oromo, as seen in Table 1. 5.2. Situation Frame detection Situation frames (SFs) are a structured representation of events intended to “enable information from many different data streams to be aggregated into a comprehensive, actionable understanding of the basic facts needed to mount a response to an emerging situation” (Strassel et al., 2017). Situation frame detection involves detecting eight humanitarian requirements (e.g. water, food, medicine, evacuation) and three background"
L18-1611,L16-1521,0,0.0229746,"ical morphological parsers, as an alternative or supplement to the typical finitestate transducers (Karttunen and Beesley, 1992; Karttunen, 1993). This paradigm offered some practical advantages over a finite-state system, allowing parsers to be written very rapidly in a familiar programming language, although at a cost of runtime efficiency. We present morphological parsers for two Afroasiatic languages, the Tigrinya language of Eritrea and Ethiopia (§4.1.), and the Oromo language of Ethiopia and Kenya (§4.2.).1 These parsers were designed during the LoReHLT17 “surprise-language” evaluation (Strassel and Tracey, 2016) (§3.) to support machine translation, entity detection and linking, and humanitarian need detection (Strassel et al., 2017). These parsers were operable within about 36 hours of learning the identity of the languages, although they underwent further development during the next two weeks of evaluation. 2. Parser combinators 2.1. Introduction The “parser combinator” paradigm (Burge, 1975; Wadler, 1985; Hutton and Meijer, 1988; Frost and Launchbury, 1989) is a kind of declarative programming that simultaneously defines the grammar being parsed and the executable code that parses it. This paradig"
levin-etal-2014-resources,khokhlova-zakharov-2010-studying,0,\N,Missing
levin-etal-2014-resources,D10-1004,0,\N,Missing
levin-etal-2014-resources,ivanova-etal-2008-evaluating,0,\N,Missing
levin-etal-2014-resources,P14-1024,1,\N,Missing
levin-etal-2014-resources,macwhinney-fromm-2014-two,1,\N,Missing
levin-etal-2014-resources,W13-0906,1,\N,Missing
levin-etal-2014-resources,feely-etal-2014-cmu,1,\N,Missing
lin-mitamura-2004-keyword,P99-1028,0,\N,Missing
lin-mitamura-2004-keyword,O99-4002,0,\N,Missing
liu-etal-2014-supervised,N10-1138,0,\N,Missing
liu-etal-2014-supervised,D11-1116,1,\N,Missing
liu-etal-2014-supervised,W09-3208,0,\N,Missing
liu-etal-2014-supervised,W99-0201,0,\N,Missing
liu-etal-2014-supervised,D12-1045,0,\N,Missing
liu-etal-2014-supervised,H05-1004,0,\N,Missing
liu-etal-2014-supervised,W11-1902,0,\N,Missing
liu-etal-2014-supervised,N04-3012,0,\N,Missing
liu-etal-2014-supervised,P02-1014,0,\N,Missing
liu-etal-2014-supervised,D10-1033,0,\N,Missing
liu-etal-2014-supervised,W12-4501,0,\N,Missing
liu-etal-2014-supervised,P10-1143,0,\N,Missing
liu-etal-2014-supervised,W13-1203,1,\N,Missing
liu-etal-2014-supervised,P13-2083,1,\N,Missing
liu-etal-2014-supervised,W09-4303,0,\N,Missing
liu-etal-2014-supervised,W06-0901,0,\N,Missing
liu-etal-2014-supervised,M93-1007,0,\N,Missing
liu-etal-2014-supervised,P05-1045,0,\N,Missing
N04-4016,P01-1012,0,\N,Missing
nyberg-etal-2002-deriving,nyberg-mitamura-2000-kantoo,1,\N,Missing
nyberg-etal-2002-deriving,C92-3168,1,\N,Missing
nyberg-etal-2002-deriving,1991.mtsummit-papers.9,1,\N,Missing
nyberg-mitamura-2000-kantoo,C92-3168,1,\N,Missing
nyberg-mitamura-2000-kantoo,1991.mtsummit-papers.9,1,\N,Missing
P06-1054,W00-0730,0,0.0330866,"g., parentheses). In Chinese, such pairs are more frequent (quotes, single quotes, and book-name marks). During parsing, we note how many opening puncClassification is the key component of our parsing model. We conducted experiments with four different types of classifiers. 3.1 Classifiers Support Vector Machine: Support Vector Machine is a discriminative classification technique which solves the binary classification problem by finding a hyperplane in a high dimensional space that gives the maximum soft margin, based on the Structural Risk Minimization Principle. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. To train a multi-class classifier, we used the one-against-all scheme. Maximum-Entropy Classifier: In a Maximum-entropy model, the goal is to estimate a set of parameters that would maximize the entropy over distributions that satisfy certain constraints. These constraints will force the model to best account for the training data (Ratnaparkhi, 1999). Maximum-entropy models have been used for Chinese character-based parsing (Fung et al., 2004; Luo, 2003) and POS tagging (Ng and Low, 2004). In our experiments, we used Le’s Maxent toolkit (Zhang, 2004). This"
P06-1054,P03-1056,0,0.167959,"Missing"
P06-1054,W03-1025,0,0.0236967,"margin, based on the Structural Risk Minimization Principle. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. To train a multi-class classifier, we used the one-against-all scheme. Maximum-Entropy Classifier: In a Maximum-entropy model, the goal is to estimate a set of parameters that would maximize the entropy over distributions that satisfy certain constraints. These constraints will force the model to best account for the training data (Ratnaparkhi, 1999). Maximum-entropy models have been used for Chinese character-based parsing (Fung et al., 2004; Luo, 2003) and POS tagging (Ng and Low, 2004). In our experiments, we used Le’s Maxent toolkit (Zhang, 2004). This implementation uses the Limited-Memory Variable Metric method for parameter estimation. We trained all our models using 300 iterations with no event cut-off, and a Gaussian prior smoothing value of 2. Maxent classifiers output not only a single class label, but 427 1 2 3 4 5 6 7 8 9 10 11 A Boolean feature indicates if a closing punctuation is expected or not. A Boolean value indicates if the queue is empty or not. A Boolean feature indicates whether there is a comma separating S(1) and S(2"
P06-1054,W04-3236,0,0.0264147,"ral Risk Minimization Principle. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. To train a multi-class classifier, we used the one-against-all scheme. Maximum-Entropy Classifier: In a Maximum-entropy model, the goal is to estimate a set of parameters that would maximize the entropy over distributions that satisfy certain constraints. These constraints will force the model to best account for the training data (Ratnaparkhi, 1999). Maximum-entropy models have been used for Chinese character-based parsing (Fung et al., 2004; Luo, 2003) and POS tagging (Ng and Low, 2004). In our experiments, we used Le’s Maxent toolkit (Zhang, 2004). This implementation uses the Limited-Memory Variable Metric method for parameter estimation. We trained all our models using 300 iterations with no event cut-off, and a Gaussian prior smoothing value of 2. Maxent classifiers output not only a single class label, but 427 1 2 3 4 5 6 7 8 9 10 11 A Boolean feature indicates if a closing punctuation is expected or not. A Boolean value indicates if the queue is empty or not. A Boolean feature indicates whether there is a comma separating S(1) and S(2) or not. Last action given by the"
P06-1054,W00-1201,0,0.526338,"Missing"
P06-1054,C04-1010,0,0.0568874,"Missing"
P06-1054,W05-1513,1,0.807595,"itute School of Computer Science Carnegie Mellon University {mengqiu,sagae,teruko}@cs.cmu.edu Abstract accuracy just below the state-of-the-art in syntactic analysis of English, but running in linear time (Sagae and Lavie, 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). Encouraging results have also been shown recently by Cheng et al. (2004; 2005) in applying deterministic models to Chinese dependency parsing. We present a novel classifier-based deterministic parser for Chinese constituency parsing. In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie, 2005), the parsing task is transformed into a succession of classification tasks. The parser makes one pass through the input sentence. At each parse state, it consults a classifier to make shift/reduce decisions. The parser then commits to a decision and enters the next parse state. Shift/reduce decisions are made deterministically based on the local context of each parse state, and no backtracking is involved. This process can be viewed as a greedy search where only one path in the whole search space is considered. Our parser produces both dependency and constituent structures, but in this paper"
P06-1054,W03-1706,0,0.241692,"Missing"
P06-1054,C02-1126,0,0.262806,"Missing"
P06-1054,N04-1032,0,0.0468719,"Missing"
P06-1054,I05-1007,0,0.0301619,"Missing"
P06-1054,W03-3023,0,0.0789132,"Missing"
P06-1054,W99-0623,0,0.0129553,"els; on the other hand, it also indicates that our deterministic model is less resilient to POS errors. Further detailed analysis is called for, to study the extent to which POS tagging errors affects the deterministic parsing model. Table 5: Comparison of parsing speed opens up lots of possibilities for continuous improvements, both in terms of accuracy and efficiency. For example, in this paper we experimented with one method of simple voting. An alternative way of doing simple voting is to let the parsers vote on membership of constituents after each parser has produced its own parse tree (Henderson and Brill, 1999), instead of voting at each step during parsing. Our initial attempt to increase the accuracy of the DTree model by applying boosting techniques did not yield satisfactory results. In our experiment, we implemented the AdaBoost.M1 (Freund and Schapire, 1996) algorithm using resampling to vary the training set distribution. Results showed AdaBoost suffered severe overfitting problems and hurts accuracy greatly, even with a small number of samples. One possible reason for this is that our sample space is very unbalanced across the different classes. A few classes have lots of training examples w"
P06-1054,J03-4003,0,\N,Missing
P07-1099,P03-1003,0,0.317782,"Missing"
P07-1099,N07-1066,1,0.842291,"Missing"
P07-1099,N03-1022,0,0.0850901,"Missing"
P07-1099,P06-1054,1,0.825811,"e same way to analyze Wikipedia documents. The idf score was calculated using word statistics from Japanese Yomiuri newspaper corpus and the NTCIR Chinese corpus. Google: The same algorithm was applied to analyze Japanese and Chinese snippets returned from Google. But we restricted the language to Chinese or Japanese so that Google returned only Chinese or Japanese documents. To calculate the distance between an answer candidate and question keywords, segmentation was done with linguistic tools. For Japanese, Chasen4 was used. For Chinese segmentation, a maximum-entropy based parser was used (Wang et al., 2006). 3) Manual Filtering Other than the features mentioned above, we manually created many rules for numeric and temporal questions to filter out invalid answers. For example, when the question is looking for a year as an answer, an answer candidate which contains only the month receives a score of -1. Otherwise, the score is 0. 4.2 Answer Similarity Features The same features used for English were applied to calculate the similarity of Chinese/Japanese answer candidates. To identify synonyms, Wikipedia were used for both Chinese and Japanese. EIJIRO dictionary was used to obtain Japanese synonym"
P07-1099,buscaldi-rosso-2006-mining,0,\N,Missing
P10-2021,W09-2107,0,0.0882658,"Missing"
R13-1006,W11-1902,0,0.0823116,"Missing"
R13-1006,J13-4004,0,0.042183,"Missing"
R13-1006,I13-2002,1,0.643072,"09)) along with super-sense taggers Ciaramita and Altun (2006), to build a system combination that can hopefully do a better job than the baseline, at least on our intrinsic test sets. Compound Annotator identifies the phrasal verbs and the compound nouns in the text and adds additional annotation to words of a compound. In-text Question Answering Annotator assigns the questions to the related named entities, and ranks them. The questions are generated using Heilman’s question generator tool (Heilman and Smith, 2010). For more details on the use of UIMA and the server architecture, please see Azab et al. (2013). 5 Lexical simplification : Text simplification can be defined as any process that reduces the syntactic or lexical complexity of a text while attempting to preserve its meaning and information content. The aim of text simplification is to make text easier to comprehend for a human user, or process by a program (Siddharthan, 2004). Text simplification has been studied for both human text readers and programs that process text. We are specifically concerned with students who try to acquire English as a second language (Petersen, 2007). Approaches for this target audience use simplification tec"
R13-1006,W06-1670,0,0.0181843,"d documents (iii) handling user-interactions, and (iv) sending queries to the server. The presentation layer is designed to be light and fast, with all the heavy processing to be done on the server side. 1 http://nlp.stanford.edu/software/ corenlp.shtml 45 Word Sense Annotator currently assigns the most frequent WordNet senses to content words by filtering the senses by just using the POS tag. able to significantly exceed the most-frequent sense heuristic. Our current plan is to incorporate multiple word-sense disambiguators (e.g., Pedersen and Kolhatkar (2009)) along with super-sense taggers Ciaramita and Altun (2006), to build a system combination that can hopefully do a better job than the baseline, at least on our intrinsic test sets. Compound Annotator identifies the phrasal verbs and the compound nouns in the text and adds additional annotation to words of a compound. In-text Question Answering Annotator assigns the questions to the related named entities, and ranks them. The questions are generated using Heilman’s question generator tool (Heilman and Smith, 2010). For more details on the use of UIMA and the server architecture, please see Azab et al. (2013). 5 Lexical simplification : Text simplifica"
R13-1006,de-marneffe-etal-2006-generating,0,0.0220657,"Missing"
R13-1006,W12-2015,0,0.017691,"adability levels. REAP chooses documents that contain cer41 Proceedings of Recent Advances in Natural Language Processing, pages 41–48, Hissar, Bulgaria, 7-13 September 2013. presents the word meaning as the default response. tain target vocabulary words that a student needs to learn. It also presents the documents within a web browser-based application along with a dictionary to provide word meanings and a set of automatically generated set of closed questions as an exercise. Recently, Eom et al. (2012) presented a system that incorporates word sense disambiguation for vocabulary assistance. Maamouri et al. (2012) presents, ARET (Arabic Reading Enhancement Tool) that aids the readers of Arabic as a second language. It provides the user with the morphological analyses, the meanings of the words and a text-to-speech module to pronounce the word. ARET also has an assessment tool that asks the user several kinds of questions to evaluate reading comprehension. Our system currently targets English and offers a wider set of functionalities to users, in addition to a software architecture which can be extended very easily with more annotation components complying with UIMA interfaces. However, our system archi"
R13-1006,W12-2038,0,0.18689,"), aimed at selecting individualized practice reading documents from the web using lexical, syntactic and readability levels. REAP chooses documents that contain cer41 Proceedings of Recent Advances in Natural Language Processing, pages 41–48, Hissar, Bulgaria, 7-13 September 2013. presents the word meaning as the default response. tain target vocabulary words that a student needs to learn. It also presents the documents within a web browser-based application along with a dictionary to provide word meanings and a set of automatically generated set of closed questions as an exercise. Recently, Eom et al. (2012) presented a system that incorporates word sense disambiguation for vocabulary assistance. Maamouri et al. (2012) presents, ARET (Arabic Reading Enhancement Tool) that aids the readers of Arabic as a second language. It provides the user with the morphological analyses, the meanings of the words and a text-to-speech module to pronounce the word. ARET also has an assessment tool that asks the user several kinds of questions to evaluate reading comprehension. Our system currently targets English and offers a wider set of functionalities to users, in addition to a software architecture which can"
R13-1006,A97-1020,0,0.82085,"rmation Management Architecture) based server (Ferrucci and Lally, 2004). These annotated documents are then accessed via browser-based clients which essentially look like traditional e-book reading environments but with a much richer set of user accessible functionality. Thus our system can also be seen as a showcase application for demonstrating 2 Using NLP in Reading Aids Recently, Computer Assisted Language Learning (CALL) systems have started making use of advanced language technology to build intelligent systems to aid and assess reading comprehension. An early project, GLOSSER Project (Nerbonne et al., 1997) developed a system that aids readers of foreign language text, by providing access to a dictionary, exploiting morphological analysis and part-of-speech disambiguation. The FreeText Project (Hamel and Girard, 2000), developed a NLP-based CALL system for intermediate to advanced learners of French. The LISTEN project at CMU on the other hand, has aimed to tutor elementary school students in reading English text by using speech technology (Mostow and Aist, 2001). The REAP (Reader Specific Lexical Practice) project (Heilman et al., 2006), aimed at selecting individualized practice reading docume"
R13-1006,N09-5005,0,0.032081,"ser status and the opened documents, (ii) displaying the opened documents (iii) handling user-interactions, and (iv) sending queries to the server. The presentation layer is designed to be light and fast, with all the heavy processing to be done on the server side. 1 http://nlp.stanford.edu/software/ corenlp.shtml 45 Word Sense Annotator currently assigns the most frequent WordNet senses to content words by filtering the senses by just using the POS tag. able to significantly exceed the most-frequent sense heuristic. Our current plan is to incorporate multiple word-sense disambiguators (e.g., Pedersen and Kolhatkar (2009)) along with super-sense taggers Ciaramita and Altun (2006), to build a system combination that can hopefully do a better job than the baseline, at least on our intrinsic test sets. Compound Annotator identifies the phrasal verbs and the compound nouns in the text and adds additional annotation to words of a compound. In-text Question Answering Annotator assigns the questions to the related named entities, and ranks them. The questions are generated using Heilman’s question generator tool (Heilman and Smith, 2010). For more details on the use of UIMA and the server architecture, please see Aza"
R13-1006,radev-etal-2004-mead,0,0.0460008,"Missing"
R13-1006,D10-1048,0,0.0454088,"Missing"
rambow-etal-2006-parallel,W04-2709,1,\N,Missing
rambow-etal-2006-parallel,passonneau-etal-2006-inter,1,\N,Missing
rambow-etal-2006-parallel,J93-2004,0,\N,Missing
rambow-etal-2006-parallel,W00-0204,0,\N,Missing
rambow-etal-2006-parallel,W02-1503,0,\N,Missing
rambow-etal-2006-parallel,rambow-etal-2002-dependency,1,\N,Missing
reeder-etal-2004-interlingual,W04-2709,1,\N,Missing
reeder-etal-2004-interlingual,W03-1601,0,\N,Missing
reeder-etal-2004-interlingual,W03-1604,0,\N,Missing
reeder-etal-2004-interlingual,P98-1013,0,\N,Missing
reeder-etal-2004-interlingual,C98-1013,0,\N,Missing
reeder-etal-2004-interlingual,1991.mtsummit-papers.9,1,\N,Missing
reeder-etal-2004-interlingual,J96-2004,0,\N,Missing
reeder-etal-2004-interlingual,A97-1011,0,\N,Missing
shima-mitamura-2012-diversifiable,E06-1052,0,\N,Missing
shima-mitamura-2012-diversifiable,J90-1003,0,\N,Missing
shima-mitamura-2012-diversifiable,I08-1047,0,\N,Missing
shima-mitamura-2012-diversifiable,J10-3003,0,\N,Missing
shima-mitamura-2012-diversifiable,P11-2046,0,\N,Missing
shima-mitamura-2012-diversifiable,P07-1059,0,\N,Missing
shima-mitamura-2012-diversifiable,N06-1057,0,\N,Missing
shima-mitamura-2012-diversifiable,W06-1610,0,\N,Missing
shima-mitamura-2012-diversifiable,P98-1013,0,\N,Missing
shima-mitamura-2012-diversifiable,C98-1013,0,\N,Missing
shima-mitamura-2012-diversifiable,P09-1034,0,\N,Missing
shima-mitamura-2012-diversifiable,N06-1058,0,\N,Missing
shima-mitamura-2012-diversifiable,W02-1028,0,\N,Missing
shima-mitamura-2012-diversifiable,P06-1015,0,\N,Missing
shima-mitamura-2012-diversifiable,D11-1010,0,\N,Missing
shima-mitamura-2012-diversifiable,U08-1013,0,\N,Missing
shima-mitamura-2012-diversifiable,kipper-etal-2006-extending,0,\N,Missing
shima-mitamura-2012-diversifiable,W04-3205,0,\N,Missing
shima-mitamura-2012-diversifiable,W04-3206,0,\N,Missing
shima-mitamura-2012-diversifiable,P09-1045,0,\N,Missing
W04-2709,P98-1013,0,0.473703,"dependency parser (details in section 6) and is a useful starting point for semantic annotation at IL1, since it allows annotators to see how textual units relate syntactically when making semantic judgments. 4.1.3 IL2 IL2 is intended to be an interlingua, a representation of meaning that is reasonably independent of language. IL2 is intended to capture similarities in meaning across languages and across different lexical/syntactic realizations within a language. For example, IL2 is expected to normalize over conversives (e.g. X bought a book from Y vs. Y sold a book to X) (as does FrameNet (Baker et al 1998)) and non-literal language usage (e.g. X started its business vs. X opened its doors to customers). The exact definition of IL2 will be the major research contribution of this project. 4.2 The Omega Ontology In progressing from IL0 to IL1, annotators have to select semantic terms (concepts) to represent the nouns, verbs, adjectives, and adverbs present in each sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998)"
W04-2709,J96-2004,0,0.0756983,"Missing"
W04-2709,2003.mtsummit-eval.3,1,0.726095,"al content, modality, speech acts, etc. At the same time, while incorporating these items, vagueness and redundancy must be eliminated from the annotation language. Many inter-event relations would need to be captured such as entity reference, time reference, place reference, causal relationships, associative relationships, etc. Finally, to incorporate these, crosssentence phenomena remain a challenge. From an MT perspective, issues include evaluating the consistency in the use of an annotation language given that any source text can result in multiple, different, legitimate translations (see Farwell and Helmreich, 2003) for discussion of evaluation in this light. Along these lines, there is the problem of annotating texts for translation without including in the annotations inferences from the source text. 9 Conclusions This is a radically different annotation project from those that have focused on morphology, syntax or even certain types of semantic content (e.g., for word sense disambiguation competitions). It is most similar to PropBank (Kingsbury et al 2002) and FrameNet (Baker et al 1998). However, it is novel in its emphasis on: (1) a more abstract level of mark-up (interpretation); (2) the assignment"
W04-2709,P03-1001,1,0.790541,"sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998), NMSU’s Mikrokosmos (Mahesh and Nirenburg, 1995), ISI's Upper Model (Bateman et al., 1989) and ISI's SENSUS (Knight and Luk, 1994). After the uppermost region of Omega was created by hand, these various resources’ contents were incorporated and, to some extent, reconciled. After that, several million instances of people, locations, and other facts were added (Fleischman et al., 2003). The ontology, which has been used in several projects in recent years (Hovy et al., 2001), can be browsed using the DINO browser at http://blombos.isi.edu:8000/dino; this browser forms a part of the annotation environment. Omega remains under continued development and extension. 4.1.2 IL1 IL1 is an intermediate semantic representation. It associates semantic concepts with lexical units like nouns, adjectives, adverbs and verbs (details of the ontology in section 4.2). It also replaces the syntactic relations in IL0, like subject and object, with thematic roles, like agent, theme and goal (de"
W04-2709,C18-2019,0,0.0664532,"Missing"
W04-2709,A97-1011,0,0.0452745,"first present the annotation process and tools used with it as well as the annotation manuals. Finally, setup issues relating to negotiating multi-site annotations are discussed. 6.1 Annotation process The annotation process was identical for each text. For the initial testing period, only English texts were annotated, and the process described here is for English text. The process for non-English texts will be, mutatis mutandis, the same. Each sentence of the text is parsed into a dependency tree structure. For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. For the initial testing period, annotators were not permitted to alter these structures. Already at this stage, some of the lexical items are replaced by features (e.g., tense), morphological forms are replaced by features on the citation form, and certain constructions are regularized (e.g., passive) and empty arguments inserted. It is this dependency structure that is loaded into the annotation tool and which each annotator then marks up. The annotator was instructed to annotate all nouns, verbs, adjectives, and adverbs. This involves annotating e"
W04-2709,1994.amta-1.25,0,0.0933693,"Missing"
W04-2709,C98-1013,0,\N,Missing
W06-1905,W04-2606,0,0.0555996,"Missing"
W06-1905,W01-1413,0,0.0648027,"Missing"
W06-1905,J90-2002,0,\N,Missing
W09-3012,J04-3002,0,0.199612,"Missing"
W09-3012,krestel-etal-2008-minding,0,0.0374878,"Missing"
W09-3012,C08-1101,0,0.0165575,"is is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief concerns propositions about the future, such as GM wil"
W09-3012,W05-0308,0,0.0144516,"d we are only interested in the writer’s beliefs. This is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief"
W11-2405,P05-1074,0,0.33339,"valuate paraphrase patterns during pattern discovery, ideally we should use an evaluation metric that strongly predicts performance on the extrinsic task (e.g. fluency and adequacy scores in MT, mean average precision in IR) where the paraphrase patterns are used. Many existing approaches use a paraphrase evaluation methodology where human assessors judge each paraphrase pair as to whether they have the same meaning. Over a set of paraphrase rules for one source term, Expected Precision (EP) is calculated by taking the mean of precision, or the ratio of positive labels annotated by assessors (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010; Metzler et al., 2011). The weakness of this approach is that EP is an intrinsic measure that does not necessarily predict how well a paraphrase-embedded system will perform in practice. For example, a set of paraphrase pairs 〈“killed”, “shot and killed”〉, 〈“killed”, “reported killed”〉 … 〈“killed”, “killed in”〉 may receive a perfect score of 1.0 in EP; however, these patterns do not provide lexical diversity (e.g. 〈“killed”, “assassinated”〉) and therefore may not perform well in an application where lexical diversity is important. The goal of this"
W11-2405,P11-2096,0,0.0947645,"valuation metric that strongly predicts performance on the extrinsic task (e.g. fluency and adequacy scores in MT, mean average precision in IR) where the paraphrase patterns are used. Many existing approaches use a paraphrase evaluation methodology where human assessors judge each paraphrase pair as to whether they have the same meaning. Over a set of paraphrase rules for one source term, Expected Precision (EP) is calculated by taking the mean of precision, or the ratio of positive labels annotated by assessors (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010; Metzler et al., 2011). The weakness of this approach is that EP is an intrinsic measure that does not necessarily predict how well a paraphrase-embedded system will perform in practice. For example, a set of paraphrase pairs 〈“killed”, “shot and killed”〉, 〈“killed”, “reported killed”〉 … 〈“killed”, “killed in”〉 may receive a perfect score of 1.0 in EP; however, these patterns do not provide lexical diversity (e.g. 〈“killed”, “assassinated”〉) and therefore may not perform well in an application where lexical diversity is important. The goal of this paper is to provide empirical evidence to support the assumption tha"
W11-2405,D07-1017,0,0.0444826,"Missing"
W11-2405,P08-1077,0,0.0265434,"ion research (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Tratz and Hovy, 2009; Przybocki et al., 2009). By intrinsic score, we mean a theory-based direct assessment result on the paraphrase patterns. By extrinsic score, we mean to measure how much the paraphrase recognition component helps the entire system to achieve a task. The correlation score is 1 if there is a perfect positive correlation, 0 if there is no correlation and -1 if there is a perfect negative correlation. Using a task performance score to evaluate a paraphrase generation algorithm has been studied previously (Bhagat and Ravichandran, 2008; Szpektor and Dagan, 2007; Szpektor and Dagan, 2008). A common issue in extrinsic evaluations is that it is hard to separate out errors, or contributions from other possibly complex modules. This paper presents an approach which can predict task performance in more simple experimental settings. 3.1 Annotated Paraphrase Resource We used the paraphrase pattern dataset “paraphrase-eval” (Metzler et al., 2011; Metzler and Hovy, 2011) which contains paraphrase patterns acquired by multiple algorithms: 1) PD (Pasca and Dienes, 2005), which is based on the left and right n-gram contexts of the sourc"
W11-2405,D08-1021,0,0.0885458,"pattern discovery, ideally we should use an evaluation metric that strongly predicts performance on the extrinsic task (e.g. fluency and adequacy scores in MT, mean average precision in IR) where the paraphrase patterns are used. Many existing approaches use a paraphrase evaluation methodology where human assessors judge each paraphrase pair as to whether they have the same meaning. Over a set of paraphrase rules for one source term, Expected Precision (EP) is calculated by taking the mean of precision, or the ratio of positive labels annotated by assessors (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010; Metzler et al., 2011). The weakness of this approach is that EP is an intrinsic measure that does not necessarily predict how well a paraphrase-embedded system will perform in practice. For example, a set of paraphrase pairs 〈“killed”, “shot and killed”〉, 〈“killed”, “reported killed”〉 … 〈“killed”, “killed in”〉 may receive a perfect score of 1.0 in EP; however, these patterns do not provide lexical diversity (e.g. 〈“killed”, “assassinated”〉) and therefore may not perform well in an application where lexical diversity is important. The goal of this paper is to provide e"
W11-2405,W07-0718,0,0.0346067,"e are interested in learning paraphrases that are out-of-vocabulary or domain-specific, D could consult a dictionary, and return a high score if the lexical entry could not be found. The DIMPLE framework is implemented in the following way4. Let Q be the ratio of positive labels 4 Implementation used for this experiment is available at http://code.google.com/p/dimple/ 36 Experiment We use the Pearson product-moment correlation coefficient to measure correlation between two vectors consisting of intrinsic and extrinsic scores on paraphrase patterns, following previous meta-evaluation research (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Tratz and Hovy, 2009; Przybocki et al., 2009). By intrinsic score, we mean a theory-based direct assessment result on the paraphrase patterns. By extrinsic score, we mean to measure how much the paraphrase recognition component helps the entire system to achieve a task. The correlation score is 1 if there is a perfect positive correlation, 0 if there is no correlation and -1 if there is a perfect negative correlation. Using a task performance score to evaluate a paraphrase generation algorithm has been studied previously (Bhagat and Ravichandran, 2008; Szpektor a"
W11-2405,W08-0309,0,0.0337839,"f pattern quality Q and lexical diversity D: gaini = Qi ⋅ Di . DIMPLE at rank k is a normalized CGk which is defined as: k CG k ∑i =1{2^ (Qi ⋅ Di ) − 1} DIMPLEk = = Z Z where Z is a normalization factor such that the perfect CG score is given. Since Q takes a real value between 0 and 1, and D takes an integer between 1 and 3, k Z = ∑i =1{2^3 − 1}. Being able to design Q and D independently is one of characteristics in DIMPLE. In theory, Q can be any quality measure on paraphrase patterns, such as the instance-based evaluation score (Szpektor et al., 2007), or alignment-based evaluation score (Callison-Burch et al., 2008). Similarly, D can be implemented depending on the domain task; for example, if we are interested in learning paraphrases that are out-of-vocabulary or domain-specific, D could consult a dictionary, and return a high score if the lexical entry could not be found. The DIMPLE framework is implemented in the following way4. Let Q be the ratio of positive labels 4 Implementation used for this experiment is available at http://code.google.com/p/dimple/ 36 Experiment We use the Pearson product-moment correlation coefficient to measure correlation between two vectors consisting of intrinsic and extri"
W11-2405,I05-5002,0,0.104901,"Missing"
W11-2405,N06-1058,0,0.0328505,"significant level (p-value<0.01). 1 Introduction We propose a diversity-aware paraphrase evaluation metric called DIMPLE1, which boosts the scores of lexically diverse paraphrase pairs. Paraphrase pairs or patterns are useful in various NLP related research domains, since there is a common need to automatically identify meaning equivalence between two or more texts. Consider a paraphrase pair resource that links “killed” to “assassinated” (in the rest of this paper we denote such a rule as 〈“killed”2, “assassinated”3〉). In automatic evaluation for Machine Translation (MT) (Zhou et al., 2006; Kauchak and Barzilay, 2006; Padó et al., 2009), this rule may enable a metric to identify phrase-level semantic similarity between a system response containing “killed”, and a reference translation containing “assassinated”. Similarly in query expansion for information retrieval (IR) (Riezler et al., 2007), this rule may enable a system to expand the query term “killed” with the paraphrase “assassinated”, in order to match a potentially relevant document containing the expanded term. To evaluate paraphrase patterns during pattern discovery, ideally we should use an evaluation metric that strongly predicts performance o"
W11-2405,N10-1017,0,0.0443433,"eally we should use an evaluation metric that strongly predicts performance on the extrinsic task (e.g. fluency and adequacy scores in MT, mean average precision in IR) where the paraphrase patterns are used. Many existing approaches use a paraphrase evaluation methodology where human assessors judge each paraphrase pair as to whether they have the same meaning. Over a set of paraphrase rules for one source term, Expected Precision (EP) is calculated by taking the mean of precision, or the ratio of positive labels annotated by assessors (Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010; Metzler et al., 2011). The weakness of this approach is that EP is an intrinsic measure that does not necessarily predict how well a paraphrase-embedded system will perform in practice. For example, a set of paraphrase pairs 〈“killed”, “shot and killed”〉, 〈“killed”, “reported killed”〉 … 〈“killed”, “killed in”〉 may receive a perfect score of 1.0 in EP; however, these patterns do not provide lexical diversity (e.g. 〈“killed”, “assassinated”〉) and therefore may not perform well in an application where lexical diversity is important. The goal of this paper is to provide empirical evidence to sup"
W11-2405,P09-1034,0,0.0169215,"0.01). 1 Introduction We propose a diversity-aware paraphrase evaluation metric called DIMPLE1, which boosts the scores of lexically diverse paraphrase pairs. Paraphrase pairs or patterns are useful in various NLP related research domains, since there is a common need to automatically identify meaning equivalence between two or more texts. Consider a paraphrase pair resource that links “killed” to “assassinated” (in the rest of this paper we denote such a rule as 〈“killed”2, “assassinated”3〉). In automatic evaluation for Machine Translation (MT) (Zhou et al., 2006; Kauchak and Barzilay, 2006; Padó et al., 2009), this rule may enable a metric to identify phrase-level semantic similarity between a system response containing “killed”, and a reference translation containing “assassinated”. Similarly in query expansion for information retrieval (IR) (Riezler et al., 2007), this rule may enable a system to expand the query term “killed” with the paraphrase “assassinated”, in order to match a potentially relevant document containing the expanded term. To evaluate paraphrase patterns during pattern discovery, ideally we should use an evaluation metric that strongly predicts performance on the extrinsic task"
W11-2405,I05-1011,0,0.0157686,"paraphrase generation algorithm has been studied previously (Bhagat and Ravichandran, 2008; Szpektor and Dagan, 2007; Szpektor and Dagan, 2008). A common issue in extrinsic evaluations is that it is hard to separate out errors, or contributions from other possibly complex modules. This paper presents an approach which can predict task performance in more simple experimental settings. 3.1 Annotated Paraphrase Resource We used the paraphrase pattern dataset “paraphrase-eval” (Metzler et al., 2011; Metzler and Hovy, 2011) which contains paraphrase patterns acquired by multiple algorithms: 1) PD (Pasca and Dienes, 2005), which is based on the left and right n-gram contexts of the source term, with scoring based on overlap; 2) BR (Bhagat and Ravichandran, 2008), based on Noun Phrase chunks as contexts; 3) BCB (Bannard and Callison-Burch, 2005) and 4) BCB-S (Callison-Burch, 2008), which are based on monolingual phrase alignment from a bilingual corpus using a pivot. In the dataset, each paraphrase pair is assigned with an annotation as to whether a pair is a correct paraphrase or not by 2 or 3 human annotators. The source terms are 100 verbs extracted from newswire about terrorism and American football. We sel"
W11-2405,P07-1059,0,0.0128865,"ommon need to automatically identify meaning equivalence between two or more texts. Consider a paraphrase pair resource that links “killed” to “assassinated” (in the rest of this paper we denote such a rule as 〈“killed”2, “assassinated”3〉). In automatic evaluation for Machine Translation (MT) (Zhou et al., 2006; Kauchak and Barzilay, 2006; Padó et al., 2009), this rule may enable a metric to identify phrase-level semantic similarity between a system response containing “killed”, and a reference translation containing “assassinated”. Similarly in query expansion for information retrieval (IR) (Riezler et al., 2007), this rule may enable a system to expand the query term “killed” with the paraphrase “assassinated”, in order to match a potentially relevant document containing the expanded term. To evaluate paraphrase patterns during pattern discovery, ideally we should use an evaluation metric that strongly predicts performance on the extrinsic task (e.g. fluency and adequacy scores in MT, mean average precision in IR) where the paraphrase patterns are used. Many existing approaches use a paraphrase evaluation methodology where human assessors judge each paraphrase pair as to whether they have the same me"
W11-2405,C08-1107,0,0.015139,"et al., 2008; Tratz and Hovy, 2009; Przybocki et al., 2009). By intrinsic score, we mean a theory-based direct assessment result on the paraphrase patterns. By extrinsic score, we mean to measure how much the paraphrase recognition component helps the entire system to achieve a task. The correlation score is 1 if there is a perfect positive correlation, 0 if there is no correlation and -1 if there is a perfect negative correlation. Using a task performance score to evaluate a paraphrase generation algorithm has been studied previously (Bhagat and Ravichandran, 2008; Szpektor and Dagan, 2007; Szpektor and Dagan, 2008). A common issue in extrinsic evaluations is that it is hard to separate out errors, or contributions from other possibly complex modules. This paper presents an approach which can predict task performance in more simple experimental settings. 3.1 Annotated Paraphrase Resource We used the paraphrase pattern dataset “paraphrase-eval” (Metzler et al., 2011; Metzler and Hovy, 2011) which contains paraphrase patterns acquired by multiple algorithms: 1) PD (Pasca and Dienes, 2005), which is based on the left and right n-gram contexts of the source term, with scoring based on overlap; 2) BR (Bhagat"
W11-2405,W06-1610,0,0.0626997,"Missing"
W11-2405,P07-1058,0,\N,Missing
W11-2405,P08-1000,0,\N,Missing
W13-1203,bejan-harabagiu-2008-linguistic,0,0.549061,"M. Felisa Vedejo from UNED Madrid. Since nobody has complete knowledge, the author’s mental image of the entity or event in question might differ from the reader’s, and from the truth. Specifically, the properties the author assumes for the event or entity might not be the ones the reader assumes. This difference has deep consequences for the treatment of the semantic meaning of a text. In particular, it fundamentally affects how one must perform coreference among entities or events. As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al., 1997). Determining when two event mentions in text corefer is, however, an unsolved problem2. Past work in NLP has avoided some of the more complex problems by considering only certain types of coreference, or by simply ignoring the major problems. The results have been partial, or inconsistent, annotations. In this paper we describe our approach to the problem of coreference among events. In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary"
W13-1203,P10-1143,0,0.495771,"Missing"
W13-1203,W09-3208,0,0.346808,"Missing"
W13-1203,N07-1011,0,0.0222776,"o so-called Identity Criteria. Guarino (1999) outlines several ‘dimensions’ along which entities can remain identical or change under transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and"
W13-1203,P08-2012,0,0.0229614,"99) outlines several ‘dimensions’ along which entities can remain identical or change under transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan et al., 2007). Most simila"
W13-1203,D10-1033,0,0.123314,"Missing"
W13-1203,hasler-etal-2006-nps,0,0.0775551,"” are identical. In “she was elected President” / “she took office as President”, it is more difficult to decide. Does being elected automatically entail taking office? In some political systems it may, and in others it may not. When in doubt, we treat the case as only quasiidentical. Thus, comparing to examples from FullIdentity: Paraphrase, the following are only quasiidentical because of additional information: “she sold the book” / “she sold Peter the book”; “she sold Peter the book” / “Peter got [not bought] the book from her”. Quasi-identity has been considered in coreference before in (Hasler et al., 2006) but not as extensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities. When applied to events, the issue becomes more complex. 3 Two Problems 3.1 Domain and Reporting Events As described above, inconsistent reporting occurs when a DE stated in reported text contains significant differences from the author’s description of the same DE. To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation because they provide a context in which a DE is stated. We identify two principal types of Reporting"
W13-1203,W97-1311,0,0.756043,". Since nobody has complete knowledge, the author’s mental image of the entity or event in question might differ from the reader’s, and from the truth. Specifically, the properties the author assumes for the event or entity might not be the ones the reader assumes. This difference has deep consequences for the treatment of the semantic meaning of a text. In particular, it fundamentally affects how one must perform coreference among entities or events. As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al., 1997). Determining when two event mentions in text corefer is, however, an unsolved problem2. Past work in NLP has avoided some of the more complex problems by considering only certain types of coreference, or by simply ignoring the major problems. The results have been partial, or inconsistent, annotations. In this paper we describe our approach to the problem of coreference among events. In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary to create a model of event ide"
W13-1203,W04-0208,0,0.037375,"dentical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan et al., 2007). Most similar to our work is that of (Hasler et al., 2006). In that study, coreferential events and their arguments (also coreference between the arguments) were annotated for the terrorism/security domain, considering five event categories (attack, defend, injure, die, contact), and five"
W13-1203,N09-1065,0,0.0251365,"uarino (1999) outlines several ‘dimensions’ along which entities can remain identical or change under transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan"
W13-1203,W05-0311,0,0.0347599,"r but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan et al., 2007). Most similar to our work is that of (Hasler et al., 2006). In that study, coreferential events and their arguments (also coreference between the arguments) were annotated for the terrorism/security domain, consi"
W13-1203,poesio-artstein-2008-anaphoric,0,0.0612666,"Missing"
W13-1203,P10-1144,1,0.848137,"e took office as President”, it is more difficult to decide. Does being elected automatically entail taking office? In some political systems it may, and in others it may not. When in doubt, we treat the case as only quasiidentical. Thus, comparing to examples from FullIdentity: Paraphrase, the following are only quasiidentical because of additional information: “she sold the book” / “she sold Peter the book”; “she sold Peter the book” / “Peter got [not bought] the book from her”. Quasi-identity has been considered in coreference before in (Hasler et al., 2006) but not as extensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities. When applied to events, the issue becomes more complex. 3 Two Problems 3.1 Domain and Reporting Events As described above, inconsistent reporting occurs when a DE stated in reported text contains significant differences from the author’s description of the same DE. To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation because they provide a context in which a DE is stated. We identify two principal types of Reporting verbs: locutionary verbs “say”, “report”, “announce”, e"
W13-1203,taule-etal-2008-ancora,0,0.0458449,"Missing"
W14-2910,W10-4305,0,0.0133783,"ht produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference while dealing with twinless mentions. A potential problem of making a single metric handle twinless mentions is that the metric would not be i"
W14-2910,P08-1090,0,0.0401423,"tion extraction, question answering, textual entailment, and contradiction detection. A key challenge for event coreference resolution is that one can define several relations between two events, where some of them exhibit subtle deviation from perfect event identity. For clarification, we refer to perfect event identity as full (event) coreference in this paper. To address the subtlety in event identity, Hovy et al. (2013) focused on two types of partial event identity: subevent and membership. Subevent relations form a stereotypical sequence of events, or a script (Schank and Abelson, 1977; Chambers and Jurafsky, 2008). Membership relations represent instances of an event collection. We refer to both as partial (event) coreference in this paper. Figure 1 shows some examples of the subevent and Figure 1: Examples of subevent and membership relations. Solid and dashed arrows represent subevent and membership relations respectively, with the direction from a parent to its subevent or member. For example, we say that E4 is a subevent of E6. Solid lines without any arrow heads represent full coreference. In this paper, we address the problem of evalu68 Proceedings of the 2nd Workshop on EVENTS: Definition, Detec"
W14-2910,D11-1096,0,0.0177431,"ees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees (Collins and Duffy, 2001; Moschitti et al., 2008) or between dependency trees (Croce et al., 2011; Srivastava et al., 2013). This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting. ating the performance of a system that detects partial coreference in the context of event coreference resolution. This problem is important because, as with other tasks, a good evaluation method for partial coreference will facilitate future research on the task in a consistent and comparable manner. When one introduces a certain"
W14-2910,P13-1012,0,0.0128099,"erarchy that simplifies the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first d"
W14-2910,N10-1145,0,0.0191369,"unclear how to define a cluster. The latter is not readily applicable to the evaluation because it is unclear how to penalize incorrect directions of links. We discuss these aspects in Section 4.1 and Section 4.2. Tree Edit Distance (TED) is one of the traditional algorithms for measuring tree similarity. It has a long history of theoretical studies (Tai, 1979; Zhang and Shasha, 1989; Klein, 1998; Bille, 2005; Demaine et al., 2009; Pawlik and Augsten, 2011). It is also widely studied in many applications, including Natural Language Processing (NLP) tasks (Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013). However, TED has a disadvantage: we need to predefine appropriate costs for basic tree-edit operations. In addition, an implementation of TED for unordered trees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely"
W14-2910,W13-1203,1,0.802376,"determine whether two event mentions refer to the same event. This task is important since resolved event coreference is useful in various tasks such as topic detection and tracking, information extraction, question answering, textual entailment, and contradiction detection. A key challenge for event coreference resolution is that one can define several relations between two events, where some of them exhibit subtle deviation from perfect event identity. For clarification, we refer to perfect event identity as full (event) coreference in this paper. To address the subtlety in event identity, Hovy et al. (2013) focused on two types of partial event identity: subevent and membership. Subevent relations form a stereotypical sequence of events, or a script (Schank and Abelson, 1977; Chambers and Jurafsky, 2008). Membership relations represent instances of an event collection. We refer to both as partial (event) coreference in this paper. Figure 1 shows some examples of the subevent and Figure 1: Examples of subevent and membership relations. Solid and dashed arrows represent subevent and membership relations respectively, with the direction from a parent to its subevent or member. For example, we say t"
W14-2910,H05-1003,0,0.0723793,"Missing"
W14-2910,P13-1049,0,0.0112315,"s the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first describe assumptions for par"
W14-2910,P10-1143,0,0.0271402,"follows: • We introduce a conceptual tree hierarchy that simplifies the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric sho"
W14-2910,D12-1045,0,0.01697,"conceptual tree hierarchy that simplifies the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In th"
W14-2910,D08-1031,0,0.0183008,"t in the gold standard but do not in a system response, or vice versa. In reality, twinless mentions often happen since an end-to-end system might produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference w"
W14-2910,H05-1004,0,0.0455022,"ving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first describe assumptions for partial coreference evaluation, and introduce the notion of conceptual event hierarchy to address the challenge posed by one of the assumptions. We then enumerate the desiderata for a metric. 69 3.1 E9 i"
W14-2910,P09-2073,0,0.0272352,"ate partial coreference because it is unclear how to define a cluster. The latter is not readily applicable to the evaluation because it is unclear how to penalize incorrect directions of links. We discuss these aspects in Section 4.1 and Section 4.2. Tree Edit Distance (TED) is one of the traditional algorithms for measuring tree similarity. It has a long history of theoretical studies (Tai, 1979; Zhang and Shasha, 1989; Klein, 1998; Bille, 2005; Demaine et al., 2009; Pawlik and Augsten, 2011). It is also widely studied in many applications, including Natural Language Processing (NLP) tasks (Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013). However, TED has a disadvantage: we need to predefine appropriate costs for basic tree-edit operations. In addition, an implementation of TED for unordered trees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for un"
W14-2910,N13-1106,0,0.0253555,"Missing"
W14-2910,J08-2003,0,0.0187277,"n addition, an implementation of TED for unordered trees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees (Collins and Duffy, 2001; Moschitti et al., 2008) or between dependency trees (Croce et al., 2011; Srivastava et al., 2013). This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting. ating the performance of a system that detects partial coreference in the context of event coreference resolution. This problem is important because, as with other tasks, a good evaluation method for partial coreference will facilitate future research on the task in a consistent and c"
W14-2910,W11-1901,0,0.0507955,"Missing"
W14-2910,D09-1101,0,0.0212462,"or vice versa. In reality, twinless mentions often happen since an end-to-end system might produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference while dealing with twinless mentions. A potential pr"
W14-2910,D13-1144,1,0.839184,"x. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees (Collins and Duffy, 2001; Moschitti et al., 2008) or between dependency trees (Croce et al., 2011; Srivastava et al., 2013). This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting. ating the performance of a system that detects partial coreference in the context of event coreference resolution. This problem is important because, as with other tasks, a good evaluation method for partial coreference will facilitate future research on the task in a consistent and comparable manner. When one introduces a certain evaluation metric to such"
W14-2910,P09-1074,0,0.0130005,"omenon, let ei ⇔ ej denote full coreference between ei and ej . In Figure 1, we s have E6 ⇔ E7 and E7 → − E8. In this case, E8 s is also a subevent of E6, i.e., E6 → − E8. The rationale behind this assumption is that if a syss s tem identifies E6 → − E8 instead of E7 → − E8, then there is no reason to argue that the identified subevent relation is incorrect given that E6 ⇔ E7 s and E7 → − E8. The discussion here also applies to membership relations. Assumptions on Partial Coreference We make the following three assumptions to evaluate partial coreference. Twinless mentions: Twinless mentions (Stoyanov et al., 2009) are the mentions that exist in the gold standard but do not in a system response, or vice versa. In reality, twinless mentions often happen since an end-to-end system might produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such"
W14-2910,M95-1005,0,0.682808,"unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first describe assumptions for partial coreference evaluation, and introduce the notion of conceptual event hierarchy to address the challenge posed"
W14-2910,C10-1131,0,0.020493,"in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference while dealing with twinless mentions. A potential problem of making a single metric handle twinless mentions is that the metric would not be i"
W15-0807,P08-1090,0,0.0420243,"e analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas and enable researchers to produce end-to-end systems. In this paper, we discuss our recent eff"
W15-0807,P09-1068,0,0.0274299,"luating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas and enable researchers to produce end-to-end systems. In this paper, we discuss our recent effort in providing a proper eval"
W15-0807,W09-3208,0,0.0363768,"r, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote researc"
W15-0807,I13-1100,0,0.0157917,"m and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas and enable researche"
W15-0807,D12-1045,0,0.0354797,"valuation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas a"
W15-0807,P13-1008,0,0.299474,"ion is important for modern natural language processing tasks. In this paper, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotat"
W15-0807,D14-1198,0,0.0987897,"for modern natural language processing tasks. In this paper, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building rob"
W15-0809,doddington-etal-2004-automatic,1,0.296897,"nre NW DF NW DF Documents 77 74 101 99 Tokens 44,962 70,427 50,997 169,740 Table 1. Event Nugget Data Profile While the Light ERE and KBP Event Argument tasks rely on character offsets for annotation and scoring, the Event Nugget Tuple Scorer 2 (Liu, Mitamura & Hovy, 2015) requires tokenized data. Therefore, prior to annotation, all selected documents were automatically tokenized in the Penn English Treebank style. No manual correction was performed on the tokenization due to time constraints. 8 8.1 Corpus and Consistency Analysis Corpus Experience with event annotation for Light ERE and ACE (Doddington et al., 2004) and related tasks suggests that a major challenge for annotation consistency is poor recall – human annotators are not highly consistent in recognizing that a mention has occurred. To reduce the impact of this known issue for the Event Nugget task, two anno2 Event Nugget Tuple refers to the tuple made up of the nugget, event type/subtype, and realis. tators independently labeled each document (two first pass annotation passes, referred to as FP1 and FP2 below); a senior annotator then adjudicated discrepancies to create a gold standard. The team consisted of four first pass annotators, two of"
W15-0809,W15-0807,1,0.678904,"ed improvement in annotation quality in the workflow by comparing the adjudicated (ADJ) and first (FP1 and FP2) passes, shown in the columns ADJ vs. FP1 and FP2 in Table 3. The noticeable improvement in score shows the advantage of including adjudication as part of the annotation process. (For IAA purposes, there is obviously no gold or system, but in order to use the scorer we arbitrarily treated one file as the “gold”.) FP1 vs. FP2 Consistency Analysis We examined annotation consistency and quality by comparing different passes of the eval set annotation using the Event Nugget Tuple Scorer (Liu, Mitamura, & Hovy, 2015) developed for the event nugget evaluation task. This scorer treats one file as “gold” and the other as “system”, and matches each nugget in the gold file to one or more nuggets in the system file. This mapping is based on the overlap of the nugget spans. By nugget span, we 3 16 event nuggets in the training set did not receive a realis attribute, due to annotation error. 72 ADJ vs. FP1 78.2 71.7 63.2 ADJ vs. FP2 Span 69.0 89.3 Type 68.2 84.3 Realis 60.0 85.7 Table 3. Scores for Event Nugget Eval Set Annotation To gain some further insight into these numbers we expanded the analysis in two di"
W15-0809,E12-2021,0,\N,Missing
W16-1004,W13-2322,1,0.735417,"me. RED also labels a causal and temporal relation between the two events, &quot;BEFORE/PRECONDITIONS&quot;, showing that the quitting event leads to, but does not directly cause, the replacement, and a temporal CONTAINS relation linking quit to Wednesday.     Event 1: quit - BEFORE DOCTIME, Actual Modality Event 2: replace - AFTER DOCTIME, Actual Modality Relation 1: quit BEFORE/ PRECONDITIONS replace Relation 2: Wednesday CONTAINS quit Although RED does not annotate the arguments of events, it is intended to be combined with semantic role annotations such as PropBank (Bonial et al., 2014) or AMR (Banarescu et al., 2013), which would provide the argument information. For this example, the quit and replace events would also be given the predicate argument structures below: quit.01 Arg0: Media Tycoon Barry Diller Arg1: as chief of Vivendi Universal Entertainment ArgM-TMP: on Wednesday replace.01 Arg2: Parent company chairman JeanRene Fourtou Arg1: Diller ArgM-MOD: will ArgM-PRD: as chief executive of US unit. EER: The following events are connected by Condition and Temporality relations:   Event 1 (Personnel.EndPosition): quit Event 2 (Personnel.StartPosition): replace 34 A preliminary analysis of the Rich ER"
W16-1004,W14-2903,1,0.927744,"Missing"
W16-1004,D12-1045,0,0.048737,"y – along with 21 sense-based subtypes (or relation senses), as shown in Table 1. Events involved in a relation play certain roles. For example, an Attack event and an Injure event in a Contingency_Causality will play Cause and Result roles respectively. Figure 1 shows more information about types and roles. Figure 1: Roles and examples specific to fine-grained event-event relation subtypes. 2.5 Richer Event Descriptions (RED) 3 RED annotation (Ikuta et al., 2014) marks all events in a document, as well as certain relations between those events. RED combines coreference (Pradhan et al., 2007; Lee et al., 2012) and THYME Temporal Relations annotation (Styler et al., 2014) to provide a thorough representation of entities, events and their relations. The RED schema also goes beyond prior annotations of coreference or temporal relations by also annotating subevent structure, cause-effect relations and reporting relations. Guidelines for RED annotation can be found at https://github.com/timjogorman/RicherEventDescr iption/blob/master/guidelines.md. 31 Annotation Data Annotated Features and The representation of events and the scope of annotation vary across the different annotation approaches. Table 2 c"
W16-1004,W15-0809,1,0.776814,"verlapping data set could be used to explore how the differences in annotation procedure lead to differences in decisions about event granularity. 2.3 Event Nugget (EN) An Event Nugget is a tuple of an event trigger, classification of event type and subtype, and realis attribute. It is similar to an event mention in ERE, but arguments are not labelled. EN annotation in 2014 focused on event nuggets (expanded triggers) only, and followed the same taxonomy of 33 event types and subtypes as Light ERE. However, instead of tagging minimal extent as the trigger, EN allowed multi-word event nuggets (Mitamura et al., 2015). Multi-word event nuggets can be either continuous or discontinuous, and are based on the goal of marking the maximal extent of a semantically meaningful unit to express the event in a sentence. EN also added a realis attribute for each event mention. The realis attribute labels each event as Actual, Generic, or Other. TAC KBP 2014 conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute. 30 Table 1: EER event relation types. In 2015,"
W16-1004,W15-0812,1,0.92996,"e taggable, and entity subtypes are not labeled). The event ontology of Light ERE is similar to ACE, with slight modification and reduction, and there is strict coreference of events within documents (Aguilar et al., 2014). As in ACE, the annotation of each event mention includes the identification of a trigger, the labeling of the event type, subtype, and participating event argument entities and time expressions. Simplifying from ACE, only attested actual events are annotated (no irrealis events or arguments). Rich ERE annotation expands on both the inventories and taggability of Light ERE (Song et al., 2015). Rich ERE Entity annotation adds nonspecific entities and nominal head marking, in addition to adding a distinction between Location and Facility entity types. Rich ERE Relation annotation doubles the Light ERE ontology to twenty relation subtypes, and also adds future, hypothetical, and 28 conditional relations. A new category of argument fillers was added for Rich ERE, to allow arguments that are not taggable as entities to be used as fillers for specific relation and event subtypes. For each event mention, Rich ERE labels the event type and subtype, its realis attribute, any of its argumen"
W16-1004,W16-1005,1,0.820435,"event relation types. In 2015, TAC KBP ran an open evaluation on EN that was expanded to three evaluation tasks: Event Nugget Detection, Event Nugget Detection and Coreference, and Event Nugget Coreference. Full Event Nugget Coreference is identified when two or more Event Nuggets refer to the same event. EN annotation in 2015 followed the Rich ERE event taxonomy, which added 5 event types and subtypes to make a total of 38 event types and subtypes, and also followed the Rich ERE guidelines on trigger extents, which adopted the minimal extent rule and disallowed discontinuous event triggers (Song et al., 2016). Annotation of Event Nugget Coreference adopted the concept of Event Hopper as in Rich ERE. 2.4 Event-Event Relations (EER) EER annotation focuses on relations between events in the ERE/ACE taxonomy, both within document and cross-document (Hong et al., 2016). Our general goal is to construct event-centric knowledge networks, where each node is an event and the edges effectively capture the relations between any two events. EER includes five main types of event relations – Inheritance, Expansion, Contingency, Comparison and Temporality – along with 21 sense-based subtypes (or relation senses)"
W16-1005,babko-malaya-etal-2012-identifying,0,0.0482332,"Missing"
W16-1005,doddington-etal-2004-automatic,1,0.709108,"nault chief George Besse in 1986 and the head of government arms sales Rene Audran a year earlier. In 2015 EN annotation, the word “murder” would be the trigger for two Life.Die events, one with the victim “George Besse” and the other with “Rene Audran” as well as two Conflict.Attack events, one occurring in 1986 and the other in 1985. In 2014 EN annotation, the word “murder” would be the trigger for only one Conflict.Attack event. 3.2 Event Taxonomy EN annotation and evaluation focus on a limited inventory of event types and subtypes, as defined in ERE, based on Automatic Content Extraction (Doddington et al., 2004; Walker et al., 2006; Aguilar et 39 al., 2014; Song et al., 2015). The 2014 EN evaluation covered the inventory of event types and subtypes from Light ERE, including 8 event types and 33 subtypes. The 2015 evaluation added a new event type (Manufacture) and four new subtypes – Movement.TransportArtifact, Contact.Broadcast, Contact.Contact, Transaction.Transaction – which aligned the EN event ontology with that of Rich ERE in order to take advantage of the existing Rich ERE annotated data as training data. The EN annotation task also adopted a new approach for applying the Contact event subtyp"
W16-1005,W15-0807,1,0.796291,"Missing"
W16-1005,W15-0809,1,0.896858,"these annotations, except the annotation of event arguments. The EN task in 2014 adapted the event annotation guidelines from the Light ERE annotation task (Aguilar, et al., 2014) by incorporating modifications by the evaluation coordinators that focused on the text extents establishing valid references to events, clarifications on transaction event types, and the additional annotation of event realis attributes, which indicated whether each event mention was asserted (Actual), generic or habitual (Generic), or some other category, such as future, hypothetical, negated, or uncertain (Other) (Mitamura, et al., 2015) . In 2015, EN annotation followed the Rich ERE Event annotation guidelines (except for the annotation of event arguments). As compared to EN annotation in 2014, Rich ERE Event annotation and 2015 EN annotation include increased taggability in several areas: slightly expanded event ontology, additional attributes for contact and transaction events, and double tagging of event mentions for multiple types/subtypes and for certain types of coordination, in addition to event coreference.General Instructions 3 Event Nugget Annotation In this section, we describe the EN annotation as well as the maj"
W16-1005,N04-1019,0,0.189396,"Missing"
W16-1005,W15-0812,1,0.922361,"icipating systems must identify full event coreference links, given the annotated event nuggets in the text. ERE was developed as an annotation task that would be supportive of multiple research directions and evaluations in the DEFT program, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting ERE annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text (Light ERE), to a richer representation of phenomena of interest to the program (Rich ERE) (Song, et al., 2015). In ERE Event annotation, each event mention has annotation of event type and subtype, its realis attribute, any of its arguments or participants that are present, and a required “trigger” string in the text; furthermore, event mentions within the same document are coreferenced into event hoppers (Song, et al., 2015). EN annotation includes all of these annotations, except the annotation of event arguments. The EN task in 2014 adapted the event annotation guidelines from the Light ERE annotation task (Aguilar, et al., 2014) by incorporating modifications by the evaluation coordinators that fo"
W16-1005,W14-2907,1,\N,Missing
W16-6005,N07-1030,0,0.0498993,"Missing"
W16-6005,D09-1120,0,0.0276697,"baseline systems using the traditional Bagof-Words model and a Neural Network architecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has"
W16-6005,N10-1061,0,0.027033,"ly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphreys et al., 1997) but the work was very specific to selected events. More recently, there have been approaches to model event coreferences separately (Liu et al., 2014) as well as jointly with entities (Lee et al., 2012). All this work makes the limiting assumption of word/phrase to word/phrase coreference (levels 1 and 2 described earlier). Our work aligns with the event coreference literature but assumes longer spans of text and tackles the more challenging problem of"
W16-6005,W97-1311,0,0.188687,"g, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphreys et al., 1997) but the work was very specific to selected events. More recently, there have been approaches to model event coreferences separately (Liu et al., 2014) as well as jointly with entities (Lee et al., 2012). All this work makes the limiting assumption of word/phrase to word/phrase coreference (levels 1 and 2 described earlier). Our work aligns with the event coreference literature but assumes longer spans of text and tackles the more challenging problem of abstract multievent/clause coreference. 3 Model vectors are trained using gradient descent, with gradients are obtained though back-propagatio"
W16-6005,W11-1902,0,0.0530903,"Missing"
W16-6005,D12-1045,0,0.0845507,"a better vacation. In this paper we generalize the idea of coreference to 3 levels based on the degree of abstraction of the coreference trigger: 3. Level 3 – Multiple Clauses: The trigger is quite generic and refers to a particular instance of an event that is described over multiple clauses or sentences (either contiguous or noncontiguous). Typically, the abstract event refers to a set of [sub]events, each of them with its own own participants or arguments. See Table 1 for examples. We use PubMed1 as our primary corpus. Almost all work on event coreference (for example, (Liu et al., 2014) (Lee et al., 2012)) applies to levels 1 or 2. In this paper, we propose a generalized coreference classification scheme and address the challenges related to resolving level-3 coreferences. Creating gold-standard training and evaluation materials for such coreferences is an uphill challenge. First, there is a significant annotation overhead and, depending on the nature of the corpus, the annotator might require significant domain knowledge. Each annotation instance might require multiple labels depending the number of abstract events mentioned in the corpus. Second, the vocabulary of the corpus is rather large"
W16-6005,liu-etal-2014-supervised,1,0.910295,"couldn’t have been a better vacation. In this paper we generalize the idea of coreference to 3 levels based on the degree of abstraction of the coreference trigger: 3. Level 3 – Multiple Clauses: The trigger is quite generic and refers to a particular instance of an event that is described over multiple clauses or sentences (either contiguous or noncontiguous). Typically, the abstract event refers to a set of [sub]events, each of them with its own own participants or arguments. See Table 1 for examples. We use PubMed1 as our primary corpus. Almost all work on event coreference (for example, (Liu et al., 2014) (Lee et al., 2012)) applies to levels 1 or 2. In this paper, we propose a generalized coreference classification scheme and address the challenges related to resolving level-3 coreferences. Creating gold-standard training and evaluation materials for such coreferences is an uphill challenge. First, there is a significant annotation overhead and, depending on the nature of the corpus, the annotator might require significant domain knowledge. Each annotation instance might require multiple labels depending the number of abstract events mentioned in the corpus. Second, the vocabulary of the corp"
W16-6005,N06-1025,0,0.0528501,"l and a Neural Network architecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this"
W16-6005,D08-1068,0,0.0347864,"we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphreys et al., 1997) but the work was very specific to selected events. More recently, there have been approaches to model event coreferences separately (Liu et al., 2014) as well as jointly with entities (Lee et al., 2012). All this work makes the limiting assumption of word/phrase"
W16-6005,D10-1048,0,0.0175362,"traditional Bagof-Words model and a Neural Network architecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplore"
W16-6005,P11-1082,0,0.0134691,"ecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphrey"
W17-2703,P98-1013,0,0.371895,"tection and its classification to one type/subtype pair, as defined by the ACE guidelines. 15 Proceedings of the Events and Stories in the News Workshop, pages 15–20, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.2 der to construct an out-domain ANN model. Peng et al. (2016) showed that it is feasible to achieve state-of-the-art results with minimal supervision. In their approach, they use only a few examples and the SRL of a candidate event in order to construct a structured vector representation, which maps the event to an ontology. FrameNet FrameNet (Baker et al., 1998) is a taxonomy of more than 1,200 manually identified semantic frames, deriving from a corpus of 200,000 annotated sentences. The aim of the FrameNet semantic frames is to capture information about the type of a linguistic structure, which can be an event, entity or relation, and its participants. This type is called Frame and the participants are called Frame Elements. Each Frame is linked to a set of words that may trigger the Frame (Lexical Units). Following the definition of FrameNet semantic frames and the ACE 2005 guidelines, it seems natural to assume a good correspondence between the t"
W17-2703,P15-1017,0,0.0330511,"Nugget, and it involves a set of participants, the Event Arguments. The term Event Nugget (TAC, 2014) refers to a semantically meaningful unit of text that denotes some action (event), while the Event Arguments are Entity mentions or temporal expressions related to the Event Nugget. In this work, we focus on the task of Event Nugget Detection and its classification to types and subtypes of Events, according to the ACE 2005 guidelines. Current Event Detection methods that achieve state-of-the-art results are based on Deep Learning techniques using shallow lexical features and word embeddings (Chen et al., 2015), (Nguyen and Gr2 2.1 Background The ACE Dataset According to the ACE 2005 Evaluations (ACE, 2005), an Event contains two spans: the Event Nugget and the Event Arguments. Although there are several types of events, the ACE annotations include only events that can be defined under a certain ontological structure. This structure contains 8 event types followed by a total of 33 event subtypes. The event types are: LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL and JUSTICE. In this work, we focus on the Event Nugget detection and its classification to one type/subtype pair, as"
W17-2703,P16-2060,0,0.027452,"Missing"
W17-2703,P16-1201,0,0.01258,"ameNet semantic frames is to capture information about the type of a linguistic structure, which can be an event, entity or relation, and its participants. This type is called Frame and the participants are called Frame Elements. Each Frame is linked to a set of words that may trigger the Frame (Lexical Units). Following the definition of FrameNet semantic frames and the ACE 2005 guidelines, it seems natural to assume a good correspondence between the two resources. This property implies that a mapping from FrameNet Frames to ACE types and subtypes can be extremely helpful in Event Detection (Liu et al., 2016). 2.3 3 In this paper, we present a system that uses a semantic-frame parser in order to generate event candidates, which are then filtered according to a mapping between ontologies. The main motivation behind this approach is that most systems based on deep learning methods do not exploit rich semantic information and therefore miss nonsurface-level equivalences, which results in low recall. Furthermore, we claim that a combination of a semantically rich system with a deep learning approach can result in better overall performance than both traditional semantic-based approaches and pure deep"
W17-2703,P14-5010,0,0.00447561,"munication means, Text creation, Request Personnel Take place of, Get a job, Hiring, Appointing, Removing, Firing, Quitting, Choosing, Becoming a member, Change of leadership Justice Arrest, Imprisonment, Detaining, Extradition, Breaking out captive, Try defendant, Pardon, Appeal, Verdict, Sentencing, Fining, Execution, Releasing, Notification of charges Table 1: Mapping of FrameNet verbs to ACE Ontology. 3.3 System Architecture We first use Semafor to generate a set of candidate Event Nuggets, their FrameNet frame and their 17 Frame Elements. Then we use the POS tagger from Stanford CoreNLP (Manning et al., 2014) in order to distinguish the candidate events to verbal events and nominal events. For every trigger in the candidate events, we use the output FrameNet Frame in order to decide whether it is an event or not. If the Frame is in the domain of the FrameNet to ACE mapping, then it means that it corresponds to some subtype of the ACE Ontology and, thus, we accept it as an event. Furthermore, according to the mapping, we assign the type and subtype of the event. In Figure 1 we see an example output of the system for one article. The events are represented with green, red and black color if they are"
W17-2703,P15-2060,0,0.0646177,"Missing"
W17-2703,D16-1038,0,0.0600801,"lude only events that can be defined under a certain ontological structure. This structure contains 8 event types followed by a total of 33 event subtypes. The event types are: LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL and JUSTICE. In this work, we focus on the Event Nugget detection and its classification to one type/subtype pair, as defined by the ACE guidelines. 15 Proceedings of the Events and Stories in the News Workshop, pages 15–20, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.2 der to construct an out-domain ANN model. Peng et al. (2016) showed that it is feasible to achieve state-of-the-art results with minimal supervision. In their approach, they use only a few examples and the SRL of a candidate event in order to construct a structured vector representation, which maps the event to an ontology. FrameNet FrameNet (Baker et al., 1998) is a taxonomy of more than 1,200 manually identified semantic frames, deriving from a corpus of 200,000 annotated sentences. The aim of the FrameNet semantic frames is to capture information about the type of a linguistic structure, which can be an event, entity or relation, and its participant"
W17-2703,C98-1013,0,\N,Missing
W18-2312,D14-1162,0,0.0811049,"pairs for a particular question to train a three-way neural classifier to predict if the relationship between the two is entailment, contradiction or neither. It is worth noting here that the embedding transformation techniques that we implemented are not specific to the NLI tasks and, in fact, enable transfer learning of a much broader set of tasks on smaller datasets like BioASQ by using the pre1. The number of assertion-sentence pairs in BioASQ is too few to train the textual entailment model effectively. 2. The models that are pre-trained on SNLI (Bowman et al., 2015) datasets use GLOVE (Pennington et al., 2014) embeddings that cannot be used for biomedical corpora which have quite different characteristics and vocabulary compared to the corpora that GLOVE was trained on. However, we have pre-trained embeddings available that were trained on PubMed and PMC texts along with Wikipedia articles (Pyysalo et al., 2013). To leverage these embeddings, we implemented an embedding-transformation methodology to projecting the PubMed embeddings to GLOVE embedding space and then fine tune the pre-trained InferSent on the BioASQ dataset for textual entailment. The hypothesis is that, since both the embeddings had"
W18-2312,D15-1075,0,0.0355495,"ence embeddings of the assertion-sentence pairs for a particular question to train a three-way neural classifier to predict if the relationship between the two is entailment, contradiction or neither. It is worth noting here that the embedding transformation techniques that we implemented are not specific to the NLI tasks and, in fact, enable transfer learning of a much broader set of tasks on smaller datasets like BioASQ by using the pre1. The number of assertion-sentence pairs in BioASQ is too few to train the textual entailment model effectively. 2. The models that are pre-trained on SNLI (Bowman et al., 2015) datasets use GLOVE (Pennington et al., 2014) embeddings that cannot be used for biomedical corpora which have quite different characteristics and vocabulary compared to the corpora that GLOVE was trained on. However, we have pre-trained embeddings available that were trained on PubMed and PMC texts along with Wikipedia articles (Pyysalo et al., 2013). To leverage these embeddings, we implemented an embedding-transformation methodology to projecting the PubMed embeddings to GLOVE embedding space and then fine tune the pre-trained InferSent on the BioASQ dataset for textual entailment. The hypo"
W18-2312,W17-2337,0,0.0808783,"ranking algorithms to generate the final predictions. Introduction In the era of ever advancing medical sciences and the age of the internet, a remarkable amount of medical literature is constantly being posted online. This has led to a need for an effective retrieval and indexing system which can allow us to extract meaningful information from these vast knowledge sources. One of the most effective and natural ways to leverage this huge amount of data 4. We improve upon the MMR framework for relevant sentence selection from the chosen snippets that was introduced in the work of Chandu et al. (2017). We experiment with a number of more informative similarity metrics to replace and improve upon the baseline Jaccard similarity metric. 109 Proceedings of the BioNLP 2018 workshop, pages 109–117 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 Relevant Literature self. The 5b version of this dataset consists of 1,799 questions in 3 distinct categories: Biomedical Question answering has always been a hot topic of research among the QA community at large due to the relative significance of the problem and the challenge of dealing with a non standard vocabu"
W18-2312,W17-2307,1,0.839086,"nd supervised ranking algorithms to generate the final predictions. Introduction In the era of ever advancing medical sciences and the age of the internet, a remarkable amount of medical literature is constantly being posted online. This has led to a need for an effective retrieval and indexing system which can allow us to extract meaningful information from these vast knowledge sources. One of the most effective and natural ways to leverage this huge amount of data 4. We improve upon the MMR framework for relevant sentence selection from the chosen snippets that was introduced in the work of Chandu et al. (2017). We experiment with a number of more informative similarity metrics to replace and improve upon the baseline Jaccard similarity metric. 109 Proceedings of the BioNLP 2018 workshop, pages 109–117 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 Relevant Literature self. The 5b version of this dataset consists of 1,799 questions in 3 distinct categories: Biomedical Question answering has always been a hot topic of research among the QA community at large due to the relative significance of the problem and the challenge of dealing with a non standard vocabu"
W18-2312,P17-1099,0,0.0371143,"l answer type questions on the BioASQ dataset. For exact answers, we incorporate neural entailment models along with a novel embedding transformation technique for answering yes/no questions, and employ LeToR ranking models to answer factoid/list based questions. For ideal answers, we improve the IR component of extractive summarization. Although this improves ROUGE scores considerably, the human readability aspect of the generated summary answer is not greatly improved. As future directions, we believe that effective abstractive summarization based approaches like Pointer Generator Networks (See et al., 2017) and Reinforcement Learning based techniques (Paulus et al., 2017) would improve the human readability of ideal answers. We aim to continue our research in this direction to achieve a good balance between ROUGE score and human readability. Learning To Rank In order to rank the candidate entities in a supervised way, we use a ranking classifier based on the features described in 5.2.2. For ranking, we choose point-wise ranking classifiers over pairwise and list-wise, because it yields similar results to ranking methods with a less time-consuming and computationally expensive approach. We use a"
W18-2312,P05-1022,0,0.285486,"Missing"
W18-2312,D17-1070,0,0.0170719,"ons for all yes/no questions. As a simple extension to this, we can also create negative assertions by using not along with the auxiliary verbs. 5.1.2 Recognizing Textual Entailment The primary goal of our NLI module is to infer if any of the sentences among the answer snippets entails or contradicts the assertion posed by the question. We segmented the answer snippets for each question to produce a set of assertionsentence pairs. To then evaluate if these assertions can be inferred or refuted from the sentences, we built a Recognizing Textual Entailment (RTE) model using the InferSent model (Conneau et al., 2017), which computes sentence embeddings for every sentence and has been shown to work well on NLI tasks. In training InferSent, we experienced two major challenges: W ∗ = arg minkW Ep |− Eg |k W subject to the constraint that W is orthogonal. The solution to this optimization problem is given by using the singular value decomposition of Eg |Ep , i.e.W ∗ = U V |where Eg |Ep = U ΣV |With this simple linear transformation, we then computed the transformed embeddings for all the words in the PubMed embeddings that are not present in the GLOVE embeddings. We also explore a non-linear transformation us"
W18-2312,W17-2309,0,0.0405859,"Missing"
W18-2312,W04-1013,0,0.0179173,"Hence, we present a Natural Language Inference (NLI)-based system that learns if the assertions made by the questions are true in the context of the documents. As a part of this system, we first generate assertions from questions and evaluate the entailment or contradiction of these assertions using a Recognizing Textual Entailment (RTE) model. We then use these entailment scores for all the sentences in the snippets or documents to heuristically evaluate if the answer to the yes/no question. Evaluation The pipeline described above is primarily designed to improve the ROUGE evaluation metric (Lin, 2004). Although a higher ROUGE score does not necessarily reflect improved human readability, MMR can improve readability by reducing redundancy in generated answers. Results for ideal answers for Task 5 phase b are shown in Table 1. We also compare our results with other state of the art approaches in Table 4. 5 Yes/No type questions Exact answers Exact answers represent the subset of the BioASQ task where the responses are not structured paragraphs, but instead either a single entity (yes/no types) or a combination of named entities (factoid or list types) that compose the correct reply to the gi"
W18-2312,W18-5300,0,0.246742,"Missing"
W18-4702,P98-1013,0,0.594043,"rarily within the respective domains. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionable whether a collection of the existing heterogenous annotation schemes for closed-domain events adequately contribute to interoperable and consistent annotation of events across domains. On the other hand, prior work on open-domain events have some limitations with respect to coverage of events and their relations. Lexical databases such as WordNet (Miller et al., 1990), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) can be viewed as a superset of event lexicon, and their subtaxonomies seem to provide an extensional definition of events. However, these databases have a narrow coverage of events because they generally do not cover current terminology and proper nouns due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank cover the proper noun ‘Hurricane Katrina’. In addition, they do not provide any principles or guidelines about how to annotate events in text by themselves due to their different focus. TimeML (Pustejovsky et al., 2003) focus"
W18-4702,cybulska-vossen-2014-using,0,0.0184551,"-level questions for reading comprehension, such as the one that requires learners to infer answers over multiple sentences (Araki et al., 2016). Our contribution is twofold. First, we annotate a wide coverage of events, comprising verbs, nouns, adjectives, and phrases which are continuous or discontinuous (see Section 3). Despite this relatively wide and flexible annotation of events on text in 10 different domains, we show that our annotation achieved high inter-annotator agreement. Second, unlike previous methodologies which generally focus on deal only with event coreference such as ECB+ (Cybulska and Vossen, 2014), we present methodologies to annotate five event relations in unrestricted domains (see Section 4). 2 Data and Annotation Procedures In this section, we describe our data and annotation procedures. Our annotation target is not restricted in any specific domains. Thus, ideally speaking, our annotation should include all kinds of events in a domain-agnostic manner. However, annotating all kinds of events manually in unrestricted domains would be unrealistic due to annotation cost. Therefore, in order to make the corpus creation manageable while retaining the domain diversity, we select 100 arti"
W18-4702,doddington-etal-2004-automatic,0,0.156065,"nts have received relatively little attention in the literature. From the perspective of information extraction, much previous work on events pays attention to domainspecific clause-level argument structure (e.g., attackers kill victims, plaintiffs sue defendants, etc), putting less emphasis on what semantically constitutes events. The formalization focusing on domain-specific clause-level argument structure often involves its own definition of events based on instantiation of event ontology for a particular domain, aimed at automatic extraction of closed-domain events, as illustrated in ACE (Doddington et al., 2004), TAC KBP (Mitamura et al., 2017), PASBio (Wattarujeekrit et al., 2004), and BioNLP (Kim et al., 2009). For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. The closed-domain formalization might be of practical use in some domain-specific scenarios. However, it designs event definitions and annotation schemes arbitrarily within the respective domains. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionab"
W18-4702,W17-0812,0,0.0123804,"to be a cause-and-effect relation, in which we can explain the causation between two event nuggets X and Y, saying “X causes Y”. One example of causality is “The tsunami was caused by the earthquake.” Causality also adds another distinctive characteristic to annotation. Causality inherently entails an event sequence. For example, if we say “The tsunami was caused by the earthquake”, it means that the tsunami happened after the earthquake. To distinguish causality from event sequences and other relations such as preconditions (Palmer et al., 2016), we perform causality tests, largely based on (Dunietz et al., 2017): 1. The “why” test: After reading the sentence, can an annotator answer “why” questions about the potential effect argument? If not, it is not causal. 2. The temporal order test: Is the cause asserted to precede the effect? If not, it is not causal. 3. The counterfactuality test: Would the effect have been just as probable to occur or not occur had the cause not happened? If so, it is not causal. 4. The ontological asymmetry test: Could you just as easily claim the cause and effect are reversed? If so, it is not causal. 5. The linguistic test: Can the sentence be rephrased as It is because (o"
W18-4702,W13-1203,1,0.901977,"fer to the same event. For two event nuggets to corefer, they should be semantically identical, have the same participants (e.g., agent, patient) or attribute (e.g., location, time), and have the same polarity. For instance, ‘Great Fire of London’ and ‘fire’ are coreferential in (11). (11) The Great Fire of London happened in 1666. The fire lasted for three days. When considering event identity for event coreference, we use the notion of event hopper from Rich ERE (Song et al., 2015), which is a more inclusive, less strict notion than the event coreference defined in ACE. Subevent. Following (Hovy et al., 2013), we define subevent relations as follows. Event A is a subevent of event B if B represents a stereotypical sequence of events, or a script (Schank and Abelson, 1977), and A is a part of that script. For example, ‘affected’, ‘flooded’ and ‘broke’ are three subevents of ‘Hurricane Katrina’ in (12). We refer to ‘Hurricane Katrina’ as a parent (event) of the three subevents. (12) On August 29, 2005, New Orleans was affected by Hurricane Katrina which flooded most of the city when city levees broke. Causality. We define causality to be a cause-and-effect relation, in which we can explain the causa"
W18-4702,W09-1401,0,0.0521808,"on, much previous work on events pays attention to domainspecific clause-level argument structure (e.g., attackers kill victims, plaintiffs sue defendants, etc), putting less emphasis on what semantically constitutes events. The formalization focusing on domain-specific clause-level argument structure often involves its own definition of events based on instantiation of event ontology for a particular domain, aimed at automatic extraction of closed-domain events, as illustrated in ACE (Doddington et al., 2004), TAC KBP (Mitamura et al., 2017), PASBio (Wattarujeekrit et al., 2004), and BioNLP (Kim et al., 2009). For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. The closed-domain formalization might be of practical use in some domain-specific scenarios. However, it designs event definitions and annotation schemes arbitrarily within the respective domains. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionable whether a collection of the existing heterogenous annotation schemes for closed-domain events adequ"
W18-4702,H05-1004,0,0.0574364,"63 21 Dise 67 94 105 30 10 Eco 51 48 37 73 15 Edu 61 101 28 62 17 Geo 52 91 87 58 8 Hist 42 119 68 117 21 Poli 46 147 36 66 24 Tran 51 93 60 88 11 Total 512 946 586 643 146 Table 2: Statistics of event coreference clusters and cluster-level event relations in SW100. For brevity, we use a prefix with 3 or 4 characters to refer to each domain. 4.4 Inter-annotator Agreement on Annotation of Event Relations One way to compute inter-annotator agreement on event coreference is to use evaluation metrics developed by prior work, such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and BLANC (Recasens and Hovy, 2011). However, these metrics are appropriate specifically for (event) coreference and cannot be consistently applied to other event relations such as subevents. Instead, a single consistent metric is ideal for comparing inter-annotator agreement. Since we have three annotators, we use Fleiss’ Kappa (Fleiss, 1971) to compute inter-annotator agreement on the five event relations annotated by the three annotators. Specifically, we consider all pairwise relations between events and propagate event relations via event coreference, following (Mitamura et al., 2017)."
W18-4702,W15-0809,1,0.9275,"n (RED) (Palmer et al., 2016) defines events and their relations in a general manner, but its annotation was performed only in the clinical domain (O’Gorman et al., 2016). In this work, we present our methodologies for annotating events and relations between events in unrestricted domains. The goal of our annotation project is to provide human-annotated data to build a generation generation application to enhance reading comprehension for English as second language (ESL) students, as a continuous effort of (Araki et al., 2016). Using the notion of eventualities (Bach, 1986) and event nuggets (Mitamura et al., 2015), our event annotation scheme defines events and annotates event spans of text while not assigning any specific event types to them. In that sense, our event annotation is span-oriented, as compared to the traditional argument-oriented annotation of events. As for relations between events, we choose to annotate five relations from the perspective of the goal: event coreference, subevent, causality, event sequence (‘after’ relations), and simultaneity. To our knowledge, this is the first work that performs human annotation of events and the five relations in unrestricted domains. We believe tha"
W18-4702,J88-2003,0,0.829549,"annotation. 6. The adjudicator finalizes event relation annotation. 3 Annotation of Events This section describes our definition of events and principles for annotation of events. 3.1 Definition of Events: Eventualities As with TimeML (Pustejovsky et al., 2003) and ISO-TimeML (ISO, 2012), our definition of events uses eventualities (Bach, 1986), which are a broader notion of events, including states, processes, and events. This definition is inclusive in the sense that it includes states in addition to events and processes. We define the three classes on the basis of durativity and telicity (Moens and Steedman, 1988; Pulman, 1997): • states: notions that remain unchanged until their change or are brought as a result of an event, e.g., He owns a car. Tom was happy when he received a present; • processes: notions that involve a change of state without an explicit goal or completion, e.g., it was raining yesterday; • events4 : notions that involve a change of state with an explicit goal or completion, e.g., walked to Boston, buy a book. We recognize that annotating states is generally more difficult than annotating processes and actions because states are often confused with attributes which are not eventiv"
W18-4702,W16-5706,0,0.0416448,"Missing"
W18-4702,J05-1004,0,0.195177,"ns. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionable whether a collection of the existing heterogenous annotation schemes for closed-domain events adequately contribute to interoperable and consistent annotation of events across domains. On the other hand, prior work on open-domain events have some limitations with respect to coverage of events and their relations. Lexical databases such as WordNet (Miller et al., 1990), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) can be viewed as a superset of event lexicon, and their subtaxonomies seem to provide an extensional definition of events. However, these databases have a narrow coverage of events because they generally do not cover current terminology and proper nouns due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank cover the proper noun ‘Hurricane Katrina’. In addition, they do not provide any principles or guidelines about how to annotate events in text by themselves due to their different focus. TimeML (Pustejovsky et al., 2003) focuses on temporal aspects of events an"
W18-4702,W15-0812,0,0.0173505,"nt sequence, and simultaneity. Event coreference. We define event coreference as a linguistic phenomenon that two event nuggets refer to the same event. For two event nuggets to corefer, they should be semantically identical, have the same participants (e.g., agent, patient) or attribute (e.g., location, time), and have the same polarity. For instance, ‘Great Fire of London’ and ‘fire’ are coreferential in (11). (11) The Great Fire of London happened in 1666. The fire lasted for three days. When considering event identity for event coreference, we use the notion of event hopper from Rich ERE (Song et al., 2015), which is a more inclusive, less strict notion than the event coreference defined in ACE. Subevent. Following (Hovy et al., 2013), we define subevent relations as follows. Event A is a subevent of event B if B represents a stereotypical sequence of events, or a script (Schank and Abelson, 1977), and A is a part of that script. For example, ‘affected’, ‘flooded’ and ‘broke’ are three subevents of ‘Hurricane Katrina’ in (12). We refer to ‘Hurricane Katrina’ as a parent (event) of the three subevents. (12) On August 29, 2005, New Orleans was affected by Hurricane Katrina which flooded most of th"
W18-4702,E12-2021,0,0.0984759,"Missing"
W18-4702,M95-1005,0,0.122093,"ations Arch 32 74 31 48 8 Chem 61 81 63 38 11 Disa 49 98 71 63 21 Dise 67 94 105 30 10 Eco 51 48 37 73 15 Edu 61 101 28 62 17 Geo 52 91 87 58 8 Hist 42 119 68 117 21 Poli 46 147 36 66 24 Tran 51 93 60 88 11 Total 512 946 586 643 146 Table 2: Statistics of event coreference clusters and cluster-level event relations in SW100. For brevity, we use a prefix with 3 or 4 characters to refer to each domain. 4.4 Inter-annotator Agreement on Annotation of Event Relations One way to compute inter-annotator agreement on event coreference is to use evaluation metrics developed by prior work, such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and BLANC (Recasens and Hovy, 2011). However, these metrics are appropriate specifically for (event) coreference and cannot be consistently applied to other event relations such as subevents. Instead, a single consistent metric is ideal for comparing inter-annotator agreement. Since we have three annotators, we use Fleiss’ Kappa (Fleiss, 1971) to compute inter-annotator agreement on the five event relations annotated by the three annotators. Specifically, we consider all pairwise relations between events and propagate event relations via event"
W18-5310,W17-2307,1,0.866427,"when compared with either approach used in isolation. The dataset we use for development of the current work is released as a part of the sixth edition of the annual BioASQ challenge (Tsatsaronis et al., 2012). The main categories of answers in this data include summary, factoid, list and yes/no. There are a total of 2,251 questions, each of which is accompanied by a list of relevant documents and a list of relevant snippets extracted from each of these documents. Our model is an extension to the highest ROUGE scoring model in the final test batch of the fifth edition of the BioASQ challenge (Chandu et al., 2017), which is based on Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998). In addition, we attempted abstractive techniques that are scoped to improve the readability and coherence aspect of the problem. We made 4 submissions to the challenge. The paper is organized as follows: Section 2 describes our overall system architecture and the implementation details. Experiments and results are discussed in Section 3 followed by conclusion and future work in 4. The ever-increasing magnitude of biomedical information sources makes it difficult and time-consuming for a human researcher to fi"
W18-5310,P17-1099,0,0.0242833,"e and abstractive summarization. Extractive summarization works by selecting the most relevant sentences in a document to generate the summary (Allahyari et al., 2017). The summaries generated using this technique generally obtain high ROUGE scores (Lin, 2004) due to the high n-gram overlap between the generated summary and the ideal answer. Abstractive summarization on the other hand works by generating the summary word by word as opposed to picking sentences in the case of extractive summarization. Recent advances in abstractive summarization using Pointer Generator Coverage (PGC) networks (See et al., 2017) have shown that neural sequence to sequence models can generate abstractive sumRelevanceScore (1) = scoreBM 25 (idealanswer, snippet) 2.2.3 RankSVM RankSVM(Cao et al., 2006) is a pairwise LETOR approach towards ranking of documents. Each pair of snippets was taken for a question and was labeled as -1 if the second snippet was ranked higher and +1 if the second snippet was ranked lower. In a pairwise approach there is an overhead of maintaining the metadata as we need to know which set of snippets are going into the SVM as input for validation of the model. Consider F (Q, S1 ) as a feature rep"
W18-5310,W12-0201,0,0.0300876,"icroarrays have been used to assess DNA replication timing in a variety of eukaryotic organisms”, the clause “in a variety of eukaryotic organisms” would be missed in the phase II of ontology creation. But in Phase II, we convert such that the verb “assess” has an attribute “{in, nodexyz }” where nodexyz is the node pertaining to the CUI of “eukaryotic organisms”. 2.1.2 Graph Creation Graph Framework: All PubMed abstracts are tokenized and relations are extracted from them. These are added as relations in the graph. We create custom data structures for the Nodes and Edges(Relations) in Neo4j (Webber, 2012). Every relation has attributes which are comma separated values of PubMed ID, location within the abstract. This is stored in order to retrieve the exact sentence that was used to create a particular relation. We hypothesize that this can improve in getting relevant snippets across the abstracts. Phase I: Ontology Creation with UMLS Concepts. Part of speech tagging is the most intuitive way of approaching the problem of extracting the relations from a given text. An initial strategy of forming the Subject Verb Object (SVO) triplets was formed based on a left-right parsing of the text. For thi"
W18-5310,P15-1128,0,0.0301019,"tion. However, common NLP tools aren’t easily leveraged on biomedical text, due to dramatic differences in the structure and content of the sentences. There exist tools for relation extraction in sub-domains such as Bacteria (Duclos et al., 2007) and disease-cause ontologies (Schriml et al., 2011), but these methods heavily rely on the presence of specific words or features at the sentence level, and cannot be easily scaled to general biotext. Most neural methods for training relation extractors require a large (O(106 )) corpus of labelled examples, which is not available for general biotext (Yih et al., 2015). In order to explore the use of ontology-based retrieval, we developed a novel Ontology-Based Information Retrieval Although a large amount of biomedical text is available in resources such as NLM (NIH, 2018), it can be difficult to leverage in the absence of supervised or automatic labeling (annotation) of the unstructured text content. Our hypothesis is that an Ontology-based retrieval module which utilizes entity and relation extraction techniques to represent and compare the content of questions and candidate answers can improve the recall of answerbearing documents from unstructured sour"
W18-5310,P14-5010,0,0.00251535,"relations is depicted in Figure 4. RE approach, which is described below. The base architecture for the RE module is depicted in Figure 3. The following 4 steps are employed for extracting relations from a sentence: 1. Noun Phrase Chunking: The sentence is parsed using the TreeTagger POS tagger (Schmid, 1995) to obtain all the Noun Chunks that form the potential nodes of the graph. For our purpose, the nodes of the graph are all Medical and Named Entities. In order to perform this, the potential nodes are passed through a Medical Entity Recogniser (GRAM-CNN) (Zhu et al.) and the Stanford NER (Manning et al., 2014) discarding the chunks that are not recognized. For an example, let us consider the following sentence: ‘Genomic microarrays have been used to assess DNA replication timing in a variety of eukaryotic organisms.’ which extract the following noun chunks: ‘Genomic Microarrays’, ‘DNA Replication Timing’ and ‘Eukaryotic Organisms’. 2. Relation Extraction: This step comprises of 2 sub parts. (2a) RE using Predicate Argument Structures: The Predicate Argument Structure (PAS) for the sentence, obtained using the Enju parser (Miyao et al., 2008), is further parsed in order to obtain possible relations"
W18-5310,P08-1006,0,0.01494,"ecogniser (GRAM-CNN) (Zhu et al.) and the Stanford NER (Manning et al., 2014) discarding the chunks that are not recognized. For an example, let us consider the following sentence: ‘Genomic microarrays have been used to assess DNA replication timing in a variety of eukaryotic organisms.’ which extract the following noun chunks: ‘Genomic Microarrays’, ‘DNA Replication Timing’ and ‘Eukaryotic Organisms’. 2. Relation Extraction: This step comprises of 2 sub parts. (2a) RE using Predicate Argument Structures: The Predicate Argument Structure (PAS) for the sentence, obtained using the Enju parser (Miyao et al., 2008), is further parsed in order to obtain possible relations for the graph. Possible relations are those that contain arguments related through a verb or a preposition. (2b) RE transformation through transitivity: Transitivity is performed on relations obtained from the Enju parser in order to ensure that the arguments of the relations represent medical or named entities in the graph. The potential nodes are passed through the NER and MER. Nodes that are not tagged or recognized by either undergo a transitive transformation to give way to new relations. For the example mentioned, the following re"
W18-6704,P17-1152,0,0.0289244,"from the similar retrieved sentences, we applied the ESMI entailment 2.1 Source Document Collection Source texts for QG were collected from English Wikipedia. To generate entailed sentences for 3 4 16 https://spacy.io https://github.com/huggingface/neuralcoref True Positive False Positive Precision 200 Number 150 100 50 0 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 Table 1: Filtering Values Entailment confidence ROUGE-1 Sentence similarity Num. of retrieved sentence words Num. of source sentence words Precision 250 Entailment Confidence Range Figure 4: Preliminary Entailment Evaluation detector (Chen et al., 2017), which was trained using MultiNLI (Williams et al., 2018). We employed the GloVe (Pennington et al., 2014) as the word embedding for the ESIM. For sentences (7), (8), and (9), the entailment detector labeled “entailment (confidence 0.93),” “entailment (confidence 0.60),” and “neutral (confidence 0.86),” respectively. Sentences (7) and (8) were kept because they were labeled as entailment. However, we eliminated sentence (9) because of the neutral label. Min. Max. 0.9 1 0.2 0.5 6 0.7 1 - Num. of retrieved sentence words - and the entailed ones, because ROUGE-1 cannot measure semantic similarit"
W18-6704,W17-5038,0,0.0272302,"rated the following question. 2. Did Kawabata win the Nobel Prize in Literature for his novel “Snow Country”? Although this question is grammatical, its educational effectiveness could be minimized, since students might not exert their reading comprehension skills due to the similarity between the generated question and the original sentence. Questions generated by these QG methods are often quite similar to the original sentences. Some researchers have tried inference QG with templates. Labutov et al. (2015) studied a QG system that utilizes ontology and templates developed by crowd workers. Chinkina and Meurers (2017) built a conceptual QG system using handcrafted pattern matching templates. Although the templates in these studies may need more work, the generated questions are more complicated than those by transforming the single sentence. Our research proposes a novel QG approach based on textual entailment. In contrast to the existing studies that directly generate questions from sources, our system firstly generates new sentences entailed by source texts and then transforms the entailed sentences into questions as shown in Figures 1 and 2. For example, we generate the following sentence entailed by th"
W18-6704,P15-1086,0,0.074938,"Missing"
W18-6704,W04-1013,0,0.0211834,"entence tokenization and their entity coreferences were resolved Then, we retrieved 61,330 similar sentences from the web (maximum 50 similar sentences per sentence). Maximum 30, 10 and 10 sentences were selected from the Google search results, English Wikipedia, and Simple Wikipedia, respectively. The entailment detector labeled 16,770 sentences as textual entailment with an argmax criterion, but 676 sentences remained after the filtering. We applied Heilman’s QG system to them. ROUGE-1 To control the ratio of word overlapping between the entailed sentences and the source sentences, ROUGE-1 (Lin, 2004) was used. Sentence Similarity We used spaCy to calculate sentence similarity between the source sentences 17 Answerable Examples: Unanswerable Examples: 1. Article: IQ Source Sentence: Unlike, for example, distance and mass, a concrete measure of intelligence cannot be achieved given the abstract nature of the concept of &quot;intelligence&quot;. H&S System’s Yes/no Question: Can distance and mass not be achieved given the abstract nature of the concept of ``intelligence&apos;&apos; for example? Our System’s Question: Is it problematic to claim that the intelligence quotient is a measure of intelligence? Retriev"
W18-6704,W16-6609,0,0.0648772,"ces entailed by source texts and then transforms the entailed sentences into questions as shown in Figures 1 and 2. For example, we generate the following sentence entailed by the sentence (1). Introduction Question generation (QG) is a practical application field of natural language generation. One important objective of QG in education is cultivating students’ reading comprehension skills. Many studies have been done on QG by transforming a single sentence into a question. Heilman and Smith (2010) researched QG based on syntactic parsing which is characterized by overgenerating and scoring. Mazidi and Tarau (2016) generated questions based on dependency parsing. Woo et al. (2016) studied QG based on dependency and semantic role labeling. Their systems can generate relatively simple but grammatical questions. Suppose the following 3. Kawabata is the writer of “Snow Country”. Now we create a question for sentence (1) by transforming sentence (3) as follows. 1 2 https://sites.google.com/site/ntcir11riteval/ http://www.cs.cmu.edu/ ark/mheilman/questions/ 15 Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2IS&NLG), pages 15–19, c Tilburg, The Netherlands, November 5 2"
W18-6704,D14-1162,0,0.0860107,"Source texts for QG were collected from English Wikipedia. To generate entailed sentences for 3 4 16 https://spacy.io https://github.com/huggingface/neuralcoref True Positive False Positive Precision 200 Number 150 100 50 0 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 Table 1: Filtering Values Entailment confidence ROUGE-1 Sentence similarity Num. of retrieved sentence words Num. of source sentence words Precision 250 Entailment Confidence Range Figure 4: Preliminary Entailment Evaluation detector (Chen et al., 2017), which was trained using MultiNLI (Williams et al., 2018). We employed the GloVe (Pennington et al., 2014) as the word embedding for the ESIM. For sentences (7), (8), and (9), the entailment detector labeled “entailment (confidence 0.93),” “entailment (confidence 0.60),” and “neutral (confidence 0.86),” respectively. Sentences (7) and (8) were kept because they were labeled as entailment. However, we eliminated sentence (9) because of the neutral label. Min. Max. 0.9 1 0.2 0.5 6 0.7 1 - Num. of retrieved sentence words - and the entailed ones, because ROUGE-1 cannot measure semantic similarity. The Word Counts of the Source Sentences and the Retrieved Ones We excluded too short sentences because t"
W18-6704,N18-1101,0,0.0131234,"ESMI entailment 2.1 Source Document Collection Source texts for QG were collected from English Wikipedia. To generate entailed sentences for 3 4 16 https://spacy.io https://github.com/huggingface/neuralcoref True Positive False Positive Precision 200 Number 150 100 50 0 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0.25 0.2 Table 1: Filtering Values Entailment confidence ROUGE-1 Sentence similarity Num. of retrieved sentence words Num. of source sentence words Precision 250 Entailment Confidence Range Figure 4: Preliminary Entailment Evaluation detector (Chen et al., 2017), which was trained using MultiNLI (Williams et al., 2018). We employed the GloVe (Pennington et al., 2014) as the word embedding for the ESIM. For sentences (7), (8), and (9), the entailment detector labeled “entailment (confidence 0.93),” “entailment (confidence 0.60),” and “neutral (confidence 0.86),” respectively. Sentences (7) and (8) were kept because they were labeled as entailment. However, we eliminated sentence (9) because of the neutral label. Min. Max. 0.9 1 0.2 0.5 6 0.7 1 - Num. of retrieved sentence words - and the entailed ones, because ROUGE-1 cannot measure semantic similarity. The Word Counts of the Source Sentences and the Retriev"
W19-5041,W19-5039,0,0.443147,"e of Natural Language Inference (NLI) in passage retrieval, answer selection and answer re-ranking to advance open-domain question answering. (Tari et al., 2007) shows effective use of UMLS (Bodenreider, 2004), a Unified Medical Language System to asses passage relevancy through semantic relatedness. All these methods work well independently, but to the best of our knowledge, there hasn’t been much work in using NLI and RQE systems in tandem for the tasks of filtering and re-ranking. Dataset & Evaluation The dataset for re-ranking and filtering has been provided by the MediQA Shared task (Ben Abacha et al., 2019) in ACL-BioNLP 2019 workshop. It consists of medical questions and their associated answers retrieved by CHiQA 2 . The training dataset consists of 208 questions while the validation and test datasets have 25 and 150 questions respectivley. Each question has upto 10 candidate answers, with each answer having the following attributes : 1. SystemRank: rank. As noted in (Romanov and Shivade, 2018), the task of Natural Language Inference is not domain agnostic, and thus is not able to transfer well to other domains. The authors use a gradient boosting classifier (Mason et al., 2000) with a variety"
W19-5041,P17-1152,0,0.0602761,"while the validation and test datasets have 25 and 150 questions respectivley. Each question has upto 10 candidate answers, with each answer having the following attributes : 1. SystemRank: rank. As noted in (Romanov and Shivade, 2018), the task of Natural Language Inference is not domain agnostic, and thus is not able to transfer well to other domains. The authors use a gradient boosting classifier (Mason et al., 2000) with a variety of hand crafted features for baselines. They then use Infersent (Conneau et al., 2017) as a sentence encoder. The paper also reports results on the ESIM Model (Chen et al., 2017) but with no visible improvements. They also discuss transfer learning and external knowledge based methods. It corresponds to CHiQA’s 2. ReferenceRank: It corresponds to the correct rank. 3. ReferenceScore: This is an additional score that is provided only in the training and validation sets, which corresponds to the manual judgment/rating of the answer [4: Excellent, 3: Correct but Incomplete, 2: Related, 1: Incorrect]. For the answer classification task, answers with scores 1 and 2 are considered as incorrect (label 0), and answers with scores 3 and 4 are considered as correct (label 1). Th"
W19-5041,D17-1070,0,0.167715,"Missing"
W19-5041,P06-1114,0,0.308079,"randomly sorted. Karan and Shefali took ownership of the NLI module while Sheetal and Prashant worked on the RQE module. Hemant researched and implemented the Question-Answering system including baseline and multi-task learning. Sheetal and Hemant worked on scraping data from icliniq. Karan and Prashant helped with integration of NLI and RQE module respectively into the multi-task system. 1 https://github.com/google-research/bert/issues/27 389 Proceedings of the BioNLP 2019 workshop, pages 389–398 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 3 challenges. (Harabagiu and Hickl, 2006) successfully shows the use of Natural Language Inference (NLI) in passage retrieval, answer selection and answer re-ranking to advance open-domain question answering. (Tari et al., 2007) shows effective use of UMLS (Bodenreider, 2004), a Unified Medical Language System to asses passage relevancy through semantic relatedness. All these methods work well independently, but to the best of our knowledge, there hasn’t been much work in using NLI and RQE systems in tandem for the tasks of filtering and re-ranking. Dataset & Evaluation The dataset for re-ranking and filtering has been provided by th"
W19-5041,P19-1139,0,0.0302928,"ncorrectly Predicting Contradiction on Test Set The model also fails while trying to differentiate between statements that are neutral versus those that entail each other. The model generally relies on lexical overlap between the hypothesis and the premise, and in cases, when it is unable to find one, falls back to assigning the label as neutral as shown in Figure 4. For the RQE task, we observe that our model la395 them randomly during training so that the model learns the semantic representation even without the medical entities. Masking entities has been shown to generalize better in ERNIE(Zhang et al., 2019) in comparison to BERT(Devlin et al., 2018). For the re-ranking and filtering tasks we look into the macro-trends and investigate what qualifies as tougher problems for both the tasks. From Figure 7, it is clear that lower ranked valid answers are generally harder answers for filtering. Observing the valid answers with low ranks, we see that they generally have only 1-2 relevant sentences each, which might be hard for the model especially in cases where the answers have a lot of sentences. Similar analysis for the filtering tasks based on the Figure 4: NLI model Incorrectly Predicting Neutral"
W19-5041,P19-1441,0,0.225157,"main to improve domain specific IR and QA systems. The challenge consists of three tasks which are evaluated separately. The first task is the Natural Language Inference (NLI) task which focuses on determining whether a natural language hypothesis can be inferred from a natural language premise. The second task is to recognize question entailment (RQE) between a pair of questions. The third task is to filter and improve the ranking of automatically retrieved answers. For the NLI and RQE tasks, we use transfer learning on prevalent pre-trained models like BERT (Devlin et al., 2018) and MT-DNN (Liu et al., 2019). These models play a pivotal role to gain deeper semantic understanding of the content for the final task (filtering and re-ranking) of the challenge (Demszky et al., 2018). Besides using usual techniques for candidate answer selection and re-ranking, we use features obtained from NLI and RQE models. We majorly concentrate on the novel multi-task approach in this paper. We also succinctly describe our NLI and RQE models and their performance on the final leaderboard. Parallel deep learning architectures like finetuned BERT and MT-DNN, have quickly become the state of the art, bypassing previo"
W19-5041,P14-5010,0,0.00250207,"delineate their medical issues, which are then paraphrased as short queries by medical experts. The user queries are treated as CHQs whereas the paraphrased queries are treated as FAQs. We extract 9,958 positive examples and generate an equal number of negative examples by random sampling. The average CHQ length is 180 tokens whereas the average FAQ length is 11 tokens. In addition, the expert answers are used to augment the MediQUAD corpus (Ben Abacha and Demner-Fushman, 2019). 4 answers having “Updated by:” are removed. A coreference resolution is run on each answer using Stanford CoreNLP (Manning et al., 2014) and all the entity-mentions are replaced with their corresponding names. 4.3 For each question in the training set we get upto N entailing questions (along with their scores and embeddings) and answers with a threshold T for confidence using RQE module. We use this system both in the baseline and the multi-task learning system. The complete process is highlighted in Figure 1. 4.4 We use pretrained RQE and NLI modules as feature extractors to compute best entailed questions and best candidate answers in our proposed pipeline. 1. Answer Source (One-hot) 2. Answer Length In Sentences 3. ChiQA Ra"
W19-5041,D18-1187,0,0.346482,"been much work in using NLI and RQE systems in tandem for the tasks of filtering and re-ranking. Dataset & Evaluation The dataset for re-ranking and filtering has been provided by the MediQA Shared task (Ben Abacha et al., 2019) in ACL-BioNLP 2019 workshop. It consists of medical questions and their associated answers retrieved by CHiQA 2 . The training dataset consists of 208 questions while the validation and test datasets have 25 and 150 questions respectivley. Each question has upto 10 candidate answers, with each answer having the following attributes : 1. SystemRank: rank. As noted in (Romanov and Shivade, 2018), the task of Natural Language Inference is not domain agnostic, and thus is not able to transfer well to other domains. The authors use a gradient boosting classifier (Mason et al., 2000) with a variety of hand crafted features for baselines. They then use Infersent (Conneau et al., 2017) as a sentence encoder. The paper also reports results on the ESIM Model (Chen et al., 2017) but with no visible improvements. They also discuss transfer learning and external knowledge based methods. It corresponds to CHiQA’s 2. ReferenceRank: It corresponds to the correct rank. 3. ReferenceScore: This is an"
W19-5048,W19-5039,0,0.214687,"fic resources. Inspired by their observations, we explore several techniques of augmenting domain-specific features with the state-of-the-art methods. We hope that the deep neural networks will help the model learn about the task itself and the domain-specific features will assist the model in tacking the issues associated with such specialized domains. For instance, the medical domain has a distinct sublanguage (Friedman et al., 2002) and it presents challenges such as abbreviations, inconsistent spellings, relationship between drugs, diseases, symptoms. Introduction The ACL-BioNLP 2019 (Ben Abacha et al., 2019) shared task focuses on improving the following three tasks for medical domain: 1) Natural Language Inference (NLI) 2) Recognizing Question Entailment (RQE) and 3) Question-Answering reranking system. Our team has made submissions to all the three tasks. We note that in this work we focus more on the task 1 and task 2 as improvements in these two tasks reflect directly on the task 3. However, as per the shared task guidelines, we do submit one model for the task 3 to complete our submission. Our approach for both task 1 and task 2 is based on the state-of-the-art natural language understanding"
W19-5048,D15-1075,0,0.0639317,"hekhar Bannihatti Kumar ∗ Ashwin Srinivasan∗ Aditi Chaudhary∗ James Route Teruko Mitamura Eric Nyberg {vbkumar, ashwinsr, aschaudh, jroute, teruko, ehn}@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Abstract the efficacy of learning universal language representations in providing a decent warm start to a task-specific model, by leveraging large amounts of unlabeled data. MT-DNN uses BERT as the encoder and uses MTL to fine-tune the multiple taskspecific layers. This model has obtained stateof-the-art results on several natural language understanding tasks such as SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018) and hence forms the basis of our approach. For the task 3, we use a simple model to combine the task 1 and task 2 models as shown in §2.5. This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our resu"
W19-5048,P19-1441,0,0.213043,"unlabeled data. MT-DNN uses BERT as the encoder and uses MTL to fine-tune the multiple taskspecific layers. This model has obtained stateof-the-art results on several natural language understanding tasks such as SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018) and hence forms the basis of our approach. For the task 3, we use a simple model to combine the task 1 and task 2 models as shown in §2.5. This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019 shared task on Textual Inference and Question Entailment in the Medical Domain. Our system is based on the prior work Liu et al. (2019) which uses a multi-task objective function for textual entailment. In this work, we explore different strategies for generalizing state-of-the-art language understanding models to the specialized medical domain. Our results on the shared task demonstrate that incorporating domain knowledge through data augmentation is a powerful strategy for addressing challenges posed by specialized domains such as medicine. 1 As discussed above, state-of-the-art models using deep neural networks have shown significant performance gains across various natural language processing (NLP) tasks. However, their g"
W19-5048,N18-1202,0,0.0561994,"ts in these two tasks reflect directly on the task 3. However, as per the shared task guidelines, we do submit one model for the task 3 to complete our submission. Our approach for both task 1 and task 2 is based on the state-of-the-art natural language understanding model MT-DNN (Liu et al., 2019), which combines the strength of multi-task learning (MTL) and language model pre-training. MTL in deep networks has shown performance gains when related tasks are trained together resulting in better generalization to new domains (Ruder, 2017). Recent works such as BERT (Devlin et al., 2018), ELMO (Peters et al., 2018) have shown ∗ Our resulting models perform fairly on the unseen test data of the ACL-MediQA shared task. On Task 1, our best model achieves +14.1 gain above the baseline. On Task 2, our five-model ensemble achieved +12.6 gain over the baseline and for Task 3 our model achieves a a +4.9 gain. equal contribution 453 Proceedings of the BioNLP 2019 workshop, pages 453–461 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 2 Approach Entailment Contradiction Neutral In this section, we first present our base model MT-DNN (Liu et al., 2019) which we use for both Task 1"
W19-5048,D18-1187,0,0.0772083,"Missing"
W19-5049,W14-3405,0,0.0299029,"5.0 75.2 75.0 Q1 (CHQ): Hello doctor, I do not have a white half moon on my nails. Is there any thyroid issue? If yes, please suggest some treatment.” Q2 (FAQ): Does the absence of the white half moon on nails indicate a thyroid problem? Gold Label: True Table 2: Baseline precision, recall and F1 values for RQE 5 Proposed Approach 5.1 Additional Datasets 5.1.4 Our hypothesis is that these parallel datasets will help our multi-task neural model capture salient biomedical features to help our main NLI and RQE tasks. 5.1.1 The dataset released by the Genetic and Rare diseases information center (Roberts et al., 2014) allows our model to learn question type information necessary for the RQE task. It contains 3137 questions each of which has one of 13 unique labels. Since the question type is an important handcrafted feature while considering traditional ML approaches for the RQE task, we use this dataset so that our multi-task model can leverage this information. The merit of this approach is shown in Table 3. PubMed RCT The Pubmed RCT dataset contains 2.3m sentences from 200k PubMed abstracts of randomized controlled trial (RCT) articles. We use the smaller subset of the sentences from 20k abstracts. The"
W19-5049,D18-1187,0,0.1053,"Missing"
W19-5049,D17-1070,0,0.0646386,"2.2 Biomedical Textual Inference The initial approaches for predicting inference relations between two sentences in the medical domain involved several neural architectures. (Ro∗ 1 Related Work *denotes equal contribution https://www.nih.gov/ 462 Proceedings of the BioNLP 2019 workshop, pages 462–470 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics 3.0.2 manov and Shivade, 2018) details the curation of the MedNLI dataset, and describes multiple baseline approaches. A Feature-based, Bag-of-Words (BOW), the ESIM model (Chen et al., 2016) and the InferSent model (Conneau et al., 2017) being among them. 2.3 RQE The RQE dataset comprises of consumer health questions (CHQs) received by the National Library of Medicine and frequently asked questions (FAQs) collected from the National Institutes of Health (NIH) websites (Ben Abacha and DemnerFushman, 2017). Biomedical Question Entailment • Training Set: 8,588 medical question pairs The initial work (Ben Abacha and DemnerFushman, 2017), in addition to creating the working dataset for RQE, uses handcrafted lexical and semantic features as an input to traditional machine learning models like SVM, Logistic Regression, and Naive Bay"
W19-5049,P19-1441,0,0.0604767,"Missing"
W89-0225,E89-1010,1,0.254902,"Missing"
X93-1020,P81-1022,0,0.030458,"Missing"
X93-1020,J92-1001,0,0.0528582,"Missing"
X93-1020,H90-1005,0,0.0653222,"Missing"
X93-1020,M92-1008,1,0.86767,"Missing"
X93-1020,P83-1017,0,0.0998563,"Missing"
