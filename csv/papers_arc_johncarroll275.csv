2020.lrec-1.638,{E}nglish Recipe Flow Graph Corpus,2020,-1,-1,3,0,17725,yoko yamakata,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present an annotated corpus of English cooking recipe procedures, and describe and evaluate computational methods for learning these annotations. The corpus consists of 300 recipes written by members of the public, which we have annotated with domain-specific linguistic and semantic structure. Each recipe is annotated with (1) {`}recipe named entities{'} (r-NEs) specific to the recipe domain, and (2) a flow graph representing in detail the sequencing of steps, and interactions between cooking tools, food ingredients and the products of intermediate steps. For these two kinds of annotations, inter-annotator agreement ranges from 82.3 to 90.5 F1, indicating that our annotation scheme is appropriate and consistent. We experiment with producing these annotations automatically. For r-NE tagging we train a deep neural network NER tool; to compute flow graphs we train a dependency-style parsing procedure which we apply to the entire sequence of r-NEs in a recipe.In evaluations, our systems achieve 71.1 to 87.5 F1, demonstrating that our annotation scheme is learnable."
C16-1082,Using Linguistic Data for {E}nglish and {S}panish Verb-Noun Combination Identification,2016,18,1,6,0,16505,uxoa inurrieta,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We present a linguistic analysis of a set of English and Spanish verb+noun combinations (VNCs), and a method to use this information to improve VNC identification. Firstly, a sample of frequent VNCs are analysed in-depth and tagged along lexico-semantic and morphosyntactic dimensions, obtaining satisfactory inter-annotator agreement scores. Then, a VNC identification experiment is undertaken, where the analysed linguistic data is combined with chunking information and syntactic dependencies. A comparison between the results of the experiment and the results obtained by a basic detection method shows that VNC identification can be greatly improved by using linguistic information, as a large number of additional occurrences are detected with high precision."
W14-3411,Chunking Clinical Text Containing Non-Canonical Language,2014,20,3,2,0,6011,aleksandar savkov,Proceedings of {B}io{NLP} 2014,0,"Free text notes typed by primary care physicians during patient consultations typically contain highly non-canonical language. Shallow syntactic analysis of free text notes can help to reveal valuable information for the study of disease and treatment. We present an exploratory study into chunking such text using off-the-shelf language processing tools and pre-trained statistical models. We evaluate chunking accuracy with respect to part-of-speech tagging quality, choice of chunk representation, and breadth of context features. Our results indicate that narrow context feature windows give the best results, but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy."
P14-1058,Learning to Predict Distributions of Words Across Domains,2014,36,15,3,0.790923,9798,danushka bollegala,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Although the distributional hypothesis has been applied successfully in many natural language processing tasks, systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the wordxe2x80x99s predominant meaning changes. However, if it were possible to predict how the distribution of a word changes from one domain to another, the predictions could be used to adapt a system trained in one domain to work in another. We propose an unsupervised method to predict the distribution of a word in one domain, given its distribution in another domain. We evaluate our method on two tasks: cross-domain partof-speech tagging and cross-domain sentiment classification. In both tasks, our method significantly outperforms competitive baselines and returns results that are statistically comparable to current stateof-the-art methods, while requiring no task-specific customisations."
R13-1045,Unsupervised Induction of {A}rabic Root and Pattern Lexicons using Machine Learning,2013,15,1,2,0,41315,bilal khaliq,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text."
I13-1137,Induction of Root and Pattern Lexicon for Unsupervised Morphological Analysis of {A}rabic,2013,11,1,2,0,41315,bilal khaliq,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"We propose an unsupervised approach to learning non-concatenative morphology, which we apply to induce a lexicon of Arabic roots and pattern templates. The approach is based on the idea that roots and patterns may be revealed through mutually recursive scoring based on hypothesized pattern and root frequencies. After a further iterative refinement stage, morphological analysis with the induced lexicon achieves a root identification accuracy of over 94%. Our approach differs from previous work on unsupervised learning of Arabic morphology in that it is applicable to naturally-written, unvowelled text."
P11-1014,Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification,2011,20,80,3,0.790923,9798,danushka bollegala,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products."
J11-3004,Dependency Parsing Schemata and Mildly Non-Projective Dependency Parsing,2011,67,19,2,0.625,2685,carlos gomezrodriguez,Computational Linguistics,0,"We introduce dependency parsing schemata, a formal framework based on Sikkel's parsing schemata for constituency parsers, which can be used to describe, analyze, and compare dependency parsing algorithms. We use this framework to describe several well-known projective and non-projective dependency parsers, build correctness proofs, and establish formal relationships between them. We then use the framework to define new polynomial-time parsing algorithms for various mildly non-projective dependency formalisms, including well-nested structures with their gap degree bounded by a constant k in time O(n52k), and a new class that includes all gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k) in time O(n43k). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism."
J10-1007,Book Review: Dependency Parsing by Sandra K{\\\u}bler, Ryan {M}c{D}onald," and Joakim {N}ivre""",2010,-1,-1,1,1,17923,john carroll,Computational Linguistics,0,None
N09-2059,Estimating and Exploiting the Entropy of Sense Distributions,2009,14,8,4,0,15069,peng jin,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"Word sense distributions are usually skewed. Predicting the extent of the skew can help a word sense disambiguation (WSD) system determine whether to consider evidence from the local context or apply the simple yet effective heuristic of using the first (most frequent) sense. In this paper, we propose a method to estimate the entropy of a sense distribution to boost the precision of a first sense heuristic by restricting its application to words with lower entropy. We show on two standard datasets that automatic prediction of entropy can increase the performance of an automatic first sense heuristic."
E09-1034,Parsing Mildly Non-Projective Dependency Structures,2009,28,21,3,0.833333,2685,carlos gomezrodriguez,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present parsing algorithms for various mildly non-projective dependency formalisms. In particular, algorithms are presented for: all well-nested structures of gap degree at most 1, with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with gap degree bounded by any constant k; and a new class of structures with gap degree up to k that includes some ill-nested structures. The third case includes all the gap degree k structures in a number of dependency treebanks."
P08-1110,A Deductive Approach to Dependency Parsing,2008,22,11,2,0.833333,2685,carlos gomezrodriguez,Proceedings of ACL-08: HLT,1,"We define a new formalism, based on Sikkelxe2x80x99s parsing schemata for constituency parsers, that can be used to describe, analyze and compare dependency parsing algorithms. This"
andersen-etal-2008-bnc,The {BNC} Parsed with {RASP}4{UIMA},2008,7,26,4,0,24155,oistein andersen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We have integrated the RASP system with the UIMA framework (RASP4UIMA) and used this to parse the XML-encoded version of the British National Corpus (BNC). All original annotation is preserved, and parsing information, mainly in the form of grammatical relations, is added in an XML format. A few specific adaptations of the system to give better results with the BNC are discussed briefly. The RASP4UIMA system is publicly available and can be used to parse other corpora or document collections, and the final parsed version of the BNC will be deposited with the Oxford Text Archive."
I08-1040,Unsupervised Classification of Sentiment and Objectivity in {C}hinese Text,2008,14,61,2,0,48671,taras zagibalov,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We address the problem of sentiment andn objectivity classification of product reviewsn in Chinese. Our approach is distinctiven in that it treats both positive / negativen sentiment and subjectivity / objectivity notn as distinct classes but rather as a continuum;n we argue that this is desirable fromn the perspective of would-be customers whon read the reviews. We use novel unsupervisedn techniques, including a one-wordn 'seed' vocabulary and iterative retrainingn for sentiment processing, and a criterion ofn 'sentiment density' for determining the extentn to which a document is opinionated.n The classifier achieves up to 87% F-measuren for sentiment polarity detection."
C08-1135,Automatic Seed Word Selection for Unsupervised Sentiment Classification of {C}hinese Text,2008,17,109,2,0,48671,taras zagibalov,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"We describe and evaluate a new method of automatic seed word selection for un-supervised sentiment classification of product reviews in Chinese. The whole method is unsupervised and does not require any annotated training data; it only requires information about commonly occurring negations and adverbials. Unsupervised techniques are promising for this task since they avoid problems of domain-dependency typically associated with supervised methods. The results obtained are close to those of supervised classifiers and sometimes better, up to an F1 of 92%."
W07-2304,Modelling control in generation,2007,10,3,3,0,32465,roger evans,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"In this paper we present a view of natural language generation in which the control structure of the generator is clearly separated from the content decisions made during generation, allowing us to explore and compare different control strategies in a systematic way. Our approach factors control into two components, a 'generation tree' which maps out the relationships between different decisions, and an algorithm for traversing such a tree which determines which choices are actually made. We illustrate the approach with examples of stylistic control and automatic text revision using both generative and empirical techniques. We argue that this approach provides a useful basis for the theoretical study of control in generation, and a framework for implementing generators with a range of control strategies. We also suggest that this approach can be developed into tool for analysing and adapting control aspects of other advanced wide-coverage generation systems."
W07-2203,Semi-supervised Training of a Statistical Parser from Unlabeled Partially-bracketed Data,2007,27,8,3,1,45784,rebecca watson,Proceedings of the Tenth International Conference on Parsing Technologies,0,"We compare the accuracy of a statistical parse ranking model trained from a fully-annotated portion of the Susanne treebank with one trained from unlabeled partially-bracketed sentences derived from this treebank and from the Penn Treebank. We demonstrate that confidence-based semi-supervised techniques similar to self-training outperform expectation maximization when both are constrained by partial bracketing. Both methods based on partially-bracketed training data outperform the fully supervised technique, and both can, in principle, be applied to any statistical parser whose output is consistent with such partial-bracketing. We also explore tuning the model to a different domain and the effect of in-domain data in the semi-supervised training processes."
W07-2207,Efficiency in Unification-Based N-Best Parsing,2007,31,32,3,0,3425,yi zhang,Proceedings of the Tenth International Conference on Parsing Technologies,0,"We extend a recently proposed algorithm for n-best unpacking of parse forests to deal efficiently with (a) Maximum Entropy (ME) parse selection models containing important classes of non-local features, and (b) forests produced by unification grammars containing significant proportions of globally inconsistent analyses. The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy."
W07-1515,Annotating Expressions of Appraisal in {E}nglish,2007,41,71,3,0,35001,jonathon read,Proceedings of the Linguistic Annotation Workshop,0,"In the context of Systemic Functional Linguistics, Appraisal is a theory describing the types of language utilised in communicating emotion and opinion. Robust automatic analyses of Appraisal could contribute in a number of ways to computational sentiment analysis by: distinguishing various types of evaluation, for example affect, ethics or aesthetics; discriminating between an author's opinions and the opinions of authors referenced by the author and determining the strength of evaluations. This paper reviews the typology described by Appraisal, presents a methodology for annotating Appraisal, and the use of this to annotate a corpus of book reviews. It discusses an inter-annotator agreement study, and considers instances of systematic disagreement that indicate areas in which Appraisal may be refined or clarified. Although the annotation task is difficult, there are many instances where the annotators agree; these are used to create a gold-standard corpus for future experimentation with Appraisal."
J07-4005,Unsupervised Acquisition of Predominant Word Senses,2007,51,100,4,0.70848,9803,diana mccarthy,Computational Linguistics,0,"There has been a great deal of recent research into word sense disambiguation, particularly since the inception of the Senseval evaluation exercises. Because a word often has more than one meaning, resolving word sense ambiguity could benefit applications that need some level of semantic interpretation of language input. A major problem is that the accuracy of word sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged data available, and even the best systems, when tagging every word token in a document, perform little better than a simple heuristic that guesses the first, or predominant, sense of a word in all contexts. The success of this heuristic is due to the skewed nature of word sense distributions. Data for the heuristic can come from either dictionaries or a sample of sense-tagged data. However, there is a limited supply of the latter, and the sense distributions and predominant sense of a word can depend on the domain or source of a document. (The first sense of xe2x80x9cstarxe2x80x9d for example would be different in the popular press and scientific journals). In this article, we expand on a previously proposed method for determining the predominant sense of a word automatically from raw text. We look at a number of different data sources and parameterizations of the method, using evaluation results and error analyses to identify where the method performs well and also where it does not. In particular, we find that the method does not work as well for verbs and adverbs as nouns and adjectives, but produces more accurate predominant sense information than the widely used SemCor corpus for nouns with low coverage in that corpus. We further show that the method is able to adapt successfully to domains when using domain specific corpora as input and where the input can either be hand-labeled for domain or automatically classified."
P06-4020,The Second Release of the {RASP} System,2006,9,324,2,0,21810,ted briscoe,Proceedings of the {COLING}/{ACL} 2006 Interactive Presentation Sessions,0,"We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text. The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model. We evaluate the released version on the WSJ using a relational evaluation scheme, and describe how the new release allows users to enhance performance using (in-domain) lexical information."
P06-2006,Evaluating the Accuracy of an Unlexicalized Statistical Parser on the {PARC} {D}ep{B}ank,2006,18,58,2,0,21810,ted briscoe,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank. We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manually-constructed treebanks. This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure. The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes."
W05-1517,Efficient Extraction of Grammatical Relations,2005,17,15,2,1,45784,rebecca watson,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unification-based parser. The approach allows a node in the forest to be assigned multiple inside and outside probabilities, enabling a set of 'weighted GRs' to be computed directly from the forest. The approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted GRs, or places extra constraints on which nodes can be packed, leading to less compact forests. Our experiments demonstrate substantial increases in parser accuracy and throughput for weighted GR output."
I05-1015,High Efficiency Realization for a Wide-Coverage Unification Grammar,2005,22,70,1,1,17923,john carroll,Second International Joint Conference on Natural Language Processing: Full Papers,0,"We give a detailed account of an algorithm for efficient tactical generation from underspecified logical-form semantics, using a wide-coverage grammar and a corpus of real-world target utterances. Some earlier claims about chart realization are critically reviewed and corrected in the light of a series of practical experiments. As well as a set of algorithmic refinements, we present two novel techniques: the integration of subsumption-based local ambiguity factoring, and a procedure to selectively unpack the generation forest according to a probability distribution given by a conditional, discriminative model."
H05-1053,Domain-Specific Sense Distributions and Predominant Sense Acquisition,2005,16,66,3,1,43094,rob koeling,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Distributions of the senses of words are often highly skewed. This fact is exploited by word sense disambiguation (WSD) systems which back off to the predominant sense of a word when contextual clues are not strong enough. The domain of a document has a strong influence on the sense distribution of words, but it is not feasible to produce large manually annotated corpora for every domain of interest. In this paper we describe the construction of three sense annotated corpora in different domains for a sample of English words. We apply an existing method for acquiring predominant sense information automatically from raw text, and for our sample demonstrate that (1) acquiring such information automatically from a mixed-domain corpus is more accurate than deriving it from SemCor, and (2) acquiring it automatically from text in the same domain as the target domain performs best by a large margin. We also show that for an all words WSD task this automatic method is best focussed on words that are salient to the domain, and on words with a different acquired predominant sense in that domain compared to that acquired from a balanced corpus."
H05-1069,Word Sense Disambiguation Using Sense Examples Automatically Acquired from a Second Language,2005,18,8,2,0,46478,xinglong wang,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a novel almost-unsupervised approach to the task of Word Sense Disambiguation (WSD). We build sense examples automatically, using large quantities of Chinese text, and English-Chinese and Chinese-English bilingual dictionaries, taking advantage of the observation that mappings between words and meanings are often different in typologically distant languages. We train a classifier on the sense examples and test it on a gold standard English WSD dataset. The evaluation gives results that exceed previous state-of-the-art results for comparable systems. We also demonstrate that a little manual effort can improve the quality of sense examples, as measured by WSD accuracy. The performance of the classifier on WSD also improves as the number of training sense examples increases."
W04-0837,Using automatically acquired predominant senses for Word Sense Disambiguation,2004,10,31,4,1,9803,diana mccarthy,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The first (or predominant) sense heuristic assumes the availability of handtagged data. Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently. In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text. We evaluate on both the SENSEVAL-2 and SENSEVAL-3 English allwords data. For accurate WSD the first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough. In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor outperformed many systems in SENSEVAL-2."
P04-1036,Finding Predominant Word Senses in Untagged Text,2004,24,284,4,1,9803,diana mccarthy,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand-tagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL-2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domain-specific corpora."
atserias-etal-2004-cross,Cross-Language Acquisition of Semantic Models for Verbal Predicates,2004,11,2,7,0,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents a semantic-driven methodology for the automatic acquisition of verbal models. Our approach relies strongly on the semantic generalizations allowed by already existing resources (e.g. Domain labels, Named Entity categories, concepts in the SUMO ontology, etc). Several experiments have been carried out using comparable corpora in four languages (Italian, Spanish, Basque and English) and two domains (FINANCE and SPORT) showing that the semantic patterns acquired can be general enough to be ported from one language to the other language."
C04-1177,Automatic Identification of Infrequent Word Senses,2004,16,7,4,1,9803,diana mccarthy,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we show that an unsupervised method for ranking word senses automatically can be used to identify infrequently occurring senses. We demonstrate this using a ranking of noun senses derived from the BNC and evaluating on the sense-tagged text available in both SemCor and the SENSEVAL-2 English all-words task. We show that the method does well at identifying senses that do not occur in a corpus, and that those that are erroneously filtered but do occur typically have a lower frequency than the other senses. This method should be useful for word sense disambiguation systems, allowing effort to be concentrated on more frequent senses; it may also be useful for other tasks such as lexical acquisition. Whilst the results on balanced corpora are promising, our chief motivation for the method is for application to domain specific text. For text within a particular domain many senses from a generic inventory will be rare, and possibly redundant. Since a large domain specific corpus of sense annotated data is not available, we evaluate our method on domain-specific corpora and demonstrate that sense types identified for removal are predominantly senses from outside the domain."
2004.tmi-1.2,Som {\\aa} kapp-ete med trollet? {--} Towards {MRS}-based {N}orwegian-{E}nglish machine translation,2004,11,36,6,0.526316,2623,stephan oepen,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We present a relatively large-scale initiative in high-quality MT based on semantic transfer, reviewing the motivation for this approach, general architecture and components involved, and preliminary experience from a first round of system integration (to be accompanied by a hands-on system demonstration, if appropriate)."
W03-1810,Detecting a Continuum of Compositionality in Phrasal Verbs,2003,20,153,3,1,9803,diana mccarthy,"Proceedings of the {ACL} 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment",0,"We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser. We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set. We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus."
J03-4004,"Disambiguating Nouns, Verbs, and Adjectives Using Automatically Acquired Selectional Preferences",2003,21,122,2,1,9803,diana mccarthy,Computational Linguistics,0,"Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information. We evaluate WSD using selectional preferences acquired for English adjective-noun, subject, and direct object grammatical relationships with respect to a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads. We also investigate use of the one-sense-per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word within the current document in order to increase coverage. Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage. In addition to quantifying performance, we analyze the results to investigate the situations in which the selectional preferences achieve the best precision and in which the one-sense-per-discourse heuristic increases performance."
W02-2229,Evaluation of {LTAG} Parsing with Supertag Compaction,2002,16,2,2,0,53118,olga shaumyan,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,"One of the biggest concerns that has been raised over the feasibility of using large-scale LTAGs in NLP is the amount of redundancy within a grammar?s elementary tree set. This has led to various proposals on how best to represent grammars in a way that makes them compact and easily maintained (Vijay-Shanker and Schabes, 1992; Becker, 1993; Becker, 1994; Evans, Gazdar and Weir, 1995; Candito, 1996). Unfortunately, while this work can help to make the storage of grammars more efficient, it does nothing to prevent the problem reappearing when the grammar is processed by a parser and the complete set of trees is reproduced. In this paper we are concerned with an approach that addresses this problem of computational redundancy in the trees, and evaluate its effectiveness."
W02-1304,{MEANING}: a Roadmap to Knowledge Technologies,2002,30,30,5,0,6129,german rigau,{COLING}-02: A Roadmap for Computational Linguistics,0,"Knowledge Technologies need to extract knowledge from existing texts, which calls for advanced Human Language Technologies (HLT). Progress is being made in Natural Language Processing but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. The MEANING project argues that we need to solve two complementary and intermediate tasks to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation and large-scale enrichment of Lexical Knowledge Bases. Innovations in this area will lead to HLT with deeper understanding of texts, and immediate progress in real applications of Knowledge Technologies."
briscoe-carroll-2002-robust,Robust Accurate Statistical Annotation of General Text,2002,28,251,2,0,21810,ted briscoe,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We describe a robust accurate domain-independent approach to statistical parsing incorporated into the new release of the ANLT toolkit, and publicly available as a research tool. The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts; it has also been used as a component in an open-domain question answering project. The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models. However, we plan to extend the system to improve parse coverage, depth and accuracy."
C02-1013,High Precision Extraction of Grammatical Relations,2002,25,43,1,1,17923,john carroll,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,A parsing system returning analyses in the form of sets of grammatical relations can obtain high precision if it hypothesises a particular relation only when it is certain that the relation is correct. We operationalise this technique---in a statistical parser using a manually-developed wide-coverage grammar of English---by only returning relations that form part of all analyses licensed by the grammar. We observe an increase in precision from 75% to over 90% (at the cost of a reduction in recall) on a test corpus of naturally-occurring text.
W01-1512,Using an Open-Source Unification-Based System for {CL}/{NLP} Teaching,2001,5,3,2,0,53792,anne copestake,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students.
S01-1029,Disambiguating Noun and Verb Senses Using Automatically Acquired Selectional Preferences,2001,8,22,2,1,9803,diana mccarthy,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"Our system for the Senseval-2 all words task uses automatically acquired selectional preferences to sense tag subject and object head nouns, along with the associated verbal predicates. The selectional preferences comprise probability distributions over WordNet nouns, and these distributions are conditioned on WordNet verb classes. The conditional distributions are used directly to disambiguate the head nouns. We use prior distributions and Bayes rule to compute the highest probability verb class, given a noun class. We also use anaphora resolution and the 'one sense per discourse' heuristic to cover nouns and verbs not occurring in these relationships in the target text. The selectional preferences are acquired without recourse to sense tagged data so our system is unsupervised."
P01-1015,From {RAGS} to {RICHES}: Exploiting the Potential of a Flexible Generation Architecture,2001,12,19,2,0,42348,lynne cahill,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"The RAGS proposals for generic specification of NLG systems includes a detailed account of data representation, but only an outline view of processing aspects. In this paper we introduce a modular processing architecture with a concrete implementation which aims to meet the RAGS goals of transparency and reusability. We illustrate the model with the RICHES system -- a generation system built from simple linguistically-motivated modules."
J01-4010,Book Reviews: Robustness in Language and Speech Technology,2001,4,0,1,1,17923,john carroll,Computational Linguistics,0,None
W00-2007,Engineering a Wide-Coverage Lexicalized Grammar,2000,6,6,1,1,17923,john carroll,Proceedings of the Fifth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+5),0,"We discuss a number of practical issues that have arisen in the development of a wide-coverage lexicalized grammar for English. In particular, we consider the way in which the design of the grammar and of its encoding was influenced by issues relating to the size of the grammar. 1. Introduction Hand-crafting a wide-coverage grammar is a difficult task, requiring consideration of a seemingly endless number of constructions in an attempt to produce a treatment that is as uniform and comprehensive as possible. In this paper we discuss a number of practical issues that have arisen in the development of a wide-coverage lexicalized grammar for English: the LEXSYS grammar. In particular, we consider the way in which the design of the grammar and of its encodingxe2x80x94from the viewpoint both of the grammar writer and of the parsing mechanismxe2x80x94was"
W00-1601,Efficient Large-Scale Parsing {--} a Survey,2000,21,1,1,1,17923,john carroll,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,We survey work on the empirical assessment and comparison of the efficiency of large-scale parsing systems. We focus on (1) grammars and data used to assess parser efficiency; (2) methods and tools for empirical assessment of parser efficiency; and (3) comparisons of the efficiency of different large-scale parsing systems.
W00-1427,"Robust, applied morphological generation",2000,20,76,2,0,53749,guido minnen,{INLG}{'}2000 Proceedings of the First International Conference on Natural Language Generation,0,"In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing. We present such a component: a fast and robust morphological generator for English based on finite-state techniques that generates a word form given a specification of the lemma, part-of-speech, and the type of inflection required. We describe how this morphological generator is used in a prototype system for automatic simplification of English newspaper text, and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application."
A00-2022,Ambiguity Packing in Constraint-based Parsing Practical Results,2000,14,51,2,0.526316,2623,stephan oepen,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a novel approach to 'packing' of local ambiguity in parsing with a wide-coverage HPSG grammar, and provide an empirical assessment of the interaction between various packing and parsing strategies. We present a linear-time, bidirectional subsumption test for typed feature structures and demonstrate that (a) subsumption- and equivalence-based packing is applicable to large HPSG grammars and (b) average parse complexity can be greatly reduced in bottom-up chart parsing with comprehensive HPSG implementations."
P99-1061,A Bag of Useful Techniques for Efficient and Robust Parsing,1999,13,68,3,0,23785,bernd kiefer,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,This paper describes new and improved techniques which help a unification-based parser to process input efficiently and robustly. In combination these methods result in a speed-up in parsing time of more than an order of magnitude. The methods are correct in the sense that none of them rule out legal rule applications.
E99-1029,Parsing with an Extended Domain of Locality,1999,23,7,1,1,17923,john carroll,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality (EDOL). We consider how this can be exploited to limit the need for feature structure unification during parsing. We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways."
E99-1042,Simplifying Text for Language-Impaired Readers,1999,0,102,1,1,17923,john carroll,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
W98-1114,Can Subcategorisation Probabilities Help a Statistical Parser,1998,30,55,1,1,17923,john carroll,Sixth Workshop on Very Large Corpora,0,None
W98-0108,The {L}ex{S}ys project,1998,8,4,1,1,17923,john carroll,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
J97-4007,Book Reviews: Industrial Parsing of Software Manuals,1997,0,0,1,1,17923,john carroll,Computational Linguistics,0,None
A97-1052,Automatic Extraction of Subcategorization from Corpora,1997,29,244,2,0.344034,21810,ted briscoe,Fifth Conference on Applied Natural Language Processing,0,"We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora. Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English. An initial experiment, on a sample of 14 verbs which exhibit multiple complementation patterns, demonstrates that the technique achieves accuracy comparable to previous approaches, which are all limited to a highly restricted set of subcategorization classes. We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount1."
1997.iwpt-1.6,Encoding Frequency Information in Lexicalized Grammars,1997,-1,-1,1,1,17923,john carroll,Proceedings of the Fifth International Workshop on Parsing Technologies,0,"We address the issue of how to associate frequency information with lexicalized grammar formalisms, using Lexicalized Tree Adjoining Grammar as a representative framework. We consider systematically a number of alternative probabilistic frameworks, evaluating their adequacy from both a theoretical and empirical perspective using data from existing large treebanks. We also propose three orthogonal approaches fo r backing off probability estimates to cope with the large number of parameters involved."
W96-0209,Apportioning Development Effort in a Probabilistic {LR} Parsing System Through Evaluation,1996,22,26,1,1,17923,john carroll,Conference on Empirical Methods in Natural Language Processing,0,"We describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-ofspeech and punctuation labels coupled with a probabilistic LR parser. We present evaluations of the systemxe2x80x99s performance along several different dimensions; these enable us to assess the contribution that each individual part is making to the success of the system as a whole, and thus prioritise the effort to be devoted to its further enhancement. Currently, the system is able to parse around 80% of sentences in a substantial corpus of general text containing a number of distinct genres. On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83% and 84% respectively when evaluated against manually-disambiguated analyses."
1995.iwpt-1.8,Developing and Evaluating a Probabilistic {LR} Parser of Part-of-Speech and Punctuation Labels,1995,17,53,2,0.627756,21810,ted briscoe,Proceedings of the Fourth International Workshop on Parsing Technologies,0,"We describe an approach to robust domain-independent syntactic parsing of unrestricted naturally-occurring (English) input. The technique involves parsing sequences of part-of-speech and punctuation labels using a unification-based grammar coupled with a probabilistic LR parser. We describe the coverage of several corpora using this grammar and report the results of a parsing experiment using probabilities derived from bracketed training data. We report the first substantial experiments to assess the contribution of punctuation to deriving an accurate syntactic analysis, by parsing identical texts both with and without naturally-occurring punctuation marks."
P94-1040,Relating Complexity to Practical Performance in Parsing With Wide-Coverage Unification Grammars,1994,29,37,1,1,17923,john carroll,32nd Annual Meeting of the Association for Computational Linguistics,1,"The paper demonstrates that exponential complexities with respect to grammar size and input length have little impact on the performance of three unification-based parsing algorithms, using a wide-coverage grammar. The results imply that the study and optimisation of unification-based parsing must rely on empirical data until complexity theory can more accurately predict the practical behaviour of such parsers1."
J93-1002,Generalized Probabilistic {LR} Parsing of Natural Language (Corpora) with Unification-Based Grammars,1993,68,191,2,0.627756,21810,ted briscoe,Computational Linguistics,0,"We describe work toward the construction of a very wide-coverage probabilistic parsing system for natural language (NL), based on LR parsing techniques. The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis. We discuss a fully automatic procedure for constructing an LR parse table from a unification-based grammar formalism, and consider the suitability of alternative LALR(1) parse table construction methods for large grammars. The parse table is used as the basis for two parsers; a user-driven interactive system that provides a computationally tractable and labor-efficient method of supervised training of the statistical information required to drive the probabilistic parser. The latter is constructed by associating probabilities with the LR parse table directly. This technique is superior to parsers based on probabilistic lexical tagging or probabilistic context-free grammar because it allows for a more context-dependent probabilistic language model, as well as use of a more linguistically adequate grammar formalism. We compare the performance of an optimized variant of Tomita's (1987) generalized LR parsing algorithm to an (efficiently indexed and optimized) chart parser. We report promising results of a pilot study training on 150 noun definitions from the Longman Dictionary of Contemporary English (LDOCE) and retesting on these plus a further 55 definitions. Finally, we discuss limitations of the current system and possible extensions to deal with lexical (syntactic and semantic) frequency of occurrence."
J92-3003,A Practical Approach to Multiple Default Inheritance for Unification-Based Lexicons,1992,24,34,3,1,46591,graham russell,Computational Linguistics,0,"This paper describes a unification-based lexicon system for NLP applications that incorporates mechanisms for multiple default inheritance. Such systems are intractable in the general case---the approach adopted here places a number of restrictions on the inheritance hierarchy in order to remove some of the sources of complexity while retaining more desirable properties. Implications of the design choices are discussed, comparisons are drawn with related work in computational linguistics and AI, and illustrative examples from the lexicons of German and English are given."
P91-1028,Multiple Default Inheritance in a Unification-Based Lexicon,1991,12,16,2,1,46591,graham russell,29th Annual Meeting of the Association for Computational Linguistics,1,"A formalism is presented for lexical specification in unification-based grammars which exploits defeasible multiple inheritance to express regularity, sub-regularity, and exceptions in classifying the properties of words. Such systems are in the general case intractable; the present proposal represents an attempt to reduce complexity while retaining sufficient expressive power for the task at hand. Illustrative examples are given of morphological analyses from English and German."
P90-1026,Asymmetry in Parsing and Generating with Unification Grammars: Case Studies From {ELU},1990,14,12,2,1,46591,graham russell,28th Annual Meeting of the Association for Computational Linguistics,1,"Recent developments in generation algorithms have enabled work in unification-based computational linguistics to approach more closely the ideal of grammars as declarative statements of linguistic facts, neutral between analysis and synthesis. From this perspective, however, the situation is still far from perfect; all known methods of generation impose constraints on the grammars they assume.We briefly consider a number of proposals for generation, outlining their consequences for the form of grammars, and then report on experience arising from the addition of a generator to an existing unification environment. The algorithm in question (based on that of Shieber et al. (1989)), though among the most permissive currently available, excludes certain classes of parsable analyses."
C88-1012,Software Support for Practical Grammar Development,1988,20,19,2,1,57635,bran boguraev,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Even though progress in theoretical linguistics does not necessarily rely on the construction of working programs, a large proportion of current research in syntactic theory is facilitated by suitable computational tools. However, when natural language processing applications seek to draw on the results from new developments in theories of grammar, not only the nature of the tools has to change, but they face the challenge of reconciling the seemingly contradictory requirements of notational perspicuity and efficiency of performance. In this paper, we present a comparison and an evaluation of a number of software systems for grammar development, and argue that they are inadequate as practical tools for building wide-coverage grammars. We discuss a number of factors characteristic of this task, demonstrate how they influence the design of a suitable software environment, and describe the implementation of a system which has supported efficient development of a large computational grammar of English."
P87-1027,The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of Contemporary {E}nglish,1987,8,64,3,1,57635,bran boguraev,25th Annual Meeting of the Association for Computational Linguistics,1,"We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyser and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readable source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally in-accurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary English as a suitable source dictionary for the target lexicon."
E83-1017,An Island Parsing Interpreter for the Full Augmented Transition Network Formalism,1983,3,6,1,1,17923,john carroll,First Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Island parsing is a powerful technique for parsing with Augmented Transition Networks (ATNs) which was developed and successfully applied in the HWIM speech understanding project. The HWIM application grammar did not, however, exploit Woods' original full ATN specification. This paper describes an island parsing interpreter based on HWIM, but containing substantial and important extensions to enable it to interpret any grammar which conforms to that full specification of 1970. The most important contributions have been to eliminate the need for prior specification of scope clauses, to provide more power by implementing LIFTR and SENDR actions within the island parsing framework, and to improve the efficiency of the techniques used to merge together partially-built islands within the utterance.This paper also presents some observations about island parsing, based on the use of the parser described, and some suggestions for future directions for island parsing research."
J75-3003,"Review: \\textit{ {I}nformal {S}peech: {A}lphabetic and {P}honemic {T}exts with {S}tatistical {A}nalyses and {T}ables}, by {E}dward {C}. {C}arterette and {M}argaret {H}ubbard {J}ones",1975,-1,-1,1,1,17923,john carroll,American Journal of Computational Linguistics,0,None
