1993.iwpt-1.16,P80-1024,0,0.151469,"Missing"
1993.iwpt-1.16,C69-0101,0,0.500621,"Missing"
1993.iwpt-1.16,E93-1036,1,0.883356,"w w w Action red(B � €) red(B � E) red(B � E) red(B � E) The sequence of parsing steps illustrated above does not terminate. We can find a non-terminating sequence of parsing steps for the LR(0) automaton for every hidden left recursive grammar. In fact , this is even so for the LR(k) , LALR(k), and SLR(k) automata, for any k. Hidden left recursion has been iden tified by Soisalon-Soininen - Tarhio (1988) as one of two sources, together with cyclicity, of the looping of LR parsers. Various other parsing ·techniques, such as left-corner parsing (Nederhof,· 1993a) and can cellation parsing (Nederhof, 1993b) , also suffer from this deficiency. 2.2 Eliminating epsilon rules We first discuss a method to allow LR parsing for hidden left-recursive grammars by simply performing a source to source transformation on grammars to eliminate the rules of which the right-hand sides only derive the empty string. To preserve the language, for each rule containing an occurrence of a nonfalse non terminal a copy must be added without that occurrence. Following Aho - Ullman (1972) , this transformation, called f- elim , is described below. The input grammar is called G. 1 . Let G0 be G. 190 N EDERHOF - SARBO A"
1997.iwpt-1.19,J82-3004,0,0.0487809,"Missing"
1997.iwpt-1.19,C69-0101,0,0.760445,"Missing"
1997.iwpt-1.19,P81-1022,0,0.250229,"Missing"
1997.iwpt-1.19,P96-1032,1,0.913166,"Missing"
1997.iwpt-1.19,P94-1017,1,0.903588,"Missing"
1997.iwpt-1.19,P91-1032,0,0.671639,"Missing"
1997.iwpt-1.19,P80-1024,0,0.439907,"Missing"
1997.iwpt-1.19,P94-1011,0,0.181868,"Missing"
2021.eacl-main.193,W13-3518,0,0.0194245,"his problem is unavoidable. 2275 4 L and R are akin to 0 and 1 from Kuhlmann et al. (2011). (0R (0R (0R (0R (0R (0R , 1 2 3, ∅ ) , 2 3, ∅ ) 1L 2R , 3, {(1, 2)} ) L R R 1 2 3 , , {(1, 2), (2, 3)}) 1L 2R , , {(1, 2), (2, 3)}) L 1 , , {(1, 2), (2, 3)}) 1L `SH `RA `RA `RE `RE Table 5: Arc-eager parsing is stuck in a configuration without applicable transitions. One possible fix is to add the unshift transition of Nivre and Fern´andez-Gonz´alez (2014); see also Honnibal and Johnson (2015). As this causes considerable complications to our framework, we will solve this in another way, reminiscent of Honnibal et al. (2013), which also helps to make a connection with shift-reduce parsing later. Our proposed fix is to allow a reduce even if the top of stack has label L, by means of the fifth transition of Table 4, reduce correct. This transition is not needed during training if only computations are considered that most straightforwardly correspond to gold trees, with left arc applied only on a token that is to become the left child of its parent. This may mean however that, in the case of labeled dependency parsing, the trained classifier has no basis to predict the dependency relation of the edge created by thi"
2021.eacl-main.193,E17-1051,0,0.0223013,"y arc-eager parsing is nonetheless superior for processing natural language. The first is that this earlier commitment made by right arc, in terms of the earlier creation of the dependency edge, offers additional information about the tree under construction, to better predict the next steps, using some type of classifier. The second argument in favor of arc-eager parsing is that the earlier creation of the dependency edge ensures that the partial tree under construction remains as connected as possible, which may help simultaneous syntactic and semantic processing. See Nivre (2004, 2008) and Damonte et al. (2017) for related discussions. Next we rephrase arc-eager parsing to use labels to express preconditions, to prepare us for Section 4. Note that a token is transferred from the remaining input to the stack by either shift or right arc. In the former case, it must eventually become a left child of its parent, and in the latter case, it becomes a right child. We use labels L and R for these cases.4 In a configuration with set T as third element, existence of a stack element aL implies @a0 [(a0 , a) ∈ T ] and aR implies ∃a0 [(a0 , a) ∈ T ]. We thereby obtain the first four transitions in Table 4. Toke"
2021.eacl-main.193,D15-1162,0,0.0927438,"is a L label anywhere in the stack. Assuming the classifier used for predicting the next step cannot look unboundedly deep in the stack, this problem is unavoidable. 2275 4 L and R are akin to 0 and 1 from Kuhlmann et al. (2011). (0R (0R (0R (0R (0R (0R , 1 2 3, ∅ ) , 2 3, ∅ ) 1L 2R , 3, {(1, 2)} ) L R R 1 2 3 , , {(1, 2), (2, 3)}) 1L 2R , , {(1, 2), (2, 3)}) L 1 , , {(1, 2), (2, 3)}) 1L `SH `RA `RA `RE `RE Table 5: Arc-eager parsing is stuck in a configuration without applicable transitions. One possible fix is to add the unshift transition of Nivre and Fern´andez-Gonz´alez (2014); see also Honnibal and Johnson (2015). As this causes considerable complications to our framework, we will solve this in another way, reminiscent of Honnibal et al. (2013), which also helps to make a connection with shift-reduce parsing later. Our proposed fix is to allow a reduce even if the top of stack has label L, by means of the fifth transition of Table 4, reduce correct. This transition is not needed during training if only computations are considered that most straightforwardly correspond to gold trees, with left arc applied only on a token that is to become the left child of its parent. This may mean however that, in the"
2021.eacl-main.193,P07-1022,0,0.0373322,"raightforward refinement of the algorithm by Nederhof (2019), blocking a token from becoming a left child of its parent if its label is R. Now assume the gold tree may be non-projective. For shift-reduce parsing, Nederhof (2019) presents a cubic-time algorithm for calculating σi , generalizing the procedure of Goldberg et al. (2014), which is applicable only on projective trees. The algorithm has a modular design, in terms of a generic tabular dependency parsing algorithm (Eisner and Satta, 1999), plus an explicitly ‘split’ bilexical context-free grammar (Eisner and Satta, 1999; Eisner, 2000; Johnson, 2007) that encodes computations of shift-reduce parsing. A given configuration is translated to an input string, and weights between pairs of input positions are set according to existence of edges between corresponding tokens in the gold tree. Exhaustive parsing of the string by the grammar, using an appropriate semiring, yields σi . Here we show that the same framework is applicable on arc-eager parsing. The generic tabular dependency parsing algorithm remains the same, but a new grammar is needed to encode computations of arc-eager parsing. Following Nederhof (2019), nonterminals are either sing"
2021.eacl-main.193,P99-1059,0,0.464801,"efore we opt for a more uniform framework, in which a stack element is a pair (a, A) consisting of a token a and a label A taken from a fixed set.2 To avoid clutter, we write aA in place of (a, A); this also emphasizes the relation to more traditional formulations of dependency parsing, which are obtained by omitting the superscripts. A first illustration of this is traditional shiftreduce dependency parsing, defined by the transitions in Table 1, here without labels, or alternatively, one may consider there to be only one such 2 There is a close connection to bilexical context-free grammars (Eisner and Satta, 1999), on the basis of which one may alternatively choose to refer to such a label as a ‘delexicalized stack symbol’, in a kind of lexicalized pushdown automaton. shift: (α, bβ, T ) `SH (αb, β, T ) reduce left: (αa1 a2 , β, T ) `RL (αa1 , β, T ∪ {(a1 , a2 )}) reduce right: (αa1 a2 , β, T ) `RR (αa2 , β, T ∪ {(a2 , a1 )}), if |α |&gt; 0 Table 1: Shift-reduce dependency parsing. shift: (αaC , bβ, T ) `SH (αaC bN , β, T ) complete: (αaN , β, T ) `CO (αaC , β, T ) reduce left: C RL C (αaC 1 a2 , β, T ) ` (αa1 , β, T ∪ {(a1 , a2 )}) reduce right: N RR N (αaC 1 a2 , β, T ) ` (αa2 , β, T ∪ {(a2 , a1 )}), if"
2021.eacl-main.193,N18-2062,0,0.76454,"Missing"
2021.eacl-main.193,C12-1059,0,0.801293,"om any mistakes. This is associated with the term dynamic oracle. If the parser is projective while the gold trees are non-projective moreover, then it is unavoidable that configurations be considered that do not correspond to the gold trees. Determining the desired output of the classifier requires calculation of the best next step given an arbitrary configuration and a gold tree. Typically, this is the step that allows the most accurate tree to be reached, in terms of the gold tree.1 For a gold tree that is projective, the optimal step can be determined in linear time for arc-eager parsing (Goldberg and Nivre, 2012, 2013) and for shift-reduce parsing (Nederhof, 2019). For a nonprojective gold tree, the optimal step can be determined for several types of non-projective parsers (G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; de Lhoneux et al., 1 There are alternative perspectives on how to determine the best next step; cf. Straka et al. (2015). 2017; Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2018), as well as for shift-reduce parsing (Nederhof, 2019). However, for arc-eager parsing, the problem has been unsolved until now. Aufrant et al. (2018) propose an approximation"
2021.eacl-main.193,Q13-1033,0,0.0605862,"Missing"
2021.eacl-main.193,Q14-1010,0,0.629418,"defined in terms of costs of transitions, rather than in terms of scores. We revisit this in Section 5. For normalized arc-eager parsing (Table 8) and projective gold trees, the problem appears to be no easier than for shift-reduce parsing, but can still be solved in linear time, by a straightforward refinement of the algorithm by Nederhof (2019), blocking a token from becoming a left child of its parent if its label is R. Now assume the gold tree may be non-projective. For shift-reduce parsing, Nederhof (2019) presents a cubic-time algorithm for calculating σi , generalizing the procedure of Goldberg et al. (2014), which is applicable only on projective trees. The algorithm has a modular design, in terms of a generic tabular dependency parsing algorithm (Eisner and Satta, 1999), plus an explicitly ‘split’ bilexical context-free grammar (Eisner and Satta, 1999; Eisner, 2000; Johnson, 2007) that encodes computations of shift-reduce parsing. A given configuration is translated to an input string, and weights between pairs of input positions are set according to existence of edges between corresponding tokens in the gold tree. Exhaustive parsing of the string by the grammar, using an appropriate semiring,"
2021.eacl-main.193,P11-1068,0,0.0548799,"Missing"
2021.eacl-main.193,W17-6314,0,0.589845,"Missing"
2021.eacl-main.193,1995.iwpt-1.23,0,0.802369,"hildren. Initially, shifted tokens carry label N (for ‘no restriction’). The 0 token always has label C. This results in Table 2. There is a simple one-to-one correspondence between a computation according to Table 2 and a computation according to Table 1 satisfying the left-before-right policy. The difference is merely an application of complete just before a token ceases to be a topmost stack element, either because it is reduced into the token to its left, or because another token is pushed on top. If a token 3 Shift-reduce dependency parsing has been known at least since Fraser (1989) and Nasr (1995). It is also referred to as ‘arc-standard’ parsing (Nivre, 2008). 2274 shift: (α, bβ, T ) `SH (αb, β, T ) left arc: (αa, bβ, T ) `LA (α, bβ, T ∪ {(b, a)}), if a 6= 0 ∧ @a0 [(a0 , a) ∈ T ] right arc: (αa, bβ, T ) `RA (αab, β, T ∪ {(a, b)}), if @a0 [(a0 , b) ∈ T ] reduce: (αa, β, T ) `RE (α, β, T ), if ∃a0 [(a0 , a) ∈ T ] shift: (α, bβ, T ) `SH (αbL , β, T ) left arc: (αaL , bβ, T ) `LA (α, bβ, T ∪ {(b, a)}) right arc: (αaX , bβ, T ) `RA (αaX bR , β, T ∪ {(a, b)}) reduce: R RE X (αaX 1 a2 , β, T ) ` (αa1 , β, T ) reduce correct: L RC X (αaX 1 a2 , β, T ) ` (αa1 , β, T ∪ {(a1 , a2 )}) Table 4: Re"
2021.eacl-main.193,Q19-1018,1,0.679419,"le. If the parser is projective while the gold trees are non-projective moreover, then it is unavoidable that configurations be considered that do not correspond to the gold trees. Determining the desired output of the classifier requires calculation of the best next step given an arbitrary configuration and a gold tree. Typically, this is the step that allows the most accurate tree to be reached, in terms of the gold tree.1 For a gold tree that is projective, the optimal step can be determined in linear time for arc-eager parsing (Goldberg and Nivre, 2012, 2013) and for shift-reduce parsing (Nederhof, 2019). For a nonprojective gold tree, the optimal step can be determined for several types of non-projective parsers (G´omez-Rodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; de Lhoneux et al., 1 There are alternative perspectives on how to determine the best next step; cf. Straka et al. (2015). 2017; Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2018), as well as for shift-reduce parsing (Nederhof, 2019). However, for arc-eager parsing, the problem has been unsolved until now. Aufrant et al. (2018) propose an approximation of the optimal step, based on the procedure for proj"
2021.eacl-main.193,W03-3017,0,0.310431,"2 , β, T ) ` (αa1 , β, T ∪ {(a1 , a2 )}) Table 4: Reformulated arc-eager parsing, with X ∈ {R, L}, with an extra fifth transition needed to make it work in practice. Table 3: Arc-eager parsing (Nivre, 2008, p. 525). becomes non-topmost and reappears later on top of the stack, after applications of reduce left that give it right children, then it will still have label C, which prevents it from taking further left children. Table 3 is almost verbatim the formulation of arc-eager parsing by Nivre (2008), except that we renamed symbols, and we ignore dependency relations; the formulations by e.g. Nivre (2003, 2004) and Nivre et al. (2004) are largely equivalent. It is easy to see that the condition @a0 [(a0 , b) ∈ T ] for right arc is redundant, as no tokens in the remaining input can obtain parents before they are shifted to the stack. Taking shift-reduce parsing as starting point, reduce right corresponds roughly to left arc, while the role of reduce left is only partly fulfilled by right arc, which postulates that b is a right child of a, but without removing b as yet, allowing b to take right children, and only later is that b removed by reduce. Here shift-reduce parsing would postpone the de"
2021.eacl-main.193,W04-0308,0,0.202831,"ents have been given why arc-eager parsing is nonetheless superior for processing natural language. The first is that this earlier commitment made by right arc, in terms of the earlier creation of the dependency edge, offers additional information about the tree under construction, to better predict the next steps, using some type of classifier. The second argument in favor of arc-eager parsing is that the earlier creation of the dependency edge ensures that the partial tree under construction remains as connected as possible, which may help simultaneous syntactic and semantic processing. See Nivre (2004, 2008) and Damonte et al. (2017) for related discussions. Next we rephrase arc-eager parsing to use labels to express preconditions, to prepare us for Section 4. Note that a token is transferred from the remaining input to the stack by either shift or right arc. In the former case, it must eventually become a left child of its parent, and in the latter case, it becomes a right child. We use labels L and R for these cases.4 In a configuration with set T as third element, existence of a stack element aL implies @a0 [(a0 , a) ∈ T ] and aR implies ∃a0 [(a0 , a) ∈ T ]. We thereby obtain the first"
2021.eacl-main.193,P15-2042,0,0.0395636,"Missing"
2021.eacl-main.193,D14-1099,0,0.882513,"Missing"
2021.eacl-main.193,J14-2002,0,0.0453873,"Missing"
2021.eacl-main.193,W04-2407,0,0.155914,"-eager parsing for non-projective trees Mark-Jan Nederhof School of Computer Science University of St Andrews, UK markjan.nederhof@googlemail.com Abstract It is shown that the optimal next step of an arceager parser relative to a non-projective dependency structure can be calculated in cubic time, solving an open problem in parsing theory. Applications are in training of parsers by means of a ‘dynamic oracle’. 1 Introduction A deterministic transition-based dependency parser is often driven by a classifier that determines the next step, given features extracted from the current configuration (Nivre et al., 2004). The classifier may be trained on parser configurations and steps that exactly correspond to ‘gold’ trees from a treebank. However, better accuracy is generally obtained by also including configurations reached by letting the parser stray from the gold trees, to let the classifier learn how best to recover from any mistakes. This is associated with the term dynamic oracle. If the parser is projective while the gold trees are non-projective moreover, then it is unavoidable that configurations be considered that do not correspond to the gold trees. Determining the desired output of the classifi"
2021.eacl-main.193,P17-2018,0,0.0131792,"ss S HIFT-R EDUCE conflicts” (Nivre, 2006, p. shift: (α, bβ, T ) `SH (αbL , β, T ) left arc: (αaL , bβ, T ) `LA (α, bβ, T ∪ {(b, a)}) right arc: (αaX , bβ, T ) `RA (αaX bR , β, T ) reduce: Y RE X (αaX 1 a2 , β, T ) ` (αa1 , β, T ∪ {(a1 , a2 )}) Table 6: Corrected arc-eager parsing, with X, Y ∈ {R, L}. 0 1 2 3 4 (0R (0R (0R (0R 1R 2R , 3 4, ∅ ) 1R 2R 3L , 4, ∅ ) 1R 2R , 4, {(4, 3)} ) 1R , 4, {(1, 2), (4, 3)}) `SH `LA `RE ` ... (0R (0R (0R (0R 1R 2R 1R 1R 3L 1R `RE `SH `LA ` ... or , 3 4, ∅ ) , 3 4, {(1, 2)} ) , 4, {(1, 2)} ) , 4, {(1, 2), (4, 3)}) Table 7: Same structure obtained in two ways. (Qi and Manning, 2017) suggest the shift-beforereduce policy could be slightly better. One way to enforce the reduce-before-shift policy is to opt for a different division of labor between stack and the leftmost token of the remaining input, whereby we must shift a node from remaining input to stack before it obtains its first left child or before it is decided whether it is to be a left child or a right child. Table 8 presents this normalized arc-eager parsing. The shift is now simply the transfer of a token from remaining input to stack, without making a commitment whether it is to become a left or right child, w"
2021.motra-1.10,ahrenberg-2017-comparing,0,0.134623,"g translated texts and originally written texts with high accuracy, it is difficult to use the same method to classify translation varieties, with the accuracy being barely over the chance level (Kunilovskaya and Lapshinova-Koltunski, 2019; Rubino et al., 2016). When comparing translation varieties, MT is used as a translation variety independent of HT or other translation varieties in some studies (Toral, 2019). Different from the conventional practice of MT evaluation that treats HT as the gold standard, some studies adopt a descriptive approach to comparing MT and HT (Bizzoni et al., 2020; Ahrenberg, 2017; Vanmassenhove et al., 2019). Among these studies, Bizzoni et al. (2020) find that MT shows independent patterns of translationese and it resembles HT only partly. This implies that MT may be different from HT in a systematic way, and it remains a question as to whether the deviation of MT from HT is a reliable measure of the quality of MT, and whether the current automatic metrics conflate differences between HT and MT with the quality of MT. According to research by Toral (2019), translation varieties differ in multiple ways. Based on research by Vanmassenhove et al. (2019), we focus on lex"
2021.motra-1.10,2020.iwslt-1.34,0,0.0390728,"Missing"
2021.motra-1.10,W19-8706,0,0.594385,"e gold standard, some studies adopt a descriptive approach to comparing MT and HT (Bizzoni et al., 2020; Ahrenberg, 2017; Vanmassenhove et al., 2019). Among these studies, Bizzoni et al. (2020) find that MT shows independent patterns of translationese and it resembles HT only partly. This implies that MT may be different from HT in a systematic way, and it remains a question as to whether the deviation of MT from HT is a reliable measure of the quality of MT, and whether the current automatic metrics conflate differences between HT and MT with the quality of MT. According to research by Toral (2019), translation varieties differ in multiple ways. Based on research by Vanmassenhove et al. (2019), we focus on lexical diversity in our experiments. We try to answer three questions in this study: • Can MT and HT be classified automatically with an accuracy above the chance level? • In what way does lexical diversity influence the classification result? • Are the results of automatic metrics influenced by the difference in lexical diversity between HT and MT? 2 Related Work As our study essentially involves comparing translation varieties, we present an overview of previous studies that compar"
2021.motra-1.10,D18-1512,0,0.021554,"Missing"
2021.motra-1.10,J12-4004,0,0.014464,"this term to refer to the ”fingerprints” that the source text leaves on the translated text. This notion is developed by Baker, who proposes the idea of universals of translation. As suggested by Baker et al. (1993), universals of translation are linguistic features that typically occur in translated texts as opposed to originally written texts, and these features are independent of the specific language pairs. Automatic means to distinguish translated texts and originally written texts have been developed and generally achieve high accuracy (Baroni and Bernardini, 2006; Ilisei et al., 2010; Lembersky et al., 2012; Rabinovich and Wintner, 2015). Meanwhile, computational approaches (Teich, 2003; Volansky et al., 2015) contribute evidence for some translation universals. 2.2 Comparing Translation Varieties Compared with the considerable amount of research on identifying translationese, the differences between translation varieties are less studied. Rubino et al. (2016) perform the classification between originally written texts and translations as well as between professional and student translations. They use surface features and distortion features which are inspired by quality estimation tasks, and su"
2021.motra-1.10,W19-5358,0,0.0123375,"MT is closest to HT, as shown by the column M T modf & HT modf , the MT BLEU score is the highest. When the lexical diversity of the reference is much lower than MT, as is the case in the column M T orig & HT modf , the MT BLEU score is the lowest. Much as in the discussion of the results of the trigram model, the difference in lexical diversity between MT and HT is a factor that needs to be taken into account when an n-gram matching based metric like BLEU is used for MT evaluation. The majority of automatic MT metrics developed in recent years such as BERTScore (Zhang et al., 2019) and Yisi (Lo, 2019) adopt contextualized embeddings. Based on accessibility and performance, we choose MoverScore (Zhao et al., 2019) as an example of a metric that uses BERT representations. Since MoverScore is not a corpus-level metric, we calculate the average 8 https://github.com/huggingface/transformers/blob/ 9aeacb58bab321bc21c24bbdf7a24efdccb1d426/src/ transformers/modeling bert.py 9 https://www.nltk.org/ sentence-level score. The result is presented in Table 8. M T orig & HT orig CS-EN 0.57 DE-EN 0.57 RU-EN 0.52 MoverScore M T modf & HT modf 0.56 0.56 0.50 M T orig & HT modf 0.55 0.55 0.50 Table 8: Mover"
2021.motra-1.10,P02-1040,0,0.113392,"and development, and it is claimed that MT achieves human parity in some tasks (Wu et al., 2016; Hassan et al., 2018; Popel et al., 2020). However, these statements are challenged by other researchers and remain open to debate (L¨aubli et al., 2018; Toral et al., 2018; Toral, 2020). Mark-Jan Nederhof School of Computer Science University of St Andrews KY16 9SX, UK The typical automatic approach to evaluating MT is to compare a machine translated text with a reference translation. The assumption is that the closer a machine translation is to a professional human translation, the better it is (Papineni et al., 2002). Automatic metrics for MT are developed based on this assumption. Human translation (HT) is treated as gold standard and the deviation from it is transformed into a measure of translation quality of MT. Many studies have shown that translated texts are different from originally written texts (Baroni and Bernardini, 2006; Ilisei et al., 2010). The typical method used for the identification of translationese is automatic classification of translated texts and originally written texts (Baroni and Bernardini, 2006). There are some studies that compare translation varieties such as professional an"
2021.motra-1.10,D14-1162,0,0.0911139,"st, we find rare words based on the frequency of lemmas in the corpus. Since there are many numerals and proper names and it is difficult to find meaningful candidates to replace them in the vector space, we set token.like num and token.is oov in spaCy processing3 to false. Among the remaining lemmas, those lemmas whose frequency is lower than a threshold will be considered to be rare words. We found that setting the frequency threshold to two is effective in reducing the lexical diversity. Second, we choose words whose vectors are close to the rare words from the pretrained GloVe embeddings (Pennington et al., 2014), which are computationally less expensive than contextualized word embeddings like BERT. We found that the words which are closest to the rare words are not necessarily the optimal candidates in terms of part-of-speech or meaning, and so we choose the top three most similar words for each rare word. We convert the GloVe vectors into word2vec for3 https://spacy.io mat with the gensim glove2word2vec API4 and set restrict vocab to 30000 in the most similar function5 so that the search for the most similar words is limited to the top 30000 words in the pretrained embeddings. The vocabulary size 3"
2021.motra-1.10,2020.eamt-1.39,0,0.0438318,"Missing"
2021.motra-1.10,Q15-1030,0,0.0154714,"he ”fingerprints” that the source text leaves on the translated text. This notion is developed by Baker, who proposes the idea of universals of translation. As suggested by Baker et al. (1993), universals of translation are linguistic features that typically occur in translated texts as opposed to originally written texts, and these features are independent of the specific language pairs. Automatic means to distinguish translated texts and originally written texts have been developed and generally achieve high accuracy (Baroni and Bernardini, 2006; Ilisei et al., 2010; Lembersky et al., 2012; Rabinovich and Wintner, 2015). Meanwhile, computational approaches (Teich, 2003; Volansky et al., 2015) contribute evidence for some translation universals. 2.2 Comparing Translation Varieties Compared with the considerable amount of research on identifying translationese, the differences between translation varieties are less studied. Rubino et al. (2016) perform the classification between originally written texts and translations as well as between professional and student translations. They use surface features and distortion features which are inspired by quality estimation tasks, and surprisal and complexity features"
2021.motra-1.10,2020.emnlp-main.213,0,0.0203168,"Missing"
2021.motra-1.10,N16-1110,0,0.019936,"Missing"
2021.motra-1.10,J85-1001,0,0.762798,"human translation, automatic metrics which measure the deviation of machine translation from human translation may conflate difference with quality. Our experiment with two different types of automatic metrics shows correlation with the result of the classification task. Therefore, we suggest the difference in lexical diversity between machine translation and human translation be given more attention in machine translation evaluation. 1 Introduction The initial interest in and support for machine translation (MT) stem from visions of highspeed and high-quality translation of arbitrary texts (Slocum, 1985), but machine translation proves to be more difficult than initially imagined. In recent years, progress has been made in MT research and development, and it is claimed that MT achieves human parity in some tasks (Wu et al., 2016; Hassan et al., 2018; Popel et al., 2020). However, these statements are challenged by other researchers and remain open to debate (L¨aubli et al., 2018; Toral et al., 2018; Toral, 2020). Mark-Jan Nederhof School of Computer Science University of St Andrews KY16 9SX, UK The typical automatic approach to evaluating MT is to compare a machine translated text with a refe"
2021.motra-1.10,tiedemann-2012-parallel,0,0.0148854,"metric are used to measure the differences between translations in written and spoken forms and produced by different types of MT systems. It is found that MT shows structural translationese, but the translationese of MT follows independent patterns that need further understanding. 3 Experiment We adopt two approaches for classifying MT and HT: developing a trigram language model with Witten-Bell smoothing and fine-tuning a pretrained BERT model for sequence classification from the Transformers library (Wolf et al., 2020). 3.1 Data The dataset is from the News commentary parallel corpus v13 (Tiedemann, 2012) provided in the WMT2018 shared task1 . We use Google Translate2 to obtain the corresponding machine translation. The language pairs used in the experiment, the number of sentences for each language pair and the average sentence length for HT and MT are presented in Table 1. Number of sentences CS-EN DE-EN RU-EN 30384 30345 30387 MT avg sentence length 26.33 26.61 28.00 HT avg sentence length 25.83 26.15 27.51 Table 1: Statistics of the dataset: translations from Czech, German and Russian to English. 3.2 Classifying HT and MT Trigram Model We train two trigram models on the HT and MT training"
2021.motra-1.10,W19-6627,0,0.131768,") is treated as gold standard and the deviation from it is transformed into a measure of translation quality of MT. Many studies have shown that translated texts are different from originally written texts (Baroni and Bernardini, 2006; Ilisei et al., 2010). The typical method used for the identification of translationese is automatic classification of translated texts and originally written texts (Baroni and Bernardini, 2006). There are some studies that compare translation varieties such as professional and student translations and post-edited MT (Kunilovskaya and Lapshinova-Koltunski, 2019; Toral, 2019; Popovi´c, 2020). While surface linguistic features and simple machine learning techniques are capable of classifying translated texts and originally written texts with high accuracy, it is difficult to use the same method to classify translation varieties, with the accuracy being barely over the chance level (Kunilovskaya and Lapshinova-Koltunski, 2019; Rubino et al., 2016). When comparing translation varieties, MT is used as a translation variety independent of HT or other translation varieties in some studies (Toral, 2019). Different from the conventional practice of MT evaluation that tre"
2021.motra-1.10,2020.eamt-1.20,0,0.0112024,"ranslation evaluation. 1 Introduction The initial interest in and support for machine translation (MT) stem from visions of highspeed and high-quality translation of arbitrary texts (Slocum, 1985), but machine translation proves to be more difficult than initially imagined. In recent years, progress has been made in MT research and development, and it is claimed that MT achieves human parity in some tasks (Wu et al., 2016; Hassan et al., 2018; Popel et al., 2020). However, these statements are challenged by other researchers and remain open to debate (L¨aubli et al., 2018; Toral et al., 2018; Toral, 2020). Mark-Jan Nederhof School of Computer Science University of St Andrews KY16 9SX, UK The typical automatic approach to evaluating MT is to compare a machine translated text with a reference translation. The assumption is that the closer a machine translation is to a professional human translation, the better it is (Papineni et al., 2002). Automatic metrics for MT are developed based on this assumption. Human translation (HT) is treated as gold standard and the deviation from it is transformed into a measure of translation quality of MT. Many studies have shown that translated texts are differe"
2021.motra-1.10,W18-6312,0,0.0232336,"tention in machine translation evaluation. 1 Introduction The initial interest in and support for machine translation (MT) stem from visions of highspeed and high-quality translation of arbitrary texts (Slocum, 1985), but machine translation proves to be more difficult than initially imagined. In recent years, progress has been made in MT research and development, and it is claimed that MT achieves human parity in some tasks (Wu et al., 2016; Hassan et al., 2018; Popel et al., 2020). However, these statements are challenged by other researchers and remain open to debate (L¨aubli et al., 2018; Toral et al., 2018; Toral, 2020). Mark-Jan Nederhof School of Computer Science University of St Andrews KY16 9SX, UK The typical automatic approach to evaluating MT is to compare a machine translated text with a reference translation. The assumption is that the closer a machine translation is to a professional human translation, the better it is (Papineni et al., 2002). Automatic metrics for MT are developed based on this assumption. Human translation (HT) is treated as gold standard and the deviation from it is transformed into a measure of translation quality of MT. Many studies have shown that translated tex"
2021.motra-1.10,2021.eacl-main.188,0,0.0353557,"Missing"
2021.motra-1.10,W19-6622,0,0.267045,"s and originally written texts with high accuracy, it is difficult to use the same method to classify translation varieties, with the accuracy being barely over the chance level (Kunilovskaya and Lapshinova-Koltunski, 2019; Rubino et al., 2016). When comparing translation varieties, MT is used as a translation variety independent of HT or other translation varieties in some studies (Toral, 2019). Different from the conventional practice of MT evaluation that treats HT as the gold standard, some studies adopt a descriptive approach to comparing MT and HT (Bizzoni et al., 2020; Ahrenberg, 2017; Vanmassenhove et al., 2019). Among these studies, Bizzoni et al. (2020) find that MT shows independent patterns of translationese and it resembles HT only partly. This implies that MT may be different from HT in a systematic way, and it remains a question as to whether the deviation of MT from HT is a reliable measure of the quality of MT, and whether the current automatic metrics conflate differences between HT and MT with the quality of MT. According to research by Toral (2019), translation varieties differ in multiple ways. Based on research by Vanmassenhove et al. (2019), we focus on lexical diversity in our experim"
2021.motra-1.10,D19-1053,0,0.0135513,"he lexical diversity of the reference is much lower than MT, as is the case in the column M T orig & HT modf , the MT BLEU score is the lowest. Much as in the discussion of the results of the trigram model, the difference in lexical diversity between MT and HT is a factor that needs to be taken into account when an n-gram matching based metric like BLEU is used for MT evaluation. The majority of automatic MT metrics developed in recent years such as BERTScore (Zhang et al., 2019) and Yisi (Lo, 2019) adopt contextualized embeddings. Based on accessibility and performance, we choose MoverScore (Zhao et al., 2019) as an example of a metric that uses BERT representations. Since MoverScore is not a corpus-level metric, we calculate the average 8 https://github.com/huggingface/transformers/blob/ 9aeacb58bab321bc21c24bbdf7a24efdccb1d426/src/ transformers/modeling bert.py 9 https://www.nltk.org/ sentence-level score. The result is presented in Table 8. M T orig & HT orig CS-EN 0.57 DE-EN 0.57 RU-EN 0.52 MoverScore M T modf & HT modf 0.56 0.56 0.50 M T orig & HT modf 0.55 0.55 0.50 Table 8: MoverScore result for MT. The MoverScore result in Table 8 shows a different pattern from the BLEU scores. The scores a"
A00-2036,P96-1023,0,0.0609448,"Missing"
A00-2036,P98-1035,0,0.0241458,"es can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and was employed at AT&T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN.&quot; BioInformatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sistemi ad Addestramento Automatico per l&apos;"
A00-2036,P97-1003,0,0.036103,"that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (A1shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibility of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as follows. We introduce bilexical context-free grammars in Section 2, and discuss parsing with the correctprefix property in Secti"
A00-2036,P81-1022,0,0.248991,"g. Although intuitive, the notion of left-to-right parsing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algor!thm (Earley, 1970), tabular left-corner and P L R parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al &quot; - a n , with n _&gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i,j] to denote string a i a i + l &quot; .aj; if i &gt; j, we define w[i, j] = c. Let G -- (VN,VT,P,S) be a CFG and let w = al ... an with n _&gt; 0 be some string over VT. A reco g n i z e r for the CFG class is an algorithm R that, on input (G,w), decides whether w E L(G). We say that R satisfies t"
A00-2036,P99-1059,1,0.764004,"chniques that do not require grammar precompilation cannot be directly extended to process the above mentioned grammars (resp. language models) within an acceptable time bound. The second result provides evidence that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (A1shawl, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibility of particular pairs of words in particular roles, something that was not necessarily true of"
A00-2036,1997.iwpt-1.10,0,0.0552158,"Missing"
A00-2036,P96-1032,1,0.897363,"Missing"
A00-2036,P94-1017,1,0.84454,"sing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algor!thm (Earley, 1970), tabular left-corner and P L R parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al &quot; - a n , with n _&gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i,j] to denote string a i a i + l &quot; .aj; if i &gt; j, we define w[i, j] = c. Let G -- (VN,VT,P,S) be a CFG and let w = al ... an with n _&gt; 0 be some string over VT. A reco g n i z e r for the CFG class is an algorithm R that, on input (G,w), decides whether w E L(G). We say that R satisfies the c o r r e c t - p r e f i x p r o p e r t y (CPP) if"
A00-2036,W97-0301,0,0.0247782,"ctional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 V0, and was employed at AT&T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN.&quot; BioInformatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sistemi ad"
A00-2036,P80-1024,0,0.205014,"ition. These algorithms may also consult input symbols from left to right, but the processing that takes place to the right of some position i does not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up methods, such as left-corner parsing without topdown filtering (Wiren, 1987). Algorithms that do satisfy the C P P make use of some form of top-down prediction. Top-down prediction can be implemented at parse-time as in the case of Earley&apos;s algorithm by means of the &quot;predictor&quot; step, or can be precompiled, as in the case of left-corner parsing (Rosenkrantz and Lewis, 1970), by means of the left-corner relation, or as in the case of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construction of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilation of the input grammar. Among algorithms that satisfy the CPP, the most popular example of a recognizer that does i A context-free g r a m m a r G is reduced if every nonterminal of G can be p a r t of at least one derivation t h a t rewrites the s t a r t s y m b o l into some string of termina"
A00-2036,J97-3004,0,0.253017,"Missing"
A00-2036,E87-1037,0,\N,Missing
A00-2036,C98-1035,0,\N,Missing
C04-1011,J97-2003,0,0.0259133,"ance is the entropy of the PCFG, which does not rely on the PFA. This means that if we are interested in the relative quality of different approximating PFAs with respect to a single input PCFG, the cross-entropy may be used instead of the KL distance. The constraint of determinism is not a problem in practice, as any FA can be determinized, and FAs derived by approximation algorithms are normally determinized (and minimized). As a second possible application, we now look more closely into the matter of determinization of finite-state models. Not all PFAs can be determinized, as discussed by (Mohri, 1997). This is unfortunate, as deterministic (P)FAs process input with time and space costs independent of the size of the automaton, whereas these costs are linear in the size of the automaton in the nondeterministic case, which may be too high for some real-time applications. Instead of distribution-preserving determinization, we may therefore approximate a nondeterministic PFA by a deterministic PFA whose probability distribution is close to, but not necessarily identical to, that of the first PFA. Again, an important question is how close the two models are to each other. It was argued before b"
C04-1011,W03-3016,1,0.719676,"· · · Xm ); h1 (π∩ ) = , if π∩ is (s, a, t) → a; • h2 (π∩ ) = , if π∩ is S∩ → (q0 , S, s); h2 (π∩ ) = τ , if π∩ is (s, a, t) → a and τ is a s 7→ t; h2 (π∩ ) = , if π∩ is (s0 , A, sm ) → (s0 , X1 , s1 ) · · · (sm−1 , Xm , sm ). We define h(d∩ ) = (h1 (d∩ ), h2 (d∩ )). It can be d∩ easily shown that if S∩ ⇒ w and h(d∩ ) = (d, c), d then forc the same w we have S ⇒ w and (q0 , w) ` (s, ), some s ∈ Qf . Conversely, d if for some w, d and c we have S ⇒ w and c (q0 , w) ` (s, ), some s ∈ Qf , then there is precisely one derivation d∩ such that h(d∩ ) = (d, c) d∩ and S∩ ⇒ w. As noted before by (Nederhof and Satta, 2003), this construction can be extended to apply to a PCFG Gp = (G, pG ) and an FA M . The output is a PCFG G∩,p = (G∩ , pG∩ ), where G∩ is defined as above and pG∩ is defined by: • pG∩ (S∩ → (q0 , S, s)) = 1; • pG∩ ((s0 , A, sm ) → (s0 , X1 , s1 ) ··· (sm−1 , Xm , sm )) = pG (A → X1 · · · Xm ); • pG∩ ((s, a, t) → a) = 1. Note that G∩,p is non-proper. More specifically, probabilities of rules with left-hand side S∩ or (s0 , A, sm ) might not sum to one. This is not a problem for the algorithms presented in this paper, as we have never assumed properness for our PCFGs. What is most important here i"
C04-1011,J00-1003,1,0.845886,"sis may be used to select a small subset of those that seem most promising for full syntactic processing in a next phase, thereby avoiding further computational costs for the less promising hypotheses. As FAs cannot describe structure as such, it is impractical to write the automata required for such applications by hand, and even difficult to derive them automatically by training. For this reason, the used FAs are often derived from CFGs, by means of some form of approximation. An overview of different methods of approximating CFGs by FAs, along with an experimental comparison, was given by (Nederhof, 2000). The next step is to assign probabilities to the transitions of the approximating FA, as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership. Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al., 1994). See also (Mohri and Nederhof, 2001) for discussion of relate"
C04-1011,E91-1027,0,0.0389262,"n derived from CFGs, by means of some form of approximation. An overview of different methods of approximating CFGs by FAs, along with an experimental comparison, was given by (Nederhof, 2000). The next step is to assign probabilities to the transitions of the approximating FA, as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership. Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al., 1994). See also (Mohri and Nederhof, 2001) for discussion of related ideas. An obvious question to ask is then how well the resulting PFA approximates the input PCFG, possibly for different methods of determining an FA and different ways of attaching probabilities to the transitions. Until now, any direct way of measuring the distance between a PCFG and a PFA has been lacking. As we will argue in this paper, the natural distance measure between probability distributions, the Kullback-Leible"
C04-1011,P94-1011,0,0.0419769,"y means of some form of approximation. An overview of different methods of approximating CFGs by FAs, along with an experimental comparison, was given by (Nederhof, 2000). The next step is to assign probabilities to the transitions of the approximating FA, as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership. Under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic CFG (PCFG), as shown for the special case of n-grams by (Rimon and Herz, 1991; Stolcke and Segal, 1994), or by training of the FA on a corpus generated by the PCFG (Jurafsky et al., 1994). See also (Mohri and Nederhof, 2001) for discussion of related ideas. An obvious question to ask is then how well the resulting PFA approximates the input PCFG, possibly for different methods of determining an FA and different ways of attaching probabilities to the transitions. Until now, any direct way of measuring the distance between a PCFG and a PFA has been lacking. As we will argue in this paper, the natural distance measure between probability distributions, the Kullback-Leibler (KL) distance, is diffic"
C14-1130,E91-1005,0,0.151022,"23-29 2014. non-topmost positions from the parsing stack are moved back to the buffer, input positions are effectively swapped and non-projective dependency structures arise. Tree adjoining grammars (TAGs) can describe strictly larger classes of word order phenomena than CFGs (Rambow and Joshi, 1997). TAG parsers have a time complexity of O(n6 ) (Vijay-Shankar and Joshi, 1985). However, the derived trees they generate are still continuous. Although their derivation trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic structures. It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtain adequate descriptions of scrambling phenomena. An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns a derivation tree of a (lexicalized) TAG into a non-projective dependency structure. A very similar mechanism is used to obtain non-projective dependency structures using linear context-free rewriting systems (LCFRSs) (Kuhlmann, 2013) that are lexicalized. In a LCFRS the synthesis of strings is normally specified by yield functions associated with rules. By an additional interpretation of the templ"
C14-1130,W07-1506,0,0.201049,"nt and dependency structures and see (Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1370 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages"
C14-1130,P04-1082,0,0.0873165,"Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1370 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1370–1381, Dublin, Ireland, August 23-29"
C14-1130,W11-2913,0,0.510109,"adequate description of their structure requires special care (Kathol and Pollard, 1995; M¨uller, 2004). Even for languages such as English, with a relatively rigid word order, there is a clear need for discontinuous structures (McCawley, 1982; Stucky, 1987). Early treebanks for English (Marcus et al., 1993) have often represented discontinuity in a way that makes it tempting to ignore it altogether, certainly for the purposes of parsing, whereas recent approaches tend to represent discontinuity in a more overt form, sometimes after transformation of existing treebanks (Choi and Palmer, 2010; Evang and Kallmeyer, 2011). In many modern treebanks, discontinuous structures have been given a prominent status (B¨ohmov´a et al., 2000). Classes of trees without discontinuity can be specified as the sets of parse trees of context-free grammars (CFGs). Somewhat larger classes can be specified by tree substitution grammars (Sima’an et al., 1994) and regular tree grammars (Brainerd, 1969; G´ecseg and Steinby, 1997). Practical parsers for these three formalisms have running time O(n3 ), where n is the length of the input sentence. Discontinuous structures go beyond their strong generative capacity however. Similarly, n"
C14-1130,N06-1024,0,0.0782059,", 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1370 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1370–1381, Dublin, Ireland, August 23-29 2014. non-topmost posi"
C14-1130,P09-1111,0,0.397683,"Missing"
C14-1130,N09-1061,0,0.188436,"Missing"
C14-1130,P02-1018,0,0.275389,"tures and see (Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1370 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1370–1381, Dublin, Irela"
C14-1130,P98-1106,0,0.336687,"the length of the input sentence. Discontinuous structures go beyond their strong generative capacity however. Similarly, non-projective dependency structures cannot be obtained by traditional dependency grammars. See (Rambow, 2010) for discussion of the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution"
C14-1130,W12-4613,0,0.0745483,"dency structures arise. Tree adjoining grammars (TAGs) can describe strictly larger classes of word order phenomena than CFGs (Rambow and Joshi, 1997). TAG parsers have a time complexity of O(n6 ) (Vijay-Shankar and Joshi, 1985). However, the derived trees they generate are still continuous. Although their derivation trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic structures. It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtain adequate descriptions of scrambling phenomena. An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns a derivation tree of a (lexicalized) TAG into a non-projective dependency structure. A very similar mechanism is used to obtain non-projective dependency structures using linear context-free rewriting systems (LCFRSs) (Kuhlmann, 2013) that are lexicalized. In a LCFRS the synthesis of strings is normally specified by yield functions associated with rules. By an additional interpretation of the templates of these yield functions in the algebra of dependency trees (with the overt lexical items as roots), the LCFRS generates both strings and (possibly non-pro"
C14-1130,J13-1006,0,0.584243,"icalized. In a LCFRS the synthesis of strings is normally specified by yield functions associated with rules. By an additional interpretation of the templates of these yield functions in the algebra of dependency trees (with the overt lexical items as roots), the LCFRS generates both strings and (possibly non-projective) dependency structures. However, the running time of LCFRS parsers is generally very high, still polynomial in the sentence length, but with a degree determined by properties of the grammar; difficulties involved in running LCFRS parsers for natural languages are described by (Kallmeyer and Maier, 2013). It follows from the above that there is considerable freedom in the design of parsers that produce discontinuous structures for given input sentences. One can distinguish between two main issues. The first is the formalism that guides the parsing of the input. This determines a class of input (string) languages, which can be that of the context-free languages, or tree adjoining languages, etc. We assume parsing with any of these formalisms results in derivations of some sort. The second main issue is the mechanism that translates such derivations into discontinuous structures. This leads to"
C14-1130,P95-1024,0,0.499707,"coupling of lexical elements. One part of a hybrid grammar generates linear structures, another generates hierarchical structures, and together they generate discontinuous structures. This formalizes and generalizes some existing mechanisms for dealing with discontinuous phrase structures and non-projective dependency structures. Moreover, it allows us to separate the degree of discontinuity from the time complexity of parsing. 1 Introduction Discontinuous phrases occur frequently in languages with relatively free word order, and adequate description of their structure requires special care (Kathol and Pollard, 1995; M¨uller, 2004). Even for languages such as English, with a relatively rigid word order, there is a clear need for discontinuous structures (McCawley, 1982; Stucky, 1987). Early treebanks for English (Marcus et al., 1993) have often represented discontinuity in a way that makes it tempting to ignore it altogether, certainly for the purposes of parsing, whereas recent approaches tend to represent discontinuity in a more overt form, sometimes after transformation of existing treebanks (Choi and Palmer, 2010; Evang and Kallmeyer, 2011). In many modern treebanks, discontinuous structures have bee"
C14-1130,J13-2004,0,0.0477553,"uous. Although their derivation trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic structures. It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtain adequate descriptions of scrambling phenomena. An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns a derivation tree of a (lexicalized) TAG into a non-projective dependency structure. A very similar mechanism is used to obtain non-projective dependency structures using linear context-free rewriting systems (LCFRSs) (Kuhlmann, 2013) that are lexicalized. In a LCFRS the synthesis of strings is normally specified by yield functions associated with rules. By an additional interpretation of the templates of these yield functions in the algebra of dependency trees (with the overt lexical items as roots), the LCFRS generates both strings and (possibly non-projective) dependency structures. However, the running time of LCFRS parsers is generally very high, still polynomial in the sentence length, but with a degree determined by properties of the grammar; difficulties involved in running LCFRS parsers for natural languages are d"
C14-1130,D08-1082,0,0.0557523,"∆  Γ would typically represent syntactic categories. For non-projective dependency structures, ∆ would be equal to Γ. Simple examples of discontinuous phrase structures are presented in Figures 1 and 2. 4 Basic grammatical formalisms The concept of hybrid grammars is illustrated in Section 5, by coupling a class of string grammars and a class of tree grammars. 1 Moreover, we need to avoid any confusion with the term “discontinuous tree” from (Bunt, 1996), which is characterized by the notion of “context daughter”, which is absent from our framework. The term “hybrid tree” was used before by (Lu et al., 2008), also for a mixture of a tree structure and a linear structure, generated by a probabilistic model. However, the linear ‘surface’ structure was obtained by a simple left-to-right tree traversal, whereas a meaning representation was obtained by a slightly more flexible traversal of the same tree. The emphasis in the current paper is rather on separating the linear structure from the tree structure. 1372 VP V hat hat ADV S gearbeitet schnell a a schnell gearbeitet b S b a Figure 1: Hybrid tree for German “[...] hat schnell gearbeitet” (“[...] has worked quickly”), after (Seifert and Fischer, 20"
C14-1130,J93-2004,0,0.0458543,"ting mechanisms for dealing with discontinuous phrase structures and non-projective dependency structures. Moreover, it allows us to separate the degree of discontinuity from the time complexity of parsing. 1 Introduction Discontinuous phrases occur frequently in languages with relatively free word order, and adequate description of their structure requires special care (Kathol and Pollard, 1995; M¨uller, 2004). Even for languages such as English, with a relatively rigid word order, there is a clear need for discontinuous structures (McCawley, 1982; Stucky, 1987). Early treebanks for English (Marcus et al., 1993) have often represented discontinuity in a way that makes it tempting to ignore it altogether, certainly for the purposes of parsing, whereas recent approaches tend to represent discontinuity in a more overt form, sometimes after transformation of existing treebanks (Choi and Palmer, 2010; Evang and Kallmeyer, 2011). In many modern treebanks, discontinuous structures have been given a prominent status (B¨ohmov´a et al., 2000). Classes of trees without discontinuity can be specified as the sets of parse trees of context-free grammars (CFGs). Somewhat larger classes can be specified by tree subs"
C14-1130,E06-1011,0,0.0326605,"put sentence. Discontinuous structures go beyond their strong generative capacity however. Similarly, non-projective dependency structures cannot be obtained by traditional dependency grammars. See (Rambow, 2010) for discussion of the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License."
C14-1130,P05-1013,0,0.0727159,"structures go beyond their strong generative capacity however. Similarly, non-projective dependency structures cannot be obtained by traditional dependency grammars. See (Rambow, 2010) for discussion of the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedin"
C14-1130,P09-1040,0,0.0724447,"nuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase structure is the reversible splitting conversion of (Boyd, 2007). See also (Johnson, 2002; Campbell, 2004; Gabbard et al., 2006). As shown by (Nivre, 2009), the second phase of pseudo-projective dependency parsing can be interleaved with the first, by replacing the usual one-way input tape by an additional stack, or buffer. Where This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1370 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1370–1381, Dublin, Ireland, August 23-29 2014. non-topmost positions from the parsing stac"
C14-1130,N10-1049,0,0.0285551,"´a et al., 2000). Classes of trees without discontinuity can be specified as the sets of parse trees of context-free grammars (CFGs). Somewhat larger classes can be specified by tree substitution grammars (Sima’an et al., 1994) and regular tree grammars (Brainerd, 1969; G´ecseg and Steinby, 1997). Practical parsers for these three formalisms have running time O(n3 ), where n is the length of the input sentence. Discontinuous structures go beyond their strong generative capacity however. Similarly, non-projective dependency structures cannot be obtained by traditional dependency grammars. See (Rambow, 2010) for discussion of the relation between constituent and dependency structures and see (Maier and Lichte, 2009) for a comparison of discontinuity and non-projectivity. One way to solve the above problems has been referred to as pseudo-projectivity, i.e. a parser produces a projective structure, which in a second phase is transformed into a non-projective structure (Kahane et al., 1998; McDonald and Pereira, 2006; Nivre and Nilsson, 2005). In particular, this may involve lifting, whereby one end point of a dependency link moves across a path of nodes. A related idea for discontinuous phrase stru"
C14-1130,C69-0101,0,0.568512,"synchronous rewriting. The input string language and the output tree language are thereby straightforwardly defined. Different from synchronous grammars (Shieber and Schabes, 1990; Satta and Peserico, 2005) is that occurrences of terminal symbols are also coupled. Thereby the linear order of the symbols in a derived string imposes an order on the coupled symbols in the synchronously derived tree; this allows a straightforward specification of a discontinuous structure. One can define a hybrid grammar consisting of a simple macro grammar (Fischer, 1968) and a simple context-free tree grammar (Rounds, 1970), but various other combinations of a string grammar and a tree grammar are possible as well. Due to lack of space we will here concentrate on only one kind of hybrid grammar, namely that consisting of a LCFRS as string grammar and a form of definite clause program as tree grammar. We will show that hybrid grammars that induce (finite) sets of hybrid trees can always be constructed, even if the allowable derivations are severely restricted, and we discuss experiments. Lastly, a negative result will be given, which shows that a certain linguistic phenomenon cannot be handled if the string gramm"
C14-1130,H05-1101,0,0.110729,"m derivations to discontinuous structures. Lastly, can we characterize the classes of output (tree-)languages for various combinations of input grammars and derivation-to-structure mappings? In this paper we provide one possible answer to these questions by a new type of formalism, which we call hybrid grammars. Such a grammar consists of a string grammar and a tree grammar. Derivations are coupled so as to achieve synchronous rewriting. The input string language and the output tree language are thereby straightforwardly defined. Different from synchronous grammars (Shieber and Schabes, 1990; Satta and Peserico, 2005) is that occurrences of terminal symbols are also coupled. Thereby the linear order of the symbols in a derived string imposes an order on the coupled symbols in the synchronously derived tree; this allows a straightforward specification of a discontinuous structure. One can define a hybrid grammar consisting of a simple macro grammar (Fischer, 1968) and a simple context-free tree grammar (Rounds, 1970), but various other combinations of a string grammar and a tree grammar are possible as well. Due to lack of space we will here concentrate on only one kind of hybrid grammar, namely that consis"
C14-1130,C90-3045,0,0.198211,"mally describe mappings from derivations to discontinuous structures. Lastly, can we characterize the classes of output (tree-)languages for various combinations of input grammars and derivation-to-structure mappings? In this paper we provide one possible answer to these questions by a new type of formalism, which we call hybrid grammars. Such a grammar consists of a string grammar and a tree grammar. Derivations are coupled so as to achieve synchronous rewriting. The input string language and the output tree language are thereby straightforwardly defined. Different from synchronous grammars (Shieber and Schabes, 1990; Satta and Peserico, 2005) is that occurrences of terminal symbols are also coupled. Thereby the linear order of the symbols in a derived string imposes an order on the coupled symbols in the synchronously derived tree; this allows a straightforward specification of a discontinuous structure. One can define a hybrid grammar consisting of a simple macro grammar (Fischer, 1968) and a simple context-free tree grammar (Rounds, 1970), but various other combinations of a string grammar and a tree grammar are possible as well. Due to lack of space we will here concentrate on only one kind of hybrid"
C14-1130,E12-1047,0,0.513844,"Missing"
C14-1130,P85-1011,0,0.796374,"oter are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1370 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1370–1381, Dublin, Ireland, August 23-29 2014. non-topmost positions from the parsing stack are moved back to the buffer, input positions are effectively swapped and non-projective dependency structures arise. Tree adjoining grammars (TAGs) can describe strictly larger classes of word order phenomena than CFGs (Rambow and Joshi, 1997). TAG parsers have a time complexity of O(n6 ) (Vijay-Shankar and Joshi, 1985). However, the derived trees they generate are still continuous. Although their derivation trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic structures. It was argued by (Becker et al., 1991) that further additions to TAGs are needed to obtain adequate descriptions of scrambling phenomena. An alternative is proposed by (Kallmeyer and Kuhlmann, 2012): a transformation is added that turns a derivation tree of a (lexicalized) TAG into a non-projective dependency structure. A very similar mechanism is used to obtain non-projective dependency struct"
C14-1130,P87-1015,0,0.926705,"Missing"
C14-1130,H86-1020,0,\N,Missing
C14-1130,C10-1061,0,\N,Missing
C14-1130,C98-1102,0,\N,Missing
C14-1130,W90-0102,0,\N,Missing
C98-2151,P90-1035,0,0.525239,"LR parsing for context-free grammars. The construction of derived trees and the computation of features also become straightforward. 1 Introduction The efficiency of LR(k) parsing techniques (Sippu and Soisalon-Soininen, 1990) appears to be very attractive from the perspective of natural language processing. This has stimulated the computational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing (Tomita, 1986). A first attempt to adapt LR parsing to treeadjoining grammars (TAGs) was made by Schabes and Vijay-Shanker (1990). The description was very complicated however, and not surprisingly, no implementation of the algorithm seems to have been made up to now. Apart from presentational difficulties, the algorithm as it was published is also incorrect. Brief indications of the nature of the incorrectness have been given before by Kinyon (1997). There seems to be no straightforward way to correct the algorithm. We therefore developed an alternative to the algorithm from Schabes and Vijay-Shanker (1990). This alternative is novel in presentational aspects, and is fundamentally different in that it incorporates redu"
C98-2152,J91-3004,0,0.347155,"Missing"
C98-2152,C92-2066,0,0.107344,"Missing"
C98-2152,C88-1075,0,0.099961,"Missing"
C98-2152,J95-2002,0,0.426521,"hnology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta~dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* Pr(at...aT~w). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are ,lot. q~'ee Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various"
C98-2152,W89-0211,0,0.0378846,"and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta~dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* Pr(at...aT~w). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are ,lot. q~'ee Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to enco"
D11-1112,P99-1070,0,0.221486,"ntains nonterminal A, and another contains nonterminal B, ∗ where A ⇒ uBα for some u, α, then the system for the latter component must be solved first. The solution for a system of equations such as those described above can be irrational and nonexpressible by radicals, even if we assume that all the probabilities of the rules in the input PCFG are rational numbers, as observed by Etessami and Yannakakis (2009). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al. (1999). This corresponds to the so-called fixed-point iteration method, which is well-known in the numerical calculus literature and is frequently applied to systems of non-linear equations because it can be easily implemented. When a number of standard conditions are met, each iteration of (4) adds a fixed number of bits to the precision of the solution; see Kelley (1995, Chapter 4). Since each iteration can easily be implemented to run in polynomial time, this means that we can approximate the partition function of a PCFG in polynomial time in the size of the PCFG itself and in the number of bits"
D11-1112,J98-2005,0,0.0637473,"of all strings: X p(w) = 1. w mk  X i=1 p(Ak → αk,i ) · |N | Y f (Aj ,αk,i ) zj j=1  . (2) Furthermore, for each i ≥ 1 we recursively define (i) functions gAk (z1 , z2 , . . . , z|N |) by (1) gAk (z1 , z2 , . . . , z|N |) = gAk (z1 , z2 , . . . , z|N |), (3) See (Booth and Thompson, 1973) for further discussion. In practice, PCFGs are often required to satisfy the additional condition: X p(π) = 1, and, for i ≥ 2, by for each A ∈ N . This condition is called properness. PCFGs that naturally arise by parameter estimation from corpora are generally consistent; see (S´anchez and Bened´ı, 1997; Chi and Geman, 1998). However, in what follows, neither properness nor consistency is guaranteed. We define the partition function of G as the function Z that assigns to each A ∈ N the value Using induction it is not difficult to show that, for (i) each k and i as above, gAk (0, 0, . . . , 0) is the sum of the probabilities of all complete derivations from Ak having depth not exceeding i. This implies that, for (i) i = 0, 1, 2, . . ., the sequence of the gAk (0, 0, . . . , 0) monotonically converges to Z(Ak ). For each k with 1 ≤ k ≤ |N |we can now write (i) gAk (z1 , z2 , . . . , z|N |) = gAk ( π=(A→α) Z(A) = X"
D11-1112,J99-1004,0,0.0552586,"lete derivations from Ak having depth not exceeding i. This implies that, for (i) i = 0, 1, 2, . . ., the sequence of the gAk (0, 0, . . . , 0) monotonically converges to Z(Ak ). For each k with 1 ≤ k ≤ |N |we can now write (i) gAk (z1 , z2 , . . . , z|N |) = gAk ( π=(A→α) Z(A) = X d,w d p(A ⇒ w). (1) Note that Z(S) = 1 means that G is consistent. More generally, in later sections we will need to compute the partition function for non-consistent PCFGs. We can characterize the partition function of a PCFG as a solution of a specific system of equations. Following the approach in (Harris, 1963; Chi, 1999), we introduce generating functions associated with the nonterminals of the grammar. For A ∈ N and α ∈ (N ∪ Σ)∗ , we write f (A, α) to denote the number of occurrences of symbol A within string α. Let N = {A1 , A2 , . . . , A|N |}. For each Ak ∈ N , let mk be the number of rules in R with left-hand side Ak , and assume some fixed order for these rules. For each i with 1 ≤ i ≤ mk , let Ak → αk,i be the i-th rule with left-hand side Ak . 1215 (4) (i−1) gA1 (z1 , z2 , . . . , z|N |), (i−1) gA2 (z1 , z2 , . . . , z|N |), . . . , (i−1) gA|N |(z1 , z2 , . . . , z|N |) ). Z(Ak ) = (i) = lim gAk (0, ."
D11-1112,J91-3004,0,0.658059,"ns in modeling of natural language syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 Giorgio Satta Dept. of Information Engineering University of Padua Italy satta@dei.unipd.it or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error cor"
D11-1112,W03-3016,1,0.684028,"he partition functions of PCFGs have been carried out in several application-oriented settings, by Wojtczak and Etessami (2007) and by Nederhof and Satta (2008), showing considerable improvements over the fixed-point iteration method. 3 Intersection of PCFG and FA It was shown by Bar-Hillel et al. (1964) that contextfree languages are closed under intersection with regular languages. Their proof relied on the construction of a new CFG out of an input CFG and an input finite automaton. Here we extend that construction by letting the input grammar be a probabilistic CFG. We refer the reader to (Nederhof and Satta, 2003) for more details. To avoid a number of technical complications, we assume the finite automaton has no epsilon transitions, and has only one final state. In the context of our use of this construction in the following sections, these restrictions are without loss of generality. Thus, a finite automaton (FA) M is represented by a 5-tuple (Σ, Q, q0 , qf , ∆), where Σ and Q are two finite sets of terminals and states, respectively, q0 is the initial state, qf is the final state, and ∆ is a a finite set of transitions, each of the form s 7→ t, where s, t ∈ Q and a ∈ Σ. A complete computation of M"
D11-1112,C92-2065,0,0.222355,"t L = {w1 , . . . , wm }. The objective is to compute: X σinfix (L, G) = p(xwy) w∈L,x,y∈Σ ∗ Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, b, c t0 c a t1 a, b, c b a b b, c a t2 t3 c t4 a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or s"
D11-1112,C92-2066,0,0.237736,"by a finite set L = {w1 , . . . , wm }. The objective is to compute: X σinfix (L, G) = p(xwy) w∈L,x,y∈Σ ∗ Again, this can be solved by first constructing a deterministic FA, which is then intersected with G. This FA can be obtained by determinizing a straightforward nondeterministic FA accepting L, or by directly constructing a deterministic FA along the lines of the Aho-Corasick algorithm (Aho and Corasick, 1975). Construction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, b, c t0 c a t1 a, b, c b a b b, c a t2 t3 c t4 a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a"
D11-1112,J95-2002,0,0.935832,"ge syntax. One such problem is the computation of prefix probabilities for PCFGs, where we are given as input a PCFG G and a string w, and we are asked to compute the probability that a sentence generated by G starts with w, that is, has w as a prefix. This quantity is defined as the possibly infinite sum of the probabilities of all strings of the form wx, for any string x over the alphabet of G. The problem of computation of prefix probabilities for PCFGs was first formulated by Persoon and Fu (1975). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word 1213 Giorgio Satta Dept. of Information Engineering University of Padua Italy satta@dei.unipd.it or part-of-speech, when a prefix of the input has already been processed, as discussed by Jelinek and Lafferty (1991). Such distributions are useful for speech recognition, where the result of the acoustic processor is represented as a lattice, and local choices must be made for a next transition. In addition, distributions for the next word are also useful for applications of word error correction, when one i"
D11-1112,P87-1015,0,0.700898,"uction of the automaton with the latter approach takes linear time. Further straightforward generalizations involve formalisms such as probabilistic tree adjoining grammars (Schabes, 1992; Resnik, 1992). The technique from Section 3 is also applicable in this case, b, c t0 c a t1 a, b, c b a b b, c a t2 t3 c t4 a Figure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac. as the construction from Bar-Hillel et al. (1964) carries over from context-free grammars to tree adjoining grammars, and more generally to the linear context-free rewriting systems of Vijay-Shanker et al. (1987). 7 Implementation We have conducted experiments with the computation of infix probabilities. The objective was to identify parts of the computation that have a high time or space demand, and that might be improved. The experiments were run on a desktop with a 3.0 GHz Pentium 4 processor. The implementation language is C++. The set-up of the experiments is similar to that in (Nederhof and Satta, 2008). A probabilistic contextfree grammar was extracted from sections 2-21 of the Penn Treebank version II. Subtrees that generated the empty string were systematically removed. The result was a CFG w"
E14-1036,N03-1016,0,0.0435389,"changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural networks instead of PCFGs. Bracketing reca"
E14-1036,P89-1018,0,0.70241,"Missing"
E14-1036,P11-1068,0,0.223022,"Missing"
E14-1036,J93-1002,0,0.339355,"parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as We propose the design of determini"
E14-1036,C10-2013,0,0.197346,"as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algorithms can be used for dependency parsing as well. The parsing steps of shift-reduce parsing with a binary grammar are in fact very close to those of many dependency parsing models. The major difference is, again, that instead of general-purpose classifiers to determine the next step, we would rely directly on a PCFG. The emphasis of this paper is on deriving the necessary equations to build several variants of deterministic shift-reduce parsers, all guided by a PCFG. W"
E14-1036,cer-etal-2010-parsing,0,0.30657,"Missing"
E14-1036,1993.iwpt-1.12,0,0.808158,"well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as We propose the design of deterministic constituent parsers that choose parser actions according to th"
E14-1036,P97-1003,0,0.362604,"ns, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural networks instead of PCFGs. Bracketing recall, precision and F-measure, are computed using evalb, with settings as in (Collins, 1997), except that punctuation was deleted.1 Table 1 reports results. A nonterminal B in the stack may occur in a small number of rules of the form A → BC. The C of one such rule is needed next in order to allow a reduction. If future input does not deliver this C, then parsing may fail. This problem becomes more severe as nonterminals become more specific, which is what happens with an increase of the number of split-merge cycles. Even more failures are introduced by removing the ability to consult the complete stack, which explains the poor results in the case of k = 1, n = 5; lower values of n l"
E14-1036,P04-1070,1,0.761532,"and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of probability distributions expressible by a LR automaton is incomparable to that expressible by a CFG (Nederhof and Satta, 2004). Second, because an LR automaton may have many more transitions than rules, more training data may be needed to accurately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as We propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar. Several variants are presented. One of these deterministically constructs a parse structure while postponing commitment to labels. We in"
E14-1036,1991.iwpt-1.18,0,0.502145,"later than when the input covered by its right-hand side has been processed. Second, the parser action that is performed at each step is the most likely one, given the left context, the lookahead, and a probability distribution over parses given by a PCFG. There are two differences with traditional LR parsing however. First, there is no explicit representation of LR states, and second, probabilities of actions are computed dynamically from a PCFG rather than retrieved as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algo"
E14-1036,W03-3011,0,0.036643,"best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural networks instead of PCFGs. Bracketing recall, precision and F-measure, are computed using evalb, with settings as in (Collins, 1997), except that punctuation was deleted.1 Table 1 reports results. A nonterminal B in the stack may occur in a small number of rules of the form A → BC. The C of one such rule is needed next in order to allow a reduction. If future input does not deliver this C, then parsing may fail. This problem becomes more severe as nonterminals become more specific, which is what happens with an increase of the number of split-merge cycles. Even more fail"
E14-1036,J08-4003,0,0.031324,"ers. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as We propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar. Several variants are presented. One of these deterministically constructs a parse structure while postponing commitment to labels. We investigate theoretical time complexities and report experiments. 1 Introduction Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2008) has attracted considerable attention, not only due to its high accuracy but also due to its small running time. The latter is often realized through determinism, i.e. for each configuration a unique next action is chosen. The action may be a shift of the next word onto the stack, or it may be the addition of a dependency link between words. Because of the determinism, the running time is often linear or close to linear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were p"
E14-1036,N06-2015,0,0.0159049,"ith ‘structural determinism’, which postpones commitment to nonterminals until the end of the input is reached. Parsers of this nature potentially run in linear time in the length of the input, but our parsers are better implemented to run in quadratic time. In terms of the grammar size, the experiments suggest that the number of rules is the dominating factor. The size of the lookahead strongly affects running time. The extra time costs of structural determinism are compensated by an increase in accuracy and a sharp decrease of the parse failures. We used the WSJ treebank from OntoNotes 4.0 (Hovy et al., 2006), with Sections 2-21 for training and the 2228 sentences of up to 40 words from Section 23 for testing. Grammars with different sizes, and in the required binary form, were extracted by using the tools from the Berkeley parser (Petrov et al., 2006), with between 1 and 6 splitmerge cycles. These tools offer a framework for handling unknown words, which we have adopted. The implementation of the parsing algorithms is in C++, running on a desktop with four 3.1GHz Intel Core i5 CPUs. The main algorithm is that of Appendix C, with lookahead k between 1 and 3, also in combination with structural det"
E14-1036,P06-1055,0,0.150375,"dratic time. In terms of the grammar size, the experiments suggest that the number of rules is the dominating factor. The size of the lookahead strongly affects running time. The extra time costs of structural determinism are compensated by an increase in accuracy and a sharp decrease of the parse failures. We used the WSJ treebank from OntoNotes 4.0 (Hovy et al., 2006), with Sections 2-21 for training and the 2228 sentences of up to 40 words from Section 23 for testing. Grammars with different sizes, and in the required binary form, were extracted by using the tools from the Berkeley parser (Petrov et al., 2006), with between 1 and 6 splitmerge cycles. These tools offer a framework for handling unknown words, which we have adopted. The implementation of the parsing algorithms is in C++, running on a desktop with four 3.1GHz Intel Core i5 CPUs. The main algorithm is that of Appendix C, with lookahead k between 1 and 3, also in combination with structural determinism (Appendix B), which is indicated here by sd. The variant that consults the stack down to bounded depth n (Appendix D) will only be reported for k = 1 and n = 5. 1 Evalb otherwise stumbles over e.g. a part of speech consisting of two single"
E14-1036,P10-1110,0,0.0814573,"ed considerable attention, not only due to its high accuracy but also due to its small running time. The latter is often realized through determinism, i.e. for each configuration a unique next action is chosen. The action may be a shift of the next word onto the stack, or it may be the addition of a dependency link between words. Because of the determinism, the running time is often linear or close to linear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). 338 Proceedings of the 14th Conference of the European Chapter of the Association for"
E14-1036,W97-0301,0,0.510968,"inear or close to linear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). 338 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338–347, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics strings of terminals, X for grammar symbols, and α, β, γ, . . . for strings of grammar symbols. For technical reasons, a CFG is often augmented by an additional rule S † → S$, where S † ∈ / N and $∈ / Σ. The symbol $ acts as an end-of-sentence marker. As"
E14-1036,J91-3004,0,0.132602,"s performed at each step is the most likely one, given the left context, the lookahead, and a probability distribution over parses given by a PCFG. There are two differences with traditional LR parsing however. First, there is no explicit representation of LR states, and second, probabilities of actions are computed dynamically from a PCFG rather than retrieved as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algorithms can be used for dependency parsing as well. The parsing steps of shift-reduce parsing with a binary grammar a"
E14-1036,W04-3203,0,0.245839,"ng the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). 338 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338–347, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics strings of terminals, X for grammar symbols, and α, β, γ, . . . for strings of grammar symbols. For technical reasons, a CFG is often augmented by an additional rule S † → S$, where S † ∈ / N and $∈ / Σ. The symbol $ acts as an end-of-sentence marker. As usual, we have a (right-most) ‘derives’ relation ⇒rm , ⇒∗rm denotes derivat"
E14-1036,J01-2004,0,0.0827884,"ix A, this brings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very dif"
E14-1036,W09-3825,0,0.0244745,"ings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-co"
E14-1036,P80-1024,0,0.436147,"Missing"
E14-1036,W05-1513,0,0.12646,"that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). 338 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338–347, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics strings of terminals, X for grammar symbols, and α, β, γ, . . . for strings of grammar symbols. For technical reasons, a CFG is often augmented by an additional rule S † → S$, where S † ∈ / N and $∈ / Σ. The symbol $ acts as an end-of-sentence marker. As usual, we have a (right-most) ‘derives’ relation ⇒rm , ⇒∗rm denotes derivation in zero or more steps, and ⇒+ rm denotes derivati"
E14-1036,P13-1043,0,0.02073,"y down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g."
E14-1036,P06-2089,0,0.0230987,"gth of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configurations, one would need to multiply the values E(α, a) as in Section 3 by the probabilities of the subderivations associated with occurrences of grammar symbols in stack α. Further variants are obtained by replacing the parsing strategy. One obvious candidate is leftcorner parsing (Rosenkrantz and Lewis II, 1970), which is considerably simpler than LR parsing. The resulting algorithm would be very different from the left-corner models of e.g. (Henderson, 2003), which rely on neural ne"
E14-1036,N09-1039,0,0.0234185,"exical rules are binary, then we can easily generalize the pars5 Other variants One way to improve accuracy is to increase the size of the lookahead, beyond the current 1, comparable to the generalization from LR(1) to LR(k) parsing. The formulas are given in Appendix C. 342 Yet another variant investigates only the topmost n stack symbols when choosing the next parser action. In combination with Appendix A, this brings the time complexity down again to linear time in the length of the input string. The required changes to the formulas are given in Appendix D. There is a slight similarity to (Schuler, 2009), in that no stack elements beyond a bounded depth are considered at each parsing step, but in our case the stack can still have arbitrary height. Whereas we have concentrated on determinism in this paper, one can also introduce a limited degree of nondeterminism and allow some of the most promising configurations at each input position to compete, applying techniques such as beam search (Roark, 2001; Zhang and Clark, 2009; Zhu et al., 2013), best-first search (Sagae and Lavie, 2006), or A∗ search (Klein and Manning, 2003) in order to keep the running time low. For comparing different configur"
E14-1036,P83-1017,0,0.590771,"w deterministic, i.e. linear-time, parsing, provided parsing decisions are postponed as long as possible. This has led to (deterministic) LR(k) parsing (Knuth, 1965; Sippu and SoisalonSoininen, 1990), which is a form of shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by att"
E14-1036,J95-2002,0,0.351081,"the most likely one, given the left context, the lookahead, and a probability distribution over parses given by a PCFG. There are two differences with traditional LR parsing however. First, there is no explicit representation of LR states, and second, probabilities of actions are computed dynamically from a PCFG rather than retrieved as part of static transitions. In particular, this is unlike some other early approaches to probabilistic LR parsing such as (Ng and Tomita, 1991). The mathematical framework is reminiscent of that used to compute prefix probabilities (Jelinek and Lafferty, 1991; Stolcke, 1995). One major difference is that instead of a prefix string, we now have a stack, which does not need to be parsed. In the first instance, this seems to make our problem easier. For our purposes however, we need to add new mechanisms in order to take lookahead into consideration. It is known, e.g. from (Cer et al., 2010; Candito et al., 2010), that constituent parsing can be used effectively to achieve dependency parsing. It is therefore to be expected that our algorithms can be used for dependency parsing as well. The parsing steps of shift-reduce parsing with a binary grammar are in fact very"
E14-1036,P88-1031,0,0.626174,"shift-reduce parsing. Here the parser needs to commit to a grammar rule only after all input covered by the right-hand side of that rule has been processed, while it may consult the next k symbols (the lookahead). LR is the optimal, i.e. most deterministic, parsing strategy that has this property. Deterministic LR parsing has also been considered relevant to psycholinguistics (Shieber, 1983). Nondeterministic variants of LR(k) parsing, for use in natural language processing, have been proposed as well, some using tabulation to ensure polynomial running time in the length of the input string (Tomita, 1988; Billot and Lang, 1989). However, nondeterministic LR(k) parsing is potentially as expensive as, and possibly more expensive than, traditional tabular parsing algorithms such as CKY parsing (Younger, 1967; Aho and Ullman, 1972), as shown by for example (Shann, 1991); greater values of k make matters worse (Lankhorst, 1991). For this reason, LR parsing is sometimes enhanced by attaching probabilities to transitions (Briscoe and Carroll, 1993), which allows pruning of the search space (Lavie and Tomita, 1993). This by itself is not uncontroversial, for several reasons. First, the space of proba"
E14-1036,W05-1514,0,0.588612,"inear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial running time, were proposed by (Huang and Sagae, 2010; Kuhlmann et al., 2011). This work has influenced, and has been influenced by, similar developments in constituent parsing. The challenge here is to deterministically choose a shift or reduce action. As in the case of dependency parsing, solutions to this problem are often expressed in terms of classifiers of some kind. Common approaches involve maximum entropy (Ratnaparkhi, 1997; Tsuruoka and Tsujii, 2005), decision trees (Wong and Wu, 1999; Kalt, 2004), and support vector machines (Sagae and Lavie, 2005). 338 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 338–347, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics strings of terminals, X for grammar symbols, and α, β, γ, . . . for strings of grammar symbols. For technical reasons, a CFG is often augmented by an additional rule S † → S$, where S † ∈ / N and $∈ / Σ. The symbol $ acts as an end-of-sentence marker. As usual, we have a (right-mos"
E14-1036,W03-3023,0,0.258039,"urately estimate all parameters. The approach we propose here retains some important properties of the above work on LR parsing. First, parser actions are delayed as long as We propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar. Several variants are presented. One of these deterministically constructs a parse structure while postponing commitment to labels. We investigate theoretical time complexities and report experiments. 1 Introduction Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre, 2008) has attracted considerable attention, not only due to its high accuracy but also due to its small running time. The latter is often realized through determinism, i.e. for each configuration a unique next action is chosen. The action may be a shift of the next word onto the stack, or it may be the addition of a dependency link between words. Because of the determinism, the running time is often linear or close to linear; most of the time and space resources are spent on deciding the next parser action. Generalizations that allow nondeterminism, while maintaining polynomial runnin"
E93-1036,P81-1022,0,0.255251,"Missing"
E93-1036,C88-1075,0,0.0391533,"Missing"
E93-1036,P89-1017,0,0.485361,"Missing"
E93-1036,E91-1012,0,0.0338041,"Missing"
E93-1036,1993.iwpt-1.16,1,0.830244,"Missing"
E93-1036,P91-1013,0,0.0435762,"Missing"
E93-1036,P80-1024,0,0.641639,"Missing"
E93-1036,P91-1014,0,0.516538,"Missing"
E93-1036,P81-1001,0,0.120713,"Missing"
E93-1036,J87-1004,0,0.160649,"Missing"
E93-1036,P88-1031,0,0.0548733,"Missing"
E93-1036,E87-1037,0,0.262647,"Missing"
E93-1036,P91-1000,0,0.253172,"Missing"
E93-1036,P89-1018,0,\N,Missing
E93-1036,W90-0207,0,\N,Missing
J00-1003,W89-0229,0,0.423139,"Schimpf (1990) amounts to the parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a p u s h d o w n automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and recently by Johnson (1998). Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method. One first constructs a modified left-corner recognizer from the grammar, in the form of a p u s h d o w n automaton. The stack height is b o u n d e d by a low number; Johnson (1998) claims a suitable n u m b e r w o u l d be 5. The motivation for using the left-corner strategy is that the height of the stack maintained by a left-corner parser is already b o u n d e d by a constant in the abs"
J00-1003,T75-2001,0,0.154378,"a m m a r of palindromes. It is intuitively clear that the language is not regular: the g r a m m a r symbols to the left of the spine from the root to E &quot; c o m m u n i c a t e &quot; with those to the right of the spine. More precisely, the prefix of the input u p to the point w h e r e it meets the final n o d e c of the spine determines the suffix after that point, in such a w a y that an u n b o u n d e d quantity of symbols from the prefix need to be taken into account. A formal explanation for w h y the g r a m m a r m a y not generate a regular language relies on the following definition (Chomsky 1959b): 18 Nederhof Experiments with Regular Approximation S a S--',a S a S-->b S b S---~ ~ S a Y b S b Figure 1 Grammar of palindromes, and a parse tree. Definition A g r a m m a r is s e l f - e m b e d d i n g if there is some A E N such that A --+* c~Afl, for some a¢eandfl¢e. If a g r a m m a r is not self-embedding, this means that w h e n a section of a spine in a parse tree repeats itself, then either no g r a m m a r symbols occur to the left of that section of the spine, or no g r a m m a r symbols occur to the right. This prevents the &quot; u n b o u n d e d communication&quot; b e t w e e n the"
J00-1003,P81-1022,0,0.0748561,"finite automata. We have further i m p l e m e n t e d a p a r a m e t e r i z e d version of the RTN approximation. A state of the nondeterministic a u t o m a t o n is n o w also associated to a list H of length IHI strictly smaller than a n u m b e r d, which is the p a r a m e t e r to the method. This list represents a history of rule positions that were encountered in the computation leading to the present state. More precisely, we define an item to be an object of the form [A ~ a • fl], where A ~ aft is a rule from the grammar. These are the same objects as the &quot;dotted&quot; productions of Earley (1970). The dot indicates a position in the right-hand side. The u n p a r a m e t e r i z e d RTN m e t h o d h a d one state qI for each i t e m / , and two states qA and q~ for each nonterminal A. The p a r a m e t e r i z e d RTN m e t h o d has one state qrH for each item I and each list of items H that represents a valid history for reaching I, and two states qaH and q~H for each nonterminal A and each list of items H that represents a valid history for reaching A. Such a valid history is defined to be a list 24 Nederhof Experiments with Regular Approximation H with 0 &lt; [HI &lt; d that represents"
J00-1003,P97-1058,0,0.270928,"history of coming from the rule position given b y item [B ~ d • Ae]), in Figure 6 we n o w have three states of the form qI~ for each I -- [A ~ a • fl], as well as three states of the form qA~r and q~H&quot; The higher we choose d, the more precise the approximation is, since the histories allow the a u t o m a t o n to simulate part of the mechanism of recursion from the original grammar, and the m a x i m u m length of the histories corresponds to the n u m b e r of levels of recursion that can be simulated accurately. 4.2 Refinement of RTN Superset Approximation We rephrase the m e t h o d of Grimley-Evans (1997) as follows: First, w e construct the approximating finite a u t o m a t o n according to the u n p a r a m e t e r i z e d RTN m e t h o d above. Then an additional mechanism is introduced that ensures for each rule A --~ X1 • .. Xm separately that the list of visits to the states qo,.. • • qm satisfies some reasonable criteria: a visit to qi, with 0 &lt; i &lt; m, should be followed b y one to qi+l or q0. The latter option amounts to a nested incarnation of the rule. There is a c o m p l e m e n t a r y condition for w h a t should precede a visit to qi, with 0 &lt; i &lt; m. Since only pairs of consecu"
J00-1003,P98-1101,0,0.289964,"parameterized RTN method from Section 4.1; note that the histories from Section 4.1 in fact function as stacks, the items being the stack symbols. 31 Computational Linguistics Volume 26, Number 1 4.5 Subset Approximation through Pushdown Automata By restricting the height of the stack of a p u s h d o w n automaton, one obstructs recognition of a set of strings in the context-free language, and therefore a subset approximation results. This idea was proposed by Krauwer and des Tombe (1981), Langendoen and Langsam (1987), and Pulman (1986), and was rediscovered by Black (1989) and recently by Johnson (1998). Since the latest publication in this area is more explicit in its presentation, we will base our treatment on this, instead of going to the historical roots of the method. One first constructs a modified left-corner recognizer from the grammar, in the form of a p u s h d o w n automaton. The stack height is b o u n d e d by a low number; Johnson (1998) claims a suitable n u m b e r w o u l d be 5. The motivation for using the left-corner strategy is that the height of the stack maintained by a left-corner parser is already b o u n d e d by a constant in the absence of self-embedding. If the"
J00-1003,P98-2147,0,0.0607995,"constructs a m i n i m a l deterministic a u t o m a t o n b y repeating the following for p = 1 , . . . , m : . M a k e a c o p y of Ap. D e t e r m i n i z e a n d m i n i m i z e the copy. If it has f e w e r transitions labeled b y n o n t e r m i n a l s t h a n the original, t h e n replace Ap b y its copy. . Replace each transition in Ap of the f o r m (q, Ar, q') b y (a c o p y of) a u t o m a t o n Ar in a s t r a i g h t f o r w a r d way. This m e a n s that n e w e-transitions connect q to the start state of Ar a n d the final states of Ar t o qt. 1 The representation in Mohri and Pereira (1998) is even more compact than ours for grammars that are not self-embedding. However, in this paper we use our representation as an intermediate result in approximating an unrestricted context-free grammar, with the final objective of obtaining a single minimal deterministic automaton. For this purpose, Mohri and Pereira's representation offers little advantage. 22 Nederhof 3. Experiments with Regular Approximation Again determinize and minimize Ap and store it for later reference. The a u t o m a t o n obtained for Am after step 3 is the desired result. 4. Methods of Regular Approximation This s"
J00-1003,1997.iwpt-1.19,1,0.860634,"n the increase of the sizes of the intermediate results and the obtained minimal deterministic automaton? Second, how &quot;precise&quot; are the approximations? That is, how much larger than the original context-free language is the language obtained by a superset approximation, and how much smaller is the language obtained by a subset approximation? (How we measure the &quot;sizes&quot; of languages in a practical setting will become clear in what follows.) Some considerations with regard to theoretical upper bounds on the sizes of the intermediate results and the finite automata have already been discussed in Nederhof (1997). In this article we will try to answer the above two questions in a practical setring, using practical linguistic grammars and sentences taken from a spoken-language corpus. • DFKI,Stuhlsatzenhausweg3, D-66123 Saarbriicken,Germany.E-mail:nederhof@dfki.de © 2000 Associationfor ComputationalLinguistics Computational Linguistics Volume 26, Number 1 The structure of this p a p e r is as follows: In Section 2 w e recall some standard definitions from language theory. Section 3 investigates a sufficient condition for a context-free g r a m m a r to generate a regular language. We also present the c"
J00-1003,W98-1302,1,0.867188,"e partition of mutually recursive nonterminals after transformation. This integration makes use, for example, of the fact that for fixed Ni and fixed f , the set of nonterminals of the form A , f , with A c Ni, is (potentially) mutually right-recursive. A set of such nonterminals can therefore be treated as the corresponding case from Figure 2, assuming the value right. The full formulation of the integrated grammar transformation and construction of the finite automaton is rather long and is therefore not given here. A very similar formulation, for another grammar transformation, is given in Nederhof (1998). 30 Nederhof Experiments with Regular Approximation 4.4 Superset Approximation through Pushdown Automata The distinction between context-free languages and regular languages can be seen in terms of the distinction between pushdown automata and finite automata. Pushdown automata maintain a stack that is potentially unbounded in height, which allows more complex languages to be recognized than in the case of finite automata. Regular approximation can be achieved by restricting the height of the stack, as we will see in Section 4.5, or by ignoring the distinction between several stacks when they"
J00-1003,P96-1032,1,0.821391,"two occurrences, including one of the two occurrences. This process defines a congruence relation on stacks, with a finite number of congruence classes. This congruence relation directly defines a finite automaton: each class is translated to a unique state of the nondeterministic finite automaton, shift actions are translated to transitions labeled with terminals, and reduce actions are translated to epsilon transitions. The method has a high complexity. First, construction of an LR automaton, of which the size is exponential in the size of the grammar, may be a prohibitively expensive task (Nederhof and Satta 1996). This is, however, only a fraction of the effort needed to compute the congruence classes, of which the number is in turn exponential in the size of the LR automaton. If the resulting nondeterministic automaton is determinized, we obtain a third source of exponential behavior. The time and space complexity of the method are thereby bounded by a triple exponential function in the size of the grammar. This theoretical analysis seems to be in keeping with the high costs of applying this method in practice, as will be shown later in this article. As proposed by Pereira and Wright (1997), our impl"
J00-1003,C92-1032,0,0.00887461,"rminal in a set Ni such that recursive(Ni) = self. 4.3 Subset Approximation by Transforming the Grammar Putting restrictions on spines is another w a y to obtain a regular language. Several m e t h o d s can be defined. The first m e t h o d we present investigates spines in a v e r y detailed way. It eliminates from the language only those sentences for which a subderivation is required of the form B --~* aBfl, for some a ~ ¢ and fl ~ e. The motivation is that such sentences do not occur frequently in practice, since these subderivations make them difficult for people to c o m p r e h e n d (Resnik 1992). Their exclusion will therefore not lead to m u c h loss of coverage of typical sentences, especially for simple application domains. We express the m e t h o d in terms of a g r a m m a r transformation in Figure 7. The effect of this transformation is that a nonterminal A is tagged with a set of pairs (B, Q), where B is a nonterminal occurring higher in the spine; for any given B, at most one such pair (B, Q) can be contained in the set. The set Q m a y contain the element l to indicate that something to the left of the part of the spine from B to A 26 Nederhof Experiments with Regular Appr"
J00-1003,P80-1024,0,0.295916,"e nonterminals, the process depends on whether the nonterminals in the corresponding set from H are mutually left-recursive or right-recursive; if they are both, which means they are cyclic, then either subprocess can be applied; in the code in Figure 2 cyclic and right-recursive subsets Ni are treated uniformly. We discuss the case in which the nonterminals are left-recursive. One new state is created for each nonterminal in the set. The transitions that are created for terminals and nonterminals not in Ni are connected in a way that is reminiscent of the construction of left-corner parsers (Rosenkrantz and Lewis 1970), and specifically of one construction that focuses on sets of mutually recursive nonterminals (Nederhof 1994, Section 5.8). An example is given in Figure 3. Four states have been labeled according to the names they are given in procedure make~fa. There are two states that are labeled qB. This can be explained by the fact that nonterminal B can be reached by descending the grammar from S in two essentially distinct ways. The code in Figure 2 differs from the actual implementation in that sometimes, for a nonterminal, a separate finite automaton is constructed, namely, for those nonterminals th"
J00-1003,P94-1011,0,0.15799,"Define the set of all terminals reachable from nonterminal A to be ~A = {a I 3c~,iliA --** o~afl]}. We n o w approximate the set of strings derivable from A by G~, which is the set of strings consisting of terminals from GA. Our implementation is m a d e slightly more sophisticated by taking ~A to be {X ] 3B, c~,fl[B E Ni A B ~ oLXfl A X ~ Ni]}, for each A such that A E Ni and recursive(Ni) = self, for some i. That is, each X E ~A is a terminal, or a nonterminal not in the same set Ni as A, but immediately reachable from set Ni, through B E Ni. This m e t h o d can be generalized, inspired by Stolcke and Segal (1994), w h o derive N-gram probabilities from stochastic context-free grammars. By ignoring the probabilities, each N = 1, 2, 3 . . . . gives rise to a superset approximation that can be described as follows: The set of strings derivable from a nonterminal A is approximated by the set of strings al ... an such that • for each substring v = ai+l . . . ai+N (0 &lt; i &lt; n -- N) we have A --+* wvy, for some w and y, • for each prefix v = al . . . ai (0 &lt; i &lt; n) such that i &lt; N we have A -** vy, for some y, and • for each suffix v = ai+l ... an (0 &lt; i &lt; n) such that n - i &lt; N we have a ---~* wv, for some w"
J00-1003,C98-1098,0,\N,Missing
J00-1003,P91-1032,0,\N,Missing
J00-1003,C98-2142,0,\N,Missing
J03-1006,P89-1018,0,0.0492626,") encodes the derivation with the lowest weight allowed by G for w. Together with the dynamic programming algorithm to be discussed in the next section that finds the derivation with the lowest weight on the basis of c(G, w), we obtain a modular approach to describing weighted parsers: One part of the description specifies how to construct grammar c(G, w) out of grammar G and input w, and the second part specifies the dynamic programming algorithm to investigate c(G, w). Such a modular way of describing parsers in the unweighted case has already been fully developed in work by Lang (1974) and Billot and Lang (1989). Instead of a deduction system, they use a pushdown transducer to express a parsing strategy such as top-down parsing, left-corner parsing or LR parsing. Such a pushdown transducer can in the context of their work be regarded as specifying a context-free grammar c(G, w), given a context-free grammar G and an input string w. The second part of the description of the parser is a dynamic programming algorithm for actually constructing c(G, w) in polynomial time in the length of w. This modular approach to describing parsing algorithms is also applicable to formalisms F other than context-free gr"
J03-1006,2000.iwpt-1.8,0,0.083181,"lar approach to describing parsing algorithms is also applicable to formalisms F other than context-free grammars. For example, it was shown by VijayShanker and Weir (1993) that tree-adjoining parsing can be realized by constructing a context-free grammar c(G, w) out of a tree-adjoining grammar G and an input string w. This can straightforwardly be generalized to weighted (in particular, stochastic) tree-adjoining grammars (Schabes 1992). 3 If there is more than one goal item, then a new symbol needs to be introduced as the start symbol. 138 Nederhof Weighted Deductive Parsing It was shown by Boullier (2000) that F may furthermore be the formalism of range concatenation grammars. Since the class of range concatenation grammars generates exactly PTIME, this demonstrates the generality of the approach.4 Instead of string input, one may also consider input consisting of a finite automaton, along the lines of Bar-Hillel, Perles, and Shamir (1964); this can be trivially extended to the weighted case. That we restrict ourselves to string input in this article is motivated by presentational considerations. 3. Knuth’s Algorithm The algorithm by Dijkstra (1959) effectively finds the shortest path from a d"
J03-1006,H90-1053,0,0.0545045,"ever, involves an additional factor because 5 Note that some authors let the term “Viterbi algorithm” refer to any algorithm that computes the “Viterbi parse,” that is, the parse with the lowest weight or highest probability. 141 Computational Linguistics Volume 29, Number 1 of the maintenance of the priority queue. Following Cormen, Leiserson, and Rivest (1990), this factor is O(log( c(G, w) )), where c(G, w) is the number of nonterminals in c(G, w), which is an upper bound on the number of elements on the priority queue at any given time. Furthermore, there are observations by, for example, Chitrao and Grishman (1990), Tjong Kim Sang (1998, Sections 3.1 and 3.4), and van Noord et al. (1999, Section 3.9), that suggest that the apparent advantage of Knuth’s algorithm does not necessarily lead to significantly lower time costs in practice. In particular, consider deduction systems with items associated with spans like, for example, that in Figure 1, in which the span of the consequent of an inference rule is the concatenation of the spans of the antecedents. If weights of individual productions in G differ only slightly, as is often the case in practice, then different derivations for an item have only slight"
J03-1006,J99-4004,0,0.400792,"ontrasts with that of Klein and Manning (2001), who offer an indivisible specification for a small collection of parsing strategies for weighted contextfree grammars only, referring to a generalization of Dijkstra’s algorithm to hypergraphs by Gallo et al. (1993). This article also addresses the efficiency of Knuth’s algorithm for weighted deductive parsing, relative to the more commonly used algorithm by Viterbi. 2. Weighted Deductive Parsing The use of deduction systems for specifying parsers has been proposed by Shieber, Schabes, and Pereira (1995) and Sikkel (1997). As already remarked by Goodman (1999), deduction systems can also be extended to manipulate weights.1 Here we de∗ Faculty of Arts, Humanities Computing, University of Groningen, P.O. Box 716, NL-9700 AS Groningen, The Netherlands. E-mail: markjan@let.rug.nl. Secondary affiliation is the German Research Center for Artificial Intelligence (DFKI). 1 Weighted deduction is closely related to probabilistic logic, although the problem considered in this article (viz., finding derivations with lowest weights) is different from typical problems in probabilistic logic. For example, Frisch and Haddawy (1994) propose inference rules that man"
J03-1006,W01-1812,0,0.0660449,"o the choice of grammatical formalism and parsing strategy, and the algorithm that finds the derivation with the lowest weight, on the other. The latter is Dijkstra’s algorithm for the shortest-path problem (Dijkstra 1959) as generalized by Knuth (1977) for a problem on grammars. It has been argued by, for example, Backhouse (2001), that this algorithm can be used to solve a wide range of problems on context-free grammars. A brief presentation of a very similar algorithm for weighted deductive parsing has been given before by Eisner (2000, Figure 3.5e). Our presentation contrasts with that of Klein and Manning (2001), who offer an indivisible specification for a small collection of parsing strategies for weighted contextfree grammars only, referring to a generalization of Dijkstra’s algorithm to hypergraphs by Gallo et al. (1993). This article also addresses the efficiency of Knuth’s algorithm for weighted deductive parsing, relative to the more commonly used algorithm by Viterbi. 2. Weighted Deductive Parsing The use of deduction systems for specifying parsers has been proposed by Shieber, Schabes, and Pereira (1995) and Sikkel (1997). As already remarked by Goodman (1999), deduction systems can also be"
J03-1006,C92-2066,0,0.00920322,"ng w. The second part of the description of the parser is a dynamic programming algorithm for actually constructing c(G, w) in polynomial time in the length of w. This modular approach to describing parsing algorithms is also applicable to formalisms F other than context-free grammars. For example, it was shown by VijayShanker and Weir (1993) that tree-adjoining parsing can be realized by constructing a context-free grammar c(G, w) out of a tree-adjoining grammar G and an input string w. This can straightforwardly be generalized to weighted (in particular, stochastic) tree-adjoining grammars (Schabes 1992). 3 If there is more than one goal item, then a new symbol needs to be introduced as the start symbol. 138 Nederhof Weighted Deductive Parsing It was shown by Boullier (2000) that F may furthermore be the formalism of range concatenation grammars. Since the class of range concatenation grammars generates exactly PTIME, this demonstrates the generality of the approach.4 Instead of string input, one may also consider input consisting of a finite automaton, along the lines of Bar-Hillel, Perles, and Shamir (1964); this can be trivially extended to the weighted case. That we restrict ourselves to"
J03-1006,J95-2002,0,0.307692,"(z1 + x2 , x1 + x2 ) : [A → αB • β, i, k]  0≤i≤j≤k≤n Set of goal items is as in Figure 1. Figure 3 Alternative weighted deduction system for top-down parsing. and α is a list of zero or more terminals or nonterminals. We assume the weight of a grammar derivation is given by the sum of the weights of the occurrences of productions therein. Weights may be atomic entities, as in the deduction systems discussed above, where they are real-valued, but they may also be composed entities. For example, Figure 3 presents an alternative form of weighted top-down parsing using pairs of values, following Stolcke (1995). The first value is the forward weight, that is, the sum of weights of all productions that were encountered in the lowest-weighted derivation in the deduction system of an item [A → α • β, i, j]. The second is the inner weight; that is, it considers the weight only of the current production A → αβ plus the weights of productions in lowest-weighted grammar derivations for nonterminals in α. These inner weights are the same values as the weights in Figures 1 and 2. In fact, if we omit the forward weights, we obtain the deduction system in Figure 2. Since forward weights pertain to larger parts"
J03-1006,E93-1045,0,0.0723199,"Missing"
J05-2002,W01-1807,1,0.811341,"Missing"
J05-2002,2000.iwpt-1.8,0,0.062562,"Missing"
J05-2002,J97-2003,0,0.0265506,"model is here seen as a (P)FA that contains exactly one state for each possible history of the n − 1 previously read symbols. It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs. Another application of our work involves determinization and minimization of PFAs. As shown by Mohri (1997), PFAs cannot always be determinized, and no practical algorithms are known to minimize arbitrary nondeterministic (P)FAs. This can be a problem when deterministic or small PFAs are required. We can, however, always compute a minimal deterministic FA equivalent to an input FA. The new results in this article offer a way to extend this determinized FA to a PFA such that it approximates the probability distribution described by the input PFA as well as possible, in terms of the KL distance. Although the proposed technique has some limitations, in particular, that the model to be trained is unamb"
J05-2002,J00-1003,1,0.928028,"d for publication: 19th September 2004 © 2005 Association for Computational Linguistics Computational Linguistics Volume 31, Number 2 the resulting PFA or PCFG approximates the input language model as well as possible, or more specifically, such that the Kullback-Leibler (KL) distance (or relative entropy) between the input model and the trained model is minimized. The input FA or CFG to be trained may be structurally unrelated to the input language model. This technique has several applications. One is an extension with probabilities of existing work on approximation of CFGs by means of FAs (Nederhof 2000). The motivation for this work was that application of FAs is generally less costly than application of CFGs, which is an important benefit when the input is very large, as is often the case in, for example, speech recognition systems. The practical relevance of this work was limited, however, by the fact that in practice one is more interested in the probabilities of sentences than in a purely Boolean distinction between grammatical and ungrammatical sentences. Several approaches were discussed by Mohri and Nederhof (2001) to extend this work to approximation of PCFGs by means of PFAs. A firs"
J05-2002,W03-3016,1,0.891636,"Missing"
J05-2002,E91-1027,0,0.0293424,"f probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA. 1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized approximating FAs that can be much larger than the input CFGs. 174 Nederhof Training Models on Models The only requirement is that the FA to be trained be unambiguous, by which we mean that each input string can be recognized by at most one computation of the FA. The special case of n-grams has already been formulated by Stolcke and Segal (1994), realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is here seen as a (P)FA that contains exactly one state for each possible history of the n − 1 previously read symbols. It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs. Another application of our work involves determinization and minimization of PFAs."
J05-2002,C69-0101,0,0.740469,"hat extends a well-known representation of the intersection of a regular and a context-free language. Thereby we merge the input model and the model to be trained into a single structure. This structure is the foundation for a number of algorithms, presented in section 5, which allow, respectively, training of an unambiguous FA on the basis of a PCFG (section 5.1), training of an unambiguous CFG on the basis of a PFA (section 5.2), and training of an unambiguous FA on the basis of a PFA (section 5.3). 2. Preliminaries Many of the definitions on probabilistic context-free grammars are based on Santos (1972) and Booth and Thompson (1973), and the definitions on probabilistic finite automata are based on Paz (1971) and Starke (1972). A context-free grammar G is a 4-tuple (Σ, N, S, R), where Σ and N are two finite disjoint sets of terminals and nonterminals, respectively, S ∈ N is the start symbol, and R is a finite set of rules, each of the form A → α, where A ∈ N and α ∈ (Σ ∪ N)∗ . A probabilistic context-free grammar G is a 5-tuple (Σ, N, S, R, pG ), where Σ, N, S and R are as above, and pG is a function from rules in R to probabilities. In what follows, symbol a ranges over the set Σ, symbols w"
J05-2002,P94-1011,0,0.0424915,"hese expected frequencies then allow us to determine the assignment of probabilities to transitions of the FA that minimizes the KL distance between the PCFG and the resulting PFA. 1 In Nederhof (2000), several methods of approximation were discussed that lead to determinized approximating FAs that can be much larger than the input CFGs. 174 Nederhof Training Models on Models The only requirement is that the FA to be trained be unambiguous, by which we mean that each input string can be recognized by at most one computation of the FA. The special case of n-grams has already been formulated by Stolcke and Segal (1994), realizing an idea previously envisioned by Rimon and Herz (1991). An n-gram model is here seen as a (P)FA that contains exactly one state for each possible history of the n − 1 previously read symbols. It is clear that such an FA is unambiguous (even deterministic) and that our technique therefore properly subsumes the technique by Stolcke and Segal (1994), although the way that the two techniques are formulated is rather different. Also note that the FA underlying an n-gram model accepts any input string over the alphabet, which does not hold for general (unambiguous) FAs. Another applicati"
J05-2002,E93-1045,0,0.527192,"Missing"
J05-2002,P80-1024,0,\N,Missing
J11-4009,P96-1023,0,0.0314915,"thermore, no more than two symbols can be used in the right-hand side of a rule, and the terminal symbol associated with the lefthand side of a rule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, Un"
J11-4009,P01-1017,0,0.0542691,"lefthand side of a rule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department"
J11-4009,J03-4003,0,0.110964,"ule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department of Information Eng"
J11-4009,C96-1058,0,0.0252044,"e of a rule, and the terminal symbol associated with the lefthand side of a rule must also occur on the right-hand side. In this way, 2-LCFGs are able to specify syntactic constraints as well as lexically speciﬁc preferences that inﬂuence the combination of predicates with their arguments or modiﬁers. Models based on 2-LCFGs have therefore been of central interest in statistical natural language parsing, as they allow selection of high-quality parse trees. One can in fact see 2-LCFGs as abstract models of the head automata of Alshawi (1996), the probabilistic projective dependency grammars of Eisner (1996), and the head-driven statistical models of Charniak (2001) and Collins (2003). Parsing algorithms based on 2-LCFGs can be very efﬁcient. In the general case, existing dynamic programming algorithms have running time of O(|w|4 ), where w is the input string. (In this article we disregard complexity factors that depend on the input grammar, which we will consider to be constants.) In cases in which, for each (lexical) head, the two processes of generating its left and right arguments are, to some ∗ School of Computer Science, University of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX,"
J11-4009,1997.iwpt-1.10,0,0.355272,"Italy. E-mail: satta@dei.unipd.it. Submission received: 8 September 2010; revised submission received: 21 January 2011; accepted for publication: 17 April 2011. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 4 extent, independent one of the other, parsing based on 2-LCFGs can be asymptotically improved to O(|w|3 ). The reader is referred to Eisner and Satta (1999) for a detailed presentation of these computational results. In the literature, this condition on independence between left and right arguments of each head has been called splittability (Eisner 1997; Eisner and Satta 1999). Testing for splittability on an input 2-LCFG is therefore of central interest to parsing efﬁciency. The computability of this test has never been investigated, however. In this article splittability is deﬁned for 2-LCFGs in terms of equivalence to another grammar in which independence between left and right arguments of each head is ensured by a simple syntactic restriction. This restriction is called split form. Informally, a 2-LCFG is in split form if it can be factorized into individual subgrammars, one for each head, and each subgrammar produces the left and the r"
J11-4009,P99-1059,1,0.841792,"iversity of St. Andrews, North Haugh, St. Andrews, Fife, KY16 9SX, Scotland. E-mail: markjan.nederhof@gmail.com. ∗∗ Department of Information Engineering, University of Padua, via Gradenigo 6/A, I-35131 Padova, Italy. E-mail: satta@dei.unipd.it. Submission received: 8 September 2010; revised submission received: 21 January 2011; accepted for publication: 17 April 2011. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 4 extent, independent one of the other, parsing based on 2-LCFGs can be asymptotically improved to O(|w|3 ). The reader is referred to Eisner and Satta (1999) for a detailed presentation of these computational results. In the literature, this condition on independence between left and right arguments of each head has been called splittability (Eisner 1997; Eisner and Satta 1999). Testing for splittability on an input 2-LCFG is therefore of central interest to parsing efﬁciency. The computability of this test has never been investigated, however. In this article splittability is deﬁned for 2-LCFGs in terms of equivalence to another grammar in which independence between left and right arguments of each head is ensured by a simple syntactic restrictio"
J17-3001,E14-1039,0,0.128452,"r all test sentences is computed, weighted by sentence length. All algorithms are implemented in Python and experiments are run on a server with two 2.6-GHz Intel Xeon E5-2630 v2 CPUs and 64 GB of RAM. Each experiment uses a single thread; the measured running time might be slightly distorted because of the usual load jitter. For probabilistic LCFRS parsing we use two off-the-shelf systems: If the induced grammar’s first component is equivalent to a FA, then we use the OpenFST (Allauzen et al. 2007) framework with the Python bindings of Gorman (2016). Otherwise, we utilize the LCFRS parser of Angelov and Ljunglöf (2014), which is part of the runtime system of the Grammatical Framework (Ranta 2011). 7.1 Dependency Parsing In our experiments on dependency parsing we use a corpus based on TIGER as provided in the 2006 CoNLL shared task (Buchholz and Marsi 2006). The task specifies splits of TIGER into a training set (39,216 sentences) and a test set (357 sentences). Each sentence in the corpus consists of a sequence of tokens. A token has up to 10 fields, including the sentence position, form, lemma, part-of-speech (POS) tag, sentence position of the head, and dependency relation (DEPREL) to the head. In TIGER,"
J17-3001,E91-1005,0,0.210375,"Missing"
J17-3001,D10-1117,0,0.0239551,"Missing"
J17-3001,W07-1506,0,0.251363,"eferred to as pseudo-projectivity in the literature on dependency parsing (Kahane, Nasr, and Rambow 1998; Nivre and Nilsson 2005; McDonald and Pereira 2006). A standard parsing system is trained on a corpus of projective dependency structures that was obtained by applying a lifting operation to non-projective structures. In a first pass, this system is applied to unlabeled sentences and produces projective dependencies. In a second pass, the lifting operation is reversed to introduce non-projectivity. A related idea for discontinuous phrase structures is the reversible splitting conversion of Boyd (2007). See also Johnson (2002), Campbell (2004), and Gabbard, Kulick, and Marcus (2006). The two passes of pseudo-projective dependency parsing need not be strictly separated in time. For example, one way to characterize the algorithm by Nivre (2009) is that it combines the first pass with the second. Here the usual one-way input tape is replaced by a buffer. A non-topmost element from the parsing stack, which holds a word previously read from the input sentence, can be transferred back to the buffer, and 0 ROOT 1 A 2 hearing 3 is 4 scheduled 5 on 6 the 7 issue 8 today Figure 1 A non-projective dep"
J17-3001,W06-2920,0,0.0475785,"asured running time might be slightly distorted because of the usual load jitter. For probabilistic LCFRS parsing we use two off-the-shelf systems: If the induced grammar’s first component is equivalent to a FA, then we use the OpenFST (Allauzen et al. 2007) framework with the Python bindings of Gorman (2016). Otherwise, we utilize the LCFRS parser of Angelov and Ljunglöf (2014), which is part of the runtime system of the Grammatical Framework (Ranta 2011). 7.1 Dependency Parsing In our experiments on dependency parsing we use a corpus based on TIGER as provided in the 2006 CoNLL shared task (Buchholz and Marsi 2006). The task specifies splits of TIGER into a training set (39,216 sentences) and a test set (357 sentences). Each sentence in the corpus consists of a sequence of tokens. A token has up to 10 fields, including the sentence position, form, lemma, part-of-speech (POS) tag, sentence position of the head, and dependency relation (DEPREL) to the head. In TIGER, 52 POS tags and 46 504 Gebhardt, Nederhof, and Vogler Hybrid Grammars DEPRELs are used. We adopt the three evaluation metrics from the shared task, namely, the percentages of tokens for which a parser correctly predicts the head (the unlabele"
J17-3001,P04-1082,0,0.0144714,"he literature on dependency parsing (Kahane, Nasr, and Rambow 1998; Nivre and Nilsson 2005; McDonald and Pereira 2006). A standard parsing system is trained on a corpus of projective dependency structures that was obtained by applying a lifting operation to non-projective structures. In a first pass, this system is applied to unlabeled sentences and produces projective dependencies. In a second pass, the lifting operation is reversed to introduce non-projectivity. A related idea for discontinuous phrase structures is the reversible splitting conversion of Boyd (2007). See also Johnson (2002), Campbell (2004), and Gabbard, Kulick, and Marcus (2006). The two passes of pseudo-projective dependency parsing need not be strictly separated in time. For example, one way to characterize the algorithm by Nivre (2009) is that it combines the first pass with the second. Here the usual one-way input tape is replaced by a buffer. A non-topmost element from the parsing stack, which holds a word previously read from the input sentence, can be transferred back to the buffer, and 0 ROOT 1 A 2 hearing 3 is 4 scheduled 5 on 6 the 7 issue 8 today Figure 1 A non-projective dependency structure. 466 Gebhardt, Nederhof,"
J17-3001,A00-2018,0,0.254538,"Missing"
J17-3001,2003.mtsummit-papers.6,0,0.0842502,"Missing"
J17-3001,D14-1082,0,0.0340945,"Missing"
J17-3001,P97-1003,0,0.743941,"Missing"
J17-3001,E12-1047,0,0.0417703,"Missing"
J17-3001,C04-1025,0,0.0604251,"es that go beyond context-free power is to use more expressive grammatical formalisms. One approach proposed by Reape (1989, 1994) is to separate linear order from the parent–child relation in syntactic structure, and to allow shuffling of the order of descendants of a node, which need not be its direct children. The set of possible orders is restricted by linear precedence constraints. A further restriction may be imposed by compaction (Kathol and Pollard 1995). As discussed by Fouvry and Meurers (2000) and Daniels and Meurers (2002), this may lead to exponential parsing complexity; see also Daniels and Meurers (2004). Separating linear order from the parent–child relation is in the tradition of headdriven phrase structure grammar (HPSG), where grammars are commonly hand-written. This differs from our objectives to induce grammars automatically from training data, as will become clear in the following sections. To stay within a polynomial time complexity, one may also consider tree adjoining grammars (TAGs), which can describe strictly larger classes of word order phenomena than CFGs (Rambow and Joshi 1997). The resulting parsers have a time complexity of O(n6 ) (Vijay-Shankar and Joshi 1985). However, the"
J17-3001,C96-1058,0,0.259366,"Missing"
J17-3001,W11-2913,0,0.2521,"of words, but may comprise one or more gaps. The need for discontinuous structures tends to be even greater for languages with relatively free word order (Kathol and Pollard 1995; Müller 2004). In the context of dependency parsing (Kübler, McDonald, and Nivre 2009), the more specific term non-projectivity is used instead of, or next to, discontinuity. See Rambow (2010) for a discussion of the relation between constituent and dependency structures and see Maier and Lichte (2009) for a comparison of discontinuity and nonprojectivity. As shown by, for example, Hockenmaier and Steedman (2007) and Evang and Kallmeyer (2011), discontinuity encoded using traces in the Penn Treebank can be rendered in alternative, and arguably more explicit, forms. In many modern treebanks, discontinuous structures have been given a prominent status (e.g., Böhmová et al. 2000). Figure 1 shows an example of a non-projective dependency structure. The most established parsing algorithms are compiled out of context-free grammars (CFGs), or closely related formalisms such as tree substitution grammars (Sima’an et al. 1994) or regular tree grammars (Brainerd 1969; Gécseg and Steinby 1997). These parsers, which have a time complexity of O"
J17-3001,N06-1024,0,0.067712,"Missing"
J17-3001,W15-4805,1,0.83483,"G is not a string language but a tree language, or more precisely, its elements are sequences of trees. As for macro grammars, we will focus our attention on CFTGs with the property that for each rule A(x1,k ) → r and each i ∈ [k], variable xi has exactly one occurrence in r. In this article, such grammars will be called simple context-free tree grammars (sCFTGs). Note that if N = N (0) , then a sCFTGs is a regular tree grammar (Brainerd 1969; Gécseg and Steinby 1997). sCFTGs are a natural generalization of the widely used TAGs; see Kepser and Rogers (2011), Maletti and Engelfriet (2012), and Gebhardt and Osterholzer (2015). Example 6 A sCFTG and a derivation are provided here. This example models a simple recursive structure, where a could stand for a noun, b for a verb, and c for an adverb that modifies exactly one of the verbs. S → A(c) A(x1 ) → S(a A(x1 ) b) A(x1 ) → S(a A(ε ) b x1 ) A(x1 ) → S(a b x1 ) S ⇒G ⇒G ⇒G ⇒G A(c) S(a A(c) b) S(a S(a A(ε ) b c) b) S(a S(a S(a b) b c) b) 4.3 Linear Context-free Rewriting Systems In Vijay-Shanker, Weir, and Joshi (1987), the semantics of LCFRS is introduced by distinguishing two phases. In the first phase, a tree over function symbols is generated by 474 Gebhardt, Nede"
J17-3001,N10-1035,0,0.0608016,"Missing"
J17-3001,N09-1061,0,0.0311363,"right half of the figure. Because now all node labels have fanout not exceeding 2, recursive traversal will make no further changes. The partitioning π0 is similar to π in the sense that subtrees that are not on the path to {3, 7} remain unchanged. Other valid choices for J0 would be {2} and {5}. Not a valid choice for J0 would be {1, 6}, as J  {1, 6} = {2, 3, 5, 7}, which has fanout 3. Algorithm 4 ensures that subsequent induction of an LCFRS (cf. Figure 11b) leads to a binary LCFRS. Note the difference between binarization algorithms such as those from Gómez-Rodríguez and Satta (2009) and Gómez-Rodríguez et al. (2009), which are applied on grammar rules, and our procedure, which is applied before any grammar is obtained. Unlike van Cranenburgh (2012), moreover, our objective is not to obtain a “coarse” grammar for the purpose of coarse-to-fine parsing. Note that if k is chosen to be 1, then the resulting partitioning is consistent with derivations of a CFG. Even simpler partitionings exist. In particular, the left-branching partitioning has internal node labels that are {1, 2, . . . , m}, each with children labeled {1, . . . , m − 1} and {m}. These are consistent with the computations of finite automata (F"
J17-3001,P09-1111,0,0.0289965,"s leads to the partitioning π0 in the right half of the figure. Because now all node labels have fanout not exceeding 2, recursive traversal will make no further changes. The partitioning π0 is similar to π in the sense that subtrees that are not on the path to {3, 7} remain unchanged. Other valid choices for J0 would be {2} and {5}. Not a valid choice for J0 would be {1, 6}, as J  {1, 6} = {2, 3, 5, 7}, which has fanout 3. Algorithm 4 ensures that subsequent induction of an LCFRS (cf. Figure 11b) leads to a binary LCFRS. Note the difference between binarization algorithms such as those from Gómez-Rodríguez and Satta (2009) and Gómez-Rodríguez et al. (2009), which are applied on grammar rules, and our procedure, which is applied before any grammar is obtained. Unlike van Cranenburgh (2012), moreover, our objective is not to obtain a “coarse” grammar for the purpose of coarse-to-fine parsing. Note that if k is chosen to be 1, then the resulting partitioning is consistent with derivations of a CFG. Even simpler partitionings exist. In particular, the left-branching partitioning has internal node labels that are {1, 2, . . . , m}, each with children labeled {1, . . . , m − 1} and {m}. These are consistent with the"
J17-3001,W16-2409,0,0.0142293,"a score for each sentence. The average of these scores for all test sentences is computed, weighted by sentence length. All algorithms are implemented in Python and experiments are run on a server with two 2.6-GHz Intel Xeon E5-2630 v2 CPUs and 64 GB of RAM. Each experiment uses a single thread; the measured running time might be slightly distorted because of the usual load jitter. For probabilistic LCFRS parsing we use two off-the-shelf systems: If the induced grammar’s first component is equivalent to a FA, then we use the OpenFST (Allauzen et al. 2007) framework with the Python bindings of Gorman (2016). Otherwise, we utilize the LCFRS parser of Angelov and Ljunglöf (2014), which is part of the runtime system of the Grammatical Framework (Ranta 2011). 7.1 Dependency Parsing In our experiments on dependency parsing we use a corpus based on TIGER as provided in the 2006 CoNLL shared task (Buchholz and Marsi 2006). The task specifies splits of TIGER into a training set (39,216 sentences) and a test set (357 sentences). Each sentence in the corpus consists of a sequence of tokens. A token has up to 10 fields, including the sentence position, form, lemma, part-of-speech (POS) tag, sentence positi"
J17-3001,P04-1013,0,0.14056,"Missing"
J17-3001,J07-3004,0,0.0170138,"need not form a contiguous sequence of words, but may comprise one or more gaps. The need for discontinuous structures tends to be even greater for languages with relatively free word order (Kathol and Pollard 1995; Müller 2004). In the context of dependency parsing (Kübler, McDonald, and Nivre 2009), the more specific term non-projectivity is used instead of, or next to, discontinuity. See Rambow (2010) for a discussion of the relation between constituent and dependency structures and see Maier and Lichte (2009) for a comparison of discontinuity and nonprojectivity. As shown by, for example, Hockenmaier and Steedman (2007) and Evang and Kallmeyer (2011), discontinuity encoded using traces in the Penn Treebank can be rendered in alternative, and arguably more explicit, forms. In many modern treebanks, discontinuous structures have been given a prominent status (e.g., Böhmová et al. 2000). Figure 1 shows an example of a non-projective dependency structure. The most established parsing algorithms are compiled out of context-free grammars (CFGs), or closely related formalisms such as tree substitution grammars (Sima’an et al. 1994) or regular tree grammars (Brainerd 1969; Gécseg and Steinby 1997). These parsers, wh"
J17-3001,P02-1018,0,0.0757297,"rojectivity in the literature on dependency parsing (Kahane, Nasr, and Rambow 1998; Nivre and Nilsson 2005; McDonald and Pereira 2006). A standard parsing system is trained on a corpus of projective dependency structures that was obtained by applying a lifting operation to non-projective structures. In a first pass, this system is applied to unlabeled sentences and produces projective dependencies. In a second pass, the lifting operation is reversed to introduce non-projectivity. A related idea for discontinuous phrase structures is the reversible splitting conversion of Boyd (2007). See also Johnson (2002), Campbell (2004), and Gabbard, Kulick, and Marcus (2006). The two passes of pseudo-projective dependency parsing need not be strictly separated in time. For example, one way to characterize the algorithm by Nivre (2009) is that it combines the first pass with the second. Here the usual one-way input tape is replaced by a buffer. A non-topmost element from the parsing stack, which holds a word previously read from the input sentence, can be transferred back to the buffer, and 0 ROOT 1 A 2 hearing 3 is 4 scheduled 5 on 6 the 7 issue 8 today Figure 1 A non-projective dependency structure. 466 Ge"
J17-3001,P98-1106,0,0.316827,"Missing"
J17-3001,W12-4613,0,0.0165361,"kar and Joshi 1985). However, the derived trees they generate are still continuous. Although their derivation trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic structures. Moreover, it was argued by Becker, Joshi, and Rambow (1991) that further additions to TAGs are needed to obtain adequate descriptions of certain non-context-free phenomena. These additions further increase the time complexity. In order to obtain desired syntactic structures, one may combine TAG parsing with an idea that is related to that of pseudo-projectivity. For example, Kallmeyer and Kuhlmann (2012) propose a transformation that turns a derivation tree of a (lexicalized) TAG into a non-projective dependency structure. The same idea has been applied to derivation trees of other formalisms, in particular (lexicalized) linear context-free rewriting systems (LCFRSs) (Kuhlmann 2013), whose weak generative power subsumes that of TAGs. Parsers more powerful than those for CFGs often incur high time costs. In particular, LCFRS parsers have a time complexity that is polynomial in the sentence length, but with a degree that is determined by properties of the grammar. This degree typically increase"
J17-3001,C10-1061,0,0.0195335,"l s-term that is evaluated in this way is an element of the language [G]. This s-term, denoted by φ(d), is called the evaluation of d. The notion of dependency graph originates from attribute grammars (Knuth 1968; Paakki 1995); it should not be confused with the linguistic concept of dependency. Note that if the rules in a derivation are given, then the choice of ri for each variable xi in each rule instance is uniquely determined. For a given string s, the set of all LCFRS derivations (in compact tabular form) can be obtained in polynomial time in the length of s (Seki et al. 1991). See also Kallmeyer and Maier (2010, 2013) for the extension with probabilities. Example 7 An example of a LCFRS G is the following: S(x1 x3 x2 x4 ) → A(x1 , x2 ) B(x3 , x4 ) A(ax1 , bx2 ) → A(x1 , x2 ) B(cx1 , dx2 ) → B(x1 , x2 ) A(ε, ε ) → ε B(ε, ε ) → ε A derivation of G and the corresponding derivation tree d is depicted in Figure 6; its evaluation is the s-term φ(d) = a a c b b d. All strings derived by G have the interlaced structure am cn bm dn with m, n ∈ N, where the i-th occurrence of a corresponds to the i-th occurrence of b and the i-th occurrence of c corresponds to the i-th occurrence of d. This resembles cross-se"
J17-3001,J13-1006,0,0.89491,"Missing"
J17-3001,P95-1024,0,0.268743,"English contains traces and other elements that encode additional structure next to the pure tree structure as indicated by the brackets. This is in keeping with observations that even English cannot be described adequately without a more general form of trees, allowing for so-called discontinuity (McCawley 1982; Stucky 1987). In a discontinuous structure, the set of leaves dominated by a node of the tree need not form a contiguous sequence of words, but may comprise one or more gaps. The need for discontinuous structures tends to be even greater for languages with relatively free word order (Kathol and Pollard 1995; Müller 2004). In the context of dependency parsing (Kübler, McDonald, and Nivre 2009), the more specific term non-projectivity is used instead of, or next to, discontinuity. See Rambow (2010) for a discussion of the relation between constituent and dependency structures and see Maier and Lichte (2009) for a comparison of discontinuity and nonprojectivity. As shown by, for example, Hockenmaier and Steedman (2007) and Evang and Kallmeyer (2011), discontinuity encoded using traces in the Penn Treebank can be rendered in alternative, and arguably more explicit, forms. In many modern treebanks, d"
J17-3001,P04-1061,0,0.139967,"Missing"
J17-3001,N03-1016,0,0.0708071,"Missing"
J17-3001,J13-2004,0,0.014525,"tions to TAGs are needed to obtain adequate descriptions of certain non-context-free phenomena. These additions further increase the time complexity. In order to obtain desired syntactic structures, one may combine TAG parsing with an idea that is related to that of pseudo-projectivity. For example, Kallmeyer and Kuhlmann (2012) propose a transformation that turns a derivation tree of a (lexicalized) TAG into a non-projective dependency structure. The same idea has been applied to derivation trees of other formalisms, in particular (lexicalized) linear context-free rewriting systems (LCFRSs) (Kuhlmann 2013), whose weak generative power subsumes that of TAGs. Parsers more powerful than those for CFGs often incur high time costs. In particular, LCFRS parsers have a time complexity that is polynomial in the sentence length, but with a degree that is determined by properties of the grammar. This degree typically increases with the amount of discontinuity in the desired structures. Difficulties in running LCFRS parsers for natural languages are described, for example, by Kallmeyer and Maier (2013). In the architectures we have discussed, the common elements are: r r a grammar, in some fixed formalism"
J17-3001,E09-1055,0,0.340649,"lisms we consider are simple context-free tree grammars (Rounds 1970) and simple definite clause programs (sDCP), inspired by Deransart and Małuszynski (1985). This gives four combinations, each leading to one class of hybrid grammars. In addition, more fine-grained subclasses can be defined by placing further syntactic restrictions on the string and tree formalisms. To place hybrid grammars in the context of existing parsing architectures, let us consider classical grammar induction from a treebank, for example, for context-free grammars (Charniak 1996) or for LCFRSs (Maier and Søgaard 2008; Kuhlmann and Satta 2009). Rules are extracted directly from the trees in the training set, and unseen input strings are consequently parsed according to these structures. Grammars induced in this way can be seen as restricted hybrid grammars, in which no freedom exists in the relation between the string component and the tree component. In particular, the presence of discontinuous structures generally leads to high time complexity of string parsing. In contrast, the framework in this article detaches the string component of the grammar from the tree component. Thereby the parsing process of input strings is no longer"
J17-3001,D08-1082,0,0.0430307,"e detaches the string component of the grammar from the tree component. Thereby the parsing process of input strings is no longer bound to follow the tree structures, while the same tree structures as before can still be produced, provided the tree component is suitably chosen. This allows string parsing with low time complexity in combination with production of discontinuous trees. Nederhof and Vogler (2014) presented experiments with various subclasses of hybrid grammars for the purpose of constituent parsing. Trade-offs between speed and accuracy 1 The term “hybrid tree” was used before by Lu et al. (2008), also for a mixture of a tree structure and a linear structure, generated by a probabilistic model. However, the linear “surface” structure was obtained by a simple left-to-right tree traversal, whereas a meaning representation was obtained by a slightly more flexible traversal of the same tree. The emphasis in the current article is rather on separating the linear structure from the tree structure. Note that similar distinctions of multiple strata were made before in both constituent linguistics (see, e.g., Chomsky 1981) and dependency linguistics (see, e.g., Mel’ˇcuk 1988). 468 Gebhardt, Ne"
J17-3001,W10-4415,0,0.136724,"terminal, which is a CoNLL token, we include only particular fields, namely, (i) POS and DEPREL, (ii) POS, or (iii) DEPREL. We call this parameter argument label. The considered methods to obtain recursive partitionings include (i) direct extraction (see Algorithm 3), (ii) transformation to fanout k (see Algorithm 4), (iii) right-branching, and (iv) left-branching. Each choice of values for the parameters determines an experimental scenario. In particular, direct extraction in combination with strict labeling achieves traditional LCFRS parsing. We compare our induction framework with rparse (Maier and Kallmeyer 2010), which induces and trains unlexicalized, binarized, and Markovized LCFRS. As parser for these LCFRS we choose the runtime system of the Grammatical Framework, because it is faster than rparse’s built-in parser (for a comparison cf. Angelov and Ljunglöf 2014). Additionally, we use MaltParser (Nivre, Hall, and Nilsson 2006) to compare our approach with a well-established transition-based parsing architecture, which is not state-of-the-art but allows us to disable features of lemmas and word forms, to match our own implementation, which does not take lemmas or word forms as part of the input. Th"
J17-3001,P12-1053,0,0.0162862,"∈ TΣ |hSi ⇒∗G s} induced by a CFTG G is not a string language but a tree language, or more precisely, its elements are sequences of trees. As for macro grammars, we will focus our attention on CFTGs with the property that for each rule A(x1,k ) → r and each i ∈ [k], variable xi has exactly one occurrence in r. In this article, such grammars will be called simple context-free tree grammars (sCFTGs). Note that if N = N (0) , then a sCFTGs is a regular tree grammar (Brainerd 1969; Gécseg and Steinby 1997). sCFTGs are a natural generalization of the widely used TAGs; see Kepser and Rogers (2011), Maletti and Engelfriet (2012), and Gebhardt and Osterholzer (2015). Example 6 A sCFTG and a derivation are provided here. This example models a simple recursive structure, where a could stand for a noun, b for a verb, and c for an adverb that modifies exactly one of the verbs. S → A(c) A(x1 ) → S(a A(x1 ) b) A(x1 ) → S(a A(ε ) b x1 ) A(x1 ) → S(a b x1 ) S ⇒G ⇒G ⇒G ⇒G A(c) S(a A(c) b) S(a S(a A(ε ) b c) b) S(a S(a S(a b) b c) b) 4.3 Linear Context-free Rewriting Systems In Vijay-Shanker, Weir, and Joshi (1987), the semantics of LCFRS is introduced by distinguishing two phases. In the first phase, a tree over function symbo"
J17-3001,J93-2004,0,0.0594219,"Missing"
J17-3001,E06-1011,0,0.263754,"r tree grammars (Brainerd 1969; Gécseg and Steinby 1997). These parsers, which have a time complexity of O(n3 ) for n being the length of the input string, operate by composing adjacent substrings of the input sentence into longer substrings. As a result, the structures they can build directly do not involve any discontinuity. The need for discontinuous syntactic structures thus poses a challenge to traditional parsing algorithms. One possible solution is commonly referred to as pseudo-projectivity in the literature on dependency parsing (Kahane, Nasr, and Rambow 1998; Nivre and Nilsson 2005; McDonald and Pereira 2006). A standard parsing system is trained on a corpus of projective dependency structures that was obtained by applying a lifting operation to non-projective structures. In a first pass, this system is applied to unlabeled sentences and produces projective dependencies. In a second pass, the lifting operation is reversed to introduce non-projectivity. A related idea for discontinuous phrase structures is the reversible splitting conversion of Boyd (2007). See also Johnson (2002), Campbell (2004), and Gabbard, Kulick, and Marcus (2006). The two passes of pseudo-projective dependency parsing need n"
J17-3001,H05-1066,0,0.133962,"Missing"
J17-3001,W10-4405,0,0.0434204,"Missing"
J17-3001,C14-1130,1,0.507849,"ers for natural languages are described, for example, by Kallmeyer and Maier (2013). In the architectures we have discussed, the common elements are: r r a grammar, in some fixed formalism, that determines the set of sentences that are accepted, and a procedure to build (discontinuous) structures, guided by the derivation of input sentences. The purpose of this article is to explore a theoretical framework that allows us to capture a wide range of parsing architectures that all share these two common elements. At the core of this framework lies a formalism called hybrid grammar, introduced in Nederhof and Vogler (2014). Such a grammar consists of a string grammar and a tree grammar. Derivations are coupled, as in synchronous grammars (Shieber and Schabes 1990; Satta and Peserico 2005). In addition, each occurrence of a terminal symbol in the string grammar is coupled to an occurrence of a terminal symbol in the tree grammar. The string grammar defines the set of accepted sentences. The tree grammar, whose rules are tied 467 Computational Linguistics Volume 43, Number 3 is hearing scheduled on A today issue the A hearing is scheduled on the issue today Figure 2 A hybrid tree corresponding to the non-projecti"
J17-3001,P05-1013,0,0.0667345,"n et al. 1994) or regular tree grammars (Brainerd 1969; Gécseg and Steinby 1997). These parsers, which have a time complexity of O(n3 ) for n being the length of the input string, operate by composing adjacent substrings of the input sentence into longer substrings. As a result, the structures they can build directly do not involve any discontinuity. The need for discontinuous syntactic structures thus poses a challenge to traditional parsing algorithms. One possible solution is commonly referred to as pseudo-projectivity in the literature on dependency parsing (Kahane, Nasr, and Rambow 1998; Nivre and Nilsson 2005; McDonald and Pereira 2006). A standard parsing system is trained on a corpus of projective dependency structures that was obtained by applying a lifting operation to non-projective structures. In a first pass, this system is applied to unlabeled sentences and produces projective dependencies. In a second pass, the lifting operation is reversed to introduce non-projectivity. A related idea for discontinuous phrase structures is the reversible splitting conversion of Boyd (2007). See also Johnson (2002), Campbell (2004), and Gabbard, Kulick, and Marcus (2006). The two passes of pseudo-projecti"
J17-3001,P09-1040,0,0.472084,"tructure as indicated by the brackets. This is in keeping with observations that even English cannot be described adequately without a more general form of trees, allowing for so-called discontinuity (McCawley 1982; Stucky 1987). In a discontinuous structure, the set of leaves dominated by a node of the tree need not form a contiguous sequence of words, but may comprise one or more gaps. The need for discontinuous structures tends to be even greater for languages with relatively free word order (Kathol and Pollard 1995; Müller 2004). In the context of dependency parsing (Kübler, McDonald, and Nivre 2009), the more specific term non-projectivity is used instead of, or next to, discontinuity. See Rambow (2010) for a discussion of the relation between constituent and dependency structures and see Maier and Lichte (2009) for a comparison of discontinuity and nonprojectivity. As shown by, for example, Hockenmaier and Steedman (2007) and Evang and Kallmeyer (2011), discontinuity encoded using traces in the Penn Treebank can be rendered in alternative, and arguably more explicit, forms. In many modern treebanks, discontinuous structures have been given a prominent status (e.g., Böhmová et al. 2000)."
J17-3001,nivre-etal-2006-maltparser,0,0.106106,"Missing"
J17-3001,W09-3811,0,0.0400487,"Missing"
J17-3001,P06-1055,0,0.308843,"ich is not present in the induction algorithm by Kuhlmann and Satta (2009). This separation also leads to different generalization over the training data, as the higher accuracy and the lower numbers of parse failures for the hybrid grammar indicate. One possible refinement of this result is to choose different argument labels for inherited and synthesized arguments. One may also use form or lemma next to, or instead of, POS tags and DEPRELs, possibly in combination with smoothing techniques to handle unknown words. Another subject for future investigation is the use of splitting and merging (Petrov et al. 2006) to determine parts of nonterminal names. New recursive partitioning strategies may be developed, and one could consider blending grammars that were induced using different recursive partitioning strategies. 7.2 Constituent Parsing The experiments for constituent parsing are carried out as in Nederhof and Vogler (2014) but on a larger portion of the TIGER corpus: We now use the first 40,000 sentences for training (omitting 10 where a single tree does not span the entire sentence), and from the remaining 10,474 sentences we remove the ones with length greater than 20, leaving 7,597 sentences fo"
J17-3001,N10-1049,0,0.0188654,"described adequately without a more general form of trees, allowing for so-called discontinuity (McCawley 1982; Stucky 1987). In a discontinuous structure, the set of leaves dominated by a node of the tree need not form a contiguous sequence of words, but may comprise one or more gaps. The need for discontinuous structures tends to be even greater for languages with relatively free word order (Kathol and Pollard 1995; Müller 2004). In the context of dependency parsing (Kübler, McDonald, and Nivre 2009), the more specific term non-projectivity is used instead of, or next to, discontinuity. See Rambow (2010) for a discussion of the relation between constituent and dependency structures and see Maier and Lichte (2009) for a comparison of discontinuity and nonprojectivity. As shown by, for example, Hockenmaier and Steedman (2007) and Evang and Kallmeyer (2011), discontinuity encoded using traces in the Penn Treebank can be rendered in alternative, and arguably more explicit, forms. In many modern treebanks, discontinuous structures have been given a prominent status (e.g., Böhmová et al. 2000). Figure 1 shows an example of a non-projective dependency structure. The most established parsing algorith"
J17-3001,E89-1014,0,0.196344,"uffer. A non-topmost element from the parsing stack, which holds a word previously read from the input sentence, can be transferred back to the buffer, and 0 ROOT 1 A 2 hearing 3 is 4 scheduled 5 on 6 the 7 issue 8 today Figure 1 A non-projective dependency structure. 466 Gebhardt, Nederhof, and Vogler Hybrid Grammars thereby input positions can be effectively swapped. This then results in a non-projective dependency structure. A second potential solution to obtain syntactic structures that go beyond context-free power is to use more expressive grammatical formalisms. One approach proposed by Reape (1989, 1994) is to separate linear order from the parent–child relation in syntactic structure, and to allow shuffling of the order of descendants of a node, which need not be its direct children. The set of possible orders is restricted by linear precedence constraints. A further restriction may be imposed by compaction (Kathol and Pollard 1995). As discussed by Fouvry and Meurers (2000) and Daniels and Meurers (2002), this may lead to exponential parsing complexity; see also Daniels and Meurers (2004). Separating linear order from the parent–child relation is in the tradition of headdriven phrase"
J17-3001,C69-0101,0,0.453976,"nal symbols in the two component grammars, which is where our theory departs from that of synchronous grammars. A hybrid grammar generates a set of hybrid trees;1 Figure 2 shows an example of a hybrid tree, which corresponds to the non-projective dependency structure of Figure 1. The general concept of hybrid grammars leaves open the choice of the string grammar formalism and that of the tree grammar formalism. In this article we consider simple macro grammars (Fischer 1968) and LCFRSs as string grammar formalisms. The tree grammar formalisms we consider are simple context-free tree grammars (Rounds 1970) and simple definite clause programs (sDCP), inspired by Deransart and Małuszynski (1985). This gives four combinations, each leading to one class of hybrid grammars. In addition, more fine-grained subclasses can be defined by placing further syntactic restrictions on the string and tree formalisms. To place hybrid grammars in the context of existing parsing architectures, let us consider classical grammar induction from a treebank, for example, for context-free grammars (Charniak 1996) or for LCFRSs (Maier and Søgaard 2008; Kuhlmann and Satta 2009). Rules are extracted directly from the trees"
J17-3001,Q13-1022,0,0.0402838,"Missing"
J17-3001,H05-1101,0,0.0386084,"e fixed formalism, that determines the set of sentences that are accepted, and a procedure to build (discontinuous) structures, guided by the derivation of input sentences. The purpose of this article is to explore a theoretical framework that allows us to capture a wide range of parsing architectures that all share these two common elements. At the core of this framework lies a formalism called hybrid grammar, introduced in Nederhof and Vogler (2014). Such a grammar consists of a string grammar and a tree grammar. Derivations are coupled, as in synchronous grammars (Shieber and Schabes 1990; Satta and Peserico 2005). In addition, each occurrence of a terminal symbol in the string grammar is coupled to an occurrence of a terminal symbol in the tree grammar. The string grammar defines the set of accepted sentences. The tree grammar, whose rules are tied 467 Computational Linguistics Volume 43, Number 3 is hearing scheduled on A today issue the A hearing is scheduled on the issue today Figure 2 A hybrid tree corresponding to the non-projective dependency structure of Figure 1. to the rules of the string grammar for synchronous rewriting, determines the resulting syntactic structures, which may be discontinu"
J17-3001,C90-3045,0,0.0439637,"are: r r a grammar, in some fixed formalism, that determines the set of sentences that are accepted, and a procedure to build (discontinuous) structures, guided by the derivation of input sentences. The purpose of this article is to explore a theoretical framework that allows us to capture a wide range of parsing architectures that all share these two common elements. At the core of this framework lies a formalism called hybrid grammar, introduced in Nederhof and Vogler (2014). Such a grammar consists of a string grammar and a tree grammar. Derivations are coupled, as in synchronous grammars (Shieber and Schabes 1990; Satta and Peserico 2005). In addition, each occurrence of a terminal symbol in the string grammar is coupled to an occurrence of a terminal symbol in the tree grammar. The string grammar defines the set of accepted sentences. The tree grammar, whose rules are tied 467 Computational Linguistics Volume 43, Number 3 is hearing scheduled on A today issue the A hearing is scheduled on the issue today Figure 2 A hybrid tree corresponding to the non-projective dependency structure of Figure 1. to the rules of the string grammar for synchronous rewriting, determines the resulting syntactic structure"
J17-3001,A97-1014,0,0.541876,"Missing"
J17-3001,P85-1011,0,0.656546,"lexity; see also Daniels and Meurers (2004). Separating linear order from the parent–child relation is in the tradition of headdriven phrase structure grammar (HPSG), where grammars are commonly hand-written. This differs from our objectives to induce grammars automatically from training data, as will become clear in the following sections. To stay within a polynomial time complexity, one may also consider tree adjoining grammars (TAGs), which can describe strictly larger classes of word order phenomena than CFGs (Rambow and Joshi 1997). The resulting parsers have a time complexity of O(n6 ) (Vijay-Shankar and Joshi 1985). However, the derived trees they generate are still continuous. Although their derivation trees may be argued to be discontinuous, these by themselves are not normally the desired syntactic structures. Moreover, it was argued by Becker, Joshi, and Rambow (1991) that further additions to TAGs are needed to obtain adequate descriptions of certain non-context-free phenomena. These additions further increase the time complexity. In order to obtain desired syntactic structures, one may combine TAG parsing with an idea that is related to that of pseudo-projectivity. For example, Kallmeyer and Kuhlm"
J17-3001,P87-1015,0,0.883723,"Missing"
J17-3001,C98-1102,0,\N,Missing
J17-3001,W90-0102,0,\N,Missing
J99-3002,P81-1022,0,0.371168,"ix. A final remark on Schabes and Joshi (1988) concerns the time complexity in terms of the size of the grammar that they report, viz. O(]GI2). This would be the same upper bound as in the case of the new algorithm. However, the correct complexity seems to be O(]G]3), since each item contains references to two nodes of the same elementary tree, and the combination in ""Right Completor"" of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the ""dotted"" items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in (_9(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and V"
J99-3002,P94-1017,1,0.846715,"e case of the new algorithm. However, the correct complexity seems to be O(]G]3), since each item contains references to two nodes of the same elementary tree, and the combination in ""Right Completor"" of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the ""dotted"" items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in (_9(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix property is not satisfied. Development of advanced parsing algorithms for TAGs with the correct-pr"
J99-3002,P98-2156,1,0.801934,"the use of the ""dotted"" items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in (_9(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix property is not satisfied. Development of advanced parsing algorithms for TAGs with the correct-prefix property is not at all straightforward. In the case of context-free grammars, the additional benefit of LR parsing, in comparison to, for example, top-down parsing, lies in 358 Nederhof Correct-Prefix Property for TAGs the ability to process multiple g r a m m a r rules simultaneously. If this is to be carried over to TAGs, then multiple elementary trees m u s t be handled simultaneously. This is difficult to combine with the mechanism we use"
J99-3002,P94-1029,1,0.838523,"erms of the size of the grammar that they report, viz. O(]GI2). This would be the same upper bound as in the case of the new algorithm. However, the correct complexity seems to be O(]G]3), since each item contains references to two nodes of the same elementary tree, and the combination in ""Right Completor"" of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the ""dotted"" items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in (_9(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix pro"
J99-3002,P95-1023,0,0.0348714,"Missing"
J99-3002,J94-2002,0,0.0185926,"ize of the grammar that they report, viz. O(]GI2). This would be the same upper bound as in the case of the new algorithm. However, the correct complexity seems to be O(]G]3), since each item contains references to two nodes of the same elementary tree, and the combination in ""Right Completor"" of two items entails the simultaneous use of three distinct nodes from the grammar. 6. Further Research The algorithm in the present paper operates in a top-down manner, being very similar to Earley&apos;s algorithm (Earley 1970), which is emphasized by the use of the ""dotted"" items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in (_9(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix pro"
J99-3002,P88-1032,0,0.16096,"property. For context-free and regular languages, the correct-prefix property can be satisfied without additional costs of space or time. Surprisingly, it has been claimed by Schabes and Waters (1995) that this property is problematic for the mildly contextsensitive languages represented by tree-adjoining grammars (TAGs): the best practical parsing algorithms for TAGs have time complexity Cg(n6) (Vijay-Shankar and Joshi [1985]; see Satta [1994] and Rajasekaran and Yooseph [1995] for lower theoretical upper bounds), whereas the only published algorithm with the correct-prefix property--that by Schabes and Joshi (1988)--has complexity O(n9). In this paper we present an algorithm that satisfies the correct-prefix property and operates in Cq(n6) time. This algorithm merely recognizes input, but it can be extended * DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid prefix property is used. (~) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 to be a parsing algorithm with the ideas from Schabes (1994), which also suggest h o w it can be"
J99-3002,J94-1004,0,0.0265483,"rve the invariant in similar ways. N o w the second claim follows: if the input up to position j has been read resulting in an item of the form [h, N --* aa * fl, i, j, fl, f2], then there is a string y such that a l . . . ajy is in the language. This y is the concatenation of the yields of the subtrees labeled I, II, and III in Figure 3. The full proofs of the two claims above are straightforward but tedious. Furthermore, our new algorithm is related to many existing recognition algorithms for TAGs (Vijay-Shankar and Joshi 1985; Schabes and Joshi 1988; Lang 1988; Vijay-Shanker and Weir 1993; Schabes and Shieber 1994; Schabes 1994), some of which were published 356 Nederhof Correct-Prefix Property for TAGs together with proofs of correctness. Therefore, including full proofs for our new algorithm does not seem necessary. 5. Complexity The steps presented in pseudoformal notation in Section 3 can easily be composed into an actual algorithm (Shieber, Schabes, and Pereira 1995). This can be done in such a way that the order of the time complexity is determined by the maximal number of different combinations of antecedents per step. If we restrict ourselves to the order of the time complexity expressed in the"
J99-3002,P90-1035,0,0.0291319,"(Earley 1970), which is emphasized by the use of the ""dotted"" items. As shown by Nederhof and Satta (1994), a family of parsing algorithms (topdown, left-corner, PLR, ELR, and LR parsing [Nederhof 1994]) can be carried over to head-driven parsing. An obvious question is whether such parsing techniques can also be used to produce variants of left-to-right parsing for TAGs. Thus, one may conjecture, for example, the existence of an LR-like parsing algorithm for arbitrary TAGs that operates in (_9(n6) and that has the correct-prefix property. Note that LR-like parsing algorithms were proposed by Schabes and Vijay-Shanker (1990) and Nederhof (1998). However, for these algorithms the correct-prefix property is not satisfied. Development of advanced parsing algorithms for TAGs with the correct-prefix property is not at all straightforward. In the case of context-free grammars, the additional benefit of LR parsing, in comparison to, for example, top-down parsing, lies in 358 Nederhof Correct-Prefix Property for TAGs the ability to process multiple g r a m m a r rules simultaneously. If this is to be carried over to TAGs, then multiple elementary trees m u s t be handled simultaneously. This is difficult to combine with"
J99-3002,J95-4002,0,0.128022,"eceding stages are correct prefixes, or more precisely, they are prefixes of some correct strings in the language. Hence, we speak of the correct-prefix property. 1 An important application can be found in the area of grammar checking: upon finding an ungrammatical sentence in a document, a grammar checker may report to the user the presumed position of the error, obtained from a parsing algorithm with the correct-prefix property. For context-free and regular languages, the correct-prefix property can be satisfied without additional costs of space or time. Surprisingly, it has been claimed by Schabes and Waters (1995) that this property is problematic for the mildly contextsensitive languages represented by tree-adjoining grammars (TAGs): the best practical parsing algorithms for TAGs have time complexity Cg(n6) (Vijay-Shankar and Joshi [1985]; see Satta [1994] and Rajasekaran and Yooseph [1995] for lower theoretical upper bounds), whereas the only published algorithm with the correct-prefix property--that by Schabes and Joshi (1988)--has complexity O(n9). In this paper we present an algorithm that satisfies the correct-prefix property and operates in Cq(n6) time. This algorithm merely recognizes input, bu"
J99-3002,P85-1011,0,0.178496,"an ungrammatical sentence in a document, a grammar checker may report to the user the presumed position of the error, obtained from a parsing algorithm with the correct-prefix property. For context-free and regular languages, the correct-prefix property can be satisfied without additional costs of space or time. Surprisingly, it has been claimed by Schabes and Waters (1995) that this property is problematic for the mildly contextsensitive languages represented by tree-adjoining grammars (TAGs): the best practical parsing algorithms for TAGs have time complexity Cg(n6) (Vijay-Shankar and Joshi [1985]; see Satta [1994] and Rajasekaran and Yooseph [1995] for lower theoretical upper bounds), whereas the only published algorithm with the correct-prefix property--that by Schabes and Joshi (1988)--has complexity O(n9). In this paper we present an algorithm that satisfies the correct-prefix property and operates in Cq(n6) time. This algorithm merely recognizes input, but it can be extended * DFKI, Stuhlsatzenhausweg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid prefix property is used."
J99-3002,J93-4002,0,0.15747,"eg 3, D-66123 Saarbriicken, Germany. E-mail: nederhof@dfki.de 1 We adopt this term from Sippu and Soisalon-Soininen (1988). In some publications, the term valid prefix property is used. (~) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 to be a parsing algorithm with the ideas from Schabes (1994), which also suggest h o w it can be extended to handle substitution in addition to adjunction. The complexity results carry over to linear indexed grammars, combinatory categorial grammars, and head grammars, since these formalisms are equivalent to TAGs (Vijay-Shanker and Weir 1993, 1994). We present the actual algorithm in Section 3, after the necessary notation has been discussed in Section 2. The correctness proofs are discussed in Section 4, and the time complexity in Section 5. The ideas in this p a p e r give rise to a n u m b e r of questions for further research, as discussed in Section 6. 2. Definitions Our definition of TAGs simplifies the explanation of the algorithm, but differs slightly from standard treatment such as that of Joshi (1987). A tree-adjoining g r a m m a r is a 4-tuple (~, NT, L A), where ~ is the set of terminals, I is the set of initial tree"
J99-3002,C98-2151,1,\N,Missing
N06-1044,P99-1070,0,0.031294,"btain pG (A → α) = P d∈D pD (d) · f (A → α, d) P = d∈D pD (d) · f (A, d) = = P f (d,D) |D |· f (A → α, d) P f (d,D) d∈D |D |· f (A, d) d∈D P f (d, D) · f (A → α, d) . d∈D f (d, D) · f (A, d) d∈D P (14) 346 This is the supervised MLE estimator in (7). This reminds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG distribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. 4 Renormalization In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = (G, pG ), with G = (N, Σ, S, R). We define the renormalization of G as the PCFG R(G) = (G, pR ) with pR specified by pR (A → α) = P d d,w pG (α ⇒ w) d,w pG (A ⇒ w) pG (A → α) · P d . (15) It is not difficult to see that R(G) is a proper PCFG. We now show an important property of R(G), discussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free"
N06-1044,J98-2005,0,0.245306,"rty for a probabilistic context-free grammar is that it be consistent, that is, the grammar should assign probability of one to the set of all finite strings or parse trees that it generates. In other words, the grammar should not lose probability mass with strings or trees of infinite length. Several methods for the empirical estimation of probabilistic context-free grammars have been proposed in the literature, based on the optimization of some function on the probabilities of the observed data, such as the maximization of the likelihood of In later work by (S´anchez and Bened´ı, 1997) and (Chi and Geman, 1998), the result was independently extended to expectation maximization, which is an unsupervised method exploited to estimate probabilistic context-free grammars by finding local maxima of the likelihood of a sample of unannotated sentences. The proof in (S´anchez and Bened´ı, 1997) makes use of spectral analysis of expectation matrices, while the proof in (Chi and Geman, 1998) is based on a simpler counting argument. Both these proofs assume restrictions on the underlying context-free grammars. More specifically, in (Chi and Geman, 1998) empty rules and unary rules are not allowed, thus excludin"
N06-1044,J99-1004,0,0.103634,"∈D pD (d) · f (A → α, d) P = d∈D pD (d) · f (A, d) = = P f (d,D) |D |· f (A → α, d) P f (d,D) d∈D |D |· f (A, d) d∈D P f (d, D) · f (A → α, d) . d∈D f (d, D) · f (A, d) d∈D P (14) 346 This is the supervised MLE estimator in (7). This reminds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG distribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. 4 Renormalization In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = (G, pG ), with G = (N, Σ, S, R). We define the renormalization of G as the PCFG R(G) = (G, pR ) with pR specified by pR (A → α) = P d d,w pG (α ⇒ w) d,w pG (A ⇒ w) pG (A → α) · P d . (15) It is not difficult to see that R(G) is a proper PCFG. We now show an important property of R(G), discussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free grammars. d"
N06-1044,N06-1043,1,0.910503,"an iterative algorithm called inside/outside (Charniak, 1993), which implements the expectation maximization (EM) method (Dempster et al., 1977). Starting with an initial function pG that probabilistically extends G, a so-called growth transformation is computed, defined as f (w, C)· Following (Baum, 1972), one can show that pG (C) ≥ pG (C). Thus, by iterating the growth transformation above, we are guaranteed to reach a local maximum for (8), or possibly a saddle point. We refer to this as the unsupervised MLE method. We now discuss a third estimation method for PCFGs, which was proposed in (Corazza and Satta, 2006). This method can be viewed as a generalization of the supervised MLE method to probability distributions defined over infinite sets of complete derivations. Let D be an infinite set of complete derivations using nonterminal symbols in N , start symbol S ∈ N and terminal symbols in Σ. We assume that the set of rules that are observed in D is drawn from some finite set R. Let pD be a probability distribution defined over D, that is, a function from set D to interval [0, 1] such that P d∈D pD (d) = 1. Consider the CFG G = (N, Σ, R, S). Note that D ⊆ D(G). We wish to extend G to some PCFG G = (G,"
N06-1044,W03-3016,1,0.838784,"→ α, d) P = d∈D pD (d) · f (A, d) = = P f (d,D) |D |· f (A → α, d) P f (d,D) d∈D |D |· f (A, d) d∈D P f (d, D) · f (A → α, d) . d∈D f (d, D) · f (A, d) d∈D P (14) 346 This is the supervised MLE estimator in (7). This reminds us of the well-known fact that maximizing the likelihood of a (finite) sample through a PCFG distribution amounts to minimizing the cross-entropy between the empirical distribution of the sample and the PCFG distribution itself. 4 Renormalization In this section we recall a renormalization technique for PCFGs that was used before in (Abney et al., 1999), (Chi, 1999) and (Nederhof and Satta, 2003) for different purposes, and is exploited in the next section to prove our main results. In the remainder of this section, we assume a fixed, not necessarily proper PCFG G = (G, pG ), with G = (N, Σ, S, R). We define the renormalization of G as the PCFG R(G) = (G, pR ) with pR specified by pR (A → α) = P d d,w pG (α ⇒ w) d,w pG (A ⇒ w) pG (A → α) · P d . (15) It is not difficult to see that R(G) is a proper PCFG. We now show an important property of R(G), discussed before in (Nederhof and Satta, 2003) in the context of so-called weighted context-free grammars. d Lemma 1 For each derivation d w"
N06-1044,P98-2190,0,0.0227201,"e used a novel proof technique that exploits an already known construction for the renormalization of probabilistic contextfree grammars. Our proof technique seems more intuitive than arguments previously used in the literature to prove the consistency property, based on counting arguments or on spectral analysis. It is not difficult to see that our proof technique can also be used with probabilistic rewriting formalisms whose underlying derivations can be characterized by means of context-free rewriting. This is for instance the case with probabilistic tree-adjoining grammars (Schabes, 1992; Sarkar, 1998), for which consistency results have not yet been shown in the literature. A f (y(d),C) (d) · pGpG(y(d)) ·f (A → α, d) d∈D(C) |C| P f (y(d),C) pG (d) · pG (y(d)) ·f (A, d) d∈D(C) |C| P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A → α, d) P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A, d) P estimator (10) already discussed in Section 3. In fact, for each w ∈ L(G) and d ∈ D(G), we have (d) pG (d |w) = ppGG(w) . We conclude with the desired result, namely that a general form PCFG obtained at any iteration of the EM method for the unsupervised MLE is always consistent. Cross-entropy minimization In or"
N06-1044,C92-2066,0,0.0526242,"results, we have used a novel proof technique that exploits an already known construction for the renormalization of probabilistic contextfree grammars. Our proof technique seems more intuitive than arguments previously used in the literature to prove the consistency property, based on counting arguments or on spectral analysis. It is not difficult to see that our proof technique can also be used with probabilistic rewriting formalisms whose underlying derivations can be characterized by means of context-free rewriting. This is for instance the case with probabilistic tree-adjoining grammars (Schabes, 1992; Sarkar, 1998), for which consistency results have not yet been shown in the literature. A f (y(d),C) (d) · pGpG(y(d)) ·f (A → α, d) d∈D(C) |C| P f (y(d),C) pG (d) · pG (y(d)) ·f (A, d) d∈D(C) |C| P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A → α, d) P P pG (d) w∈C f (w, C)· y(d)=w pG (w) ·f (A, d) P estimator (10) already discussed in Section 3. In fact, for each w ∈ L(G) and d ∈ D(G), we have (d) pG (d |w) = ppGG(w) . We conclude with the desired result, namely that a general form PCFG obtained at any iteration of the EM method for the unsupervised MLE is always consistent. Cross-entropy min"
N06-1044,C98-2185,0,\N,Missing
P02-1015,P81-1022,0,0.306895,"rom   ] ÁT' f  . If the item in   , and such that YS'X4  > A ? + @ I ? B | > D / @ 5 F G jl  n can be derived by the algorithm, then the intersection of the language generated by   and the language accepted by the PDA (generated by  ) is non-empty. 4 Earley’s algorithm The CKY algorithm from Figure 5 can be seen to filter out a selection of the computations that may be derived by the deduction system from Figure 2. One may however be even more selective in determining which computations of the PDA to consider. The basis for the algorithm in this section is Earley’s algorithm (Earley, 1970). This algorithm differs from the CKY algorithm in that it satisfies the correct-prefix property (Harrison, 1978). The new algorithm is presented by Figure 6. There are now two types of item involved. The first item has the form j  0Âm:1Z WÂÃS'UWÂÃ¥Tn , where £ 0ÄmÅ1 has the same role as the dotted rules in Earley’s original algorithm. The second and third components are stacks of the PDA as before, but these stacks now contain a distinguished position, indicated by Ã . The existence of an item j Æ 0 m¥1# WÃÇS'XWÃTn implies that ^W_S' 3  ] ^WT' f  , where 3 is now a strin"
P02-1015,A00-2023,0,0.535887,"Italy satta@dei.unipd.it one by one then leads to an unnecessary duplication of subcomputations, since each occurrence of a repeated substring has to be independently parsed. As this approach may be prohibitively expensive, it is preferable to find a parsing algorithm that shares subcomputations among different strings by working directly on the nonterminals and the rules of the non-recursive CFG. In this way, “parsing” a nonterminal of the grammar amounts to shared parsing of all the substrings encoded by that nonterminal. To give a few examples, in some natural language generation systems (Langkilde, 2000) nonrecursive CFGs are used to encode very large sets of candidate sentences realizing some input conceptual representation (Langkilde calls such grammars forests). Each CFG is later “parsed” using a language model, in order to rank the sentences in the set according to their likelyhood. Similarly, in some approaches to automatic speech understand ing (Corazza and Lavelli, 1994) the -best sentences obtained from the speech recognition module are “compressed” into a non-recursive CFG grammar, which is later provided as input to a parser. Finally, in some machine translation applications relate"
P04-1069,P99-1070,0,0.665635,"he question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than"
P04-1069,P89-1018,0,0.330703,"p(τ ) = 1, for each X, Y ∈ Q such that there is at least one transition Y X 7→ Z, Z ∈ Q. The probability of a computation c = τ1 · · · τm , τi ∈ ∆ for 1 ≤ i ≤ m, is p(c) = Q m Pi=1 p(τi ). The probability of a string w is p(w) = (Xin ,w,ε)`c (Xf in ,ε,v) p(c). A PPDT is consistent if Σw∈Σ ∗ p(w) = 1. A PPDT (A, p) is reduced if A is reduced. 3 Parsing Strategies The term “parsing strategy” is often used informally to refer to a class of parsing algorithms that behave similarly in some way. In this paper, we assign a formal meaning to this term, relying on the observation by (Lang, 1974) and (Billot and Lang, 1989) that many parsing algorithms for CFGs can be described in two steps. The first is a construction of push-down devices from CFGs, and the second is a method for handling nondeterminism (e.g. backtracking or dynamic programming). Parsing algorithms that handle nondeterminism in different ways but apply the same construction of push-down devices from CFGs are seen as realizations of the same parsing strategy. Thus, we define a parsing strategy to be a function S that maps a reduced CFG G = (Σ , N, S, R) to a pair S(G) = (A, f ) consisting of a reduced PDT A = (Σ , Σ , Q, Xin , Xf in , ∆), and"
P04-1069,J93-1002,0,0.190802,", who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991; Briscoe and Carroll, 1993; Inui et al., 2000) under the assumption that it would allow more fine-grained probability distributions than the underlying PCFGs. However, this is not the case in general. Consider a PCFG with rule/probability pairs: B → bC , 23 B → bD, 13 C → xc, 1 D → xd , 1 S → AB , 1 A → aC , 13 A → aD, 23 There are two key transitions in the associated LR automaton, which represent shift actions over c and d (we denote LR states by their sets of kernel items and encode these states into stack symbols): τc : {C {C τd : {C {C → x • c, D → x • c, D → x • c, D → x • c, D c → x • d} 7→ → x • d} {C → xc •} d"
P04-1069,J98-2005,0,0.322983,"consisting of complete derivations d, such that for each rule π in G there is at least one d ∈ D in which π occurs. Let nπ,d be the number of occurrences of rule π in derivation d ∈ D, and let nπ be Σd∈D nπ,d , the total number of occurrences of π in D. Let nA be the sum of nπ for all rules π with A in the left-hand side. A probability function pG can be defined through “maximum-likelihood estimation” such that pG (π) = nnAπ for each rule π = A → α. For all nonterminals A, Σπ=A→α pG (π) = Σπ=A→α nnAπ = nnA = 1, which means that the A PCFG (G, pG ) is proper. Furthermore, it has been shown in (Chi and Geman, 1998; S´anchez and Bened´ı, 1997) that a PCFG (G, pG ) is consistent if pG was obtained by maximum-likelihood estimation using a set of derivations. Finally, since nπ &gt; 0 for each π, also pG (π) &gt; 0 for each π, and pG (d) &gt; 0 for all complete derivations d. We say a computation is a shortest dead computation if it is dead and none of its proper prefixes is dead. Note that each dead computation has a unique prefix that is a shortest dead computation. For a PDT A, let TA be the union of the set of all complete computations and the set of all shortest dead computations. Lemma 2 For each Σc∈TA pA (c)"
P04-1069,J99-1004,0,0.423485,"where c is the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0 ∈TA pA (c0 ) ≥ Σw∈Σ∗ pA (w) + pA (c) &gt; Σw∈Σ∗ pA (w) = Σw∈Σ∗ pG (w). This is in contradiction with the consistency of (G, pG ). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG ), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G ) is proper and, for every com1 0 plete P derivation d, p0G (d) = C · pG (d), where C = S⇒d0 w,w∈Σ ∗ pG (d ). Note that if PCFG (G, pG ) in the above lemma is consistent, then C = 1 and (G, p0G ) and (G, pG ) define the same distribution on derivations. The normalization procedure P underlying Lemma 4 makes use of quantities A⇒d w,w∈Σ ∗ pG (d) for each A ∈ N . These quantities can be computed to any degree of precision, as discussed for instance in"
P04-1069,H90-1053,0,0.364753,"ted PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the a"
P04-1069,P89-1017,0,0.0532444,"abilistic transduction, or in other words, we may simplify PDTs to PDAs. The proof of Theorem 5 also leads to the observation that parsing strategies with the CPP and the SPP as well as their probabilistic extensions can be described as grammar transformations, as follows. A given (P)CFG is mapped to an equivalent (P)PDT by a (probabilistic) parsing strategy. By ignoring the output components of swap transitions we obtain a (P)PDA, which can be mapped to an equivalent (P)CFG as shown above. This observation gives rise to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG G, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SP"
P04-1069,W03-3016,1,0.763272,"the above-mentioned dead computation. Due to Lemma 2, 1 ≥ Σc0 ∈TA pA (c0 ) ≥ Σw∈Σ∗ pA (w) + pA (c) &gt; Σw∈Σ∗ pA (w) = Σw∈Σ∗ pG (w). This is in contradiction with the consistency of (G, pG ). Hence, a probability function pA with the properties we required above cannot exist, and therefore S cannot be extended to become a probabilistic parsing strategy. 5 Strong Predictiveness In this section we present our main result, which is a sufficient condition allowing the probabilistic extension of a parsing strategy. We start with a technical result that was proven in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). Lemma 4 Given a non-proper PCFG (G, pG ), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G ) is proper and, for every com1 0 plete P derivation d, p0G (d) = C · pG (d), where C = S⇒d0 w,w∈Σ ∗ pG (d ). Note that if PCFG (G, pG ) in the above lemma is consistent, then C = 1 and (G, p0G ) and (G, pG ) define the same distribution on derivations. The normalization procedure P underlying Lemma 4 makes use of quantities A⇒d w,w∈Σ ∗ pG (d) for each A ∈ N . These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973)"
P04-1069,P94-1017,1,0.725967,"e to an extension with probabilities of the work on covers by (Nijholt, 1980; Leermakers, 1989). 6 Applications Many well-known parsing strategies with the CPP also have the SPP. This is for instance the case for top-down parsing and left-corner parsing. As discussed in the introduction, it has already been shown that for any PCFG G, there are equivalent PPDTs implementing these strategies, as reported in (Abney et al., 1999) and (Tendeau, 1995), respectively. Those results more simply follow now from our general characterization. Furthermore, PLR parsing (Soisalon-Soininen and Ukkonen, 1979; Nederhof, 1994) can be expressed in our framework as a parsing strategy with the CPP and the SPP, and thus we obtain as a new result that this strategy allows probabilistic extension. The above strategies are in contrast to the LR parsing strategy, which has the CPP but lacks the SPP, and therefore falls outside our sufficient condition. As we have already seen in the introduction, it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy. Related to LR parsing is ELR parsing (Purdom and Brown, 1981; Nederhof, 1994), which also lacks the SPP. By an argument similar to the one"
P04-1069,P99-1054,0,0.149528,"the sum of the probabilities of all rules rewriting A must be 1. This Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it means that, out of a total of say m rules rewriting A, only m − 1 rules represent “free” parameters. Depending on the choice of the parsing strategy, the constructed PDA may allow different probability distributions than the underlying CFG, since the set of free parameters may differ between the CFG and the PDA, both quantitatively and qualitatively. For example, (Sornlertlamvanich et al., 1999) and (Roark and Johnson, 1999) have shown that a probability distribution that can be obtained by training the probabilities of a CFG on the basis of a corpus can be less accurate than the probability distribution obtained by training the probabilities of a PDA constructed by a particular parsing strategy, on the basis of the same corpus. Also the results from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be"
P04-1069,P80-1024,0,0.802942,"Missing"
P04-1069,C69-0101,0,0.813844,"nd Geman, We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics, as well as to provide simpler proofs of already known results. We introduce our framework in Section 3 and report our main results in Sections 4 and 5. We discuss applications of our results in Section 6. 2 Preliminaries In this paper we assume some familiarity with definitions of (P)CFGs and (P)PDAs. We refer the reader to standard textbooks and publications as for instance (Harrison, 1978; Booth and Thompson, 1973; Santos, 1972). A CFG G is a tuple (Σ, N, S, R), with Σ and N the sets of terminals and nonterminals, respectively, S the start symbol and R the set of rules. In this paper we only consider left-most derivations, represented as strings d ∈ R∗ and simply called derivations. For α, β ∈ (Σ ∪ N )∗ , we write α ⇒d β with the usual meaning. If α = S and β = w ∈ Σ ∗ , we call d a complete derivation of w. We say a CFG is reduced if each rule in R occurs in some complete derivation. A PCFG is a pair (G, p) consisting of a CFG G and a probability function p from R to real numbers P in the interval [0, 1]. A PCFG is"
P04-1069,J95-2002,0,0.192971,"a 4 Given a non-proper PCFG (G, pG ), G = (Σ, N, S, R), there is a probability function p0G such that PCFG (G, p0G ) is proper and, for every com1 0 plete P derivation d, p0G (d) = C · pG (d), where C = S⇒d0 w,w∈Σ ∗ pG (d ). Note that if PCFG (G, pG ) in the above lemma is consistent, then C = 1 and (G, p0G ) and (G, pG ) define the same distribution on derivations. The normalization procedure P underlying Lemma 4 makes use of quantities A⇒d w,w∈Σ ∗ pG (d) for each A ∈ N . These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). Thus normalization of a PCFG can be effectively computed. For a fixed PDT, we define the binary relation ; on stack symbols by: Y ; Y 0 if and only if (Y, w, ε) `∗ (Y 0 , ε, v) for some w ∈ Σ∗ and v ∈ Σ∗ . In words, some subcomputation of the PDT may start with stack Y and end with stack Y 0 . Note that all stacks that occur in such a subcomputation must have height of 1 or more. We say that a (P)PDA or a (P)PDT has the strong predictiveness property (SPP) if the existence of three transitions X 7→ XY , XY1 7→ Z1 and XY2 7→ Z2 such that Y ; Y1 and Y ; Y2 implies Z1 = Z2 . Informally, this"
P04-1069,1995.iwpt-1.28,0,0.318533,"ts from (Chitrao and Grishman, 1990), (Charniak and Carroll, 1994) and (Manning and Carpenter, 2000) could be seen in this light. The question arises of whether parsing strategies can be extended probabilistically, i.e., whether a given construction of PDAs from CFGs can be “augmented” with a function defining the probabilities for the target PDA, given the probabilities associated with the input CFG, in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent. Some first results on this issue have been presented by (Tendeau, 1995), who shows that the already mentioned left-corner parsing strategy can be extended probabilistically, and later by (Abney et al., 1999) who show that the pure top-down parsing strategy and a specific type of shift-reduce parsing strategy can be probabilistically extended. One might think that any “practical” parsing strategy can be probabilistically extended, but this turns out not to be the case. We briefly discuss here a counter-example, in order to motivate the approach we have taken in this paper. Probabilistic LR parsing has been investigated in the literature (Wright and Wrigley, 1991;"
P04-1070,P89-1018,0,0.3381,"anguage processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this context, the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs. The acronym ‘LR’ stands for ‘Left-to-right processing of the input, producing a Right-most derivation (in reverse)’. When we construct a PDT A from Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it a CFG G by the LR parsing strategy and apply it on an input sentence, then the set of output strings of A represents the set of all right-most derivations that G"
P04-1070,J93-1002,0,0.652688,"of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (P"
P04-1070,J98-2005,0,0.0327906,"rammars (PCFGs); we say a PCFG is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Bened´ı, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in exist"
P04-1070,J99-1004,0,0.127671,"ay a PCFG is proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Bened´ı, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in existing literat"
P04-1070,H90-1053,0,0.0207152,"ank. In other words, the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pG on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e."
P04-1070,W01-1802,0,0.0313342,"parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several transitions are applicable for a given stack, then the sum of probabilities assigned to those transitions by probability function pA should be 1. This assumption may be motivated by pragmatic considerations, as such a prop"
P04-1070,J98-4004,0,0.481322,"ulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pG on rules of the original grammar. A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context, i.e. information on rules applied earlier, so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities. The explicit use of left context in probabilistic context-free models was investigated by e.g. (Chitrao and Grishman, 1990; Johnson, 1998), who also demonstrated that this may significantly improve accuracy. Note that the probability distributions of language may be beyond the reach of a given context-free grammar, as pointed out by e.g. (Collins, 2001). Therefore, the use of left context, and the resulting increase in the number of parameters of the model, may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language. One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al., 2000) is that trained probabilistic LR parsers should be proper, i.e. if several trans"
P04-1070,1993.iwpt-1.12,0,0.678874,", under the restrictions of the existing approaches to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an i"
P04-1070,P96-1032,1,0.810879,". There are a few superficial differences with LR parsing as it is commonly found in the literature. The most obvious difference is that we divide reductions into ‘binary’ steps. The main reason is that this allows tabular interpretation with a time complexity cubic in the length of the input. Otherwise, the time complexity would be O(nm+1 ), where m is the length of the longest right-hand side of a rule in the CFG. This observation was made before by (Kipps, 1991), who proposed a solution similar to ours, albeit formulated differently. See also a related formulation of tabular LR parsing by (Nederhof and Satta, 1996). To be more specific, instead of one step of the PDT taking stack: σp0 p1 · · · pm immediately to stack: σp0 q where (A → X1 · · · Xm •) ∈ pm , σ is a string of stack symbols and goto(p0 , A) = q, we have a number of smaller steps leading to a series of stacks: σp0 p1 · · · pm−1 pm σp0 p1 · · · pm−1 (A, m−1) σp0 p1 · · · (A, m−2) .. . σp0 (A, 0) σp0 q There are two additional differences. First, we want to avoid steps of the form: σp0 (A, 0) σp0 q ε,ε by transitions p0 (A, 0) 7→ p0 q, as such transitions complicate the generic definition of ‘properness’ for PDTs, to be discussed in the follow"
P04-1070,W03-3016,1,0.915007,"s proper if for each nonterminal A, the probabilities of all rules with left-hand side A sum to 1. Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees. In other words, if a probability distribution can be defined by attaching probabilities to rules, then we may reassign the probabilities such that that PCFG becomes proper, while preserving the probability distribution. This even holds if the input grammar is non-tight, meaning that probability mass is lost in ‘infinite derivations’ (S´anchez and Bened´ı, 1997; Chi and Geman, 1998; Chi, 1999; Nederhof and Satta, 2003). Although CFGs and PDTs are weakly equivalent, they behave very differently when they are extended with probabilities. In particular, there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality. Below we will discuss two constraints, which we will call properness and reverseproperness. Neither of these is suitable for all types of PDTs, but as we will show, the second is more suitable for probabilistic LR parsing than the first. This is surprising, as only properness has been described in existing literature on probabilistic PDTs ("
P04-1070,P04-1069,1,0.80105,"s than prfe does. (By ‘consistent’ we mean that the probabilities of all strings that are accepted sum to 1.) It may even be the case that a (proper and consistent) probability function pG on the rules of the input grammar G exists that assigns a higher likelihood to the corpus than prfe , and therefore it is not guaranteed that LR parsers allow better probability estimates than the CFGs from which they were constructed, if we constrain probability functions pA to be proper. In this respect, LR parsing differs from at least one other well-known parsing strategy, viz. left-corner parsing. See (Nederhof and Satta, 2004) for a discussion of a property that is shared by left-corner parsing but not by LR parsing, and which explains the above difference. As main contribution of this paper we establish that this restriction on expressible probability distributions can be dispensed with, without losing the ability to perform training by relative frequency estimation. What comes in place of properness is reverse-properness, which can be seen as properness of the reversed pushdown automaton that processes input from right to left instead of from left to right, interpreting the transitions of A backwards. As we will"
P04-1070,1991.iwpt-1.18,0,0.519785,"nsition Q P 7→ R by relative frequency estimation. The resulting PPDT is proper. It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies, among which are top-down parsing and left-corner parsing. This does not hold for PDTs constructed by the LR parsing strategy however, and in fact, properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG. This was formally proven by (Nederhof and Satta, 2004), after (Ng and Tomita, 1991) and (Wright and Wrigley, 1991) had already suggested that creating a probabilistic LR parser that is equivalent to an input PCFG is difficult in general. The same difficulty for ELR parsing was suggested by (Tendeau, 1997). For this reason, we investigate a practical alternative, viz. reverse-properness. Now we have to assume that for each stack symbol R, we either have a,b one or more transitions of the form P 7→ R or a,b Q P 7→ R, or one or more transitions of the form a,b P 7→ P R, but no combination thereof. In the first case, reverse-properness demands that the sum of a,b a,b probabiliti"
P04-1070,C00-2098,0,0.120212,"to training of LR parsers. We present an alternative way of training that is provably optimal, and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser. We also demonstrate empirically that this kind of training can be effectively applied on a large treebank. 1 Introduction The LR parsing strategy was originally devised for programming languages (Sippu and SoisalonSoininen, 1990), but has been used in a wide range of other areas as well, such as for natural language processing (Lavie and Tomita, 1993; Briscoe and Carroll, 1993; Ruland, 2000). The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic, in order to deal with ambiguous context-free grammars (CFGs). Nondeterminism can be handled in a number of ways, but the most efficient is tabulation, which allows processing in polynomial time. Tabular LR parsing is known from the work by (Tomita, 1986), but can also be achieved by the generic tabulation technique due to (Lang, 1974; Billot and Lang, 1989), which assumes an input pushdown transducer (PDT). In this co"
P04-1070,C69-0101,0,0.753126,"rsers do not use lookahead to decide between alternative transitions, they are called LR(0) parsers. More generally, if LR parsers look ahead k symbols, they are called LR(k) parsers; some simplified LR parsing models that use lookahead are called SLR(k) and LALR(k) parsing (Sippu and Soisalon-Soininen, 1990). In order to simplify the discussion, we abstain from using lookahead in this article, and ‘LR parsing’ can further be read as ‘LR(0) parsing’. We would like to point out however that our observations carry over to LR parsing with lookahead. The theory of probabilistic pushdown automata (Santos, 1972) can be easily applied to LR parsing. A probability is then assigned to each transition, by a function that we will call the probability function pA , and the probability of an accepting computation of A is the product of the probabilities of the applied transitions. As each accepting computation produces a right-most derivation as output string, a probabilistic LR parser defines a probability distribution on the set of parses, and thereby also a probability distribution on the set of sentences generated by grammar G. Disambiguation of an ambiguous sentence can be achieved on the basis of a co"
P11-1047,P99-1070,0,0.699695,"−1 for each j (1 ≤ j ≤ r). Consider again a synchronous rule s of the form in (2). We say s is an epsilon rule if r = 0 and u10 = u20 = . We say s is a unit rule if r = 1 and u10 = u11 = u20 = u21 = . Similarly to context-free grammars, absence of epsilon rules and unit rules guarantees that there are no cyclic dependencies between items and in this case the inside algorithm correctly computes pG ([w1 , w2 ]). Epsilon rules can be eliminated from PSCFGs by a grammar transformation that is very similar to the transformation eliminating epsilon rules from a probabilistic context-free grammar (Abney et al., 1999). This is sketched in what follows. We first compute the set of all nullable linked pairs of nonterminals of the underlying SCFG, that is, the set of all [A1 , A2 ] ∈ N [2] such that [A11 , A21 ] ⇒∗G [ε, ε]. This can be done in linear time O(|G|) using essentially the same algorithm that identifies nullable nonterminals in a context-free grammar, as presented for instance by Sippu and Soisalon-Soininen (1988). Next, we identify all occurrences of nullable pairs [A1 , A2 ] in the right-hand side components of a rule s, such that A1 and A2 have the same index. For every possible choice of a subs"
P11-1047,J99-1004,0,0.0447973,"veral syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy satta@dei.unipd.it This paper considers a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix s"
P11-1047,J07-2003,0,0.0608215,"owing interest in so-called syntaxbased translation models, that is, models that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy sa"
P11-1047,W02-1020,0,0.0203238,"an be computed as a special case of the joint prefix probability. Concretely, one can extend the input and the grammar by introducing an end-of-sentence marker $. Let G0 be the underlying SCFG grammar after the extension. Then: pr−prefix ([v1 , v2 ]) = pprefix ([v1 $, v2 ]) G G0 Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation of speech, or alternatively as a predictive tool in applications of interactive machine translation, of the kind described by Foster et al. (2002). We provide some technical details here, generalizing to PSCFGs the approach by Jelinek and Lafferty (1991). Let G = (G, pG ) be a PSCFG, with Σ the alphabet of terminal symbols. We are interested in the probability that the next terminal in the target translation is a ∈ Σ, after having processed a prefix v1 of the source sentence and having produced a prefix v2 468 of the target translation. This can be computed as: pr−word (a |[v1 , v2 ]) = G pprefix ([v1 , v2 a]) G pprefix ([v1 , v2 ]) G Two considerations are relevant when applying the above formula in practice. First, the computation of"
P11-1047,N04-1035,0,0.117483,"Missing"
P11-1047,N07-1019,0,0.0172894,"component. By standard complexity analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1 |rmax +1 · |w2 |rmax +1 ), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes pG ([w1 , w2 ]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic idea, let us first assume that each it"
P11-1047,D10-1063,0,0.0186147,"analysis of deduction systems, for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P |· |w1 |rmax +1 · |w2 |rmax +1 ), where rmax is the maximum number of right-hand side nonterminals in either component of a synchronous rule. The algorithm therefore runs in exponential time, when the grammar G is considered as part of the input. Such computational behavior seems unavoidable, since the recognition problem for SCFG is NP-complete, as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above. The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items, as instantiations of the consequent and the antecedents, respectively, of the inference rule in figure 1. A probabilistic parsing algorithm that computes pG ([w1 , w2 ]), defined in (1), can also be obtained from the recognition algorithm above, by associating each item with a probability. To explain the basic idea, let us first assume that each item can be inferred in finitely m"
P11-1047,J91-3004,0,0.748813,"ders a parsing problem that is well understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the form vw, for any string w over the alphabet of the model. This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech. This has applications in incremental processing of text or speech from left to right; see again (Jelinek and Lafferty, 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search (Corazza et al., 1991). This paper investigates the problem of computing prefix probabilities for PSCFGs. In this context, a pair of strings v1 and v2 is given as input, and we are asked to compute the probability that any st"
P11-1047,H05-1101,1,0.890375,"or the computation of inside probabilities for PSCFG. This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. Our method for computing the prefix probabilities for PSCFGs runs in exponential time, since that is the running time of existing methods for computing the inside probabilities for PSCFGs. It is unlikely this can be improved, because the recognition problem for PSCFG is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. 2 by annotating nonterminals with indices from an infinite set. We define I(N ) = {A t |A ∈ N, t ∈ N} and VI = I(N ) ∪ Σ. For a string γ ∈ VI∗ , we write index(γ) to denote the set of all indices that appear in symbols in γ. Two strings γ1 , γ2 ∈ VI∗ are synchronous if each index from N occurs at most once in γ1 and at most once in γ2 , and index(γ1 ) = index(γ2 ). Therefore γ1 , γ2 have the general form: t t t γ1 = u10 A111 u11 A122 u12 · · · u1r−"
P11-1047,J95-2002,0,0.658593,"understood for probabilistic context-free grammars but that has never been investigated in the context of PSCFGs, viz. the computation of prefix probabilities. In the case of a probabilistic context-free grammar, this problem is defined as follows. We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input. This quantity is defined as the (possibly infinite) sum of the probabilities of all strings of the form vw, for any string w over the alphabet of the model. This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995). Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech. This has applications in incremental processing of text or speech from left to right; see again (Jelinek and Lafferty, 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search (Corazza et al., 1991). This paper investigates the problem of computing prefix probabilities for PSCFGs. In this context, a pair of strings v1 and v2 is given as input, and we are asked to compute the probability that any string in the source lan"
P11-1047,J97-3002,0,0.205746,"area of statistical machine translation, there has been a growing interest in so-called syntaxbased translation models, that is, models that define mappings between languages through hierarchical sentence structures. Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction. Probabilistic synchronous context-free grammars (PSCFGs) are one among the most popular examples of such models. PSCFGs subsume several syntax-based statistical translation models, as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text, as in Galley et al. (2004). Despite the widespread usage of models related to PSCFGs, our theoretical understanding of this class is quite limited. In contrast to the closely related class of probabilistic context-free grammars, a syntax model for which several interesting mathematical and statistical properties have been investigated, as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs. Giorgio Satta Dept. of Information Engineeri"
P11-1047,N06-1033,0,0.0224984,"in the introduction, since the recognition problem for PSCFGs is NP-complete, as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs. One should add that, in real world machine translation applications, it has been observed that recognition (and computation of inside probabilities) for SCFGs can typically be carried out in low-degree polynomial time, and the worst cases mentioned above are not observed with real data. Further discussion on this issue is due to Zhang et al. (2006). 5 Discussion We have shown that the computation of joint prefix probabilities for PSCFGs can be reduced to the computation of inside probabilities for the same model. Our reduction relies on a novel grammar transformation, followed by elimination of epsilon rules and unit rules. Next to the joint prefix probability, we can also consider the right prefix probability, which is defined by: pr−prefix ([v1 , v2 ]) = G X pG ([v1 , v2 w]) w In words, the entire left string is given, along with a prefix of the right string, and the task is to sum the probabilities of all string pairs for different s"
P16-1106,E99-1008,0,0.114598,"Missing"
P16-1106,P87-1015,0,0.502128,"Missing"
P16-1106,P12-1070,0,0.0298667,"Missing"
P16-1106,P83-1001,0,0.371657,"Missing"
P94-1017,E93-1036,1,0.909467,"two entries represent the same subderivation. Introduction Left-corner (LC) parsing is a parsing strategy which has been used in different guises in various areas of computer science. Deterministic LC parsing with k symbols of lookahead can handle the class of LC(k) grammars. Since LC parsing is a very simple parsing technique and at the same time is able to deal with left recursion, it is often used as an alternative to top-down (TD) parsing, which cannot handle left recursion and is generally less efficient. Nondeterministic LC parsing is the foundation of a very efficient parsing algorithm [7], related to Tomita's algorithm and Earley's algorithm. It has one disadvantage however, which becomes noticeable when the g r a m m a r contains m a n y rules whose right-hand sides begin with the same few g r a m m a r s symbols, e.g. A ~ c~f~l I ~f~2 I ... where ~ is not the e m p t y string. After an LC parser has recognized the first symbol X of such an c~, it will as next step predict all aforementioned rules. This amounts to much nondeterminism, which is detrimental both to the time-complexity and the space-complexity. *Supported by the Dutch Organisation for Scientific Research (NWO),"
P94-1017,P94-1029,1,0.723127,"urce of bad performance for this technique. Then, a multitude of parsing techniques which exhibit better t r e a t m e n t of common prefixes is discussed. These techniques, including nondeterministic PLR, ELR, and CP parsing, have their origins in theory of deterministic, parallel, and t a b u l a r parsing. Subsequently, the application to parallel and tabular parsing is investigated more closely. Further, we briefly describe how rules with e m p t y right-hand sides complicate the parsing process. The ideas described in this p a p e r can be generalized to head-driven parsing, as argued in [9]. We will take some liberty in describing algorithms from the existing literature, since using the original descriptions would blur the similarities of the algorithms to one another. In particular, we will not treat the use of lookahead, and we will consider all algorithms working on a stack to be nondeterministic. We will only describe recognition algorithms. Each of the algorithms can however be easily extended to yield parse trees as a side-effect of recognition. notation of rules A --* a l , A --* a 2 , . . , with the same lhs is often simplified to A ~ c~1]a21... A rule of the form A --~"
P94-1017,1993.iwpt-1.18,0,0.0614533,"sing is presented in [20]. Some useful data structures for practical implementation of tabular and non-tabular PLR, E L R and CP parsing are described in [S], Finding an optimal tabular algorithm In [14] Schabes derives the LC algorithm from LR parsing similar to the way that E L R parsing can be derived from LR parsing. The LC algorithm is obtained by not only splitting up the goto function into goto 1 and goto 2 but also splitting up goto~ even further, so that it nondeterministically yields the closure of one single kernel item. (This idea was described earlier in [5], and more recently in [10].) Schabes then argues that the LC algorithm can be determinized (i.e. made more deterministic) by manipulating the goto functions. One application of this idea is to take a fixed g r a m m a r and choose different goto functions for different parts of the grammar, in order to tune the parser to the grammar. In this section we discuss a different application of this idea: we consider various goto functions which are global, i.e. which are the same for all parts of a grammar. One example is E L R parsing, as its goto~ function can be seen as a determinized version of the goto 2 function of LC p"
P94-1017,P80-1024,0,0.833602,"f, B --~ f?C7 • p t such that D / * C 4. (F[B --*/3][A --, a ] , v ) ~- (F[B --*/~A], v) where A --~ a • pT and where there is B --~/3A7 • pt Note that since the a u t o m a t o n does not use any lookahead, Step 3 may also have replaced [T ---* F •] by any other item besides [T --* T • • F] whose rhs starts with T and whose lhs satisfies the condition of topdown filtering with regard to E, i.e. by [T --~ T • * * F ] , [E ~ T . T El, or [E ~ T •]. [] LC parsing with k symbols of lookahead can handle deterministically the so called LC(k) grammars. This class of g r a m m a r s is formalized in [13]. 1 How LC parsing can be improved to handle common su~xes efficiently is discussed in [6]; in this p a p e r we restrict our attention to common prefixes. E x a m p l e 2 Consider the g r a m m a r from Example 1. Using Predictive LR, recognition of a * a is realised by: PLR, ELR, and CP parsing [E' ~ ] [E' In this section we investigate a number of algorithms which exhibit a better treatment of common prefixes. ][F a *a a] [E' --~ ][T ---* F] [E' --* ][T --* T] [E' --* ][T ~ T .] : Predictive LR parsing Predictive LR (PLR) parsing with k symbols of lookahead was introduced in [17] as an algo"
P94-1017,P91-1014,0,0.279535,"~ a] which would not occur in any search p a t h of the nondeterministic ELR algorithm, because in general such a A is the union of m a n y sets A' of items [A ~ --~ a] which would be manipulated at the same input position by the nondeterministic algorithm in different search paths. With minor differences, the above tabular E L R algorithm is described in [21]. A tabular version of pseudo E L R parsing is presented in [20]. Some useful data structures for practical implementation of tabular and non-tabular PLR, E L R and CP parsing are described in [S], Finding an optimal tabular algorithm In [14] Schabes derives the LC algorithm from LR parsing similar to the way that E L R parsing can be derived from LR parsing. The LC algorithm is obtained by not only splitting up the goto function into goto 1 and goto 2 but also splitting up goto~ even further, so that it nondeterministically yields the closure of one single kernel item. (This idea was described earlier in [5], and more recently in [10].) Schabes then argues that the LC algorithm can be determinized (i.e. made more deterministic) by manipulating the goto functions. One application of this idea is to take a fixed g r a m m a r and c"
P94-1017,P89-1018,0,0.190087,"forest (a compact representation of all parse trees) which is output by a tabular algorithm may in this case not be optimally dense. We conclude t h a t we have a tradeoff between the case that the g r a m m a r allows almost deterministic parsing and the case that the stack algorithm is very nondeterministic for a certain grammar. In the former case, sophistication leads to less entries in the table, and in the latter case, sophistication leads to more entries, provided this sophistication is realised by an increase in the n u m b e r of states. This is corroborated by empirical d a t a from [1, 4], which deal with tabular LR parsing. As we will explain, CP and E L R parsing are more deterministic t h a n most other parsing algorithms for m a n y grammars, but their tabular realizations can never compute the same subderivation twice. This represents an o p t i m u m in a range of possible parsing algorithms. This p a p e r is organized as follows. First we discuss nondeterministic left-corner parsing, and demonstrate how common prefixes in a g r a m m a r m a y be a source of bad performance for this technique. Then, a multitude of parsing techniques which exhibit better t r e a t m e n"
P94-1017,P89-1017,0,0.439068,"the first symbol of an item which has been introduced in Q by means of the closure function. In the second case, a state which is a variant of goto(Q,X) is pushed on top of state Q as usual. In the first case, however, state Q on top of the stack is replaced by a variant of goto(Q, X ) . This is safe since we will never need to return to Q if after some more steps we succeed in recognizing some rule corresponding with one of the items in Q. A consequence of the action in the first case is that upon reduction we need to pop only one state off the stack. Further work in this area is reported in [5], which treats nondeterministic E L R parsing and therefore does not regard it as an obstacle if a choice between cases a) and b) cannot be uniquely made. We are not concerned with extended context-free g r a m m a r s in this paper. However, a very interesting algorithm results from E L R parsing if we restrict its application to ordinary context-free grammars. (We will maintain the name ""extended LR"" to stress the origin of the algorithm.) This results in the new nondeterministic ELR(0) algorithm that we describe below, derived from the formulation of E L K parsing in [5]. First, we define a"
P94-1029,E93-1010,0,0.171951,"entful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of G r a m m a r (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th"
P94-1029,P89-1017,0,0.366895,"his p a p e r which has already a p p e a r e d in the literature, in different guises [6, 13, 2, 14]. Extended HI parsing T h e P H I a l g o r i t h m can process s i m u l t a n e o u s l y a comm o n infix a in two different rules A --* 131_~-/1 and A --* 132_~72, which reduces n o n d e t e r m i n i s m . We m a y however also specify an a l g o r i t h m which succeeds in s i m u l t a n e o u s l y processing all c o m m o n infixes, irrespective of whether the left-hand sides of the corresponding rules are the same. This a l g o r i t h m is inspired by exlended L R ( E L R ) parsing [12, 7] for extended context-free g r a m m a r s (where right-hand sides consist of regular expressions over V). By analogy, it will be called extended H I (EHI) parsing. This a l g o r i t h m uses yet a n o t h e r kind of item, viz. of the form [ i , k , { A 1 , A ~ , . . . , A p } --* - / , m , j ] , where there exists at least one rule A --* a_713 for each A E {A1,Au,...,Ap}. W i t h such an item, we simulate c o m p u t a t i o n of different items [i, k, A --* a * -/ * 13, m, j] E I He which would be treated individually by an H C parser. Formally, we have Predictive HI parsing We say two rul"
P94-1029,P94-1017,1,0.723882,"ation of left recursion for traditional T D parsing. In the ease of g r a m m a r s with some p a r a m e t e r mechanism, top-down parsing has the advantage over other kinds of parsing that top-down propagation of parameter values is possible in collaboration with context-free parsing (eft the standard evaluation of definite clause grammars), which m a y lead to more efficient processing. This holds for left-to-right parsing as well as for head-driven parsing [10]. A family of head-driven algorithms This section investigates the adaptation of a family of left-to-right parsing algorithms from [8], viz. top-down, left-corner, PLR, ELR, and LR parsing, to head grammars. Top-down parsing The following is a straightforward adaptation of topdown (TD) parsing [1] to head grammars. There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, the other of the form [i, k, A --* a • 7 •/3, m, j], which also indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, but specifically using the rule A --~ ot7/3, where 7 -&apos;-~* ak+x . . . a,n has already been esta"
P94-1029,1993.iwpt-1.16,1,0.793803,"thm 6 on rheaa is twofold. Firstly, the algorithm above is more appropriate for presentational purposes than an alternative sit is interesting to compare LR parsing for a context-free grammar G with LR parsing for the transformed grammar rtwo(G). The transformation has the effect that a reduction with a rule is replaced by a cascade of reductions with smaller rules; apart from this, the transformation does not affect the global run-time behaviour of LR parsing. More serious are the consequences for the size of the parser: the required number of LR states for the transformed grammar is smaller [9]. 215 P•S ~3,0 X3,1 ~3,1 X3,2 of a rule, and then the remaining members of the rhs, in an outward order• Conversely, we have head-inward ( H I ) derivations, where first the remaining m e m b e r s in the rhs are expanded, in an inward order (toward the head), after which the head itself is recursively expanded. Note that HI parsing recognizes a string by computing an HI derivation in reverse (of. Lit parsing). Let w = axa2 • . - a n , n &gt; 1, be a string over T and let a0 = .1_. For - 1 &lt; i &lt; j &lt; n, we write ( i , j ] , , to denote substring ai+ l • • • aj . ~2 T h e o r e m 1 For A one o f A"
P94-1029,H93-1101,0,0.140272,"for some A. Head-driven T D parsing m a y loop exactly for the g r a m m a r s which are head-recursive. Head reeursion is a generalization of left recursion for traditional T D parsing. In the ease of g r a m m a r s with some p a r a m e t e r mechanism, top-down parsing has the advantage over other kinds of parsing that top-down propagation of parameter values is possible in collaboration with context-free parsing (eft the standard evaluation of definite clause grammars), which m a y lead to more efficient processing. This holds for left-to-right parsing as well as for head-driven parsing [10]. A family of head-driven algorithms This section investigates the adaptation of a family of left-to-right parsing algorithms from [8], viz. top-down, left-corner, PLR, ELR, and LR parsing, to head grammars. Top-down parsing The following is a straightforward adaptation of topdown (TD) parsing [1] to head grammars. There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+l • • • aj, the other of the form [i, k, A --* a • 7 •/3, m, j], which also indicates that some subderivation from A is needed"
P94-1029,W89-0205,1,0.843449,"entful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of G r a m m a r (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th"
P94-1029,1993.iwpt-1.21,0,0.601823,"entful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of G r a m m a r (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th"
P94-1029,W89-0206,0,\N,Missing
P96-1032,P89-1018,0,0.122098,"nces, the graph-structured stacks used to describe Tomita&apos;s algorithm differ very little from parse fables, or in other words, generalized LR parsing is one of the so called tabular parsing algorithms, among which also the CYK algorithm (Harrison, 1978) and Earley&apos;s algorithm (Earley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given by Lang (1974) and Billot and Lang (1989): the construction of pushdown automata from grammars and the simulation of these automata by means of tabulation yield different tabular algorithms for different such constructions. Another example, on which our presentation is based, was first suggested by Leermakers (1989): a grammar is first transformed and then a standard tabular algorithm along with some filtering condition is applied using the transformed grammar. In our case, the transformation and the subsequent application of the tabular algorithm result in a new form of tabular LR parsing. Our method is more efficient than Tomita&apos;s"
P96-1032,P94-1029,1,0.885891,"such that q&apos; = goto&apos;(q, A). (ii) A 6 Uj,j if (A --+ e) 6 P, A E pred(Uj); (iv) (X,q) (o~) ~ Note that in the case of a reduce/reduce conflict with two grammar rules sharing some suffix in the right-hand side, the gathering steps of A2Lrt will treat both rules simultaneously, until the parts of the right-hand sides are reached where the two rules differ. (See Leermakers (1992a) for a similar sharing of computation for common suffixes.) An interesting fact is that the automaton .A2LR is very similar to the automaton .ALR constructed for a grammar transformed by the transformation rtwo given by Nederhof and Satta (1994). 2 5 The (iii) A 6 Ui,j if B 6 Ui,~, C 6 Uk,j, (A ---. BC) 6 P, A 6 pred(Ui); (iv) A 6 Uij if B 6 Uij, (A ~ B) 6 P, A 6 pred(UO. algorithm This section presents a tabular LR parser, which is the main result of this paper. The parser is derived from the 2LR automata introduced in the previous section. Following the general approach presented by Leermakers (1989), we simulate computations of 2For the earliest mention of this transformation, we have encountered pointers to Schauerte (1973). Regrettably, we have as yet not been able to get hold of a copy of this paper. 242 The string has been acc"
P96-1032,P81-1022,0,0.579562,"omputational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing, also known as Tomita&apos;s algorithm, described by Tomita (1986) and further investigated by, for example, Tomita (1991) and Nederhof (1994a). Despite appearances, the graph-structured stacks used to describe Tomita&apos;s algorithm differ very little from parse fables, or in other words, generalized LR parsing is one of the so called tabular parsing algorithms, among which also the CYK algorithm (Harrison, 1978) and Earley&apos;s algorithm (Earley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given by Lang (1974) and Billot and Lang (1989): the construction of pushdown automata from grammars and the simulation of these automata by means of tabulation yield different tabular algorithms for different such constructions. Another example, on which our presentation is based, was first suggested by Leermakers"
P96-1032,P91-1014,0,0.191887,"wo parsing techniques, expressed by the 245 automata A~,R and A2La. The tabular realisation of the former automata is very close to a variant of Tomita&apos;s algorithm by Kipps (1991). The objective of our experiments was to show that the automata ~4~La provide a better basis than .A~a for tabular LR parsing with regard to space and time complexity. Parsing algorithms that are not based on the LR technique have however been left out of consideration, and so were techniques for unification grammars and techniques incorporating finite-state processes. 3 Theoretical considerations (Leermakers, 1989; Schabes, 1991; Nederhof, 1994b) have suggested that for natural language parsing, LR-based techniques may not necessarily be superior to other parsing techniques, although convincing empirical data to this effect has never been shown. This issue is difficult to resolve because so much of the relative efficiency of the different parsing techniques depends on particular grammars and particular input, as well as on particular implementations of the techniques. We hope the conceptual framework presented in this paper may at least partly alleviate this problem. Acknowledgements The first author is supported by"
P96-1032,P89-1017,0,0.160208,"ley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given by Lang (1974) and Billot and Lang (1989): the construction of pushdown automata from grammars and the simulation of these automata by means of tabulation yield different tabular algorithms for different such constructions. Another example, on which our presentation is based, was first suggested by Leermakers (1989): a grammar is first transformed and then a standard tabular algorithm along with some filtering condition is applied using the transformed grammar. In our case, the transformation and the subsequent application of the tabular algorithm result in a new form of tabular LR parsing. Our method is more efficient than Tomita&apos;s algorithm in two respects. F i r s t , reduce operations are implemented in an efficient way, by splitting them into several, more primitive, operations (a similar idea has been proposed by Kipps (1991) for Tomita&apos;s algorithm). Second, several paths in the computation that mu"
P96-1032,E93-1036,1,0.905825,"the number of parses found by the algorithm for any input, is reduced to exactly that of the source grammar. A practical implementation would construct the parse trees on-the-fly, attaching them to the table entries, allowing packing and sharing of subtrees (cf. the literature on parse forests (Tomita, 1986; Elllot and Lang, 1989)). Our algorithm actually only needs one (packed) subtree for several (X, q) E Ui,k with fixed X , i , k but different q. The resulting parse forests would then be optimally compact, contrary to some other LR-based tabular algorithms, as pointed out by Rekers (1992), Nederhof (1993) and Nederhof (1994b). 6 of the algorithm In this section, we investigate how the steps performed by Algorithm 1 (applied to the 2LR cover) relate to those performed by .A2LR, for the same input. We define a subrelation ~ + of t-+ as: (6, uw) ~+ (66&apos;,w) if and only if (6, uw) = (6, zlz2"".&apos;zmw) t(88l,z2..-zmw) ~- ... ~ (68re,w) = (86&apos;,w), for some m &gt; 1, where I~kl &gt; 0 for all k, 1 &lt; k &lt; m. Informally, we have (6, uw) ~+ (6~&apos;, w) if configuration (~8&apos;, w) can be reached from (6, uw) without the bottom-most part 8 of the intermediate stacks being affected by any of the transitions; furthermore,"
P96-1032,P94-1017,1,0.648277,"Secondly, the static and dynamic complexity of parsing, both in space and time, is significantly reduced. 1 Introduction The efficiency of LR(k) parsing techniques (Sippu and Soisalon-Soininen, 1990) is very attractive from the perspective of natural language processing applications. This has stimulated the computational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing, also known as Tomita&apos;s algorithm, described by Tomita (1986) and further investigated by, for example, Tomita (1991) and Nederhof (1994a). Despite appearances, the graph-structured stacks used to describe Tomita&apos;s algorithm differ very little from parse fables, or in other words, generalized LR parsing is one of the so called tabular parsing algorithms, among which also the CYK algorithm (Harrison, 1978) and Earley&apos;s algorithm (Earley, 1970) can be found. (Tabular parsing is also known as chart parsing.) In this paper we investigate the extension of LR parsing to general context-free grammars from a more general viewpoint: tabular algorithms can often be described by the composition of two constructions. One example is given"
P98-2156,P90-1035,0,0.508439,"for context-free grammars. The construction of derived trees and the computation of features also become straightforward. 1 Introduction The efficiency of LR(k) parsing techniques (Sippu and Soisalon-Soininen, 1990) a p p e a r s to be very attractive from the perspective of natural language processing. This has stimulated the computational linguistics community to develop extensions of these techniques to general context-free grammar parsing. The best-known example is generalized LR parsing (Tomita, 1986). A first a t t e m p t to adapt LR parsing to treeadjoining grammars (TAGs) was made by Schabes and Vijay-Shanker (1990). The description was very complicated however, and not surprisingly, no implementation of the algorithm seems to have been made up to now. Apart from presentational difficulties, the algorithm as it was published is also incorrect. Brief indications of the nature of the incorrectness have been given before by Kinyon (1997). There seems to be no straightforward way to correct the algorithm. We therefore developed an alternative to the algorithm from Schabes and Vijay-Shanker (1990). This alternative is novel in presentational aspects, and is fundamentally different in that it incorporates redu"
P98-2157,J91-3004,0,0.178971,"amme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adj"
P98-2157,C92-2066,0,0.0422418,"s that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities ~we~* P r"
P98-2157,C88-1075,0,0.0391071,"s for stochastic TAGs. flff/~2 n 4.1 C General equations E The prefix probability is given by: Figure 2: Wrapping of auxiliary trees when computing the prefix probability To derive a method for the computation of prefix probabilities, we give some simple recursive equations. Each equation decomposes an item into other items in all possible ways, in the sense that it expresses the probability of that item as a function of the probabilities of items associated with equal or smaller portions of the input. In specifying the equations, we exploit techniques used in the parsing of incomplete input (Lang, 1988). This allows us to compute the prefix probability as a by-product of computing the inside probability. 955 Pr(al...anw) wEE* = ~ P([t,O,n,-,-]), fEZ where P is a function over items recursively defined as follows: P([t,i,j, fl,f2]) = P([Rt, i,j, fl,f2]); P([t~N,i,j,-,-]) = P([a,i,k,-,-]) k(i < k < j) . P([N,k,j,-,-]), if a ¢ e A -~dft(aN); P([t~N, i, j, fl, f2]) = Z P([a,i,k,-,-])-P([N,k,j, k(i < k < fl) if ~ ¢ ¢ A dft(g); (1) (2) (3) fl,f2]), P([aN, i, j, fl, f2]) = (4) P([a, i, k, fl, f2]). P([N, k, j, -, -]), k(f2 <_ k <_j) if # c^ ((i',j') = (i,j))A = (fl, f2)v ((f~ = f~ = iV f{ = f~ = j"
P98-2157,J95-2002,0,0.358178,"hnology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations i"
P98-2157,W89-0211,0,0.0328774,"and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be use"
Q19-1018,N18-2066,0,0.0176189,"ound that accuracy improves if the classifier is trained not just on configurations that correspond to the ground-truth, or ‘‘gold’’, tree, but also on configurations that a parser would typically reach when a classifier strays from the optimal predictions. This is known as a dynamic oracle.1 The effective calculation of the optimal step for some kinds of parsing relies on ‘arc-decomposibility’, as in the case of Goldberg and Nivre (2012, 2013). This generally requires a projective training corpus; an attempt to extend this to non-projective training corpora had to resort to an approximation (Aufrant et al., 2018). It is known how to calculate the optimal step for a number of non1 A term we avoid here, as dynamic oracles are neither oracles nor dynamic, especially in our formulation, which allows gold trees to be non-projective. Following, for example, Kay (2000), an oracle informs a parser whether a step may lead to the correct parse. If the gold tree is non-projective and the parsing strategy only allows projective trees, then there are no steps that lead to the correct parse. At best, there is an optimal step, by some definition of optimality. An algorithm to compute the optimal step, for a given co"
Q19-1018,P99-1059,0,0.646094,"i) = 1, if (B, C ) → ai 0, otherwise  1, if (B, C ) → ai L 0, otherwise W` (B, C, i, j ) = k Wr (B, i, k ) ⊗ L W` (C, k + 1, j ) ⊗ w(j, i) Wr (B, C, i, j ) = k Wr (B, i, k ) ⊗ L W` (C, k + 1, j ) ⊗ w(i, j ) W` (C 0 , i, j ) = A→(D,B ), (C 0 , )→A (C, ), k W` (D, i, k ) ⊗ W` (B, C, k, j ) L Wr (B 0 , i, j ) = ( ,B 0 )→( ,B ) A, A→(C,D), k Wr (B, C, i, k ) ⊗ Wr (D, k, j ) L W = S →(B,C ) W` (B, 0, 0) ⊗ Wr (C, 0, n) Wr (C, i, i) = Table 2: Weighted parsing, for an arbitrary semiring, with 0 ≤ i < j ≤ n. Note that the additional requirements make the grammar explicitly ‘‘split’’ in the sense of Eisner and Satta (1999), Eisner (2000), and Johnson (2007). That is, the two processes of attaching left and right children, respectively, are independent, with rules (B, C ) → a creating ‘‘initial states’’ B and C , respectively, for these two processes. Rules of the form A → (B, C ) then combine the end results of these two processes, possibly placing constraints on allowable combinations of B and C . To bring out the relation between our subclass of CFGs and bilexical grammars, one could explicitly write (B, C )(a) → a, A(a) → (B, C )(a), (B 0 , )(b) → A(a) (B, )(b), and ( , C 0 )(c) → ( , C )(c) A(a). Purely sym"
Q19-1018,N18-2062,0,0.255766,"Missing"
Q19-1018,N18-2109,0,0.0499132,"Missing"
Q19-1018,D14-1099,0,0.72472,"Missing"
Q19-1018,P10-1110,0,0.0396541,"the optimal step is regarded to be difficult. The best known algorithm is cubic and is only applicable if the training corpus is projective (Goldberg et al., 2014). We present a new cubic-time algorithm that is also applicable to non-projective training corpora. Moreover, its architecture is modular, expressible as a generic tabular algorithm for dependency parsing plus a context-free grammar that expresses the allowable transitions of the parsing strategy. This differs from approaches that require specialized tabular algorithms for different kinds of parsing (G´omez-Rodr´ıguez et al., 2008; Huang and Sagae, 2010; Kuhlmann et al., 2011). The generic tabular algorithm is interesting in its own right, and can be used to determine the optimal projectivization of a non-projective tree. This is not to be confused with pseudo-projectivization (Kahane et al., 1998; Nivre and Nilsson, 2005), which generally has a different architecture and is used for a different purpose, namely, to allow a projective parser to produce non-projective structures, by encoding non-projectivity into projective structures before training, and then reconstructing potential non-projectivity after parsing. A presentational difference"
Q19-1018,P07-1022,0,0.75998,"f (B, C ) → ai L 0, otherwise W` (B, C, i, j ) = k Wr (B, i, k ) ⊗ L W` (C, k + 1, j ) ⊗ w(j, i) Wr (B, C, i, j ) = k Wr (B, i, k ) ⊗ L W` (C, k + 1, j ) ⊗ w(i, j ) W` (C 0 , i, j ) = A→(D,B ), (C 0 , )→A (C, ), k W` (D, i, k ) ⊗ W` (B, C, k, j ) L Wr (B 0 , i, j ) = ( ,B 0 )→( ,B ) A, A→(C,D), k Wr (B, C, i, k ) ⊗ Wr (D, k, j ) L W = S →(B,C ) W` (B, 0, 0) ⊗ Wr (C, 0, n) Wr (C, i, i) = Table 2: Weighted parsing, for an arbitrary semiring, with 0 ≤ i < j ≤ n. Note that the additional requirements make the grammar explicitly ‘‘split’’ in the sense of Eisner and Satta (1999), Eisner (2000), and Johnson (2007). That is, the two processes of attaching left and right children, respectively, are independent, with rules (B, C ) → a creating ‘‘initial states’’ B and C , respectively, for these two processes. Rules of the form A → (B, C ) then combine the end results of these two processes, possibly placing constraints on allowable combinations of B and C . To bring out the relation between our subclass of CFGs and bilexical grammars, one could explicitly write (B, C )(a) → a, A(a) → (B, C )(a), (B 0 , )(b) → A(a) (B, )(b), and ( , C 0 )(c) → ( , C )(c) A(a). Purely symbolic parsing is extended to weight"
Q19-1018,P98-1106,0,0.404978,"ing corpora. Moreover, its architecture is modular, expressible as a generic tabular algorithm for dependency parsing plus a context-free grammar that expresses the allowable transitions of the parsing strategy. This differs from approaches that require specialized tabular algorithms for different kinds of parsing (G´omez-Rodr´ıguez et al., 2008; Huang and Sagae, 2010; Kuhlmann et al., 2011). The generic tabular algorithm is interesting in its own right, and can be used to determine the optimal projectivization of a non-projective tree. This is not to be confused with pseudo-projectivization (Kahane et al., 1998; Nivre and Nilsson, 2005), which generally has a different architecture and is used for a different purpose, namely, to allow a projective parser to produce non-projective structures, by encoding non-projectivity into projective structures before training, and then reconstructing potential non-projectivity after parsing. A presentational difference with earlier work is that we do not define optimality in terms of ‘‘loss’’ or ‘‘cost’’ functions but directly in terms of attainable accuracy. This perspective is shared by Straka et al. (2015), who also relate accuracies of competing steps, albeit"
Q19-1018,C12-1059,0,0.508723,"inistic parser may rely on a classifier that predicts the next step, given features extracted from the present configuration (Yamada and Matsumoto, 2003; Nivre et al., 2004). It was found that accuracy improves if the classifier is trained not just on configurations that correspond to the ground-truth, or ‘‘gold’’, tree, but also on configurations that a parser would typically reach when a classifier strays from the optimal predictions. This is known as a dynamic oracle.1 The effective calculation of the optimal step for some kinds of parsing relies on ‘arc-decomposibility’, as in the case of Goldberg and Nivre (2012, 2013). This generally requires a projective training corpus; an attempt to extend this to non-projective training corpora had to resort to an approximation (Aufrant et al., 2018). It is known how to calculate the optimal step for a number of non1 A term we avoid here, as dynamic oracles are neither oracles nor dynamic, especially in our formulation, which allows gold trees to be non-projective. Following, for example, Kay (2000), an oracle informs a parser whether a step may lead to the correct parse. If the gold tree is non-projective and the parsing strategy only allows projective trees, t"
Q19-1018,Q13-1033,0,0.323061,"Missing"
Q19-1018,2000.iwpt-1.3,0,0.157946,"known as a dynamic oracle.1 The effective calculation of the optimal step for some kinds of parsing relies on ‘arc-decomposibility’, as in the case of Goldberg and Nivre (2012, 2013). This generally requires a projective training corpus; an attempt to extend this to non-projective training corpora had to resort to an approximation (Aufrant et al., 2018). It is known how to calculate the optimal step for a number of non1 A term we avoid here, as dynamic oracles are neither oracles nor dynamic, especially in our formulation, which allows gold trees to be non-projective. Following, for example, Kay (2000), an oracle informs a parser whether a step may lead to the correct parse. If the gold tree is non-projective and the parsing strategy only allows projective trees, then there are no steps that lead to the correct parse. At best, there is an optimal step, by some definition of optimality. An algorithm to compute the optimal step, for a given configuration, would typically not change over time, and therefore is not dynamic in any generally accepted sense of the word. 283 Transactions of the Association for Computational Linguistics, vol. 7, pp. 283–296, 2019. Action Editor: Francois Yvon. Submi"
Q19-1018,Q14-1010,0,0.877335,"Andrews, UK markjan.nederhof@googlemail.com Abstract projective parsing algorithms, however (G´omezRodr´ıguez et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Fern´andez-Gonz´alez G´omez-Rodr´ıguez, 2018a); see also de Lhoneux et al. (2017). Ordinary shift-reduce dependency parsing is known at least since Fraser (1989); see also Nasr (1995). Nivre (2008) calls it ‘‘arc-standard parsing.’’ For shift-reduce dependency parsing, calculation of the optimal step is regarded to be difficult. The best known algorithm is cubic and is only applicable if the training corpus is projective (Goldberg et al., 2014). We present a new cubic-time algorithm that is also applicable to non-projective training corpora. Moreover, its architecture is modular, expressible as a generic tabular algorithm for dependency parsing plus a context-free grammar that expresses the allowable transitions of the parsing strategy. This differs from approaches that require specialized tabular algorithms for different kinds of parsing (G´omez-Rodr´ıguez et al., 2008; Huang and Sagae, 2010; Kuhlmann et al., 2011). The generic tabular algorithm is interesting in its own right, and can be used to determine the optimal projectivizat"
Q19-1018,P11-1068,0,0.463698,"Missing"
Q19-1018,P08-1110,0,0.0874001,"Missing"
Q19-1018,W17-6314,0,0.236198,"Missing"
Q19-1018,P15-2042,0,0.370616,"Missing"
Q19-1018,J08-4003,0,0.0526899,"Missing"
Q19-1018,W04-2407,0,0.0985701,"ms of best attainable accuracies. We present a new cubic-time algorithm to calculate the optimal next step in shift-reduce dependency parsing, relative to ground truth, commonly referred to as dynamic oracle. Unlike existing algorithms, it is applicable if the training corpus contains non-projective structures. We then show that for a projective training corpus, the time complexity can be improved from cubic to linear. 1 Introduction A deterministic parser may rely on a classifier that predicts the next step, given features extracted from the present configuration (Yamada and Matsumoto, 2003; Nivre et al., 2004). It was found that accuracy improves if the classifier is trained not just on configurations that correspond to the ground-truth, or ‘‘gold’’, tree, but also on configurations that a parser would typically reach when a classifier strays from the optimal predictions. This is known as a dynamic oracle.1 The effective calculation of the optimal step for some kinds of parsing relies on ‘arc-decomposibility’, as in the case of Goldberg and Nivre (2012, 2013). This generally requires a projective training corpus; an attempt to extend this to non-projective training corpora had to resort to an appro"
Q19-1018,P05-1013,0,0.0893394,", its architecture is modular, expressible as a generic tabular algorithm for dependency parsing plus a context-free grammar that expresses the allowable transitions of the parsing strategy. This differs from approaches that require specialized tabular algorithms for different kinds of parsing (G´omez-Rodr´ıguez et al., 2008; Huang and Sagae, 2010; Kuhlmann et al., 2011). The generic tabular algorithm is interesting in its own right, and can be used to determine the optimal projectivization of a non-projective tree. This is not to be confused with pseudo-projectivization (Kahane et al., 1998; Nivre and Nilsson, 2005), which generally has a different architecture and is used for a different purpose, namely, to allow a projective parser to produce non-projective structures, by encoding non-projectivity into projective structures before training, and then reconstructing potential non-projectivity after parsing. A presentational difference with earlier work is that we do not define optimality in terms of ‘‘loss’’ or ‘‘cost’’ functions but directly in terms of attainable accuracy. This perspective is shared by Straka et al. (2015), who also relate accuracies of competing steps, albeit by means of actual parser"
Q19-1018,W03-3023,0,0.238364,"parser output and not in terms of best attainable accuracies. We present a new cubic-time algorithm to calculate the optimal next step in shift-reduce dependency parsing, relative to ground truth, commonly referred to as dynamic oracle. Unlike existing algorithms, it is applicable if the training corpus contains non-projective structures. We then show that for a projective training corpus, the time complexity can be improved from cubic to linear. 1 Introduction A deterministic parser may rely on a classifier that predicts the next step, given features extracted from the present configuration (Yamada and Matsumoto, 2003; Nivre et al., 2004). It was found that accuracy improves if the classifier is trained not just on configurations that correspond to the ground-truth, or ‘‘gold’’, tree, but also on configurations that a parser would typically reach when a classifier strays from the optimal predictions. This is known as a dynamic oracle.1 The effective calculation of the optimal step for some kinds of parsing relies on ‘arc-decomposibility’, as in the case of Goldberg and Nivre (2012, 2013). This generally requires a projective training corpus; an attempt to extend this to non-projective training corpora had"
Q19-1018,P17-2018,0,0.259268,"er show that if the training corpus is projective, then the time complexity can be reduced to linear. To achieve this, we develop a new approach of excluding computations whose accuracies are guaranteed not to exceed the accuracies of the remaining computations. The main theoretical conclusion is that arc-decomposibility is not a necessary requirement for efficient calculation of the optimal step. Despite advances in unrestricted non-projective parsing, as, for example, Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2018b), many state-ofthe-art dependency parsers are projective, as, for example, Qi and Manning (2017). One main practical contribution of the current paper is that it introduces new ways to train projective parsers using non-projective trees, thereby enlarging the portion of trees from a corpus that is available for training. This can be done either after applying optimal projectivization, or by computing optimal steps directly for non-projective trees. This can be expected to lead to more accurate parsers, especially if a training corpus is small and a large proportion of it is non-projective. 2 Table 1: Shift-reduce dependency parsing. of the form (0, ε, T ), where ε denotes the empty strin"
W01-1404,J00-1004,0,0.0168575,"tively learned from examples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word  The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is th"
W01-1404,N01-1018,0,0.0751241,"transitions are only applied if all others fail. 3. The driver of the automaton is changed so that it restarts at the initial state when it gets stuck at some input word, and when necessary, that input word is deleted. The output string with the lowest weight obtained so far (preferably attached to final states, or to other states with outgoing transitions labelled by input symbols) is then concatenated with the output string resulting from processing subsequent input. 6 Experiments We have investigated a corpus of English/Japanese sentence pairs, related by hierarchical alignment (see also (Bangalore and Riccardi, 2001)). We have taken the first 500, 1000, 1500, . . . aligned sentence pairs from this corpus to act as training corpora of varying sizes; we have taken 300 other sentence pairs to act as test corpus. We have constructed a bilexical transduction grammar from each training corpus, in the form of a context-free grammar, and this grammar was approximated by a finite automaton. The input sentences from the test corpus were then processed by context-free and finite-state machinery (in the sequel referred to by cfg and fa, respectively). We have also carried out experiments with robust finite-state proc"
W01-1404,J90-2002,0,0.212259,"Missing"
W01-1404,P99-1059,0,0.0293423,"the selection of appropriate lexical items. Furthermore, for limited domains, automatic learning of weighted context-free transductions from examples seems to be reasonably successful. However, practical algorithms for computing the most likely context-free derivation have a cubic time complexity, in terms of the length of the input string, or in the case of a graph output by a speech recognizer, in terms of the number of nodes in the graph. For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). This may pose problems, especially for real-time speech systems. Therefore, we have investigated approximation of weighted context-free transduction by means of weighted rational transduction. The finite-state machinery for implementing the latter kind of transduction in general allows faster processing. We can also more easily obtain robustness. We hope the approximating model is able to preserve some of the accuracy of the context-free model. In the next section, we discuss preliminary definitions, adapted from existing literature, making no more than small changes in presentation. In Sect"
W01-1404,P94-1017,1,0.804488,"ure took more than 24 hours for both fa(2) and robust fa(2) , which suggests these methods become unrealistic for training corpus sizes considerably larger than 10,000 bitexts. 7 Conclusions For our application, context-free transduction has a relatively high accuracy, but it also has a high time consumption, and it may be difficult to obtain robustness without further increasing the time costs. These are two major obstacles for use in spoken language systems. We have tried to obtain a rational transduction that approximates a 4 It uses a trie to represent productions (similar to ELR parsing (Nederhof, 1994)), postponing generation of output for a production until all nonterminals and all input symbols from the right-hand side have been found. 1 cfg2 cfg fa2 fa bigram robust_fa2 robust_fa 0.9 word accuracy 0.8 ° 0.7 0.6 0.5 0.4 0.3 0.2 0 1000 2000 3000 4000 5000 training corpus size 6000 7000 8000 Figure 1: Average word accuracy for transduced sentences. 1 fa fa2 cfg cfg2 accepted 0.8 0.6 ± 0.4 0.2 0 0 1000 2000 3000 4000 5000 training corpus size 6000 7000 8000 Figure 2: Fraction of the sentences that were transduced. context-free transduction, preserving some of its accuracy. Our experiments sh"
W01-1404,J00-1003,1,0.826684,"G  G a1  j 0 | | a 1  G a In the first production, the RHS nonterminals occur in the same order as in the left half of the original production, but reorder operators have been added to indicate that, after parsing, some substrings of the output string are to be reordered. Our reorder operators are similar to the two op erators and  from (Vilar and others, 1999), but the former are more powerful, since the latter allow only single words to be moved instead of whole phrases. 4 Finite-state approximation There are several methods to approximate context-free grammars by regular languages (Nederhof, 2000). We will consider here only the so called RTN method, which is applied in a simplified form.3 3 As opposed to (Nederhof, 2000), we assume here that all nonterminals are mutually recursive, and the grammar contains self-embedding. We have observed that typical grammars that we obtain in the context of this article indeed have the property that almost all nonterminals belong to the same mutually recursive set. A finite automaton is constructed as follows. from the grammar we inFor each nonterminal j R  R  troduce two states and a . For each produc5/ / / 9 P tion j¡k£¢ ¢ we introduce ¤ stat"
W01-1404,C00-2123,0,0.0229765,"ime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word  The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence (DFKI). order between two languages, and the selection of appropriate lexical items. Furthermore, for limited domains, automatic learning of weighted co"
W01-1404,C00-2131,0,0.0157085,"amples. This paper investigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word  The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Cente"
W01-1404,C00-2135,0,0.0186431,"stigates the approximation of such transduction by means of weighted rational transduction. The advantage is increased processing speed, which benefits realtime applications involving spoken language. 1 Introduction Several studies have investigated automatic or partly automatic learning of transductions for machine translation. Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000). In this paper we will investigate both contextfree and finite-state models. The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word  The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence ("
W09-3802,P97-1003,0,0.124552,"composition is another (W)SLIG. It should be noted that a (W)RTS (or (W)LIG) can be seen as a (W)SRTG (or (W)SLIG, respectively) that represents the identity relation on its tree language. 6.2 Binarization In the discussion of complexity in Section 4.1, we assumed that rules are binary, that is, that they have at most two states in each right-hand side. However, whereas any context-free grammar can be transformed into a binary form (e.g. Chomsky normal form), the grammars as we have defined them cannot be. We will show that this is to a large extent a consequence of our definitions, which 22 (Collins, 1997). Also the literature on unranked tree automata is very relevant; see for example (Schwentick, 2007). Binarization for LIGs was considered before by (Vijay-Shanker and Weir, 1993a). Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, chapter 9, pages 116–150. Addison-Wesley, Reading, Massachusetts. 6.3 J. Berstel. 1979. Transductions and Context-Free Languages. B.G. Teubner, Stuttgart. Beyond TAGs In the light of results by (Kepser and M¨onni"
W09-3802,N04-1014,0,0.0632084,"on 2 and Section 3 discusses a number of analyses of these formalisms that will be used in later sections. Section 4 starts by explaining how parsing of a string can be seen as the construction of a grammar that generates the intersection of two languages, and then moves on to a type of parsing involving intersection of tree languages in place of string languages. In order to illustrate the implications of the theory, we consider how it can be used to solve a practical problem, in Section 5. A number of possible extensions are outlined in Section 6. 2 tree grammars, the reader is referred to (Graehl and Knight, 2004). Weighted tree adjoining grammars are a straightforward generalization of probabilistic (or stochastic) tree adjoining grammars, as introduced by (Resnik, 1992) and (Schabes, 1992). For both regular tree grammars (RTGs) and tree adjoining grammars (TAGs), we will write a labeled and ordered tree as A(α). where A is the label of the root node, and α is a sequence of expressions of the same form that each represent an immediate subtree. In our presentation, labels do not have explicit ranks, that is, the number of children of a node is not determined by its label. This allows an interesting gen"
W09-3802,W03-3016,1,0.729242,"f all such rules equals one. We say a weighted regular tree grammar, or weighted linear indexed grammar, respectively, is consistent if the sum of weights of all (left-most) derivations is one. This is equivalent to saying that the sum of weights of all trees is one, and to saying that the sum of weights of all strings is one. For each consistent WRTG (WLIG, respectively), there is an equivalent proper and consistent PRTG (PLIG, respectively). The proof lies in normalization. For WRTGs this is a trivial extension of normalization of weighted context-free grammars, as described for example by (Nederhof and Satta, 2003). For WLIGs (and weighted TAGs), the problem of normalization also becomes very similar once we consider that the set of derivation trees of tree adjoining grammars can be described with context-free grammars, and that this carries over to weighted derivation trees. See also (Sarkar, 1998). WLIGs seemingly incur an extra complication, if a state may occur in combination with an index on top of the associated stack such that no rules are applicable. However, for LIGs that encode TAGs, 1. a bottom-up phase to identify the grammar symbols that generate substrings, which may include the start symb"
W09-3802,J05-2002,1,0.922166,"ternatively, one may do both, with a PRTG used in a first phase to heuristically reduce the search space. This section outlines how a suitable PRTG G2 can be extracted out of a PLIG G1 , assuming the underlying RTG G20 without weights is already given. The tree language generated by G20 may be an approximation of that generated by G1 . The objective is to make G2 as close as possible to G1 in terms of probability distributions over trees. We assume that G20 is unambiguous, that is, for each tree it generates, there is at most one derivation. The procedure is a variant of the one described by (Nederhof, 2005). The idea is that derivations in G1 are mapped to those in G20 , via the trees in the intersection of the two tree languages. The probability distribution of states and rules in G2 is estimated based on the expected frequencies of states and rules from G20 in the intersection. Concretely, we turn the RTG G20 into a PRTG G200 that is obtained simply be assigning weight one to all rules. We then compute the intersection grammar G as in Section 4.2. Subsequently, the inside and outside values are computed for G, X s,q Y in((sk , qk )) + k out((s0 , q0 ), (s, q)) · in((sj , qj ), (s, q)) · Y in(("
W09-3802,W98-0134,0,0.0242401,"has time complexity O(|S |+ |R1 |· |S |+ |R2 | + |R3 |· |R4 |· |S|), where |S |= O(n) is the number of states of G, and the numbers of rules are |R1 |= O(n), |R2 |= |R3 |= |R4 |= O(n). Note that |R1 |= O(n) because we have assumed that G2 allows only one derivation of one tree t, Intersection of tree languages We now shift our attention from strings to trees, and consider the intersection of the tree language 20 hence q0 uniquely determines q1 , . . . , qm . Overall, we obtain O(n3 ) steps, which concurs with a known result about the complexity of TAG parsing of trees, as opposed to strings (Poller and Becker, 1998). Another special case is if WLIG G1 simplifies to a WRTG (i.e. the stacks of indices remain always empty), which means we compute the intersection of two weighted regular tree grammars G1 and G2 . For recognition, or in other words to decide non-emptiness of the intersection, we can still use Figure 1, although now only inference rules (b) and (d) are applicable (with a small refinement to the algorithm we can block spurious application of (a) where no rules exist that pop indices.) The complexity is determined by (d), which requires O(|G1 |· |G2 |) steps. 5 as explained in Section 3. The exp"
W09-3802,C92-2065,0,0.332978,"n as the construction of a grammar that generates the intersection of two languages, and then moves on to a type of parsing involving intersection of tree languages in place of string languages. In order to illustrate the implications of the theory, we consider how it can be used to solve a practical problem, in Section 5. A number of possible extensions are outlined in Section 6. 2 tree grammars, the reader is referred to (Graehl and Knight, 2004). Weighted tree adjoining grammars are a straightforward generalization of probabilistic (or stochastic) tree adjoining grammars, as introduced by (Resnik, 1992) and (Schabes, 1992). For both regular tree grammars (RTGs) and tree adjoining grammars (TAGs), we will write a labeled and ordered tree as A(α). where A is the label of the root node, and α is a sequence of expressions of the same form that each represent an immediate subtree. In our presentation, labels do not have explicit ranks, that is, the number of children of a node is not determined by its label. This allows an interesting generalization, to be discussed in Section 6.2. Where we are interested in the string language generated by a tree-generating grammar, we may distinguish between tw"
W09-3802,P98-2190,0,0.0299729,"hts of all strings is one. For each consistent WRTG (WLIG, respectively), there is an equivalent proper and consistent PRTG (PLIG, respectively). The proof lies in normalization. For WRTGs this is a trivial extension of normalization of weighted context-free grammars, as described for example by (Nederhof and Satta, 2003). For WLIGs (and weighted TAGs), the problem of normalization also becomes very similar once we consider that the set of derivation trees of tree adjoining grammars can be described with context-free grammars, and that this carries over to weighted derivation trees. See also (Sarkar, 1998). WLIGs seemingly incur an extra complication, if a state may occur in combination with an index on top of the associated stack such that no rules are applicable. However, for LIGs that encode TAGs, 1. a bottom-up phase to identify the grammar symbols that generate substrings, which may include the start symbol if the generated language is non-empty; and 2. a top-down phase to identify the grammar symbols that are reachable from the start symbol. The intersection of the generating symbols and the reachable symbols gives the set of useful symbols. One can then identify useless rules as those th"
W09-3802,C92-2066,0,0.175987,"on of a grammar that generates the intersection of two languages, and then moves on to a type of parsing involving intersection of tree languages in place of string languages. In order to illustrate the implications of the theory, we consider how it can be used to solve a practical problem, in Section 5. A number of possible extensions are outlined in Section 6. 2 tree grammars, the reader is referred to (Graehl and Knight, 2004). Weighted tree adjoining grammars are a straightforward generalization of probabilistic (or stochastic) tree adjoining grammars, as introduced by (Resnik, 1992) and (Schabes, 1992). For both regular tree grammars (RTGs) and tree adjoining grammars (TAGs), we will write a labeled and ordered tree as A(α). where A is the label of the root node, and α is a sequence of expressions of the same form that each represent an immediate subtree. In our presentation, labels do not have explicit ranks, that is, the number of children of a node is not determined by its label. This allows an interesting generalization, to be discussed in Section 6.2. Where we are interested in the string language generated by a tree-generating grammar, we may distinguish between two kinds of labels, t"
W09-3802,J93-4002,0,0.718578,"f occurrences of state s to be expressed as: E(s, s0 ) = in(s, s0 ) · out(s, s0 ) We will return to this issue in Section 5. 4 Weighted intersection Before we discuss intersection on the level of trees, we first show how a well-established type of intersection on the level of strings, with weighted context-free grammars and weighted finite automata (WFAs), can be trivially extended to replace CFGs with RTGs or LIGs. The intersection paradigm is originally due to (Bar-Hillel et al., 1964). Extension to tree adjoining grammars and linear indexed grammars was proposed before by (Lang, 1994) and (Vijay-Shanker and Weir, 1993b). 4.1 Intersection of string languages Let us assume a WLIG G with terminal and nonterminal labels. Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: a q 7→ q 0 hwi, E(s) = in(s) · out(s) where q and q 0 are states, a is a terminal symbol, and w is a weight. To simplify the presentation, Similarly, the expected number of subderivations 18 out(s0 ) = δ(s0 = s` ) + X w · out(s0 , s) · in(sj , s) · s0 [◦◦] → A(s1 [ ] · · · sj [◦◦] · · · sm [ ]) hwi k ∈ {1, . . . , sj−1 , sj+1 , . . . , sm"
W09-3802,E93-1045,0,0.884667,"f occurrences of state s to be expressed as: E(s, s0 ) = in(s, s0 ) · out(s, s0 ) We will return to this issue in Section 5. 4 Weighted intersection Before we discuss intersection on the level of trees, we first show how a well-established type of intersection on the level of strings, with weighted context-free grammars and weighted finite automata (WFAs), can be trivially extended to replace CFGs with RTGs or LIGs. The intersection paradigm is originally due to (Bar-Hillel et al., 1964). Extension to tree adjoining grammars and linear indexed grammars was proposed before by (Lang, 1994) and (Vijay-Shanker and Weir, 1993b). 4.1 Intersection of string languages Let us assume a WLIG G with terminal and nonterminal labels. Furthermore, we assume a weighted finite automaton A, with an input alphabet equal to the set of terminal labels of G. The transitions of A are of the form: a q 7→ q 0 hwi, E(s) = in(s) · out(s) where q and q 0 are states, a is a terminal symbol, and w is a weight. To simplify the presentation, Similarly, the expected number of subderivations 18 out(s0 ) = δ(s0 = s` ) + X w · out(s0 , s) · in(sj , s) · s0 [◦◦] → A(s1 [ ] · · · sj [◦◦] · · · sm [ ]) hwi k ∈ {1, . . . , sj−1 , sj+1 , . . . , sm"
W09-3802,J00-4006,0,\N,Missing
W09-3802,J08-3004,0,\N,Missing
W09-3802,C98-2185,0,\N,Missing
W10-2505,W99-0630,0,0.0917088,"Missing"
W10-2505,W06-1001,0,0.0311541,"ation are discussed in Section 5. A brief overview of an implementation is given in Section 6. We investigate the problem of structurally changing lexica, while preserving the information. We present a type of lexicon transformation that is complete on an interesting class of lexica. Our work is motivated by the problem of merging one or more lexica into one lexicon. Lexica, lexicon schemas, and lexicon transformations are all seen as particular kinds of trees. 1 Introduction A standard for lexical resources, called Lexical Markup Framework (LMF), has been developed under the auspices of ISO (Francopoulo et al., 2006). At its core is the understanding that most information represented in a lexicon is hierarchical in nature, so that it can be represented as a tree. Although LMF also includes relations between nodes orthogonal to the tree structure, we will in this paper simplify the presentation by treating only purely tree-shaped lexica. There is a high demand for tools supporting the merger of a number of lexica. A few examples of papers that express this demand are Chan KaLeung and Wu (1999), Jing et al. (2000), Monachini et al. (2004) and Ruimy (2006). A typical scenario is the following. The ultimate g"
W10-2505,W00-1428,0,0.0417664,"s, called Lexical Markup Framework (LMF), has been developed under the auspices of ISO (Francopoulo et al., 2006). At its core is the understanding that most information represented in a lexicon is hierarchical in nature, so that it can be represented as a tree. Although LMF also includes relations between nodes orthogonal to the tree structure, we will in this paper simplify the presentation by treating only purely tree-shaped lexica. There is a high demand for tools supporting the merger of a number of lexica. A few examples of papers that express this demand are Chan KaLeung and Wu (1999), Jing et al. (2000), Monachini et al. (2004) and Ruimy (2006). A typical scenario is the following. The ultimate goal of a project is the creation of a single lexicon for a given language. In order to obtain the necessary data, several field linguists independently gather lexical resources. Despite efforts to come to agreements before the start of the field work, there will generally be overlap in the scope of the respective resources and there are frequently inconsistencies both in the lexical information itself and in the form in which information is represented. 2 Lexica and their structures In this section,"
W10-2505,kemps-snijders-etal-2006-lexus,1,0.748263,"spect the dependencies between sets of attributes. Within these bounds, an attribute a may be located in a restrictor in τ anywhere between the root node and the leaf node labelled a. 43 6 Implementation a source lexicon in terms of a lexicon base. A full description of the implementation would go beyond the context of this paper. The mathematical framework in this paper models a restricted case of merging and restructuring a number of input lexica. An implementation was developed as a potential new module of LEXUS, which is a web-based tool for manipulating lexical resources, as described by Kemps-Snijders et al. (2006). The restriction considered here involves only one input lexicon, and we have abstracted away from a large number of features present in the actual implementation, among which are provisions to interact with the user, to access external linguistic functions (e.g. morphological operations), and to rename attributes. These simplifications have allowed us to isolate one essential and difficult problem of lexicon merging, namely how to carry over the underlying information from one lexicon to another, in spite of possible significant differences in structure. The framework considered here assumes"
W10-2505,monachini-etal-2004-unifying,0,0.0169841,"rkup Framework (LMF), has been developed under the auspices of ISO (Francopoulo et al., 2006). At its core is the understanding that most information represented in a lexicon is hierarchical in nature, so that it can be represented as a tree. Although LMF also includes relations between nodes orthogonal to the tree structure, we will in this paper simplify the presentation by treating only purely tree-shaped lexica. There is a high demand for tools supporting the merger of a number of lexica. A few examples of papers that express this demand are Chan KaLeung and Wu (1999), Jing et al. (2000), Monachini et al. (2004) and Ruimy (2006). A typical scenario is the following. The ultimate goal of a project is the creation of a single lexicon for a given language. In order to obtain the necessary data, several field linguists independently gather lexical resources. Despite efforts to come to agreements before the start of the field work, there will generally be overlap in the scope of the respective resources and there are frequently inconsistencies both in the lexical information itself and in the form in which information is represented. 2 Lexica and their structures In this section, we formalize the notions"
W10-2505,ruimy-2006-merging,0,0.0251462,"en developed under the auspices of ISO (Francopoulo et al., 2006). At its core is the understanding that most information represented in a lexicon is hierarchical in nature, so that it can be represented as a tree. Although LMF also includes relations between nodes orthogonal to the tree structure, we will in this paper simplify the presentation by treating only purely tree-shaped lexica. There is a high demand for tools supporting the merger of a number of lexica. A few examples of papers that express this demand are Chan KaLeung and Wu (1999), Jing et al. (2000), Monachini et al. (2004) and Ruimy (2006). A typical scenario is the following. The ultimate goal of a project is the creation of a single lexicon for a given language. In order to obtain the necessary data, several field linguists independently gather lexical resources. Despite efforts to come to agreements before the start of the field work, there will generally be overlap in the scope of the respective resources and there are frequently inconsistencies both in the lexical information itself and in the form in which information is represented. 2 Lexica and their structures In this section, we formalize the notions of lexica, lexico"
W11-2903,J96-1002,0,0.00936378,"(2008) use a variant of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for a given translation pair be representable by a WRTG. In most cases, this can be computed by restricting the grammar at hand to the given translation pair, that is, by applying input and output product. Note that the pair can contain strings or trees or even some combination thereof. Feature Weight Estimation. In the systems mentioned at the beginning of this section, a probability distribution of the form p(e, d |f ) is chosen from a log-linear model (Berger et al., 1996; Och and Ney, 2002), where e, d, and f are an English Restricting the input or the output of a grammar-induced translation to a given set of trees plays an important role in statistical machine translation. The problem for practical systems is to find a compact (and in particular, finite) representation of said restriction. For the class of synchronous treeadjoining grammars, partial solutions to this problem have been described, some being restricted to the unweighted case, some to the monolingual case. We introduce a formulation of this class of grammars which is effectively closed under in"
W11-2903,W10-2506,1,0.85613,"Missing"
W11-2903,N04-1035,0,0.0735921,"Missing"
W11-2903,J07-2003,0,0.748366,"a compact way, e.g., using a weighted regular tree grammar (WRTG) (Alexandrakis and Bozapalidis, 1987). The process of obtaining this representation is called tree parsing or string parsing, depending on the type of restriction. We illustrate the importance of input and output product by considering its role in three essential tasks of SMT. Grammar Estimation. After the rules of the grammar have been obtained from a sample of translation pairs (rule extraction), the probabilities of the rules need to be determined. To this end, two approaches have been employed. Some systems such as those by Chiang (2007) and DeNeefe and Knight (2009) hypothesize a canonical derivation for each translation pair, and apply relative-frequency estimation to the resulting derivations to obtain rule probabilities. While this procedure is computationally inexpensive, it only maximizes the likelihood of the training data under the assumption that the canonical derivations are the true ones. Other systems such as those by Eisner (2003), Nesson et al. (2006), and Graehl et al. (2008) use a variant of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for"
W11-2903,J99-4004,0,0.301897,"orem 4.12 of (Engelfriet and Vogler, 1985) to the composition of a macro tree transducer and a top-down tree transducer (also cf. (Rounds, 1970)); in fact, our direct construction is very similar to the latter one. Section 5 contains Algorithm 1, which computes our construction (modulo reduction). It is inspired by a variant of Earley’s algorithm (Earley, 1970; Graham et al., 1980). In this way we avoid computation of a certain portion of useless rules, and we ensure that the complexity is linear in the size of the input WSTAG. The algorithm is presented in the framework of deductive parsing (Goodman, 1999; Nederhof, 2003). In Sections 6 and 7, we discuss the correctness of our algorithm and its complexity, respectively. We denote the set of all unranked, ordered, labeled trees over some alphabet Σ by UΣ . We represent trees as well-formed expressions, e.g., S(Adv(yesterday), ∗); a graphical representation of this tree occurs at the very bottom of Fig. 1(a). Sometimes we assign a rank (or: arity) k ∈ N to a symbol σ ∈ Σ and then require that every σ-labeled position of a tree has exactly k successors. We denote the set of all positions of a tree t ∈ UΣ by pos(t). A position is represented as a"
W11-2903,J08-3004,0,0.146072,"extraction), the probabilities of the rules need to be determined. To this end, two approaches have been employed. Some systems such as those by Chiang (2007) and DeNeefe and Knight (2009) hypothesize a canonical derivation for each translation pair, and apply relative-frequency estimation to the resulting derivations to obtain rule probabilities. While this procedure is computationally inexpensive, it only maximizes the likelihood of the training data under the assumption that the canonical derivations are the true ones. Other systems such as those by Eisner (2003), Nesson et al. (2006), and Graehl et al. (2008) use a variant of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for a given translation pair be representable by a WRTG. In most cases, this can be computed by restricting the grammar at hand to the given translation pair, that is, by applying input and output product. Note that the pair can contain strings or trees or even some combination thereof. Feature Weight Estimation. In the systems mentioned at the beginning of this section, a probability distribution of the form p(e, d |f ) is chosen from a log-linear model (Berger"
W11-2903,D09-1076,0,0.222651,"g., using a weighted regular tree grammar (WRTG) (Alexandrakis and Bozapalidis, 1987). The process of obtaining this representation is called tree parsing or string parsing, depending on the type of restriction. We illustrate the importance of input and output product by considering its role in three essential tasks of SMT. Grammar Estimation. After the rules of the grammar have been obtained from a sample of translation pairs (rule extraction), the probabilities of the rules need to be determined. To this end, two approaches have been employed. Some systems such as those by Chiang (2007) and DeNeefe and Knight (2009) hypothesize a canonical derivation for each translation pair, and apply relative-frequency estimation to the resulting derivations to obtain rule probabilities. While this procedure is computationally inexpensive, it only maximizes the likelihood of the training data under the assumption that the canonical derivations are the true ones. Other systems such as those by Eisner (2003), Nesson et al. (2006), and Graehl et al. (2008) use a variant of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for a given translation pair be re"
W11-2903,W10-2502,1,0.843368,"Missing"
W11-2903,W05-1506,0,0.0282842,"ombines information from different sources, called features, such as the grammar or a probability distribution over English sentences. The features are represented by real-valued functions hi (e, d, f ). For said combination, each feature gets a weight λi . The feature weights are usually estimated using minimum-error-rate training (Och, 2003). For this it is necessary to compute, for a given f , the set Df of n highest ranking derivations generating f on the foreign side. Roughly speaking, this set can be computed by applying the input product with f , and then applying the n-best algorithm (Huang and Chiang, 2005; B¨uchse et al., 2010). We note that, while f is usually a string, it can in some circumstances be a phrase-structure tree, as in (Huang et al., 2006). Decoding. The actual translation, or decoding, problem amounts to finding, for a given f , eˆ = argmaxe P Q d i hi (e, d, f ) λi . Even for the simplest grammars, this problem is NP hard (Casacuberta and de la Higuera, 2000). As a result, SMT systems use approximations such as crunching or variational decoding (Li et al., 2009). Here we focus on the former, which amounts to restricting the sum in the equation to the set Df . Since this set is"
W11-2903,2006.amta-papers.8,0,0.306451,"roduce a formulation of this class of grammars which is effectively closed under input and output restrictions to regular tree languages, i.e., the restricted translations can again be represented by grammars. Moreover, we present an algorithm that constructs these grammars for input and output restriction, which is inspired by Earley’s algorithm. 1 Introduction Many recent systems for statistical machine translation (SMT) (Lopez, 2008) use some grammar at their core. Chiang (2007), e. g., uses synchronous context-free grammars (SCFG) that derive pairs of translationally equivalent sentences. Huang et al. (2006) use tree-to-string transducers (called xRLNS) that describe pairs of the form (phrasestructure tree, string). Other systems, such as (Eisner, 2003; Zhang et al., 2008; Nesson et al., 2006; DeNeefe and Knight, 2009), use variants of synchronous tree-adjoining grammars (STAGs) (Abeille et al., 1990; Joshi and Schabes, 1997) that derive pairs of dependency or phrase-structure trees. Common variants of STAGs are synchronous tree-substitution grammars (STSGs) and synchronous tree-insertion grammars (STIGs). For grammar-based systems, a variety of tasks can be described using the general concepts o"
W11-2903,N09-1026,0,0.0367699,"ailored to the input product with a regular weighted string language. For this scenario, several contributions exist, requiring additional restictions however. For instance, Nesson et al. (2006) show a CYK-like algorithm for intersecting a STIG with a pair of strings. Their algorithm requires that the trees of the grammar be binarized. As DeNeefe and Knight (2009) point out, this makes the grammar strictly less powerful. They in turn propose a construction which converts the STIG into an equivalent treeto-string transducer, and they use corresponding algorithms for parsing, such as the one by DeNero et al. (2009). However, their construction relies on the fact that tree-insertion grammars are weakly equivalent to context-free grammars. Thus, it is not applicable to the more general STAGs. Complexity Analysis In this section, we analyse the worst-case space and time complexity of step 1 of Algorithm 1. The space complexity is  ! O |G|in · |RH |· |P |C , which is determined by the number of possible items of the form [ρ, w, j, r, r1 · · · rk , θ]. The first factor, P |G|in , denotes the input size of G, defined by ρ∈R |pos(ζ(ρ))|, where ζ(ρ) is the input tree of ρ. It captures the components ρ, w, and"
W11-2903,P81-1022,0,0.748068,"WRTGs (cf. Theorem 1). We do this by means of a direct construction (cf. Sec. 4). Our construction is based on the standard technique for composing two top-down tree transducers (cf. page 195 of (Baker, 1979)). This technique has been extended in Theorem 4.12 of (Engelfriet and Vogler, 1985) to the composition of a macro tree transducer and a top-down tree transducer (also cf. (Rounds, 1970)); in fact, our direct construction is very similar to the latter one. Section 5 contains Algorithm 1, which computes our construction (modulo reduction). It is inspired by a variant of Earley’s algorithm (Earley, 1970; Graham et al., 1980). In this way we avoid computation of a certain portion of useless rules, and we ensure that the complexity is linear in the size of the input WSTAG. The algorithm is presented in the framework of deductive parsing (Goodman, 1999; Nederhof, 2003). In Sections 6 and 7, we discuss the correctness of our algorithm and its complexity, respectively. We denote the set of all unranked, ordered, labeled trees over some alphabet Σ by UΣ . We represent trees as well-formed expressions, e.g., S(Adv(yesterday), ∗); a graphical representation of this tree occurs at the very bottom of"
W11-2903,P03-2041,0,0.461839,"from a sample of translation pairs (rule extraction), the probabilities of the rules need to be determined. To this end, two approaches have been employed. Some systems such as those by Chiang (2007) and DeNeefe and Knight (2009) hypothesize a canonical derivation for each translation pair, and apply relative-frequency estimation to the resulting derivations to obtain rule probabilities. While this procedure is computationally inexpensive, it only maximizes the likelihood of the training data under the assumption that the canonical derivations are the true ones. Other systems such as those by Eisner (2003), Nesson et al. (2006), and Graehl et al. (2008) use a variant of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for a given translation pair be representable by a WRTG. In most cases, this can be computed by restricting the grammar at hand to the given translation pair, that is, by applying input and output product. Note that the pair can contain strings or trees or even some combination thereof. Feature Weight Estimation. In the systems mentioned at the beginning of this section, a probability distribution of the form p(e,"
W11-2903,P09-1067,0,0.0246999,"eaking, this set can be computed by applying the input product with f , and then applying the n-best algorithm (Huang and Chiang, 2005; B¨uchse et al., 2010). We note that, while f is usually a string, it can in some circumstances be a phrase-structure tree, as in (Huang et al., 2006). Decoding. The actual translation, or decoding, problem amounts to finding, for a given f , eˆ = argmaxe P Q d i hi (e, d, f ) λi . Even for the simplest grammars, this problem is NP hard (Casacuberta and de la Higuera, 2000). As a result, SMT systems use approximations such as crunching or variational decoding (Li et al., 2009). Here we focus on the former, which amounts to restricting the sum in the equation to the set Df . Since this set is finite, the sum is then zero for almost all e, which makes the computation of eˆ feasible. As mentioned before, the input product can be used to compute Df . As we have seen in these tasks, tree parsing is employed in recent SMT systems. Table 1 lists five relevant contributions in this area. These contributions can be classified according to a number of characteristics indicated by the column headings. One of these characteristics is the abstraction level (AL), which we catego"
W11-2903,P10-1109,0,0.660335,"led xRLNS) that describe pairs of the form (phrasestructure tree, string). Other systems, such as (Eisner, 2003; Zhang et al., 2008; Nesson et al., 2006; DeNeefe and Knight, 2009), use variants of synchronous tree-adjoining grammars (STAGs) (Abeille et al., 1990; Joshi and Schabes, 1997) that derive pairs of dependency or phrase-structure trees. Common variants of STAGs are synchronous tree-substitution grammars (STSGs) and synchronous tree-insertion grammars (STIGs). For grammar-based systems, a variety of tasks can be described using the general concepts of input product and output product (Maletti, 2010b). Roughly speaking, these products restrict the 14 Proceedings of the 12th International Conference on Parsing Technologies, pages 14–25, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. of trees. Eisner (2003) describes an algorithm for computing the set of derivations for the input and output product of an STSG with a single pair of trees. We note that the grammar classes covered so far are strictly less powerful than STAGs. This is due to the fact that STAGs additionally permit an operation called adjoining. As Nesson et al. (2006) and DeNeefe an"
W11-2903,P02-1038,0,0.0342138,"of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for a given translation pair be representable by a WRTG. In most cases, this can be computed by restricting the grammar at hand to the given translation pair, that is, by applying input and output product. Note that the pair can contain strings or trees or even some combination thereof. Feature Weight Estimation. In the systems mentioned at the beginning of this section, a probability distribution of the form p(e, d |f ) is chosen from a log-linear model (Berger et al., 1996; Och and Ney, 2002), where e, d, and f are an English Restricting the input or the output of a grammar-induced translation to a given set of trees plays an important role in statistical machine translation. The problem for practical systems is to find a compact (and in particular, finite) representation of said restriction. For the class of synchronous treeadjoining grammars, partial solutions to this problem have been described, some being restricted to the unweighted case, some to the monolingual case. We introduce a formulation of this class of grammars which is effectively closed under input and output restr"
W11-2903,W09-3801,0,0.224913,"Missing"
W11-2903,W04-3312,0,0.155108,"lisms In this paper, we propose a weighted formulation of STAGs which is closed under input and output product with WRTGs, and we present a corresponding tree-parsing algorithm. This paper is organized as follows. In Sec. 2, we introduce our formulation of STAGs, which is called weighted synchronous tree-adjoining grammar (WSTAG). The major difference with respect to the classical STAGs is twofold: (i) we use states and (ii) we encode substitution and adjoining sites as variables in the tree. The states make intersection with regular properties possible (without the need for relabeling as in (Shieber, 2004) and (Maletti, 2010a)). In addition, they permit implementing all features of conventional STAG/STIG, such as potential adjoining and left/right adjoining. The variables are used for synchronization of the input and output sides. In Sec. 3, we show that WSTAGs are closed under input and output product with tree languages generated by WRTGs (cf. Theorem 1). We do this by means of a direct construction (cf. Sec. 4). Our construction is based on the standard technique for composing two top-down tree transducers (cf. page 195 of (Baker, 1979)). This technique has been extended in Theorem 4.12 of ("
W11-2903,E06-1048,0,0.572761,"(w1), . . . , κ(w rkt (w)) . The weight of a run κ on t is the value wt(κ, t) ∈ R≥0 defined by Y p(ρ(κ, t, w)) . wt(κ, t) = w∈pos(t) The weighted tree language generated by WRTG H is the mapping L(H) : UΣ → R≥0 16 Rules of the forms (α) and (β) are called (m, l)rules; ζ and ζ ′ are called the input tree and the output tree of the rule, respectively. For fixed q and f , the sets of all rules of the form (α) and (β) are denoted by Rq and Rf , resp. Figure 1(a) shows an example of a WSTAG. In the following, let G = (Q, F, Σ, q0 , R, p) be a WSTAG. We define the semantics in terms of bimorphisms (Shieber, 2006). For this we define a WRTG HG that generates the weighted tree language of derivation trees of G, and two tree homomorphisms h1 and h2 that retrieve from a derivation tree the derived input tree and output tree, respectively. The derivation tree WRTG of G is the WRTG HG = (Q ∪ F, R, q0 , R′ , p′ ) where • we assign the rank m + l to every (m, l)-rule, • R′ is the set of all rules D(ρ) with ρ ∈ R and – if ρ is of the form (α), then D(ρ) = q → ρ(q1 , . . . , qm , f1 , . . . , fl ), – if ρ is of the form (β), then D(ρ) = f → ρ(q1 , . . . , qm , f1 , . . . , fl ), • and p′ (D(ρ)) = p(ρ). Recall t"
W11-2903,C90-3045,0,0.627206,"ivation tree of (s, t) We note that for every derivation tree d there is a unique run κd of HG on d: κd (w) is the state occurring in the left-hand side of the rule d(w), for every w ∈ pos(d). As an example, we reconsider dex , which is a derivation tree of the translation pair (s, t), given by Fig. 1(b) and Fig. 1(c). Q Then we have L(HG )(dex ) = wt(κdex , dex ) = 5i=1 p(ρi ) = 0.24. Since dex is the only derivation tree of (s, t), we have that T (G)(s, t) = 0.24. STSGs as defined in F¨ul¨op et al. (2010) are WSTAGs which only have nullary states. Also classical STAGs (Abeille et al., 1990; Shieber and Schabes, 1990) and STIGs (Nesson et al., 2005; Nesson et al., 2006; DeNeefe and Knight, 2009) can be viewed as particular WSTAGs. In particular, potential adjoining as it occurs in classical STAGs can be simulated, as the following excerpt of a WSTAG shall illustrate: We will only prove the closure under input product, because the proof for the output product is similar. For the unweighted case, the closure result follows from classical results. The unweighted case is obtained if we replace the algebra in Sec. 2 by another one: R≥0 is replaced by the set B = {true, false} and the operations + and · are repl"
W11-2903,P08-1064,0,0.0164621,"ons can again be represented by grammars. Moreover, we present an algorithm that constructs these grammars for input and output restriction, which is inspired by Earley’s algorithm. 1 Introduction Many recent systems for statistical machine translation (SMT) (Lopez, 2008) use some grammar at their core. Chiang (2007), e. g., uses synchronous context-free grammars (SCFG) that derive pairs of translationally equivalent sentences. Huang et al. (2006) use tree-to-string transducers (called xRLNS) that describe pairs of the form (phrasestructure tree, string). Other systems, such as (Eisner, 2003; Zhang et al., 2008; Nesson et al., 2006; DeNeefe and Knight, 2009), use variants of synchronous tree-adjoining grammars (STAGs) (Abeille et al., 1990; Joshi and Schabes, 1997) that derive pairs of dependency or phrase-structure trees. Common variants of STAGs are synchronous tree-substitution grammars (STSGs) and synchronous tree-insertion grammars (STIGs). For grammar-based systems, a variety of tasks can be described using the general concepts of input product and output product (Maletti, 2010b). Roughly speaking, these products restrict the 14 Proceedings of the 12th International Conference on Parsing Techn"
W11-2903,J03-1006,1,0.912846,"ngelfriet and Vogler, 1985) to the composition of a macro tree transducer and a top-down tree transducer (also cf. (Rounds, 1970)); in fact, our direct construction is very similar to the latter one. Section 5 contains Algorithm 1, which computes our construction (modulo reduction). It is inspired by a variant of Earley’s algorithm (Earley, 1970; Graham et al., 1980). In this way we avoid computation of a certain portion of useless rules, and we ensure that the complexity is linear in the size of the input WSTAG. The algorithm is presented in the framework of deductive parsing (Goodman, 1999; Nederhof, 2003). In Sections 6 and 7, we discuss the correctness of our algorithm and its complexity, respectively. We denote the set of all unranked, ordered, labeled trees over some alphabet Σ by UΣ . We represent trees as well-formed expressions, e.g., S(Adv(yesterday), ∗); a graphical representation of this tree occurs at the very bottom of Fig. 1(a). Sometimes we assign a rank (or: arity) k ∈ N to a symbol σ ∈ Σ and then require that every σ-labeled position of a tree has exactly k successors. We denote the set of all positions of a tree t ∈ UΣ by pos(t). A position is represented as a finite sequence o"
W11-2903,W09-3802,1,0.787639,"n et al. (2006) and DeNeefe and Knight (2009) point out, the adjoining operation has a well-founded linguistic motivation, and permitting it improves translation quality. There are two papers approaching the problem of tree parsing for STAGs, given in the fourth and fifth entries of the table. These papers establish closure properties, that is, their constructions yield a grammar of the same type as the original grammar. Since the resulting grammars are compact representations of the derivations of the input product or output product, respectively, these constructions constitute tree parsing. Nederhof (2009) shows that weighted linear index grammars (WLIGs) are closed under weighted intersection with tree languages generated by WRTGs. WLIGs derive phrase-structure trees, and they are equivalent to tree-adjoining grammars (TAGs). His construction can be extended to some kind of synchronous WLIG without problems. However, synchronization interacts with the height restriction present for WLIG rules in a way that makes synchronous WLIGs less powerful than STAGs. Maletti (2010a) uses an alternative representation of STAG, namely as extended tree transducers (XTT) with explicit substitution. In this fr"
W11-2903,2006.amta-papers.15,0,0.351348,"f translation pairs (rule extraction), the probabilities of the rules need to be determined. To this end, two approaches have been employed. Some systems such as those by Chiang (2007) and DeNeefe and Knight (2009) hypothesize a canonical derivation for each translation pair, and apply relative-frequency estimation to the resulting derivations to obtain rule probabilities. While this procedure is computationally inexpensive, it only maximizes the likelihood of the training data under the assumption that the canonical derivations are the true ones. Other systems such as those by Eisner (2003), Nesson et al. (2006), and Graehl et al. (2008) use a variant of the EM algorithm (Dempster et al., 1977) called Inside-Outside. This algorithm requires that the set of derivations for a given translation pair be representable by a WRTG. In most cases, this can be computed by restricting the grammar at hand to the given translation pair, that is, by applying input and output product. Note that the pair can contain strings or trees or even some combination thereof. Feature Weight Estimation. In the systems mentioned at the beginning of this section, a probability distribution of the form p(e, d |f ) is chosen from"
W11-2903,P03-1021,0,0.00944064,"grammars, such as projection. Ultimately, SMT tasks may be described in this framework, as witnessed by toolboxes that exist for WFSTs (Mohri, 2009) and XTTs (May and Knight, 2006). sentence, a derivation, and a foreign sentence, respectively. Such a distribution combines information from different sources, called features, such as the grammar or a probability distribution over English sentences. The features are represented by real-valued functions hi (e, d, f ). For said combination, each feature gets a weight λi . The feature weights are usually estimated using minimum-error-rate training (Och, 2003). For this it is necessary to compute, for a given f , the set Df of n highest ranking derivations generating f on the foreign side. Roughly speaking, this set can be computed by applying the input product with f , and then applying the n-best algorithm (Huang and Chiang, 2005; B¨uchse et al., 2010). We note that, while f is usually a string, it can in some circumstances be a phrase-structure tree, as in (Huang et al., 2006). Decoding. The actual translation, or decoding, problem amounts to finding, for a given f , eˆ = argmaxe P Q d i hi (e, d, f ) λi . Even for the simplest grammars, this pr"
W11-2903,W10-2501,1,\N,Missing
W11-2903,C90-3001,0,\N,Missing
W11-2903,W90-0102,0,\N,Missing
W11-2919,P99-1070,0,0.131943,"Missing"
W11-2919,E09-1055,1,0.917854,"ism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of multiple context-free grammars, such as those considered by Kato et al. (2006). Nonterminals in a PLCFRS can generate discontinuous constituents. For this reason, (P)LCFRSs have recently been used to model discontinuous phrase structure treebanks as well as non-projective dependency treebanks; see (Maier and Lichte, 2009; Kuhlmann and Satta, 2009; Kallmeyer and Maier, 2010). The main contribution of this paper is a method for computing prefix probabilities for PLCFRSs. We are not aware of any existing algorithm in the literature for this task. Our method implies existence of algorithms for the computation of prefix probabilities for probabilistic versions of forWe present a novel method for the computation of prefix probabilities for linear context-free rewriting systems. Our approach streamlines previous procedures to compute prefix probabilities for context-free grammars, synchronous context-free grammars and tree adjoining grammars"
W11-2919,P81-1022,0,0.334633,"Missing"
W11-2919,P09-1111,1,0.900299,"Missing"
W11-2919,J09-4009,0,0.0325231,"some constant, then composition can be carried out in polynomial time. The process of reducing the length of rules in a LCFRS is called factorization. It is known that not all LCFRSs can be factorized in such a way that each rule has length bounded by some constant (Rambow and Satta, 1999). However, in the context of natural language parsing, it has been observed that the vast majority of rules in real world applications can be factorized to some small length, and that excluding the worst-case rules which cannot be handled in this way does not significantly affect accuracy; see for instance (Huang et al., 2009) and (Kuhlmann and Satta, 2009) for discussion. Efficient algorithms for factorization of LCFRSs have been presented by Kuhlmann and Satta (2009), G´omez-Rodr´ıguez and Satta (2009) and Sagot and Satta (2010). We have thus reduced the problem of computing the inside probability of w under G to the problem of computing the values of the partition function for G 0 . Because LG (w) can be less than 1, it is clear that G 0 need not be consistent, even if we assume that G is. As we have discussed in Section 2, if G 0 is any PLCFRS that may not be proper or consistent, then the values LG 0 (A) for t"
W11-2919,J91-3004,0,0.43936,"on There are a number of problems related to probabilistic grammatical formalisms that involve summing an infinite number of values. For example, if P is a probability distribution over strings defined by a probabilistic grammar, and w is a string, then the prefix probability of w is defined to be: X P (wv) v In words, all possible suffixes v that may follow prefix w are considered, and the probabilities of the concatenations of v and w are summed. Prefix probabilities can be exploited to predict the next word or part of speech, for incremental processing of text or speech from left to right (Jelinek and Lafferty, 1991). They can also be used in speech processing to score partial hypotheses in beam search (Corazza et al., 1991). At first sight, it is not clear that prefix probabilities can be effectively computed, as the number of possible strings v is infinite. It was shown however by Jelinek and Lafferty (1991) that in the case of probabilistic context-free grammars, the infinite sums can be isolated from any particular w, and these sums can be computed off-line by solving linear systems of equations. For any particular w, the prefix probability can then be computed in cubic time in the length of w, on the"
W11-2919,P04-1084,1,0.921457,"linek and Lafferty (1991) that in the case of probabilistic context-free grammars, the infinite sums can be isolated from any particular w, and these sums can be computed off-line by solving linear systems of equations. For any particular w, the prefix probability can then be computed in cubic time in the length of w, on the 151 Proceedings of the 12th International Conference on Parsing Technologies, pages 151–162, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. malisms that are special cases of LCFRSs, such as the generalized multitext grammars of Melamed et al. (2004), which are used to model translation, and the already mentioned formalism proposed by Kuhlmann and Satta (2009) to model non-projective dependency structures. We follow essentially the same approach as Nederhof and Satta (2011b), and reduce the problem of computing the prefix probabilities for PLCFRSs to the well-known problem of computing inside probabilities for PLCFRSs. The reduction is obtained by the composition of a PLCFRS with a special finite-state transducer. Most importantly, this composition is independent of the specific input string for which we need to solve the prefix probabili"
W11-2919,C10-1061,0,0.0254805,"context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of multiple context-free grammars, such as those considered by Kato et al. (2006). Nonterminals in a PLCFRS can generate discontinuous constituents. For this reason, (P)LCFRSs have recently been used to model discontinuous phrase structure treebanks as well as non-projective dependency treebanks; see (Maier and Lichte, 2009; Kuhlmann and Satta, 2009; Kallmeyer and Maier, 2010). The main contribution of this paper is a method for computing prefix probabilities for PLCFRSs. We are not aware of any existing algorithm in the literature for this task. Our method impli"
W11-2919,P87-1015,0,0.394068,"uniquely identified with one rule π. The rank of LCFRS G, written ρ(G), is the maximum rank among all rules of G. The fanout of LCFRS G, written φ(G), is the maximum fan-out among all nonterminals of G. Let a rule π be: π : A → g(A1 , A2 , . . . , Ar ), where g( hx1,1 , . . . , x1,φ(A1 ) i, ..., hxr,1 , . . . , xr,φ(Ar ) i ) = h y1,1 · · · y1,m1 , ..., yφ(A),1 · · · yφ(A),mφ(A) i Definitions This section summarizes the terminology and notation of linear context-free rewriting systems, and their probabilistic extension. For more detailed definitions on linear context-free writing systems, see Vijay-Shanker et al. (1987). For an integer n ≥ 1, we write [n] to denote the set {1, . . . , n} and [0] = ∅. We write [n]0 to denote [n] ∪ {0}. A linear context-free rewriting system (LCFRS for short) is a tuple G = (N, Σ, P, S), where N and Σ are finite, disjoint sets of nonterminal and terminal symbols, respectively. Each A ∈ N is associated with an integer value φ(A) ≥ 1, called its fan-out. The nonterminal S is the start symbol, with φ(S) = 1. 152 A LCFRS is said to be reduced if the function g from each rule occurs in some derivation from D(S). Because each function uniquely identifies a rule, this means that also"
W11-2919,D11-1112,1,0.666842,"reas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of"
W11-2919,P11-1047,1,0.710507,"reas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely that of probabilistic linear context-free rewriting systems (PLCFRS). This formalism is equivalent to the probabilistic simple RCGs discussed by Maier and Søgaard (2008) and by Kallmeyer and Maier (2010), and probabilistic extensions of"
W11-2919,C98-2152,1,0.826569,"versity of St Andrews North Haugh, St Andrews, Fife KY16 9SX United Kingdom Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy Abstract basis of the values computed off-line. Whereas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a string. In this paper we focus on a much more general formalism than those mentioned above, namely"
W11-2919,P10-1054,1,0.819341,"such a way that each rule has length bounded by some constant (Rambow and Satta, 1999). However, in the context of natural language parsing, it has been observed that the vast majority of rules in real world applications can be factorized to some small length, and that excluding the worst-case rules which cannot be handled in this way does not significantly affect accuracy; see for instance (Huang et al., 2009) and (Kuhlmann and Satta, 2009) for discussion. Efficient algorithms for factorization of LCFRSs have been presented by Kuhlmann and Satta (2009), G´omez-Rodr´ıguez and Satta (2009) and Sagot and Satta (2010). We have thus reduced the problem of computing the inside probability of w under G to the problem of computing the values of the partition function for G 0 . Because LG (w) can be less than 1, it is clear that G 0 need not be consistent, even if we assume that G is. As we have discussed in Section 2, if G 0 is any PLCFRS that may not be proper or consistent, then the values LG 0 (A) for the different nonterminals of G 0 can be expressed in terms of a system of equations. Solving such equations can be computationally expensive. A more efficient way to compute the values LG 0 (A) is possible ho"
W11-2919,P92-1012,1,0.653713,"π |new rules in G0 that are derived from π, where each new rule has size O(|π|). Thus the target grammar has size exponential in |G|. Our algorithm for composition can be easily implemented to run in time O(|G0 |), that is, in linear time in the size of the output grammar. Because of the above discussion, the algorithm runs in exponential time in the size of the input. Exponential time for the composition construction is not unexpected: the problem at hand is a generalization of the parsing problem for LCFRS, and the latter problem is known to be NP-hard when the grammar is part of the input (Satta, 1992). The critical term in the above analysis is |π|. If we can cast our LCFRS in a form in which each rule has length bounded by some constant, then composition can be carried out in polynomial time. The process of reducing the length of rules in a LCFRS is called factorization. It is known that not all LCFRSs can be factorized in such a way that each rule has length bounded by some constant (Rambow and Satta, 1999). However, in the context of natural language parsing, it has been observed that the vast majority of rules in real world applications can be factorized to some small length, and that"
W11-2919,J95-2002,0,0.527676,"for Linear Context-Free Rewriting Systems Mark-Jan Nederhof School of Computer Science University of St Andrews North Haugh, St Andrews, Fife KY16 9SX United Kingdom Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo, 6/A I-35131 Padova Italy Abstract basis of the values computed off-line. Whereas Jelinek and Lafferty (1991) consider parsing in the style of the Cocke-Kasami-Younger algorithm (Younger, 1967; Harrison, 1978), prefix probabilities for probabilistic context-free grammars can also be computed in the style of the algorithm by Earley (1970), as shown by Stolcke (1995). This approach is not restricted to context-free grammars. It was shown by Nederhof et al. (1998) that prefix probabilities can also be effectively computed for probabilistic tree adjoining grammars. That effective computation is also possible for probabilistic synchronous context-free grammars was shown by Nederhof and Satta (2011b), which departed from earlier papers on the subject in that the solution was divided into a number of steps, namely a new type of transformation of the grammar, followed by elimination of epsilon and unit rules, and the computation of the inside probability of a s"
W11-2919,P98-2157,1,\N,Missing
W11-2928,P92-1017,0,0.232637,"nstrated by experiments measuring running time. In addition, some possible optimizations are proposed. We end our paper with conclusions, in Section 8. The issue of avoiding spurious ambiguity was considered before by (Wieling et al., 2005). Our treatment differs in that the solution is at the same time more precise, in terms of synchronous CFGs rather than grammar transformations, and more succinct, using simpler constraints on allowable structures. Also novel is our parsing algorithm, which is versed towards practical application. Earlier work on partially bracketed strings, such as that by Pereira and Schabes (1992), has involved matching brackets only. native, namely to allow the human annotator to specify only the beginning of a phrase, or only the end of a phrase. This is formalized in the remainder of this paper as an unmatched open bracket, or an unmatched close bracket. Such a bracket may be labelled with a category or not. Parse selection is not constrained to be unidirectional, and at each iteration, brackets can be placed at arbitrary positions in the input sentence, and thereupon the most likely parse is (re-)computed that is consistent with the provided brackets so far. In our notation, the un"
W11-2928,W09-3835,1,0.902704,"Missing"
W11-2928,J09-1002,0,0.023296,"Missing"
W11-2928,C10-2140,1,0.865044,"Missing"
W11-2928,P81-1022,0,0.694796,"section we simplify the discussion by looking only at the context-free rules in the right parts of the synchronous rules defined in Table 1. These rules generate the right projection of TGfuzzy . We will discuss a recognition algorithm on the basis of these rules, with the tacit assumption that this can be extended to a parsing algorithm, to obtain fully bracketed strings as output, for a given partially bracketed strings as input. Naively, one could use any parsing algorithm instantiated to the set of rules in the right parts of (5). For example, one could use the classical Earley algorithm (Earley, 1970; Stolcke, 1995). This manipulates items of the form JhA → α • The information about brackets attached to the current node, or brackets from further down if no brackets are attached to the current node, is passed on bottom-up, as expressed by (9). For technical reasons, we need to augment the grammar with a new start symbol, as shown in (10). 235 For given CFG G, with set N of nonterminals, set Σ of terminals, and start symbol S ∈ N , the SCFG Gfuzzy has one synchronous rule: (l) (r) h A → (A X1 · · · Xm )A , A(s0 , s0 ) → y (l) Y1 · · · Ym y (r) i (5) for each rule A → X1 · · · Xm in G, for e"
W11-2928,H05-1101,0,0.0157066,"y. Our theoretical framework is that of order-preserving synchronous context-free grammars, to be summarized in Section 3. With this machinery, mappings between unbracketed, bracketed and partially bracketed strings will be presented in SecFor example, we can ‘fuzzify’ a fully bracketed string: (NP (Adj big )Adj (NP (Adj angry )Adj (NP (N dog )N )NP )NP )NP 232 (1) 3 Preliminaries by a partially bracketed string: big angry ( dog ) ]NP In order to formalize the main problem and its solution, we turn to a restricted type of synchronous context-free grammar (SCFG), in notation similar to that in Satta and Peserico (2005). A SCFG G defines a relation between a source language generated by a context-free grammar G1 and a target language generated by a context-free grammar G2 . In the general case, each ‘synchronous’ rule in G has the form hA → α, B → βi, where A → α is a rule in G1 and B → β is a rule in G2 . The number of nonterminal symbols in α must equal that in β. Each such synchronous rule hA → α, B → βi is also associated with a bijection from the nonterminal occurrences in α to the nonterminal occurrences in β. By this bijection, one may express a reordering of constituents between source and target str"
W11-2928,J95-2002,0,0.495768,"plify the discussion by looking only at the context-free rules in the right parts of the synchronous rules defined in Table 1. These rules generate the right projection of TGfuzzy . We will discuss a recognition algorithm on the basis of these rules, with the tacit assumption that this can be extended to a parsing algorithm, to obtain fully bracketed strings as output, for a given partially bracketed strings as input. Naively, one could use any parsing algorithm instantiated to the set of rules in the right parts of (5). For example, one could use the classical Earley algorithm (Earley, 1970; Stolcke, 1995). This manipulates items of the form JhA → α • The information about brackets attached to the current node, or brackets from further down if no brackets are attached to the current node, is passed on bottom-up, as expressed by (9). For technical reasons, we need to augment the grammar with a new start symbol, as shown in (10). 235 For given CFG G, with set N of nonterminals, set Σ of terminals, and start symbol S ∈ N , the SCFG Gfuzzy has one synchronous rule: (l) (r) h A → (A X1 · · · Xm )A , A(s0 , s0 ) → y (l) Y1 · · · Ym y (r) i (5) for each rule A → X1 · · · Xm in G, for each choice of th"
W11-2928,P01-1044,0,0.034215,"(l) (r) values of s0 , s0 , y (l) , s1 , sm , y (r) in an item 7 Experiments We have implemented the construction from Table 1 and the parsing algorithm from Table 2. Our aim was to assess the feasibility in practical terms. The latter algorithm was based on an implementation of the standard Earley algorithm, which we used as a base line. The implementation language is C++ and the experiments were performed on a laptop computer with a 2.66 GHz Intel Core 2 Duo processor. First, a context-free grammar was extracted from sections 2-21 of the Penn Treebank, with NoTransform and NoEmpties as in (Klein and Manning, 2001), and unary rules were collapsed. This grammar has 84613 rules, 372 nonterminals and 44389 terminals (words). With this grammar, we parsed the (unbracketed) sentences from section 23 that had length 10 or less. Of these, 92 sentences were outside the language generated by the grammar and were discarded. Parsing of the remaining 178 sentences using the standard Earley algorithm took 8m27s in total. Next, we tried to construct a context-free grammar that generates partially bracketed sentences without spurious ambiguity, as the right-projection of the construction in Table 1. Predictably, this w"
W11-2928,J93-2004,0,0.0414286,"model and human linguistic judgement. In a first step, the most likely parse is computed on the basis of the model. This parse is displayed to the human annotator, who looks for possible errors and enters corrections. Each correction takes the form of an occurrence of a phrase that the parse should contain. The model is then consulted anew, to recompute the most likely parse, but now under the constraint that all occurrences of phrases entered previously by the linguist must be included. This process is repeated until no more errors remain. Applications can be found in creation of treebanks (Marcus et al., 1993) and computer-assisted translation (Barrachina et al., 2009). Apart from the exact language model used in the process, there are various ways to implement interactive parse selection. One obvious approach is to demand that errors are corrected strictly from left to right. That is, where the occurrence of a 231 Proceedings of the 12th International Conference on Parsing Technologies, pages 231–240, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. tion 4. A sketch of a proof that spurious ambiguity is avoided as claimed is the subject of Section 5. The"
W11-2928,J03-1006,1,0.771901,"on that the number of possible labelled and unlabelled brackets does not contribute an extra factor to the time complexity. The recognition algorithm can also be straightforwardly extended to compute the most likely parse or the inside probability, similarly to how this is done by Jelinek et al. (1992). Note that unambiguity is essential in the latter case. The probabilities manipulated by the parser would then be based on the probabilities of rules from the original grammar, similarly to how this is normally done in probabilistic parsing algorithms based on Earley’s algorithm (Stolcke, 1995; Nederhof, 2003). βi, i, jK, which means that of a rule A → αβ from the grammar, the first part α has been processed and was found to generate the substring ai+1 · · · aj of a fixed input string a1 · · · an . The problem is that there is a very large number of rules as in the right parts of (5). This number is in fact exponential in the length m of the largest rule from the original grammar G. Casual inspection of the constraints on the rules in Table 1 (l) (r) reveals however that only the values of s0 , s0 , (l) (r) y (l) , s1 , sm , y (r) are ever used in the current rule. (l) The values of si for 1 &lt; i ≤"
W11-4401,W03-3016,1,0.763893,"Missing"
W11-4401,D11-1112,1,0.812824,"Missing"
W11-4401,W11-2919,1,0.891034,"Missing"
W11-4401,P11-1047,1,0.822588,"Missing"
W11-4401,J05-2002,1,0.864586,"Missing"
W11-4401,W09-3802,1,0.852475,"s and automata. The paradigm has its origins in (Bar-Hillel et al., 1964), where a general construction was used to prove closure of context-free languages under intersection with regular languages. It was pointed out by (Lang, 1994) that such a construction isolates the parsing problem from the recognition problem. The latter can be solved by a reduction of the outcome of intersection. The paradigm has been extended in various ways, by considering more powerful formalisms, such as tree adjoining grammars (Vijay-Shanker and Weir, 1993), simple RCGs (Bertsch and Nederhof, 2001), tree grammars (Nederhof, 2009), and probabilistic extensions of grammatical formalisms (Nederhof and Satta, 2003). Different applications have been identified, such as computation of distances between languages (Nederhof and Satta, 2008), and parameter estimation of probabilistic models (Nederhof, 2005). The lecture will focus on another application, namely the computation of prefix probabilities (Nederhof and Satta, 2011c) and infix probabilities (Nederhof and Satta, 2011a) and will address novel generalisations to linear context-free rewriting systems (Nederhof and Satta, 2011b). References Y. Bar-Hillel, M. Perles, and"
W11-4401,E93-1045,0,0.146624,"iterature to obtain elegant and general solutions to numerous problems involving grammars and automata. The paradigm has its origins in (Bar-Hillel et al., 1964), where a general construction was used to prove closure of context-free languages under intersection with regular languages. It was pointed out by (Lang, 1994) that such a construction isolates the parsing problem from the recognition problem. The latter can be solved by a reduction of the outcome of intersection. The paradigm has been extended in various ways, by considering more powerful formalisms, such as tree adjoining grammars (Vijay-Shanker and Weir, 1993), simple RCGs (Bertsch and Nederhof, 2001), tree grammars (Nederhof, 2009), and probabilistic extensions of grammatical formalisms (Nederhof and Satta, 2003). Different applications have been identified, such as computation of distances between languages (Nederhof and Satta, 2008), and parameter estimation of probabilistic models (Nederhof, 2005). The lecture will focus on another application, namely the computation of prefix probabilities (Nederhof and Satta, 2011c) and infix probabilities (Nederhof and Satta, 2011a) and will address novel generalisations to linear context-free rewriting syst"
W11-4401,W01-1807,1,\N,Missing
W12-4607,W01-1807,1,0.461835,"n its right-hand side to make up the strings for the parameters on its left-hand side. In fact, LCFRSs are attribute grammars with synthesized attributes only (Knuth, 1968) interpreted over the set of strings with concatenation. LCFRGs are weakly equivalent to multiple context-free grammars (MCFGs) (Seki et al., 1991). The string languages induced by linear CFTGs are the same as those induced by well-nested linear context-free rewriting systems (cf. footnote 3 of (Kanazawa, 2009)). A synchronous variant of well-nested LCFRSs can easily be defined in terms of generalized bimorphisms (see also (Bertsch and Nederhof, 2001)), but the connection to SCFTGs is yet to be clarified. Context-free hypergraph grammars (CFHG) (Bauderon and Courcelle, 1987; Habel and Kreowski, 1987; Engelfriet and Heyker, 1991) are context-free grammars that generate hypergraphs. Each rule of a CFHG G specifies how a hyperedge, carrying a state and adjacent with n nodes, is replaced by a hypergraph with n port (or interface) nodes. The set of derivation trees of G is a regular tree language. The hypergraph language induced by G is the set of all hypergraphs that only contain hyperedges labelled by terminals. 61  ,  q(x1 , x2 ) → σ"
W12-4607,W11-2903,1,0.888733,"Missing"
W12-4607,J07-2003,0,0.0777024,"characterization in terms of bimorphisms. An advantage over synchronous variants of linear context-free rewriting systems is the ability to specify tree-to-tree transductions. 1 Introduction Machine translation involves mappings between strings in two languages, formalized as string transductions. Early models of string transductions include syntax-directed translation schemata (Lewis II and Stearns, 1968; Aho and Ullman, 1969b; Aho and Ullman, 1969a). These are precursors of more recent models of translation, such as inversion transduction grammars (Wu, 1997), and models in the Hiero system (Chiang, 2007). The underlying assumption in such models is that source and target languages are contextfree, which is often too restrictive for practical applications. Therefore, more powerful models have been investigated, such as synchronous tree adjoining grammars (STAGs) (Shieber and Schabes, 1990), which assume that the translation to be modelled is between two tree adjoining languages. Such grammars offer an extended domain of locality, beyond the power of context-free grammars. All of the above models translate between string pairs via a hierarchical structure (i.e. a parse tree) imposed on the sour"
W12-4607,W10-2501,1,0.872853,"Missing"
W12-4607,J11-3004,0,0.0405183,"Missing"
W12-4607,W09-3810,0,0.0152847,"et al., 2009; Maletti, 2011; Maletti, 2012). The rationale for treating tree transductions as an isolated issue in machine translation is one of modularity: parsing a source sentence to produce a parse tree is challenging enough to be investigated as a separate task, next to the problem of transferring the source-language structure to the target-language structure. The awareness that phrase structure may be discontinuous, and hence exceeds the power of context-free formalisms, has been growing steadily over the past few years, owing to treebanks for many different languages. See for example (Kallmeyer et al., 2009) for evidence that synchronous rewriting cannot be avoided. The ‘gap degree’ found in some treebanks in fact even exceeds the power of tree adjoining grammars (G´omez-Rodr´ıguez et al., 2011). This suggests that more powerful formalisms such as linear context-free rewriting systems (LCFRSs) (VijayShanker et al., 1987) may be needed. While LCFRSs induce derivation trees, they lack a natural notion of derived trees. As a consequence, transduction between strings via synchronous LCFRSs do not, in any obvious 55 Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related"
W12-4607,P12-1053,0,0.020026,"anguages. The purpose of the present paper is to remedy this by introducing a formalism that combines the flexibility of synchronous context-free and synchronous tree adjoining grammars, with some of the additional generative capacity offered by LCFRSs. The new formalism consists of pairs of simple context-free tree grammars (sCFTGs) (Rounds, 1970; Engelfriet and Schmidt, 1977; Engelfriet and Schmidt, 1978), which are coupled through synchronous rewriting. The relevance of sCFTG to natural language processing is suggested by recent findings involving lexicalization of tree adjoining grammars (Maletti and Engelfriet, 2012). Among the properties that make the new formalism suitable for applications in machine translation are the following. First, it is based on tree transductions, but indirectly also describes string transductions. It can therefore be used to translate strings to strings, but also trees to trees, allowing separate modules to handle parsing/generation. Second, its generative capacity contains that of synchronous tree adjoining grammars, offering the potential to handle some difficult cases of non-projective linguistic structures. Third, parsing complexity is polynomial in the size of the input st"
W12-4607,P11-1083,0,0.0177584,"and another such structure imposed on the target string. These formalisms therefore involve a mapping between parse trees, in addition to a mapping between strings. STAGs also involve derivation trees next to parse trees. Translations between trees, formalized as tree transductions, are the main focus of formalisms such as top-down tree transducers (Rounds, 1970; Thatcher, 1970) and bottom-up tree transducers (Thatcher, 1973). These have attracted much interest in the area of statistical machine translation (SMT) (Knight and Graehl, 2005). Recent developments include (Engelfriet et al., 2009; Maletti, 2011; Maletti, 2012). The rationale for treating tree transductions as an isolated issue in machine translation is one of modularity: parsing a source sentence to produce a parse tree is challenging enough to be investigated as a separate task, next to the problem of transferring the source-language structure to the target-language structure. The awareness that phrase structure may be discontinuous, and hence exceeds the power of context-free formalisms, has been growing steadily over the past few years, owing to treebanks for many different languages. See for example (Kallmeyer et al., 2009) for"
W12-4607,N12-1027,0,0.0180245,"h structure imposed on the target string. These formalisms therefore involve a mapping between parse trees, in addition to a mapping between strings. STAGs also involve derivation trees next to parse trees. Translations between trees, formalized as tree transductions, are the main focus of formalisms such as top-down tree transducers (Rounds, 1970; Thatcher, 1970) and bottom-up tree transducers (Thatcher, 1973). These have attracted much interest in the area of statistical machine translation (SMT) (Knight and Graehl, 2005). Recent developments include (Engelfriet et al., 2009; Maletti, 2011; Maletti, 2012). The rationale for treating tree transductions as an isolated issue in machine translation is one of modularity: parsing a source sentence to produce a parse tree is challenging enough to be investigated as a separate task, next to the problem of transferring the source-language structure to the target-language structure. The awareness that phrase structure may be discontinuous, and hence exceeds the power of context-free formalisms, has been growing steadily over the past few years, owing to treebanks for many different languages. See for example (Kallmeyer et al., 2009) for evidence that sy"
W12-4607,C69-0101,0,0.405307,"djoining languages. Such grammars offer an extended domain of locality, beyond the power of context-free grammars. All of the above models translate between string pairs via a hierarchical structure (i.e. a parse tree) imposed on the source string and another such structure imposed on the target string. These formalisms therefore involve a mapping between parse trees, in addition to a mapping between strings. STAGs also involve derivation trees next to parse trees. Translations between trees, formalized as tree transductions, are the main focus of formalisms such as top-down tree transducers (Rounds, 1970; Thatcher, 1970) and bottom-up tree transducers (Thatcher, 1973). These have attracted much interest in the area of statistical machine translation (SMT) (Knight and Graehl, 2005). Recent developments include (Engelfriet et al., 2009; Maletti, 2011; Maletti, 2012). The rationale for treating tree transductions as an isolated issue in machine translation is one of modularity: parsing a source sentence to produce a parse tree is challenging enough to be investigated as a separate task, next to the problem of transferring the source-language structure to the target-language structure. The awaren"
W12-4607,C90-3045,0,0.879238,"string transductions. Early models of string transductions include syntax-directed translation schemata (Lewis II and Stearns, 1968; Aho and Ullman, 1969b; Aho and Ullman, 1969a). These are precursors of more recent models of translation, such as inversion transduction grammars (Wu, 1997), and models in the Hiero system (Chiang, 2007). The underlying assumption in such models is that source and target languages are contextfree, which is often too restrictive for practical applications. Therefore, more powerful models have been investigated, such as synchronous tree adjoining grammars (STAGs) (Shieber and Schabes, 1990), which assume that the translation to be modelled is between two tree adjoining languages. Such grammars offer an extended domain of locality, beyond the power of context-free grammars. All of the above models translate between string pairs via a hierarchical structure (i.e. a parse tree) imposed on the source string and another such structure imposed on the target string. These formalisms therefore involve a mapping between parse trees, in addition to a mapping between strings. STAGs also involve derivation trees next to parse trees. Translations between trees, formalized as tree transductio"
W12-4607,W04-3312,0,0.0342374,"er and Schabes, 1990) captures mildly context-sensitive phenomena in natural languages. STAGs with states (B¨uchse et al., 2011; B¨uchse et al., 2012) are characterized by bitransformations in which the input and output transformations are EMBs (Shieber, 2006). Thus, in view of Theorem 1, every STAG with states can be simulated by a SCFTG. Synchronous tree-substitution grammar (STSG) (Schabes, 1990) is STAG without adjoining. STSGs with states (F¨ul¨op et al., 2010) are characterized by bitransformations in which the input and output transformations are linear, nondeleting tree homomorphisms (Shieber, 2004) (also cf. Thm. 4 of (F¨ul¨op et al., 2010)). Extended top-down tree transducers (XTOP) (Rounds, 1970; Arnold and Dauchet, 1976) and extended bottom-up tree transducers (XBOT) (F¨ul¨op et al., 2011) are top-down tree transducers and bottom-up tree transducers, resp., in which the input patterns occurring in the left-hand sides of rules may have arbitrary depth. XTOPs have been used to specify e.g. English-Arabic translation (Maletti et al., 2009). The linear, nondeleting restrictions of XTOP and XBOT are denoted by ln-XTOP and ln-XBOT, respectively, and both classes are strongly equivalent (cf"
W12-4607,E06-1048,0,0.092028,"1) , and ζ1 , . . . , ζm ∈ RHS. A MAC M is linear and nondeleting if for each rule of the form (2), ζ contains exactly one occurrence of each yj (j ∈ [n]) and one of each xi (i ∈ [k]), and contains no other variables. It is pure if |Q(m) |≤ 1 for every m ∈ N. It is monadic if Q = Q(1) ∪ Q(2) . It is total and deterministic if for each q ∈ Q and δ ∈ ∆ there is exactly one rule with q and δ in its left-hand side. A MAC M is called an enriched embedded tree transducer (eEMB) if it is linear and nondeleting, pure, and total and deterministic; an eEMB M is called an embedded tree transducer (EMB) (Shieber, 2006) if it is monadic. Based on the concept of term rewriting, we can define the binary derivation relation ⇒N of N in the usual way. The tree transformation computed by N is the set JN K = {[t1 , t2 ] ∈ T∆ × TΣ | q0 (t1 ) ⇒∗N t2 }. Theorem 1. Let T ⊆ T∆ × T∆ . Then the following are equivalent. 1. There is a SCFTG G such that T = JGK. 2. There are eEMBs M1 and M2 and a regular tree language L such that T = J(JM1 K, L, JM2 K)K. Proof. 1 ⇒ 2. Let G = (Q, q0 , Σ, R) be a SCFTG. We construct the RTG H = (Q × Q, (q0 , q0 ), R, R′ ) where rkR (r) is the synchronization breadth of r for each r ∈ R, and"
W12-4607,P87-1015,0,0.876852,"ave been used to specify e.g. English-Arabic translation (Maletti et al., 2009). The linear, nondeleting restrictions of XTOP and XBOT are denoted by ln-XTOP and ln-XBOT, respectively, and both classes are strongly equivalent (cf. Prop. 3.3 of (F¨ul¨op et al., 2011)). Moreover, nl-XTOP (and hence, nl-XBOT) is strongly equivalent to STSG with states, because these classes have the same bimorphism characterization (Arnold and Dauchet, 1976) (also cf. Thm. 4.2 of (F¨ul¨op et al., 2011)). Hence, the power of nl-XTOP and nl-XBOT is subsumed by SCFTG. A linear context-free rewriting system (LCFRS) (Vijay-Shanker et al., 1987) is a string-generating device that can be thought of a context-free grammar in which each nonterminal has a fixed number of parameter positions, each of which contains a string. Moreover, each rule specifies how to synthesize the strings contained in the parameters on its right-hand side to make up the strings for the parameters on its left-hand side. In fact, LCFRSs are attribute grammars with synthesized attributes only (Knuth, 1968) interpreted over the set of strings with concatenation. LCFRGs are weakly equivalent to multiple context-free grammars (MCFGs) (Seki et al., 1991). The string"
W12-4607,J97-3002,0,0.262954,"mars. The new formalism has an alternative characterization in terms of bimorphisms. An advantage over synchronous variants of linear context-free rewriting systems is the ability to specify tree-to-tree transductions. 1 Introduction Machine translation involves mappings between strings in two languages, formalized as string transductions. Early models of string transductions include syntax-directed translation schemata (Lewis II and Stearns, 1968; Aho and Ullman, 1969b; Aho and Ullman, 1969a). These are precursors of more recent models of translation, such as inversion transduction grammars (Wu, 1997), and models in the Hiero system (Chiang, 2007). The underlying assumption in such models is that source and target languages are contextfree, which is often too restrictive for practical applications. Therefore, more powerful models have been investigated, such as synchronous tree adjoining grammars (STAGs) (Shieber and Schabes, 1990), which assume that the translation to be modelled is between two tree adjoining languages. Such grammars offer an extended domain of locality, beyond the power of context-free grammars. All of the above models translate between string pairs via a hierarchical st"
W12-4607,W90-0102,0,\N,Missing
W15-4804,P03-1054,0,0.0581978,"obtain language models from a tree corpus using probabilistic regular tree grammars (prtg). Starting with a prtg only generating trees from the corpus, the prtg is generalized step by step by merging nonterminals. We focus on bottom-up deterministic prtg to simplify the calculations. 1 Introduction Constituent parsing plays an important role in natural language processing (nlp). One can easily read off a pcfg from a tree corpus and use it for parsing. This might work quite well (Charniak, 1996), but it can be even more fruitful to introduce a state behaviour that is not visible in the corpus (Klein and Manning, 2003). The ExpectationMaximization Algorithm (Dempster et al., 1977) can be used to train probabilities if the state behaviour is fixed (Matsuzaki et al., 2005). This can be improved by adapting the state behaviour automatically by cleverly splitting and merging states (Petrov et al., 2006). More generally, finding a grammar by examining terminal objects is one of the problems investigated in the field of grammatical inference. There are many results for the string case, e.g., on how to learn deterministic stochastic finite (string) automata from text (Carrasco and Oncina, 1994; Carrasco and Oncina"
W15-4804,P05-1010,0,0.0378604,"prtg is generalized step by step by merging nonterminals. We focus on bottom-up deterministic prtg to simplify the calculations. 1 Introduction Constituent parsing plays an important role in natural language processing (nlp). One can easily read off a pcfg from a tree corpus and use it for parsing. This might work quite well (Charniak, 1996), but it can be even more fruitful to introduce a state behaviour that is not visible in the corpus (Klein and Manning, 2003). The ExpectationMaximization Algorithm (Dempster et al., 1977) can be used to train probabilities if the state behaviour is fixed (Matsuzaki et al., 2005). This can be improved by adapting the state behaviour automatically by cleverly splitting and merging states (Petrov et al., 2006). More generally, finding a grammar by examining terminal objects is one of the problems investigated in the field of grammatical inference. There are many results for the string case, e.g., on how to learn deterministic stochastic finite (string) automata from text (Carrasco and Oncina, 1994; Carrasco and Oncina, 1999). For the tree case, there are, e.g., results for identifying function distinguishable regular tree languages from text (Fernau, 2002). There is als"
W15-4804,P06-1055,0,0.0728809,"roduction Constituent parsing plays an important role in natural language processing (nlp). One can easily read off a pcfg from a tree corpus and use it for parsing. This might work quite well (Charniak, 1996), but it can be even more fruitful to introduce a state behaviour that is not visible in the corpus (Klein and Manning, 2003). The ExpectationMaximization Algorithm (Dempster et al., 1977) can be used to train probabilities if the state behaviour is fixed (Matsuzaki et al., 2005). This can be improved by adapting the state behaviour automatically by cleverly splitting and merging states (Petrov et al., 2006). More generally, finding a grammar by examining terminal objects is one of the problems investigated in the field of grammatical inference. There are many results for the string case, e.g., on how to learn deterministic stochastic finite (string) automata from text (Carrasco and Oncina, 1994; Carrasco and Oncina, 1999). For the tree case, there are, e.g., results for identifying function distinguishable regular tree languages from text (Fernau, 2002). There is also a generalization of n-grams to trees including smoothing techniques (Rico-Juan et al., 2000; Rico-Juan et al., 2002). The mention"
W15-4810,W11-4410,0,0.194963,"Missing"
W15-4810,C08-1068,0,0.0135727,"en hieroglyphic text and transliteration, as considered by Nederhof (2008), who used an automaton-based approach with configurations, similar to that in Section 4, except that manually determined penalties were used instead of probabilities. Relating hieroglyphic texts and their Egyptological transliteration is an instance of relating two alternative orthographic representations of the same language. The problem of mechanizing this task is known as machine transliteration. For example, Knight and Graehl (1998) consider translation of names and technical terms between English and katakana, and Malik et al. (2008) consider transliteration between Hindi and Urdu. Another very related problem is conversion between graphemes and phonemes, considered for example by Galescu and Allen (2002). Typical approaches to solve these tasks involve finite-state transducers. This can be justified by the local dependencies between input and output, that is, ultimately the transliteration can be broken down into mappings from at most n to at most m symbols, for some small n and m. For Ancient Egyptian however, it is unclear what those bounds on n and m would be. In this sense, Ancient Egyptian may pose a challenge to th"
W16-2340,P07-2045,0,0.00425038,"510 fr-en 6 815 ru-en 13 782 all 53 3708 Table 5: Sizes of corpora used for all empirical calculations, all produced during WMT 2015 the version of Meteor in the former is a standard off-the-shelf installation. For DTED, the W column indicates whether sentences were considered equally when aggregating, or were Weighted based on aligned word content as per section 3.5. Results run on Flattened trees (section 3.4) are indicated by the column F. All scores except those for DTED and Meteor were calculated using implementations of the metrics provided with the well-known open-source system Moses (Koehn et al., 2007). In all cases, the numbers shown are Pearson correlation coefficients between the output of the given metric at the system level and the normalised human judgements provided at WMT 2015. into the evaluation of machine translation word order. Our results suggest that this approach, while not as holistically accurate as metrics designed for that purpose, nonetheless provides scores with non-trivial similarities to human ratings. This suggests that our metric does indeed measure a significant component of humans’ intuition on sentence quality for English. While not a conclusion that can be drawn"
W16-2340,D08-1078,0,0.0230109,"etrics have been created to evaluate individual aspects of the translated output in isolation (Zeman et al., 2011; Popovi´c, 2011). When developing such granular metrics, the question of which linguistic aspects of translations to focus on is far from trivial. While there has been much related discussion in the professional and educational spheres of the factors which can affect understanding of a given translation, the academic sphere has been less prolific. Nonetheless, a widely-used taxonomy on the distinct problem types which can be observed has been produced by Vilar et al. (2006), while Birch et al. (2008) investigated those which most affect overall understanding of a translation. One of the prime factors identified by Birch et al. (2008) was word order, and metrics have been produced since then which focus on this factor (Talbot et al., 2011; Birch et al., 2010). These metrics apply various techniques, but most are based on the concept of comparing individual substrings of a source and reference sentence. While these techniques allow lightweight algorithms to produce rough scores, they ignore how the structure of a sentence can dramatically affect the impact of a mistake in ordering. For exam"
W16-2340,W07-0734,0,0.23828,"year. We find moderate correlations, despite the human judgements being based on all aspects of the sentences while our metric is based only on word order. 1 Introduction In the ever-growing field of translation metrics, a number of systems exist which attempt to provide an overall rating for a sentence. Most of these use one or more reference translations produced by a human as a gold standard. One of the earliest examples of such a metric may be BLEU (Papineni et al., 2002), using an adapted version of the well-known principle of Precision. More recently, NIST (Doddington, 2002) and Meteor (Lavie and Agarwal, 2007) have used n-gram analysis to provide similar heuristics, and many other techniques have been proposed (Dahlmeier et al., 2011; Gamon et al., 2005). These metrics are useful when making highlevel comparisons between several machine translation systems, but they offer very limited insight into the linguistic workings of the machine translation process. They can be used in automatic processes such as training systems through hillclimbing iterations, or as broad descriptions of a system’s overall quality. It is however difficult to use this kind of score to gain more precise insights into a syste"
W16-2340,P06-4018,0,0.0153958,"We have thus modified Meteor trivially to ignore the initial harmonic mean and produce only a fragmentation score; results for both this and the offthe-shelf system are reported in section 4. Talbot et al. (2011) use a similar technique to Meteor-Frag, basing its results on the number of chunks of contiguous words aligned by a human annotator. Birch et al. (2010) provide a different approach to the problem, representing word order as mathematical permutations and counting indi492 sentences, which can then be compared. We used the dependency parsing framework provided by Python’s NLTK toolkit (Bird, 2006). This in turn wraps around the Java-implemented Malt parser (Nivre, 2003). vidual disagreements in order, and transformations required to convert one sentence into another, in a number of ways. They ignore all features of a text other than word order of aligned nodes, to produce a mathematically pure model, but sacrifice some of the less vital – but still useful – information represented by unaligned nodes and inter-word semantic relationships. Contrary to the above metrics’ focus on word order in isolation, two tools have been designed to provide a simple approximation of several error categ"
W16-2340,E06-1031,0,0.166984,"othesis I spoke there to him. She let it and be left. 2 2.1 Related Work Holistic metrics Word Error Rate (Nießen et al., 2000) uses an approach closely linked to Levenshtein distances (Levenshtein, 1965), producing a straightforward count of the number of insertions, deletions and substitutions needed to convert the hypothesis into a given reference. The PositionIndependent Error Rate (Tillmann et al., 1997) performs similar calculations without considering word ordering. More recently, Translation Error Rate (Snover et al., 2006) allows ‘phrase shifting’ of word groups together, while CDer (Leusch et al., 2006) places higher priority and level of detail on block movement calculations. BLEU (Papineni et al., 2002) on the other hand has achieved success by directly comparing ngrams between the two sentences: it calculates a geometric mean of n-gram precisions and applies a penalty for short sentences. A more recent and substantial metric, Meteor (Lavie and Agarwal, 2007), first applies the parameterised harmonic mean of the Precision and Recall (Rijsbergen, 1979), which measures the correctness of the individual word choices in the hypothesis sentence. It includes a second step, taking into account th"
W16-2340,W05-0904,0,0.097591,"to produce rough scores, they ignore how the structure of a sentence can dramatically affect the impact of a mistake in ordering. For example, the mistake in the hypothesis of sentence 1 of Table 1 is much less significant than that of sentence 2, despite the latter being closer in a ‘flat’ judgement. In an attempt to mitigate these problems, though without the explicit goal of focusing on word order, some work has been done using structural evaluation of sentences through dependency parsing (Gaifman, 1965). These systems either focus on applying BLEU-style n-gram matching to a tree context (Liu and Gildea, 2005; Owczarzak et al., 2007) or focus on specific relationships between We present DTED, a submission to the WMT 2016 Metrics Task using structural information generated by dependency parsing and evaluated using tree edit distances. In this paper we apply this system to translations produced during WMT 2015, and compare our scores with human rankings from that year. We find moderate correlations, despite the human judgements being based on all aspects of the sentences while our metric is based only on word order. 1 Introduction In the ever-growing field of translation metrics, a number of systems"
W16-2340,W11-2106,0,0.0224196,"s based only on word order. 1 Introduction In the ever-growing field of translation metrics, a number of systems exist which attempt to provide an overall rating for a sentence. Most of these use one or more reference translations produced by a human as a gold standard. One of the earliest examples of such a metric may be BLEU (Papineni et al., 2002), using an adapted version of the well-known principle of Precision. More recently, NIST (Doddington, 2002) and Meteor (Lavie and Agarwal, 2007) have used n-gram analysis to provide similar heuristics, and many other techniques have been proposed (Dahlmeier et al., 2011; Gamon et al., 2005). These metrics are useful when making highlevel comparisons between several machine translation systems, but they offer very limited insight into the linguistic workings of the machine translation process. They can be used in automatic processes such as training systems through hillclimbing iterations, or as broad descriptions of a system’s overall quality. It is however difficult to use this kind of score to gain more precise insights into a system’s features; for example, different tasks may have different priorities for which er491 Proceedings of the First Conference o"
W16-2340,niessen-etal-2000-evaluation,0,0.406874,", or as broad descriptions of a system’s overall quality. It is however difficult to use this kind of score to gain more precise insights into a system’s features; for example, different tasks may have different priorities for which er491 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 491–498, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 1 2 Reference I spoke to him there. She let it be and left. Hypothesis I spoke there to him. She let it and be left. 2 2.1 Related Work Holistic metrics Word Error Rate (Nießen et al., 2000) uses an approach closely linked to Levenshtein distances (Levenshtein, 1965), producing a straightforward count of the number of insertions, deletions and substitutions needed to convert the hypothesis into a given reference. The PositionIndependent Error Rate (Tillmann et al., 1997) performs similar calculations without considering word ordering. More recently, Translation Error Rate (Snover et al., 2006) allows ‘phrase shifting’ of word groups together, while CDer (Leusch et al., 2006) places higher priority and level of detail on block movement calculations. BLEU (Papineni et al., 2002) on"
W16-2340,W03-3017,0,0.0971901,"n and produce only a fragmentation score; results for both this and the offthe-shelf system are reported in section 4. Talbot et al. (2011) use a similar technique to Meteor-Frag, basing its results on the number of chunks of contiguous words aligned by a human annotator. Birch et al. (2010) provide a different approach to the problem, representing word order as mathematical permutations and counting indi492 sentences, which can then be compared. We used the dependency parsing framework provided by Python’s NLTK toolkit (Bird, 2006). This in turn wraps around the Java-implemented Malt parser (Nivre, 2003). vidual disagreements in order, and transformations required to convert one sentence into another, in a number of ways. They ignore all features of a text other than word order of aligned nodes, to produce a mathematically pure model, but sacrifice some of the less vital – but still useful – information represented by unaligned nodes and inter-word semantic relationships. Contrary to the above metrics’ focus on word order in isolation, two tools have been designed to provide a simple approximation of several error categories at once. Both Addicter (Zeman et al., 2011) and Hjerson (Popovi´c, 2"
W16-2340,W07-0714,0,0.0387919,"Missing"
W16-2340,P02-1040,0,0.0965664,"distances. In this paper we apply this system to translations produced during WMT 2015, and compare our scores with human rankings from that year. We find moderate correlations, despite the human judgements being based on all aspects of the sentences while our metric is based only on word order. 1 Introduction In the ever-growing field of translation metrics, a number of systems exist which attempt to provide an overall rating for a sentence. Most of these use one or more reference translations produced by a human as a gold standard. One of the earliest examples of such a metric may be BLEU (Papineni et al., 2002), using an adapted version of the well-known principle of Precision. More recently, NIST (Doddington, 2002) and Meteor (Lavie and Agarwal, 2007) have used n-gram analysis to provide similar heuristics, and many other techniques have been proposed (Dahlmeier et al., 2011; Gamon et al., 2005). These metrics are useful when making highlevel comparisons between several machine translation systems, but they offer very limited insight into the linguistic workings of the machine translation process. They can be used in automatic processes such as training systems through hillclimbing iterations, or a"
W16-2340,2005.eamt-1.15,0,0.0409971,"er. 1 Introduction In the ever-growing field of translation metrics, a number of systems exist which attempt to provide an overall rating for a sentence. Most of these use one or more reference translations produced by a human as a gold standard. One of the earliest examples of such a metric may be BLEU (Papineni et al., 2002), using an adapted version of the well-known principle of Precision. More recently, NIST (Doddington, 2002) and Meteor (Lavie and Agarwal, 2007) have used n-gram analysis to provide similar heuristics, and many other techniques have been proposed (Dahlmeier et al., 2011; Gamon et al., 2005). These metrics are useful when making highlevel comparisons between several machine translation systems, but they offer very limited insight into the linguistic workings of the machine translation process. They can be used in automatic processes such as training systems through hillclimbing iterations, or as broad descriptions of a system’s overall quality. It is however difficult to use this kind of score to gain more precise insights into a system’s features; for example, different tasks may have different priorities for which er491 Proceedings of the First Conference on Machine Translation"
W16-2340,2006.amta-papers.25,0,0.11174,"ational Linguistics 1 2 Reference I spoke to him there. She let it be and left. Hypothesis I spoke there to him. She let it and be left. 2 2.1 Related Work Holistic metrics Word Error Rate (Nießen et al., 2000) uses an approach closely linked to Levenshtein distances (Levenshtein, 1965), producing a straightforward count of the number of insertions, deletions and substitutions needed to convert the hypothesis into a given reference. The PositionIndependent Error Rate (Tillmann et al., 1997) performs similar calculations without considering word ordering. More recently, Translation Error Rate (Snover et al., 2006) allows ‘phrase shifting’ of word groups together, while CDer (Leusch et al., 2006) places higher priority and level of detail on block movement calculations. BLEU (Papineni et al., 2002) on the other hand has achieved success by directly comparing ngrams between the two sentences: it calculates a geometric mean of n-gram precisions and applies a penalty for short sentences. A more recent and substantial metric, Meteor (Lavie and Agarwal, 2007), first applies the parameterised harmonic mean of the Precision and Recall (Rijsbergen, 1979), which measures the correctness of the individual word ch"
W16-2340,W11-2102,0,0.0723754,"far from trivial. While there has been much related discussion in the professional and educational spheres of the factors which can affect understanding of a given translation, the academic sphere has been less prolific. Nonetheless, a widely-used taxonomy on the distinct problem types which can be observed has been produced by Vilar et al. (2006), while Birch et al. (2008) investigated those which most affect overall understanding of a translation. One of the prime factors identified by Birch et al. (2008) was word order, and metrics have been produced since then which focus on this factor (Talbot et al., 2011; Birch et al., 2010). These metrics apply various techniques, but most are based on the concept of comparing individual substrings of a source and reference sentence. While these techniques allow lightweight algorithms to produce rough scores, they ignore how the structure of a sentence can dramatically affect the impact of a mistake in ordering. For example, the mistake in the hypothesis of sentence 1 of Table 1 is much less significant than that of sentence 2, despite the latter being closer in a ‘flat’ judgement. In an attempt to mitigate these problems, though without the explicit goal of"
W16-2340,vilar-etal-2006-error,0,0.106829,"Missing"
W16-2340,C14-1193,0,0.0773837,"ords. It does this by ‘chunking’ the sentences, finding the smallest number of groups of aligned words such that each contains words which are both adjacent and identical in both hypothesis and reference sentences. The ratio of the chunk count to the total number of aligned words represents the ‘goodness’ of the ordering, and is then multiplied with the original harmonic mean to produce a final score. Table 1: Example word order errors and groupings of nodes in the trees and compare those features between hypothesis and reference trees to produce holistic judgements (Habash and Elkholy, 2008; Yu et al., 2014). The approach of our system, named DTED (Dependency-based Tree Edit Distance), differs from existing word order literature by including dependency structures, but adds to the body of dependency-based work by focusing on node order rather than attempting to give an overall score. We work on complete dependency trees, rather than specific subsections, to produce an edit distance between the hypothesis and reference trees. A tree edit distance is a count of the actions required to convert one ordered tree into another. In the manner of Levenshtein distances (Levenshtein, 1965) and Word Error Rat"
W16-2340,W15-3001,0,\N,Missing
W16-2403,Q13-1033,0,0.0209722,"nded number of possible values, that is, those that were encountered during training, which is necessarily finite. It is not clear to us how their parser would behave if a value is encountered during testing that is larger than the maximum one encountered during training. 7 The probabilistic case One may redefine Ω to be a probability distribution, constrained by: • if Ω(A → b |[τ ], a) > 0, then a = b; and • if Ω(A → B1 · · · Bk |[τ ], a) > 0, then τ can be written as τ 0 t1 · · · tk , where root(ti ) = Bi for each i (1 ≤ i ≤ k). This is in the same spirit as the non-deterministic oracles of Goldberg and Nivre (2013). With Ω now being a probability distribution, we can refine the semantics of our automata to assign a probability to each computation, which is the product of the probabilities of all used steps. The construction from Section 5 can be extended to produce a L-PCFG, where: • S † → S ([ε],[t],$) is assigned probability 1, • A([τ ],[A(a)],b) → a is assigned Ω(A → a | [τ ], a), ([τ ],[t ],a ) ([τ ],[t ],ak ) • A([τ0 ],[t0 ],a0 ) → B1 1 1 1 · · · Bk k k is assigned Ω(A → B1 · · · Bk |[τk ], a0 ). If desired, the L-PCFG can be normalized to become proper, i.e. so that the probabilities of all rules"
W16-2403,W97-0302,0,0.228692,"as also been explored for constituent parsing (Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011). Close links between discontinuous constituent parsing and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empiri"
W16-2403,C10-2013,0,0.270469,"in Section 7, the results carry over to probabilistic automata and grammars. (2003) and Hall and Nov´ak (2005). Dependency parsers have been used to perform constituent parsing (Ma et al., 2010). Transformations from unlabeled dependency structures to constituent structures were discussed by Johnson (2007), and transformations from labeled dependency structures were discussed by Miyao et al. (2008). It has been observed that constituent parsers used to perform dependency parsing can be at least as accurate as dedicated dependency parsers, although they are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on a particular form of constituent parsing that is based on latent-variable probabilistic context-free grammars, which currently offers state-of-the-art accuracy. One apparent complication is that there are competing ways of obtaining such grammars, roughly divided into forms of EM training (Matsuzaki et al., 2005; Petrov et al., 2006) or of spectral learning (Narayan and Cohen, 2015). We circumvent this complication by loo"
W16-2403,W05-1505,0,0.0967037,"Missing"
W16-2403,J98-2004,0,0.2281,"plored for constituent parsing (Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011). Close links between discontinuous constituent parsing and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constitu"
W16-2403,P10-1110,0,0.0335662,"e and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and dependency structures. Transformation from constituent structures to dependency structures is addressed by Lin (1998), Collins (2003), Yamada and Matsumoto We provide a theoretical argument that a common form of projective transitionbased dependency parsing is less powerful than constituent parsing using latent variables. The argument is a proof that, under reasonable assumptions, a transition-based dependency parser can be"
W16-2403,cer-etal-2010-parsing,0,0.0607513,"Missing"
W16-2403,P07-1022,0,0.0293658,"ransition-based dependency parsing. It is shown in Section 5 that oracle automata can, under reasonable assumptions, be transformed into latentvariable context-free grammars. Section 6 further explores these assumptions as relating to common implementations of transition-based dependency parsing. As shown in Section 7, the results carry over to probabilistic automata and grammars. (2003) and Hall and Nov´ak (2005). Dependency parsers have been used to perform constituent parsing (Ma et al., 2010). Transformations from unlabeled dependency structures to constituent structures were discussed by Johnson (2007), and transformations from labeled dependency structures were discussed by Miyao et al. (2008). It has been observed that constituent parsers used to perform dependency parsing can be at least as accurate as dedicated dependency parsers, although they are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on a particular form of constituent parsing that is based on latent-variable probabilistic context-free grammars, w"
W16-2403,J99-1004,0,0.101004,"Missing"
W16-2403,P98-1106,0,0.271515,"Missing"
W16-2403,J03-4003,0,0.132889,"tic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and dependency structures. Transformation from constituent structures to dependency structures is addressed by Lin (1998), Collins (2003), Yamada and Matsumoto We provide a theoretical argument that a common form of projective transitionbased dependency parsing is less powerful than constituent parsing using latent variables. The argument is a proof that, under reasonable assumptions, a transition-based dependency parser can be converted to a latent-variable context-free grammar producing equivalent structures. 1 Introduction Over the last decade, transition-based dependency parsers have received much attention, to a large extent due to Nivre (2003), Nivre and Scholz (2004), Nivre et al. (2004) and following publications. The t"
W16-2403,W12-4613,0,0.0219136,"Andrews, UK Abstract tures can be generalized from the projective case to the non-projective case, but also this cannot explain the divergence from the theory of constituent parsing, as much the same style is used for describing projective dependency parsing and for non-projective dependency parsing; cf. Nivre (2009) for the latter. Furthermore, discontinuity has also been explored for constituent parsing (Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011). Close links between discontinuous constituent parsing and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close t"
W16-2403,P99-1059,0,0.275175,"the theory of constituent parsing is based. Differences lie in notation, in terminology and in the overall conceptual framework. An explanation for this cannot immediately be found by contrasting the foundations of dependency parsing and constituent parsing. Some of the earliest literature on dependency parsing (Hays, 1964; Gaifman, 1965) discusses the two kinds of parsing on an equal footing. Also more recent literature (Carroll and Charniak, 1992; Klein and Manning, 2004) discusses dependency parsing as closely related to constituent parsing. The concept of bilexical context-free grammars (Eisner and Satta, 1999) establishes further explicit connections between phrase-structure grammar and dependency grammar. See also Rambow (2010) for a discussion about the relation between constituent and dependency structures. One advantage of dependency grammar is the ease with which the definition of parse struc21 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 21–31, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics 1998; Nivre and Nilsson, 2005). The idea is that a first phase of projective parsing is followed by a lifting operation that rearrange"
W16-2403,C10-1061,0,0.536178,"Missing"
W16-2403,W04-3203,0,0.0256155,"tuent parsing and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constitu"
W16-2403,P04-1061,0,0.0273896,"and following publications. The theory represented in these publications seems to differ significantly from traditional automata theory, on which the theory of constituent parsing is based. Differences lie in notation, in terminology and in the overall conceptual framework. An explanation for this cannot immediately be found by contrasting the foundations of dependency parsing and constituent parsing. Some of the earliest literature on dependency parsing (Hays, 1964; Gaifman, 1965) discusses the two kinds of parsing on an equal footing. Also more recent literature (Carroll and Charniak, 1992; Klein and Manning, 2004) discusses dependency parsing as closely related to constituent parsing. The concept of bilexical context-free grammars (Eisner and Satta, 1999) establishes further explicit connections between phrase-structure grammar and dependency grammar. See also Rambow (2010) for a discussion about the relation between constituent and dependency structures. One advantage of dependency grammar is the ease with which the definition of parse struc21 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 21–31, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Li"
W16-2403,C12-1059,0,0.0232958,"vial tree and stack congruences result by equivalence classes that each contain a single tree or stack, respectively. This would entail an infinite number of equivalence classes. As argued above however, we may reasonably assume that the number of equivalence classes is finite, considering typical oracles would use a bounded number of features. These features are likely to investigate only a bounded number of top-most trees on the 24 5 problem. It is the oracle that ensures that only one structure is produced. Spurious ambiguity of transition-based dependency parsing is discussed at length by Goldberg and Nivre (2012). If we apply oracle automata on the above bilexical context-free grammars, we obtain what Nivre et al. (2007) call the arc-standard strategy of transition-based dependency parsing. This contrasts with the arc-eager strategy. The latter has a shift operation, which corresponds exactly to our shift. The left-arc operation corresponds to reduction with a rule Aa → Ab Aa , or in other words, a step (τ Ab (τb )Aa (τa ), v$) ` (τ Aa (Ab (τb )Aa (τa )), v$). The formulation of e.g. Nivre et al. (2007) has the top of the stack as part of the remaining input, which is largely an inconsequential notati"
W16-2403,P11-1068,0,0.0485363,"Missing"
W16-2403,nivre-etal-2006-maltparser,0,0.0451214,"A(t1 · · · tk )) = A(t01 · · · t0k0 t00k00 · · · t00k ) where the t0i and t00i are defined below. First, let imax = max{i ∈ head (F ) |F ∈ G, 1 ≤ i ≤ k} and imin = min{i ∈ head (F ) |F ∈ G, 1 ≤ −i ≤ k}. In words, in potential next applications of child in functions in G, we consider the 27 priate refinements of erase, such that the depth of the resulting trees remains bounded, by keeping only the relevant nodes near selected leaves. We will now discuss the features used by the MaltParser, one of the most widely publicized transition-based dependency parsers. The descriptions will be based on Nivre et al. (2006) and Nivre et al. (2007). All features are defined in terms of word form, part of speech or dependency relation. In our oracle automata, this information can all be encoded as parts of names of terminals and nonterminals. In the MaltParser, the word forms, parts of speech and dependency relations are attached to ‘tokens’ in the state of the parser. These tokens are found in the stack or in the remaining input. Tokens can be addressed by an index, which for the stack counts from the top downward (cf. our function nth with negative first argument), and for the remaining input counts rightward fr"
W16-2403,J13-2004,0,0.0271023,"generalized from the projective case to the non-projective case, but also this cannot explain the divergence from the theory of constituent parsing, as much the same style is used for describing projective dependency parsing and for non-projective dependency parsing; cf. Nivre (2009) for the latter. Furthermore, discontinuity has also been explored for constituent parsing (Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011). Close links between discontinuous constituent parsing and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for d"
W16-2403,1993.iwpt-1.12,0,0.0972693,"hermore, discontinuity has also been explored for constituent parsing (Kallmeyer and Maier, 2010; van Cranenburgh et al., 2011). Close links between discontinuous constituent parsing and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2"
W16-2403,W03-3017,0,0.0598894,"m constituent structures to dependency structures is addressed by Lin (1998), Collins (2003), Yamada and Matsumoto We provide a theoretical argument that a common form of projective transitionbased dependency parsing is less powerful than constituent parsing using latent variables. The argument is a proof that, under reasonable assumptions, a transition-based dependency parser can be converted to a latent-variable context-free grammar producing equivalent structures. 1 Introduction Over the last decade, transition-based dependency parsers have received much attention, to a large extent due to Nivre (2003), Nivre and Scholz (2004), Nivre et al. (2004) and following publications. The theory represented in these publications seems to differ significantly from traditional automata theory, on which the theory of constituent parsing is based. Differences lie in notation, in terminology and in the overall conceptual framework. An explanation for this cannot immediately be found by contrasting the foundations of dependency parsing and constituent parsing. Some of the earliest literature on dependency parsing (Hays, 1964; Gaifman, 1965) discusses the two kinds of parsing on an equal footing. Also more"
W16-2403,W10-4146,0,0.014928,"ards constituent parsing, but Section 4 shows that they allow a clean formalization of arc-standard and arc-eager transition-based dependency parsing. It is shown in Section 5 that oracle automata can, under reasonable assumptions, be transformed into latentvariable context-free grammars. Section 6 further explores these assumptions as relating to common implementations of transition-based dependency parsing. As shown in Section 7, the results carry over to probabilistic automata and grammars. (2003) and Hall and Nov´ak (2005). Dependency parsers have been used to perform constituent parsing (Ma et al., 2010). Transformations from unlabeled dependency structures to constituent structures were discussed by Johnson (2007), and transformations from labeled dependency structures were discussed by Miyao et al. (2008). It has been observed that constituent parsers used to perform dependency parsing can be at least as accurate as dedicated dependency parsers, although they are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on"
W16-2403,P09-1040,0,0.0322247,"y structures. One advantage of dependency grammar is the ease with which the definition of parse struc21 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 21–31, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics 1998; Nivre and Nilsson, 2005). The idea is that a first phase of projective parsing is followed by a lifting operation that rearranges edges to make them cross one another. A related idea for discontinuous constituent parsing is the reversible splitting conversion of Boyd (2007). A related but different approach is due to Nivre (2009). Here, the usual one-way input tape is replaced by a buffer. A non-topmost element from the parsing stack, which holds a word previously read from the input sentence, can be transferred back to the buffer, and thereby input positions can be effectively swapped, and non-projective structures result. We see no reason why the same idea would not equally well apply to constituent parsing. This paper has the following structure. After fixing notation in Section 2, we present a formal model of deterministic parsing in Section 3, in terms of oracle automata. These automata appear at first sight to b"
W16-2403,P05-1010,0,0.25936,"n be at least as accurate as dedicated dependency parsers, although they are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on a particular form of constituent parsing that is based on latent-variable probabilistic context-free grammars, which currently offers state-of-the-art accuracy. One apparent complication is that there are competing ways of obtaining such grammars, roughly divided into forms of EM training (Matsuzaki et al., 2005; Petrov et al., 2006) or of spectral learning (Narayan and Cohen, 2015). We circumvent this complication by looking at the general class of non-probabilistic latent-variable contextfree grammars, and show that these have sufficient formal power to subsume deterministic transition-based dependency parsing. The implication is that latent-variable probabilistic contextfree grammars, obtained through EM training, spectral learning, or any other method still to be developed, have the potential to be at least as accurate as deterministic transition-based dependency parsing. 2 Preliminaries In this"
W16-2403,P08-1006,0,0.0275684,"der reasonable assumptions, be transformed into latentvariable context-free grammars. Section 6 further explores these assumptions as relating to common implementations of transition-based dependency parsing. As shown in Section 7, the results carry over to probabilistic automata and grammars. (2003) and Hall and Nov´ak (2005). Dependency parsers have been used to perform constituent parsing (Ma et al., 2010). Transformations from unlabeled dependency structures to constituent structures were discussed by Johnson (2007), and transformations from labeled dependency structures were discussed by Miyao et al. (2008). It has been observed that constituent parsers used to perform dependency parsing can be at least as accurate as dedicated dependency parsers, although they are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on a particular form of constituent parsing that is based on latent-variable probabilistic context-free grammars, which currently offers state-of-the-art accuracy. One apparent complication is that there are c"
W16-2403,P06-1055,0,0.238728,"e as dedicated dependency parsers, although they are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on a particular form of constituent parsing that is based on latent-variable probabilistic context-free grammars, which currently offers state-of-the-art accuracy. One apparent complication is that there are competing ways of obtaining such grammars, roughly divided into forms of EM training (Matsuzaki et al., 2005; Petrov et al., 2006) or of spectral learning (Narayan and Cohen, 2015). We circumvent this complication by looking at the general class of non-probabilistic latent-variable contextfree grammars, and show that these have sufficient formal power to subsume deterministic transition-based dependency parsing. The implication is that latent-variable probabilistic contextfree grammars, obtained through EM training, spectral learning, or any other method still to be developed, have the potential to be at least as accurate as deterministic transition-based dependency parsing. 2 Preliminaries In this paper, a tree refers t"
W16-2403,D15-1214,0,0.191679,"ey are generally slower (Cer et al., 2010; Candito et al., 2010). The present article aims to elucidate part of the relation between the theory of transition-based dependency parsing and the theory of constituent parsing. We will focus on a particular form of constituent parsing that is based on latent-variable probabilistic context-free grammars, which currently offers state-of-the-art accuracy. One apparent complication is that there are competing ways of obtaining such grammars, roughly divided into forms of EM training (Matsuzaki et al., 2005; Petrov et al., 2006) or of spectral learning (Narayan and Cohen, 2015). We circumvent this complication by looking at the general class of non-probabilistic latent-variable contextfree grammars, and show that these have sufficient formal power to subsume deterministic transition-based dependency parsing. The implication is that latent-variable probabilistic contextfree grammars, obtained through EM training, spectral learning, or any other method still to be developed, have the potential to be at least as accurate as deterministic transition-based dependency parsing. 2 Preliminaries In this paper, a tree refers to a term built of leaf symbols, from the alphabet"
W16-2403,N10-1049,0,0.0323943,"n explanation for this cannot immediately be found by contrasting the foundations of dependency parsing and constituent parsing. Some of the earliest literature on dependency parsing (Hays, 1964; Gaifman, 1965) discusses the two kinds of parsing on an equal footing. Also more recent literature (Carroll and Charniak, 1992; Klein and Manning, 2004) discusses dependency parsing as closely related to constituent parsing. The concept of bilexical context-free grammars (Eisner and Satta, 1999) establishes further explicit connections between phrase-structure grammar and dependency grammar. See also Rambow (2010) for a discussion about the relation between constituent and dependency structures. One advantage of dependency grammar is the ease with which the definition of parse struc21 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 21–31, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics 1998; Nivre and Nilsson, 2005). The idea is that a first phase of projective parsing is followed by a lifting operation that rearranges edges to make them cross one another. A related idea for discontinuous constituent parsing is the reversible splitting"
W16-2403,E14-1036,1,0.849097,"ncy parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and dependency structures. Transformation f"
W16-2403,W97-0301,0,0.277673,"Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and dependency structures. Transformation from constituent structures to dependency"
W16-2403,P05-1013,0,0.0568224,"dency parsing as closely related to constituent parsing. The concept of bilexical context-free grammars (Eisner and Satta, 1999) establishes further explicit connections between phrase-structure grammar and dependency grammar. See also Rambow (2010) for a discussion about the relation between constituent and dependency structures. One advantage of dependency grammar is the ease with which the definition of parse struc21 Proceedings of the ACL Workshop on Statistical NLP and Weighted Automata, pages 21–31, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics 1998; Nivre and Nilsson, 2005). The idea is that a first phase of projective parsing is followed by a lifting operation that rearranges edges to make them cross one another. A related idea for discontinuous constituent parsing is the reversible splitting conversion of Boyd (2007). A related but different approach is due to Nivre (2009). Here, the usual one-way input tape is replaced by a buffer. A non-topmost element from the parsing stack, which holds a word previously read from the input sentence, can be transferred back to the buffer, and thereby input positions can be effectively swapped, and non-projective structures"
W16-2403,C04-1010,0,0.0583651,"structures to dependency structures is addressed by Lin (1998), Collins (2003), Yamada and Matsumoto We provide a theoretical argument that a common form of projective transitionbased dependency parsing is less powerful than constituent parsing using latent variables. The argument is a proof that, under reasonable assumptions, a transition-based dependency parser can be converted to a latent-variable context-free grammar producing equivalent structures. 1 Introduction Over the last decade, transition-based dependency parsers have received much attention, to a large extent due to Nivre (2003), Nivre and Scholz (2004), Nivre et al. (2004) and following publications. The theory represented in these publications seems to differ significantly from traditional automata theory, on which the theory of constituent parsing is based. Differences lie in notation, in terminology and in the overall conceptual framework. An explanation for this cannot immediately be found by contrasting the foundations of dependency parsing and constituent parsing. Some of the earliest literature on dependency parsing (Hays, 1964; Gaifman, 1965) discusses the two kinds of parsing on an equal footing. Also more recent literature (Carrol"
W16-2403,W05-1513,0,0.248407,"and non-projective dependency parsing follow from the work of, among others, Kallmeyer and Kuhlmann (2012) and Kuhlmann (2013). Recent literature on dependency parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and depen"
W16-2403,H89-1026,0,0.318847,"parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and dependency structures. Transformation from constituent structures to dependency structures is addressed by Lin (1998), Collins (2003), Yamada and Matsumoto We provide a theoretical ar"
W16-2403,W05-1514,0,0.0392362,"y parsing has a strong emphasis on parsing speed. Often, parsing algorithms are close to linear-time, or close to quadratic-time in the worst case (Covington, 2001). However, there is also a considerable body of literature on speeding up constituent parsing (Lavie and Tomita, 1993; Goodman, 1997; Caraballo and Charniak, 1998). Deterministic parsing algorithms for constituent parsing were proposed by e.g. Wong and Wu (1999), Kalt (2004), Sagae and Lavie (2005) and Nederhof and McCaffery (2014), while the parser of Ratnaparkhi (1997) is close to linear time; for deterministic chunk parsing, see Tsuruoka and Tsujii (2005). Seneff (1989) suggests that deterministic constituent parsing was more or less the norm at the end of the 1980s. Conversely, transition-based dependency parsing has been generalized to non-deterministic parsing (Kuhlmann et al., 2011; Huang and Sagae, 2010). An empirical connection between constituent parsing and dependency parsing has been established by several investigations of conversion between constituent structures and dependency structures. Transformation from constituent structures to dependency structures is addressed by Lin (1998), Collins (2003), Yamada and Matsumoto We provide a"
W16-2403,W11-3805,0,0.0597508,"Missing"
W16-2403,W03-3023,0,0.2621,"Missing"
W16-2403,W07-1506,0,\N,Missing
W16-2403,C98-1102,0,\N,Missing
W16-2403,P91-1032,0,\N,Missing
W19-3109,J93-2003,0,0.122289,"Missing"
W19-3109,J07-2003,0,0.156008,"combination of regular transductions and multiple context-free grammars. 1 Introduction In machine translation, one is interested in automatically translating sentences of one natural language into sentences of another natural language. Such translations can be considered as string-tostring transductions by viewing the words of a natural language as symbols of a formal language, and viewing sentences as strings. Several formal models for such transductions have been proposed, e.g., syntax-directed translation schemata (Lewis and Stearns, 1968), also known as synchronous context-free grammars (Chiang, 2007), two-way generalized sequential machines (2gsm) (Sheperdson, 1959; Aho and Ullman, 1970), MSO definable string-to-string transductions (MSO-sst) (Courcelle and Engelfriet, 2012), and streaming string transˇ y, 2010). ducers (SST) (Alur and Cern´ Boja´nczyk (2013) and Boja´nczyk et al. (2017a,b) investigated the concept of regular transductions with origin semantics, where the origin semantics of a regular transducer A is a set of the origin graphs that A can create: if A produces a portion v 0 of the output while reading the input symbol at position i, then each position of v 0 is aligned to"
W19-3109,W13-0808,0,0.0249707,"such that [[G]] = Σ∗1 . The result then follows from Lemma 5.1. This invariant implies that for every w ∈ Σ∗1 and v ∈ Σ∗2 : S 0 (w; v) ⇒∗G 0 ε if and only if S(w) ⇒∗G ε ∧ (w, v) ∈ [[A]]. Thus [[G 0 ]] = [[A]] ∩ ([[G]] × Σ∗2 ). By the assumption that G is uni-lexicalized, furthermore [[G 0 ]]o = [[A]]o e ([[G]] × Σ∗2 ). On the basis of Lemma 5.3, one can obtain complexity bounds on typical tasks involving NSST, such as deciding whether (w, v) ∈ [[A]] for given strings w and v and NSST A, relying on known complexity results for synchronous MCFG, and related formalisms such as synchronous LCFRS (Kaeshammer, 2013). Example 5.2. We consider the NSST A of Example 3.1 and the MCFG G = (N, A, Σ, P ) with N = {A}, fo(A) = 1, and for each γ ∈ Σ, P contains the rules However, the relation between NSST and synchronous MCFG does not in any obvious way suggest a practical algorithm to do inference of NSST on the basis of sets of origin graphs, and this problem must remain outside the scope of the present paper.1 A(γx1 ) → A(x1 ) and A(γ) → ε . Obviously, [[G]] = Σ∗ . We apply the construction of Lemma 5.1 to A and G and we obtain the unilexicalized synchronous MCFG G 0 which contains for each γ ∈ Σ and i ∈ {0, 1"
W97-0614,P96-1009,0,0.0261893,"d by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. whole sentences are described. Yet, as part of this it also des"
W97-0614,H91-1034,0,0.0202964,"is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. whole sentences are des"
W97-0614,H89-1043,0,0.0331708,"easible in terms of accuracy and computational resources, and thus is a viable alternative to pure concept spotting. Although the added benefit of grammatical analysis over concept spotting is not clear for our relatively simple application, the grammatical approach may become essential as soon as the application is extended in such a way that mor~ complicated grammatical constructions need to be recognized. In that case, simple concept spotting may not be able to correctly process all constructions, whereas the capabilities of the grammatical approach extend much further. Whereas some (e.g. (Moore et al., 1989)) argue that grammatical analysis may improve recognition accuracy, our current experiments have as yet not been able to reveal a clear advantage in this respect. As the basis for our implementation we have chosen definite-clause grammars (DCGs) (Pereira and Warren, 1980), a flexible formalism which is related to various kinds of common linguistics description, and which allows application of various parsing algorithms. DCGs can be translated directly into Prolog, for which interpreters and compilers exist that are fast enough to handle real-time processing of spoken input. The grammar for OVI"
W97-0614,J97-3004,1,0.875354,"Missing"
W97-0614,H89-1018,0,0.0360997,"ation, which is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. w"
W98-0130,J91-3004,0,0.0372569,"n.edu Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova satta©dei.unipd.it Abstract vVe show how prefix probabilities can be computed for stochastic linear indexed grammars (SLIGs). Our results apply as weil to stochastic tree-adjoining grammars (STAGs), due to their equivalence to SLIGs. 1 Introd uction Thc problcm of computing prefix probabilities for stochastic context-free languages is defined as follows. Given a word sequence ai ···an over some alphabet E, which we call the input prefix, we must compute quantity LweE• Pr(a1 · · ·anw). This problem has been discussed in [1, 4] with the main motivation of applications in speech recognition, where we are given some word sequence a1 • • • an-li and must hypothesize the next word an. The main idea leading to the solution of this problem is that all parts of context-free derivations that are potentially of unbounded size are captured into a set of equations that can be solved &quot;off-line&quot;, i.e., before a specific prefix is considered. This is possible because the involved deriva.tions do not depend on the given prefix. Once these equations have been solved, the results are stored. When computing the prefix probability for"
W98-0130,C98-2152,1,0.731264,"input symbols. The probabilities of the former subderivations can be computed off-line, and the results are combined with subderivations of the latter kind during computation of the prefix probability for a given string. The distinction between the two kinds of subderivations requires a certain notational system that is difficult to define for tree-adjoining grammars. We will therefore concentrate on stochastic linear indexed grammars instead, relying on their equivalence to STAGs [3]. The solution proposed in the present paper is an alternative to a different approach by the same authors in [2]. In that publication, a set of equations is transformed in order to distinguish off-line and on-line computations. 2 Computation of prefix probabilities We refer the reader to [2] for the definition ofLIG. In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol. Without loss of generality we require that rules are of the form A[17 oo] - a B[17&apos; oo] ß with 11111&apos;1 = 1, or of the form A[] - z, where lzl :::; 1. As usual, - is extended to a binary relation b"
W98-0130,C92-2066,0,0.0278857,"ded size and are -* 116 independent on actual input, and parts that are always of bounded length and do depend on input symbols. The probabilities of the former subderivations can be computed off-line, and the results are combined with subderivations of the latter kind during computation of the prefix probability for a given string. The distinction between the two kinds of subderivations requires a certain notational system that is difficult to define for tree-adjoining grammars. We will therefore concentrate on stochastic linear indexed grammars instead, relying on their equivalence to STAGs [3]. The solution proposed in the present paper is an alternative to a different approach by the same authors in [2]. In that publication, a set of equations is transformed in order to distinguish off-line and on-line computations. 2 Computation of prefix probabilities We refer the reader to [2] for the definition ofLIG. In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol. Without loss of generality we require that rules are of the form A[17 oo] - a B[17&apos;"
W98-0130,J95-2002,0,0.0273054,"n.edu Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova satta©dei.unipd.it Abstract vVe show how prefix probabilities can be computed for stochastic linear indexed grammars (SLIGs). Our results apply as weil to stochastic tree-adjoining grammars (STAGs), due to their equivalence to SLIGs. 1 Introd uction Thc problcm of computing prefix probabilities for stochastic context-free languages is defined as follows. Given a word sequence ai ···an over some alphabet E, which we call the input prefix, we must compute quantity LweE• Pr(a1 · · ·anw). This problem has been discussed in [1, 4] with the main motivation of applications in speech recognition, where we are given some word sequence a1 • • • an-li and must hypothesize the next word an. The main idea leading to the solution of this problem is that all parts of context-free derivations that are potentially of unbounded size are captured into a set of equations that can be solved &quot;off-line&quot;, i.e., before a specific prefix is considered. This is possible because the involved deriva.tions do not depend on the given prefix. Once these equations have been solved, the results are stored. When computing the prefix probability for"
W98-1302,P89-1018,0,0.0348008,"+ B . b] d&apos;~[ (B--~do) "" b:7(A--~Bbo ) [B --~ d l] [A -~ Bb •] [S -~ A , a] a (S -~ Aa [s-+~.] [A-~ • Sb] I s - , • A~] Figure 4. A finite automaton resulting from simulating the transducer on input cdba (thick lines), and the subsequent table U of dotted items (thin lines). 5 Retrieving a Parse Forest Using ""the compact representation of all possible output strings discussed above, we can obtain the structure of the input according to the context-free gTammtlr; by ""structure"" of the input we mean the collection of all parse trees. Again, we use a tabular representation, called a parse /o,zst [6, 10, 2]. Our particular kind of parse forest is a table U consisting of dott~ items of the form [q, A ~ t~,/~, q&apos;], where q and q&apos; are states from K &apos; and A ~ cl/~ is a rule. The dot indicates to how far recognition of the fight-hand side has progressed. To be more precise, the meaning of the above dotted item is that the input symbols on a path from q to q&apos; can be derived from/~. Note that recognition of fight-hand sides is done from right to left, i.e. in. reversed order with respect to Earley&apos;s algorithm [6]. For a certain instance of a rule, the initial position of the dot is given by the positio"
W98-1302,P81-1022,0,0.0459868,"+ B . b] d&apos;~[ (B--~do) "" b:7(A--~Bbo ) [B --~ d l] [A -~ Bb •] [S -~ A , a] a (S -~ Aa [s-+~.] [A-~ • Sb] I s - , • A~] Figure 4. A finite automaton resulting from simulating the transducer on input cdba (thick lines), and the subsequent table U of dotted items (thin lines). 5 Retrieving a Parse Forest Using ""the compact representation of all possible output strings discussed above, we can obtain the structure of the input according to the context-free gTammtlr; by ""structure"" of the input we mean the collection of all parse trees. Again, we use a tabular representation, called a parse /o,zst [6, 10, 2]. Our particular kind of parse forest is a table U consisting of dott~ items of the form [q, A ~ t~,/~, q&apos;], where q and q&apos; are states from K &apos; and A ~ cl/~ is a rule. The dot indicates to how far recognition of the fight-hand side has progressed. To be more precise, the meaning of the above dotted item is that the input symbols on a path from q to q&apos; can be derived from/~. Note that recognition of fight-hand sides is done from right to left, i.e. in. reversed order with respect to Earley&apos;s algorithm [6]. For a certain instance of a rule, the initial position of the dot is given by the positio"
W98-1302,P97-1058,0,0.157685,"show that context-free parsing can be realised by a 2-phase process, relying on an approximated context-freegr~mm~r. In the firstphase {tfinitetransducer performs parsing according to the approximation. In the second phase, the approximated parses are refined according to the originalgrammar. 1 Introduction A recent publication [15] presented a novel way of transforming a context-free grammar into a new grammar that generates a regular language. This new language is a superset of the orighal language. It was argued that this approach has advantages over other methods of regular approximation [16, 7]. Our method of approximation is the following. We define a condition on context-free grammars that is a suiBcient condition for a grammar to generate a regular language. We then give a transformation that turns an arbitrary grammar into another grammar that satisfies this condition. This transformation is obviously not language-preserving; it adds strings to the language generated by the original grammar, in such a way that the language becomes regular. In the present communication we show how this procedure needs to be extended so t h a t context-free parsing can be realised by a 2-phase pro"
W98-1302,J97-2003,0,0.0295629,". X ~ o ) , q B ) } end; let iI = ,413 {(qA,~le, ql)} in E E end else for each (A -~ ~) E P (* A is not recursive *) do let q = fresh_state; make_]st(qo, l~,q); let A = A U {(q,c[(A -+/3 o),ql)} end end nn I i E I i end end. p r o c e d u r e fresh.state(): create some fresh object q; let K = K U {q}; r e t u r n q end. Figure 2. Transformation/Tom a strongly regular grammar G -- (E, N, P, S) to a finite transducer T = (K, ,~, ~ U Ii,it, A, s, F). 4 Tabular Simulation of Finite Transducers After a finite transducer has been obtained, it may sometimes be turned into a deterministic transducer [13]. However, this is not always possible since not all regular transductions can be 17 I1 I1 II I : S . - + Aa 2 : A - &apos; , SB 3 : A . - + Bb 4:B~cB 5 : B --&apos;, d _-. e[£ ~./dld {s,a,B} N1 = {S,A} reeursive(N1) = left N2 = {B} recnrsive(N2) = ri#ht I~ [ B - . . d~,.__ blb_~[A---.-Bb* ala ,. e [ S-.*. aa qB qa&apos;~ - v elg qs £[B_.,.coB c]c F i g u r e 3. Application of the code from Figure 2 on a small grammar. described by means of deterministic finite transducers. In this case, input can be processed by Simulating a nondetermlni~ic transducer in a tabular way. Assume we have a finite transducer 7"""
W98-1302,1997.iwpt-1.19,1,0.868472,"ecursive(Ni) = self, as indicated in Figure 5. After this, the grammar will be strongly regular. Consider the grammar of palindromes in the left half Of Figure 1. The approximation algorithm leads to the grammar in the right half. Figure 1 (b) shows the effect on the structure of 4-- parse trees. Note that the left sides of former spines are treated by the new nonterminal Ss and the right sides by the new nonterminal Ss. This example deals with the special case that each nonterminal can lead to at most one recursive call of itself. The general case is more complicated and is treated elsewhere [15]. 7 Obtaining Correct Parse Trees In Section 5 we discussed how the table resulting from simulating the transducer should be interpreted in order to obtain a parse forest. However, we assumed then that the transducer had been constructed from a grammar that was strongly regular. In case the original grammar is not strongly regular we have to approach this task in a different way. 20 One possibility is to first apply the grammar transformation from the previous section and subsequently perform the 2-phase process as before. However, this approach results in a parse forest that reflects the stru"
W98-1302,P80-1024,0,0.0713339,"Missing"
W98-1302,P91-1032,0,\N,Missing
