2020.codi-1.14,P13-1048,0,0.0327387,"rgue that one should be able to take advantage of this fact in sense-labelling these relations.2 Section 5 describes two different ways of doing so, along with a way of dealing with another difference in sense distribution — that of implicit relations that co-occur with explicit relations and implicit relations that do not. While the particular methods used here for sense-labelling may not advance the state-of-the-art, it is the way we use them 2 Some previous approaches to discourse parsing have also distinguished relations that occur within a sentence from those that occur across sentences (Joty et al., 2013, 2015), but it was not felt to be needed in the PDTB-2, where implicit relations only appeared across sentences. 135 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 135–147 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 that should deliver a new baseline for recognizing a fuller range of implicit relations and contribute to the next generation of shallow discourse parsers.3 (1) Treasury bonds got off to a strong start, advanc2 (2) After a bad start, Treasury bonds were buoyed by ing modestly during"
2020.codi-1.14,J15-3002,0,0.0614014,"Missing"
2020.codi-1.14,J93-2004,0,0.0817664,"Missing"
2020.codi-1.14,W12-1614,0,0.0274755,"minate explicit relations linked with implicit relations from ones that are not so linked. It comprises two steps: extracting sentences that contain explicit relations as our datasets, and then recognizing the ones linked with implicit relations. Model architecture: To detect linked implicit relations from explicit relations, we use a naive Bayes classifier — specifically, the one provided in NLTK (Bird and Loper, 2004). Production rules are selected as input feature as it has been proven notably effective in feature-based implicit discourse relation recognition task among different features (Park and Cardie, 2012). Models trained in Task 1 will be adopted for linked sense classification. Training and evaluation: We follow the standard split to select the training and test set. Each token in the training set consists of Arg1, connective and Arg2, and are parsed to extract syntactic productions used in parent-child nodes in the argument parse trees. The 100 most-frequent production rules are used to build a feature dictionary for input. A production rule feature is labeled as 1 in the dictionary if it appears in the parse tree of the token, otherwise it will be 0. The linked/stand-alone label is determin"
2020.codi-1.14,prasad-etal-2008-penn,1,0.503802,"implicit relations have now been annotated within sentences as well as between them. In addition, some now cooccur with explicit discourse relations, instead of standing on their own. Here we show that while this can complicate the problem of identifying the location of implicit discourse relations, it can in turn simplify the problem of identifying their senses. We present data to support this claim, as well as methods that can serve as a non-trivial baseline for future stateof-the-art recognizers for implicit discourse relations. 1 Introduction Most readers will be familiar with the PDTB-2 (Prasad et al., 2008). At the time of its creation, it was the largest public repository of annotated discourse relations (over 43K), including over 18.4K signalled by explicit discourse connectives (coordinating or subordinating conjunctions, or discourse adverbials). In the corpus, discourse relations comprise two arguments labelled Arg1 and Arg2, with each relation anchored by either an explicit discourse connective or adjacency. In the latter case, annotators inserted one or more implicit connectives to signal the sense(s) they inferred to hold between the arguments. The size and availability of the PDTB-2 spa"
2020.codi-1.14,C10-2118,1,0.765056,". 1 https://catalog.ldc.upenn.edu/ LDC2019T05 Work on shallow discourse parsing (including the CoNLL shared tasks, as well as (Bai and Zhao, 2018; Dai and Huang, 2018; Rutherford et al., 2017; Shi and Demberg, 2017)) consistently shows that recognizing and sense labelling implicit discourse relations poses more of a challenge than doing so for explicit discourse relations. Hence, implicit relations are the focus of the current work. But there is another reason as well: Work on the PDTB-2 has assumed (correctly) that non-explicit discourse relations (i.e., implicit relations, AltLex relations (Prasad et al., 2010) and entity relations) only hold between adjacent sentences as they did in the PDTB-2, so that a sentence boundary is the only position that needs to be checked for the presence of a non-explicit relation. The difficult problem lay in assigning sense-labels to implicit relations. In Section 2, we show that, with the PDTB-3, this is no longer the case because non-explicit relations can hold within sentences as well as between them. This in turn motivates a new approach to handle implicit discourse relations in shallow discourse parsing, involving both finding them as well as identifying their s"
2020.codi-1.14,W17-6814,1,0.736355,"kens of sentence-initial ”But” in the Penn WSJ corpus and over 660 tokens of sentence-initial ”And”. 136 a late burst of buying, to end modestly higher. Conn=therefore (R ESULT) [wsj 0400] (3) Father McKenna moves through the house praying in Latin, urging the demon to split. (C ONJUNCTION) [wsj 0413] Because implicit relations within sentences don’t all occur at a single, well-defined position, this adds to the problems of shallow discourse parsing. In addition to stand-alone implicits in the PDTB3, annotators were allowed to indicate implicit relations that co-occur with explicit relations (Rohde et al., 2017, 2018), as a way of indicating a relation that did not derive from the explicit connective, but rather from what the annotator inferred from the arguments themselves, as in Ex. 4–6: (4) We’ve got to get out of the Detroit mentality and Implicit=instead be part of the world mentality, declares Charles M. Jordan, GM’s vice president for design . . . [wsj 0956] (E XPANSION .C ONJUNCTION , E XPANSION .S UBSTITUTION .A RG 2- AS - SUBST) (5) . . . Exxon Corp. built the plant but (Implicit=then) closed it in 1985. [wsj 1748] (C OMPARISON .C ONCESSION .A RG 2- AS - DENIER , T EMPORAL .A SYNCHRONOUS ."
2020.codi-1.14,P18-1210,1,0.90156,"Missing"
2020.codi-1.14,E17-1027,0,0.48583,"to hold between the arguments. The size and availability of the PDTB-2 spawned work on shallow discourse parsing, as in the 2015 and 2016 CoNLL shared tasks (Xue et al., 2015, 2016). With the release of the PDTB-31 , there are now ∼12.5K additional intra-sentential relations annotated (i.e., relations that lie wholly within the projection of a top-level S-node) and ∼1K additional inter-sentential relations (Webber et al., 2019). 1 https://catalog.ldc.upenn.edu/ LDC2019T05 Work on shallow discourse parsing (including the CoNLL shared tasks, as well as (Bai and Zhao, 2018; Dai and Huang, 2018; Rutherford et al., 2017; Shi and Demberg, 2017)) consistently shows that recognizing and sense labelling implicit discourse relations poses more of a challenge than doing so for explicit discourse relations. Hence, implicit relations are the focus of the current work. But there is another reason as well: Work on the PDTB-2 has assumed (correctly) that non-explicit discourse relations (i.e., implicit relations, AltLex relations (Prasad et al., 2010) and entity relations) only hold between adjacent sentences as they did in the PDTB-2, so that a sentence boundary is the only position that needs to be checked for the pr"
2020.codi-1.14,K16-2007,0,0.0134898,"n layer, a dense layer, and a softmax layer. Inputs to the model consist of pairs of discourse arguments, each represented as a sequence of word vectors. The output is a probability distribution of the senses between the discourse argument spans. The two sequences of word vectors are encoded by LSTMs in order to capture positional information within the sequential structure. Max-pooling on the output of the LSTMs is used to compose meaning and reduce parameters for the model, as it has been proven effective in Conneau et al. (2017). Modeling the interaction between discourse arguments follows Rutherford and Xue (2016), who argue that discourse relations can only be determined by jointly analyzing the arguments. In addition, Rutherford et al. (2017) observed the influence of different configurations on the performance of the model for the implicit sense classification task, suggesting an interaction between the lexical information in word vectors and the structural information encoded in the model itself. We follow them in adopting a 300-dimension word2vec (Mikolov et al., 2013b) word embedding and hidden size of 100 for the Basic Model. 4 Differences in the distribution of sense relations To argue for sepa"
2020.codi-1.14,K15-2001,0,0.275669,"course relations (over 43K), including over 18.4K signalled by explicit discourse connectives (coordinating or subordinating conjunctions, or discourse adverbials). In the corpus, discourse relations comprise two arguments labelled Arg1 and Arg2, with each relation anchored by either an explicit discourse connective or adjacency. In the latter case, annotators inserted one or more implicit connectives to signal the sense(s) they inferred to hold between the arguments. The size and availability of the PDTB-2 spawned work on shallow discourse parsing, as in the 2015 and 2016 CoNLL shared tasks (Xue et al., 2015, 2016). With the release of the PDTB-31 , there are now ∼12.5K additional intra-sentential relations annotated (i.e., relations that lie wholly within the projection of a top-level S-node) and ∼1K additional inter-sentential relations (Webber et al., 2019). 1 https://catalog.ldc.upenn.edu/ LDC2019T05 Work on shallow discourse parsing (including the CoNLL shared tasks, as well as (Bai and Zhao, 2018; Dai and Huang, 2018; Rutherford et al., 2017; Shi and Demberg, 2017)) consistently shows that recognizing and sense labelling implicit discourse relations poses more of a challenge than doing so f"
2020.emnlp-main.223,2020.lrec-1.129,1,0.838482,"t Arg1-as-manner Arg2-as-manner Table 2: PDTB-3 Sense Hierarchy (Webber et al., 2019). The Level-2 senses are used in assessing system performance (Section 5.1). relation between two arguments, we tried to insert a connective for this relation. If a connective conveys more than one sense or more than one relation can be inferred, multiple senses would be assigned to the token. And we use a set of consistency rules due to specific linguistic properties in Chinese such as ellipsis of subject, pair connectives. As some syntactic and textual contexts could not been annotated in our previous work (Long et al., 2020), we loosen the constraints on arguments, connectives, and distance of arguments. In this way, more relations are acquired effectively on the same texts, thus revealing the discourse coherence and structure more fully and clearly. The following are the main additions to our annotation scheme, which future efforts at Chinese discourse annotation might consider adopting as well. In the examples throughout the paper, explicit connectives are underlined, while connectives inserted for implicit relations are both underlined and parenthesized. Sense labels are indicated after the connectives. plicit"
2020.emnlp-main.223,N19-1423,0,0.0483395,"Missing"
2020.emnlp-main.223,D19-1257,0,0.0227234,"nsfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2). • benchmark results on Level-2 discourse relation classification for future"
2020.emnlp-main.223,I11-1170,0,0.033305,"lines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence explicit discourse relations. The largest Chinese discourse relation corpus for written texts is HIT-CDTB (Zhang et al., 2013), which presents a new Chinese discourse relation hierarchy adapted from the PD"
2020.emnlp-main.223,P19-1206,0,0.0136128,"h same-language cross-domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2)."
2020.emnlp-main.223,P14-2047,0,0.0219731,"ntential relations in PDTB-3 are almost the same, but clearly, we can see that the discourse relations in our corpus are more commonly annotated within the sentence, consisting of 9,847 intra-sentential relations and 5,693 inter-sentential relations. The reason perhaps lies in the use of punctuation, which is quite different in Chinese than in English. For example, a comma sometimes serves the same function as a full stop in English (Xue and Yang, 2011). Therefore, a long Chinese sentence may require the use of multiple English sentences to express the same content and preserve grammatically (Li et al., 2014). This may be why there are more intra-sentential relations in Chinese than in English. We also compared the CDTB and our TED-CDB with respect to the sense distribution. This is displayed in Figure 1(a) and 1(b). CDTB uses an annotation style similar to the PDTB for the texts from the Chinese Treebank corpus. For a discourse relation, one of eight discourse relation senses is assigned. Although all senses in the CDTB are at the same level of the hierarchy, we can map them to the four top-level relation senses in the PDTB hierarchy according to their definitions: Alternative → Expansion; Causat"
2020.emnlp-main.223,2021.ccl-1.108,0,0.0903242,"Missing"
2020.emnlp-main.223,P15-1121,0,0.0605438,"Missing"
2020.emnlp-main.223,P19-1442,0,0.0446789,"Missing"
2020.emnlp-main.223,D19-6505,0,0.0260351,"xperiments have been carried out with the TED-CDB for both same-language cross-domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accom"
2020.emnlp-main.223,prasad-etal-2008-penn,1,0.498579,"ere has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence exp"
2020.emnlp-main.223,W17-5502,0,0.0239043,"ith an audience, speakers often insert material meant to explain the details of the first argument to audience. Relations can be found across non-adjacent sentences in our annotations. The following are two examples – the first, of an explicit relation, and the second, of an implicit relation. Relations have been annotated across nonadjacent sentences While relations between non-adjacent sentences have only been annotated in the PDTB if Arg1 of an explicit connective is not adjacent to Arg2, implicit relations between non-adjacent sentences were not annotated, except in a small-scale study by Prasad et al. (2017) of relations between paragraph-initial sentences and material in the previous text. In contrast, we annotate relations across non-adjacent sentences not only for explicit relations but also im2795 (1) [我们在空间很早的时候，是做了一个接宝藏的 游戏]1 。[这种设计在现在看起来好像有点不可思 议，但是当时确实有效。因为它帮我们留住了 一些 实 在 等 不了 的 用 户 ， 也 避 免 了 用 户 流 失 。 所 以从 早 一 开 始 ， 我 们 空 间 跟 游 戏 就 息 息相 关 了]2 。ThenASYCHROUNOUS [后来， 我 们 的 团 队 也 参与去做了QQ农场的游戏]3 。 “[When we started to do Qzone, we designed a game about collecting treasures]1 . [This design may seem a bit weird now, but it worked at the time. Because it helps us retain some users who can’t wai"
2020.emnlp-main.223,I08-7010,0,0.0520237,"ere has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence exp"
2020.emnlp-main.223,P17-1090,0,0.0545751,"Missing"
2020.emnlp-main.223,E17-1027,0,0.0341222,"Missing"
2020.emnlp-main.223,P11-2111,0,0.0377191,"we try to detect all possible Altlex expressions that are capable of conveying the discourse relations. The number of the intra-sentential relations and inter-sentential relations in PDTB-3 are almost the same, but clearly, we can see that the discourse relations in our corpus are more commonly annotated within the sentence, consisting of 9,847 intra-sentential relations and 5,693 inter-sentential relations. The reason perhaps lies in the use of punctuation, which is quite different in Chinese than in English. For example, a comma sometimes serves the same function as a full stop in English (Xue and Yang, 2011). Therefore, a long Chinese sentence may require the use of multiple English sentences to express the same content and preserve grammatically (Li et al., 2014). This may be why there are more intra-sentential relations in Chinese than in English. We also compared the CDTB and our TED-CDB with respect to the sense distribution. This is displayed in Figure 1(a) and 1(b). CDTB uses an annotation style similar to the PDTB for the texts from the Chinese Treebank corpus. For a discourse relation, one of eight discourse relation senses is assigned. Although all senses in the CDTB are at the same leve"
2020.emnlp-main.223,W19-2707,0,0.0665757,"Missing"
2020.emnlp-main.223,L18-1301,0,0.020605,"k do not mention the number. systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora (Section 6). 2 Related Work Most annotations in PDTB style are conducted on written texts originating from news reports. Before 2015, there has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sen"
2020.emnlp-main.223,tonelli-etal-2010-annotation,0,0.0354948,"News report Internet news Sino and travel set TED Talks Total Relations 5,534 21,505 3,081 15,540 Availability Through LDC From owner From owner From owner Freely public available Table 1: Comparison of our corpus to related data sets. “-” means the work do not mention the number. systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora (Section 6). 2 Related Work Most annotations in PDTB style are conducted on written texts originating from news reports. Before 2015, there has been just one corpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discours"
2020.emnlp-main.223,P19-1116,0,0.0365635,"Missing"
2020.emnlp-main.223,2020.acl-main.451,0,0.0276861,"domain transfer and same-domain crosslanguage transfer. Both demonstrate that the TED-CDB can improve the performance of systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora. The dataset and our Chinese annotation guidelines has been made freely available.1 1 Introduction Recent years have witnessed increasing attention to the properties of discourse for a wide variety of natural language processing (NLP) tasks, e.g., machine translation (Ohtani et al., 2019; Voita et al., 2019), summarization (Isonuma et al., 2019; Xu et al., 2020), machine reading comprehension (Mihaylov and Frank, 2019). One of those interesting properties is the coherence between clauses and sentences arising from shallow discourse relations. As empirical approaches for modeling discourse relations usually require corpora annotated with 1 https://github.com/wanqiulong0923/TED-CDB • the largest PDTB-style Chinese discourse corpus over spoken monologues (Section 3.1). Table 1 compares the TED-CDB with other discourse-annotated Chinese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2). • benchmark resul"
2020.emnlp-main.223,P13-1013,0,0.189869,"inese corpora. • new annotation elements to accommodate Chinese-specific discourse phenomena (Section 3.2). • benchmark results on Level-2 discourse relation classification for future comparison with other models (Section 5). • experiments with cross-domain and crosslingual transfer learning that show that the TED-CDB can improve the performance of 2793 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2793–2803, c November 16–20, 2020. 2020 Association for Computational Linguistics Corpus CDTB (Zhou and Xue, 2015) CUHK (Zhou et al., 2014) HIT-CTDB (Zhang et al., 2013) NTU (Huang and Chen, 2011a) TED-CDB (ours) Domain News report News report Internet news Sino and travel set TED Talks Total Relations 5,534 21,505 3,081 15,540 Availability Through LDC From owner From owner From owner Freely public available Table 1: Comparison of our corpus to related data sets. “-” means the work do not mention the number. systems being developed for languages other than Chinese and would be helpful for insufficient or unbalanced data in other corpora (Section 6). 2 Related Work Most annotations in PDTB style are conducted on written texts originating from news reports. Bef"
2020.emnlp-main.223,zhou-etal-2014-cuhk,0,0.0168606,"iscourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence explicit discourse relations. The largest Chinese discourse relation corpus for written texts is HIT-CDTB (Zhang et al., 2013), which presents a new Chinese discourse relation hierarchy adapted from the PDTB system. Nevertheless, these four corpora can only be acquired by either purchasing or applying from the owners. Therefore, the scarcity of Chinese datasets, especially the lack of corpora fo"
2020.emnlp-main.223,P12-1008,0,0.0314872,"orpus for spoken discourse (Tonelli et al., 2010), where PDTB annotations are constructed on Italian dialogues. Recently, researchers have realized that the PDTB annotation guidelines should be used more widely instead of just being confined to construct corpora of written texts. Zeyrek et al. (2018) annotate 6 TED talks for 7 languages. Scheffler et al. (2019) build a discourse corpus on Twitter Conversations. Regarding Chinese discourse corpora for discourse relations, as illustrated in Table 1, there are mainly 4 Chinese discourse corpora based on the PDTB framework (Prasad et al., 2008a). Zhou and Xue (2012) present a PDTB-style discourse corpus for Chinese, which is further expanded to contain 164 documents, namely the Treebank (CDTB) (CDTB)(Zhou and Xue, 2015). Huang and Chen (2011b) construct a Chinese discourse corpus with 81 articles. They adopt the top-level senses from PDTB sense hierarchy and focus on the annotation of inter-sentential relations. Zhou et al. (2014) present the CUHK Discourse Treebank. They adapt the annotation scheme of Penn Discourse Treebank 2 (PDTB-2) to Chinese and re-annotate the documents of the Chinese Treebank and with only intersentence explicit discourse relatio"
2020.emnlp-main.223,N06-1055,0,0.0721642,"nk (Xue and Palmer, 2003). There is a verb “去” ，which is translated to “to” in this English translation. While the verb “去” is a poly semantic word, and it often refers to “go” in English, it tends to act as a structural auxiliary word in this example. There are several Chinese verbs that have the same function like “来” ，“让”， “用”. They always signal senses of relation like Condition, Purpose, Result and Manner. Noun phrases can serve as arguments Noun phrases have been annotated as arguments previously in Chinese discourse corpora like the CDTB (Zhou and Xue, 2015). While the Chinese NomBank (Xue, 2006) annotates the nominalized predicate, and also the Chinese Proposition Bank (Xue and Palmer, 2009) performs similar annotation of nominalized verbs. Accordingly, we do not annotate all noun phrases as arguments but those nouns which are nominalizations of their verbal form. Chinese verbs and their nominalizations share the same form, but we identify this kind of arguments, depending on whether the structure NP + 的 (of) + nominalizations of predicate appears. Moreover, in this structure, the NP can always be regarded as the object or subject of the nominalized predicate for the argument. 他 自 由"
2020.emnlp-main.223,W03-1707,0,0.180596,"iscourse relations can be expressed through a combination of the adverbial of Arg2 and the anaphoric reference to Arg1 as the implicit subject. In terms of Chinese PropBank Annotation, “使得我们比赛输了（made us lose the game)” is the ARGM-ADV, and there is a relation expressing Cause.result between between the two clauses. (4) [我到柏林]1 toPURPOSE [去参加一个16天的德语强 化]2 。 “[I went to Berlin]1 [toPURPOSE attend a 16 days’ German intensive course]2 .” In the Example (3),“参加一个16天的德语强化 （attend a 16 days’ German intensive course)” is the purpose and has been labelled as an ARGMPRP adjunct in the Chinese PropBank (Xue and Palmer, 2003). There is a verb “去” ，which is translated to “to” in this English translation. While the verb “去” is a poly semantic word, and it often refers to “go” in English, it tends to act as a structural auxiliary word in this example. There are several Chinese verbs that have the same function like “来” ，“让”， “用”. They always signal senses of relation like Condition, Purpose, Result and Manner. Noun phrases can serve as arguments Noun phrases have been annotated as arguments previously in Chinese discourse corpora like the CDTB (Zhou and Xue, 2015). While the Chinese NomBank (Xue, 2006) annotates the"
2020.findings-emnlp.203,D15-1075,0,0.0116155,"by leveraging Open Information Extraction (Banko et al., 2007) along with parsed dependency trees of the input text. Zhang et al. (2019) developed a framework to evaluate the factual correctness of generated summaries by employing an information extraction module to check facts against the source document, and proposed a training strategy that optimizes the model using reinforcement learning with factual correctness as a reward policy. Falke et al. (2019) proposed a re-ranking approach to improve factual consistency of summarization models. Their approach used natural language inference (NLI; Bowman et al. 2015) models to score candidate summaries obtained in beam search by averaging the entailment probability between all sentence pairs of source document and summary. The summary with the highest score is up-ranked and used as final output of the summarization system. After evaluating their approach using summaries generated by summarization systems trained on the CNN-DailyMail corpus (Hermann et al., 2015), they concluded that out-of-the-box NLI models transfer poorly to the task of evaluating factual correctness, limiting the effectiveness of re-ranking. 3 Methodology Let X be the article and S be"
2020.findings-emnlp.203,P19-1213,0,0.292762,"Missing"
2020.findings-emnlp.203,W18-2501,0,0.0230247,"Missing"
2020.findings-emnlp.203,N18-1065,0,0.0259571,"accuracy is defined as Precision between claims made in the source document and the generated summary, where claims are represented as subject-relation-object triplets. Durmus et al. (2020) proposed an automatic question answering based metric for evaluating faithfulness. The metric has high correlation with human evaluations, especially for highly abstractive summaries. Several studies have focused on tackling the problem of factual inconsistencies between inputs and outputs of summarization models by exploring different model architectures and methods for training and inference. Cao et al. (2018) attempted to solve the problem by encoding extracted facts as additional inputs to the system. The fact descriptions are obtained by leveraging Open Information Extraction (Banko et al., 2007) along with parsed dependency trees of the input text. Zhang et al. (2019) developed a framework to evaluate the factual correctness of generated summaries by employing an information extraction module to check facts against the source document, and proposed a training strategy that optimizes the model using reinforcement learning with factual correctness as a reward policy. Falke et al. (2019) proposed"
2020.findings-emnlp.203,N19-1423,0,0.00690062,"instances which cannot be perturbed to obtain an UNVERIFIED summary. MAN , 5 2 https://spacy.io/api/annotation# named-entities Recall 78.13 71.28 85.63 83.93 100.0 F1 76.63 73.14 85.20 83.89 100.0 Table 4: Results of H ERMAN on the test set using GloVe word embedding. Label B-V B-U I-V I-U O Experiments For all experiments, we set the hidden dimensions to 256, the word embeddings to 100, and the vocabulary size to 50k. The word embeddings are initialized using pre-trained GloVe (Pennington et al., 2014) vectors (6B tokens, uncased). We also experimented using a pre-trained, base-uncased BERT (Devlin et al., 2019) for word embedding initialization. Our training used the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. We also use gradient clipping with a maximum gradient norm of 5 and we do not use any kind of regularization. We use loss on the validation set to perform early stopping. We set α to 0.66, suggesting local verification is more important than global verification. Our model was trained on a single GeForce GTX 1080 Ti GPU with a batch size of 32. We use PyTorch (Paszke et al., 2019) for our model implementation. For CRF, we used the AllenNLP library (Gardner et al., 2018)"
2020.findings-emnlp.203,2020.acl-main.454,0,0.21577,"ther witnesses are sought at this time” . . . Several people have been injured in a three-car collision on ... B-V O O O O O O B-V O O ... 1 0 0 0 0 0 0 1 0 0 ... VERIFIED Table 2: An example of a VERIFIED summary with its labels from our dataset. Cyan text highlights the support in the source document for the quantity token highlighted green in the summary. metric for estimating the factual accuracy of generated text. Factual accuracy is defined as Precision between claims made in the source document and the generated summary, where claims are represented as subject-relation-object triplets. Durmus et al. (2020) proposed an automatic question answering based metric for evaluating faithfulness. The metric has high correlation with human evaluations, especially for highly abstractive summaries. Several studies have focused on tackling the problem of factual inconsistencies between inputs and outputs of summarization models by exploring different model architectures and methods for training and inference. Cao et al. (2018) attempted to solve the problem by encoding extracted facts as additional inputs to the system. The fact descriptions are obtained by leveraging Open Information Extraction (Banko et a"
2020.findings-emnlp.203,D19-1051,0,0.0378765,"Missing"
2020.findings-emnlp.203,W04-1013,0,0.0793426,"Missing"
2020.findings-emnlp.203,P11-1052,0,0.0500705,"nstrate that the ROUGE scores of such up-ranked summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall, resulting in higher F1 . Preliminary human evaluation of up-ranked vs. original summaries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phras"
2020.findings-emnlp.203,D19-1387,0,0.0154075,"ion. We use loss on the validation set to perform early stopping. We set α to 0.66, suggesting local verification is more important than global verification. Our model was trained on a single GeForce GTX 1080 Ti GPU with a batch size of 32. We use PyTorch (Paszke et al., 2019) for our model implementation. For CRF, we used the AllenNLP library (Gardner et al., 2018) with constrained decoding for the BIO scheme. To evaluate our verification model, we need outputs from abstractive summarization systems. We obtain those from three selected systems: TC ONV S2S (Narayan et al., 2018a), B ERT S UM (Liu and Lapata, 2019), and BART (Lewis et al., 2019) using pre-trained checkpoints provided by the authors. Precision 75.18 75.11 84.78 83.86 100.0 Precision 72.83 75.73 84.58 85.03 100.0 Recall 81.24 69.28 87.27 83.47 100.0 F1 76.81 72.37 85.90 84.24 100.0 Table 5: Results of H ERMAN on the test set using BERT word embedding. 6 Results Automatic Evaluation We first present results in Table 4 from our verification model using GloVe on the test set. On the binary classification task of determining whether a summary is VERIFIED or UNVERIFIED, the model achieved accuracy of 80.12 and F1 of 80.94. The results using BE"
2020.findings-emnlp.203,W11-1605,0,0.0779032,"Missing"
2020.findings-emnlp.203,2020.acl-main.173,0,0.211366,"Missing"
2020.findings-emnlp.203,D18-1206,1,0.928767,"d summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall, resulting in higher F1 . Preliminary human evaluation of up-ranked vs. original summaries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are c"
2020.findings-emnlp.203,N18-1158,1,0.91391,"d summaries have a higher Precision than summaries that have not been upranked, without a comparable loss in Recall, resulting in higher F1 . Preliminary human evaluation of up-ranked vs. original summaries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are c"
2020.findings-emnlp.203,D14-1162,0,0.0830157,"Missing"
2020.findings-emnlp.203,D15-1044,0,0.0166339,"aries shows people’s preference for the former. 1 Introduction Automatic summarization is the task of compressing a lengthy text to a more concise version that preserves the information of the original text. Common approaches are either extractive, selecting and assembling salient words, phrases and sentences from the source text to form the summary (Lin and Bilmes, 2011; Nallapati et al., 2017; Narayan et al., 2018b), or abstractive, generating the summary from scratch, containing novel words and phrases that are paraphrased from important parts of the original text (Clarke and Lapata, 2008; Rush et al., 2015; Wang et al., 2019). The latter is more challenging as it involves human-like capabilities, e.g., paraphrasing, generalizing, inferring and including Table 1: Examples of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are correct with respect to the article, whereas red highlighting indicates hallucinations. Note that the first article describes both a new eruption and a previous one in 2014. It was in the previous er"
2020.findings-emnlp.203,P17-1099,0,0.0345211,"of system generated abstractive summaries with hallucinated quantities. Phrases in the articles highlighted in cyan have been used by the summarization system to generate summaries. Phrases in the summaries highlighted in green are correct with respect to the article, whereas red highlighting indicates hallucinations. Note that the first article describes both a new eruption and a previous one in 2014. It was in the previous eruption that more than a dozen people were killed, hence a hallucination of at least 11 people killed and at least 20 injured in the new eruption. real-world knowledge (See et al., 2017). Abstractive summarization has attracted increasing attention recently, thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018a) and advances on neural architectures (Sutskever et al., 2014; Bahdanau et al., 2015a; Vinyals et al., 2015; Vaswani et al., 2017). Although modern abstractive summarization systems generate relatively fluent summaries, recent work has called attention to the problem they have with 2237 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2237–2249 c November 16 - 20,"
2020.findings-emnlp.203,P19-1207,0,0.0536199,"Missing"
2020.law-1.13,D18-1241,0,0.0129875,"from Stack Exchange. • We define five types of MSQ according to how they are intended to be answered, inferring intent from relations between them. • We design a baseline classifier based on surface features. ∗ Alphabetical order, equal contribution This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 138 The 14th Linguistic Annotation Workshop, pages 138–147 Barcelona, Spain (Online), December 12, 2020. Licence details: http:// 2 Prior work Prior work on QA has focused on either single questions contained within dialogue (Choi et al., 2018; Reddy et al., 2019; Saeidi et al., 2018; Clark et al., 2018), or questions composed of two or more sentences crowd-sourced by community QA (cQA) services (John and Kurian, 2011; Tamura et al., 2005). Our definition of MSQs is similar to the latter, but it should be noted that sentences in existing cQA datasets can be declarative or standalone, while in our case they must be a sequence of questions that jointly imply some user intent. Popular tasks on cQA have only considered the semantics of individual questions and answers, while we are more focused on interactions between questions. Huang"
2020.law-1.13,D08-1097,0,0.0597572,", 2018; Reddy et al., 2019; Saeidi et al., 2018; Clark et al., 2018), or questions composed of two or more sentences crowd-sourced by community QA (cQA) services (John and Kurian, 2011; Tamura et al., 2005). Our definition of MSQs is similar to the latter, but it should be noted that sentences in existing cQA datasets can be declarative or standalone, while in our case they must be a sequence of questions that jointly imply some user intent. Popular tasks on cQA have only considered the semantics of individual questions and answers, while we are more focused on interactions between questions. Huang et al. (2008) and Krishnan et al. (2005) classify questions to improve QA performance, but their work is limited to standalone questions. Ciurca (2019) was the first to identify MSQs as a distinct phenomenon, and curated a small dataset consisting of 300 MSQs extracted from Yahoo Answers. However, this dataset is too small to enable significant progress on automatic classification of MSQ intent. 3 Large-scale MSQ dataset Stack Exchange is a network of question-answering sites, where each site covers a particular topic. Questions on Stack Exchange are formatted to have a short title and then a longer body d"
2020.law-1.13,P17-1147,0,0.02323,"n? (Q2) A standard question answering system might consider these questions separately: (2) A1: You can take them in the car with you. A2: British Airways fly from NYC to London. However, this na¨ıve approach does not result in a good answer, since the querent intends that an answer take both questions into account: in (1), Q2 clarifies that taking pets by car is not a relevant option. The querent is likely looking for an answer like (3): (3) A: British Airways will let you fly pets from NYC to London. Whilst question answering (QA) has received significant research attention in recent years (Joshi et al., 2017; Agrawal et al., 2017), there is little research to date on answering MSQs, despite their prevalence in English. Furthermore, existing QA datasets are not appropriate for the study of MSQs as they tend to be sequences of standalone questions constructed in relation to a text by crowdworkers (e.g. SQuAD (Rajpurkar et al., 2016)). We are not aware of any work that has attempted to improve QA performance on MSQs, despite the potential for obvious errors as in the example above. Our contribution towards the broader research goal of automatically answering MSQs is as follows: • We create a new dat"
2020.law-1.13,H05-1040,0,0.108226,"19; Saeidi et al., 2018; Clark et al., 2018), or questions composed of two or more sentences crowd-sourced by community QA (cQA) services (John and Kurian, 2011; Tamura et al., 2005). Our definition of MSQs is similar to the latter, but it should be noted that sentences in existing cQA datasets can be declarative or standalone, while in our case they must be a sequence of questions that jointly imply some user intent. Popular tasks on cQA have only considered the semantics of individual questions and answers, while we are more focused on interactions between questions. Huang et al. (2008) and Krishnan et al. (2005) classify questions to improve QA performance, but their work is limited to standalone questions. Ciurca (2019) was the first to identify MSQs as a distinct phenomenon, and curated a small dataset consisting of 300 MSQs extracted from Yahoo Answers. However, this dataset is too small to enable significant progress on automatic classification of MSQ intent. 3 Large-scale MSQ dataset Stack Exchange is a network of question-answering sites, where each site covers a particular topic. Questions on Stack Exchange are formatted to have a short title and then a longer body describing the question, mea"
2020.law-1.13,P12-3005,0,0.0234055,"om the Stack Exchange network. We chose 93 sites within the network, and queried each site for entries with at least two question marks in the body of the question. We removed any questions with TEX and mark-up tags, then replaced any text matching a RegEx pattern for a website with ‘[website]’. From this cleaned text, we extracted pairs of MSQs by splitting the cleaned body of the question into sentences, then finding two adjacent sentences ending in ‘?’. We removed questions under 5 or over 300 characters in length. Finally, we removed any question identified as non-English using langid.py (Lui and Baldwin, 2012). Many of the questions labelled as ‘nonEnglish’ were in fact badly formed English, making language identification a useful pre-processing step. After cleaning and processing, we extracted 162,745 questions from 93 topics2 . A full list of topics and the number of questions extracted from each is given in Appendix A. We restrict the dataset to pairs of questions, leaving longer sequences of MSQs for future work. 4 MSQ type as a proxy for speaker intent MSQs are distinct from sequences of standalone questions in that their subparts need to be considered as a unit (see (1) in Section 1). This is"
2020.law-1.13,D16-1264,0,0.044019,", Q2 clarifies that taking pets by car is not a relevant option. The querent is likely looking for an answer like (3): (3) A: British Airways will let you fly pets from NYC to London. Whilst question answering (QA) has received significant research attention in recent years (Joshi et al., 2017; Agrawal et al., 2017), there is little research to date on answering MSQs, despite their prevalence in English. Furthermore, existing QA datasets are not appropriate for the study of MSQs as they tend to be sequences of standalone questions constructed in relation to a text by crowdworkers (e.g. SQuAD (Rajpurkar et al., 2016)). We are not aware of any work that has attempted to improve QA performance on MSQs, despite the potential for obvious errors as in the example above. Our contribution towards the broader research goal of automatically answering MSQs is as follows: • We create a new dataset of 162,745 English two-question MSQs from Stack Exchange. • We define five types of MSQ according to how they are intended to be answered, inferring intent from relations between them. • We design a baseline classifier based on surface features. ∗ Alphabetical order, equal contribution This work is licensed under a Creativ"
2020.law-1.13,Q19-1016,0,0.0195901,". • We define five types of MSQ according to how they are intended to be answered, inferring intent from relations between them. • We design a baseline classifier based on surface features. ∗ Alphabetical order, equal contribution This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. 138 The 14th Linguistic Annotation Workshop, pages 138–147 Barcelona, Spain (Online), December 12, 2020. Licence details: http:// 2 Prior work Prior work on QA has focused on either single questions contained within dialogue (Choi et al., 2018; Reddy et al., 2019; Saeidi et al., 2018; Clark et al., 2018), or questions composed of two or more sentences crowd-sourced by community QA (cQA) services (John and Kurian, 2011; Tamura et al., 2005). Our definition of MSQs is similar to the latter, but it should be noted that sentences in existing cQA datasets can be declarative or standalone, while in our case they must be a sequence of questions that jointly imply some user intent. Popular tasks on cQA have only considered the semantics of individual questions and answers, while we are more focused on interactions between questions. Huang et al. (2008) and Kr"
2020.law-1.13,W15-2703,1,0.925567,"e dataset to pairs of questions, leaving longer sequences of MSQs for future work. 4 MSQ type as a proxy for speaker intent MSQs are distinct from sequences of standalone questions in that their subparts need to be considered as a unit (see (1) in Section 1). This is because they form a discourse: a coherent sequence of utterances (Hobbs, 1979). In declarative sentences, the relationship between their different parts is specified by “discourse relations” (Stede, 2011; Kehler, 2006), which may be signalled with discourse markers (e.g. if, because) or discourse adverbials (e.g. as a result, see Rohde et al. (2015)). We propose adapting the notion of discourse relations to interrogatives. A particularly useful approach to discourse relations in the context of MSQs is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), which understands them to be an expression of the speaker’s communicative intent. Listeners can infer this intent under the assumptions that speakers are “cooperative” and keep their contributions as brief and relevant as possible (Grice, 1975). Transposing this theory to interrogatives, we can conceptualise the querent’s communicative intent as a specific kind of answer. Reflecti"
2020.law-1.13,D18-1233,0,0.0255959,"Missing"
2020.lrec-1.129,P14-1065,0,0.175133,"Missing"
2020.lrec-1.129,I11-1170,0,0.266975,"Missing"
2020.lrec-1.129,miltsakaki-etal-2004-penn,1,0.613407,"otation before, we annotated Chinese Ted talks to help others be aware of the differences between the Chinese discourse structure of written and spoken texts and will make our corpus publicly available to benefit the discourse-level NLP researches for spoken discourses. 3. PDTB and our Annotation Scheme The annotation scheme we adopted in this work is based on the framework of PDTB, incorporating the most recent PDTB (PDTB-3) relational taxonomy and sense hierarchy (Webber et al., 2019), shown in Table 1. PDTB follows a lexically grounded approach to the representation of discourse relations (Miltsakaki et al., 2004). Discourse relations are taken to hold between two abstract object arguments, named Arg1 and Arg2 using syntactic conventions, and are triggered either by explicit connectives or, otherwise, by adjacency between clauses and sentences. As we can see from Table 1, the PDTB-3 sense hierarchy has 4 top-level senses (Expansion, Temporal, Contingency, Contrast) and second- and third-level senses for 1 CDTB uses a flat set of senses in which Conjunction and Expansion are distinct. Previously, all Chinese annotation work using PDTB style followed the settings of PDTB-2. Some researchers tried to adap"
2020.lrec-1.129,prasad-etal-2008-penn,1,0.729435,"ction 2, we review the related existing discourse annotation work. In Section 3, we briefly introduce PDTB-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse connectives, definition and scope of arguments, and senses disambiguation, and they argued that determining the ar"
2020.lrec-1.129,C10-2118,1,0.834316,"Missing"
2020.lrec-1.129,J14-4007,1,0.831499,"-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse connectives, definition and scope of arguments, and senses disambiguation, and they argued that determining the argument scope is the most challenging part of the annotation. To further promote their research, Zhou and X"
2020.lrec-1.129,L16-1165,0,0.0556004,"Missing"
2020.lrec-1.129,L18-1301,0,0.523318,"pe of text – the planned monologues found in TED talks, following the annotation style used in the Penn Discourse TreeBank, but adapted to take account of properties of Chinese described in Section 3. TED talks (TED is short for technology, entertainment, design), as examples of planned monologues delivered to a live audience (Greenbaum, 1996), are scrupulously translated to various languages. Although TED talks have been annotated for discourse relations in several languages Wanqiu Long and Xinyi Cai have contributed equally to this work. Corresponding author: Deyi Xiong, dyxiong@tju.edu.cn (Zeyrek et al., 2018), this is the first attempt to annotate TED talks in Chinese (either translated into Chinese, or presented in Chinese), providing data on features of Chinese spoken discourse. Our annotation by and large follows the annotation scheme in the PDTB-3, adapted to features of Chinese spoken discourse described below. The rest of the paper is organized as follows: in Section 2, we review the related existing discourse annotation work. In Section 3, we briefly introduce PDTB-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and th"
2020.lrec-1.129,P12-1008,0,0.385725,"amples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse connectives, definition and scope of arguments, and senses disambiguation, and they argued that determining the argument scope is the most challenging part of the annotation. To further promote their research, Zhou and Xue (2012) presented a PDTB-style discourse corpus for Chinese."
2020.lrec-1.129,zhou-etal-2014-cuhk,0,0.855707,"features of Chinese spoken discourse described below. The rest of the paper is organized as follows: in Section 2, we review the related existing discourse annotation work. In Section 3, we briefly introduce PDTB-3 (Webber et al., 2019) and our adapted annotation scheme by examples. In Section 4, we elaborate our annotation process and the results of our inteannotator-agreement study. Finally, in Section 5, we display the results of our annotation and preliminarily analyze corpus statistics, which we compare to the relation distribution of the CUHK Discourse TreeBank for Chinese. (CUHK-DTBC)(Zhou et al., 2014). 2. Related work Following the release of the Penn Discourse Treebank (PDTB-2) in 2008 (Prasad et al., 2008), several remarkable Chinese discourse corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Chinese Discourse Treebank (Zhou and Xue, 2012), HIT Chinese Discourse Treebank (HIT-CDTB) Zhou et al. (2014), and the Discourse Treebank for Chinese (DTBC) (Zhou et al., 2014). Specifically, Xue proposed the Chinese Discourse Treebank (CDTB) Project (Xue et al., 2005). From their annotation work, they discussed the matters such as features of Chinese discourse conne"
2021.codi-main.10,W18-2501,0,0.052628,"Missing"
2021.codi-main.10,2020.acl-main.480,0,0.0223276,"T-based models, which were trained on the next sentence prediction task, benefited implicit inter-sentential discourse relation classification. Here we assess whether they also benefit classifying intra-sentential implicit relations. Looking at implicit relations in the PDTB-3, Prasad et al. (2017) consider the difficulty in extending implicit relations to relations that cross paragraph boundaries. Kurfalı and Östling (2019) examine whether implicit relation annotation in the PDTB-3 can be used as a basis for learning to classify implicit relations in languages that lack discourse annotation. Kim et al. (2020) explored whether the PDTB-3 could be used to learn finegrained (Level-2) sense classification in general, while Liang et al. (2020) looked at whether separating inter-sentential implicits from intra-sentential implicits could improve their sense classification. They also took a first step towards recognizing what sentences contained intra-sentential implicit relations, finding this benefitted from the use of linearized parse tree features. Outside the PDTB-3 framework, intra-sentential discourse relations are handled by (1) identifying 2 Related Work discourse units (DUs), (2) attaching them"
2021.codi-main.10,C12-1115,0,0.076822,"Missing"
2021.codi-main.10,D14-1162,0,0.0885821,"Missing"
2021.codi-main.10,prasad-etal-2008-penn,1,0.466269,"r different scenarios, pointing out limitations and noting future directions. 1 Introduction form of argument adjacency (with or without intervening punctuation), though annotators were asked to record one or more discourse connectives that, if present, would explicitly signal the sense(s) they inferred to hold between the arguments. Where annotators felt that the relation was already signalled by an alternative (non-connective) expression, the expression was annotated as evidence for what was called an AltLex relation (Prasad et al., 2010). The first major release of the PDTB was the PDTB-2 (Prasad et al., 2008) whose guidelines limited annotation to (a) Explicit relations lexicalized by discourse connectives, and (b) Implicit and AltLex relations between paragraph-internal adjacent sentences and between complete clauses within sentences separated by colons or semi-colons. Since there were only ∼530 intra-sentential implicit relations among the ∼15,500 implicit relations annotated in the PDTB-2, they were ignored in work on discourse parsing (Lin et al., 2014; Wang and Lan, 2015; Xue et al., 2015, 2016), which took implicit relations to hold only between adjacent sentences. The situation changed with"
2021.codi-main.10,W17-5502,0,0.0167741,"n the relation recognition task; (3) evidence that the use of parse tree features can improve model performance, as was earlier found useful in simply recognizing when a sentence contained at least one implicit intra-sentential relation (Liang et al., 2020). explicit relations. Shi and Demberg (2019) discovered that BERT-based models, which were trained on the next sentence prediction task, benefited implicit inter-sentential discourse relation classification. Here we assess whether they also benefit classifying intra-sentential implicit relations. Looking at implicit relations in the PDTB-3, Prasad et al. (2017) consider the difficulty in extending implicit relations to relations that cross paragraph boundaries. Kurfalı and Östling (2019) examine whether implicit relation annotation in the PDTB-3 can be used as a basis for learning to classify implicit relations in languages that lack discourse annotation. Kim et al. (2020) explored whether the PDTB-3 could be used to learn finegrained (Level-2) sense classification in general, while Liang et al. (2020) looked at whether separating inter-sentential implicits from intra-sentential implicits could improve their sense classification. They also took a fi"
2021.codi-main.10,C10-2118,1,0.729545,"a comprehensive analysis of our results, showcasing model performance under different scenarios, pointing out limitations and noting future directions. 1 Introduction form of argument adjacency (with or without intervening punctuation), though annotators were asked to record one or more discourse connectives that, if present, would explicitly signal the sense(s) they inferred to hold between the arguments. Where annotators felt that the relation was already signalled by an alternative (non-connective) expression, the expression was annotated as evidence for what was called an AltLex relation (Prasad et al., 2010). The first major release of the PDTB was the PDTB-2 (Prasad et al., 2008) whose guidelines limited annotation to (a) Explicit relations lexicalized by discourse connectives, and (b) Implicit and AltLex relations between paragraph-internal adjacent sentences and between complete clauses within sentences separated by colons or semi-colons. Since there were only ∼530 intra-sentential implicit relations among the ∼15,500 implicit relations annotated in the PDTB-2, they were ignored in work on discourse parsing (Lin et al., 2014; Wang and Lan, 2015; Xue et al., 2015, 2016), which took implicit rel"
2021.codi-main.10,K15-2001,0,0.189994,"called an AltLex relation (Prasad et al., 2010). The first major release of the PDTB was the PDTB-2 (Prasad et al., 2008) whose guidelines limited annotation to (a) Explicit relations lexicalized by discourse connectives, and (b) Implicit and AltLex relations between paragraph-internal adjacent sentences and between complete clauses within sentences separated by colons or semi-colons. Since there were only ∼530 intra-sentential implicit relations among the ∼15,500 implicit relations annotated in the PDTB-2, they were ignored in work on discourse parsing (Lin et al., 2014; Wang and Lan, 2015; Xue et al., 2015, 2016), which took implicit relations to hold only between adjacent sentences. The situation changed with the release of the PDTB-3 (Webber et al., 2019). Among the ∼5.6K sentence-internal implicit relations annotated in the PDTB-3 are relations between VPs or clauses conjoined implicitly by punctuation (Ex. 1), between a free adjunct or free to-infinitive and its matrix clause (Ex. 2), and between a marked syntactic construction and its matrix clause. There are also implicit relations co-occurring with explicit relations (Webber et al., 2019), as noted in Section 3.1. (1) Father McKenna move"
2021.codi-main.10,K16-2001,1,0.890918,"Missing"
2021.codi-main.10,W19-2713,0,0.19154,"e parsing is the task of identifying and categorizing discourse relations between discourse segments in a given text. The task is considered to be important for downstream tasks such as question answering (Jansen et al., 2014), machine translation (Li et al., 2014), and text summarization (Cohan et al., 2018). There are various approaches to discourse parsing, corresponding to different views of (1) what constitutes the segments of discourse, (2) what structures can be built from such segments, and (3) what semantic and/or rhetorical relations can hold between such segments (Xue et al., 2015; Zeldes et al., 2019). Approaches to discourse structure generally allow discourse relations to hold between segments within a sentence (i.e., intra-sentential discourse relations) or across sentences (i.e. inter-sentential in Latin, (Implicit=and) urging the demon to split. discourse relations) (Joty et al., 2012; Muller et al., [wsj_0413] 2012; Stede, 2011; Stede et al., 2016). (2) Father McKenna moves through the house (Implicit=while) praying in Latin, urging the In the Penn Discourse Treebank (PDTB; Prasad demon to split. [wsj_0413] et al., 2008), all discourse relations have two arguments, called Arg1 and Ar"
2021.emnlp-main.421,L16-1494,0,0.026028,"restricting our analysis to the system’s output, truly assessing whether an NLG system is producing relevant personalized output necessitates extrinsic, task-based evaluations involving users. Such methods have long been used in the evaluation of automated summarization systems (Hand, 1997; He et al., 1999; Mani, 2001; McKeown et al., 2005) as well as NLG systems (Mellish and Dale, 1998; Reiter et al., 2001; Colineau et al., 2002), though we note that recent years have seen somewhat less of this sort of ecologically valid evaluation, and much more focus on statistical evaluation; the work of Barker et al. (2016) and Newman et al. (2020) represent examples of very welcome exceptions to this trend. 7 Future work In this opinion paper we argue for the benefit of personalizing NLG tasks. We hope that through this work and others’ we will continue to make steps towards personalized text, from developing relevant focused datasets through methods that would make text more accessible and of practical use for real users. 8 Acknowledgements We would like to thank the anonymous reviewers who provided very useful comments. This research was supported by the NSF National AI Institute for Student-AI Teaming (iSAT)"
2021.emnlp-main.421,2020.findings-emnlp.428,0,0.0146242,"n scenarios that make is meant as a broad and somewhat abstract term, and encomuse of multiple reference outputs, as in such cases passes to the totality of the situation surrounding the use of a tool, including the user, their setting, and their task. Elsewhere, an implicit assumption is that the references should the word “situation” is sometimes used in a more narrow and typically exhibit minor linguistic variation, rather concrete manner, referring to either the setting in which a task takes place or to a specific task itself. than summaries that vary substantially in their con5192 tents (Cachola et al., 2020; See et al., 2017; Grusky et al., 2018; Harman and Over, 2004). While this approach simplifies the development and evaluation process, in practice, different users would likely find different aspects of the source article to be more relevant to their needs than others; in other words, if an original article includes facts A through E, one user’s optimal summary might involve facts {A, B, C} while another user’s would instead feature {A, C, D}. This example will be referred as the source-to-target transformation example. Beyond the factual content of a summary, users could also vary in terms o"
2021.emnlp-main.421,W02-2117,0,0.0813093,"d take into account the degree to which the system’s output was responsive to the additional data. Another useful avenue to explore is that of richer evaluation methodologies. Rather than restricting our analysis to the system’s output, truly assessing whether an NLG system is producing relevant personalized output necessitates extrinsic, task-based evaluations involving users. Such methods have long been used in the evaluation of automated summarization systems (Hand, 1997; He et al., 1999; Mani, 2001; McKeown et al., 2005) as well as NLG systems (Mellish and Dale, 1998; Reiter et al., 2001; Colineau et al., 2002), though we note that recent years have seen somewhat less of this sort of ecologically valid evaluation, and much more focus on statistical evaluation; the work of Barker et al. (2016) and Newman et al. (2020) represent examples of very welcome exceptions to this trend. 7 Future work In this opinion paper we argue for the benefit of personalizing NLG tasks. We hope that through this work and others’ we will continue to make steps towards personalized text, from developing relevant focused datasets through methods that would make text more accessible and of practical use for real users. 8 Ackn"
2021.emnlp-main.421,2020.eval4nlp-1.13,1,0.668876,"uring training. One exThe evaluation of natural language generation tasks ample of this is the well-documented behavior of (such as automatic summarization, machine transla- machine translation systems defaulting to “male” tion, and dialogue generation) is commonly framed for certain categories of phrase in gender-marked as one of comparing an automated system’s gen- languages (Prates et al., 2019); another can be seen erated output to some reference output,1 with the in the tendency for neural language models to overgoal of achieving as close an alignment as possible. predict frequent words (Dudy and Bedrick, 2020). Implicit in this experimental framing is the notion Recent work by Flek (2020) made a compelling that for any given system input, there must exist a argument for an increased emphasis on user- and single, “correct” output. task-level personalization in NLP applications, parWhile this may arguably be a necessary simpliticularly those involved in classification or predicfying assumption in terms of experimental evalution. In this work, we build on their foundation and ation,2 this “one-size-fits-all” philosophy has also turn our attention specifically to tasks involving constrained the ways in"
2021.emnlp-main.421,2020.acl-main.700,0,0.0413315,"he well-documented behavior of (such as automatic summarization, machine transla- machine translation systems defaulting to “male” tion, and dialogue generation) is commonly framed for certain categories of phrase in gender-marked as one of comparing an automated system’s gen- languages (Prates et al., 2019); another can be seen erated output to some reference output,1 with the in the tendency for neural language models to overgoal of achieving as close an alignment as possible. predict frequent words (Dudy and Bedrick, 2020). Implicit in this experimental framing is the notion Recent work by Flek (2020) made a compelling that for any given system input, there must exist a argument for an increased emphasis on user- and single, “correct” output. task-level personalization in NLP applications, parWhile this may arguably be a necessary simpliticularly those involved in classification or predicfying assumption in terms of experimental evalution. In this work, we build on their foundation and ation,2 this “one-size-fits-all” philosophy has also turn our attention specifically to tasks involving constrained the ways in which NLG systems are natural language generation. While the paramount designed"
2021.emnlp-main.421,N18-1065,0,0.0207101,"Missing"
2021.emnlp-main.421,W97-0706,0,0.627373,"to ground its behavior; the same input passage could be repeated with different grounding questions, and the evaluation design could take into account the degree to which the system’s output was responsive to the additional data. Another useful avenue to explore is that of richer evaluation methodologies. Rather than restricting our analysis to the system’s output, truly assessing whether an NLG system is producing relevant personalized output necessitates extrinsic, task-based evaluations involving users. Such methods have long been used in the evaluation of automated summarization systems (Hand, 1997; He et al., 1999; Mani, 2001; McKeown et al., 2005) as well as NLG systems (Mellish and Dale, 1998; Reiter et al., 2001; Colineau et al., 2002), though we note that recent years have seen somewhat less of this sort of ecologically valid evaluation, and much more focus on statistical evaluation; the work of Barker et al. (2016) and Newman et al. (2020) represent examples of very welcome exceptions to this trend. 7 Future work In this opinion paper we argue for the benefit of personalizing NLG tasks. We hope that through this work and others’ we will continue to make steps towards personalized"
2021.emnlp-main.421,W04-1003,0,0.127699,"ct term, and encomuse of multiple reference outputs, as in such cases passes to the totality of the situation surrounding the use of a tool, including the user, their setting, and their task. Elsewhere, an implicit assumption is that the references should the word “situation” is sometimes used in a more narrow and typically exhibit minor linguistic variation, rather concrete manner, referring to either the setting in which a task takes place or to a specific task itself. than summaries that vary substantially in their con5192 tents (Cachola et al., 2020; See et al., 2017; Grusky et al., 2018; Harman and Over, 2004). While this approach simplifies the development and evaluation process, in practice, different users would likely find different aspects of the source article to be more relevant to their needs than others; in other words, if an original article includes facts A through E, one user’s optimal summary might involve facts {A, B, C} while another user’s would instead feature {A, C, D}. This example will be referred as the source-to-target transformation example. Beyond the factual content of a summary, users could also vary in terms of the level of detail that they would find useful (Louis and Ne"
2021.emnlp-main.421,J88-3002,0,0.785766,"icularly those involved in classification or predicfying assumption in terms of experimental evalution. In this work, we build on their foundation and ation,2 this “one-size-fits-all” philosophy has also turn our attention specifically to tasks involving constrained the ways in which NLG systems are natural language generation. While the paramount designed, trained, and deployed. For example, stanimportance of user-level personalization in NLG dard approaches to automated document summaapplications has long been known in our field (Rich, rization and machine translation rely only on the 1979; Kass and Finin, 1988), recent years have seen 1 In some cases, a small set of reference outputs may be reduced emphasis on this aspect of system design used, as in BLEU’s original formulation. and evaluation.We challenge the universalist sim2 Statistical metrics based on this assumption often fail plifying assumption that underlies much of how to reflect human judgments of quality or performance; see Novikova et al.’s recent analysis. such systems are built and evaluated today, and 5190 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5190–5202 c November 7–11, 2021. 20"
2021.emnlp-main.421,D19-1012,0,0.0141413,"the semantic and stylistic content produced by the agent (Li et al., 2016a; Zhang et al., 2018). Going the other direction, Madotto et al. (2019) and others have worked to infer relevant properties of the interlocutor from the conversation itself, rather than relying on a pre-specified persona. An additional direction of work in dialogue personalization has been in efforts to have automated dialogue agents behave “empathetically” with their interlocutors, by attempting to match the register and contents of their output with what they perceive to be the emotional state of their user (see e.g. Lin et al., 2019).7 It is perhaps unsurprising that dialogue systems have focused on personalization to a larger extent than have other types of NLG application, as dialogue systems are deeply and necessarily useroriented, both in terms of their design and their evaluation, in a way that other types of NLG are not traditionally thought of as being. In human-human interaction we communicate in a more collaborative fashion, considering what may be additional information required to solve a problem, knowing that the same question might be responded differently depending on the user’s age, situation, gender, exper"
2021.emnlp-main.421,P16-1094,0,0.210675,"Crucially, however, there has been significant attention paid to the problem of personalizing dialogue agents, to a much greater degree than is the case in other realms of NLG. Some of this work has focused on the use of psycho-linguistically informed parameters (verbosity, etc.) to tune a system’s output (Mairesse and Walker, 2011), while other work has focused on ways to make use of more general “personas,” meant to represent salient features of the dialogue agent or its interlocutor (or both), and to use those features to influence the semantic and stylistic content produced by the agent (Li et al., 2016a; Zhang et al., 2018). Going the other direction, Madotto et al. (2019) and others have worked to infer relevant properties of the interlocutor from the conversation itself, rather than relying on a pre-specified persona. An additional direction of work in dialogue personalization has been in efforts to have automated dialogue agents behave “empathetically” with their interlocutors, by attempting to match the register and contents of their output with what they perceive to be the emotional state of their user (see e.g. Lin et al., 2019).7 It is perhaps unsurprising that dialogue systems have"
2021.emnlp-main.421,W11-1605,0,0.0300787,"Over, 2004). While this approach simplifies the development and evaluation process, in practice, different users would likely find different aspects of the source article to be more relevant to their needs than others; in other words, if an original article includes facts A through E, one user’s optimal summary might involve facts {A, B, C} while another user’s would instead feature {A, C, D}. This example will be referred as the source-to-target transformation example. Beyond the factual content of a summary, users could also vary in terms of the level of detail that they would find useful (Louis and Nenkova, 2011), some would enjoy in-depth summarization, while other users would benefit from a simplified writing style (Scarton et al., 2018a). Similarly, in a paraphrasing (Witteveen et al., 2019) task, two different users, each with different intents and goals, would likely find different paraphrases to be “correct.” A development and evaluation paradigm which assumes a single reference output (or a small set of semantically-equivalent reference outputs), is unlikely to support (or encourage) the generation of user- and task-personalized output. Thus far, we have described families of task that we will"
2021.emnlp-main.421,2021.clpsych-1.7,0,0.0346392,"ed demographic and clinical data about millions of patients from participating health care organizations. After obtaining approval by their local institutional review board (IRB) and executing a formal data use agreement, users who 5197 have been granted access are able to analyze and interact with the dataset in a secure cloud computing environment. The environment is designed such that data may not be removed or exported, and the platform includes capabilities for machine learning, statistical analysis, and data visualization. A similar enclave was used during the 2021 CL-Psych Shared Task (MacAvaney et al., 2021) in an NLP context, in order to allow the research community to interact with sensitive data relating to mental health in an IRB-controlled manner. A related approach has been used in information retrieval in situations where even limited and controlled access to the original data is not possible. Under the “Evaluation-as-a-Service” paradigm (Lin and Efron, 2013; Eggel et al., 2018), developers virtualize their systems, and send them to a secure computing environment where they are trained and evaluated on an entirely private dataset, with the results being shared among the community. While mo"
2021.emnlp-main.421,P19-1542,0,0.0208901,"the problem of personalizing dialogue agents, to a much greater degree than is the case in other realms of NLG. Some of this work has focused on the use of psycho-linguistically informed parameters (verbosity, etc.) to tune a system’s output (Mairesse and Walker, 2011), while other work has focused on ways to make use of more general “personas,” meant to represent salient features of the dialogue agent or its interlocutor (or both), and to use those features to influence the semantic and stylistic content produced by the agent (Li et al., 2016a; Zhang et al., 2018). Going the other direction, Madotto et al. (2019) and others have worked to infer relevant properties of the interlocutor from the conversation itself, rather than relying on a pre-specified persona. An additional direction of work in dialogue personalization has been in efforts to have automated dialogue agents behave “empathetically” with their interlocutors, by attempting to match the register and contents of their output with what they perceive to be the emotional state of their user (see e.g. Lin et al., 2019).7 It is perhaps unsurprising that dialogue systems have focused on personalization to a larger extent than have other types of N"
2021.emnlp-main.421,D16-1127,0,0.274178,"Crucially, however, there has been significant attention paid to the problem of personalizing dialogue agents, to a much greater degree than is the case in other realms of NLG. Some of this work has focused on the use of psycho-linguistically informed parameters (verbosity, etc.) to tune a system’s output (Mairesse and Walker, 2011), while other work has focused on ways to make use of more general “personas,” meant to represent salient features of the dialogue agent or its interlocutor (or both), and to use those features to influence the semantic and stylistic content produced by the agent (Li et al., 2016a; Zhang et al., 2018). Going the other direction, Madotto et al. (2019) and others have worked to infer relevant properties of the interlocutor from the conversation itself, rather than relying on a pre-specified persona. An additional direction of work in dialogue personalization has been in efforts to have automated dialogue agents behave “empathetically” with their interlocutors, by attempting to match the register and contents of their output with what they perceive to be the emotional state of their user (see e.g. Lin et al., 2019).7 It is perhaps unsurprising that dialogue systems have"
2021.emnlp-main.421,J11-3002,0,0.044075,"n that there is no one ground truth that is expected. Dialogue agents tend to generate general prompts that may address a given question (the query) and present a natural and informative response, but are indifferent to the user, irrespective of anything but the question it was asked about. Crucially, however, there has been significant attention paid to the problem of personalizing dialogue agents, to a much greater degree than is the case in other realms of NLG. Some of this work has focused on the use of psycho-linguistically informed parameters (verbosity, etc.) to tune a system’s output (Mairesse and Walker, 2011), while other work has focused on ways to make use of more general “personas,” meant to represent salient features of the dialogue agent or its interlocutor (or both), and to use those features to influence the semantic and stylistic content produced by the agent (Li et al., 2016a; Zhang et al., 2018). Going the other direction, Madotto et al. (2019) and others have worked to infer relevant properties of the interlocutor from the conversation itself, rather than relying on a pre-specified persona. An additional direction of work in dialogue personalization has been in efforts to have automated"
2021.emnlp-main.421,2021.acl-long.353,0,0.0768533,"Missing"
2021.emnlp-main.421,2020.scil-1.16,0,0.0670776,"pting a one-size-fits-all approach to system development and evaluation is thus more than a simplifying assumption; it ignores a key and integral aspect of system behavior. Only by taking into account the user and their task can the output of a system be made relevant; one way to achieve that is through communicating information to a user in ways that are relevant to them and/or their situation. Early work on personalized natural language systems such as that of Kass and Finin (1988) recognized this, and placed heavy emphasis on building systems around rich user models. More recently, work by Newman et al. (2020) makes the link directly to the core purpose of an NLG system, framing the problem as one of modeling the “communicative function of language.” They point out that “a speaker’s goal is not only to produce well-formed expressions, but to convey relevant information to a listener”; in the context of NLG, this must necessarily take a personalized form. As an example of how a user’s immediate situation may relate to the content and pragmatics of generated language, consider a user who asks a question-answering system “how to put out a fire.” If they are in the kitchen standing over a grease fire,"
2021.emnlp-main.421,D17-1238,0,0.0620835,"Missing"
2021.emnlp-main.421,P01-1057,0,0.341772,"valuation design could take into account the degree to which the system’s output was responsive to the additional data. Another useful avenue to explore is that of richer evaluation methodologies. Rather than restricting our analysis to the system’s output, truly assessing whether an NLG system is producing relevant personalized output necessitates extrinsic, task-based evaluations involving users. Such methods have long been used in the evaluation of automated summarization systems (Hand, 1997; He et al., 1999; Mani, 2001; McKeown et al., 2005) as well as NLG systems (Mellish and Dale, 1998; Reiter et al., 2001; Colineau et al., 2002), though we note that recent years have seen somewhat less of this sort of ecologically valid evaluation, and much more focus on statistical evaluation; the work of Barker et al. (2016) and Newman et al. (2020) represent examples of very welcome exceptions to this trend. 7 Future work In this opinion paper we argue for the benefit of personalizing NLG tasks. We hope that through this work and others’ we will continue to make steps towards personalized text, from developing relevant focused datasets through methods that would make text more accessible and of practical us"
2021.emnlp-main.421,N16-3020,0,0.0121813,"sense of agency to a user. This can be done through allowing them to define their specific needs and interests, and by that providing a sense of control over the system as described by Synofzik et al. (2008). A personalized system can find ways to reason/explain on the type of data presented to its users, promoting transparency. For instance to paraphrase a sentence for a user, a system can indicate what were the relevant dimensions it maintained and why (see the source-to-target example in section 2.1). Moreover, if a user knows how their data is used that may contribute to developing trust. Ribeiro et al. (2016) identified two types of trust; the first, whether a user trusts an individual prediction sufficiently to take some action based on it, and the second, whether the user trusts a model to behave in reasonable ways if deployed “in the wild.” We add to that a third sense of trust, can we trust the developer, that their knowledge integrated into these systems is serving the needs of the user faithfully and respectfully. Projecting aspects of value-sensitive design into text personalization tasks is the first step towards making a human-centered communication system. 5 Architectures for Personaliza"
2021.emnlp-main.421,L18-1553,0,0.341051,"different aspects of the source article to be more relevant to their needs than others; in other words, if an original article includes facts A through E, one user’s optimal summary might involve facts {A, B, C} while another user’s would instead feature {A, C, D}. This example will be referred as the source-to-target transformation example. Beyond the factual content of a summary, users could also vary in terms of the level of detail that they would find useful (Louis and Nenkova, 2011), some would enjoy in-depth summarization, while other users would benefit from a simplified writing style (Scarton et al., 2018a). Similarly, in a paraphrasing (Witteveen et al., 2019) task, two different users, each with different intents and goals, would likely find different paraphrases to be “correct.” A development and evaluation paradigm which assumes a single reference output (or a small set of semantically-equivalent reference outputs), is unlikely to support (or encourage) the generation of user- and task-personalized output. Thus far, we have described families of task that we will henceforth refer to as source-to-target transformation generation tasks. This category encompasses tasks that are firmly grounde"
2021.emnlp-main.421,P17-1099,0,0.0411472,"is meant as a broad and somewhat abstract term, and encomuse of multiple reference outputs, as in such cases passes to the totality of the situation surrounding the use of a tool, including the user, their setting, and their task. Elsewhere, an implicit assumption is that the references should the word “situation” is sometimes used in a more narrow and typically exhibit minor linguistic variation, rather concrete manner, referring to either the setting in which a task takes place or to a specific task itself. than summaries that vary substantially in their con5192 tents (Cachola et al., 2020; See et al., 2017; Grusky et al., 2018; Harman and Over, 2004). While this approach simplifies the development and evaluation process, in practice, different users would likely find different aspects of the source article to be more relevant to their needs than others; in other words, if an original article includes facts A through E, one user’s optimal summary might involve facts {A, B, C} while another user’s would instead feature {A, C, D}. This example will be referred as the source-to-target transformation example. Beyond the factual content of a summary, users could also vary in terms of the level of det"
2021.emnlp-main.421,2020.acl-main.468,0,0.0186384,"ditional context, and suggest that relevance (as or a machine translation system whose output is used in Information Retrieval) be thought of as a crucial tool for designing user-oriented of an inappropriate register of formality. Other eftext-generating tasks. We further discuss posfects are more subtle; for example, consider that, sible harms and hazards around such personby designing systems to produce a single universal alization, and argue that value-sensitive design output, we amplify the effects of label and samrepresents a crucial path forward through these ple bias in training data (Shah et al., 2020), since challenges. when there is only one “right” answer, a statistical model will generally tend towards whatever it 1 Introduction has seen most frequently during training. One exThe evaluation of natural language generation tasks ample of this is the well-documented behavior of (such as automatic summarization, machine transla- machine translation systems defaulting to “male” tion, and dialogue generation) is commonly framed for certain categories of phrase in gender-marked as one of comparing an automated system’s gen- languages (Prates et al., 2019); another can be seen erated output to"
2021.emnlp-main.421,N16-1008,0,0.0119012,"uccessful communicators.” The optimal modality may also depend on the situation, and it is reasonable to consider an event in which a user may opt for more than one modality within the same task; in the example of a driver asking Siri “how can one recognize Mount Saint Helens?”, while an image would be a highly informative modality, it would provide a less optimal output at that precise point in time. Instead, in order to avoid removing the driver’s sight from the road, a verbal description would be a less risky alternative. At times one modality may complement another as shown by the work of Wang et al. (2016) and Zhu et al. (2018), in which the output of a summarization task was presented not only through text but through images; in Wang et al. (2016) the output was also structured temporally on a timeline. Liao et al. (2018) introduced a multimodal dialogue agent for fashion retail where the creased complexity. visual appearance of clothes and matching styles are crucial in understanding the user’s intention. Returning to an earlier example, multimodality can help overcome a system’s limitations. Recall the example of a recipe system providing its beginnerlevel user with the (unhelpful) instructi"
2021.emnlp-main.421,1994.amta-1.25,0,0.47891,"Missing"
2021.emnlp-main.421,D19-5623,0,0.023845,"evant to their needs than others; in other words, if an original article includes facts A through E, one user’s optimal summary might involve facts {A, B, C} while another user’s would instead feature {A, C, D}. This example will be referred as the source-to-target transformation example. Beyond the factual content of a summary, users could also vary in terms of the level of detail that they would find useful (Louis and Nenkova, 2011), some would enjoy in-depth summarization, while other users would benefit from a simplified writing style (Scarton et al., 2018a). Similarly, in a paraphrasing (Witteveen et al., 2019) task, two different users, each with different intents and goals, would likely find different paraphrases to be “correct.” A development and evaluation paradigm which assumes a single reference output (or a small set of semantically-equivalent reference outputs), is unlikely to support (or encourage) the generation of user- and task-personalized output. Thus far, we have described families of task that we will henceforth refer to as source-to-target transformation generation tasks. This category encompasses tasks that are firmly grounded in a specific source text, and which must produce fluen"
2021.emnlp-main.421,D15-1237,0,0.064496,"Missing"
2021.emnlp-main.421,P18-1205,0,0.0223939,"r, there has been significant attention paid to the problem of personalizing dialogue agents, to a much greater degree than is the case in other realms of NLG. Some of this work has focused on the use of psycho-linguistically informed parameters (verbosity, etc.) to tune a system’s output (Mairesse and Walker, 2011), while other work has focused on ways to make use of more general “personas,” meant to represent salient features of the dialogue agent or its interlocutor (or both), and to use those features to influence the semantic and stylistic content produced by the agent (Li et al., 2016a; Zhang et al., 2018). Going the other direction, Madotto et al. (2019) and others have worked to infer relevant properties of the interlocutor from the conversation itself, rather than relying on a pre-specified persona. An additional direction of work in dialogue personalization has been in efforts to have automated dialogue agents behave “empathetically” with their interlocutors, by attempting to match the register and contents of their output with what they perceive to be the emotional state of their user (see e.g. Lin et al., 2019).7 It is perhaps unsurprising that dialogue systems have focused on personaliza"
2021.emnlp-main.421,2020.emnlp-main.531,0,0.0248243,"n we communicate in a more collaborative fashion, considering what may be additional information required to solve a problem, knowing that the same question might be responded differently depending on the user’s age, situation, gender, expertise, register, patience, and underlying intent when posing a question. Thus, when a dialogue system fails to perform in this way, it represents an obvious failure of the system, much more than slightly agrammatical output in a generated summary might, for example. Put Open domain dialogue systems (Li et al., 2016b; 5193 7 These directions can be combined (Zhong et al., 2020). in terms of the “fluency” and “adequacy” dimensions often used in MT evaluation (White et al., 1994), the two are much more closely tied together in the case of a dialogue system than they are in an MT system. All of that said, current neuralnetwork-based dialogue systems are very much in their infancy with regards to personalization; in a recent work discussing aspects of human-computer interaction in relation to dialogue systems, Kopp and Krämer (2021) suggest that the field should “(re-)emphasize the hallmarks of human communication and its complexity, and ... argue that we should not los"
2021.emnlp-main.421,D18-1448,0,0.0169095,".” The optimal modality may also depend on the situation, and it is reasonable to consider an event in which a user may opt for more than one modality within the same task; in the example of a driver asking Siri “how can one recognize Mount Saint Helens?”, while an image would be a highly informative modality, it would provide a less optimal output at that precise point in time. Instead, in order to avoid removing the driver’s sight from the road, a verbal description would be a less risky alternative. At times one modality may complement another as shown by the work of Wang et al. (2016) and Zhu et al. (2018), in which the output of a summarization task was presented not only through text but through images; in Wang et al. (2016) the output was also structured temporally on a timeline. Liao et al. (2018) introduced a multimodal dialogue agent for fashion retail where the creased complexity. visual appearance of clothes and matching styles are crucial in understanding the user’s intention. Returning to an earlier example, multimodality can help overcome a system’s limitations. Recall the example of a recipe system providing its beginnerlevel user with the (unhelpful) instruction to “fold in the che"
C10-2118,D09-1036,0,0.0226129,"Missing"
C10-2118,P02-1047,0,0.106414,"Missing"
C10-2118,J93-2004,0,0.03453,"tation of 624 tokens of AltLex in the PDTB. We turn to our analysis of these expressions in the next section. 3 What is found in AltLex? Several questions arise when considering the AltLex annotations. What kind of expressions are they? What can we learn from their syntax? Do they project discourse relations of a different sort than connectives? How can they be identified, both during manual annotation and automatically? To address these questions, we examined the AltLex annotation for annotated senses, and for common lexico-syntactic patterns extracted using alignment with the Penn Treebank (Marcus et al., 1993).4 3.1 Lexico-syntactic Characterization We found that we could partition AltLex annotation into three groups by (a) whether or not they belonged to one of the syntactic classes admitted as explicit connectives in the PDTB, and (b) whether the expression was frozen (ie, blocking free substitution, modification or deletion of any of its parts) or open-ended. The three groups are shown in Table 1 and discussed below. 4 The source texts of the PDTB come from the Penn Treebank (PTB) portion of the Wall Street Journal corpus. The PDTB corpus provides PTB tree alignments of all its text span annotat"
C10-2118,W09-3029,1,0.821637,"Missing"
C10-2118,P09-2004,0,0.00784831,"verbs, and prepositional phrases. Thus the literature presents lists of DRMs, which researchers try to make as complete as possible for their chosen language. In annotating lexicalized discourse relations of the Penn Discourse Treebank (Prasad et al., 2008), this same assumption drove the initial phase of annotation. A list of “explicit connectives” was collected from various sources and provided to annotators, who then searched for these expressions in the text and annotated them, along with their arguments and senses. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Since expressions functioning as DRMs can also have non-DRM functions, the task is framed as one of classifying given individual tokens as DRM or not DRM. In this paper, we argue that placing such syntactic and lexical restrictions on DRMs limits a proper understanding of discourse relations, which can be realized in other ways as well. For example, one should recognize that the instantiation (or exemplification) relation between the two sentences in Ex. (3) is explicitly signalled in the second sentence by the phrase Probably the most egregious example is, which is sufficient to express the"
C10-2118,P09-1077,0,0.0325156,"Missing"
C10-2118,W07-2314,0,0.011281,"ntra-clausal, he does not observe that verbalized discourse relations can hold across sentences as well, where a verb and one of its arguments function similarly to a discourse adverbial, and in the end, he does not provide a proposal for how to systematically identify these alternative realizations. Le Huong et al. (2003), in developing an algorithm for recognizing discourse relations, consider non-verbal realizations (called NP cues) in addition to verbal realizations (called VP cues). However, they provide only one example of such a cue (“the result”). Like Kibble (1999), Danlos (2006) and Power (2007) also focus only on identifying verbalizations of discourse relations, although they do consider cases where such relations hold across sentences. What has not been investigated in prior work is the basis for the alternation between connectives and AltLex’s, although there are several accounts of why a language may provide more than one connective that conveys the same relation. For example, the alternation in Dutch between dus (“so”), daardoor (“as a result”), and daarom (“that’s why”) is explained by Pander Maat and Sanders (2000) as having its basis in “subjectivity”. class expressions is p"
C10-2118,prasad-etal-2008-penn,1,0.419254,"tify explicit signals of discourse relations, exemplified in Ex. (1). To refer to all such signals, we use the term “discourse relation markers” (DRMs). Past research (e.g., (Halliday and Hasan, 1976; Martin, 1992; Knott, 1996), among others) has assumed that DRMs are frozen or fixed expressions from a few welldefined syntactic classes, such as conjunctions, adverbs, and prepositional phrases. Thus the literature presents lists of DRMs, which researchers try to make as complete as possible for their chosen language. In annotating lexicalized discourse relations of the Penn Discourse Treebank (Prasad et al., 2008), this same assumption drove the initial phase of annotation. A list of “explicit connectives” was collected from various sources and provided to annotators, who then searched for these expressions in the text and annotated them, along with their arguments and senses. The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Since expressions functioning as DRMs can also have non-DRM functions, the task is framed as one of classifying given individual tokens as DRM or not DRM. In this paper, we argue that placing such syntactic and lexical restriction"
C10-2118,miltsakaki-etal-2004-penn,1,\N,Missing
C10-2118,D08-1021,0,\N,Missing
C16-2026,al-saif-markert-2010-leeds,0,0.0966919,"overview of the PDTB Framework and discusses the tool’s features, setup requirements and how it can also be used for adjudication. 1 Introduction In recent years, discourse relations have become a topic of some interest and there has in effect been a rise in the number of corpora annotated for discourse relations. Following the release of the Penn Discourse TreeBank (PDTB) in 2008 (Prasad et al., 2008), a number of comparable corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Hindi Discourse Relation Bank (Oza et al., 2009), the Leeds Arabic Discourse TreeBank (Al-Saif and Markert, 2010), the Biomedical Discourse Relation Bank (Prasad et al., 2011), the Chinese Discourse TreeBank (Zhou and Xue, 2012), the Turkish Discourse Bank (Zeyrek et al., 2013), the discourse layer of the Prague Dependency Treebank 3.0 (Bejˇcek et al, 2013) and the TED-Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2016). Groups starting new discourse annotation projects have sought an openly available resource to support their work. To address this for annotation in the PDTB framework, we have packaged an updated version of our annotation tool - the PDTB Annotator - for use by the research commun"
C16-2026,C14-2008,0,0.0367071,"Missing"
C16-2026,J05-1004,0,0.0819207,"Missing"
C16-2026,W16-1704,1,0.881334,"72) This work has been supported by the National Science Foundation under grants RI 1422186 and RI 1421067. It is licensed under a Creative Commons Attribution 4.0 International Licence. License details: http://creativecommons.org/ licenses/by/4.0/ 121 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 121–125, Osaka, Japan, December 11-17 2016. All relations are taken to have two arguments - Arg1 (shown in italics) and Arg2 (in bold). As per the revised argument-naming conventions in recent ongoing work on PDTB enrichment (Webber et al., 2016), the Arg2 in syntactically coordinated relations follows (i.e. is to the right of) Arg1, while the Arg2 in syntactically subordinated relations is (syntactically) subordinate to Arg1, regardless of textual order. Discourse relations are not always realized as Explicit connectives. In such cases, a connective is left to be inferred by the annotator, who lexically encodes this inferred relation. This is shown in (2), where a Reason relation between the two adjacent sentences is annotated with because as the Implicit connective: (2) Also unlike Mr. Ruder, Mr. Breeden appears to be in a position"
C16-2026,P12-1008,0,0.0196254,"ication. 1 Introduction In recent years, discourse relations have become a topic of some interest and there has in effect been a rise in the number of corpora annotated for discourse relations. Following the release of the Penn Discourse TreeBank (PDTB) in 2008 (Prasad et al., 2008), a number of comparable corpora have since adapted the PDTB framework (Prasad et al., 2014), including the Hindi Discourse Relation Bank (Oza et al., 2009), the Leeds Arabic Discourse TreeBank (Al-Saif and Markert, 2010), the Biomedical Discourse Relation Bank (Prasad et al., 2011), the Chinese Discourse TreeBank (Zhou and Xue, 2012), the Turkish Discourse Bank (Zeyrek et al., 2013), the discourse layer of the Prague Dependency Treebank 3.0 (Bejˇcek et al, 2013) and the TED-Multilingual Discourse Bank (TED-MDB) (Zeyrek et al., 2016). Groups starting new discourse annotation projects have sought an openly available resource to support their work. To address this for annotation in the PDTB framework, we have packaged an updated version of our annotation tool - the PDTB Annotator - for use by the research community. Some of the potential benefits of using the PDTB Annotator include the following: i) the tool is Java-based an"
C16-2026,prasad-etal-2008-penn,1,\N,Missing
C16-2026,W03-2120,0,\N,Missing
C82-1066,P81-1016,0,0.0649695,", respond to a query) are not necessarily the best ones to use in Justifying a result. What one wants rather is the ability to use the system&apos;s reasoning to suggest and instantiate conceptually more accessible strategles fnr organlz~ng and presenting justifications. Both claims will be discussed in this section. &quot; 416 B. WEBBER ~nd A. JOSH1 Many researchers have already observed that explanations have a tree-llke structure. This observation reflects a view of each supported assertion as a non-terminal node in a tree, with the sub-tree under it corresponding to the reasons given in its support [2,11]. Since a statement acting as a &quot;reason&quot; may in turn be supported by other statements/reasons, explanations have a recursive structure. While the above is true, it masks what we see as a more significant recursive organization - one that reflects the inherently recursive strategies that people use in reasoning (i.e., in supporting or denying propositions). These strategies are recurslve because they contain subtasks that call in turn for other propositions to be supported or denied. One way to accomplish this is to chose and invoke another strategy. The kinds of strategies we have in mind are"
C82-1066,C82-1066,1,0.0528322,"and Information Science University of Pennsylvania Philadelphia PA 19104 I. Introduction In answering a factual database query, one often has the option of providing more than just the answer explicitly requested. As part of our research on Natural Language interactions with databases~ we have been looking at three ways in which the system could so &quot;take the initiative&quot; in constructing a response: (i) pointing out incorrect presuppositions reflected in the user&apos;s query [4,5]; (2) offering to &quot;monitor&quot; for the requested information or additional relevant information as the system learns of it [6,7]; and (3) providing grounds for the system&apos;s response i.e., &quot;justifying why&quot;. The following responses illustrate &quot;presupposition correctlon&quot;~ &quot;monitor offers&quot; and &quot;justification&quot;, respectively. This paper describes our research on producing justifications. (&quot;U&quot; refers to the user, &quot;S&quot; to the system.) U: SI: $2: $3: Is John taking four courses? No. John can&apos;t take any courses: he&apos;s not a student. NoD three. Shall I let you know if he registers for a fourth? No, three - CIS531, CIS679 and Linguistics 650. Database systems are growing more complex in both their domain models and reasoning capabil"
C90-2068,H89-2010,1,0.866125,"Missing"
C90-2068,J88-2003,0,0.530404,"of the hill), it is not the intended termination of the action in the context of these instructions. Its intended termination is the point at which the action of &quot;taking the first right&quot; commences - that is, when the agent recognizes that s/he has reached the first right. In Section 3, we will provide many more examples of this feature of instructions. 2. I n s t r u c t i o n s m a y describe a range o f behavior appropriate u n d e r different circumstances. The agent is 2This is not the case in &quot;Simon Says&quot; type instructions, where each action description contains a n intrinsic culmination [6]. 2 Figure h Control Panel Animation o,dy meant to do that which s/he recognizes the situation as demanding during its performance. For example, the following are part of instructions for installing a diverter spout: face side; if you're using a power saw, saw from the back side. Otherwise you'll produce ragged edges on the face because a handsaw cuts down and a power saw cuts up. Diverter spout is provided with insert for 1/2&quot; pipe threads. If supply pipe is larger (3/4&quot;), unscrew insert and use spout without it. Such cases as these illustrate an indirect relation between instructions and beh"
D18-1466,P98-2176,0,0.10454,"12) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the source. Earlier work (Radev, 1998) modeled the choice of the best expression (from a lexicon) to fit the specific semantic context during text generation. Supplementing these efforts, we model the progression of entities’ status from hearer-new to hearer-old as it changes across, rather than within, documents. In the social sciences, Graus et al. (2017) analyzed distributions of entity mentions, and intervals between mentions, to identify patterns of entities becoming common knowledge, but without looking at the content of the REs. The coinage and subsequent acceptance/extinction of lexical innovations is another domain that m"
D18-1466,P14-5010,0,0.00434734,"Missing"
D18-1466,J93-2004,0,0.0612252,"Business Machines Corporation, which is the secondbiggest advertiser on the Internet International Business Machines Corporation, the worlds largest computer company International Business Machines Corporation, based in Armonk, N.Y. Western Resources Inc., worth $1.7 billion The National Basketball Association in New York International Business Machines International Business Machines Corporation International Business Machines Corporation Western Resources Inc. National Basketball Association Table 1: Regular expressions (regex) for finding RE spans within an NP. The regex use Penn Treebank (Marcus et al., 1993) tags. The full phrase is the RE, the string of NNPs ‘First Lieut. Kelly Flinn’ that comprise the embedded NP is the base expression, and the remaining NP ‘the pilot’ is the descriptor. Certain adjustments had to be made. For example, NPs of the form ‘NNP of NNP’ (e.g. “University of Virginia”) are treated as base expressions. In addition, some connectives and symbols were included in the base to accommodate names such as “Food and Drug Administration”. These adjustments lead to their own errors. For instance, for “Dan Zegart of Titusville”, our exception rule will mark the full expression as"
D18-1466,P12-1084,0,0.0210443,"and discourse-old respectively), or either newly introduce the entity to an audience (hearer-new) or be part of common knowledge (hearer-old). According to Prince (1992), hearer-old entities are more often mentioned with definite expressions, since the hearer can pick out the unique referent based on background knowledge. Corpus studies (Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the source. Earlier work (Rad"
D18-1466,N03-2024,0,0.155809,"company does, whereas now such elaboration is needed rarely, if at all. This paper presents a first computational study that relates the form of an entity’s referring expressions (RE) in articles written at different times to the entity’s changing information status.2 Previous work has focused on predicting how REs for an entity vary within a single text. This type of information status can improve coreference resolution (Recasens et al., 2013) and 1 article 1386221 from Sandhaus (2008) 2 Corpus available at http://groups.inf.ed.ac. uk/cup/ref/ help generate references in automatic summaries (Nenkova and McKeown, 2003). But there has been little exploration of the change in REs to an entity over time and across articles, as the entity is accepted into common knowledge. The current work is driven by linguistic interest in characterizing REs over time. In addition, knowing the current acceptance of an entity can help in generating time-appropriate expressions. From a social science perspective, there is also great interest in capturing the birth, acceptance into common parlance, but also possible death, and subsequent reintroductions of entities. In this paper, we disambiguate and track thousands of person (P"
D18-1466,W06-1612,0,0.0489405,"tion for Computational Linguistics and discourse-old respectively), or either newly introduce the entity to an audience (hearer-new) or be part of common knowledge (hearer-old). According to Prince (1992), hearer-old entities are more often mentioned with definite expressions, since the hearer can pick out the unique referent based on background knowledge. Corpus studies (Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references i"
D18-1466,D11-1099,0,0.0277042,"tational Linguistics and discourse-old respectively), or either newly introduce the entity to an audience (hearer-new) or be part of common knowledge (hearer-old). According to Prince (1992), hearer-old entities are more often mentioned with definite expressions, since the hearer can pick out the unique referent based on background knowledge. Corpus studies (Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the so"
D18-1466,N13-1071,0,0.0670792,"Missing"
D18-1466,J11-4007,0,0.0281636,"(Nenkova and McKeown, 2003; Yoshida, 2011) have corroborated similar trends that subsequent mentions to established entities within a discourse tend to be reduced or definite noun phrases. In computational work, many studies (Nissim, 2006; Rahman and Ng, 2011; Markert et al., 2012) classify entities in a text as discourse-new/old. Here, hearer-old entities are a separate mediated class (not introduced in the text but which readers infer based on common knowledge), and predicted using features such as definiteness and the entity name itself. In the context of text summarization, the system of Siddharthan et al. (2011) classifies entities in source documents as hearerold or not, based on frequency, syntax, and coreference (within the documents). The predicted status is then used in a rule-based algorithm to generate references in summaries of the source. Earlier work (Radev, 1998) modeled the choice of the best expression (from a lexicon) to fit the specific semantic context during text generation. Supplementing these efforts, we model the progression of entities’ status from hearer-new to hearer-old as it changes across, rather than within, documents. In the social sciences, Graus et al. (2017) analyzed di"
D18-1466,C18-1135,0,0.0149887,"eneration. Supplementing these efforts, we model the progression of entities’ status from hearer-new to hearer-old as it changes across, rather than within, documents. In the social sciences, Graus et al. (2017) analyzed distributions of entity mentions, and intervals between mentions, to identify patterns of entities becoming common knowledge, but without looking at the content of the REs. The coinage and subsequent acceptance/extinction of lexical innovations is another domain that models expressions over time, often by mapping properties of the speakers who use them within a community (see Tredici and Fernandez, 2018 and work reviewed therein). Our focus here is on REs specifically. In what follows, we explain our RE extraction (Section 3) and linguistic features (Section 4). Analysis of the REs and the model for information status prediction are in Sections 5 and 6. 3 Extracting REs over time We use the New York Times Annotated Corpus (NYTAC) (Sandhaus, 2008), containing the 1.8M articles published in the New York Times over the period 1987–2007 (20 years). Given the complexity in identifying potentially interesting entities and disambiguating references to them over time, we limited our scope to person"
D18-1466,P13-1043,0,0.0606745,"Missing"
D18-1466,D14-1162,0,0.0813896,"include the number of mentions within the one month bin, and the average time gap between consecutive mentions (multiplied by the log of number of mentions to compensate for frequency). We also include the entity’s current age (time since introduction). Context: We employ the topic metadata from NYTAC to capture a notion of world context of an entity’s mentions. Every article has topic tags (sports, finance, technology, politics, etc.) and also a section label (travel, economics, culture, etc.). We clustered the thousands of topic tags into 20 broad topics by using the Glove word embeddings (Pennington et al., 2014), and K-means clustering. A small set of 17 clusters were also created for the newspaper sections. The count of mentions belonging to articles in each cluster is a feature. 5 Features versus acceptance time We built a linear model (LM) to test which features are significantly predictive of time to acceptance. An example is the set of REs for an entity from a one month bin. The dependent variable is the time left until acceptance, i.e. the value (in months) from the current age of the entity (month were the mentions were taken from) until the acceptance age. For our corpus, the possible values"
D19-1462,D16-1245,0,0.0314284,"ion, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogu"
D19-1462,P16-1154,0,0.271776,"dialogue modeling. Unlike previous methods that combine detection and ranking models, our generation-based formulation is not constrained by the syntactic forms of ellipsis or co-reference in sentences. They can be either words (e.g., noun, verb) or phrases or even clauses. Furthermore, the formulation does not need to provide a set of candidate antecedents to be resolved. Previous studies usually need to Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence."
D19-1462,K15-1002,0,0.0260799,"end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialog"
D19-1462,D14-1162,0,0.0819929,"us studies usually need to Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence. Utterance and Context Encoder We use a single-layer bidirectional GRU to construct both encoders. The forward and backward hidden states over the input embeddings from the embedding layer are concatenated to form the hidden states of the two encoders. Decoder The decoder is a single-layer unidirectional GRU. In the decoder, the attention distribution at is calculated as in Bahdanau et al"
D19-1462,C16-1190,0,0.113933,"ement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogue or QA tasks. Kumar and Joshi (2016) train a semantic sequence model to learn semantic patterns and a syntactic sequence model to learn linguistic patterns to tackle with the non-sentential (incomplete) questions in a question answering system. Zheng et al. (2018) builds a seq2seq neural network model for short texts to identify and recover ellipsis. However, these methods are still limited to short texts or one-shot dialogues. Our work is the first attempt to provide both solution and dataset for ellipsis and co-reference resolution in multi-turn dialogues. End-to-end task-oriented dialogue: Taskoriented dialogue systems have e"
D19-1462,D17-1018,0,0.0425292,"ference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogue or QA tasks. Kumar and Joshi (2016) train a semantic sequence model to learn semantic pat"
D19-1462,N18-2108,0,0.0758611,"Missing"
D19-1462,P18-1133,0,0.082913,"framework, we manually construct a new dataset on the basis of the public dataset CamRest676 with both ellipsis and co-reference annotation. On this dataset, intrinsic evaluations on the resolution of ellipsis and co-reference show that the GECOR model significantly outperforms the sequenceto-sequence (seq2seq) baseline model in terms of EM, BLEU and F1 while extrinsic evaluations on the downstream dialogue task demonstrate that our multi-task learning framework with GECOR achieves a higher success rate of task completion than TSCP, a state-of-the-art end-to-end task-oriented dialogue model (Lei et al., 2018). 1 Introduction Due to the rhetorical principle of saving words and avoiding repetitions, ellipsis and co-reference occur frequently in multi-turn dialogues leaving utterances paragmatically incomplete if they are separate from context. Humans can easily understand utterances with anaphorically referenced or absent ∗ Work performed during an internship at Lenovo Research AI Lab. † Corresponding author information (e.g., Q2 and Q3 in Table 1) based on the dialogue context while dialogue systems often fail to understand such utterances correctly, which may result in false or incoherent response"
D19-1462,D16-1127,0,0.0365081,"dialogue system. The overall results demonstrate that the proposed multi-task learning framework for the end-to-end dialogue is able to improve the task completion rate by incorporating an auxiliary ellipsis/co-reference resolution task. Since the BSpan decoder is also used in the baseline system to capture contextual information and track dialogue states, we believe that our multi-task learning model with the integrated GECOR will play a more important role in endto-end dialgoue models that do not use state tracking modules, e.g., neural open-domain conversation models (Vinyals and Le, 2015; Li et al., 2016). improve the performance in terms of the exact match rate, BLEU and word-level F1 score. Experiments on the dialogue task demonstrate that the task completion rate of the task-oriented dialogue system is significantly improved with the aid of ellipsis and co-reference resolution. Our work could be extended to end-to-end open-domain multi-turn dialogue. We will further improve our model by incorporating syntactic and location information. We would also like to adapt the proposed methods to document-level neural machine translation in the future. 7 Mihail Eric and Christopher D Manning. 2017a."
D19-1462,W16-0705,0,0.0142549,"ar as we know is the PUNDIT system (Palmer et al., 1986) which discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. Dalrymple et al. (1991) and Shieber et al. (1996) establish 1 The new dataset and the code of our proposed system are available at https://multinlp.github.io/ GECOR/ a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In r"
D19-1462,P86-1004,0,0.582024,"ence resolution network with two phases of detection and candidate ranking. • To the best of our knowledge, this is the first attempt to combine the task of ellipsis and coreference resolution with the multi-turn taskoriented dialogue. The success rate of task completion is significantly improved with the assistance of the ellipsis and co-reference resolution. • We construct a new dataset based on CamRest676 for ellipsis and co-reference resolution in the context of task-oriented dialogue.1 2 Related Work Ellipsis recovery: The earliest work on ellipsis as far as we know is the PUNDIT system (Palmer et al., 1986) which discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. Dalrymple et al. (1991) and Shieber et al. (1996) establish 1 The new dataset and the code of our proposed system are available at https://multinlp.github.io/ GECOR/ a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of"
D19-1462,P02-1040,0,0.104733,"Missing"
D19-1462,P17-1099,0,0.026974,"detection and ranking models, our generation-based formulation is not constrained by the syntactic forms of ellipsis or co-reference in sentences. They can be either words (e.g., noun, verb) or phrases or even clauses. Furthermore, the formulation does not need to provide a set of candidate antecedents to be resolved. Previous studies usually need to Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence. Utterance and Context Encoder We use a single-layer bidir"
D19-1462,I13-1012,0,0.0309292,"/multinlp.github.io/ GECOR/ a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking co-reference resolution. Lee et al. (2017) introduce the first endto-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine"
D19-1462,D16-1233,0,0.116042,"Missing"
E14-1017,C00-1072,0,0.0862228,"ake advantage of likely words in the domain and consistency, but which also adapt to topic shifts. 3.1 A general domain model This system seeks to bias translations towards words which occur often in biography articles. The topic cache is filled with word unigrams that are more likely to occur in biographies com156 pared to general news documents. We compare the words from 1,475 English Wikipedia biographies articles to those in a large collection (64,875 articles) of New York Times (NYT) news articles (taken from the NYT Annotated Corpus (Sandhaus, 2008)). We use a log-likelihood ratio test (Lin and Hovy, 2000) to identify words which occur with significantly higher probability in biographies compared to NYT. We collect only words indicated with 0.0001 significance by the test to be more likely in biographies. We rank this set of 18,597 words in decreasing order of frequency in the biography article set and assign to each word a score equal to 1/rank of the word. These words with their associated scores form the contents of the topic cache. In the general domain model, these same words are assumed to be useful for the full document and so the cache contents remain constant during translation of the"
E14-1017,P03-1021,0,0.0397194,"kipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights could be done jointly. We present the findings from our grid search below. The struct-topic cache has two parameters, the number of topics T and the number of most probable words from each topic which get loaded into the cache. We ran the tuning for T = 25, 50, 100 and 200 topi"
E14-1017,P02-1040,0,0.0904727,"Missing"
E14-1017,P12-1048,0,0.0198003,"m that translates Wikipedia biographies from French to English by adapting a system 1 Corpus available at http://homepages.inf.ed. ac.uk/alouis/wikiBio.html. 155 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155–163, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics A different line of work very relevant to our study is the creation of topic-specific translations by either inferring a topic for the source document as a whole, or at the other extreme, finer topics for individual sentences (Su et al., 2012; Eidelman et al., 2012). Neither of these granularities seem intuitive in natural discourse. In this work, we propose that tailoring translations to topics associated with discourse segments in the article is likely to be beneficial for two reasons: a) subtopics of such granularity can be assumed with reasonable confidence to re-occur in documents from the same domain and b) we can hypothesize that a domain will have a small number of segment-level topics. mann (2010a). In such methods, cache(s) can be filled with relevant items for translation and translation hypotheses that match a greater"
E14-1017,P12-2023,0,0.0213013,"Wikipedia biographies from French to English by adapting a system 1 Corpus available at http://homepages.inf.ed. ac.uk/alouis/wikiBio.html. 155 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 155–163, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics A different line of work very relevant to our study is the creation of topic-specific translations by either inferring a topic for the source document as a whole, or at the other extreme, finer topics for individual sentences (Su et al., 2012; Eidelman et al., 2012). Neither of these granularities seem intuitive in natural discourse. In this work, we propose that tailoring translations to topics associated with discourse segments in the article is likely to be beneficial for two reasons: a) subtopics of such granularity can be assumed with reasonable confidence to re-occur in documents from the same domain and b) we can hypothesize that a domain will have a small number of segment-level topics. mann (2010a). In such methods, cache(s) can be filled with relevant items for translation and translation hypotheses that match a greater number of cache items ar"
E14-1017,W10-2602,0,0.0596777,"rom the 1-best translations of previous sentences in the same document. Each word is associated with an age value and a score. Age indicates when a word entered the cache and introduces a ‘decay effect’. Words used in immediately previous sentences have a low age value while higher age values indicate words from sentences much prior in the document. Scores are inversely proportional to age. Both the types of caches are present in both the general domain and structured models, but the cache words and scores are computed differently. Related work The study that is closest to our work is that of Tiedemann (2010a), which proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (20"
E14-1017,D08-1035,0,0.030374,"verage BLEU score gains from a structured cache (compared to domain caches) split by different properties of documents in the test set document structure to match that handled better by the structured model. We test this hypothesis by segmenting all test documents with an ideal segment size. The model seems to perform better when each segment has around 5 to 10 sentences (longer segments are also preferred but we have few very long documents in our corpus), so we try to re-segment the articles to contain approximately 7 sentences in each segment. We use an automatic topic segmentation method (Eisenstein and Barzilay, 2008) to segment the source articles in our test corpus. For each article we request (document length)/7 segments to be created.5 We then run the structured topic and consistency models on the automatically segmented corpus using the same feature weights as before. The results are shown in Table 6. Model Struct-topic Struct-consistency BLEU (doc) 17.94 17.51 BLEU (sent) 17.94 17.46 Table 6: Translation performance on automatically segmented test corpus The struct-topic cache now reaches our best result of 0.5 BLEU improvement over the out-ofdomain model and 0.3 improvement over the unstructured dom"
E14-1017,2012.iwslt-evaluation.1,0,0.0609649,"Missing"
E14-1017,W10-1728,0,0.277657,"rom the 1-best translations of previous sentences in the same document. Each word is associated with an age value and a score. Age indicates when a word entered the cache and introduces a ‘decay effect’. Words used in immediately previous sentences have a low age value while higher age values indicate words from sentences much prior in the document. Scores are inversely proportional to age. Both the types of caches are present in both the general domain and structured models, but the cache words and scores are computed differently. Related work The study that is closest to our work is that of Tiedemann (2010a), which proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (20"
E14-1017,2010.amta-papers.24,0,0.0347941,"is harder to do on very divergent domains. In addition, biographies have a fairly regular discourse structure: a central entity (person who is the topic of the biography), recurring subtopics such as ‘childhood’, ‘schooling’, ‘career’ and ‘later life’, and a likely chronological order to these topics. These regularities become more predictable in documents from sources such as Wikipedia. This setting allows us to explore the utility of models which make translation decisions depending on the discourse structure. Translation methods for structured documents have only recently been explored in Foster et al. (2010). However, their system was developed for parlimentary proceedings and translations were adapted using separate language models based upon the identity of the speaker, text type (questions, debate, etc.) and the year when the proceedings took place. Biographies constitute a more realistic discourse context to develop structured models. This paper introduces a new corpus consisting of paired French-English translations of biography articles from Wikipedia.1 We translate this corpus by developing cache-based domain adaptation methods, a technique recently proposed by TiedeWe present a French to"
E14-1017,P06-2124,0,0.0736738,"Missing"
E14-1017,D11-1084,0,0.049085,"proposed cache models to adapt a Europarl-trained system to medical documents. The system used caching in two ways: a cache-based language model (stores target language words from translations of preceding sentences in the same document) and a cache-based translation model (stores phrase pairs from preceding sentence translations). These caches encouraged the system to imitate the ‘consistency’ aspect of domain-specific texts i.e., the property that words or phrases are likely to be repeated in a domain and within the same document. Cache models developed in later work, Tiedemann (2010b) and Gong et al. (2011), were applied for translating in-domain documents. Gong et al. (2011) introduced additional caches to store (i) words and phrase pairs from training documents most similar to a current source article, and (ii) words from topical clusters created on the training set. However, a central issue in these systems is that caches become noisy over time, since they ignore topic shifts in the documents. This paper presents cache models which not only take advantage of likely words in the domain and consistency, but which also adapt to topic shifts. 3.1 A general domain model This system seeks to bias t"
E14-1017,D12-1108,0,0.107521,"Missing"
E14-1017,N03-1017,0,0.0288526,"erage no. of segments per article Tuning 15 430 13 59 4.7 Test 30 1008 12 85 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (Tiedemann, 2010b). In future work, we will explore how successful optimization of baseline and cache feature weights could be done jointly. We present the findings from our grid search below. The struct-topic cache has two parameters, the number of topics T and the number of most probable words from"
E14-1017,P07-2045,0,0.0170959,"ain subtopics. We select only articles that have at least 10 segments (sections) to ensure 4 http://sourceforge.net/projects/ aligner/ A filtered set of 13,400 entries from www.dict.cc 158 No. of article pairs Total sentences pairs Min. article size (in sentences) Max. article size (in sentences) Average no. of segments per article Tuning 15 430 13 59 4.7 Test 30 1008 12 85 5.3 Table 1: Summary of Wikipedia biographies data that they are comprehensive ones. This collection contains 1000 French and 1000 English articles. 5 Experimental settings We use the Moses phrase-based translation system (Koehn et al., 2007) to implement our models. Figure 1: Effect of feature weights and number of topics on accuracy for structured topic cache 5.1 Out-of-domain model This baseline model is trained on the WMT 2012 training sets described in the previous section and uses the six standard features from Koehn et al. (2003). We build a 5-gram language model using SRILM. The features were tuned using MERT (Och, 2003) on the WMT 2012 tuning sets. This system does not use any data about biographies. 5.2 weights on the new features directly. Previous work has noted that MERT fails to find good settings for cache models (T"
E14-1017,federico-etal-2012-iwslt,0,\N,Missing
E14-1063,P05-1066,0,0.823018,"in 1911, where the focus is the temporal argument in 1911, one can see that the system should not lose track of the focus of negation when producing the hypothesis translation. Although negation must be appropriately rendered to ensure correct representation of the semantics of the source sentence in the machine output, only some of the efforts to improve the translation of negation-bearing sentences in SMT address the problem. Wetzel and Bond (2012) considered negation as a problem of data sparsity and so attempted to enrich the training data with negative paraphrases of positive sentences. Collins et al. (2005) and Li et al. (2009) both addressed differences in the placement of negation in source and target texts, by re-ordering negative elements in the source sentence to better resemble their position in the corresponding target text. Although these approaches show improvement over the baseline, neither considers negation as a linguistic phenomenon with specific characteristics. This we do in the work presented here: We identify the elements of negation that an MT system has to reproduce and then devise a strategy to ensure that they are output correctly. These elements we take to be the cue, event"
E14-1063,W12-4201,0,0.0230745,"to deal with sentence-level phenomena whose locality goes beyond n-grams or single hierarchical rules. It involves re-ranking the list of target-language hypotheses produced by decoding, using additional features extracted from the source sentence. In the case of negation, N-best list re-ranking allows us to assess whether a system is able to correctly translate the elements of negation, while failing to place the best hypothesis on these grounds at the top of the n-best list. The current work follows the same approach as other n-best list re-rankers (Och et al. (2004); Specia et al. (2008); Apidianaki et al. (2012)) but using conj  and obj  Mary  did  not det amod buy  a  blue  car  . nsubj(buy-6, and-2) , conj(and-2, Peter-1), conj (and-2, Mary-3), aux(buy-6, did-4), neg(buy-6, not-5), root(ROOT-0, buy-6), det(car-9, a-7), amod(car-9, blue-8) , dobj(buy-6, car-9) The ‘neg’ dependency relation conveys both the negation cue (not-5) and the negation event (buy-6) of the sentence ‘Peter and Mary did not buy a blue car’. An approximate scope can be recovered by following the path from the event (included) to the terminal nodes and collecting all the lexical elements along the way. Also in the case o"
E14-1063,N07-2015,0,0.0160176,"s a basis for automatically detecting scope and focus of negation using simple 3 Decomposing negation Correctly translating negation involves more than placing a negative marker in the right position. We follow Blanco and Moldovan (2011) in decomposing negation into three main components: 599 • a negation cue, including negative markers, affixes and all the words or multiwords units that inherently express negation. negation as the additional feature. Negation is here defined as the degree of overlap of cue, event and scope between the hypothesis translation and the source sentence. Following Hasan et al. (2007), we use an n-best list of 10000 sentences but we do not initially tune the negation feature using MERT or interpolate it with other features. This is because in order to assess the degree of overlap between the scope in the source and the hypothesis sentence, a n-gram based score is used which conveys the same information as that of the language model score in the log-linear model. Moreover, our re-ranking exploits lexical translation probabilities, thereby resembling a simple translation model. • a negation event, i.e. the event that is directly negated. Events can be either verbs (e.g. ‘I d"
E14-1063,S12-1042,0,0.046606,"e cases where, although not fully-capturing the scope, we want to translate correctly the part that is directly negated or emphasised. 598 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 598–606, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics heuristics. Machine-learning has been used by systems participating in the *SEM 2012 shared task on automatically detecting the scope and focus of negation. Those systems with the best F1 measures (Chowdhury and Mahbub (2012), Read et al. (2012) and Lapponi et al. (2012) all use a mixture of SVM (Support Vector Machines) and CRF. Their performance improves significantly when syntactic features are also considered. In particular, Lapponi et al. (2012) use features extracted from a dependency parse to guide their system to detect the correct scope boundary. In translation, only few efforts have focussed on the problem of translating negation. Wetzel and Bond (2012) treat it as resulting from data sparsity. To remedy this, they enrich their Japaneseto-English training set with negative paraphrases of positive sentences, where negation is inserted as a ‘handle’ t"
E14-1063,S12-1045,0,0.132674,"tant to correctly reproduce the focus; there might be cases where, although not fully-capturing the scope, we want to translate correctly the part that is directly negated or emphasised. 598 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 598–606, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics heuristics. Machine-learning has been used by systems participating in the *SEM 2012 shared task on automatically detecting the scope and focus of negation. Those systems with the best F1 measures (Chowdhury and Mahbub (2012), Read et al. (2012) and Lapponi et al. (2012) all use a mixture of SVM (Support Vector Machines) and CRF. Their performance improves significantly when syntactic features are also considered. In particular, Lapponi et al. (2012) use features extracted from a dependency parse to guide their system to detect the correct scope boundary. In translation, only few efforts have focussed on the problem of translating negation. Wetzel and Bond (2012) treat it as resulting from data sparsity. To remedy this, they enrich their Japaneseto-English training set with negative paraphrases of positive sentenc"
E14-1063,W09-0433,0,0.779197,"is the temporal argument in 1911, one can see that the system should not lose track of the focus of negation when producing the hypothesis translation. Although negation must be appropriately rendered to ensure correct representation of the semantics of the source sentence in the machine output, only some of the efforts to improve the translation of negation-bearing sentences in SMT address the problem. Wetzel and Bond (2012) considered negation as a problem of data sparsity and so attempted to enrich the training data with negative paraphrases of positive sentences. Collins et al. (2005) and Li et al. (2009) both addressed differences in the placement of negation in source and target texts, by re-ordering negative elements in the source sentence to better resemble their position in the corresponding target text. Although these approaches show improvement over the baseline, neither considers negation as a linguistic phenomenon with specific characteristics. This we do in the work presented here: We identify the elements of negation that an MT system has to reproduce and then devise a strategy to ensure that they are output correctly. These elements we take to be the cue, event and scope of negatio"
E14-1063,P12-1083,0,0.166298,"e. We carry out our exploration of N-best list re-ranking in two steps: • First, an oracle translation is computed both to assess the validity of the approach and to understand the maximal extent to which it could possibly enhance performance. An oracle translation is obtained by performing nbest list re-ranking using reference translations as a gold-standard. To avoid the problem in Chinese-English Hierarchical Phrase-Based (HPB) translation of loss and/or misplacement of negation-related elements when hierarchical phrases are built, Chinese source sentences are first broken into sub-clauses Yang and Xue (2012), then translated and finally ”stitched” back together for evaluation. • Standard n-best list re-ranking is then performed using only source-side information. Hypotheses are re-ranked according to the degree of similarity between the negationrelated elements in the hypotheses and those in the source sentence. Here the correspondence between source and target text is established through lexical translation probabilities output after training. Results of this method show that n-best list reranking does lead to a significant improvement in BLEU score. However, BLEU says nothing about semantics, s"
E14-1063,N04-1021,0,0.0522726,"ter N-best list re-ranking is used in SMT to deal with sentence-level phenomena whose locality goes beyond n-grams or single hierarchical rules. It involves re-ranking the list of target-language hypotheses produced by decoding, using additional features extracted from the source sentence. In the case of negation, N-best list re-ranking allows us to assess whether a system is able to correctly translate the elements of negation, while failing to place the best hypothesis on these grounds at the top of the n-best list. The current work follows the same approach as other n-best list re-rankers (Och et al. (2004); Specia et al. (2008); Apidianaki et al. (2012)) but using conj  and obj  Mary  did  not det amod buy  a  blue  car  . nsubj(buy-6, and-2) , conj(and-2, Peter-1), conj (and-2, Mary-3), aux(buy-6, did-4), neg(buy-6, not-5), root(ROOT-0, buy-6), det(car-9, a-7), amod(car-9, blue-8) , dobj(buy-6, car-9) The ‘neg’ dependency relation conveys both the negation cue (not-5) and the negation event (buy-6) of the sentence ‘Peter and Mary did not buy a blue car’. An approximate scope can be recovered by following the path from the event (included) to the terminal nodes and collecting all the le"
E14-1063,W09-0404,0,0.422977,"This work starts instead from the questions of what is meant by negation and what makes a good translation of negation. These questions have led us to explore the use of semantics of negation in SMT — specifically, identifying core semantic elements of negation (cue, event and scope) in a source-side dependency parse and reranking hypotheses on the n-best list produced after decoding according to the extent to which an hypothesis realises these elements. The method shows considerable improvement over the baseline as measured by BLEU scores and Stanford’s entailmentbased MT evaluation metric (Padó et al. (2009)). 1 Introduction Translating negation is a task that involves more than the correct rendering of a negation marker in the target sentence. For instance, translating Italy did not defeat France in 1909 differs from translating Italy defeated France in 1909, or France did not defeat Italy in 1909, or Italy did not conquer France in 1909. These examples show that translating negation also involves placing in the right position the semantic arguments as well as the event directly negated. Moreover, if the source 1 Due to its ambiguity and the fact that it is already included in the scope, we have"
E14-1063,S12-1041,0,0.0599773,"he focus; there might be cases where, although not fully-capturing the scope, we want to translate correctly the part that is directly negated or emphasised. 598 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 598–606, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics heuristics. Machine-learning has been used by systems participating in the *SEM 2012 shared task on automatically detecting the scope and focus of negation. Those systems with the best F1 measures (Chowdhury and Mahbub (2012), Read et al. (2012) and Lapponi et al. (2012) all use a mixture of SVM (Support Vector Machines) and CRF. Their performance improves significantly when syntactic features are also considered. In particular, Lapponi et al. (2012) use features extracted from a dependency parse to guide their system to detect the correct scope boundary. In translation, only few efforts have focussed on the problem of translating negation. Wetzel and Bond (2012) treat it as resulting from data sparsity. To remedy this, they enrich their Japaneseto-English training set with negative paraphrases of positive sentences, where negation i"
E14-1063,I05-3027,0,0.0391155,"Missing"
E14-1063,W12-4203,0,0.102543,"sity of Edinburgh Edinburgh, UK, EH8 9AB ffancellu@cngl.ie bonnie@inf.ed.ac.uk Abstract sentence was uttered in response to the statement I think Italy defeated France in 1911, where the focus is the temporal argument in 1911, one can see that the system should not lose track of the focus of negation when producing the hypothesis translation. Although negation must be appropriately rendered to ensure correct representation of the semantics of the source sentence in the machine output, only some of the efforts to improve the translation of negation-bearing sentences in SMT address the problem. Wetzel and Bond (2012) considered negation as a problem of data sparsity and so attempted to enrich the training data with negative paraphrases of positive sentences. Collins et al. (2005) and Li et al. (2009) both addressed differences in the placement of negation in source and target texts, by re-ordering negative elements in the source sentence to better resemble their position in the corresponding target text. Although these approaches show improvement over the baseline, neither considers negation as a linguistic phenomenon with specific characteristics. This we do in the work presented here: We identify the el"
E17-2010,P16-1047,1,0.557328,"m, detection accuracy is low and undersampling the easy training examples does not substantially improve accuracy. We demonstrate that this is partly an artifact of annotation guidelines, and we argue that future negation scope annotation efforts should focus on these more difficult cases. 1 But S HERLOCK is only one of several corpora annotated for negation scope, each the result of different annotation decisions and targeted to specific applications or domains. Does the same approach work equally well across all corpora? In answer to this question, we offer two contributions. 1. We evaluate Fancellu et al. (2016)’s model on all other available negation scope corpora in English and Chinese. Although we confirm that it is state-of-the-art, we show that it can be improved by making joint predictions for all words, incorporating an insight from Morante et al. (2008) that classifiers tend to leave gaps in what should otherwise be a continuous prediction. We accomplish this with a sequence model over the predictions. Introduction 2. We show that in all corpora except S HER negation scope is most often delimited by punctuation. That is, in these corpora, examples like (2) outnumber those like (1). LOCK , Tex"
E17-2010,konstantinova-etal-2012-review,0,0.425617,"Missing"
E17-2010,morante-daelemans-2012-conandoyle,0,0.560846,"confirm that it is state-of-the-art, we show that it can be improved by making joint predictions for all words, incorporating an insight from Morante et al. (2008) that classifiers tend to leave gaps in what should otherwise be a continuous prediction. We accomplish this with a sequence model over the predictions. Introduction 2. We show that in all corpora except S HER negation scope is most often delimited by punctuation. That is, in these corpora, examples like (2) outnumber those like (1). LOCK , Textual negation scope is the largest span affected by a negation cue in a negative sentence (Morante and Daelemans, 2012).1 For example, given the marker not in (1), its scope is use the 56k conextant modem.2 (2) It helps activation , [not inhibition of ibrf1 cells] . (1) I do not [use the 56k conextant modem] since I have cable access for the internet Our experiments demonstrate that negation scope detection is very accurate for sentences like (2) and poor for others, suggesting that most classifiers simply overfit to this feature of the data. When we attempt to mitigate this effect by undersampling examples like (2) in training, our system does not improve on examples like (1) in test, suggesting that more tra"
E17-2010,D08-1075,0,0.897089,"ocus on these more difficult cases. 1 But S HERLOCK is only one of several corpora annotated for negation scope, each the result of different annotation decisions and targeted to specific applications or domains. Does the same approach work equally well across all corpora? In answer to this question, we offer two contributions. 1. We evaluate Fancellu et al. (2016)’s model on all other available negation scope corpora in English and Chinese. Although we confirm that it is state-of-the-art, we show that it can be improved by making joint predictions for all words, incorporating an insight from Morante et al. (2008) that classifiers tend to leave gaps in what should otherwise be a continuous prediction. We accomplish this with a sequence model over the predictions. Introduction 2. We show that in all corpora except S HER negation scope is most often delimited by punctuation. That is, in these corpora, examples like (2) outnumber those like (1). LOCK , Textual negation scope is the largest span affected by a negation cue in a negative sentence (Morante and Daelemans, 2012).1 For example, given the marker not in (1), its scope is use the 56k conextant modem.2 (2) It helps activation , [not inhibition of ib"
E17-2010,D16-1078,0,0.442549,"ition grounds the phenomenon at word level. 2 For all examples in this paper, negation cues are in bold, human-annotated negation scope is in square brackets [ ], and automatically predicted negation scope is underlined. 58 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 58–63, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics cus on these cases.3 2 wider range of complex phenomena including ellipsis, long-range dependencies and affixal negation. Though widely used (e.g. Qian et al. (2016)), the SFU, BioScope and CNeSp corpora contain simplifications that are sometimes hard to justify linguistically. In SFU and BioScope, for instance, scope is usually annotated only to the right of the cue, as in (1). The only exception is passive constructions, where the subject to the left is also annotated: Models We use the bi-directional LSTM of Fancellu et al. (2016). The input to the network is a negative sentence w = w1 ...w|w |containing a negation cue. If there is more than one cue, we consider each cue and its corresponding scope as a separate classification instance. Given a represe"
E17-2010,J12-2005,0,0.110589,"Missing"
E17-2010,W08-0606,0,0.73622,"Missing"
E17-2010,P15-1064,0,0.0253879,"Missing"
E17-2010,D13-1099,0,0.0341495,"Missing"
E17-4004,J15-3002,0,0.0199867,"tic (Section 4.3) and semantic features (Section 4.4), and then the extent to which information internal to the sister VPs suffices to determine how they relate to one another, or whether features external to the pair are also needed (Section 4.5). We also assess the extent to which performance drops when argument spans are provided by an ’off-the-shelf’ parser rather than manual annotation (Section 5). Introduction Discourse relations can hold between intersentential and intra-sentential arguments. As Language Technology has much to gain from recognizing intra-sentential discourse relations (Joty et al., 2015), the Penn Discourse TreeBank project has annotated the discourse senses of conjoined verb phrases in the Wall Street Journal corpus (Webber et al., 2016). Broadly construed, conjoined VPs are sisters in a parse tree, separated from each other by a conjunction and/or punctuation, and possibly one or more adverbs or adverbial phrases as well. As with other units of discourse, more than one sense relation can hold between conjoined VPs. An explicit conjunction may itself convey multiple senses, or additional senses may arise through inference or be signaled with other lexico-syntactic cues (Webb"
E17-4004,W10-4327,0,0.0253717,"d Nenkova (2009) used a small collection of syntactic features to do single-label sense classification from a set of four high-level sense types. Rutherford and Xue (2014) mention that Brown Clusters are helpful to classify implicit relations. For the machine learning algorithms, Meyer et al. (2015) claim that a Maximum Entropy classifier is suitable for sense classification as it learns feature combinations. Hernault et al. (2010) propose the use of a SVM for its suitability for a larger feature-space. corporated into other NLP systems, such as Machine Translation or Automatic Summarization. Louis et al. (2010), for example, showed the benefit of discourse features as importance indicators for automatic summarization, Meyer et al. (2015) used sense labeled discourse connectives in an improved phrase based machine translation system and Prasad and Joshi (2008) generated questions using properties and arguments of specific discourse relations. 2 Background The sense annotation of discourse relations is part of shallow discourse parsing, involving the identification of pairs of discourse arguments (Arg1 and Arg2) and the sense(s) i which they are related. (2) 3 Corpus The Penn Discourse TreeBank has be"
E17-4004,P09-2004,0,0.0625306,"Missing"
E17-4004,J14-4007,1,0.935237,"rse TreeBank project has annotated the discourse senses of conjoined verb phrases in the Wall Street Journal corpus (Webber et al., 2016). Broadly construed, conjoined VPs are sisters in a parse tree, separated from each other by a conjunction and/or punctuation, and possibly one or more adverbs or adverbial phrases as well. As with other units of discourse, more than one sense relation can hold between conjoined VPs. An explicit conjunction may itself convey multiple senses, or additional senses may arise through inference or be signaled with other lexico-syntactic cues (Webber et al., 2016; Prasad et al., 2014). With no explicit conjunction, sense relations will arise through inference or are signaled with other The novel contribution of this work is its use of multi-label classification in determining the discourse sense(s) that hold between conjoined VPs. This type of sense classification on conjoined VPs has not been done before to our knowledge. The evaluation of the features and the feature scope could provide a useful starting-point for future systems that classify inter-sentential discourse relations. Such a classifier could be in33 Proceedings of the Student Research Workshop at the 15th Con"
E17-4004,E14-1068,0,0.105927,"at classify inter-sentential discourse relations. Such a classifier could be in33 Proceedings of the Student Research Workshop at the 15th Conference of the European Chapter of the Association for Computational Linguistics, c pages 33–42, Valencia, Spain, April 3-7 2017. 2017 Association for Computational Linguistics 2016)). In terms of features, Subba and Di Eugenio (2009) mention VerbNet as a resource to generalize the semantics of verbs. Pitler and Nenkova (2009) used a small collection of syntactic features to do single-label sense classification from a set of four high-level sense types. Rutherford and Xue (2014) mention that Brown Clusters are helpful to classify implicit relations. For the machine learning algorithms, Meyer et al. (2015) claim that a Maximum Entropy classifier is suitable for sense classification as it learns feature combinations. Hernault et al. (2010) propose the use of a SVM for its suitability for a larger feature-space. corporated into other NLP systems, such as Machine Translation or Automatic Summarization. Louis et al. (2010), for example, showed the benefit of discourse features as importance indicators for automatic summarization, Meyer et al. (2015) used sense labeled dis"
E17-4004,N09-1064,0,0.0716731,"Missing"
E17-4004,P10-1040,0,0.00949523,"the semantic combination of the two arguments might be suitable. For this purpose the Cartesian product between the corresponding representation of the words in Arg1 and in Arg2 is constructed. VerbNet (Schuler, 2005) features are implemented as the Cartesian product of the verbs in the VPs and also as a tfIdf weighted bag-of-words representation. Since we are working with VP conjunctions the role of the verbs is assumed to be important for the sense of the relation. BrownCluster classes represent words as semantic clusters, through a hierarchical clustering approach using mutual information (Turian et al., 2010). For the BC features the Brown Clusters from the CoNNL-2016 Shared Task2 , containing 100 clusters, are used. Previous research on discourse relations showed that Brown Clusters are especially useful for the classification of implicit relations (Rutherford and Xue, 2014). 4.4.2 Results The three semantic feature-types, BrownCluster, VerbNet and WordNet, are evaluated in combination with the connectives/discourse adverbials features. Table 5 shows that the ’Implicit’ classifier profits the most from the semantic features. This indicates that the semantic information contained in a connective,"
E17-4004,K15-2002,0,0.0189072,"the conjoined VP sub-corpus of the PDTB 3.0 (Webber et al., 2016), the left argument is labeled Arg1 and the right argument, Arg2. The goal of shallow discourse parsing is thus to automatically identify the arguments, their spans, the connective (for an explicit relation), and the sense(s) in which they are related. It is called ’shallow’ because it does not recursively construct a discourse ’parse tree’ (Stede, 2011). The first end-to-end shallow discourse parsers carried out subtasks in a pipeline, separating the tasks of parsing explicit and implicit discourse relations (Lin et al., 2014; Wang and Lan, 2015). Shallow discourse parsing of conjoined VPs differs from this model of discourse parsing in that the arguments must be sister VPs in a parse tree. Thus, syntactic parsing (either phrase-structure or dependency) must precede identification of sister VPs, whether there is an explicit connective between them or not. This makes shallow discourse parsing more dependent on parser accuracy than in the past. As we will show in Section 5, parsers often fail to accurately parse conjoined VPs (or conjoined structures in general, (Ficler and Goldberg, 1. Two Explicit senses: One sense is associated with"
E17-4004,W16-1704,1,0.861104,"y relate to one another, or whether features external to the pair are also needed (Section 4.5). We also assess the extent to which performance drops when argument spans are provided by an ’off-the-shelf’ parser rather than manual annotation (Section 5). Introduction Discourse relations can hold between intersentential and intra-sentential arguments. As Language Technology has much to gain from recognizing intra-sentential discourse relations (Joty et al., 2015), the Penn Discourse TreeBank project has annotated the discourse senses of conjoined verb phrases in the Wall Street Journal corpus (Webber et al., 2016). Broadly construed, conjoined VPs are sisters in a parse tree, separated from each other by a conjunction and/or punctuation, and possibly one or more adverbs or adverbial phrases as well. As with other units of discourse, more than one sense relation can hold between conjoined VPs. An explicit conjunction may itself convey multiple senses, or additional senses may arise through inference or be signaled with other lexico-syntactic cues (Webber et al., 2016; Prasad et al., 2014). With no explicit conjunction, sense relations will arise through inference or are signaled with other The novel con"
E17-4004,H92-1116,0,\N,Missing
E17-4004,P16-2012,0,\N,Missing
guillou-etal-2014-parcor,broscheit-etal-2010-extending,0,\N,Missing
guillou-etal-2014-parcor,de-marneffe-etal-2006-generating,0,\N,Missing
guillou-etal-2014-parcor,W11-1211,0,\N,Missing
guillou-etal-2014-parcor,J93-2004,0,\N,Missing
guillou-etal-2014-parcor,popescu-belis-etal-2012-discourse,0,\N,Missing
guillou-etal-2014-parcor,W11-1901,0,\N,Missing
guillou-etal-2014-parcor,D13-1037,1,\N,Missing
guillou-etal-2014-parcor,W11-1902,0,\N,Missing
guillou-etal-2014-parcor,W08-0325,0,\N,Missing
guillou-etal-2014-parcor,P06-1055,0,\N,Missing
guillou-etal-2014-parcor,E12-3001,1,\N,Missing
guillou-etal-2014-parcor,W10-1737,0,\N,Missing
guillou-etal-2014-parcor,2010.iwslt-papers.10,1,\N,Missing
guillou-etal-2014-parcor,prasad-etal-2008-penn,1,\N,Missing
guillou-etal-2014-parcor,hajic-etal-2012-announcing,0,\N,Missing
guillou-etal-2014-parcor,postolache-etal-2006-transferring,0,\N,Missing
guillou-etal-2014-parcor,M98-1029,0,\N,Missing
guillou-etal-2014-parcor,W13-3306,0,\N,Missing
guillou-etal-2014-parcor,2012.eamt-1.60,0,\N,Missing
H86-1017,P84-1030,1,0.89698,"e should be ""'You can't drop 577; Pi isn~ true."" Alternatively, the language generator might paraphrase the whole response as, ""if Pi were true, you could drop."" Of course there are potentially many ways to try to achieve a goal: by a single action, by a single event, or by an event and an action .... In fact, the search for a sequence of events or actions that would achieve the goal may consider many alternatives. If all fail, it is far from obvious which blocked condition to notify Q of, and knowledge is needed to guide the choice. Some heuristics for dealing with that problem ~ .. given in [12]. 3.2. A n n o n p r o d u c t i v e a c t Suppose the proposed action does not achieve Q's l-goal, cL [6]. For example, dropping the course may still mean that failing status would be recorded as a WF (withdrawal while failing). R may initially plan to answer ""You can drop 577 by ...'. However, Q would expect to be told that his proposed action does not achieve his l-goal. Formula [7] states R's belief about this expectation. [6] RB(-holds(-fail(Q,C), drop(Q,C](Sc)) & admissible(drop(Q,C}(Sc)) ) [7] RBQB(RB[ want(Q,-,fail(Q,c)) & -,holds(-fail(Q,C),drop(Q,C](Sc)) I~ admissible( drop (Q, C]( S"
H86-1017,P84-1029,1,\N,Missing
H89-1037,P86-1032,0,0.0351018,"ncluding Kaplan&apos;s work on responding when presuppositions fail [7], Mays&apos; work both on responding when queries fail intensionally [8] and on determining competent monitor offers [9], McKeown&apos;s T E X T system for explaining concepts known to a database system [11], McCoy&apos;s system for correcting object-related misconceptions [10], Hirschberg&apos;s work on scalar implicatures and their use in avoiding the production of misleading responses [5], and Pollack&apos;s plan inference model for recognizing and responding to discrepancies between the system&apos;s and the user&apos;s beliefs about domain plans and actions [12]. Other explorations of cooperative communication include [1], [2], [4], [6], [13], [15], and [17]. For more complete references, the reader is referred to Cheikes [3]. The results of these studies have been highly informative. Many different kinds of cooperative behavior have seen identified, and computational models of them proposed. What is of interest to us here is the fact that all efforts to date in this area share the same implicit assumption--that cooperative response generation can be decomposed into separate reasoning processes. But this &quot;decomposability assumption&quot; in turn raises th"
H89-1037,J88-3004,0,0.141303,"h on responding when queries fail intensionally [8] and on determining competent monitor offers [9], McKeown&apos;s T E X T system for explaining concepts known to a database system [11], McCoy&apos;s system for correcting object-related misconceptions [10], Hirschberg&apos;s work on scalar implicatures and their use in avoiding the production of misleading responses [5], and Pollack&apos;s plan inference model for recognizing and responding to discrepancies between the system&apos;s and the user&apos;s beliefs about domain plans and actions [12]. Other explorations of cooperative communication include [1], [2], [4], [6], [13], [15], and [17]. For more complete references, the reader is referred to Cheikes [3]. The results of these studies have been highly informative. Many different kinds of cooperative behavior have seen identified, and computational models of them proposed. What is of interest to us here is the fact that all efforts to date in this area share the same implicit assumption--that cooperative response generation can be decomposed into separate reasoning processes. But this &quot;decomposability assumption&quot; in turn raises the inlegralion problem--the problem of getting those elements to work together in t"
I08-7009,W05-0305,1,\N,Missing
I08-7009,J03-4002,1,\N,Missing
I08-7009,I08-7010,0,\N,Missing
J03-4002,P01-1009,0,0.146503,"phone and picked up the receiver. Here the receiver denotes the receiver associated with (by virtue of being part of) the already-mentioned phone Myra darted to. Coreference and indirect anaphora can be uniformly modeled by saying that the discourse referent eα denoted by an anaphoric expression α is either equal to or associated with an existing discourse referent er , that is, eα =er or eα ∈assoc(er ). But coreference and associative anaphora do not exhaust the space of constructs that derive all or part of their sense from the discourse context and are thus anaphoric. Consider “other NPs” (Bierner 2001a; Bierner and Webber 2000; Modjeska 2001, 2002), as in: (26) Sue grabbed one phone, as Tom darted to the other phone. Although “other NPs” are clearly anaphoric, should the referent of the other phone (eα )—the phone other than the one Sue grabbed (er )—simply be considered a case of eα ∈ assoc(er )? Here are two reasons why they should not. First, in all cases of associative anaphora discussed in the literature, possible associations have depended only on the antecedent er and not on the anaphor. For example, only antecedents that have parts participate in whole-part associations (e.g., phon"
J03-4002,P02-1011,0,0.254166,"situation with anaphors so far, we have coreference when eα =er , indirect anaphora when eα ∈assoc(er ), and lexically specified anaphora when eα =fα (ei ) where ei = er or ei ∈assoc(er ). 3.2 Discourse Adverbials as Lexical Anaphors There is nothing in this generalized approach to discourse anaphora that requires that the source of er be an NP, or that the anaphor be a pronoun or NP. For example, the antecedent er of a singular demonstrative pronoun (in English, this or that) is often an eventuality that derives from a clause, a sentence, or a larger unit in the recent discourse (Asher 1993; Byron 2002; Eckert and Strube 2000; Webber 1991). We will show that this is the case with discourse adverbials as well. The extension we make to the general framework presented above in order to include discourse adverbials as discourse anaphors is to allow more general functions fα to be associated with lexically specified anaphors. In particular, for the discourse adverbials considered in this article, the function associated with an adverbial maps its anaphoric argument—an eventuality derived from the current discourse context—to a function that applies to the interpretation of the adverbial’s matrix"
J03-4002,W02-0204,1,0.645077,"plied to either an existing discourse referent or an entity associated with it through a bridging inference. In the case of the premodifier other, fα applied to its argument produces contextually 11 With respect to how many discourse adverbials there are, Quirk et al. (1972) discuss 60 conjunctions and discourse adverbials under the overall heading time relations and 123 under the overall heading conjuncts. Some entries appear under several headings, so that the total number of conjunctions and discourse adverbials they present is closer to 160. In another enumeration of discourse adverbials, Forbes and Webber (2002) start with all annotations of sentence-level adverbials in the Penn Treebank, then filter them systematically to determine which draw part of their meaning from the preceding discourse and how they do so. What we understand from both of these studies is that there are fewer than 200 adverbials to be considered, many of which are minor variations of one another (in contrast, by contrast, by way of contrast, in comparison, by comparison, by way of comparison that are unlikely to differ in their anaphoric properties, and some of which, such as contrariwise, hitherto, and to cap it all, will occu"
J03-4002,C92-1048,0,0.0487587,"n. So the variables must be the discourse variables usually used to translate other kinds of discourse anaphors.6 These arguments have been directed at the behavioral similarity between discourse adverbials and what we normally take to be discourse anaphors. But this isn’t the only reason to recognize discourse adverbials as anaphors: In the next section, we suggest a framework for anaphora that is broad enough to include discourse adverbials as well as definite and demonstrative pronouns and NPs, along with other discourse phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992, 1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank and Kamp 1997; Stone and Hardt 1999). 3. A Framework for Anaphora Here we show how only a single extension to a general framework for discourse anaphora is needed to cover discourse adverbials. The general framework is presented in Section 3.1, and the extension in Section 3.2. 3.1 Discourse Referents and Anaphor Interpretation The simplest discourse anaphors are coreferential: definite pronouns and definite NPs that denote one (or more) discourse referents in focus within the current discourse 6 Although r"
J03-4002,P85-1008,0,0.251834,"ing or deriving an eventuality from the current discourse context that meets the constraints of the adverbial with respect to the eventuality interpretation of the matrix clause. (Examples of this are given throughout the rest of the article.) 3.3 A Logical Form for Eventualities Before using this generalized view of anaphora to show what discourse adverbials contribute to discourse and how they interact with discourse relations that arise from adjacency or explicit discourse connectives, we briefly describe how we represent clausal interpretations in logical form (LF). Essentially, we follow Hobbs (1985) in using a rich ontology and a representation scheme that makes explicit all the individuals and abstract objects (i.e., propositions, facts/beliefs, and eventualities) (Asher 1993) involved in the LF interpretation of an utterance. We do so because we want to make intuitions about individuals, eventualities, lexical meaning, and anaphora as clear as possible. But certainly, other forms of representation are possible. In this LF representation scheme, each clause and each relation between clauses is indexed by the label of its associated abstract object. So, for example, the LF interpretation"
J03-4002,P02-1003,0,0.0133857,"eg SPunct |SPunct | on the one hand Seg on the other hand Seg | not only Seg but also Seg SPunct := S Punctuation Punctuation := . |; |: |? |! S := S Coord S |S Subord S |Subord S S |Sadv S | NP Sadv VP |S Sadv |. . . Coord := and |or |but |so Subord := although |after |because |before |... Sadv := DAdv |SimpleAdv DAdv := instead |otherwise |for example |meanwhile |... SimpleAdv := yesterday |today |surprisingly |hopefully |... Figure 6 PS rules for a discourse grammar. when semantics underspecifies syntactic dependency (as discourse semantics must, on our account) is known to be intractable (Koller and Striegnitz 2002). An effective solution is to generate semantics and syntax simultaneously, which is straightforward with a lexicalized grammar (Stone et al. 2001). Given the importance of various types of inference in discourse understanding, there is a second argument for using a lexicalized discourse grammar that derives from the role of implicature in discourse. Gricean reasoning about implicatures requires a hearer be able to infer the meaningful alternatives that a speaker had in composing a sentence. With lexicalization, these alternatives can be given by a grammar, allowing the hearer, for example, to"
J03-4002,P92-1004,0,0.0506838,"ure context. (Under coreference we include split reference, in which a plural anaphor such as the companies denotes all the separately mentioned companies in focus within the discourse context.) Much has been written about the factors affecting what discourse referents are taken to be in focus. For a recent review by Andrew Kehler, see chapter 18 of Jurafsky and Martin (2000). For the effect of different types of quantifiers on discourse referents and focus, see Kibble (1995). Somewhat more complex than coreference is indirect anaphora (Hellman and Fraurud 1996) (also called partial anaphora [Luperfoy 1992], textual ellipsis [Hahn, Markert, and Strube 1996], associative anaphora [Cosse 1996] bridging anaphora [Clark 1975; Clark and Marshall 1981; Not, Tovena, and Zancanaro 1999], and inferrables [Prince 1992]), in which the anaphor (usually a definite NP) denotes a discourse referent associated with one (or more) discourse referents in the current discourse context; for example, (25) Myra darted to a phone and picked up the receiver. Here the receiver denotes the receiver associated with (by virtue of being part of) the already-mentioned phone Myra darted to. Coreference and indirect anaphora c"
J03-4002,P02-1047,0,0.0341497,"genuine, or they may be eliminated by a lexical specification. Multicomponent TAG tree sets are used to provide an appropriate compositional treatment for quantifiers, which we borrow for interpreting for example (examples (66c–d)). In showing how DLTAG and an interpretative process on its derivations operate, we must, of necessity, gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 577 Computational Linguistics Volume 29, Number 4 units: It may be a matter simply of statistical inference, as in Marcu and Echihabi (2002), or of more complex inference, as in Hobbs et al. (1993). As we noted, our view is that there are three mechanisms at work in discourse semantics, just as there are in clause-level semantics: Inference isn’t the only process involved. Thus the focus of our presentation here is on how compositional rules and anaphor resolution (which itself often appears to require inference) operate together with inference to yield discourse semantics. We start with previous examples (44) (here (66c)) and (47) (here (66d)) and two somewhat simpler variants (66a–b): (66) a. You shouldn’t trust John because he"
J03-4002,J88-2003,0,0.226457,"ian accomplishment. In example (37), though, there is no culminated eventuality in the discourse context for then), to take as its first argument. (37) a. Go west on Lancaster Avenue. b. Then turn right on County Line. How does (37b) get its interpretation? As with (36d), the relevant elements of (37b) can be represented as α = then Rα = after S = turn right on County Line σ = e3 :turn-right(you, county line) and the unresolved interpretation of (37b) is thus [λ x . after(x, EV)]e3 ≡ after(e3 , EV) 559 Computational Linguistics Volume 29, Number 4 As for resolving EV, in a well-known article, Moens and Steedman (1988) discuss several ways in which an eventuality of one type (e.g., a process) can be coerced into an eventuality of another type (e.g., an accomplishment, which Moens and Steedman call a culminated process). In this case, the matrix argument of then (the eventuality of turning right on County Line) can be used to coerce the process eventuality in (37b) into a culminated process of going west on Lancaster Avenue until County Line. We treat this coercion as a type of associative or bridging inference, as in the examples discussed in section 3.1. That is, e2 = culmination(e1 )∈assoc(e1 ), where e1"
J03-4002,J92-4007,0,0.165676,"discourse structure and discourse semantics, we will continue to assume for as long as possible that an LF representation will suffice. Now it may appear as if there is no difference between treating adverbials as anaphors and treating them as structural connectives, especially in cases like (37) in which the antecedent comes from the immediately left-adjacent context, and in which the only obvious semantic relation between the adjacent sentences appears to be the one expressed by the discourse adverbial. (Of course, there may also be a separate intentional relation between the two sentences [Moore and Pollack 1992], independent of the relation conveyed by the discourse adverbial.) One must distinguish, however, between whether a theory allows a distinction to be made and whether that distinction needs to be made in a particular case. It is clear that there are many examples in which the two approaches (i.e., a purely structural treatment of all connectives, versus one that treats adverbials as linking into the discourse context anaphorically) appear to make the same prediction. We have already, however, demonstrated cases in which a purely structural account makes the wrong prediction, and in the next"
J03-4002,P95-1018,0,0.052621,"get their interpretations in different ways. Consider the two texts in example (69): 580 Webber et al. Anaphora and Discourse Structure α: so β: but α: because_mid β:then then τ1 T1 T3 τ3 * so but * T2 because α: so τ2 1 3 τ1 0 τ2 β: but τ4 T4 3 α: because_mid 1 3 τ4 τ3 0 β: then but because so T2 T1 T3 then T4 Figure 16 Derivation of example (68). (69) a. You should eliminate part 2 before part 3 because part 2 is more susceptible to damage. b. You should eliminate part 2 before part 3. This is because part 2 is more susceptible to damage. Example (69b) is a simpler version of an example in Moser and Moore (1995), in which This is because is treated as an unanalyzed cue phrase, no different from because in (69a). We show here that this isn’t necessary: One can analyze (69b) using compositional semantics and anaphor resolution and achieve the same results. First consider (69a). Given the interpretations of its two component clauses, its overall interpretation follows in the same way as (66a), shown in Figure 12. Now consider (69b) and the derivation shown in Figure 17. Here the initial tree α:because mid T1 α:because_mid τ1 T2 . 0 because TB β: punct1 β: punct1 T1 3 1 τ2 * because α:because_mid 3 T2 TB"
J03-4002,J96-3006,0,0.0132699,"Missing"
J03-4002,W99-0105,0,0.0441425,"Missing"
J03-4002,C88-2120,0,0.594403,"for resolving them. This is explored in section 3. 3. Any theory of discourse must still provide an account of how a sequence of adjacent discourse units (clauses, sentences, and the larger units that they can comprise) means more than just the sum of its component 547 Computational Linguistics Volume 29, Number 4 units. This is a goal that researchers have been pursuing for some time, using both compositional rules and defeasible inference to determine these additional aspects of meaning (Asher and Lascarides 1999; Gardent 1997; Hobbs et al. 1993; Kehler 2002; Polanyi and van den Berg 1996; Scha and Polanyi 1988; Schilder 1997a, 1997b; van den Berg 1996) If that portion of discourse semantics that can be handled by mechanisms already needed for resolving other forms of anaphora and deixis is factored out, there is less need to stretch and possibly distort compositional rules and defeasible inference to handle everything.2 Moreover, recognizing the possibility of two separate relations (one derived anaphorically and one associated with adjacency and/or a structural connective) admits additional richness to discourse semantics. Both points are discussed further in section 4. 4. Understanding discourse"
J03-4002,P98-2204,0,0.0570362,"(iii) Figure 5 Discourse structures for examples (11)–(13). Structural dependencies are indicated by solid lines and dependencies associate with discourse adverbials are indicated by broken lines. (explanation is the inverse of explanation—i.e., with its arguments in reverse order. Such relations are used to maintain the given linear order of clauses.) 551 Computational Linguistics Volume 29, Number 4 (14) Every mani tells every womanj hei meets that shej reminds himi of hisi mother. (15) Suei drives an Alfa Romeo. Shei drives too fast. Maryj races heri on weekends. Shej often beats heri . (Strube 1998) This suggests that in examples (11)–(13), the relationship between the discourse adverbial and its (initial) argument from the previous discourse might usefully be taken to be anaphoric as well.4 2.2 Discourse Adverbials Do Behave like Anaphors There is additional evidence to suggest that otherwise, then, and other discourse adverbials are anaphors. First, anaphors in the form of definite and demonstrative NPs can take implicit material as their referents. For example, in (16) Stack five blocks on top of one another. Now close your eyes and try knocking {the tower, this tower} over with your"
J03-4002,J88-2006,1,0.660385,"usually used to translate other kinds of discourse anaphors.6 These arguments have been directed at the behavioral similarity between discourse adverbials and what we normally take to be discourse anaphors. But this isn’t the only reason to recognize discourse adverbials as anaphors: In the next section, we suggest a framework for anaphora that is broad enough to include discourse adverbials as well as definite and demonstrative pronouns and NPs, along with other discourse phenomena that have been argued to be anaphoric, such as VP ellipsis (Hardt 1992, 1999; Kehler 2002), tense (Partee 1984; Webber 1988) and modality (Kibble 1995; Frank and Kamp 1997; Stone and Hardt 1999). 3. A Framework for Anaphora Here we show how only a single extension to a general framework for discourse anaphora is needed to cover discourse adverbials. The general framework is presented in Section 3.1, and the extension in Section 3.2. 3.1 Discourse Referents and Anaphor Interpretation The simplest discourse anaphors are coreferential: definite pronouns and definite NPs that denote one (or more) discourse referents in focus within the current discourse 6 Although rhetorical structure theory (RST) (Mann and Thompson 19"
J03-4002,P92-1013,1,0.612638,"Missing"
J03-4002,W98-0315,1,0.480953,"scourse clause (Dc ): a clause or a structure composed of discourse clauses. One reason for taking something to be an initial tree is that its local dependencies can be stretched long distance. At the sentence level, the dependency between apples and likes in Apples John likes is localized in all the trees for likes. This dependency can be stretched long distance, as in Apples, Bill thinks John may like. In discourse, as we noted in section 2, local dependencies can be stretched long distance as well, as in (59) a. Although John is generous, he’s hard to find. 16 Although in an earlier paper (Webber and Joshi 1998), we discuss reasons for taking the lexical anchors of the initial trees in Figures 7 and 8 to be feature structures, following the analysis in Knott (1996) and Knott and Mellish (1996), here we just take them to be specific lexical items. 574 Webber et al. Anaphora and Discourse Structure α:contrast Dc Dc On the one hand Dc On the other Figure 8 An initial tree for parallel constructions. This particular tree is for a contrastive construction anchored by on the one hand and on the other hand. b. Although John is generous—for example, he gives money to anyone who asks him for it—he’s hard to f"
J03-4002,P99-1006,1,0.654145,"Missing"
J03-4002,W93-0239,0,0.295266,"me family of discourse relations. But what if the relational meaning conveyed by cue phrases could in fact interact with discourse meaning in multiple ways? Then Knott’s substitution patterns among cue phrases may have reflected these complex interactions, as well as the meanings of individual cue phrases themselves. This article argues that cue phrases do depend on another mechanism for conveying extrasentential meaning—specifically, anaphora. One early hint that adverbial cue phrases (called here discourse connectives) might be anaphoric can be found in an ACL workshop paper in which Janyce Wiebe (1993) used the following example to question the adequacy of tree structures for discourse: (1) a. The car was finally coming toward him. b. He [Chee] finished his diagnostic tests, c. feeling relief. d. But then the car started to turn right. The problem Wiebe noted was that the discourse connectives but and then appear to link clause (1d) to two different things: then to clause (1b) in a sequence relation (i.e., the car’s starting to turn right being the next relevant event after Chee’s finishing his tests) and but to a grouping of clauses (1a) and (1c) (i.e., reporting a contrast between, on the"
J03-4002,J00-4006,0,\N,Missing
J03-4002,C98-2199,0,\N,Missing
J11-2004,C94-2174,0,0.302568,"Missing"
J11-2004,P97-1005,0,0.879349,"Missing"
J11-2004,J93-2004,0,0.0361578,"Missing"
J11-2004,sharoff-etal-2010-web,0,0.0696728,"Missing"
J11-2004,N03-1033,0,0.0130244,"Missing"
J12-4007,J94-4002,0,0.327831,"search/cb/projects/art/art-corpus/ 2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/ 918 Book Reviews The discussion of anaphora resolution covers rule-based approaches to resolving nominal anaphora (Section 3.5) and then supervised machine learning methods for anaphora resolution (Section 3.6). The latter follows the structure (albeit not the content) of Ng’s survey (2010), in discussing mention-pair models, and then entity-mention models. Whereas Ng then discusses ranking models, including his cluster ranker (Rahman and Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approach described in Section 3.5, Stede discusses a range of more recent models, most of which are subsequent to Ng’s survey. Section 3.8 surveys methods evaluating coreference resolution and some of the known problems in doing so. A good complement to this is Byron’s too-little-known discussion of problems in the consistent reporting of such results (Byron 2001). Chapter 3 concludes with a section on Recent Trends, which would also have been useful in Chapter 2. Chapter 4 The fourth and longest chapter deals with semantic or pragmatically oriented coherence relations that hold between adjac"
J12-4007,P09-2004,0,0.0257457,"relations. The ﬁrst task requires ﬁnding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a coherence relation, given that evidence is often ambiguous; (2) if it does, what constitutes its arguments; and (3) what is its sense. Although Chapter 4 covers some of this work (Dinesh et al. 2005; Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), its appearance within the context of a discussion of RST-tasks may lead to some confusion. Chapter 4 concludes with a brief discussion of some important open issues regarding coherence relations, including problems with associating a large text span with a single recursive structure of coherence relations and problems with inter-annotator agreement. Summary For its intended audience, this monograph will serve as a compact, readable introduction to the subject of discourse processing. The relevant phenomena are presented clearly, as are many of the computation"
J12-4007,prasad-etal-2008-penn,1,0.671726,"rk of RST, and thus doesn’t adhere to several of its assumptions—in particular, that a text is divisible into a covering sequence of elementary discourse units, that only one relation can hold between discourse units, that the arguments to a coherence relation must be adjacent, that one argument to a coherence relation may intrinsically convey information that is more important to the speaker’s purpose than the other, and that coherence relations impose an overall tree structure on a text in terms of recursively deﬁned discourse units. Although Chapter 4 discusses the Penn Discourse TreeBank (Prasad et al. 2008) and its “somewhat modest annotations” (page 126), the discussion is framed in terms of RST tasks, whereas the assumptions underlying the Penn Discourse TreeBank reﬂect its concerns with a quite different set of tasks involved in recognizing coherence relations. The ﬁrst task requires ﬁnding evidence for a coherence relation (in the form of a discourse connective such as a coordinating or subordinating conjunction or a discourse adverbial, or in the form of sentence adjacency) and then determining (1) if the evidence 919 Computational Linguistics Volume 38, Number 4 does indeed signal a cohere"
J12-4007,prasad-etal-2010-exploiting,1,0.885827,"Missing"
J12-4007,D09-1101,0,0.0555866,"Missing"
J12-4007,D07-1010,0,0.0202591,"Missing"
J14-4007,C12-1163,0,0.0216544,"s for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used th"
J14-4007,W12-4703,0,0.0192018,"s for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used th"
J14-4007,W13-2610,0,0.105535,"Missing"
J14-4007,W01-1605,0,0.526226,"Missing"
J14-4007,F12-2042,0,0.10177,"nd Xue (in press)), the Turkish Discourse Bank or TDB (Zeyrek et al. 2008, 2009; Aktas¸, Bozs¸ahin, and Zeyrek 2010; Zeyrek et al. 2010; Demirsahin et al. 2013; Zeyrek et al. 2013), the Hindi Discourse Relation Bank (Oza et al. 2009; Kolachina et al. 2012; Sharma et al. 2013), and the Prague Discourse TreeBank, or PDiT (Mladov´a, Zik´anov´a, and Hajiˇcov´a 2008; J´ınov´a, M´ırovsky, ´ and Pol´akov´a 2012; Rysov´a 2012; Pol´akov´a et al. 2013), now part of the Prague Dependency TreeBank, version 3.0, PDT 3.0 (Bejˇcek et al. 2013). (A comparable discourse treebank is being developed for French (Danlos et al. 2012), but it has not yet been released and the information needed to compare it to the other corpora in Table 4 is not available.) Although these comparable corpora differ in ways to be discussed subsequently, they all adhere to the key ideas of PDTB annotation (Section 2) in being neutral to any discourse structure beyond the argument structure of individual discourse relations and in grounding discourse relations in lexical expressions. Where they annotate implicit discourse relations (Table 4), these comparable corpora follow the PDTB in annotating an inferred lexical grounding. All of the corp"
J14-4007,W13-2315,0,0.0301067,"Missing"
J14-4007,W05-0305,1,0.280638,"Missing"
J14-4007,I11-1120,0,0.0122955,"e outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the ai"
J14-4007,ghosh-etal-2012-improving,0,0.0782143,"Missing"
J14-4007,J93-3003,0,0.567299,"er, and Joshi 2006). This set was then enlarged as new connectives were found in the WSJ corpus itself. Also identified during this phase were productive modifiers of explicit connectives such as apparently, at least partly, in large part, even, only, and so on, which were then annotated as connective modifiers.2 What were not taken to be discourse connectives were adverbial cue phrases, including sentence-initial Now (Example (3)), Well (Example (4)), So (Example (5)), and OK (Example (6)), because they signal topic changes such as the beginning of a subtopic or a return to a previous topic (Hirschberg and Litman 1993), rather than relating particular discourse elements. (3) Now why, you have to ask yourself, would intelligent beings haul a bunch of rocks around the universe? [wsj 0550] (4) Well, mankind can rest easier for now. [wsj 1272] (5) So, OK kids, everybody on stage for “Carry On Trading.” [wsj 2402] (6) When Mr. Jacobson walked into the office at 7:30 a.m. EDT, he announced: “OK, buckle up.” [wsj 1171] We did not intend to annotate as discourse connectives pragmatic markers such as actually and in fact, which serve to signal the conversational role of the speaker’s matrix utterance—specifically, t"
J14-4007,W12-4704,0,0.0298796,"Missing"
J14-4007,kolachina-etal-2012-evaluation,1,0.889811,"Missing"
J14-4007,J93-2004,0,0.0567604,"Missing"
J14-4007,P11-3009,0,0.0119687,"n both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDT"
J14-4007,W12-0117,0,0.0254165,"age technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents wha"
J14-4007,W13-3303,1,0.724081,"istics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since"
J14-4007,W04-2703,1,0.292051,"Missing"
J14-4007,mladova-etal-2008-sentence,0,0.117353,"Missing"
J14-4007,J88-2003,0,0.713677,"Missing"
J14-4007,J05-1004,0,0.126043,"Missing"
J14-4007,pareti-2012-database,0,0.0241257,"cessary for the discourse relation in Example (20).5 (19) Defense contractors “cannot continue to get contracts on that basis,” said Howard Rubel, an analyst with C.J. Lawrence, Morgan Grenfell Inc. in New York. (implicit=because) “The pain is too great.” [wsj 0673] 5 The PDTB also annotates attribution relations, capturing their textual signal and semantic features over each discourse relation and each of its arguments. For a full description of attribution and its annotation, the reader is referred to Prasad et al. (2007). Attribution is now being annotated as a separate layer over the WSJ (Pareti 2012), building on the PDTB attribution scheme, but aiming to capture the phenomena more comprehensively than in the PDTB. 929 Computational Linguistics (20) Volume 40, Number 4 Mr. Asman is also annoyed that Mr. Castro has resisted collaboration with U.S. officials, even though by his own account that collaboration has been devised essentially as a mechanism for acts directly hostile to the Cuban regime, such as facilitating defections. [wsj 1416] Attribution differs from supplementary information in that, when its polarity is negative, it can interact with discourse relations. (Sup has no such in"
J14-4007,D13-1094,0,0.452089,"ns (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since release of the corpus in 2008. Secondly, for those researchers who have used the PDTB, Section 3 aims to point out significant featur"
J14-4007,P09-2004,0,0.0946201,"it=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create compar"
J14-4007,C08-2022,1,0.898974,"of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of an"
J14-4007,I13-1011,0,0.176202,"Missing"
J14-4007,prasad-etal-2008-penn,1,0.9379,"on that may have weakened previous results or the performance of decision procedures induced from the data; (3) to explain variations seen in the annotation of comparable resources in other languages and genres, which should allow developers of future comparable resources to recognize whether the variations are relevant to them; and (4) to enumerate and explain relationships between PDTB annotation and complementary annotation of other linguistic phenomena. The paper draws on work done by ourselves and others since the corpus was released. 1. Introduction The Penn Discourse TreeBank, or PDTB (Prasad et al. 2008; PDTB-Group 2008) is the largest manually annotated resource of discourse relations. This annotation has been added to the million-word Wall Street Journal portion of the Penn Treebank (PTB) corpus ∗ Department of Health Informatics and Administration, University of Wisconsin-Milwaukee, 2025 E. Newport Ave (NWQB), Milwaukee WI 53211. E-mail: prasadr@uwm.edu. ∗∗ School of Informatics, University of Edinburgh, 10 Crichton Street (IF4.29), Edinburgh UK EH8 9AB. E-mail: bonnie.webber@ed.ac.uk. † Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut Street (Suite 400"
J14-4007,prasad-etal-2010-exploiting,1,0.857193,"aining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages"
J14-4007,C10-2118,1,0.673052,"aining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages"
J14-4007,W05-0302,0,0.0584237,"Missing"
J14-4007,rysova-2012-alternative,0,0.0860411,"Missing"
J14-4007,W13-0124,1,0.948187,"ve begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). What then are the aims of this paper? First, for those researchers who are unaware of the PDTB, Section 2 of the paper lays out the key ideas behind the PDTB annotation methodology, and Section 3 describes the corpus in more detail than previous papers (Prasad et al. 2008; PDTB-Group 2008) and presents what we have learned since"
J14-4007,D07-1010,0,0.00884648,"ction Agency imposed a gradual ban on virtually all uses of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Othe"
J14-4007,W05-0312,0,0.038729,"Missing"
J14-4007,W09-3006,0,0.0495351,"Missing"
J14-4007,W10-1844,0,0.0503983,"Missing"
J14-4007,P12-1008,0,0.0294487,"nature and the sources of this variation, so that people contemplating development of comparable resources in additional languages and/or genres will recognize variation that is appropriate to their situation, while avoiding unnecessary variation that prevents inter-operability of these comparable corpora (Bunt, Prasad, and Joshi 2012). Table 4 identifies the corpora we will discuss and the extent of their current annotation: the BioDRB (Prasad et al. 2011), the Leeds Arabic Discourse TreeBank, or LADTB (Al-Saif and Markert 2010, 2011; Al-Saif 2012), the Chinese Discourse TreeBank (Xue 2005; Zhou and Xue 2012; Zhou and Xue (in press)), the Turkish Discourse Bank or TDB (Zeyrek et al. 2008, 2009; Aktas¸, Bozs¸ahin, and Zeyrek 2010; Zeyrek et al. 2010; Demirsahin et al. 2013; Zeyrek et al. 2013), the Hindi Discourse Relation Bank (Oza et al. 2009; Kolachina et al. 2012; Sharma et al. 2013), and the Prague Discourse TreeBank, or PDiT (Mladov´a, Zik´anov´a, and Hajiˇcov´a 2008; J´ınov´a, M´ırovsky, ´ and Pol´akov´a 2012; Rysov´a 2012; Pol´akov´a et al. 2013), now part of the Prague Dependency TreeBank, version 3.0, PDT 3.0 (Bejˇcek et al. 2013). (A comparable discourse treebank is being developed for"
J14-4007,C10-2172,0,0.133404,"ng asbestos will be outlawed. [wsj 0003] Over 18K explicitly signalled relations and over 16K implicit forms have been annotated in the PDTB 2.0 (cf. Section 3.2, Table 1), which was released in February 2008, through the Linguistic Data Consortium (LDC).1 Researchers since then, in both language technology and psycholinguistics, have begun to use the PDTB in their research, developing methods and tools for automatically annotating discourse relations (Wellner and Pustejovsky 2007; Elwell and Baldridge 2008; Pitler et al. 2008; Pitler and Nenkova 2009; Wellner 2009; Prasad et al. 2010a, 2011; Zhou et al. 2010; Ghosh et al. 2011a, 2011b, 2012; Lin, Ng, and Kan 2012; Ramesh et al. 2012), generating questions (Prasad and Joshi 2008; Agarwal, Shah, and Mannem 2011), ensuring an appropriate realization of discourse relations in the output of statistical machine translation (Meyer 2011; Meyer and Popescu-Belis 2012; Meyer and Webber 2013), and testing hypotheses about human discourse processing (Asr and Demberg 2012a, 2012b, 2013; Jiang 2013; Patterson and Kehler 2013). Other researchers have adapted the PDTB style of annotation to create comparable resources in other languages and genres (Section 4). W"
J14-4007,W10-1832,0,\N,Missing
J14-4007,D11-1068,0,\N,Missing
J14-4007,W09-3029,1,\N,Missing
J14-4007,al-saif-markert-2010-leeds,0,\N,Missing
J14-4007,W04-0211,0,\N,Missing
J14-4007,W11-1401,0,\N,Missing
J18-3001,W05-0305,1,0.681618,"be attached along an orthogonal dimension because it bore a different semantic relation to “book” than say “thick book” or “book about insects.” For Aravind, discourse provided this orthogonal dimension, reducing the number of “no-win” decisions that followed from insisting on a single parse tree over a sentence. Having spent so much time with sentences from the Penn TreeBank (constructed from articles from the ACL/DCI Wall Street Journal corpus), Aravind found the examples that made the most convincing demand for distributing the burden between syntax and discourse to be attribution phrases (Dinesh et al. 2005). He felt that sometimes an attribution phrase like “the company says” belongs to sentential syntax, feeding its semantics up to that of the sentence as a whole (as in the contrast expressed in “Observers say negotiations have halted, while the company says it is talking with several prospects”). In other cases, however, such as the concession expressed in “There have been no orders for the Cray-3 so far, though the company says it is talking with several prospects,” he felt that the attribution phrase belongs to an orthogonal discourse dimension, because what is contrary to expectations assoc"
J18-3001,J95-2003,0,0.870321,"Missing"
J18-3001,P84-1029,1,0.158118,"Missing"
J18-3001,H86-1017,1,0.523584,"Missing"
J18-3001,W09-3029,0,0.0227225,"urse (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998), in the form of work on “Centering,” which was meant to account for ease of inference and the use of anaphoric expressions, linked by the observation that an entity that can be accessed with an expression as small as a pronoun must also be prominent. r discourse and syntax (Webber et al. 1999, 2003), where Aravind reconceptualized discourse connectives in the framework of LTAG, culminating in development of the NSF-funded Penn Discourse TreeBank3 and similarly annotated corpora in Chinese (Zhou and Xue 2012, 2015), Hindi (Oza et al. 2009), Turkish (Zeyrek et al. 2010), and biomedicine (Prasad et al. 2011). 2 https://www.ircs.upenn.edu/. 3 http://www.seas.upenn.edu/∼pdtb/. 388 Webber Obituary Here it is worth saying a bit more about two aspects of Aravind’s work: His work on grammar formalisms and his work on discourse. In the early 1980s, Aravind identified a set of computational properties that provided an informal definition of a class of Mildly Context Sensitive (MCS) languages that he claimed properly included all human languages and was properly included among the much vaster class of contextsensitive languages. These pro"
J18-3001,P99-1006,1,0.720554,"the range of phenomena in human language syntax while remaining computationally tractable (Joshi and Schabes 1997). r cooperative Question Answering and the range of inference it requires (Joshi, Webber, and Weischedel 1984, 1986) r prominence in discourse (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998), in the form of work on “Centering,” which was meant to account for ease of inference and the use of anaphoric expressions, linked by the observation that an entity that can be accessed with an expression as small as a pronoun must also be prominent. r discourse and syntax (Webber et al. 1999, 2003), where Aravind reconceptualized discourse connectives in the framework of LTAG, culminating in development of the NSF-funded Penn Discourse TreeBank3 and similarly annotated corpora in Chinese (Zhou and Xue 2012, 2015), Hindi (Oza et al. 2009), Turkish (Zeyrek et al. 2010), and biomedicine (Prasad et al. 2011). 2 https://www.ircs.upenn.edu/. 3 http://www.seas.upenn.edu/∼pdtb/. 388 Webber Obituary Here it is worth saying a bit more about two aspects of Aravind’s work: His work on grammar formalisms and his work on discourse. In the early 1980s, Aravind identified a set of computational"
J18-3001,P12-1008,0,0.0310367,"1984, 1986) r prominence in discourse (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998), in the form of work on “Centering,” which was meant to account for ease of inference and the use of anaphoric expressions, linked by the observation that an entity that can be accessed with an expression as small as a pronoun must also be prominent. r discourse and syntax (Webber et al. 1999, 2003), where Aravind reconceptualized discourse connectives in the framework of LTAG, culminating in development of the NSF-funded Penn Discourse TreeBank3 and similarly annotated corpora in Chinese (Zhou and Xue 2012, 2015), Hindi (Oza et al. 2009), Turkish (Zeyrek et al. 2010), and biomedicine (Prasad et al. 2011). 2 https://www.ircs.upenn.edu/. 3 http://www.seas.upenn.edu/∼pdtb/. 388 Webber Obituary Here it is worth saying a bit more about two aspects of Aravind’s work: His work on grammar formalisms and his work on discourse. In the early 1980s, Aravind identified a set of computational properties that provided an informal definition of a class of Mildly Context Sensitive (MCS) languages that he claimed properly included all human languages and was properly included among the much vaster class of conte"
J78-3012,C69-7001,0,\N,Missing
J78-3012,C69-6902,0,\N,Missing
J88-2006,P83-1007,0,0.0418252,"Missing"
J88-2006,P86-1003,0,0.067558,"ast'. That is, like the Simple Past Tense, the Past Perfect demands an already established past point of reference. (Leech: 47) L e e c h did not elaborate further on how reference points are used in the interpretation of simple past tense and past perfect tense, or on what has b e c o m e the main problem in the semantics and pragmatics of tense: reconciling the (usual) forward m o v e m e n t of events in narratives with a belief in the anaphoric (or contextdependent) character of tense. The first explicit reference I have to tense being anaphoric like a definite pronoun is in an article by McCawley (1971:110), who said: H o w e v e r the tense m o r p h e m e does not just express the time relationship between the clause it is in and the next higher clause--it also refers to the time of the clause that it is in, and indeed, refers to it in a way that is rather like the way in which personal pronouns refer to what they stand for. McCawley also tried to fit in his view of tense as pronoun with the interpretation of tense in simple narratives. Here he proposed that the event described in one clause serves as the antecedent of the event described in the next, but that it may be related to that ev"
J88-2006,P87-1001,0,0.0832706,"Missing"
J88-2006,P87-1003,0,0.0491697,"Missing"
J88-2006,P87-1021,1,0.710594,"Missing"
J88-2006,J86-3001,0,\N,Missing
K16-2001,K16-2003,0,0.135731,"nvolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a"
K16-2001,K16-2015,0,0.0587712,"6) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes, MaxEnt word embeddings no no cip2016 (Kang et al., 2016) Institute of Automation, CAS syntactic parses, MPQA subjectivity, VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitt"
K16-2001,K16-2018,0,0.0493477,"Missing"
K16-2001,P13-2013,0,0.0614398,"unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder, 2016; Wang and Lan, 2016;"
K16-2001,K16-2016,0,0.0652388,"olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn) syntactic parses, Brown clusters none Word embeddings, parse trees,"
K16-2001,K15-2004,0,0.140524,"Missing"
K16-2001,K16-2009,0,0.166348,"IST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labe"
K16-2001,K16-2021,0,0.296275,"lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters none Soochow Averaged perceptron (for both sequence labeling and sense) SVM and Maxent (Scikit-learn)"
K16-2001,K16-2013,0,0.191078,"tional overhead. On TIRA, the blind test set can only be accessed in the Connective identification The identification of discourse connectives is not a simple dictionary lookup as some discourse connective expressions are ambiguous and may function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia T"
K16-2001,K16-2011,0,0.0335294,"Missing"
K16-2001,K16-2017,0,0.107933,"ay function as discourse connectives in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for i"
K16-2001,K16-2008,0,0.0961919,"Missing"
K16-2001,K16-2022,0,0.0226855,"Missing"
K16-2001,P09-1077,0,0.372145,"ourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse relation senses, participants have generally adopted “conventional” machine learning techniques such as SVM and MaxEnt models that rely on manually designed features. Explicit discourse relation senses can be predicted with high accuracy. The main challenge is predicting implicit discourse relation senses, which has received a considerable amount of attention in recent years (Pitler et al., 2009; Biran and McKeown, 2013; Rutherford and Xue, 2014). Determining implicit discourse relation senses relies on information from the two arguments of the relation. For this subtask, there is a good balance between “conventional” machine learning techniques such as Support Vector Machines and Maximum Entropy models that rely heavily on handcrafted features, and neural network based approaches. A wide variety of features have been used for this subtask, and they include features extracted from syntactic parses (Kang et al., 2016; Kong et al., 2016; Stepanov and Riccardi, 2016; Jain and Majumder,"
K16-2001,I05-3025,1,0.0947611,"as follows: • News articles were extracted from the Wikinews XML dump2 using the publicly available WikiExtractor.py script.3 • Additional processing was done to remove any remaining XML annotations and produce a raw text version of each article (including its title and date). 4 Evaluation The scorer that computes all of the available evaluation metrics is open-source with some contribution from the participants during the task period6 . • Articles written purely in simplified Chinese were identified using the Dragon Mapper4 Python library, and segmented using the NUS Chinese word segmenter (Low et al., 2005). 4.1 Main evaluation metric: End-to-end discourse parsing 1 https://zh.wikinews.org/ https://dumps.wikimedia.org/zhwikinews/20151020/ zhwikinews-20151020-pages-meta-current.xml.bz2 3 http://medialab.di.unipi.it/wiki/Wikipedia_Extractor 4 http://dragonmapper.readthedocs.io/en/latest/index. html A shallow discourse parser (SDP) is evaluated based on the end-to-end F1 score on a per2 5 6 4 https://www.seas.upenn.edu/~pdtb/tools.shtml#annotator http://www.github.com/attapol/conll16st. Sense Definition Alternative Causation Condition Conjunction Contrast Relation between two alternatives Relation"
K16-2001,P14-5010,0,0.00268188,"nefits from the precise evaluation of the progress and improvement since the system is based off the exact same implementation. • Brown clusters (implementation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and L"
K16-2001,prasad-etal-2008-penn,1,0.839291,"ilingual Shallow Discourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper"
K16-2001,K16-2014,0,0.241056,"Missing"
K16-2001,J14-4007,1,0.889259,"ourse Parsing (SDP). While the 2015 task focused on newswire text data in English, this year we added a new language, Chinese. Given a natural language text as input, the goal of an SDP system is to detect and categorize discourse relations between discourse segments in the text. The conceptual framework of the Shallow Discourse Parsing 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics task is that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008; Prasad et al., 2014), where a discourse relation is viewed as a predicate that takes two abstract objects as arguments. The two arguments may be realized as clauses or sentences, or occasionally phrases. It is “shallow” in that sense that the system is not required to output a tree or graph that covers the entire text, and the discourse relations are not hierarchically organized. As such, it differs from analyses according to either Rhetorical Structure (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003). The rest of this overview paper is structured as follo"
K16-2001,K16-2020,0,0.0356651,"Missing"
K16-2001,K16-2010,0,0.282836,"ources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16 (Chandrasekar et al., 2016) nikko University of Trento CRF++, AdaBoost SJTU SVM (explicit sense), CNN (implicit word sense) Maxent (OpenNLP) Brown clusters, dependency/phrase structure parses, VerbNet, MPQA Lexicon word embedings (implicit word sense) Wikipedia (for training event embeddings) none Olso-PotsdamTeesside Purdue University SJTU none syntactic parses none Soochow University Maxent (OpenNLP), SVM (for Chinese) syntactic parses, Brown clusters n"
K16-2001,K16-2002,0,0.416621,", VerbNet, Word embeddings (word2vec) syntactic parses, word embeddings ECNU IIT-Hyderabad IITBHU MaxEnt (Mallet) no Table 3: Approaches of participating systems (Part I). Teams that have not submitted a system description paper are marked with ∗. subtask are represented in this competition. One is to collect all candidate discourse connective by looking up a list of possible connectives compiled from the training data and train a classifier to disambiguate them. There are two variants in this approach: one strategy is to train a classifier for each individual discourse connective expression (Oepen et al., 2016), and the other is to train one classifier for all discourse connective expressions (Wang and Lan, 2016; Kong et al., 2015; Laali et al., 2016). Alternatively, connective identification is treated as a token-level sequence labeling task, solved with sequence labeling models like CRF (Stepanov and Riccardi, 2016). Argument extraction Different strategies were used for extracting the arguments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that"
K16-2001,E14-1068,1,0.449193,"ent algorithms and models can be more meaningfully compared. In the open track, the focus of the evaluation is on the overall performance and the use of all possible means to improve the performance of a task. This distinction was easier to maintain for early CoNLL tasks such as noun phrase chunking and named entity recognition, where competitive performance could be achieved without having to use resources other than the provided training set. However, this is no longer true for a high-level task like discourse parsing where external resources such as Brown clusters have proved to be useful (Rutherford and Xue, 2014). In addition, to be competitive in the discourse parsing task, one also has to process the data with syntactic and possibly semantic parsers, which may also be trained on data that is outside the training set. As a compromise, therefore, we allowed participants in the closed track to use the following linguistic resources, in addition to the training set: For English, For purposes of evaluation, an explicit discourse connective predicted by a parser is considered correct if and only if the predicted raw connective includes the gold raw connective head, while allowing for the tokens of the pre"
K16-2001,K16-2019,0,0.096966,"Missing"
K16-2001,K16-2007,1,0.844548,"relations. A variety of neural network architectures are represented. (Schenk et al., 2016) used a feedforward neural network, with dependency structures used to re-weight the word embeddings used as input to the network. (Wang and Lan, 2016; Qin et al., 2016) achieved competitive performance using a Convolutional Neural Network architecture for this subtask. Finally, (Weiss and Bajec, 2016) produced competitive results with a focused RNN. Word embeddings were typically used as input to the neural network models and different pooling methods have been used to derive the vectors for arguments. Rutherford and Xue (2016) used simple summation pooling in a feedforward network and achieved competitive performance in classifying implicit discourse relation senses. Relation sense classification All systems have separate classifiers for explicit and implicit discourse connectives. For explicit relations, the discourse connective itself is the best predictor of the discourse relation. Many discourse connectives are unambiguous, always mapping to one discourse relation sense. For ambiguous discourse connectives, discourse relation sense classification amounts to word sense disambiguation. For explicit discourse rela"
K16-2001,K16-2012,0,0.156471,"Missing"
K16-2001,K15-2002,0,0.156384,"al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discourse relations in a particular context, and for predicting the senses of the discourse relations. Here we briefly summarize the approaches used in each subtask. We use a new web service called TIRA as the platform for system evaluation (Gollub et al., 2012; Potthast et al., 2014). Traditionally, participating teams have been asked to manually run their system on the blind test set without the gold standard labels, and submit the output for evaluation. Sta"
K16-2001,K16-2004,0,0.188138,"ves in some context but not in others. Several approaches to this 7 ID Institution Learning methods Resources used Extra resources steven bit (Jian et al., 2016) Aicyber.com BIT SVM (for English explicits, English and Chinese implicits), rule-based method for Chinese explicit Word embeddings ttr (Rutherford and Xue, 2016) clac (Laali et al., 2016) Brandeis University Feedforword (implicit sense only, pooling before hidden layers) word embeddings General Inquirer lexicon, HowNet, Central News of Taiwan no Concordia syntactic parses, word embeddings no devenshu (Jain and Majumder, 2016) ecnucs (Wang and Lan, 2016) DA-IICT CRF, decision tree (C4.5), Convolutional Network (implicit discourse senses) Maxent (openNLP) syntactic parses no phrase structure parses no goethe (Schenk et al., 2016) Goethe University Frankfurt syntactic parses, Brown clusters no gtnlp tbmihaylov (Mihaylov and Frank, 2016) aarjay iitbhu (Kaur et al., 2016) Georgia Tech Heidelberg Liblinear, convolutional network for implicit relation (for English implicit) Feed-forward neural network, CRF (connective and argument extraction), SVM (explicit sense) Liblinear (scikit-learn) (for explicit sense), CNN (for implicit sense) Naive Bayes,"
K16-2001,P14-1069,1,0.15687,"ation from (Liang, 2005)) • Word embeddings (word2vec) To make the task more manageable for participants, we provided them with training and test data with the following layers of automatic linguistic annotation produced using state-of-the-art NLP tools: For English, • Phrase structure parses predicted using the Berkeley parser (Petrov and Klein, 2007); • Dependency parses converted from phrase structure parses using the Stanford converter (Manning et al., 2014). For Chinese, • Phrase structure parses predicted with 10-fold cross validation on CTB8.0 using the transition-based Chinese parser (Wang and Xue, 2014); • Dependency parses converted from phrase structure parses using the Penn2Malt converter. 4.5 5 Approaches Evaluation Platform: TIRA Teams could participate in either English or Chinese or both, and either submit an end-toend system or just compete in the discourse relation sense prediction component. All endto-end systems for English adopted some variation of the pipeline architecture proposed by Lin et al (2014) and perfected by Wang and Lan (2015), which has components for identifying discourse connectives and extracting their arguments, for determining the presence or absence of discours"
K16-2001,K16-2006,0,0.0402375,"ments for explicit and for implicit discourse relations. Determining the arguments of implicit discourse relations is relatively straightforward. Most systems adopted a heuristics–based extraction strategy that parallels the PDTB annotation strategy for implicit discourse relations: for each pair of adjacent sentences that do not straddle a paragraph boundary, if an explicit discourse relation does not already exist, posit 8 ID Institution Learning methods Resources used Extra resources nguyenlab (Nguyen, 2016) JAIST phrase structure trees, MPQA Subjectivity lexicon, word embeddings none gw0 (Weiss and Bajec, 2016) olslopots (Oepen et al., 2016) purduenlp (Pacheco et al., 2016) Univ. of Ljubljana CRF (CRF++) for detecting connectives and arguments, SMO and Random Forest for classifying senses Focused RNN (sense only, for both explicit and implicit) SVM (SVMl ight), heuristic argument extraction SVM (explicit sense), Feedforword (implicit sense) word embeddings none Brown clusters none word embeddings stepanov (Stepanov and Riccardi, 2016) tao0920 (Qin et al., 2016) Rival2710 (Li et al., 2016b) lib16b (Kong et al., 2016; Li et al., 2016a) Soochow (Fan et al., 2016) ykido (Kido and Aizawa, 2016) VTNLPS16"
K16-2001,K15-2001,1,0.641779,"prises a long pipeline, and it is hard for teams that do not have a pre-existing system to put together a competitive full system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactical"
K16-2001,P12-1008,1,0.873541,"ull system. This year we therefore allowed participants to focus solely on predicting the sense of discourse relations, given gold-standard connectives and their arguments. 3 Data 3.1 Training and Development The training and development sets for English remain exactly the same as those used in the CoNLL-2015 shared task. Details regarding how the data was adapted from the Penn Discourse TreeBank 2.0 (PDTB 2.0) are provided in the overview paper of the CoNLL 2015 shared task (Xue et al., 2015). The Chinese training and development sets are taken from the Chinese Discourse TreeBank (CDTB) 0.5 (Zhou and Xue, 2012; Zhou and Xue, 2015), available from the LDC (http://ldc.upenn.edu), supplemented with additional annotated data from the Chinese TreeBank (Xue et al., 2005). The CDTB adopts the general annotation strategy of the PDTB, associating discourse relations with explicit or implicit discourse connectives and the two spans that serve as their arguments. In the case of explicit discourse relations (Example 1), there is an overt discourse connective, which may be realized syntactically as a subordinating or coordinating conjunction, or a discourse adverbial. Implicit discourse relations are cases wher"
L16-1629,P14-2064,0,0.0465076,"Missing"
L16-1629,W06-1670,0,0.0628994,"ecifically belongs to the verb (e.g. ‘listen to’, ‘look for’) and nomi3986 1 2 http://www.cs.cmu.edu/˜ark/LexSem/ http://www.inf.u-szeged.hu/rgai/mwe nal compounds that include more than two elements (e.g., ‘pumpkin spice latte’ or ‘surprise birthday party’. 2.2. Supersense Labels Both methods described below for detecting potential inconsistencies are based on ranking each ambiguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proport"
L16-1629,dickinson-lee-2008-detecting,0,0.27069,"erent labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on contextual features. When it comes to lexical semantic annota"
L16-1629,A00-2020,0,0.712067,"t have been labeled differently. A prominent family of methods considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring me"
L16-1629,S14-1001,0,0.0251135,"biguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proportion of favorable evidence (e.g., the number of times an expression M occurs annotated as an MWE) minus the proportion of unfavorable evidence (e.g., the number of times M occurs not annotated as an MWE). Weighted Discrepancy Ranking for MWE Annotations As already noted, M ∈ M is the set of tokens that have been annotated at least once in the STREUSLE corpus for a given sequ"
L16-1629,P10-2014,0,0.124836,"considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on co"
L16-1629,E09-1060,0,0.0698249,"ar instances that have been labeled differently. A prominent family of methods considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-bas"
L16-1629,C02-1101,0,0.391767,"ction in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on contextual features. When it comes to lexical semantic annotation, we leave to future work the possibility of exploiting context to detect inconsistencies, though the benefits of doing so may be limited for our small corpora. Other approaches have taken advantage of multiple annotations from different annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014). Our methods only consider one annotation per sentence, and therefore do not depend on"
L16-1629,Q14-1025,0,0.0579799,"ing context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3), except it conditions on contextual features. When it comes to lexical semantic annotation, we leave to future work the possibility of exploiting context to detect inconsistencies, though the benefits of doing so may be limited for our small corpora. Other approaches have taken advantage of multiple annotations from different annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014). Our methods only consider one annotation per sentence, and therefore do not depend on information which is available only for some corpora. 7. Conclusion Since inconsistency detection in semantic annotation is a largely unexplored topic and semantic inconsistencies are more difficult to grasp than syntactic inconsistencies, 3989 we explore two ranking-based methods to approach this task. We apply both methods to annotations of multiword expressions and supersense labels on different data sets. Overall, the proposed ranking methods are successful in detecting inconsistency candidates with hig"
L16-1629,P14-2083,0,0.0266289,"ncies can be found in any manually-annotated corpus due to underspecified or even missing guidelines, ambiguity, insufficient annotator expertise, and/or human errors. A consistent corpus is not only a high-quality lexical resource, but a foundation for robust automatic language processing via supervised learning. Since inconsistencies in the training corpus can lead to low performance, consistent annotation is of practical as well as theoretical benefit. Annotation inconsistencies can be subclassified into annotation errors and hard cases, although they cannot always be easily distinguished (Plank et al., 2014). An annotation error is an instance that is annotated incorrectly according to the guidelines. Annotation errors have a correct answer. To give an example from part-of-speech annotation, consider the phrase ‘a summer feeling’. The word ‘summer’ should be annotated as a common noun; marking it as a comparative adjective or a determiner would be an annotation error. Linguistically hard cases are instances that do not have one correct answer; they may have multiple correct answers or it may not be obvious what the label should be. Such annotations are not necessarily incorrect, but can be ambigu"
L16-1629,N15-1177,1,0.861515,"re than two elements (e.g., ‘pumpkin spice latte’ or ‘surprise birthday party’. 2.2. Supersense Labels Both methods described below for detecting potential inconsistencies are based on ranking each ambiguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proportion of favorable evidence (e.g., the number of times an expression M occurs annotated as an MWE) minus the proportion of unfavorable evidence (e.g., the number of times M occurs n"
L16-1629,P12-2050,1,0.878986,".u-szeged.hu/rgai/mwe nal compounds that include more than two elements (e.g., ‘pumpkin spice latte’ or ‘surprise birthday party’. 2.2. Supersense Labels Both methods described below for detecting potential inconsistencies are based on ranking each ambiguous type in terms of differences in how its tokens have been labelled. Supersenses are coarse-grained semantic classes such as PERSON , TIME, and ARTIFACT for nouns and MOTION , EMOTION , and COMMUNICATION for verbs (Ciaramita and Altun, 2006).3 Supersense annotation is the task of assigning one of these labels to selected tokens in a corpus (Schneider et al., 2012). 3.2. In addition to MWEs, Schneider and Smith (2015) also annotated supersense labels for all verbs and nouns in the STREUSLE corpus, including all strong MWEs. As a second corpus, the publicly available Twitter data sets4 by Johannsen et al. (2014) are annotated with supersenses using the BIO (Begin-Inside-Other) notation. In total this data set comprises 19,232 tokens.5 3.2.1. Weighted Discrepancy Ranking Discrepancy is expressed as the difference between the proportion of favorable evidence (e.g., the number of times an expression M occurs annotated as an MWE) minus the proportion of unfa"
L16-1629,schneider-etal-2014-comprehensive,1,0.893213,"Missing"
L16-1629,W15-1612,1,0.80389,"ed and not annotated occurrences is an indication that the type is inconsistently annotated. This discrepancy is weighted (scaled) by the number of times the MWE was annotated to put more weight on frequent multiword expressions. With the above measure, we rank types in descending order. The hypothesis is that types with greater annotated-MWE frequency overall, and greater discrepancy between annotated and unannotated tokens, are more likely to contain inconsistently labeled tokens. 3.2.2. 3 Supersense inventories have also been proposed for adjectives and prepositions (Tsvetkov et al., 2014; Schneider et al., 2015), though supersense annotations are limited to nouns and verbs in the data sets we use. 4 https://github.com/coastalcph/ supersense-data-twitter 5 A new, larger corpus that includes STREUSLE was compiled for the DiMSUM 2016 shared task on MWE identification and supersense tagging (Schneider et al., 2016): https://github.com/dimsum16/dimsum-data Weighted Discrepancy Ranking for Supersense Annotations A similar weighted discrepancy ranking method can be defined for supersense labels. A type S ∈ S is defined as a word form—noun or verb— paired with a single POS tag. For instance, in the Twitter c"
L16-1629,tsvetkov-etal-2014-augmenting-english,1,0.864974,"frequencies of annotated and not annotated occurrences is an indication that the type is inconsistently annotated. This discrepancy is weighted (scaled) by the number of times the MWE was annotated to put more weight on frequent multiword expressions. With the above measure, we rank types in descending order. The hypothesis is that types with greater annotated-MWE frequency overall, and greater discrepancy between annotated and unannotated tokens, are more likely to contain inconsistently labeled tokens. 3.2.2. 3 Supersense inventories have also been proposed for adjectives and prepositions (Tsvetkov et al., 2014; Schneider et al., 2015), though supersense annotations are limited to nouns and verbs in the data sets we use. 4 https://github.com/coastalcph/ supersense-data-twitter 5 A new, larger corpus that includes STREUSLE was compiled for the DiMSUM 2016 shared task on MWE identification and supersense tagging (Schneider et al., 2016): https://github.com/dimsum16/dimsum-data Weighted Discrepancy Ranking for Supersense Annotations A similar weighted discrepancy ranking method can be defined for supersense labels. A type S ∈ S is defined as a word form—noun or verb— paired with a single POS tag. For i"
L16-1629,ule-simov-2004-unexpected,0,0.244919,"nt family of methods considers variation n-grams—word sequences that receive different labelings in different parts of the corpus—and heuristically rank them to identify likely errors (Dickinson and Meurers, 2003; Boyd et al., 2007). Here, we consider ambiguous types, which in the case of supersenses are variation 1-grams. (For MWEs the analogy doesn’t hold as well because an n-gram of two or more words receives a label as a unit.) Most previous work on error detection in corpus annotation has focused on syntactic annotations—POS tags (Loftsson, 2009; Eskin, 2000; Ma et al., 2001) and parses (Ule and Simov, 2004; Kato and Matsubara, 2010)—rather than semantic annotations. Dickinson and Lee (2008) is one exception: the authors considered inconsistencies in annotations of predicate-argument structures. To our knowledge, no previous methods have been developed for inconsistencies in lexical semantic segmentation or tagging. Previous work has found benefit to considering context when determining whether an ambiguous expression is inconsistently annotated (Nakagawa and Matsumoto, 2002). For instance, Nguyen et al. (2015) applied an entropy-based scoring method that is similar to our entropy measure (3.3),"
L16-1629,R11-1040,0,0.123599,"Missing"
L16-1629,S16-1084,1,\N,Missing
L18-1005,P02-1040,0,0.107184,"4b; Anastasiou, 2010). Given the difficulty of idiom translation in MT, it would be helpful to have a method to evaluate idiom translation performance. There is a wide range of methods for evaluating the performance of MT systems, but none of them are satisfactory for the targeted evaluation of idiom translation. The most straightforward method is human evaluation. While human evaluation is highly valuable, it is desirable to develop complementary automatic methods that are low-cost and fast, thus allowing for more rapid and frequent feedback cycles. Popular automatic MT metrics such as BLEU (Papineni et al., 2002) are inexpensive, but are unsuitable for a targeted evaluation. This paper tries to fill this gap by presenting a method to assess the quality of idiom translations. We introduce a new method called “blacklist method” for performance evaluation on idioms, which is based on the intuition that a literal translation of the components of the idiom is likely to be wrong, and easy to spot by defining a blacklist of words that indicate a likely literal translation error. We perform a case study on a special class of Chinese idioms that typically consist of 4 characters, called “cheng2 1. Idiom transl"
L18-1005,W14-1007,0,0.169978,"on. In this research we will only focus on those semantically non-transparent words, which have different literal meanings and idiomatic meanings. We will subsequently refer to them as “Chinese idioms”. We also introduce the CIBB dataset1 for actually executing this evaluation on Chinese→English MT systems. Based on this dataset, we conduct experiments on a state-of-theart NMT system. From the experiments we draw the following conclusions: Idioms are a special figure of speech that are noncompositional and non-literal, though occasionally share surface realizations with literal language uses (Salton et al., 2014b). Idioms are considered highly problematic for a wide variety of NLP tasks (Sag et al., 2002). This belief also holds true for machine translation, because MT systems often make the assumption that meaning is compositional, which is not true for idioms. The compositionality assumption leads to literal translation errors, the word-byword translation of idioms, resulting in a translation that is confusing and not understandable. Therefore, idiom translation is a hard problem in MT and has attracted considerable research interest (Cap et al., 2015; Salton et al., 2014b; Anastasiou, 2010). Given"
L18-1005,W14-0806,0,0.175996,"on. In this research we will only focus on those semantically non-transparent words, which have different literal meanings and idiomatic meanings. We will subsequently refer to them as “Chinese idioms”. We also introduce the CIBB dataset1 for actually executing this evaluation on Chinese→English MT systems. Based on this dataset, we conduct experiments on a state-of-theart NMT system. From the experiments we draw the following conclusions: Idioms are a special figure of speech that are noncompositional and non-literal, though occasionally share surface realizations with literal language uses (Salton et al., 2014b). Idioms are considered highly problematic for a wide variety of NLP tasks (Sag et al., 2002). This belief also holds true for machine translation, because MT systems often make the assumption that meaning is compositional, which is not true for idioms. The compositionality assumption leads to literal translation errors, the word-byword translation of idioms, resulting in a translation that is confusing and not understandable. Therefore, idiom translation is a hard problem in MT and has attracted considerable research interest (Cap et al., 2015; Salton et al., 2014b; Anastasiou, 2010). Given"
L18-1005,P16-1162,1,0.245143,"Missing"
L18-1005,E17-2060,1,0.927475,"biguity, composition, function words, multi-word expressions and so on. (Burlot and Yvon, 2017) introduced a new scheme to evaluate the performance of English→MRL (morphologically rich languages) MT systems on morphological difficulties. The test suite they built consists of three parts, focusing on a system’s morphological adequacy (generating different morphological features in different contexts), fluency (word agreement) and certainty (generating the same morphological features in different contexts), respectively. Evaluation is based on automatic morphological analysis of the MT output. (Sennrich, 2017) proposed a method to construct the test suite automatically for evaluating English→German NMT systems on word agreement, polarity, transliteration, etc. The test suite is made up with minimal translation pairs, where a reference translation is paired with a contrastive translation which introduces a single translation error, allowing to measure the sensitivity of a neural MT (NMT) system towards this type of error. The score on the test suite is also obtained automatically by calculating the precision of the NMT system to assign a higher probability to the correct translation than to the cont"
L18-1005,2006.amta-papers.25,0,0.0701929,"special class of Chinese idioms that typically consist of 4 characters, called “cheng2 1. Idiom translation remains an open problem in Chinese→English NMT 2. Literal translation error is still a prevalent error type 3. The blacklist method is effective at detecting literal translation errors. 2. 2.1. Related Work Global Evaluation Metrics Global evaluation metrics are metrics that evaluate the overall performance of MT systems and allow automatically calculation. There are many well-known global evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), etc. However, these metrics only provide global evaluation and are unable to evaluate MT systems’ performance on specific aspects. Therefore, they are unsatisfactory in evaluating idiom translation performance. 1 This dataset is released https://github.com/sythello/CIBB-dataset 31 at 2.2. Test Suite Methods test suites and efficient evaluation. Test suite methods construct a set of sentences that focus on specific types of difficulties in MT. Typically, we design a set of sentences in the source language for the MT system to translate, and a scoring method to evaluate the translations. The s"
L18-1005,W05-0909,0,0.151617,"or. We perform a case study on a special class of Chinese idioms that typically consist of 4 characters, called “cheng2 1. Idiom translation remains an open problem in Chinese→English NMT 2. Literal translation error is still a prevalent error type 3. The blacklist method is effective at detecting literal translation errors. 2. 2.1. Related Work Global Evaluation Metrics Global evaluation metrics are metrics that evaluate the overall performance of MT systems and allow automatically calculation. There are many well-known global evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), etc. However, these metrics only provide global evaluation and are unable to evaluate MT systems’ performance on specific aspects. Therefore, they are unsatisfactory in evaluating idiom translation performance. 1 This dataset is released https://github.com/sythello/CIBB-dataset 31 at 2.2. Test Suite Methods test suites and efficient evaluation. Test suite methods construct a set of sentences that focus on specific types of difficulties in MT. Typically, we design a set of sentences in the source language for the MT system to translate, and a scoring method to evalu"
L18-1005,W17-4705,0,0.293701,"mance on specific issues. There are many previous works belonging to this category. (Isabelle et al., 2017) proposed a challenge set approach to evaluate English→French MT systems’ performance on divergence problems. The English sentences in the challenge set are chosen so that their closest French equivalent will be structurally divergent from them in some crucial way. (Burchardt et al., 2017) constructed a test suite for English→German MT systems. This test suite covers a wide variety of linguistic phenomena, such as ambiguity, composition, function words, multi-word expressions and so on. (Burlot and Yvon, 2017) introduced a new scheme to evaluate the performance of English→MRL (morphologically rich languages) MT systems on morphological difficulties. The test suite they built consists of three parts, focusing on a system’s morphological adequacy (generating different morphological features in different contexts), fluency (word agreement) and certainty (generating the same morphological features in different contexts), respectively. Evaluation is based on automatic morphological analysis of the MT output. (Sennrich, 2017) proposed a method to construct the test suite automatically for evaluating Engl"
L18-1005,W15-0903,0,0.0200246,"ace realizations with literal language uses (Salton et al., 2014b). Idioms are considered highly problematic for a wide variety of NLP tasks (Sag et al., 2002). This belief also holds true for machine translation, because MT systems often make the assumption that meaning is compositional, which is not true for idioms. The compositionality assumption leads to literal translation errors, the word-byword translation of idioms, resulting in a translation that is confusing and not understandable. Therefore, idiom translation is a hard problem in MT and has attracted considerable research interest (Cap et al., 2015; Salton et al., 2014b; Anastasiou, 2010). Given the difficulty of idiom translation in MT, it would be helpful to have a method to evaluate idiom translation performance. There is a wide range of methods for evaluating the performance of MT systems, but none of them are satisfactory for the targeted evaluation of idiom translation. The most straightforward method is human evaluation. While human evaluation is highly valuable, it is desirable to develop complementary automatic methods that are low-cost and fast, thus allowing for more rapid and frequent feedback cycles. Popular automatic MT me"
L18-1005,N10-1029,0,0.0181006,"chieves about half the BLEU score of the same system when applied to sentences that do not contain idioms. Among all the translation errors caused by idioms, literal translation errors are believed to be an important error type. (Manojlovic et al., 2017) demonstrated that literal translations predominate in the output of a phrase-based English↔Croatian MT system when translating sentences with idioms. According to our preliminary observations, literal translation errors also occur often in state-of-the-art Chinese→English NMT systems. In order to improve the performance of idiom translation, (Carpuat and Diab, 2010) investigate two strategies: treating idioms and multiword expressions as an atomic unit, and adding a phrase-level feature that identifies multiword expressions. They find that both strategies improve the translation of non-compositional expressions. (Salton et al., 2014b) propose a substitution method that replaces idioms in the source sentence with their literal meaning before translation; after translation, the translation of the literal meaning is replaced with a target language idiom, if possible. 1. Automatic construction, automatic evaluation: large 32 3. Blacklist Method and only need"
L18-1005,D17-1263,0,0.0868153,"t suite methods construct a set of sentences that focus on specific types of difficulties in MT. Typically, we design a set of sentences in the source language for the MT system to translate, and a scoring method to evaluate the translations. The sentence set and the scoring method are designed so that the score assigned to a system indicates the system’s performance on the focused difficulty. This kind of methods makes up for the drawbacks of global evaluation metrics that they cannot assess a system’s performance on specific issues. There are many previous works belonging to this category. (Isabelle et al., 2017) proposed a challenge set approach to evaluate English→French MT systems’ performance on divergence problems. The English sentences in the challenge set are chosen so that their closest French equivalent will be structurally divergent from them in some crucial way. (Burchardt et al., 2017) constructed a test suite for English→German MT systems. This test suite covers a wide variety of linguistic phenomena, such as ambiguity, composition, function words, multi-word expressions and so on. (Burlot and Yvon, 2017) introduced a new scheme to evaluate the performance of English→MRL (morphologically"
L18-1005,L16-1147,0,0.0268605,"on the list. The blacklist consists of the direct translation of the characters mentioned in the last step. 3. Gather source language (Chinese) sentences containing idioms on the list. Note that the method itself does not need reference translations. Nevertheless, if someone is not a speaker of the source language but wishes to get some ideas about the detected literal translation errors, or to check whether the detection is correct, then using translation pairs is more desirable than monolingual sentences. Translation Pairs The translation pairs were extracted from OpenSubtitles2016 dataset (Lison and Tiedemann, 2016), where we searched for Chinese→English translation pairs with idioms on our list. In order to balance the frequency of all the idioms in the translation pairs, preventing the majority being taken up by only a few idioms, we restricted the maximum occurrences of any idiom to be 40. Under such restrictions, we extracted a total of 1194 translation pairs. 4. Feed all the sentences to the MT system to get the translations. 5. Calculate the percentage of translations triggering the blacklist, which is the evaluation score for the system. We draw on an existing idiom list for step 1, and perform st"
L18-1547,P02-1050,0,0.142429,"e a discontinuous span of text and is therefore able to capture long range dependencies that other corpora do not take into account. Unlike other corpora, C ONAN D OYLE -N EG also annotates affixal negation (e.g. ‘impatient’) as well as the negated event, which allows for better coverage of the negated elements in a sentence. 2.2. Annotation projection To our knowledge, there has not been any previous work on projecting negation across languages. However previous studies have experimented with projecting semantic annotations via word-alignment information extracted from large parallel corpora.Hwa et al. (2002) used word alignment to project parses from English to Chinese and later improved the performance by implementing a set of linguistically-informed post-processing rules. Pad´o and Lapata (2009) used word alignment information in their constituent-based projection algorithm to transfer semantic role labels from English to German. 3. 3.1. Creating the corpus We built our annotated parallel corpus by aligning the four stories in C ONAN D OYLE -N EG (‘The Hound of the Baskervilles’, ‘The Adventure of Wisteria Lodge’, ‘The Adventure of the Cardboard Box’ and ‘The Adventure of the Red Circle’) to th"
L18-1547,konstantinova-etal-2012-review,0,0.0384578,"Missing"
L18-1547,morante-daelemans-2012-conandoyle,0,0.386939,"matics, University of Edinburgh ql261@cam.ac.uk f.fancellu@sms.ed.ac.uk bonnie@inf.ed.ac.uk Abstract Although the existence of English corpora annotated for negation has allowed for extensive work on monolingual negation detection, little is understood on how negation-related phenomena translate across languages. The current study fills this gap by presenting NegPar, the first English-Chinese parallel corpus annotated for negation in the narrative domain (a collection of stories from Conan Doyle’s Sherlock Holmes). While we followed the annotation guidelines in the C ONAN D OYLE -N EG corpus (Morante and Daelemans, 2012), we reannotated certain scope-related phenomena to ensure more consistent and interpretable semantic representation. To both ease the annotation process and analyze how similar negation is signaled in the two languages, we experimented with first projecting the annotations from English and then manually correcting the projection output in Chinese. Results show that projecting negation via word-alignment offers limited help to the annotation process, as negation can be rendered in different ways across languages. Keywords: negation, annotation projection, English, Chinese, parallel corpora, cr"
L18-1547,I05-3027,0,0.0401847,"u do not wish [...’ 4.1. Annotation Projection Methodology The goal of the annotation projection is to investigate whether we can ease the burden of annotating from scratch in the presence of parallel text. Annotations are projected using word alignment information computed by the IBM model 2, as implemented in the fast align toolkit (Dyer et al., 2013). The training data for the alignment model consists of the aligned sentence pairs in NegPar and the English-Chinese UN parallel corpus (Ziemski et al., 2016); the Chinese side of the corpus was also tokenized using the Stanford Word Segmenter (Tseng et al., 2005). We used the symmetrical two-way alignment results as the basis for projection. In our work, we experimented with two types of alignment models: (1).English word to Chinese word (word-level projection); (2).English word to Chinese character (characterlevel projection). An example of the former is shown in Fig. 2 where all elements are projected correctly, except for the scope projection of the subject ‘我’(‘I’). For both levels of projection, we report precision, recall, F1 measure and number of gold (both English and Chinese) and projected spans for cue, event and scope independently, as they"
L18-1547,W08-0606,0,0.328047,"Missing"
L18-1547,L16-1561,0,0.0251691,", and ‘相信’ , ‘to believe’, where we do not. 1. 我 知道 您 决 不 愿意 [...] I know you not want [...] ‘I know that you do not wish [...’ 4.1. Annotation Projection Methodology The goal of the annotation projection is to investigate whether we can ease the burden of annotating from scratch in the presence of parallel text. Annotations are projected using word alignment information computed by the IBM model 2, as implemented in the fast align toolkit (Dyer et al., 2013). The training data for the alignment model consists of the aligned sentence pairs in NegPar and the English-Chinese UN parallel corpus (Ziemski et al., 2016); the Chinese side of the corpus was also tokenized using the Stanford Word Segmenter (Tseng et al., 2005). We used the symmetrical two-way alignment results as the basis for projection. In our work, we experimented with two types of alignment models: (1).English word to Chinese word (word-level projection); (2).English word to Chinese character (characterlevel projection). An example of the former is shown in Fig. 2 where all elements are projected correctly, except for the scope projection of the subject ‘我’(‘I’). For both levels of projection, we report precision, recall, F1 measure and num"
L18-1547,P15-1064,0,0.0193899,"rts, stresses the importance of recognizing negation for information extraction from medical records; the SFU PRODUCT REVIEW CORPUS (Konstantinova et al., 2012) annotates negation on top of product reviews, acknowledging its importance for sentiment analysis tasks. Finally, C ONAN D OYLE -N EG (Morante and Daelemans, 2012) annotates negation in narrative texts – a collection of four stories from Conan Doyle’s Sherlock Holmes. There have also been some attempts in developing corpora annotated for negation in other languages as demonstrated by CN E S P (Chinese Negation and Speculation corpus) (Zou et al., 2015), which closely follows the annotation style of the B IO S COPE corpus. However, tailoring the annotation style to a specific domain leads these corpora to differ in what was annotated and how. For instance, the SFU and B IO S COPE corpora consider negation scope in purely syntactic terms (that we infer 3464 is ‘the maximum constituent c-commanded by the negation cue’), including only the tokens to the right of the cue and excluding the subject of the clause except in passive constructions. (2) exemplifies this. # sents # neg. sents # cues # events # scopes (2) a. B IO S COPE: It helps activat"
miltsakaki-etal-2004-penn,kingsbury-palmer-2002-treebank,0,\N,Missing
miltsakaki-etal-2004-penn,J93-2004,0,\N,Missing
miltsakaki-etal-2004-penn,W04-2703,1,\N,Missing
miltsakaki-etal-2004-penn,J03-4002,1,\N,Missing
miltsakaki-etal-2004-penn,P02-1045,0,\N,Missing
P09-1076,W01-1007,0,0.0518281,"t al., 2008). . . . we would probably not use the term “genre” to describe merely the class of 674 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 674–682, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP texts that have the objective of persuading someone to do something, since that class – which would include editorials, sermons, prayers, advertisements, and so forth – has no distinguishing formal properties (Kessler et al., 1997, p. 33). Such features are of similar interest in text processing – in particular, automated genre classification (Dewdney et al., 2001; Finn and Kushmerick, 2006; Kessler et al., 1997; Stamatatos et al., 2000; Wolters and Kirsten, 1999) – which relies on there being reliably detectable features that can be used to distinguish one class from another. This is where the caveat from (Kessler et al., 1997) becomes relevant: A particular genre shouldn’t be taken so broadly as to have no distinguishing features, nor so narrowly as to have no general applicability. But this still allows variability in what is taken to be a genre. There is no one “right set”. A balanced corpus like the Brown Corpus of American English or the British"
P09-1076,C00-2117,0,0.394856,"ibe merely the class of 674 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 674–682, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP texts that have the objective of persuading someone to do something, since that class – which would include editorials, sermons, prayers, advertisements, and so forth – has no distinguishing formal properties (Kessler et al., 1997, p. 33). Such features are of similar interest in text processing – in particular, automated genre classification (Dewdney et al., 2001; Finn and Kushmerick, 2006; Kessler et al., 1997; Stamatatos et al., 2000; Wolters and Kirsten, 1999) – which relies on there being reliably detectable features that can be used to distinguish one class from another. This is where the caveat from (Kessler et al., 1997) becomes relevant: A particular genre shouldn’t be taken so broadly as to have no distinguishing features, nor so narrowly as to have no general applicability. But this still allows variability in what is taken to be a genre. There is no one “right set”. A balanced corpus like the Brown Corpus of American English or the British National Corpus, will sample texts from different genres, to give a repres"
P09-1076,D07-1010,0,0.0203262,"k (hereafter, PDTB) in Section 4, Sections 5 and 6 show that these four genres display differences in connective frequency and in terms of the senses associated with intra-sentential connectives (eg, subordinating conjunctions), inter-sentential connectives (eg, inter-sentential coordinating conjunctions) and those inter-sentential relations that are not lexically marked. Section 7 considers recent efforts to induce effective procedures for automated sense labelling of discourse relations that are not lexically marked (Elwell and Baldridge, 2008; Marcu and Echihabi, 2002; Pitler et al., 2009; Wellner and Pustejovsky, 2007; Wellner, 2008). It makes two points. First, because genres differ from each other in the senses associated with such relations, genre should be made a factor in their automated sense labelling. Secondly, because different senses are being conveyed when a relation is lexically marked than when it isn’t, lexically marked relations provide a poor model for automated sense labelling of relations that are not lexically marked. Articles in the Penn TreeBank were identified as being reviews, summaries, letters to the editor, news reportage, corrections, wit and short verse, or quarterly profit repo"
P09-1076,P97-1005,0,0.388399,"Missing"
P09-1076,E99-1019,0,0.60095,"74 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 674–682, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP texts that have the objective of persuading someone to do something, since that class – which would include editorials, sermons, prayers, advertisements, and so forth – has no distinguishing formal properties (Kessler et al., 1997, p. 33). Such features are of similar interest in text processing – in particular, automated genre classification (Dewdney et al., 2001; Finn and Kushmerick, 2006; Kessler et al., 1997; Stamatatos et al., 2000; Wolters and Kirsten, 1999) – which relies on there being reliably detectable features that can be used to distinguish one class from another. This is where the caveat from (Kessler et al., 1997) becomes relevant: A particular genre shouldn’t be taken so broadly as to have no distinguishing features, nor so narrowly as to have no general applicability. But this still allows variability in what is taken to be a genre. There is no one “right set”. A balanced corpus like the Brown Corpus of American English or the British National Corpus, will sample texts from different genres, to give a representative view of how the lan"
P09-1076,P02-1047,0,0.0727821,"rief introduction to the Penn Discourse TreeBank (hereafter, PDTB) in Section 4, Sections 5 and 6 show that these four genres display differences in connective frequency and in terms of the senses associated with intra-sentential connectives (eg, subordinating conjunctions), inter-sentential connectives (eg, inter-sentential coordinating conjunctions) and those inter-sentential relations that are not lexically marked. Section 7 considers recent efforts to induce effective procedures for automated sense labelling of discourse relations that are not lexically marked (Elwell and Baldridge, 2008; Marcu and Echihabi, 2002; Pitler et al., 2009; Wellner and Pustejovsky, 2007; Wellner, 2008). It makes two points. First, because genres differ from each other in the senses associated with such relations, genre should be made a factor in their automated sense labelling. Secondly, because different senses are being conveyed when a relation is lexically marked than when it isn’t, lexically marked relations provide a poor model for automated sense labelling of relations that are not lexically marked. Articles in the Penn TreeBank were identified as being reviews, summaries, letters to the editor, news reportage, correc"
P09-1076,D08-1020,0,0.0543637,"Missing"
P09-1076,C08-2022,0,0.0130901,"the future.) With respect to explicit intra-sentential connectives, the main point of interest in Figure 4 is that SUMMARIES display a significantly lower density of intra-sentential connectives overall than the other three genres, as well as a significantly lower relative frequency of intra-sentential discourse adverbials. As the next section will show, these intra-sentential connectives, while few, are selected most often to express C ONTRAST and situations changing over time, reflecting the nature of SUMMARIES as regular periodic summaries of a changing world. 6 Connective Sense by Genre (Pitler et al., 2008) show a difference across Level 1 senses (C OMPARISON, C ONTINGENCY, T EM PORAL and E XPANSION ) in the PDTB in terms of their tendency to be realised by explicit connectives (a tendency of C OMPARISON and T EMPO RAL relations) or by Implicit Connectives (a tendency of C ONTINGENCY and E XPANSION). Here 7 Automated Sense Labelling of Discourse Connectives The focus here is on automated sense labelling of discourse connectives (Elwell and Baldridge, 2008; Marcu and Echihabi, 2002; Pitler et al., 2009; Wellner and Pustejovsky, 2007; Wellner, 679 Genre ESSAYS SUMMARIES LETTERS NEWS Total Sentence"
P09-1076,P09-1077,0,0.0228792,"enn Discourse TreeBank (hereafter, PDTB) in Section 4, Sections 5 and 6 show that these four genres display differences in connective frequency and in terms of the senses associated with intra-sentential connectives (eg, subordinating conjunctions), inter-sentential connectives (eg, inter-sentential coordinating conjunctions) and those inter-sentential relations that are not lexically marked. Section 7 considers recent efforts to induce effective procedures for automated sense labelling of discourse relations that are not lexically marked (Elwell and Baldridge, 2008; Marcu and Echihabi, 2002; Pitler et al., 2009; Wellner and Pustejovsky, 2007; Wellner, 2008). It makes two points. First, because genres differ from each other in the senses associated with such relations, genre should be made a factor in their automated sense labelling. Secondly, because different senses are being conveyed when a relation is lexically marked than when it isn’t, lexically marked relations provide a poor model for automated sense labelling of relations that are not lexically marked. Articles in the Penn TreeBank were identified as being reviews, summaries, letters to the editor, news reportage, corrections, wit and short"
P09-1076,prasad-etal-2008-penn,1,0.952544,"ying that a genre should not be so broad that the texts belonging to it don’t share any distinguishing properties — Introduction It is well-known that texts differ from each other in a variety of ways, including their topic, the reading level of their intended audience, and their intended purpose (eg, to instruct, to inform, to express an opinion, to summarize, to take issue with or disagree, to correct, to entertain, etc.). This paper considers differences in texts in the wellknown Penn TreeBank (hereafter, PTB) and in particular, how these differences show up in the Penn Discourse TreeBank (Prasad et al., 2008). . . . we would probably not use the term “genre” to describe merely the class of 674 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 674–682, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP texts that have the objective of persuading someone to do something, since that class – which would include editorials, sermons, prayers, advertisements, and so forth – has no distinguishing formal properties (Kessler et al., 1997, p. 33). Such features are of similar interest in text processing – in particular, automated genre classification (Dewdney et a"
P09-1076,W01-1605,0,\N,Missing
P10-5003,W09-3006,0,0.0251236,"Missing"
P10-5003,W09-3029,0,\N,Missing
P10-5003,N04-1015,0,\N,Missing
P10-5003,J08-1001,0,\N,Missing
P10-5003,prasad-etal-2008-penn,1,\N,Missing
P13-1163,P09-2082,1,0.859812,"Missing"
P13-1163,D07-1074,0,0.0152147,") trained over 2013 annotated utterances. Once the question has been typed, QA proceeds to focus detection also using machine learning techniques (Mikhailsian et al., 2009). Detected foci include possibly anaphoric expressions (“Who was he?”, “Tell me more about the castle”). These expressions are resolved against the dialogue history and geographical context. QA then proceeds to a textual search on texts from the Gazetteer of Scotland (Gittings, 2012) and Wikipedia, and definitions from WordNet glosses. The task is similar to TAC KBP 2013 Entity Linking Track and named entity disambiguation (Cucerzan, 2007). Candidate answers are reranked using a trained confidence score with the top candidate used as the final answer. These are usually long, descriptive answers and are provided as a flow of sentence chunks that the user can interrupt (see table 2). The Interaction Manager queries the QA model and pushes information when a salient PoI is in the vicinity of the user. “Edinburgh’s most famous and historic thoroughfare, which has formed the heart of the Old Town since mediaeval times. The Royal Mile includes Castlehill, the Lawnmarket, the Canongate and the Abbey Strand, but, is officially known si"
P13-1163,P11-2115,0,0.0276641,"Missing"
P13-1163,W11-2830,1,0.844185,"c, mix navigation with exploration. But such apps present information primarily visually on the screen for the user to read. Some of these are available for download at the Google Play Android app store1 . Several dialogue and natural language systems have addressed the issue 1 of pedestrian navigation (Malaka and Zipf, 2000; Raubal and Winter, 2002; Dale et al., 2003; Bartie and Mackaness, 2006; Shroder et al., 2011; Dethlefs and Cuay´ahuitl, 2011). There has also been recent interest in shared tasks for generating navigation instructions in indoor and urban environments (Byron et al., 2007; Janarthanam and Lemon, 2011). Some dialogue systems deal with presenting information concerning points of interest (Ko et al., 2005; Kashioka et al., 2011) and interactive question answering (Webb and Webber, 2009). In contrast, Spacebook has the objective of keeping the user’s cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, it allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1). 3 Architecture The architecture of the Spacebook system is shown"
P13-1163,W12-1619,1,0.816848,"wering (Webb and Webber, 2009). In contrast, Spacebook has the objective of keeping the user’s cognitive load low and preventing users from being distracted (perhaps dangerously so) from walking in the city (Kray et al., 2003). Also, it allows users to interleave the two sub-tasks seamlessly and can keep entities discussed in both tasks in shared context (as shown in Table 1). 3 Architecture The architecture of the Spacebook system is shown in figure 1. Our architecture brings together Spoken Dialogue Systems (SDS), Geographic Information Systems (GIS) and QuestionAnswering (QA) technologies (Janarthanam et al., 2012). Its essentially a spoken dialogue system (SDS) consisting of an automatic speech recogniser (ASR), a semantic parser, an Interaction Manager, an utterance generator and a text-tospeech synthesizer (TTS). The GIS modules in this architecture are the City Model, the Visibility Engine, and the Pedestrian tracker. Users communicate with the system using a smartphone-based client app (an Android app) that sends users’ position, pace rate, and spoken utterances to the system, and delivers synthesised system utterances to the user. Figure 1: System Architecture https://play.google.com/store 1661 3."
P16-1047,S12-1045,0,0.221049,"Missing"
P16-1047,S12-1037,0,0.418202,"Missing"
P16-1047,E14-1063,1,0.872253,"Missing"
P16-1047,W15-1301,1,0.817692,"Neural Networks For Negation Scope Detection Federico Fancellu and Adam Lopez and Bonnie Webber School of Informatics University of Edinburgh 11 Crichton Street, Edinburgh f.fancellu[at]sms.ed.ac.uk, {alopez,bonnie}[at]inf.ed.ac.uk Abstract given the importance of recognizing negation for information extraction from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we"
P16-1047,S12-1036,0,0.127179,"Missing"
P16-1047,S12-1043,0,0.324713,"Missing"
P16-1047,S12-1038,0,0.245613,"Missing"
P16-1047,S12-1042,0,0.566171,"Missing"
P16-1047,S12-1040,0,0.190353,"r information extraction from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we investigate whether neural network based sequence-tosequence models (§ 4) are a valid alternative. The first advantage of neural networks-based methods for NLP is that we could perform classification by means of unsupervised word-embeddings features only, under the assumption that"
P16-1047,S12-1035,0,0.0336994,"tences respectively. If a sentence contains multiple negation instances, we create as many copies as the number of instances. If the sentence contains a morphological cue (e.g. impatient) we split it into affix (im-) and root (patient), and consider the former as cue and the latter as part of the scope. Both neural network architectures are implemented using TensorFlow (Abadi et al., 2015) with a 200-units hidden layer (400 in total for two concatenated hidden layers in the BiLSTM), the Adam optimizer (Kingma and Ba, 2014) with a 5 For the statistics regarding the data, we refer the reader to Morante and Blanco (2012). 498 dings are pre-trained using external data. We experimented with both keeping the wordembedding matrix fixed and updating it during training but we found small or no difference between the two settings. To do this, we train a word-embedding matrix using Word2Vec (Mikolov et al., 2013) on 770 million tokens (for a total of 30 million sentences and 791028 types) from the ‘One Billion Words Language Modelling’ dataset 6 and the Sherlock Holmes data (5520 sentences) combined. The dataset was tokenized and morphological cues split into negation affix and root to match the Conan Doyle’s data. I"
P16-1047,P14-1007,0,0.239289,"on from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we investigate whether neural network based sequence-tosequence models (§ 4) are a valid alternative. The first advantage of neural networks-based methods for NLP is that we could perform classification by means of unsupervised word-embeddings features only, under the assumption that they also encode struct"
P16-1047,S15-1008,0,0.015296,", given a negative instance, to identify which tokens are affected by negation (§2). As shown in (1), only the first clause is negated and therefore we mark he and the car, along with the predicate was driving as inside the scope, while leaving the other tokens outside. (1) He was not driving the car and she left to go home. 1. Comparable or better performance: We show that neural networks perform on par with previously developed classifiers, with a bi-directional LSTM outperforming them In the BioMedical domain there is a long line of research around the topic (e.g. Velldal et al. (2012) and Prabhakaran and Boguraev (2015)), 495 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 495–504, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics when tested on data from the same genre. [I, do, not, love, you, and, you, are, no, longer, invited]; in (3a), the vector c1 is 1 only at index 3 (w2 =‘not’), while in (3b) c2 is 1 at position 9, 10 (where w9 w10 = ‘no longer’); finally the vectors s1 and s2 are I only at the indices of the words underlined and O anywhere else. 2. Better understanding of the problem: We analyze in more detail the diff"
P16-1047,S12-1041,0,0.540142,"k, {alopez,bonnie}[at]inf.ed.ac.uk Abstract given the importance of recognizing negation for information extraction from medical records. In more general domains, efforts have been more limited and most of the work centered around the *SEM2012 shared task on automatically detecting negation (§3), despite the recent interest (e.g. machine translation (Wetzel and Bond, 2012; Fancellu and Webber, 2014; Fancellu and Webber, 2015)). The systems submitted for this shared task, although reaching good overall performance are highly feature-engineered, with some relying on heuristics based on English (Read et al. (2012)) or on tools that are available for a limited number of languages (e.g. Basile et al. (2012), Packard et al. (2014)), which do not make them easily portable across languages. Moreover, the performance of these systems was only assessed on data of the same genre (stories from Conan Doyle’s Sherlock Holmes) but there was no attempt to test the approach on data of different genre. Given these shortcomings, we investigate whether neural network based sequence-tosequence models (§ 4) are a valid alternative. The first advantage of neural networks-based methods for NLP is that we could perform clas"
P16-1047,J12-2005,0,0.0440514,"scope of negation, that is, given a negative instance, to identify which tokens are affected by negation (§2). As shown in (1), only the first clause is negated and therefore we mark he and the car, along with the predicate was driving as inside the scope, while leaving the other tokens outside. (1) He was not driving the car and she left to go home. 1. Comparable or better performance: We show that neural networks perform on par with previously developed classifiers, with a bi-directional LSTM outperforming them In the BioMedical domain there is a long line of research around the topic (e.g. Velldal et al. (2012) and Prabhakaran and Boguraev (2015)), 495 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 495–504, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics when tested on data from the same genre. [I, do, not, love, you, and, you, are, no, longer, invited]; in (3a), the vector c1 is 1 only at index 3 (w2 =‘not’), while in (3b) c2 is 1 at position 9, 10 (where w9 w10 = ‘no longer’); finally the vectors s1 and s2 are I only at the indices of the words underlined and O anywhere else. 2. Better understanding of the problem"
P16-1047,W12-4203,0,0.0405733,"Missing"
P16-1047,S12-1044,0,0.276556,"antic parser cannot create a reliable representation for a sentence, the system ‘backs-off’ to the hybrid model of Read et al. (2012), which uses syntactic information instead. This system suffers however from the same shortcomings mentioned above, in particular, given that MRS representation can only be built for a small set of languages. 4 For more details on LSTM and related mathematical formulations, we refer to reader to Hochreiter and Schmidhuber (1997) 497 Closed track *SEM2012 Open track UiO1 (Read et al., 2012) UiO2 (Lapponi et al., 2012) FBK (Chowdhury and Mahbub, 2012) UWashington (White, 2012) UMichigan (Abu-Jbara and Radev, 2012) UABCoRAL (Gyawali and Solorio, 2012) UiO2 (Lapponi et al., 2012) UGroningen (Basile et al., 2012) UCM-1 (de Albornoz et al., 2012) UCM-2 (Ballesteros et al., 2012) Packard et al. (2014) Method heuristics + SVM CRF CRF CRF CRF SVM CRF rule-based rule-based rule-based heuristics + SVM Scope tokens3 Prec. Rec. F1 81.99 88.81 85.26 86.03 81.55 83.73 81.53 82.44 81.89 83.26 83.77 83.51 84.85 80.66 82.70 85.37 68.86 76.23 82.25 82.16 82.20 69.20 82.27 75.15 85.37 68.53 76.03 58.30 67.70 62.65 86.1 90.4 88.2 Exact scope match Prec. Rec. F1 87.43 61.45 72.17 85.7"
P18-1210,W17-0812,0,0.0231798,"rature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals multiple discourse relations as holding over a text – for example, since as a subordinating conjunction may, in particular contexts, signal both a TEMPORAL relation and a CAUSAL relation, rather than just one or the other (Miltsakaki et al., 2005). We are aware of only two resources that allow more than one discourse relation to be annotated between two segments – the Penn Discourse TreeBank (PDTB; Prasad et al., 2008, 2014) and, more recently, the BECauSE Corpus 2.0 (Dunietz et al., 2017). The PDTB allows multiple discourse relations of the third and fourth types noted above. It also allows them to be annotated if there is no explicit connective between a pair of segments but annotators see more than one sense relation as linking them, as in the following variant of (4): (5) It’s too far to walk. Let’s take the bus. Here a RESULT relation can be associated with an implicit token of so between the clauses, while a SUBSTITUTION relation can be associated with an implicit token of instead. The above are the main cases in which PDTB annotates multiple relations. Relevant to this p"
P18-1210,P12-1007,0,0.0215427,"ken of so between the clauses, while a SUBSTITUTION relation can be associated with an implicit token of instead. The above are the main cases in which PDTB annotates multiple relations. Relevant to this paper, the PDTB does not annotate implicit conjunction relations where there is already an explicit discourse adverbial. Thus the PDTB would either ignore the implicit RESULT relation for (1) or (incorrectly) annotate instead in (1) as conveying both SUBSTITUTION and RESULT. Moreover, while the PDTB has been used in training many (but not all) discourse parsers (Marcu, 2000; Lin et al., 2014; Feng and Hirst, 2012; Xue et al., 2015, 2016; Ji and Eisenstein, (3) a. George Bush supports big business. b. He’s sure to veto House Bill 1711. 2014), discourse parsing has for the most part igAt the level of intentions, (3a) aims to provide EVI - nored its annotations of multiple concurrent relations between clauses, except in the case of distinct DENCE for the claim in (3b), while at an informational level, (3a) serves as the CAUSE of the situa- explicit connectives expressing distinct relations. Instead, they have arbitrarily taken just a single retion in (3b). RST would force annotators to choose lation to h"
P18-1210,J86-3001,0,0.79163,"ice, researchers working in the RST framework standardly produce a single analysis of a text, with a single relational labeling, selecting the analysis that is “most plausible in terms of the perceived goals of the writer” (Mann et al., 1989, pp. 34–35). If that single analysis is later mapped into a different structure to support further processing – e.g., a binary branching tree structure – the mapping does not change the chosen relational labeling. Multiple relations may additionally hold in theories of discourse coherence that posit multiple levels of text analysis. For example, following Grosz and Sidner (1986), Moore and Pollack (1992) characterized text as having both an informational structure (relating information conveyed by discourse segments) and an intentional structure (relating the functions of those segments with respect to what the speaker is trying to accomplish through the text). The kinds of relations at the two levels are different, as can be seen in the following example from (Moore and Pollack, 1992, p. 540): Finally, a fourth way in which the previous literature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals"
P18-1210,P14-1002,0,0.0589878,"Missing"
P18-1210,prasad-etal-2008-penn,1,0.900776,"ollack, 1992, p. 540): Finally, a fourth way in which the previous literature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals multiple discourse relations as holding over a text – for example, since as a subordinating conjunction may, in particular contexts, signal both a TEMPORAL relation and a CAUSAL relation, rather than just one or the other (Miltsakaki et al., 2005). We are aware of only two resources that allow more than one discourse relation to be annotated between two segments – the Penn Discourse TreeBank (PDTB; Prasad et al., 2008, 2014) and, more recently, the BECauSE Corpus 2.0 (Dunietz et al., 2017). The PDTB allows multiple discourse relations of the third and fourth types noted above. It also allows them to be annotated if there is no explicit connective between a pair of segments but annotators see more than one sense relation as linking them, as in the following variant of (4): (5) It’s too far to walk. Let’s take the bus. Here a RESULT relation can be associated with an implicit token of so between the clauses, while a SUBSTITUTION relation can be associated with an implicit token of instead. The above are the"
P18-1210,J14-4007,1,0.889918,"Missing"
P18-1210,W15-2703,1,0.899234,"symptomatic of participant naïveté or bias, but arise reliably from the concurrent availability of multiple relations between segments – some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing. 1 Introduction A question that remains unresolved in work on discourse coherence is the nature and number of relations that can hold between clauses in a coherent text (Halliday and Hasan, 1976; Stede, 2012). Our earlier work (Rohde et al., 2015, 2016) showed that, in the presence of explicit discourse adverbials, people also infer additional discourse relations that they take to hold jointly with those associated with the adverbials. For example, in: best expressed how they took the segments to be related. Rohde et al. (2017) also asked participants to select any other conjunctions that they took to convey the same sense as their “best” choice. (More details of these experiments are given in Section 3.) All three studies showed participants selecting conjunctions whose sense differed from that of the explicit discourse adverbial. Bu"
P18-1210,W16-1707,1,0.547365,"junction(s) that best expressed how the two segments link together. The presentation of conjunction choices varied in order for each participant, but always consisted of AND, BECAUSE, BUT, OR, SO, NONE. While the task admittedly encourages participants to select one (or more) conjunctions, our prior work has shown that participants are very willing to use NONE if no conjunction is appropriate. We there4 Datasets fore take their insertion of a conjunction as their endorsement of the relation signaled by that conjunc- 4.1 In other words Dataset tion. To further control data quality, we included Rohde et al. (2016) report an OR∼SO response 6 catch trials with an expected correct conjunction split for in other words when participants could inlike “To be ______ not to be”. sert only their top choice of conjunction. Figure 1 Three of the explicit discourse adverbials that we chose are anaphoric: in other words, other- shows SO dominating participants’ choice in all cases, but OR showing up among their choices in wise, and instead (Webber et al., 2000). Unlike all but one passage (leftmost vertical bar). Addiconjunctions such as AND, BECAUSE, BUT, OR tionally, several passages elicited BUT as the top and SO"
P18-1210,W17-6814,1,0.933186,"on discourse coherence and lead to higher levels of performance in discourse parsing. 1 Introduction A question that remains unresolved in work on discourse coherence is the nature and number of relations that can hold between clauses in a coherent text (Halliday and Hasan, 1976; Stede, 2012). Our earlier work (Rohde et al., 2015, 2016) showed that, in the presence of explicit discourse adverbials, people also infer additional discourse relations that they take to hold jointly with those associated with the adverbials. For example, in: best expressed how they took the segments to be related. Rohde et al. (2017) also asked participants to select any other conjunctions that they took to convey the same sense as their “best” choice. (More details of these experiments are given in Section 3.) All three studies showed participants selecting conjunctions whose sense differed from that of the explicit discourse adverbial. But Rohde et al. (2015, 2016) also showed participants often selecting conjunctions that signal different coherence relations than those selected by other participants. And Rohde et al. (2017) showed participants often identifying very different conjunctions as conveying the same meaning."
P18-1210,W17-0803,0,0.0383961,"Missing"
P18-1210,W13-0124,1,0.798155,"re asked to select which of three options they took to be a valid paraphrase of the passage. Each use of otherwise was assigned a distinct paraphrase to link the left-hand and right-hand segments (LHS, RHS). 28 instead 21 14 7 0 none other so or but before because and Figure 3: Stacked bar chart for participants’ (N=28) conjunction insertions in instead passages (Rohde et al., 2016) We also allowed participants to choose a second paraphrase if they thought it appropriate. instead simply conveys that what follows is an alternative to an unrealised situation in the context (Prasad et al., 2008; Webber, 2013). The current experiment tests the hypothesis that this BUT∼SO split is a consequence of inference from properties of the segments themselves. To test this hypothesis, we created 16 minimal pairs of passages containing instead, one of which emphasized the information structural parotherwise none 28 allelism between the clauses, as in (13a), and another other variant (13b) that de-emphasized that par21 so by a allelism in favor of a causal link implied or 14 downward-entailing construction such as too X but (Webber, 2013). For each passage, half the particibefore 7 pants saw the parallelism var"
P18-1210,J92-4007,0,0.794365,"in the RST framework standardly produce a single analysis of a text, with a single relational labeling, selecting the analysis that is “most plausible in terms of the perceived goals of the writer” (Mann et al., 1989, pp. 34–35). If that single analysis is later mapped into a different structure to support further processing – e.g., a binary branching tree structure – the mapping does not change the chosen relational labeling. Multiple relations may additionally hold in theories of discourse coherence that posit multiple levels of text analysis. For example, following Grosz and Sidner (1986), Moore and Pollack (1992) characterized text as having both an informational structure (relating information conveyed by discourse segments) and an intentional structure (relating the functions of those segments with respect to what the speaker is trying to accomplish through the text). The kinds of relations at the two levels are different, as can be seen in the following example from (Moore and Pollack, 1992, p. 540): Finally, a fourth way in which the previous literature has taken multiple discourse relations to hold is when a single phrase or lexico-syntactic construction jointly signals multiple discourse relatio"
P81-1021,P81-1022,0,0.0294935,"Missing"
P84-1029,P83-1007,1,\N,Missing
P87-1021,P87-1001,0,0.474301,"successful. aexcept for ""pronouns of laziness"" which can evoke and specify new entities through the use of previous dascriptions 149 be gained by pushing the analogy between tense and definite NPs. 3. T e m p o r a l F o c u s In this section, I give a more specific account of how the discourse interpretation of tense relates to e/s structure construction. As I noted above, a definite NP can specify an entity &apos;strongly&apos; associated with its antecedent. One might thus consider what is &apos;strongly&apos; associated with an event. One answer to this question appears in two separate papers in this volume [8, 13], each ascribing a tripartite structure to the way we view and talk about events. This structure consists of a preparatory phase, a culmination, and a consequence phase, to use the terminology of [8]. (Such a structure is proposed, in part, to give a uniform account of how the interpretation of temporal adverbials interacts with the interpretation of tense and aspect.) At any point N in the discourse, there is one node of e/s structure that provides a context for the interpretation of the RT of the next ctause. I will call it the temporal focus or TF. There are three possibilities: (1) the FIT"
P87-1021,P87-1003,0,0.102903,"successful. aexcept for ""pronouns of laziness"" which can evoke and specify new entities through the use of previous dascriptions 149 be gained by pushing the analogy between tense and definite NPs. 3. T e m p o r a l F o c u s In this section, I give a more specific account of how the discourse interpretation of tense relates to e/s structure construction. As I noted above, a definite NP can specify an entity &apos;strongly&apos; associated with its antecedent. One might thus consider what is &apos;strongly&apos; associated with an event. One answer to this question appears in two separate papers in this volume [8, 13], each ascribing a tripartite structure to the way we view and talk about events. This structure consists of a preparatory phase, a culmination, and a consequence phase, to use the terminology of [8]. (Such a structure is proposed, in part, to give a uniform account of how the interpretation of temporal adverbials interacts with the interpretation of tense and aspect.) At any point N in the discourse, there is one node of e/s structure that provides a context for the interpretation of the RT of the next ctause. I will call it the temporal focus or TF. There are three possibilities: (1) the FIT"
P87-1021,J86-3001,0,0.0575504,"147 Heuristic for returning and resuming the current narrative. The need for each of these is shown by example. f.uture perfect: ST<ET<RT In Section 4, I show that relative temporal adverbials display the same anaphoric property as simple tense. That it is RT that it is interpreted anaphorically, and not either El"" or tense as a whole can be seen by considering Example 3. John will have climbed Mt. McKinley. That the interpretation of tense should be entwined with discourse structure in this way should not come as a surprise, as a similar thing has been found true of other discourse anaphora [5]. 2. Tense as .Example 3 John went to the hospital. He had twisted his ankle on a patch of ice. It is not the El"" of John&apos;s twisting his ankle that is interpreted anaphorically with respect to his going to the hospital. Rather, it is the RT of the second clause: its ET is interpreted as prior to that because the clause is in the past perfect tense (see above). Anaphor Tense does not seem prima facie anaphoric: an isolated sentence like ""John went to bed"" or ""1 met a man who looked like a basset hound = appears to make sense without previously establishing when it happened. On the other hand, i"
P87-1021,P86-1003,0,0.172069,"trongest case, claiming that pronouns and tense display the same range of antecedent-anaphor linkages: Example 2 After he finished his chores, John went to bed. John partied until 3arn. He came home and went to bed. Oeictic Antecedents pro: She left reel (said by a man crying on the stoop) s tense: I left the oven onl (said by a man to his wife in the car) In each case, John&apos;s going to bed is linked to an explictly mentioned time or event. This linkage is the anaphoric property of tense that previous authors have described. Indefinite Antecedents Hinrichs[6] and Bauerle[1], following McCawley [7] and Partee [11], showed that it is not tense per se that is interpreted anaphorically, but that part of tense called by Reichenbach [14] reference time. 4 According to Reichenbach, the interpretation of tense requires three notions: speech time (ST), event time lET), and reference time (RT). RT is the time from which the event/situation described in the sentence is viewed. It may be the same as ST, as in pro: I bought a banana. I took it home with me. tense: I bought a banana. I took it home with me. <1 took it home after I bought it.&gt; Bound Variables pro: Every man thinks he is a genius. ten"
P87-1021,J88-2004,0,\N,Missing
P87-1021,E87-1042,0,\N,Missing
P88-1014,P84-1055,0,0.100872,"e these theories with a uniform way of explaining what it is that noun phrases (NP) and pronouns in a discourse refer to. Some NPs evoke a new discourse entity in the listener's evolving model of the discourse (which I have called simply a discourse model), others refer to ones that are already there. Such entities may correspond to something in the outside world, but they do not have to. To avoid confusion with a sense of ""referring in the outside world"", I will use the 113 DS-k DS-kl DS-k2 os,- DS-k21 D7 I DS-k21 | sj DS-k2 1 1Z / * DS-k2i J I DS-k21 j 5j+l Figure 1. Discourse Segrnentation [2] and Rachel Reichman [15])have discussed problems inherent in this discourse parsing task, among which is the lack of precise definition of its basic building block. While discourse segment is usually deemed recursively, theories differ in what they take the minimal segment to be. Hobhs takes it to be a sentence, and Polanyi [12], a clause. Grosz & Sidner do not state explicitly how much is needed to express a single purpose, but from their examples, it appears to be a single sentence as wen. (Unlike Hohbs and Polanyi, Grosz & Siduer do not consider every sentence to be a discourse segment per"
P88-1014,P87-1023,0,0.034433,"Missing"
P88-1014,P88-1012,0,0.0269421,"se understanding have adopted in at least some form. The In'st is the discourse entity, first introduced by Lauri Karmunen in 1976 (under the name ""discourse referent"") [9] and employed (under various other names) by many researchers, including myself [18]. The other is the discourse segment. What is taken to unify a segment is different in different theories: fox example, among computational linguists, Grosz & Sidner [5] take a discourse segment to be a chunk of text that expresses a common purpose (what they have called a discourse segment purpose) with respect to the speaker's plans; Hobbs [8] takes a discourse segment to be a chunk of text that has a common meaning; while Nakhimovsky [12], considering only narrative, takes a discourse segment to be a chunk of text that describes a single event from a single perspective. Discourse entities provide these theories with a uniform way of explaining what it is that noun phrases (NP) and pronouns in a discourse refer to. Some NPs evoke a new discourse entity in the listener's evolving model of the discourse (which I have called simply a discourse model), others refer to ones that are already there. Such entities may correspond to somethi"
P88-1014,J88-2003,0,0.0693163,"abilities are unaffected by locally confined damage to the brain. For example, binocular stereo fusion is known to take place in a specific area of the cortex near the back of the head. Patients with damage to this area of the cortex have visual handicaps but show no obvious impairment in their ability to think. This .... Having taken the initial step of interpreting a pronoun as pointing to the representation of a discourse segment, the proposed process must then be At this point in the discourse, there are several things that this can be taken as specifying. 117 able to further c o e r c e [8,11] that interpretation to be some property of the discourse segment representation or to some entity within it. Example 6 (above) illustrates the first type of coercion, Example 8, the latter. On the other hand, what appears to be an additional ambiguity in resolving this/that may not be one at all That is, a listener who is asked what a given this/that refersm to must describe the representation that s/he has created. This act of description is subject to alot of variability. For example, given a segment in which a statement A is supported by several pieces of evidence {B,C,D}, the listener mig"
P88-1014,J88-2004,0,\N,Missing
P88-1014,J86-3001,0,\N,Missing
P88-1014,C69-7001,0,\N,Missing
P88-1014,C69-6902,0,\N,Missing
P92-1013,P92-1016,0,0.0138999,"egration to be effective, we argue that it must be based on a representation of events that captures people's uncertainty about their outcome - in particular, people's incomplete expectations about the changes effected by events. An understanding system can then use these expectations to accommodate [15] the particular changes that are mentioned in subsequent discourse (Section 3). In Section 4, we discuss our initial implementation of these ideas. This work is being carried out as part of a project ( A n l m N L ) aimed at creating animated task simulations from Natural Language instructions [2; 4; 5; 6; 7; 14; 20]. Instructions are a form of text rich in the specification of events intended to alter the world in some way. Because of this, the issues discussed in this paper are particularly important to both understanding and generating instructions. Introduction Consider the following example: Example 1 John made a handbag from an inner-tube. a. He sold it for twenty dollars. b. *He sold them for fifty dollars. c. He had taken it from his brother's car. d. Neither of them was particularly useful. Here two entities are introduced via indefinite noun phrases (NPs) in the first sentence. The alternative f"
P92-1013,C92-4181,0,0.0110937,"egration to be effective, we argue that it must be based on a representation of events that captures people's uncertainty about their outcome - in particular, people's incomplete expectations about the changes effected by events. An understanding system can then use these expectations to accommodate [15] the particular changes that are mentioned in subsequent discourse (Section 3). In Section 4, we discuss our initial implementation of these ideas. This work is being carried out as part of a project ( A n l m N L ) aimed at creating animated task simulations from Natural Language instructions [2; 4; 5; 6; 7; 14; 20]. Instructions are a form of text rich in the specification of events intended to alter the world in some way. Because of this, the issues discussed in this paper are particularly important to both understanding and generating instructions. Introduction Consider the following example: Example 1 John made a handbag from an inner-tube. a. He sold it for twenty dollars. b. *He sold them for fifty dollars. c. He had taken it from his brother's car. d. Neither of them was particularly useful. Here two entities are introduced via indefinite noun phrases (NPs) in the first sentence. The alternative f"
P92-1013,P91-1044,0,\N,Missing
P97-1012,J92-4007,0,0.116136,"re reflected in the coherence relations between its units. In the figures presented here, non-terminal nodes in a discourse structure are labelled with coherence relations merely to indicate the functions that project appropriate content, beliefs and other side effects into the recipient&apos;s discourse model. This view is, we believe, consistent with the more detailed formal interfaces to discourse semantics/pragmatics presented in (Gardent, 1997; Schilder, 1997; van den Berg, 1996), and also allows for multiple discourse relations (intentional and informational) to hold between discourse units (Moore and Pollack, 1992; Moser and Moore, 1995; Moser and Moore, 1996) and contribute to the semantic/pragmatics effects on the recipient&apos;s discourse model. 2 Expectations in Corpora The examples given in the Introduction were all &quot;minimal pairs&quot; created to illustrate the relevant phenomenon as succinctly as possible. Empirical questions thus include: (1) the range of lexicosyntactic constructions that raise expectations with the specific properties mentioned above; (2) the frequency of expectation-raising constructions in text; (3) the frequency with which expectations are satisfied immediately, as opposed to being"
P97-1012,P95-1018,0,0.017574,"ence relations between its units. In the figures presented here, non-terminal nodes in a discourse structure are labelled with coherence relations merely to indicate the functions that project appropriate content, beliefs and other side effects into the recipient&apos;s discourse model. This view is, we believe, consistent with the more detailed formal interfaces to discourse semantics/pragmatics presented in (Gardent, 1997; Schilder, 1997; van den Berg, 1996), and also allows for multiple discourse relations (intentional and informational) to hold between discourse units (Moore and Pollack, 1992; Moser and Moore, 1995; Moser and Moore, 1996) and contribute to the semantic/pragmatics effects on the recipient&apos;s discourse model. 2 Expectations in Corpora The examples given in the Introduction were all &quot;minimal pairs&quot; created to illustrate the relevant phenomenon as succinctly as possible. Empirical questions thus include: (1) the range of lexicosyntactic constructions that raise expectations with the specific properties mentioned above; (2) the frequency of expectation-raising constructions in text; (3) the frequency with which expectations are satisfied immediately, as opposed to being delayed by material th"
P97-1012,J96-3006,0,0.113403,"its units. In the figures presented here, non-terminal nodes in a discourse structure are labelled with coherence relations merely to indicate the functions that project appropriate content, beliefs and other side effects into the recipient&apos;s discourse model. This view is, we believe, consistent with the more detailed formal interfaces to discourse semantics/pragmatics presented in (Gardent, 1997; Schilder, 1997; van den Berg, 1996), and also allows for multiple discourse relations (intentional and informational) to hold between discourse units (Moore and Pollack, 1992; Moser and Moore, 1995; Moser and Moore, 1996) and contribute to the semantic/pragmatics effects on the recipient&apos;s discourse model. 2 Expectations in Corpora The examples given in the Introduction were all &quot;minimal pairs&quot; created to illustrate the relevant phenomenon as succinctly as possible. Empirical questions thus include: (1) the range of lexicosyntactic constructions that raise expectations with the specific properties mentioned above; (2) the frequency of expectation-raising constructions in text; (3) the frequency with which expectations are satisfied immediately, as opposed to being delayed by material that elaborates the unit r"
P99-1006,P97-1012,1,0.841173,"Because y ~5&quot;, they cannot together be realised as &quot;Although ~ because y [3 &&quot; with the same meaning as &quot;Although o¢ [3. Because y 8&quot;. The same is true of certain relations whose realisation spans multiple sentences, such as ones realisable as &quot;On the one hand oz. On the other hand 13.&quot; and &quot;Not only T- But also &&quot; Together, they cannot be realised as &quot;On the one hand o¢. Not only T. On the other hand 13. But also &&quot; with the same meaning as in strict sequence. Thus we take such constructions to be structural as well (Webber and Joshi, 1998; Webber et al., 1999). Framework In previous papers (Cristea and Webber, 1997; Webber and Joshi, 1998; Webber et al., 1999), we have argued for using the more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is lexically null. The semantic contribution of a lexical anchor includes both w"
P99-1006,W98-0304,0,0.0231865,"modifier (e.g. &quot;He&apos;s an otherwise happy boy.&quot;) or a clausal modifier (e.g., &quot;The physical layer is different, but otherwise it&apos;s identical to metropolitan networks.&quot;). What is presupposed here are one or more actual properties of the situation under discussion. If the light had been red, John would have stopped. Otherwise, he would have carded straight on. But as it turned out, he never got to the light. 46 (9) You should take a coat with you because otherwise you&apos;ll get cold. Clearly, more remains to be done. First, the approach demands a precise semantics for connectives, as in the work of Grote (1998), Grote et al. (1997), Jayez and Rossari (1998) and Lagerwerf (1998). Secondly, the approach demands an understanding of the attentional characteristics of presuppositions. In particular, preliminary study seems to suggest that p-bearing elements differ in what source can license them, where this source can be located, and what can act as distractors for this source. In fact, these differences seem to resemble the range of differences in the information status (Prince, 1981; Prince, 1992) or familiarity (Gundel et al., 1993) of referential NPs. Consider, for example: and earlier examples. (Not"
P99-1006,J92-4007,0,0.1424,"ventions, etc., can then make defeasible contributions to discourse interpretation that elaborate the nondefeasible propositions contributed by compositional semantics. Introduction Research on discourse structure has, by and large, attempted to associate all meaningful relations between propositions with structural connections between discourse clauses (syntactic clauses or structures composed of them). Recognising that this could mean multiple structural connections between clauses, Rhetorical Structure Theory (Mann and Thompson, 1988) simply stipulates that only a single relation may hold. Moore and Pollack (1992) argue that both informational (semantic) and intentional relations can hold between clauses simultaneously and independently. This suggests that factoring the two kinds of relations might lead to a pair of structures, each still with no more than a single structural connection between any two clauses. But examples of multiple semantic relations are easy to find (Webber et al., 1999). Having structure account for all of them leads to the complexities shown in Figure 1, including the crossing dependencies shown in Fig. l c. These structures are no longer trees, making it difficult to define a c"
P99-1006,P97-1026,1,0.0647756,"1999), we have argued for using the more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is lexically null. The semantic contribution of a lexical anchor includes both what it presupposes and what it asserts (Stone and Doran, 1997; Stone, 1998; Stone and Webber, 1998). A feature structure anchor will either unify with a lexical item with compatible features (Knott and Mellish, 1996), yielding the previous case, or have an empty realisation, though one On the other hand, the p-bearing adverb &quot;then&quot;, which asserts that one eventuality starts after the culmination of another, has only one of its arguments coming structurally. The other argument is presupposed and thus able to come from across a structural boundary, as in (1) a. b. c. d. 1One may still need to admit structures having both a link back and a link forward to"
P99-1006,W98-1419,1,0.852452,"more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is lexically null. The semantic contribution of a lexical anchor includes both what it presupposes and what it asserts (Stone and Doran, 1997; Stone, 1998; Stone and Webber, 1998). A feature structure anchor will either unify with a lexical item with compatible features (Knott and Mellish, 1996), yielding the previous case, or have an empty realisation, though one On the other hand, the p-bearing adverb &quot;then&quot;, which asserts that one eventuality starts after the culmination of another, has only one of its arguments coming structurally. The other argument is presupposed and thus able to come from across a structural boundary, as in (1) a. b. c. d. 1One may still need to admit structures having both a link back and a link forward to different clauses (Gardent, 1997). But"
P99-1006,W98-0315,1,0.532764,"two relations, one realisable as &quot;Although o¢ [3, the other realisable as &quot;Because y ~5&quot;, they cannot together be realised as &quot;Although ~ because y [3 &&quot; with the same meaning as &quot;Although o¢ [3. Because y 8&quot;. The same is true of certain relations whose realisation spans multiple sentences, such as ones realisable as &quot;On the one hand oz. On the other hand 13.&quot; and &quot;Not only T- But also &&quot; Together, they cannot be realised as &quot;On the one hand o¢. Not only T. On the other hand 13. But also &&quot; with the same meaning as in strict sequence. Thus we take such constructions to be structural as well (Webber and Joshi, 1998; Webber et al., 1999). Framework In previous papers (Cristea and Webber, 1997; Webber and Joshi, 1998; Webber et al., 1999), we have argued for using the more complex structures (elementary trees) of a Lexicalized Tree-Adjoining Grammar (LTAG) and its operations (adjoining and substitution) to associate structure and semantics with a sequence of discourse clauses. 2 Here we briefly review how it works. In a lexicalized TAG, each elementary tree has at least one anchor. In the case of discourse, the anchor for an elementary tree may be a lexical item, punctuation or a feature structure that is"
P99-1006,J88-2006,1,0.218389,"at source. However, as with pronominal and definite NP anaphora, while attentional constraints on their interpretation may be influenced by structure, the links themselves are not structural. * Our thanks to Mark Steedman, Katja Markert, Gann Bierner and three ACL&apos;99 reviewers for all their useful comments. 41 The idea of combining compositional semantics with defeasible inference is not new. Neither is the idea of taking certain lexical items as anaphorically presupposing an eventuality or a set of eventualities: It is implicit in all work on the anaphoric nature of tense (cf. Partee (1984), Webber (1988), inter alia) and modality (Stone, 1999). What is new is the way we enable anaphoric presupposition to contribute to semantic relations and modal operators, in a way R1 Ci Ci (a) CI Ci Ck Ci (b) C i R2 Ck Cm (c) Figure 1: Multiple semantic links ( R j ) between discourse clauses ( C i ) : (a) back to the same discourse clause; (b) back to different discourse clauses; (c) back to different discourse clauses, with crossing dependencies. that maintains its semantic features. that does not lead to the violations of tree structure mentioned earlier.t We discuss these differences in more detail in S"
prasad-etal-2008-penn,W98-0315,1,\N,Missing
prasad-etal-2008-penn,J93-2004,0,\N,Missing
prasad-etal-2008-penn,W04-2703,1,\N,Missing
prasad-etal-2008-penn,W06-0305,1,\N,Missing
prasad-etal-2008-penn,W05-0305,1,\N,Missing
prasad-etal-2008-penn,W01-1605,0,\N,Missing
prasad-etal-2008-penn,J03-4002,1,\N,Missing
prasad-etal-2008-penn,J05-1004,0,\N,Missing
prasad-etal-2010-exploiting,poesio-artstein-2008-anaphoric,0,\N,Missing
prasad-etal-2010-exploiting,J97-1003,0,\N,Missing
prasad-etal-2010-exploiting,W05-0305,1,\N,Missing
prasad-etal-2010-exploiting,C00-1031,0,\N,Missing
prasad-etal-2010-exploiting,J95-2003,1,\N,Missing
prasad-etal-2010-exploiting,P09-1076,1,\N,Missing
prasad-etal-2010-exploiting,prasad-etal-2008-penn,1,\N,Missing
prasad-etal-2010-exploiting,W04-2322,0,\N,Missing
R19-1021,U12-1003,0,0.019713,"ssociated with are rarely found in a Related Work section and are unlikely to be helpful in writing feedback for this section. They are nevertheless useful in supporting writing feedback on Abstracts and summaries of PhDs (Feltrim et al., 2006). Related Work does have in common with other sections the fact that it should contain citations. Understanding the motivations or function of a citation can help determine an author intention (Teufel et al., 2006). Work on citation function has been an area of research for several decades (Weinstock, 1971; Oppenheim and Renn, 1978; Teufel et al., 2006; Angrosh et al., 2012), with more recent work considering how this recognition can be automated. Jurgens et al. (2018) investigates the framing of citations and how this can be used to study the evolution of a field. Teufel et al. (2006) work on automated recognition of citation function and show a strong relationship between function and sentiment. One work that specifically looks at context identification of sentences in ReRecognising Author Intentions – Specific phrasing has been shown to function in structuring discourse by guiding readers through a text (Hyland, 2012) and can be found to align to sections, suc"
R19-1021,bird-etal-2008-acl,0,0.0763219,"Missing"
R19-1021,W19-4011,1,0.789639,"014). Recognizing argument components in this case focuses on premises and claims largely based on the Toulmin model of argumentation (Toulmin, 2003) which is a different approach to ours. In addition, all this work focuses on feedback for persuasive essays which will differ in linguistic practices found in scientific papers and from the author intention structure of a Related Work. Overall, whilst aspects may be relevant in general, these methods would not facilitate the kind of content feedback that would help a writer with Related Work. 3 3.1 3.2 Annotated Dataset The annotated dataset in (Casey et al., 2019) is composed of papers from the ACL anthology (Bird et al., 2008) that have been pre-annotated for citations and co-reference to the author’s own work by (Sch¨afer et al., 2012). We use 94 papers with Related Work sections after removing one due to OCR issues. All papers were conference papers 6-8 pages in length. The authors report annotator agreement, based on Cohen Kappa (Cohen, 1960) at 0.77, which increased to 0.85 following a round of discussion. Author Intentions to Support Feedback 3.3 Annotation Schema for Data The need for annotated data is something that previous methods have in com"
R19-1021,Q18-1028,0,0.293866,"how that these novel features contribute to classifier performance with performance being favourable compared to other similar works that classify author intentions and consider feedback for academic writing. 1 Introduction Argument structures are key in allowing an author to construct a persuasive message that realizes the author’s intention. The automatic identification of such intentions has been shown to be a valuable resource in areas such as summarising information (Teufel and Moens, 2002; Cohan and Goharian, 2015), and understanding citation function and sentiment (Teufel et al., 2006; Jurgens et al., 2018). Recent years have seen more academic writing tools focused on content that use an understanding of expected author intentions to assist in feedback. This is an important resource for Post-Graduate (PG) students who struggle to gain the necessary skills in academic writing that are critical to their success (Aitchison et al., 2012; 178 Proceedings of Recent Advances in Natural Language Processing, pages 178–187, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_021 lated Work is (Angrosh et al., 2010). This work focuses on sentences in terms of their ability to suppor"
R19-1021,D15-1045,0,0.0198638,"ing to citation types and co-reference, along with patterns found from studying Related Works. We show that these novel features contribute to classifier performance with performance being favourable compared to other similar works that classify author intentions and consider feedback for academic writing. 1 Introduction Argument structures are key in allowing an author to construct a persuasive message that realizes the author’s intention. The automatic identification of such intentions has been shown to be a valuable resource in areas such as summarising information (Teufel and Moens, 2002; Cohan and Goharian, 2015), and understanding citation function and sentiment (Teufel et al., 2006; Jurgens et al., 2018). Recent years have seen more academic writing tools focused on content that use an understanding of expected author intentions to assist in feedback. This is an important resource for Post-Graduate (PG) students who struggle to gain the necessary skills in academic writing that are critical to their success (Aitchison et al., 2012; 178 Proceedings of Recent Advances in Natural Language Processing, pages 178–187, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_021 lated Wor"
R19-1021,W04-1205,0,0.0723155,"and show that the novel features we introduce contribute significantly to the classifier performance, improving on performance of existing writer feedback tools. 2 Related Work Automating Author Intentions – Previous models of author intentions in research articles have been successfully automated. One of the first and widely used is Teufel (1999) who proposed Argument Zoning (AZ) which labels sentences with zones representing the rhetoric purpose (author intent) within the global context of a document e.g. background, aim or conclusion. Further work has applied this schema to biology papers (Mizuta and Collier, 2004), with a modified, finer grained approach applied to papers on chemistry (Teufel et al., 2009). Liakata et al. (2012) took a different approach to labelling author intentions, studying the conceptual structure of biology articles treating the article as an investigation. Fisas et al. (2015) develop a schema based on both Liakata and Teufel’s work to represent scientific concepts that appear in computer graphics articles. These works have successfully identified author intentions, but they differ from our work by seeking intentions in a global context across a whole article. For example, AZ was"
R19-1021,W15-1605,0,0.0155256,"d. One of the first and widely used is Teufel (1999) who proposed Argument Zoning (AZ) which labels sentences with zones representing the rhetoric purpose (author intent) within the global context of a document e.g. background, aim or conclusion. Further work has applied this schema to biology papers (Mizuta and Collier, 2004), with a modified, finer grained approach applied to papers on chemistry (Teufel et al., 2009). Liakata et al. (2012) took a different approach to labelling author intentions, studying the conceptual structure of biology articles treating the article as an investigation. Fisas et al. (2015) develop a schema based on both Liakata and Teufel’s work to represent scientific concepts that appear in computer graphics articles. These works have successfully identified author intentions, but they differ from our work by seeking intentions in a global context across a whole article. For example, AZ was developed to support summarisation and information access. The author intentions that these activities would be associated with are rarely found in a Related Work section and are unlikely to be helpful in writing feedback for this section. They are nevertheless useful in supporting writing"
R19-1021,P16-2089,0,0.0224438,"tion (Cotos and Pendar, 2016; Anthony and 179 lates to the cited work or background in general. The sentence label schema we use can be found in Table 1, and we indicate which of the qualities each label falls into. V. Lashkia, 2003; Abel, 2018). The Criterion online writing service, focuses on automated persuasive essay evaluation and uses recognition of discourse elements based on aspects such as supporting ideas, introductions and conclusion (Burstein et al., 2003, 2004). Several other works have focused on identifying argument components and relations and how these relate to essay scores (Ghosh et al., 2016; Song et al., 2014). Recognizing argument components in this case focuses on premises and claims largely based on the Toulmin model of argumentation (Toulmin, 2003) which is a different approach to ours. In addition, all this work focuses on feedback for persuasive essays which will differ in linguistic practices found in scientific papers and from the author intention structure of a Related Work. Overall, whilst aspects may be relevant in general, these methods would not facilitate the kind of content feedback that would help a writer with Related Work. 3 3.1 3.2 Annotated Dataset The annota"
R19-1021,C12-2103,0,0.0227833,"Missing"
R19-1021,W10-1913,0,0.022039,"k by (Sch¨afer et al., 2012). We use 94 papers with Related Work sections after removing one due to OCR issues. All papers were conference papers 6-8 pages in length. The authors report annotator agreement, based on Cohen Kappa (Cohen, 1960) at 0.77, which increased to 0.85 following a round of discussion. Author Intentions to Support Feedback 3.3 Annotation Schema for Data The need for annotated data is something that previous methods have in common, each using an annotation schema that supports the intentions they seek. It is known that annotation schemas benefit from being task-orientated (Guo et al., 2010). We use an annotation schema developed to recognise author intentions in Related Work sections and provide authors with useful feedback (Casey et al., 2019). This schema uses qualities that should be present in Related Work sections, following (Kamler and Thomson, 2006). Qualities group into four areas: Background – helps the author locate their work in the field, demonstrating they understand their field and its history through indicating seminal works and relevant research fields; Cited works – in addition to generally identifying the field, the author should demonstrate specifically (i) wh"
R19-1021,W14-2110,0,0.0317234,"ar, 2016; Anthony and 179 lates to the cited work or background in general. The sentence label schema we use can be found in Table 1, and we indicate which of the qualities each label falls into. V. Lashkia, 2003; Abel, 2018). The Criterion online writing service, focuses on automated persuasive essay evaluation and uses recognition of discourse elements based on aspects such as supporting ideas, introductions and conclusion (Burstein et al., 2003, 2004). Several other works have focused on identifying argument components and relations and how these relate to essay scores (Ghosh et al., 2016; Song et al., 2014). Recognizing argument components in this case focuses on premises and claims largely based on the Toulmin model of argumentation (Toulmin, 2003) which is a different approach to ours. In addition, all this work focuses on feedback for persuasive essays which will differ in linguistic practices found in scientific papers and from the author intention structure of a Related Work. Overall, whilst aspects may be relevant in general, these methods would not facilitate the kind of content feedback that would help a writer with Related Work. 3 3.1 3.2 Annotated Dataset The annotated dataset in (Case"
R19-1021,J02-4002,0,0.380361,"s novel features pertaining to citation types and co-reference, along with patterns found from studying Related Works. We show that these novel features contribute to classifier performance with performance being favourable compared to other similar works that classify author intentions and consider feedback for academic writing. 1 Introduction Argument structures are key in allowing an author to construct a persuasive message that realizes the author’s intention. The automatic identification of such intentions has been shown to be a valuable resource in areas such as summarising information (Teufel and Moens, 2002; Cohan and Goharian, 2015), and understanding citation function and sentiment (Teufel et al., 2006; Jurgens et al., 2018). Recent years have seen more academic writing tools focused on content that use an understanding of expected author intentions to assist in feedback. This is an important resource for Post-Graduate (PG) students who struggle to gain the necessary skills in academic writing that are critical to their success (Aitchison et al., 2012; 178 Proceedings of Recent Advances in Natural Language Processing, pages 178–187, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-"
R19-1021,D09-1155,0,0.0231366,"e, improving on performance of existing writer feedback tools. 2 Related Work Automating Author Intentions – Previous models of author intentions in research articles have been successfully automated. One of the first and widely used is Teufel (1999) who proposed Argument Zoning (AZ) which labels sentences with zones representing the rhetoric purpose (author intent) within the global context of a document e.g. background, aim or conclusion. Further work has applied this schema to biology papers (Mizuta and Collier, 2004), with a modified, finer grained approach applied to papers on chemistry (Teufel et al., 2009). Liakata et al. (2012) took a different approach to labelling author intentions, studying the conceptual structure of biology articles treating the article as an investigation. Fisas et al. (2015) develop a schema based on both Liakata and Teufel’s work to represent scientific concepts that appear in computer graphics articles. These works have successfully identified author intentions, but they differ from our work by seeking intentions in a global context across a whole article. For example, AZ was developed to support summarisation and information access. The author intentions that these a"
R19-1021,W06-1613,0,0.3355,"g Related Works. We show that these novel features contribute to classifier performance with performance being favourable compared to other similar works that classify author intentions and consider feedback for academic writing. 1 Introduction Argument structures are key in allowing an author to construct a persuasive message that realizes the author’s intention. The automatic identification of such intentions has been shown to be a valuable resource in areas such as summarising information (Teufel and Moens, 2002; Cohan and Goharian, 2015), and understanding citation function and sentiment (Teufel et al., 2006; Jurgens et al., 2018). Recent years have seen more academic writing tools focused on content that use an understanding of expected author intentions to assist in feedback. This is an important resource for Post-Graduate (PG) students who struggle to gain the necessary skills in academic writing that are critical to their success (Aitchison et al., 2012; 178 Proceedings of Recent Advances in Natural Language Processing, pages 178–187, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_021 lated Work is (Angrosh et al., 2010). This work focuses on sentences in terms of"
T78-1006,C69-7001,0,\N,Missing
T78-1006,C69-6902,0,\N,Missing
W02-0204,J00-3005,0,\N,Missing
W02-0204,J03-4002,1,\N,Missing
W02-0204,W03-2608,1,\N,Missing
W02-0307,W02-0308,0,\N,Missing
W03-0105,H92-1045,0,0.0117369,"always) some subtle heuristic reasoning. Grounding place names mentioned in a text can support effective visualization – for instance, in a multimedia document surrogate that contains textual, video and map elements (e.g. in a question answering scenario), where we want to ensure that the video shows the region and the map is centered around the places mentioned. To make use of linguistic context in resolving ambiguous place names, we apply two different minimality heuristics (Gardent and Webber, 2001). The first we borrow (slightly modified) from work in automatic word sense disambiguation (Gale et al., 1992), calling it “one referent per discourse”. It assumes that a place name mentioned in a discourse refers to the same location throughout the discourse, just as a word is assumed to be used in the same one sense throughout the discourse. Neither is logically necessary, and hence both are simply interpretational biases. The second minimality heuristic assumes that, in cases where there is more than one place name mentioned in some span of text, the smallest region that is able to ground the whole set is the one that gives them their interpretation.5 This can be used to resolve referential ambigui"
W03-0105,P79-1017,0,0.498089,"d) are ignored. 11 -122˚ -5˚ -4˚ -3˚ -2˚ -1˚ -121˚ -120˚ -119˚ -118˚ 39˚ 39˚ 38˚ 38˚ 37˚ 37˚ 36˚ 36˚ 35˚ 35˚ 34˚ 34˚ 0˚ 56˚ 56˚ 55˚ 55˚ 54˚ 54˚ 53˚ 53˚ -122˚ -121˚ -120˚ -119˚ -118˚ Figure 8: Automatic Visualization of Story C: A Pregnant Woman is Missing in Modeno, CA (Local View; Final Paragraph Excluded). 52˚ 52˚ -5˚ -4˚ -3˚ -2˚ -1˚ 0˚ Figure 7: Automatic Visualization of Story B: A Baby Flown from London to Glasgow for Medical Treatment Dies there. -120˚ -105˚ -90˚ -75˚ -120˚ -105˚ -90˚ -75˚ Figure 9: Story C: The Final Paragraph Places the Event in Context (Global View; Complete Story). (Shanon, 1979) discusses how the granularity of the answers to where-questions depends on the reference points of speaker and listener (Where is the Empire State Building? – (a) In New York, (b) In the U.S.A, (c) On 34th Street and 3rd Avenue); the map generation task depends on such levels of granularity in the sense that to create a useful map, entities that belong to the same level of granularity or scale should be marked (e.g. city–city rather than village–continent). The work presented here perhaps most closely resembles that of (Mackinlay, 1986; Casner, 1990; Roth and Hefley, 1993) who describe system"
W03-0105,P02-1060,0,0.0119551,"nding the named entity (i.e. establishing its denotation with respect to the world or a model). The latter aspect has so far been neglected. In this paper, we show how geo-spatial named entities can be grounded using geographic coordinates, and how the results can be visualized using off-the-shelf software. We use this to compare a “textual surrogate” of a newspaper story, with a “visual surrogate” based on geographic coordinates. 1 Introduction The task of named entity annotation of unseen text has recently been successfully automated, achieving near-human performance using machine learning (Zheng and Su, 2002). But many applications also require grounding – i.e., associating each classified text span with a referent in the world or some model thereof. The current paper discusses spatial grounding of named entities that may be referentially ambiguous, using a minimality heuristic that is informed by external geographic knowledge sources. We then apply these ideas to the creation of “visual surrogates” for news articles. This paper is structured as follows: Section 2 discusses how spatial named entities can be grounded and how this interacts with their extraction and applications. Section 3 describes"
W03-0105,E99-1001,0,0.00654444,"e from the UNECE Web site1 and contains more than 36 000 locations in 234 countries (UNECE, 1998). The Alexandria Gazetteer (Smith et al., 1996; Frew et al., 1998) is another database of geographical entities, including both their coordinates and relationships such as: in-state-of, in-province-of, in-county-of, in-country-of, in-region-of, part-of and formerly-known-as. To date, Named Entity Recognition (NER) has only used gazetteers as evidence that a text span could be some kind of place name (LOCATION), even though their finite nature makes lists of names of limited use for classification (Mikheev et al., 1999). Here we use them for spatial grounding – relating linguistic entities of subtype LOCA TION (Grishman and Sundheim, 1998) to their real-world counterparts. “World Atlases” and the gazetteers that index them are not the only resources than can be used for grounding spatial terms. In biomedicine, there are are several brain atlases of different species, using various different techniques, and focussing on both normal and disease state; as well as a digital atlas of the human body 1 http://www.unece.org/cefact/locode/service/main.htm 3 Place-Name Resolution for Information Extraction Figure 1: G"
W03-2406,M95-1012,0,\N,Missing
W03-2406,P97-1003,0,\N,Missing
W03-2406,P99-1042,0,\N,Missing
W03-2406,grover-etal-2000-lt,1,\N,Missing
W03-2406,W01-1201,0,\N,Missing
W03-2608,W98-0315,1,\N,Missing
W03-2608,J03-4002,1,\N,Missing
W03-2608,P02-1011,0,\N,Missing
W04-0212,P97-1011,0,0.0334976,"Missing"
W04-0212,kingsbury-palmer-2002-treebank,0,0.0252442,"Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have played a central role in speech and natural language research. However, with the demand for more powerful NLP applications comes a need for greater richness in annotation – hence, the development of PropBank (Kingsbury and Palmer, 2002), which adds basic semantics to the PTB in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexi"
W04-0212,J93-2004,0,0.0345992,"nk (PDTB) is a new resource built on top of the Penn Wall Street Journal corpus, in which discourse connectives are annotated along with their arguments. Its use of standoff annotation allows integration with a stand-off version of the Penn TreeBank (syntactic structure) and PropBank (verbs and their arguments), which adds value for both linguistic discovery and discourse modeling. Here we describe the PDTB and some experiments in linguistic discovery based on the PDTB alone, as well as on the linked PTB and PDTB corpora. 1 Introduction Large scale annotated corpora such as the Penn TreeBank (Marcus et al., 1993) have played a central role in speech and natural language research. However, with the demand for more powerful NLP applications comes a need for greater richness in annotation – hence, the development of PropBank (Kingsbury and Palmer, 2002), which adds basic semantics to the PTB in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their ar"
W04-0212,W03-2608,1,0.888723,"or a state, and discourse deictics that denote an abstract object. What we describe to annotators as arguments to discourse connectives are actually the textual span from which the argument is derived (Webber et al., 1999a; Webber et al., 2003). This is especially clear in the case of the first argument of instead in (3), which does not actually include the negation, although it is part of the selected text.3 2 For a more detailed discussion of how discourse adverbials can be distinguished from clausal adverbials, see Forbes (2003). 3 For a corpus-based study of the arguments of instead, see (Miltsakaki et al., 2003). (3) [No price for the new shares has been set]. Instead, [the companies will leave it up to the marketplace to decide]. How far does an argument extend? One particularly significant addition to the guidelines came as a result of differences among annotators as to how large a span constituted the argument of a connective. During pilot annotations, annotators used three annotation tags: CONN for the connective and ARG1 and ARG2 for the two arguments. To this set, we have added two optional tags, SUP1 and SUP2 (supplementary), for cases when the annotator wants to mark textual spans s/he consid"
W04-0212,W04-2703,1,0.796962,"and restrict the profits businessmen could make]. As a result, [industry operated out of small, expensive, highly inefficient industrial units]. (2) Strangely, conventional wisdom inside the Beltway regards these transfer payments as “uncontrollable” or “nondiscretionary.” Implicit connectives are taken to occur between adjacent sentences not related by any explicit connective. They are annotated with whatever explicit connective the annotator feels could be inserted, with the original meaning retained. Assessment of inter-annotator agreement groups these annotations into five coarse classes (Miltsakaki et al., 2004). Currently, we are not annotating implicit connectives intra-sententially (such as between a main clause and a free adjunct) or across paragraphs. What counts as a legal argument? The simplest argument to a connective is what we take to be the minimum unit of discourse. Because we take discourse relations to hold between abstract objects, we require that an argument contain at least one clause-level predication (usually a verb – tensed or untensed), though it may span as much as a sequence of clauses or sentences. The two exceptions are nominal phrases that express an event or a state, and di"
W04-0212,J88-2003,0,0.332427,"Missing"
W04-0212,P95-1018,0,0.0380731,"inconsistencies in how the lexical items are analyzed. We believe that the PDTB annotation can contribute to a range of linguistic discovery and language modeling tasks, such as      providing empirical evidence for the DLTAG claim that discourse adverbials get one argument anaphorically, while structural connectives such as conjunctions establish relations between adjacent units of text (Creswell et al., 2002). acquiring common usage patterns of connectives and identifying their dependencies, in order to support “natural” choices in Natural Language Generation (di Eugenio et al., 1997; Moser and Moore, 1995; Williams and Reiter, 2003). developing decision procedures for resolving and interpreting discourse adverbials (Miltsakaki et al., 2003) which can be built on top of discourse parsing systems (Forbes et al., 2003). developing “word sense disambiguation” procedures for distinguishing among different senses of a connective and hence interpreting connectives correctly (e.g., distinguishing between temporal and explanatory since, between hypothetical and counterfactual if, between epistemic and semantic because, etc.) providing empirical evidence for theories of anaphoric phenomena such as verb"
W04-0212,P04-1011,1,0.743438,"Missing"
W04-0212,W98-0315,1,0.941827,"in the form of verb predicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexicalized approach to discourse – DLTAG (Webber and Joshi, 1998; Webber et al., 1999a; Webber et al., 2000; Webber et al., 2003). Here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicateargument relations whose predicates come mainly from discourse connectives1 and whose arguments 1 Despite this, we have deliberately adopted a policy of havcome from units of discourse – clausal, sentential or multi-sentential units. The PDTB therefore differs from the RST-annotated corpus (Carlson et al., 2003) which starts with (abstract) rhetorical relations (Mann and Thompson, 1988) and annotates a subset of th"
W04-0212,P99-1006,1,0.939969,"dicateargument annotation and eventually similar annotation of nominalizations. We have been developing yet another annotation layer above these both. The Penn Discourse TreeBank (PDTB) adds low-level discourse structure and semantics through the annotation of discourse connectives and their arguments, using connective-specific semantic role labels. With this added knowledge, the PDTB (together with the PTB and PropBank) should support more in-depth NLP research and more powerful applications. Work on the PDTB is grounded in a lexicalized approach to discourse – DLTAG (Webber and Joshi, 1998; Webber et al., 1999a; Webber et al., 2000; Webber et al., 2003). Here, low-level discourse structure and semantics are taken to result (in part) from composing elementary predicateargument relations whose predicates come mainly from discourse connectives1 and whose arguments 1 Despite this, we have deliberately adopted a policy of havcome from units of discourse – clausal, sentential or multi-sentential units. The PDTB therefore differs from the RST-annotated corpus (Carlson et al., 2003) which starts with (abstract) rhetorical relations (Mann and Thompson, 1988) and annotates a subset of the Penn WSJ corpus wit"
W04-0212,W01-1605,0,\N,Missing
W04-0212,J03-4002,1,\N,Missing
W04-1212,grover-etal-2000-lt,0,0.0173917,"current study concerns whether NLP techniques can help to improve performance of classification, we have postponed experimenting with different machine learning techniques. We will do so after we find which NLP techniques are the most useful. The Rainbow3 Naive Bayes classification tool was used. Raychaudhuri et al. induced a single N-ary classifier, whereas this study induced 21 binary classifiers, i.e. an article was classified as either being related to a particular biological process or unrelated. 2.2.2 NLP techniques We applied both Part-of-Speech tagging and stemming. The LT-TTT tagger (Grover et al., 2000) was used to tag the part of speech each word belonged to. This allowed us to experiment with building classifiers based only on single parts of speech as well as ones based on all words. The most widely used stemmer among the NLP community is the Porter stemmer (Porter, 1980). A Perl version of this was used to produce stemmed sets of the articles. We experimented with four strategies to find the best performance in classification: bag of words; bag of nouns; bag of stems; bag of stemmed nouns. 2.2.3 Training There were too few full text articles to both train and test on, so the classifiers"
W04-2703,J93-2004,0,0.0274317,"being built directly on top of the Penn TreeBank and Propbank, thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms. We provide a detailed preliminary analysis of inter-annotator agreement – both the level of agreement and the types of inter-annotator variation. 1 Introduction Large scale annotated corpora have played a critical role in speech and natural language research. The Penn TreeBank (PTB) is an example of such a resource with worldwide impact on natural language processing (Marcus et al., 1993). However, the PTB deals with text only at the sentence level: with the demand for more powerful NLP applications comes a need for greater richness in annotation. At the sentence level, Penn Propbank is adding predicate-argument annotation to sentences in PTB (Kingsbury and Palmer, 2002). At the discourselevel are efforts to produce corpora annotated with rhetorical relations (Carlson et al., 2003). This paper describes a more basic discourse-level annotation project – the Penn Discourse TreeBank (PDTB) – that aims to produce a large-scale corpus in which discourse connectives are annotated, a"
W04-2703,W03-2608,1,0.843833,"reement between them and by correcting for chance expected agreement. However, the statistic requires the data tokens to be classified into discrete categories, and as a result, we could not apply it to our data since the PDTB annotation tokens cannot be classified as such. Rather, annotation in the PDTB constitutes either selection of a span of text for the arguments of connectives which can be of indeterminate length or providing explicit expressions for implicit connectives from an open-ended class of expressions. 8 For a preliminary corpus-based analysis of the arguments of ‘instead’, see Miltsakaki et al. (2003). Instead, we have assessed inter-annotator agreement in terms of agreement/disagreement on span or named expression identity for each token as a percentage of the pairs of spans or expressions that actually matched versus those that should have. For the argument annotations, we use a most conservative measure - the exact match criterion. In addition, we also used different diagnostics for the argument annotations for the explicit connectives, reporting percentage agreement on different classes of tokens, such as those in which the first argument (ARG1) annotations and second argument (ARG2) a"
W04-2703,P99-1006,1,0.897467,"nyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations. Specification of the discourse relations for a discourse thus constitutes a description of a certain level of discourse structure. Rather than starting from (abstract) discourse relations, we describe an approach to annotating a largescale corpus in terms of a more basic characterisation of discourse structure in terms of discourse connectives and their arguments. The motivation for such an approach stems from work by Webber and Joshi (1998), Webber et al. (1999a), Webber et al. (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, LTAG and DLTAG, respectively). 1 This allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, a goal also shared by Gardent (1997), Schilder (1997) and Polanyi and van den Berg (1996), among others. 2 Discourse connectives and their arguments can be successfully annotated with high reliability (cf. Section 4). This is not surprising, given that the task resembles"
W04-2703,kingsbury-palmer-2002-treebank,0,0.269727,"annotator agreement – both the level of agreement and the types of inter-annotator variation. 1 Introduction Large scale annotated corpora have played a critical role in speech and natural language research. The Penn TreeBank (PTB) is an example of such a resource with worldwide impact on natural language processing (Marcus et al., 1993). However, the PTB deals with text only at the sentence level: with the demand for more powerful NLP applications comes a need for greater richness in annotation. At the sentence level, Penn Propbank is adding predicate-argument annotation to sentences in PTB (Kingsbury and Palmer, 2002). At the discourselevel are efforts to produce corpora annotated with rhetorical relations (Carlson et al., 2003). This paper describes a more basic discourse-level annotation project – the Penn Discourse TreeBank (PDTB) – that aims to produce a large-scale corpus in which discourse connectives are annotated, along with their arguments. There have been several approaches to describing discourse in terms of discourse relations (Mann and Thompson, 1988; Asher and Lascarides, 1998; Polanyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the se"
W04-2703,W98-0315,1,0.725528,"nd Lascarides, 1998; Polanyi and van den Berg, 1996). In these approaches, the additional meaning the discourse contributes beyond the sentence derives from discourse relations. Specification of the discourse relations for a discourse thus constitutes a description of a certain level of discourse structure. Rather than starting from (abstract) discourse relations, we describe an approach to annotating a largescale corpus in terms of a more basic characterisation of discourse structure in terms of discourse connectives and their arguments. The motivation for such an approach stems from work by Webber and Joshi (1998), Webber et al. (1999a), Webber et al. (2000) which integrates sentence level structures with discourse level structure (using tree-adjoining grammars for both cases, LTAG and DLTAG, respectively). 1 This allows structural composition and its associated semantic composition at the sentence level to be smoothly carried over to the discourse level, a goal also shared by Gardent (1997), Schilder (1997) and Polanyi and van den Berg (1996), among others. 2 Discourse connectives and their arguments can be successfully annotated with high reliability (cf. Section 4). This is not surprising, given tha"
W04-2703,J03-4002,1,\N,Missing
W05-0305,P97-1003,0,0.0334303,"Missing"
W05-0305,J00-3005,0,0.0156263,"t. Consider example (12), where the PTB requires annotators to include the verb of attribution said and its subject Delmed in the complement of although. But although as a discourse connective denies the expectation that the supply of dialysis products will be discontinued when the distribution arrangement ends. It does not convey the expectation that Delmed will not say such things. On the other hand, in (13), the contrast established by while is between the opinions of two entities i.e., advocates and their opponents.4 4 This distinction is hard to capture in an RST-based parsing framework (Marcu, 2000). According to the RST-based annotation scheme (Carlson et al., 2003) ‘although Delmed said’ and ‘while opponents argued’ are elementary discourse units 32 (12) The current distribution arrangement ends in March 1990, although Delmed said it will continue to provide some supplies of the peritoneal dialysis products to National Medical, the spokeswoman said. (13) Advocates said the 90-cent-an-hour rise, to $4.25 an hour by April 1991, is too small for the working poor, while opponents argued that the increase will still hurt small business and cost many thousands of jobs. In Section 5, we will"
W05-0305,W04-2703,1,0.741875,"iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntactic"
W05-0305,miltsakaki-etal-2004-penn,1,0.916062,"iscourse structure, in terms of the arguments of connectives, due in large part to attribution. We describe these differences, an algorithm for detecting them, and finally some experimental results. These results have implications for automating discourse annotation based on syntactic annotation. 1 Introduction The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al., 1993) with a layer of discourse annotations. A preliminary report on this project was presented at the 2004 workshop on Frontiers in Corpus Annotation (Miltsakaki et al., 2004a), where we described our annotation of discourse connectives (both explicit and implicit) along with their (clausal) arguments. Further work done since then includes the annotation of attribution: that is, who has expressed each argument to a discourse connective (the writer or some other speaker or author) and who has expressed the discourse relation itself. These ascriptions need not be the same. Of particular interest is the fact that attribution may or may not play a role in the relation established by a connective. This may lead to a lack of congruence between arguments at the syntactic"
W05-0305,W04-0212,1,0.860298,"the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitrary) set of discourse relations whose identification may confound multiple sources of discourse meaning, we start with the annotation of discourse connectives and their arguments, thus exposing a clearly defined level of discourse representation. The PDTB annotates as explicit discourse connectives all subordinating conjunctions, coordinating conjunctions and discourse adverbials. These predicates establish relations between two abstract objects s"
W05-0305,W03-1014,0,0.0063381,"re looking to raise in the year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3 The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/ pdtb.  arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case"
W05-0305,W98-0315,1,0.823932,"the attribution of the arguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an"
W05-0305,P99-1006,1,0.857391,"rguments of a connective and the relation it conveys. In Sections 4 and 5, we describe mismatches that arise between the discourse arguments of a connective and the syntactic annotation as provided by the Penn TreeBank (PTB), in the cases where all the arguments of the connective are in the same sentence. In Section 6, we will discuss some implications of these issues for the theory and practice of discourse annotation and their relevance even at the level of sentence-bound annotation. 2 Overview of the PDTB The PDTB builds on the DLTAG approach to discourse structure (Webber and Joshi, 1998; Webber et al., 1999; Webber et al., 2003) in which connectives are discourse-level predicates which project predicate-argument structure on a par with verbs at 29 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 29–36, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the sentence level. Initial work on the PDTB has been described in Miltsakaki et al. (2004a), Miltsakaki et al. (2004b), Prasad et al. (2004). The key contribution of the PDTB design framework is its bottom-up approach to discourse structure: Instead of appealing to an abstract (and arbitr"
W05-0305,J04-3002,0,0.0189811,"he year ending March 21 compares with only $2.7 billion raise on the capital market in the previous year. IMPLICIT - in contrast In fiscal 1984, before Mr. Gandhi came into power, only $810 million was raised. When complete, the PDTB will contain approximately 35K annotations: 15K annotations of the 100 explicit connectives identified in the corpus and 20K annotations of implicit connectives.3 3 Annotation of attribution Wiebe and her colleagues have pointed out the importance of ascribing beliefs and assertions expressed in text to the agent(s) holding or making them (Riloff and Wiebe, 2003; Wiebe et al., 2004; Wiebe et al., 2005). They have also gone a considerable way towards specifying how such subjective material should be annotated (Wiebe, 2002). Since we take discourse connectives to convey semantic predicate-argument relations between abstract objects, one can distinguish a variety of cases depending on the attribution of the discourse relation or its 3 The annotation guidelines for the PDTB are available at http://www.cis.upenn.edu/ pdtb.  arguments; that is, whether the relation or arguments are ascribed to the author of the text or someone other than the author. Case 1: The relation and"
W05-0305,J93-2004,0,\N,Missing
W05-0305,J03-4002,1,\N,Missing
W06-0305,J93-2004,0,\N,Missing
W06-0305,W04-2703,1,\N,Missing
W06-0305,W05-0308,0,\N,Missing
W06-0305,W04-0212,1,\N,Missing
W06-0305,H05-1116,0,\N,Missing
W06-0305,W05-0305,1,\N,Missing
W06-0305,W03-1017,0,\N,Missing
W06-0305,J03-4002,1,\N,Missing
W06-0305,J04-3002,0,\N,Missing
W06-0305,P02-1053,0,\N,Missing
W06-0305,W02-1011,0,\N,Missing
W06-0907,W01-1309,0,0.0833406,"Missing"
W06-0907,W02-0307,1,0.893739,"Missing"
W06-3902,P99-1058,0,0.0751223,"ine-assisted. While the design of more expressive logics makes the composition of specifications easier, using them for model checking needs the creation of more expressive models (which requires more effort). As a result, there is a trade-off between amount of effort spent in obtaining models, and that in obtaining the specifications. Our decision to work with less expressive models is motivated by the extensive tool support available for creating and extracting such models [5,11]. Further, subsets of NL for which automatic translation is guaranteed, such as the one derived by Holt and Klein [10], assume (among other things) that references are resolved and hence cannot be directly applied to regulatory documents. We are thus left with the choice of making the procedure machine-assisted. There have been two kinds of machine-assisted approaches to extracting temporal logic specifications: (a) composing the semantics in a general semantic framework which is then mapped to temporal logic [7], and (b) attempting to compose the semantics in the temporal logic directly [6]. In the latter approach, a human specifies denotations for a portion of the sentence, and the rest of the composition h"
W07-1206,P98-1013,0,0.0467783,"es can indeed be highly beneficial for a QA system. 1 Introduction A large part of the work done in NLP deals with exploring how different tools and resources can be used to improve performance on a task. The quality and usefulness of the resource certainly is a major factor for the success of the research, but equally so is the creativity with which these tools or resources are used. There usually is more than one way to employ these, and the approach chosen largely determines the outcome of the work. This paper illustrates the above claims with respect to three lexical resources – FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) – that convey information about lexical predicates and their arguments. We describe two new and complementary techniques for using these resources and show the improvements to be gained when they are used individually and then together. We also point out problems that must be overcome to achieve these results. • They all provide verb-argument structures for a large number of lexical entries. • FrameNet and PropBank contain semantically annotated sentences that exemplify the underlying frame. • FrameNet contains not only verbs but als"
W07-1206,P06-1113,0,0.134344,"Missing"
W07-1206,J05-1004,0,0.164613,"ial for a QA system. 1 Introduction A large part of the work done in NLP deals with exploring how different tools and resources can be used to improve performance on a task. The quality and usefulness of the resource certainly is a major factor for the success of the research, but equally so is the creativity with which these tools or resources are used. There usually is more than one way to employ these, and the approach chosen largely determines the outcome of the work. This paper illustrates the above claims with respect to three lexical resources – FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) – that convey information about lexical predicates and their arguments. We describe two new and complementary techniques for using these resources and show the improvements to be gained when they are used individually and then together. We also point out problems that must be overcome to achieve these results. • They all provide verb-argument structures for a large number of lexical entries. • FrameNet and PropBank contain semantically annotated sentences that exemplify the underlying frame. • FrameNet contains not only verbs but also lexical entries for other part"
W07-1206,P06-1112,0,0.00853588,"ding our second method, two papers describe related ideas: Firstly, in (Bouma et al., 2005) the authors describe a Dutch QA system which makes extensive use of dependency relations. In a pre-processing step they parsed and stored the full text collection for the Dutch CLEF QA-task. When their system is asked a question, they match the dependency structure of the question against the dependency structures of potential answer candidates. Additionally, a set of 13 equivalence rules allows transformations of the kind “the coach of Norway, Egil Olsen” ⇔ “Egil Olsen, the coach of Norway”. Secondly, Shen and Klakow (2006) use dependency relation paths to rank answer candidates. In their work, a candidate sentence supports an answer if relations between certain phrases in the candidate sentence are similar to the corresponding ones in the question. Our work complements that described in both these papers, based as it is on a large collection of semantically annotated example sentences: We only require a candidate sentence to match one of the annotated example sentences. This allows us to deal with a much wider range of syntactic possibilities, as the resources we use do not only document verb argument structure"
W07-1206,C98-1013,0,\N,Missing
W07-1530,W05-0305,1,0.887762,"Missing"
W07-1530,J93-2004,0,0.0279621,"Missing"
W07-1530,W04-2703,1,0.832896,"Missing"
W07-1530,W04-0212,1,0.90064,"Missing"
W07-1530,J05-2005,0,0.0850358,"Missing"
W07-1530,W04-0213,1,0.81694,"d projects/muc/. 2 The Automated Content Extraction program, www.nist.gov/speech/tests/ace/. 192 genre-specific corpus of German newspaper commentaries, taken from the daily papers M¨arkische Allgemeine Zeitung and Tagesspiegel. One central aim is to provide a tool for studying mechanisms of argumentation and how they are reflected on the linguistic surface. The corpus on the one hand is a collection of “raw” data, which is used for genreoriented statistical explorations. On the other hand, we have identified two sub-corpora that are subject to a rich multi-level annotation (MLA). The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (1215 sentences), with 33.000 tokens in total. The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate3 tool. In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007)) and rhetorical structure according to RST (Mann and Thompson, 1988). Our annotation software architecture consists of a variety of standard, external tools that can be u"
W07-1530,J02-4002,1,0.73024,"a single, very complex annotation step; • end up with less ambiguity in the annotations, since the reasons for specific decisions can be made explicit (by annotations on “simpler” levels); • be more explicit than a single tree can be: if a discourse fulfills, for example, a function both for thematic development and for the writer’s intention, they can both be accounted for; • provide the central information that a “traditional” rhetorical tree conveys, without loosing essential information. 5 AZ Corpus (Simone Teufel, Cambridge) The Argumentative Zoning (AZ) annotation scheme (Teufel, 2000; Teufel and Moens, 2002) is concerned with marking argumentation steps in scientific articles. One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches. In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teu"
W07-1530,E99-1015,1,0.662031,"02) is concerned with marking argumentation steps in scientific articles. One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches. In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teufel et al., 1999) reports on interannotator agreement studies with this scheme. There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument. Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification. CFC– annotation records for each citation which rhetorical function it plays in the argument. This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)). An example for a ci193 tation function"
W07-1530,W06-1312,1,0.772883,"sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teufel et al., 1999) reports on interannotator agreement studies with this scheme. There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument. Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification. CFC– annotation records for each citation which rhetorical function it plays in the argument. This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)). An example for a ci193 tation function would be “motivate that the method used is sound”. The annotation scheme contains 12 functions, clustered into “superiority”, “neutral comparison/contrast”, “praise or usage” and “neutral”. One type of research we hope to do in the future is to study the relationship between these rhetorical phonemena with more tradi"
W08-1809,C04-1188,0,0.0427314,"Missing"
W08-1809,W00-1107,0,0.0145833,"answers. (Jijkoun et al. (2004) follows a similar approach.) This basically Information Extraction approach taken here can complement our own work for the benefit of increased precision for select types of questions. In Clifton and Teahan (2004), their knowledge framework based QA system, QITEKAT, prestores possible answers along with their corresponding question templates based on manual and automatic regular expression patterns. That the potential questions are stored as well the answers make this approach different from our approach. The bi-topic method in this paper has some similarity to Katz and Lin (2000). Here, ternary relations are extracted off-line using manually constructed regular expression patterns on a target text and stored in a database for the use in Question Answering such as in the START QA system (Katz et al., 2002). With bi-topic documents in this paper, instead of the precise relations between the two topics, the aggregate context between two particular topics are captured by assembling all statements that mention these two topics together in one file. While this does not give the exact characteristics of the relations involved, it does give some statistical characterization b"
W08-1809,W01-1202,0,0.00944027,"ses that are potential answers are marked and annotated with respect to their answer types (or QA-tokens as they call them) including PERSON$1 , DURATION$, etc. Then the text is indexed not only with ordinary terms but also with these QA-tokens as indexing elements. The main advantage of this approach is that QA-tokens are used as part of the query enhancing the passage retrieval performance. Our work in this paper uses the same predictive annotation technique but differs in that the named entities are indexed as topics and are retrieved directly as answer candidates. Similar to our approach, Kim et al. (2001) applies predictive annotation method to retrieve answers directly rather than supporting text. For every potential answer in the corpus, a set of text spans up to three sentences long (the sentence in which it appears, plus whatever following sentences that are linked to this sentence via lexical chain totalling no more than three sentences in size) is stored and later sued to retrieve a potential answer. Although similar to our work, the main difference is in the way the textual evidence is aggregated. In Topic Indexing and Retrieval, all the evidence (aka textual content) available througho"
W08-1809,C02-1150,0,0.0586897,"ut also a significant amount of world knowledge typically associated to the topic, due to the nature of Wikipedia categories as descriptive tags. For example, ‘Bill Gates’ is identified as ‘CEO’ (a title-role), and ‘Pusan’ as ‘a province of Korea’ (geographical knowledge). Such diverse and significant knowledge, as well as the breadth and the depth of the fine types contained in the topic-type hash table, enable a very powerful match between the answer type from a question to that of a candidate topic. The set of fine-grained answer types used here differs from the set of answer types such as Li and Roth (2002) used elsewhere in that the set is openended, and new types can be added for an entity at any time. The topic repository is used in re-ranking answer candidates by the fine-grained anwer type and for question topic identification, as well as in building topic document collection to be explained next. Preprocessing This section describes the technical details of how to collect these three kinds of information used for topic based QA, and how to process and store them off-line in order to enable fast and efficient online question answering. The stored material consists of (1) a Topic Repository,"
W08-1809,W03-0424,0,0.0205309,"stored material consists of (1) a Topic Repository, which stores topics with their variant names and ontological types, (2) a topic document collection that stores the textual content of topics, and (3) a set of indices created by indexing the topic document collection for fast and efficient retrieval. 3.1 The Make Up of Topic Repository The Topic Repository stores topics, along their variant names and their ontological types, in hash tables for fast look-up. Building a topic repository requires identifying topics within the given corpus. For this we have used the C&C named entity recogniser (Curran and Clark, 2003), which is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics. This also identifies the base type of a subset of named entities as PERSON, LOCATION and ORGANISATION. This is stored for later use in building type-separated indices. When a named entity is identified, we first check whether it represents a topic already found in the topic repository. This is done by checking the topic-name hash table in the repository, which serves as the main data storage for the variant names of topics. To resolve a target named entity to the appropr"
W08-1809,P03-1001,0,\N,Missing
W11-4603,al-saif-markert-2010-leeds,0,0.0152567,"And just as pred-arg relations within a sentence can conveyed through adjacency (eg, English noun-noun modifiers such as container ship crane operator courses – courses to train operators of cranes that load/unload ships whose cargo is packed in containers), pred-arg relations in discourse can be conveyed through adjacency between clauses or sentences. The Penn Discourse TreeBank is currently the largest resource manually annotated for discourse connectives, their arguments, and the senses they convey (Prasad et al., 2008). Related resources are also being created for Modern Standard Arabic (Al-Saif and Markert, 2010), Chinese (Xue, 2005), Czech (Mladov´a et al., 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (E"
W11-4603,J06-1002,0,0.0165305,"e, though certain sequences may be more common than others (Figure 1). (3) The topic of a segment will differ from those of its adjacent sisters. (Adjacent spans that share a topic will belong to the same segment.) (4) Topic predicts lexical choice, either of all the words of a segment or just of its content words (ie, excluding “stop-words”). Making topic structure explicit (ie, topic segmentation) is based on either semanticrelatedness, where each segment is taken to consist of words more related to each other than to words outside the segment (Hearst, 1994; Hearst, 1997; Choi et al., 2001; Bestgen, 2006; Galley et al., 2003; Malioutov and Barzilay, 2006) or topic models, where each segment is taken to be produced by a distinct, compact lexical distribution (Purver et al., 2006; Eisenstein and Barzilay, 2008; Chen et al., 2009). 3.2 Function-based structure Texts within a given genre (eg, news reports, errata, scientific papers, letters to the editor, etc.) generally share a similar structure that is independent of topic and reflects the function played by each of its parts. Best known is the inverted pyramid of news reports, consisting of a headline; a lead paragraph, conveying who is involv"
W11-4603,W10-1817,0,0.013592,"as container ship crane operator courses – courses to train operators of cranes that load/unload ships whose cargo is packed in containers), pred-arg relations in discourse can be conveyed through adjacency between clauses or sentences. The Penn Discourse TreeBank is currently the largest resource manually annotated for discourse connectives, their arguments, and the senses they convey (Prasad et al., 2008). Related resources are also being created for Modern Standard Arabic (Al-Saif and Markert, 2010), Chinese (Xue, 2005), Czech (Mladov´a et al., 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et"
W11-4603,N09-1042,0,0.012724,"c predicts lexical choice, either of all the words of a segment or just of its content words (ie, excluding “stop-words”). Making topic structure explicit (ie, topic segmentation) is based on either semanticrelatedness, where each segment is taken to consist of words more related to each other than to words outside the segment (Hearst, 1994; Hearst, 1997; Choi et al., 2001; Bestgen, 2006; Galley et al., 2003; Malioutov and Barzilay, 2006) or topic models, where each segment is taken to be produced by a distinct, compact lexical distribution (Purver et al., 2006; Eisenstein and Barzilay, 2008; Chen et al., 2009). 3.2 Function-based structure Texts within a given genre (eg, news reports, errata, scientific papers, letters to the editor, etc.) generally share a similar structure that is independent of topic and reflects the function played by each of its parts. Best known is the inverted pyramid of news reports, consisting of a headline; a lead paragraph, conveying who is involved, what happened, when it happened, where it happened, why it happened, and (optionally) how it happened; a body that provides more detail; and a tail, containing less important information. This is why the first (ie, lead) par"
W11-4603,W01-0514,0,0.0180335,"if any, is sequence, though certain sequences may be more common than others (Figure 1). (3) The topic of a segment will differ from those of its adjacent sisters. (Adjacent spans that share a topic will belong to the same segment.) (4) Topic predicts lexical choice, either of all the words of a segment or just of its content words (ie, excluding “stop-words”). Making topic structure explicit (ie, topic segmentation) is based on either semanticrelatedness, where each segment is taken to consist of words more related to each other than to words outside the segment (Hearst, 1994; Hearst, 1997; Choi et al., 2001; Bestgen, 2006; Galley et al., 2003; Malioutov and Barzilay, 2006) or topic models, where each segment is taken to be produced by a distinct, compact lexical distribution (Purver et al., 2006; Eisenstein and Barzilay, 2008; Chen et al., 2009). 3.2 Function-based structure Texts within a given genre (eg, news reports, errata, scientific papers, letters to the editor, etc.) generally share a similar structure that is independent of topic and reflects the function played by each of its parts. Best known is the inverted pyramid of news reports, consisting of a headline; a lead paragraph, conveyin"
W11-4603,J96-3006,0,0.10052,"ween the text units associated with its daughters, and precedence corresponded to their order in the text. In work on generating task instructions (Dale, 1992), each internal node corresponded to the next step to take to accomplish the plan associated with its parent. In (Grosz and Sidner, 1986), which I will return to in Section 4, internal nodes corresponded to speaker intentions, with dominance in the tree corresponding to a daughter node’s intention supporting that of its parent and precedence corresponding to one intention needing to be accomplished before another. The internal nodes in (Moser and Moore, 1996) reflected an attempt to reconcile Grosz and Sidner’s approach with that of Mann and Thompson. Work that attempted to show that a simple linear model might be a better account for types of expository text (Sibun, 1992) was, by and large, ignored. Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 12–16 Discourse Structures and Language Technologies 3 Current computatational approaches to discourse As well as further elaboration of recursive discourse structures (Asher and Lascarides, 2003; Polanyi et al., 2004), current computational"
W11-4603,W09-3029,0,0.012941,"argo is packed in containers), pred-arg relations in discourse can be conveyed through adjacency between clauses or sentences. The Penn Discourse TreeBank is currently the largest resource manually annotated for discourse connectives, their arguments, and the senses they convey (Prasad et al., 2008). Related resources are also being created for Modern Standard Arabic (Al-Saif and Markert, 2010), Chinese (Xue, 2005), Czech (Mladov´a et al., 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008). 4 Future computatational approaches to"
W11-4603,P09-2004,0,0.0126723,"anks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008). 4 Future computatational approaches to discourse This story closes with some speculations about the future. I have sketched a past in which computational approaches to discourse structure were hostage to theory and a present in which they are essentially free of theory. What we really want is an empirical approach underpinned by theory, that allows us to understand (at the very least) the ways in which the various types of discourse structures fit together. Early on, (Grosz and Sidner, 1986) attempted to meld a theory of int"
W11-4603,C08-2022,0,0.0162153,", 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008). 4 Future computatational approaches to discourse This story closes with some speculations about the future. I have sketched a past in which computational approaches to discourse structure were hostage to theory and a present in which they are essentially free of theory. What we really want is an empirical approach underpinned by theory, that allows us to understand (at the very least) the ways in which the various types of discourse structures fit together. Early on, (Grosz and"
W11-4603,P09-1077,0,0.0129354,"talian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008). 4 Future computatational approaches to discourse This story closes with some speculations about the future. I have sketched a past in which computational approaches to discourse structure were hostage to theory and a present in which they are essentially free of theory. What we really want is an empirical approach underpinned by theory, that allows us to understand (at the very least) the ways in which the various types of discourse structures fit together. Early on, (Grosz and Sidner, 1986) attempt"
W11-4603,W04-0211,0,0.0223696,"Missing"
W11-4603,prasad-etal-2008-penn,1,0.497936,", a conjunction like because or but, or a discourse adverbial like nevertheless or instead. And just as pred-arg relations within a sentence can conveyed through adjacency (eg, English noun-noun modifiers such as container ship crane operator courses – courses to train operators of cranes that load/unload ships whose cargo is packed in containers), pred-arg relations in discourse can be conveyed through adjacency between clauses or sentences. The Penn Discourse TreeBank is currently the largest resource manually annotated for discourse connectives, their arguments, and the senses they convey (Prasad et al., 2008). Related resources are also being created for Modern Standard Arabic (Al-Saif and Markert, 2010), Chinese (Xue, 2005), Czech (Mladov´a et al., 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentime"
W11-4603,prasad-etal-2010-exploiting,1,0.831932,"zen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008). 4 Future computatational approaches to discourse This story closes with some speculations about the future. I have sketched a past in which computational approaches to discourse structure were hostage to theory and a present in which they are essentially free of theory. What we really want is an empirical approach underpinned by theory, that allows us to understand (at the very least) the ways in which the various types of discourse structures fit together. Early on, (Grosz and Sidner, 1986) attempted to meld a theory of intention14 Discourse St"
W11-4603,P06-1003,0,0.0131443,"e a topic will belong to the same segment.) (4) Topic predicts lexical choice, either of all the words of a segment or just of its content words (ie, excluding “stop-words”). Making topic structure explicit (ie, topic segmentation) is based on either semanticrelatedness, where each segment is taken to consist of words more related to each other than to words outside the segment (Hearst, 1994; Hearst, 1997; Choi et al., 2001; Bestgen, 2006; Galley et al., 2003; Malioutov and Barzilay, 2006) or topic models, where each segment is taken to be produced by a distinct, compact lexical distribution (Purver et al., 2006; Eisenstein and Barzilay, 2008; Chen et al., 2009). 3.2 Function-based structure Texts within a given genre (eg, news reports, errata, scientific papers, letters to the editor, etc.) generally share a similar structure that is independent of topic and reflects the function played by each of its parts. Best known is the inverted pyramid of news reports, consisting of a headline; a lead paragraph, conveying who is involved, what happened, when it happened, where it happened, why it happened, and (optionally) how it happened; a body that provides more detail; and a tail, containing less importan"
W11-4603,W04-0213,0,0.0357283,"nes that load/unload ships whose cargo is packed in containers), pred-arg relations in discourse can be conveyed through adjacency between clauses or sentences. The Penn Discourse TreeBank is currently the largest resource manually annotated for discourse connectives, their arguments, and the senses they convey (Prasad et al., 2008). Related resources are also being created for Modern Standard Arabic (Al-Saif and Markert, 2010), Chinese (Xue, 2005), Czech (Mladov´a et al., 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008)."
W11-4603,W09-3909,0,0.0463431,"ethods (also called Method, Study Design, or Methodology); Results (also called Outcomes); Discussion and optionally, Conclusions. This does not mean that every sentence within a section realises the same function: Fine-grained functional characterizations of scientific papers (Liakata et al., 2010; Teufel, 2010) show a range of functions served by the sentences in a section. Interest in automatic annotation of functional structure comes from its value for summarization (noted above), sentiment analysis, where words may have an objective sense in one section and a subjective sense in another (Taboada et al., 2009), and citation analysis, where a citation may mean different things in different sections (Teufel, 2010). As with computational models of topicbased structure, computational models of function-based structure make assumptions that may or may not actually hold: (1) Relations hold between the function of a segment and that of the discourse as a whole: While relations may hold between sisters (eg, Methods constrain Results), only sequence has been used in modelling. (2) Function predicts more than lexical choice: it can predict indicative phrases such as “results show” (→ Results) or indicative s"
W11-4603,D07-1010,0,0.0297503,"n der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge, 2008; Lin et al., 2010; Pitler et al., 2008; Pitler et al., 2009; Pitler and Nenkova, 2009; Prasad et al., 2010; Wellner and Pustejovsky, 2007; Wellner, 2008). 4 Future computatational approaches to discourse This story closes with some speculations about the future. I have sketched a past in which computational approaches to discourse structure were hostage to theory and a present in which they are essentially free of theory. What we really want is an empirical approach underpinned by theory, that allows us to understand (at the very least) the ways in which the various types of discourse structures fit together. Early on, (Grosz and Sidner, 1986) attempted to meld a theory of intention14 Discourse Structures and Language Technolog"
W11-4603,W05-0312,0,0.0231333,"n a sentence can conveyed through adjacency (eg, English noun-noun modifiers such as container ship crane operator courses – courses to train operators of cranes that load/unload ships whose cargo is packed in containers), pred-arg relations in discourse can be conveyed through adjacency between clauses or sentences. The Penn Discourse TreeBank is currently the largest resource manually annotated for discourse connectives, their arguments, and the senses they convey (Prasad et al., 2008). Related resources are also being created for Modern Standard Arabic (Al-Saif and Markert, 2010), Chinese (Xue, 2005), Czech (Mladov´a et al., 2008), Danish and Italian parallel treebanks (Buch-Kromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), German (Stede, 2004; Stede, 2008), Hindi (Oza et al., 2009), and Turkish (Zeyrek et al., 2010). The potential value of being able to automatically recognize these discourse relations, their arguments and their senses comes from their help in question generation (Mannem et al., 2010), extractive summarization (Louis et al., 2010) and sentiment detection (Taboada et al., 2009). So efforts are increasing to automatically recognize them (Elwell and Baldridge,"
W11-4603,kingsbury-palmer-2002-treebank,0,\N,Missing
W11-4603,liakata-etal-2010-corpora,0,\N,Missing
W11-4603,mladova-etal-2008-sentence,0,\N,Missing
W11-4603,D08-1035,0,\N,Missing
W11-4603,P94-1002,0,\N,Missing
W11-4603,P03-1071,0,\N,Missing
W11-4603,P06-1004,0,\N,Missing
W11-4603,W10-4327,0,\N,Missing
W11-4603,W06-3309,0,\N,Missing
W11-4603,N09-1017,0,\N,Missing
W11-4603,I08-1050,0,\N,Missing
W11-4603,W10-1844,0,\N,Missing
W12-3205,J95-2003,1,0.611654,"Missing"
W12-3205,J97-1003,0,0.229511,"t and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). topics such the geography of a country, followed by its history, its demographics, its economy, its legal structures, etc. Segmentation is usually done on a sentence-by-sentence basis, with segments not assumed to overlap. Methods for topic segmenation emply semantic, lexical and referential similarity or, more recently, language models (Bestgen, 2006; Chen et al., 2009; Choi et al., 2001; Eisenstein and Barzilay, 2008; Galley et al., 2003; Hearst, 1997; Malioutov and Barzilay, 2006; Purver et al., 2006; Purver, 2011). Functional structure and automated functional segmentation aims to identify sections within a discourse that serve different functions. These functions are genre-specific. In the case of scientific journals, high-level sections generally include the Background (work that motivates the objectives of the work and/or the hypothesis or claim being tested), followed by its Methods, and Results, ending with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage"
W12-3205,I08-1050,0,0.166124,"done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is used to refer to any segmentation of a discourse into a “high-level” linear structure. Nevertheless, segmentation by function exploits different features (and in some cases, dif45 ferent methods) than segmentation by topic, so they are worth keeping distinct. Attention to event structure and the identification of"
W12-3205,W98-1123,0,0.0833374,"l unaddressed. 4.1 Evidence for discourse structures The first issue has to do with what should be taken as evidence for a particular discourse structure. While one could simply consider all features that can be computed reliably and just identify the most accurate predictors, this is both expensive and, in the end, unsatisfying. With topic structure, content words do seem to provide compelling evidence for segmentation, either using language models or semantic relatedness. On the other hand, this might be improved through further evidence in the form of entity chains, as explored earlier in (Kan et al., 1998), but using today’s more accurate approaches to automated coreference recognition (Strube, 2007; Charniak and Elsner, 2009; Ng, 2010). Whatever the genre, evidence for function structure seems to come from the frequency and distribution of closed-class words, particular phrases (or phrase patterns), and in the case of speech, intonation. So, for example, Niekrasz (2012) shows that what he calls participant-relational features that indicate the participants relationships to the text provide convincing evidence for segmenting oral narrative by the type of narrative activity taking place. These f"
W12-3205,J97-3006,0,0.111836,"Missing"
W12-3205,W00-1411,0,0.113483,"Missing"
W12-3205,liakata-etal-2010-corpora,0,0.0987406,"ion aims to identify sections within a discourse that serve different functions. These functions are genre-specific. In the case of scientific journals, high-level sections generally include the Background (work that motivates the objectives of the work and/or the hypothesis or claim being tested), followed by its Methods, and Results, ending with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage of a new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Reco"
W12-3205,W06-3309,0,0.166454,"with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage of a new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is u"
W12-3205,P06-1004,0,0.0177177,"t of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). topics such the geography of a country, followed by its history, its demographics, its economy, its legal structures, etc. Segmentation is usually done on a sentence-by-sentence basis, with segments not assumed to overlap. Methods for topic segmenation emply semantic, lexical and referential similarity or, more recently, language models (Bestgen, 2006; Chen et al., 2009; Choi et al., 2001; Eisenstein and Barzilay, 2008; Galley et al., 2003; Hearst, 1997; Malioutov and Barzilay, 2006; Purver et al., 2006; Purver, 2011). Functional structure and automated functional segmentation aims to identify sections within a discourse that serve different functions. These functions are genre-specific. In the case of scientific journals, high-level sections generally include the Background (work that motivates the objectives of the work and/or the hypothesis or claim being tested), followed by its Methods, and Results, ending with a Discussion of the results or outcomes, along with conclusions to be drawn. Finergrained segments might include the advantage of a new method (method-new-ad"
W12-3205,J00-3005,0,0.317433,"priate entity and over-writing previous buffer entries when the buffer was full. The next wave of work in computational discourse processing sought greater generality through stronger theoretical grounding, appealing to thencurrent theories of discourse such as Centering Theory (Grosz et al., 1986; Grosz et al., 1995), used as a basis for anaphor resolution (Brennan et al., 1987; Walker et al., 1997; Tetreault, 2001) and text generation (Kibble and Power, 2000), Rhetorical Structure Theory (Mann and Thompson, 1988), used as a basis for text generation (Moore, 1995) and document summarization (Marcu, 2000b), and Grosz and Sidner’s theory of discourse based on intentions (Grosz and Sidner, 1986a) and shared plans (Grosz and Sidner, 1990), used in developing animated agents (Johnson and Rickel, 2000). Issues related to fully characterizing centering are explored in great detail in (Kehler, 1997) and (Poesio et al., 2004). The approaches considered during this period never saw more than a few handfuls of examples. But, as has been clear from developments in PoStagging, Named Entity Recognition and parsing, Language Technology demands approaches that can deal with whatever data are given them. So"
W12-3205,P07-1075,0,0.0175543,"d be able to automate this through understanding the various ways that information is conveyed in discourse. Other examples of LT applications already benefitting from recognizing and applying discourse-level information include automated assessment of student essays (Burstein and Chodorow, 2010); summarization (Thione et al., 2004), infor42 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 42–54, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics mation extraction (Patwardhan and Riloff, 2007; Eales et al., 2008; Maslennikov and Chua, 2007), and more recently, statistical machine translation (Foster et al., 2010). These are described in more detail in (Webber et al., 2012). Our aim here then, on this occasion of ACL’s 50th Annual Meeting, is to briefly describe the evolution of computational approaches to discourse structure, reflect on where the field currently stands, and what new challenges it faces in trying to deliver on its promised benefit to Language Technology. 2 2.1 Background Early Methods The challenges mentioned above are not new. Question-Answering systems like LUNAR (Woods, 1968; Woods, 1978) couldn’t answer succe"
W12-3205,mladova-etal-2008-sentence,0,0.0649322,"Missing"
W12-3205,J92-4007,0,0.361511,"ering the condition s3 s1 condition (a) s2 s1 motivation s2 motivation s3 (b) Figure 1: Proposed discourse structures for Ex. 4: (a) In terms of informational relations; (b) in terms of intentional relations words in a sentence. At issue though was the nature of the structure. One issue concerned the nature of the relation between parent and child nodes in a discourse tree, and/or the relation between siblings. While Rhetorical Structure Theory (Mann and Thompson, 1988) posited a single discourse relation holding between any two discourse units (i.e., units projecting to adjacent text spans), Moore and Pollack (1992) gave an example of a simple discourse (Ex. 4) in which different choices about the discourse relation holding between pairs of units, implied different and nonisomorphic structures. (4) Come home by 5:00. s1 Then we can go to the hardware store before it closes. s2 That way we can finish the bookshelves tonight. s3 Example 4 could be analysed purely in terms of information-based discourse relations, in which s1 specified the CONDITION under which s2 held, which in turn specified the CONDITION under which s3 held. This would make s1 subordinate to s2, which in turn would be subordinate to s3,"
W12-3205,J96-3006,0,0.154458,"Missing"
W12-3205,P10-1142,0,0.198904,"ears have seen progress to differing degrees on at least four different types of discourse structures: topic structure, functional structure, event structure, and a structure of coherence relations. First we say a bit about the structures, and then about the resources employed in recognizing and labelling them. 3.1 Types of discourse structures Topic structure and automated topic segmentation aims to break a discourse into a linear sequence of 1 For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). topics such the geography of a country, followed by its history, its demographics, its economy, its legal structures, etc. Segmentation is usually done on a sentence-by-sentence basis, with segments not assumed to overlap. Methods for topic segmenation emply semantic, lexical and referential similarity or, more recently, language models (Bestgen, 2006; Chen et al., 2009; Choi et al., 2001; Eisenstein and Barzilay, 2008; Galley et al., 2003; Hearst, 1997; Malioutov and Barzilay, 2006; Purver et al., 2006; Purver, 2011). Functional structure and automated functional segmentati"
W12-3205,W09-3029,1,0.844135,", as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous computational approaches encountered, it also reveals new ones, while leaving some earlier problems still unaddressed. 4.1 Evidence for discourse structures The"
W12-3205,D07-1075,0,0.0167337,"e sequence of simple sentences. Researchers should be able to automate this through understanding the various ways that information is conveyed in discourse. Other examples of LT applications already benefitting from recognizing and applying discourse-level information include automated assessment of student essays (Burstein and Chodorow, 2010); summarization (Thione et al., 2004), infor42 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 42–54, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics mation extraction (Patwardhan and Riloff, 2007; Eales et al., 2008; Maslennikov and Chua, 2007), and more recently, statistical machine translation (Foster et al., 2010). These are described in more detail in (Webber et al., 2012). Our aim here then, on this occasion of ACL’s 50th Annual Meeting, is to briefly describe the evolution of computational approaches to discourse structure, reflect on where the field currently stands, and what new challenges it faces in trying to deliver on its promised benefit to Language Technology. 2 2.1 Background Early Methods The challenges mentioned above are not new. Question-Answering systems like LUNAR"
W12-3205,P10-1056,0,0.0156477,"y (LT) researchers should care about discourse: Rather, discourse can enable LT to overcome known obstacles to better performance. Consider automated summarization and machine translation: Humans regularly judge output quality in terms that include referential clarity and coherence. Systems can only improve here by paying attention to discourse — i.e., to linguistic features above the level of ngrams and single sentences. (In fact, we predict that as soon as cheap — i.e., non-manual – methods are found for reliably assessing these features — for example, using proxies like those suggested in (Pitler et al., 2010) — they will supplant, or at least complement today’s common metrics, Bleu and Rouge that say little about what matters to human text understanding (Callison-Burch et al., 2006).) Consider also work on automated text simplification: One way that human editors simplify text is by re-expressing a long complex sentence as a discourse sequence of simple sentences. Researchers should be able to automate this through understanding the various ways that information is conveyed in discourse. Other examples of LT applications already benefitting from recognizing and applying discourse-level information"
W12-3205,J04-3003,0,0.0349316,"Missing"
W12-3205,prasad-etal-2008-penn,1,0.854536,"n made available for other researchers. For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2 . For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/a"
W12-3205,prasad-etal-2010-exploiting,1,0.929235,"olf and Gibson, 2005). If such a cover is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a; Wellner, 2008; Wellner and Pustejovsky, 2007). 3.2 Resources for discourse structure All automated systems for segmenting and labelling text are grounded in data — whether the data has informed the manual creation of rules or has been a source of features for an approach based on machine learning. In the case of topic structure and high-level functional structure, there is now a substantial amount of data that is freely available. For other types of discourse structure, manual annotation has been required and, depending on the type of structure, different amounts are currently available. Mo"
W12-3205,C10-2118,1,0.917829,"olf and Gibson, 2005). If such a cover is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a; Wellner, 2008; Wellner and Pustejovsky, 2007). 3.2 Resources for discourse structure All automated systems for segmenting and labelling text are grounded in data — whether the data has informed the manual creation of rules or has been a source of features for an approach based on machine learning. In the case of topic structure and high-level functional structure, there is now a substantial amount of data that is freely available. For other types of discourse structure, manual annotation has been required and, depending on the type of structure, different amounts are currently available. Mo"
W12-3205,P06-1003,0,0.0671148,"Missing"
W12-3205,rysova-2012-alternative,0,0.0176292,"l to have allowed a conventional landing. What’s more, the seven mail personnel aboard were missing. [wsj 0550] (7) The two companies each produce market pulp, containerboard and white paper. That means goods could be manufactured closer to customers, saving shipping costs, he said. [wsj 0317] The discovery of these other forms of evidence3 raises the question of when it is that a word or phrase signals a discourse relation. For example, only 15 of the 33 tokens of that means in the PDTB were annotated as evidence of a discourse relation. While the 3 which English is not alone in having, cf. (Rysova, 2012) 47 three paragraph-initial instances were left unannotated due to resource limitations (ie, no paragraph initial sentences were annotated unless they contained an explicit discourse connective), the majority were ignored because they followed an explicit connective. As Wiebe’s example (5) showed, there can be multiple explicit discourse connectives in a clause, each of which is evidence for a separate discourse relation (albeit possibly between the same arguments). All of these are annotated in the PDTB – eg, both but and then in (8) Congress would have 20 days to reject the package with a 50"
W12-3205,W04-0213,0,0.0257582,"For all other kinds of discourse structures, dedicated manual annotation has been required, both for segmentation and labelling, and many of these resources have been made available for other researchers. For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2 . For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseanno"
W12-3205,J01-4003,0,0.0608202,"Missing"
W12-3205,J02-4002,0,0.0515376,"clude the advantage of a new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is used to refer to any segmentation of a discourse into a “high-level” linear structure. Nevertheless, segmentation by fu"
W12-3205,D09-1155,0,0.0119415,"new method (method-new-advantage) or of an old method (method-old-advantage) or the disadvantage of one or the other (Liakata et al., 2010). Again, segmentation is usually done on a sentenceby-sentence basis, with sentences not assumed to fill more than one function. Methods for functional segmentation have employed specific cue words and phrases, as well as more general language models (Burstein et al., 2003; Chung, 2009; Guo et al., 2010; Kim et al., 2010; Lin et al., 2006; McKnight and Srinivasan, 2003; Ruch et al., 2007; Mizuta et al., 2006; Palau and Moens, 2009; Teufel and Moens, 2002; Teufel et al., 2009; Agarwal and Yu, 2009). The BIO approach to sequential classication (Beginning/Inside/Outside) used in Named Entity Recognition has also proved useful (Hirohata et al., 2008), recognizing that the way the start of a functional segement is signalled may differ from how it is continued. Note that topic segmentation and functional segmentation are still not always distinguished. For example, in (Jurafsky and Martin, 2009), the term discourse segmentation is used to refer to any segmentation of a discourse into a “high-level” linear structure. Nevertheless, segmentation by function exploits diffe"
W12-3205,W04-1009,0,0.219571,"Missing"
W12-3205,tonelli-etal-2010-annotation,1,0.827173,"er and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous computational approaches encountered, it also reveals new ones, while leaving some earlier problems still unaddressed. 4.1 Evidence for discourse structures The first issue has to do with what should be taken as evidence for a particular discourse structure. While one could simply consider all features that can be computed reliably and just identify the most accurate predictors, this is b"
W12-3205,W98-0315,1,0.650817,"egmentation and labelling, and many of these resources have been made available for other researchers. For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2 . For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 201"
W12-3205,D07-1010,0,0.0306337,"r is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a; Wellner, 2008; Wellner and Pustejovsky, 2007). 3.2 Resources for discourse structure All automated systems for segmenting and labelling text are grounded in data — whether the data has informed the manual creation of rules or has been a source of features for an approach based on machine learning. In the case of topic structure and high-level functional structure, there is now a substantial amount of data that is freely available. For other types of discourse structure, manual annotation has been required and, depending on the type of structure, different amounts are currently available. More specifically, work on topic structure and seg"
W12-3205,W93-0239,0,0.165457,"ther issue during this period concerned the nature of discourse structure: Was it really a tree? Sibun (1992), looking at people’s descriptions of the layout of their house or apartment, argued that they resembled different ways of linearizing a graph of the rooms and their connectivity through doors and 44 halls. None of these linearizations were trees. Similarly, Knott et al. (2001), looking at transcriptions of museum tours, argued that each resembled a linear sequence of trees, with one or more topic-based connections between their root nodes — again, not a single covering tree structure. Wiebe (1993), looking at simple examples such as (5) The car was finally coming toward him. s1 He finished his diagnostic tests, s2 feeling relief. s3 But then the car started to turn right. s4 pointed multiple lexical items explicitly relating a clause to multiple other clauses. Here, but would relate s4 to s3 via a CONTRAST relation, while then would relate s4 to s2 via a temporal SUCCESSION relation. The most well-known of work from this period is that of Mann and Thompson (1988), Grosz and Sidner (1986b), Moore and Moser (1996), Polanyi and van den Berg (1996), and Asher and Lascarides (2003).1 The wa"
W12-3205,J05-2005,0,0.0394554,", 2011; Finlayson, 2009). The automated identification of discourse relations aims to identify discourse relations such as CONDITION and MOTIVATION , as in Example 4, and CONTRAST and SUCCESSION, as in Example 5. These have also been called coherence relations or rhetorical relations. Methods used depend on whether or not a text is taken to be divisible into a covering sequence of a non-overlapping discourse units related to adjacent units by discourse relations as in Rhetorical Structure Theory (Mann and Thompson, 1988) or to both adjacent and nonadjacent units as in the Discourse GraphBank (Wolf and Gibson, 2005). If such a cover is assumed, methods involve parsing a text into units using lexical and punctuational cues, followed by labelling the relation holding between them (Marcu, 2000a; Marcu, 2000b; Wolf and Gibson, 2005). If text is not assumed to be divisible into discourse units, then methods involve finding evidence for discourse relations (including both explicit words and phrases, and clausal and sentential adjacency) and their arguments, and then labelling the sense of the identified relation (Elwell and Baldridge, 2008; Ghosh et al., 2011; Lin et al., 2010; Lin, 2012; Prasad et al., 2010a;"
W12-3205,W05-0312,0,0.026529,"relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the prob"
W12-3205,I08-7009,1,0.817926,"corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous computational approaches encountered, it also reveals new ones, while leaving some earlier problems still unaddressed. 4.1 Evidence for discourse structures The first issue has to do with what should"
W12-3205,W09-3006,0,0.0555721,"Missing"
W12-3205,W10-1844,0,0.0541576,"Missing"
W12-3205,P12-1008,0,0.0224601,"nnotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004). For discourse relations annotated in the lexicallygrounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (Al-Saif and Markert, 2010; Al-Saif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladov´a et al., 2008), Danish (Buch-Kromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010). Also available are discourseannotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010). 2 http://www.aber.ac.uk/en/cs/research/cb/projects/art/artcorpus/ 46 4 New challenges Although the largely empirically-grounded, multistructure view of discourse addresses some of the problems that previous co"
W12-3205,D11-1068,0,\N,Missing
W12-3205,W01-0514,0,\N,Missing
W12-3205,E06-1032,0,\N,Missing
W12-3205,miltsakaki-etal-2004-penn,1,\N,Missing
W12-3205,W11-0401,0,\N,Missing
W12-3205,D11-1027,0,\N,Missing
W12-3205,N09-1042,0,\N,Missing
W12-3205,D08-1035,0,\N,Missing
W12-3205,al-saif-markert-2010-leeds,0,\N,Missing
W12-3205,W01-1605,0,\N,Missing
W12-3205,P03-1071,0,\N,Missing
W12-3205,D08-1021,0,\N,Missing
W12-3205,P87-1022,0,\N,Missing
W12-3205,J06-1002,0,\N,Missing
W12-3205,E09-1018,0,\N,Missing
W12-3205,P08-1090,0,\N,Missing
W12-3205,J86-3001,0,\N,Missing
W12-3205,W10-1817,0,\N,Missing
W12-3205,W10-1913,0,\N,Missing
W13-0124,N09-1016,0,0.0991054,". ”. [Research Fortnight, 28 April 2004] Other DE predicates that appear in Arg1 of explicit CHOSEN ALTERNATIVE in the PDTB or BioDRB are shown in Figure 1: This list, although long, is only a subset of DE constructions. What about other ones? Since neither dictionaries nor other lexical resources record direction of entailing as a property, DanescuNiculescu-Mizil et al. (2009) attempted to extract DE constructions from the large BLLIP corpus (LDC catalogue LDC2000T43), using cooccurence with Negative Polarity Items (NPI) like “any” as a cue. Figure 2 shows the 55 most frequent DE lemmas that Danescu-Niculescu-Mizil et al. (2009) extracted from the corpus: Four are negation markers or contain them (cannot, never, nobody, nothing), twelve have attested occurences in Arg1 of instead in the PDTB or BioDRB (as indicated in Figure 1), and all but two of the others (compensate for and essential for) can be found on the web in similar Arg1 position as the attested forms. Why does neither compensate for nor essential for seem to license an alternative being excluded from consideration, as in (16) Olivia compensates for eating by exercising. *Instead she ?? (17) Talent is essential for singing. *Instead ??? First, observe that"
W13-0124,P12-1007,0,0.0273016,"more theoretically-motivated features and their (exclusive) values. In later work, Knott and Sanders (1998) show how the approach works for subsets of connectives in both English and Dutch. It does not, however, explain how the same coherence relations may be seen to hold when connectives are absent. Automated approaches to recognizing coherence relations do not assume that their sense arises solely from connectives. Rather, these approaches take as evidence, lexical and syntactic features of the arguments of the coherence relation, nearby coherence relations, high-level text structure, etc. (Feng and Hirst, 2012; Ghosh et al., 2011; Hernault et al., 2010; Lin et al., 2010; Lin, 2012; Marcu, 2000; Marcu and Echihabi, 2002; Sagae, 2009; Sporleder and Lascarides, 2008; Subba et al., 2006). As one might expect, automated approaches use simple features that can be computed reliably. However, performance in recognizing coherence relations in the absense of connectives is still low, and significant improvement is unlikely to come from simply trying new Machine Learning methods over the same set of simple features. A bigger pay-off might come from identifying more predictive features. That is the goal of the"
W13-0124,I11-1120,0,0.0427151,"ivated features and their (exclusive) values. In later work, Knott and Sanders (1998) show how the approach works for subsets of connectives in both English and Dutch. It does not, however, explain how the same coherence relations may be seen to hold when connectives are absent. Automated approaches to recognizing coherence relations do not assume that their sense arises solely from connectives. Rather, these approaches take as evidence, lexical and syntactic features of the arguments of the coherence relation, nearby coherence relations, high-level text structure, etc. (Feng and Hirst, 2012; Ghosh et al., 2011; Hernault et al., 2010; Lin et al., 2010; Lin, 2012; Marcu, 2000; Marcu and Echihabi, 2002; Sagae, 2009; Sporleder and Lascarides, 2008; Subba et al., 2006). As one might expect, automated approaches use simple features that can be computed reliably. However, performance in recognizing coherence relations in the absense of connectives is still low, and significant improvement is unlikely to come from simply trying new Machine Learning methods over the same set of simple features. A bigger pay-off might come from identifying more predictive features. That is the goal of the current work. The p"
W13-0124,P02-1047,0,0.028052,"show how the approach works for subsets of connectives in both English and Dutch. It does not, however, explain how the same coherence relations may be seen to hold when connectives are absent. Automated approaches to recognizing coherence relations do not assume that their sense arises solely from connectives. Rather, these approaches take as evidence, lexical and syntactic features of the arguments of the coherence relation, nearby coherence relations, high-level text structure, etc. (Feng and Hirst, 2012; Ghosh et al., 2011; Hernault et al., 2010; Lin et al., 2010; Lin, 2012; Marcu, 2000; Marcu and Echihabi, 2002; Sagae, 2009; Sporleder and Lascarides, 2008; Subba et al., 2006). As one might expect, automated approaches use simple features that can be computed reliably. However, performance in recognizing coherence relations in the absense of connectives is still low, and significant improvement is unlikely to come from simply trying new Machine Learning methods over the same set of simple features. A bigger pay-off might come from identifying more predictive features. That is the goal of the current work. The particular coherence relation of interest here is one that holds when the discourse connecti"
W13-0124,J92-4007,0,0.695817,"PARISON .C ONTRAST relation? 2. Are the features that allow Arg1 to be interpreted as an excluded alternative, sufficient to label an implicit discourse relation as having the sense CHOSEN ALTERNATIVE, or do negation markers, DE constructions, event modals and other less frequent licensers of excluded alternatives occur in Arg1 of other constructions? 3. What features suggest that the two arguments of a coherence relations denote alternatives? With respect to the first question, researchers since Mann and Thompson (1988) have drawn a distinction between SEMANTIC and PRAGMATIC relations, which Moore and Pollack (1992) call INFORMATIONAL and INTENTIONAL, respectively. Moore and Pollack (1992) make a convincing arguement that relations of both types can hold simultaneously. With respect to instead, I think it can be argued that it conveys a purely informational relation. That is, instead (when it is not in construction with of, followed by the alternative being excluded) is anaphoric: Its excluded alternative must be derived from the (previous) discourse context. The most common thing that a speaker does with this excluded alternative may be to compare or contrast it with the alternative still in considerati"
W13-0124,prasad-etal-2008-penn,1,0.928762,"omated approaches use simple features that can be computed reliably. However, performance in recognizing coherence relations in the absense of connectives is still low, and significant improvement is unlikely to come from simply trying new Machine Learning methods over the same set of simple features. A bigger pay-off might come from identifying more predictive features. That is the goal of the current work. The particular coherence relation of interest here is one that holds when the discourse connective instead is present, but can also hold when it isn’t. In the Penn Discourse TreeBank 2.0 (Prasad et al., 2008), the sense is called CHOSEN ALTERNATIVE. It is defined as holding when “two alternatives are evoked in the discourse but only one is taken” — meaning still being considered while the other isn’t (The PDTB Research Group, 2008). Such a definition leads to two questions: What, if any, features suggest that the two arguments of a coherence relation denote alternatives, and what, if any, features indicate that one of them has been excluded from further consideration? As Sporleder and Lascarides (2008) argue, one should not assume a priori that the same features will be at work when a connective i"
W13-0124,W09-3813,0,0.0276035,"ks for subsets of connectives in both English and Dutch. It does not, however, explain how the same coherence relations may be seen to hold when connectives are absent. Automated approaches to recognizing coherence relations do not assume that their sense arises solely from connectives. Rather, these approaches take as evidence, lexical and syntactic features of the arguments of the coherence relation, nearby coherence relations, high-level text structure, etc. (Feng and Hirst, 2012; Ghosh et al., 2011; Hernault et al., 2010; Lin et al., 2010; Lin, 2012; Marcu, 2000; Marcu and Echihabi, 2002; Sagae, 2009; Sporleder and Lascarides, 2008; Subba et al., 2006). As one might expect, automated approaches use simple features that can be computed reliably. However, performance in recognizing coherence relations in the absense of connectives is still low, and significant improvement is unlikely to come from simply trying new Machine Learning methods over the same set of simple features. A bigger pay-off might come from identifying more predictive features. That is the goal of the current work. The particular coherence relation of interest here is one that holds when the discourse connective instead is"
W13-0124,W06-2605,0,0.0161652,"and Dutch. It does not, however, explain how the same coherence relations may be seen to hold when connectives are absent. Automated approaches to recognizing coherence relations do not assume that their sense arises solely from connectives. Rather, these approaches take as evidence, lexical and syntactic features of the arguments of the coherence relation, nearby coherence relations, high-level text structure, etc. (Feng and Hirst, 2012; Ghosh et al., 2011; Hernault et al., 2010; Lin et al., 2010; Lin, 2012; Marcu, 2000; Marcu and Echihabi, 2002; Sagae, 2009; Sporleder and Lascarides, 2008; Subba et al., 2006). As one might expect, automated approaches use simple features that can be computed reliably. However, performance in recognizing coherence relations in the absense of connectives is still low, and significant improvement is unlikely to come from simply trying new Machine Learning methods over the same set of simple features. A bigger pay-off might come from identifying more predictive features. That is the goal of the current work. The particular coherence relation of interest here is one that holds when the discourse connective instead is present, but can also hold when it isn’t. In the Pen"
W13-3303,W11-1211,1,0.842447,"elp raise its BLEU score by more closely resembling its more implicit human reference text. Introduction Discourse connectives (DCs), a class of frequent cohesive markers, such as although, however, for example, in addition, since, while, yet, etc., are especially prone to ‘translationese’, i.e. the use of constructions in the target language (TL) that differ in frequency or position from how they would be found in texts born in the language. That is, ’translationese’ makes DCs prone to being translated in ways that can differ markedly from their use in the source language. (Blum-Kulka, 1986; Cartoni et al., 2011; Ilisei et al., 2010; Halverson, 2004; Hansen-Schirra et al., 2007; Zufferey et al., 2012). For cohesive markers and DCs, Koppel and Ordan (2011) and Cartoni et al. (2011) have shown that they may be more explicit (increased use) or less explicit (decreased use) in translationese. The paper focuses on the latter case, but the same detection method can be applied in reverse, in order to find increased use (explicitation) as well. In English about 100 types of explicit DCs have been annotated in the Penn Discourse TreeBank, The paper presents work in progress on a corpus study where zero-transl"
W13-3303,J03-1002,0,0.00515195,"a frequency above 20 in the Penn Discourse TreeBank Version 2.0 (Prasad et al., 2008). In order to identify which discourse relations are most frequently translated as zero, we have assigned each of the EN DCs the level-2 discourse relation that it is most frequently associated with in the PDTB corpus. The total list of EN connectives is given in Table 1. For every source connective, we queried its most frequent target connective translations from the online dictionary Linguee2 and added them to dictionaries of possible FR and DE equivalents. With these dictionaries and Giza++ word alignment (Och and Ney, 2003), the SL connectives can be located and the sentences of its translation (reference and/or automatic) can be scanned for an aligned occurrence of the TL dictionary entries. If more than one DC appears in the source sentence and/or a DC is not aligned with a connective or connective-equivalent found in the dictionaries, the word position (word index) of the SL connective is compared to the word indexes of the translation in order to detect whether a TL connective (or connective-equivalent from the dictionaries) appears in a 5-word window to its left and right.3 . This also helps filtering out c"
W13-3303,prasad-etal-2008-penn,1,0.229205,"3.2). It contains two EN connectives — as and otherwise — that were annotated in the PDTB1 . Using the set of discourse relations of the PDTB, as can be said to signal the discourse relation CAUSE (subtype Reason), and otherwise the discourse relation ALTER NATIVE. This is discussed further in Section 3.1. 3 Semi-automatic detection of zero-translations 3.1 Method The semi-automatic method that identifies zero- or non-connective translations in human references and machine translation output is based on a list of 48 EN DCs with a frequency above 20 in the Penn Discourse TreeBank Version 2.0 (Prasad et al., 2008). In order to identify which discourse relations are most frequently translated as zero, we have assigned each of the EN DCs the level-2 discourse relation that it is most frequently associated with in the PDTB corpus. The total list of EN connectives is given in Table 1. For every source connective, we queried its most frequent target connective translations from the online dictionary Linguee2 and added them to dictionaries of possible FR and DE equivalents. With these dictionaries and Giza++ word alignment (Och and Ney, 2003), the SL connectives can be located and the sentences of its transl"
W13-3303,C10-2118,1,0.451323,"Missing"
W13-3303,P07-2045,0,\N,Missing
W13-3303,P11-1132,0,\N,Missing
W13-3303,2005.mtsummit-papers.11,0,\N,Missing
W15-1003,W09-0437,0,0.0178963,"We use the constrained decoding feature included in Moses (Koehn et al., 2007) to this purpose. In its basic implementation, constrained decoding assesses the degree of overlap between hypothesis and reference sentence; given a source span, the feature function assigns a score to each of the target hypothesis as follows:  1 if ∃ h ∈ Hp ∧ h ∈ Rp sconstrDec = −∞ if 6 ∃ h ∈ Hp ∧ h ∈ Rp where h is a phrase in the hypothesis phrase set Hp and Rp is the set of reference phrases. Constrained decoding can potentially reveal induction errors and distinguish between search and model errors. Following Auli et al. (2009), we try to increase the translation option limit parameter which determines how many target translations are considered for each source span; if larger values lead to the system being able to decode more references, induction errors are occurring. Using the same heuristics as Wisniewski and Yvon (2013), we can also distinguish between search error vs. model errors by checking whether the oracle has a total model score higher than the previous 1-best output or vice versa. We also take into consideration the interaction between induction and search errors. A bigger search space would be needed"
W15-1003,J12-2006,0,0.0179799,"king to address these weaknesses using a chart analysis based on oracle hypotheses, guided by the negation elements contained in a source span and by how these elements are expected to be translated at each decoding step. Preliminary results show chart analysis is able to give a more in-depth analysis of the above errors and better explains the results of the manual analysis. 1 Introduction In recent years there has been increasing interest in improving the quality of SMT systems over a wide range of linguistic phenomena, including coreference resolution (Hardmeier et al., 2014) and modality (Baker et al., 2012). Negation, however, is a problem that has still not been researched thoroughly (section 2). Our previous study (Fancellu and Webber, 2015) takes a first step towards understanding why negation is a problem in SMT, through manual analysis of the kinds of errors involved in its translation. Our error analysis employs a small set of standard stringbased operations, applying them to the semantic elements involved in the meaning of negation (section 3). The current paper describes our current work on understanding the causes of these errors. Focussing on the distinction between induction, search a"
W15-1003,J07-2003,0,0.0354396,"1) To overcome these problems, we propose the use of an oracle hypothesis, instead of an oracle sentence, that relies uniquely on the negation elements contained in the source span and how these are expected to be translated in the target hypothesis at a given time during decoding (section 4.2). Sections 5 and 6 report results of the analysis on a Chinese-to-English Hierarchical Phrase Based 21 Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 21–29, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Model (Chiang, 2007). We show that even if it possible to detect the presence of model errors through the use of an oracle sentence, computing an oracle hypotheses at each step during decoding offers a more robust, in-depth analysis around the problem of translating negation and helps explaining the errors observed during the manual analysis. 2 Previous Work While recent years have seen work on automatically detecting negation in monolingual texts (Chowdhury and Mahbub, 2012; Read et al., 2012), SMT has mainly considered it a side problem. For this reason, no actual analysis on the type of errors involved in tran"
W15-1003,S12-1045,0,0.0185309,"o the ”long tail” of rare occurrences. The main two challenges at this point are to know (a) which elements in the source are negation elements and (b) whether they are translated correctly in the target hypothesis. In the case of (a) we use the manual annotation presented in (Fancellu and Webber, 2015). Future work will try to automate the process. Challenge (b) requires a way to compute those expectations on the target (English) side. In order to detect the presence of a cue, we build a list of English negation cues from the training data using the exact same heuristics and training data as Chowdhury and Mahbub (2012) and check whether a given hypothesis contains a cue from this list. In order to deal with those cases of lexical negation where cues in the source are rendered as part of the meaning of a word in the target (e.g. zh: but´ ` ong → en: ‘different’), we extract a mapping between Chinese cues and these words covertly expressing negation from the manually aligned GALE ChineseEnglish Word Alignment and Tagging Training data (Li et al., 2012). In order to recognise the presence of a correct event, it is possible to check whether the hypothesis contains a good translation of the source using bilingua"
W15-1003,P05-1066,0,0.170439,"Missing"
W15-1003,E14-1063,1,0.908675,"rd approach has been to formulate a hypothesis about what can go wrong when translating negation, modify the SMT system in a way aimed at reducing the number of times that happens, and then assume that any increase in BLEU score - the standard automatic evaluation metric used in SMT - confirms the initial hypothesis. Collins et al. (2005) and Li et al. (2009) considers negation, along with other linguistic phenomena, as a problem of structural mismatch between source and target; Wetzel and Bond (2012) considers it instead as a problem of training data sparsity; finally Baker et al. (2012) and Fancellu and Webber (2014) considers it as a model problem, where the system needs enhancement with respect to the semantics of negation. Only a few efforts have tried to investigate errors occurring during decoding. Automatic evaluation metrics are in fact only informative about the quality of the output, but not about the decoding process that produces the output. As such, the most relevant related work are two studies on the main categories of errors during decoding (Auli et al., 2009; Wisniewski and Yvon, 2013). Both works use the reference sentence as a proxy to generate an oracle hypothesis but they differ in the"
W15-1003,W15-1301,1,0.62477,"ource span and by how these elements are expected to be translated at each decoding step. Preliminary results show chart analysis is able to give a more in-depth analysis of the above errors and better explains the results of the manual analysis. 1 Introduction In recent years there has been increasing interest in improving the quality of SMT systems over a wide range of linguistic phenomena, including coreference resolution (Hardmeier et al., 2014) and modality (Baker et al., 2012). Negation, however, is a problem that has still not been researched thoroughly (section 2). Our previous study (Fancellu and Webber, 2015) takes a first step towards understanding why negation is a problem in SMT, through manual analysis of the kinds of errors involved in its translation. Our error analysis employs a small set of standard stringbased operations, applying them to the semantic elements involved in the meaning of negation (section 3). The current paper describes our current work on understanding the causes of these errors. Focussing on the distinction between induction, search and model errors, we point out the challenges in trying to use existing techniques to quantify these three types of errors in the context of"
W15-1003,N13-1092,0,0.0195794,"Missing"
W15-1003,W08-0509,0,0.0138116,"dency parse (Manning, 2008) and apply it to each of the target (English) hypothesis in the cell’s stack to check whether a subordinatehead relation is established between the two. Given 1 http://www.mdbg.net/chindict/chindict.php?page=cedict 5 System We carried out the error analysis on the output of the Chinese-to-English hierarchical phrase based system submitted by the University of Edinburgh for the NIST12 MT evaluation campaign. The system was trained on ∼2.1 millions length-filtered segments in the news domain, with 44678806 tokens on the source and 50452704 on the target, with MGIZA++ (Gao and Vogel, 2008) used for alignment. The Chinese side of the training and the test set were segmented using the LDCWordSegmenter. The system was tuned using MERT (Och, 2003) on the NIST06 set. The automatic error analysis was carried out on a sub-set of 54 segments the NIST MT08 test set2 , each containing at least an instance of negation on the source side. Although small, this set was considered to be representative given that it clearly shows a pattern in the errors involved in translation negation. 2 This sub-set containing only negative sentences was extracted during the manual evaluation. Out of 1357 se"
W15-1003,P01-1030,0,0.147297,"Missing"
W15-1003,P07-2045,0,0.00386961,"tion higher than the optimal one. This is because the scoring function lacks relevant features or the features present have not been properly weighted. • Induction error: e cannot be generated because its components (phrases or rules) are absent from the search space. 4.1 Constrained Decoding The first technique involves forcing the decoder to reproduce reference sentences if they contain negation. It reflects the assumption that if the system is able to reconstruct such oracles, it is potentially able to translate negation correctly. We use the constrained decoding feature included in Moses (Koehn et al., 2007) to this purpose. In its basic implementation, constrained decoding assesses the degree of overlap between hypothesis and reference sentence; given a source span, the feature function assigns a score to each of the target hypothesis as follows:  1 if ∃ h ∈ Hp ∧ h ∈ Rp sconstrDec = −∞ if 6 ∃ h ∈ Hp ∧ h ∈ Rp where h is a phrase in the hypothesis phrase set Hp and Rp is the set of reference phrases. Constrained decoding can potentially reveal induction errors and distinguish between search and model errors. Following Auli et al. (2009), we try to increase the translation option limit parameter w"
W15-1003,W09-0433,0,0.0325119,"Missing"
W15-1003,S12-1035,0,0.0171339,"ed analysis described in section 4. The manual error analysis makes two assumptions: • the semantic structure of negation can be annotated in a similar way across different languages, because the essentials of negation are language-independent. • for analytic languages like English and Chinese, a set of string-based operations (deletion, insertion and reordering) can be used to assess translation errors in the semantics of negation. Both assumptions involve first of all reducing a rather abstract semantic phenomenon into elements tangible at string-level. Following Blanco and Moldoval (2011), Morante and Blanco (2012) and Fancellu and Webber (2014), we decompose negation into its three main components, described below, and use them as the target of our analysis. • Cue, i.e. the word or multi-words unit inherently expressing negation (e.g. ‘He is not washing his clothes’) • Event, i.e. the lexical event the cue directly refers to (e.g. ‘He is not washing his clothes) • Scope, i.e. all the elements whose falsity would prove the statement to be true (e.g. ‘He is not washing his clothes’); the event is taken to be part of the scope, since its falsity influences the truth value of negation. In the error analysi"
W15-1003,P03-1021,0,0.0115111,"een the two. Given 1 http://www.mdbg.net/chindict/chindict.php?page=cedict 5 System We carried out the error analysis on the output of the Chinese-to-English hierarchical phrase based system submitted by the University of Edinburgh for the NIST12 MT evaluation campaign. The system was trained on ∼2.1 millions length-filtered segments in the news domain, with 44678806 tokens on the source and 50452704 on the target, with MGIZA++ (Gao and Vogel, 2008) used for alignment. The Chinese side of the training and the test set were segmented using the LDCWordSegmenter. The system was tuned using MERT (Och, 2003) on the NIST06 set. The automatic error analysis was carried out on a sub-set of 54 segments the NIST MT08 test set2 , each containing at least an instance of negation on the source side. Although small, this set was considered to be representative given that it clearly shows a pattern in the errors involved in translation negation. 2 This sub-set containing only negative sentences was extracted during the manual evaluation. Out of 1357 segments in the NIST MT08 set, we randomly picked 250 segments and annotate all instances of negation whether present 26 16 15 reference reachability (#) that"
W15-1003,S12-1041,0,0.046108,"Missing"
W15-1003,W12-4203,0,0.0392361,"Missing"
W15-1003,P09-1094,0,0.0122222,"bordinate relationship with the event. Finally, we use the dependency parse to verify that the fillers are correctly connected to negated event. This is a problem that needs more consideration and is therefore left for future work. The correct rendering of the fillers in the negation scope is in fact related to the more general open-problem of preserving predicate-argument structure during translation. We are also exploring a second approach where we detect these elements on the English side by generating as many paraphrases as possible from the reference sentences using the same approach of (Zhao et al., 2009) and the PPDB database. We then extract cues, events and fillers from these paraphrases automatically and check whether they are present in the chart hypotheses and they correctly relate to each other. 14 13 12 11 cpl cpl cpl cpl cpl 10 9 8 = 100 = 500 = 1000 = 5000 = 10000 7 5 10 20 50 100 200 500 1000 tol Figure 1: Number of reachable oracle negation instances plotted against the translation option limit (tol) for each of the five cube pruning pop limit (cpl). 6 Results In this section we present the results related to the two methods introduced in sect. 4. As shown in Figure 1, given the de"
W15-1003,de-marneffe-etal-2006-generating,0,\N,Missing
W15-1301,J12-2006,0,0.30606,"terise errors in the translation of those elements. Results on a Chinese-to-English translation task confirm the robustness of our analysis cross-linguistically and the basic assumptions can inform an automated investigation into the causes of translation errors. Conclusions drawn from this analysis should guide future work on improving the translation of negative sentences. 1 Introduction In recent years, there has been increasing interest in improving the quality of SMT systems over a wide range of linguistic phenomena, including coreference resolution (Hardmeier et al., 2014) and modality (Baker et al., 2012). Amongst these, however, translating negation is still a problem that has not been researched thoroughly. This paper takes an empirical approach towards understanding why negation is a problem in SMT. More specifically, we try to answer two main questions: 1. What kind of errors are involved in translating negation? 2. What are the causes of these errors during decoding? While previous work (section 2) has shown that translating negation is a problem, it has not addressed either of these questions. The present paper focuses on the first one; we show that tailoring to a semantic task, string-b"
W15-1301,J07-2003,0,0.0793481,"ors are involved in translating negation? 2. What are the causes of these errors during decoding? While previous work (section 2) has shown that translating negation is a problem, it has not addressed either of these questions. The present paper focuses on the first one; we show that tailoring to a semantic task, string-based error categories standardly used to evaluate the quality of the machine translation output, allows us to cover the wide range of errors occurring while translating negative sentences (section 3). We report the results of the analysis of a Hierarchical Phrase Based Model (Chiang, 2007) on a Chineseto-English translation task (section 4), where we show that all error categories occur to some extent with scope reordering being the most frequent (section 5). Addressing question (2) requires connecting the assumptions behind this manual error analysis to errors occurring along the translation pipeline. As such, we complete the analysis by briefly introduce an automatic method to investigate the causes of the errors at decoding time (section 6). Conclusion and future works are reported in section 7 and 8. 2 Previous Work In recent years, automatic recognition of negation has bee"
W15-1301,S12-1045,0,0.23279,"ld prove negation to be false; given that the cue is not included, the scope is often discontinuous (e.g. ‘He is not driving a car’) • Focus: the portion of the statement negation primarily refers to (e.g. ‘He is not driving a car). The *SEM 2012 shared task represented a first attempt to apply machine learning methods to the problem of automatically detect the aforementioned elements in English. In particular CRFs and SVMs, making use of syntactic (both constituent and dependency based) clues, were shown to lead to the best results in a supervised machine learning setting (Read et al., 2012; Chowdhury and Mahbub, 2012). The shared task also saw the release of a fully annotated corpus in the literature domain, which represents, along with the BioScope corpus (Szarvas et al., 2008), the only resource specifically annotated for negation. There were also a few attempts in automatically detecting negation in Chinese texts. Li et al. (2008) designed a negation detection algorithm based on syntactic patterns; similarly, Zheng et al. (2014) implemented an FSA for automatic recognition of negation structures in Chinese medical texts, using a list of manually defined cues and the syntactic structures they appear in."
W15-1301,P05-1066,0,0.268589,"Missing"
W15-1301,E14-1063,1,0.688931,"ard approach has been to formulate an hypothesis about what can go wrong when translating negation, modify the SMT system in a way aimed at reducing the number of times that happens, and then assume that any increase in BLEU score - the standard automatic evaluation metric used in SMT - confirms the initial hypothesis. Collins et al. (2005) and Li et al. (2009) consider negation, along with other linguistic phenomena, as a problem of structural mismatch between source and target; Wetzel and Bond (2012) consider it instead as a problem of training data sparsity; finally Baker et al. (2012) and Fancellu and Webber (2014) consider it as a model problem, where the system needs enhancement with respect to the semantics of negation. Given that all these works assess the quality of translation of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-negation related elements. Evaluating the semantic adequacy of the SMT output has also stimulated interest in recent years. Traditional error categories, such as the ones presented in (Vilar et al., 2006), are mostly based on n-gram overlap between hypothesis and r"
W15-1301,W08-0509,0,0.01533,"ctic constituent tags for non-terminals but instead use an X as placeholder for recursion. A rule used in a Hierarchical Phrase based system looks like the following, ne veux plus X1 → do not want X1 anymore where the French source (also referred to as the left hand side - LHS of the rule) and the English target side (the right hand side - RHS) allows arbitrary insertion of another rule where the placeholder X is located. The system was trained on approximately 2.1 million length-filtered segments in the news domain, with 44678806 tokens on the source and 50452704 on the target, with MGIZA++ (Gao and Vogel, 2008) used for alignment. The system was tuned using MERT (Minimal Error Rate Training, (Och, 2003)) on the NIST06 set. Two different test sets were considered to assess differences that might be associated with genre: the NIST MT08 test set, containing data from the newswire domain and the IWSLT14 tst2012 test set, containing transcriptions of TED talks. We hypothesise that the difference in genre can influence the kinds of negation related error occurring during translation: as a collection of planned spoken inspirational talks, we expect the IWSLT’14 test set to contain shorter sentences, and on"
W15-1301,W09-0433,0,0.145886,"bilingual setting such as the SMT, however, most work has only considered negation as a side problem. For this reason, no actual analysis on the type of errors involved in translating negation or their causes has been specifically carried out. The standard approach has been to formulate an hypothesis about what can go wrong when translating negation, modify the SMT system in a way aimed at reducing the number of times that happens, and then assume that any increase in BLEU score - the standard automatic evaluation metric used in SMT - confirms the initial hypothesis. Collins et al. (2005) and Li et al. (2009) consider negation, along with other linguistic phenomena, as a problem of structural mismatch between source and target; Wetzel and Bond (2012) consider it instead as a problem of training data sparsity; finally Baker et al. (2012) and Fancellu and Webber (2014) consider it as a model problem, where the system needs enhancement with respect to the semantics of negation. Given that all these works assess the quality of translation of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-ne"
W15-1301,lo-wu-2010-evaluating,0,0.0133234,"on of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-negation related elements. Evaluating the semantic adequacy of the SMT output has also stimulated interest in recent years. Traditional error categories, such as the ones presented in (Vilar et al., 2006), are mostly based on n-gram overlap between hypothesis and reference and so are the most widely used automatic evaluation metrics used in SMT (e.g. BLEU (Papineni et al., 2002) and TER (Snover et al., 2009)). In contrast, MEANT (Lo and Wu, 2010, 2011) and its human counterpart, HMEANT, attempt to abstract from simple string matching and assess the degree of semantic similarity between machine output and reference sentence. To do so, both sides are annotated using Propbank-like semantic labels, and the fillers matched if both sides contain the same event. To assign a score to the test set evaluated, an F1 measure over precision and recall of matched fillers is then computed. 3 3.1 Methodology Manual Annotation First, we start with the assumption that negation is a language independent semantic phenomenon which can be defined as a str"
W15-1301,P11-1023,0,0.0240672,"the translation process (a.k.a. the decoding process) and why the error occurs, we also investigated the trace of rules used to build the 1-best machine output. This is particularly useful in the case of deletion: this may occur because a certain Chinese word or sequence of Chinese words (generally referred in SMT as phrases) has not been seen during training (so called out-of-vocabulary items - OOVs) and the system is therefore unable to translate them. After the elements of negation have been annotated in both the source sentences and machine outputs, we use the same heuristic as (H)MEANT (Lo and Wu, 2011) to decide whether a translated unit is correct or partially correct. We also consider correct translations that are synonyms of the source negation element since they are taken to convey the same meaning. This also includes those elements that are negated in the source but are rendered in the machine output by means of a lexical element inherently expressing negation (e.g. fails) or by paraphrase into positive (e.g. b`u t´ong, lit. ‘not similar’ → different). We consider partially correct translated elements that do not contain errors which impact the overall meaning. In the case of the event"
W15-1301,S12-1035,0,0.0526646,"l error categories occur to some extent with scope reordering being the most frequent (section 5). Addressing question (2) requires connecting the assumptions behind this manual error analysis to errors occurring along the translation pipeline. As such, we complete the analysis by briefly introduce an automatic method to investigate the causes of the errors at decoding time (section 6). Conclusion and future works are reported in section 7 and 8. 2 Previous Work In recent years, automatic recognition of negation has been the focus of considerable work. Following Blanco and Moldovan (2011) and Morante and Blanco (2012) detecting negation is a task of unraveling its structure, i.e. locating in a text its four main components: • Cue: the word or multi-word unit inherently expressing negation (e.g. ‘He is not driving a car’) • Event: the lexical element the cue directly refers to (e.g. ‘He is not driving a car’) • Scope: all the elements whose falsity would prove negation to be false; given that the cue is not included, the scope is often discontinuous (e.g. ‘He is not driving a car’) • Focus: the portion of the statement negation primarily refers to (e.g. ‘He is not driving a car). The *SEM 2012 shared task r"
W15-1301,P03-1021,0,0.0834635,"n a Hierarchical Phrase based system looks like the following, ne veux plus X1 → do not want X1 anymore where the French source (also referred to as the left hand side - LHS of the rule) and the English target side (the right hand side - RHS) allows arbitrary insertion of another rule where the placeholder X is located. The system was trained on approximately 2.1 million length-filtered segments in the news domain, with 44678806 tokens on the source and 50452704 on the target, with MGIZA++ (Gao and Vogel, 2008) used for alignment. The system was tuned using MERT (Minimal Error Rate Training, (Och, 2003)) on the NIST06 set. Two different test sets were considered to assess differences that might be associated with genre: the NIST MT08 test set, containing data from the newswire domain and the IWSLT14 tst2012 test set, containing transcriptions of TED talks. We hypothesise that the difference in genre can influence the kinds of negation related error occurring during translation: as a collection of planned spoken inspirational talks, we expect the IWSLT’14 test set to contain shorter sentences, and on average, more instances of negation. On the contrary, we expect the NIST MT08, where data are"
W15-1301,P02-1040,0,0.0930841,"ics of negation. Given that all these works assess the quality of translation of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-negation related elements. Evaluating the semantic adequacy of the SMT output has also stimulated interest in recent years. Traditional error categories, such as the ones presented in (Vilar et al., 2006), are mostly based on n-gram overlap between hypothesis and reference and so are the most widely used automatic evaluation metrics used in SMT (e.g. BLEU (Papineni et al., 2002) and TER (Snover et al., 2009)). In contrast, MEANT (Lo and Wu, 2010, 2011) and its human counterpart, HMEANT, attempt to abstract from simple string matching and assess the degree of semantic similarity between machine output and reference sentence. To do so, both sides are annotated using Propbank-like semantic labels, and the fillers matched if both sides contain the same event. To assign a score to the test set evaluated, an F1 measure over precision and recall of matched fillers is then computed. 3 3.1 Methodology Manual Annotation First, we start with the assumption that negation is a la"
W15-1301,S12-1041,0,0.147449,"s whose falsity would prove negation to be false; given that the cue is not included, the scope is often discontinuous (e.g. ‘He is not driving a car’) • Focus: the portion of the statement negation primarily refers to (e.g. ‘He is not driving a car). The *SEM 2012 shared task represented a first attempt to apply machine learning methods to the problem of automatically detect the aforementioned elements in English. In particular CRFs and SVMs, making use of syntactic (both constituent and dependency based) clues, were shown to lead to the best results in a supervised machine learning setting (Read et al., 2012; Chowdhury and Mahbub, 2012). The shared task also saw the release of a fully annotated corpus in the literature domain, which represents, along with the BioScope corpus (Szarvas et al., 2008), the only resource specifically annotated for negation. There were also a few attempts in automatically detecting negation in Chinese texts. Li et al. (2008) designed a negation detection algorithm based on syntactic patterns; similarly, Zheng et al. (2014) implemented an FSA for automatic recognition of negation structures in Chinese medical texts, using a list of manually defined cues and the syntacti"
W15-1301,W08-0606,0,0.0407085,"negation primarily refers to (e.g. ‘He is not driving a car). The *SEM 2012 shared task represented a first attempt to apply machine learning methods to the problem of automatically detect the aforementioned elements in English. In particular CRFs and SVMs, making use of syntactic (both constituent and dependency based) clues, were shown to lead to the best results in a supervised machine learning setting (Read et al., 2012; Chowdhury and Mahbub, 2012). The shared task also saw the release of a fully annotated corpus in the literature domain, which represents, along with the BioScope corpus (Szarvas et al., 2008), the only resource specifically annotated for negation. There were also a few attempts in automatically detecting negation in Chinese texts. Li et al. (2008) designed a negation detection algorithm based on syntactic patterns; similarly, Zheng et al. (2014) implemented an FSA for automatic recognition of negation structures in Chinese medical texts, using a list of manually defined cues and the syntactic structures they appear in. In a bilingual setting such as the SMT, however, most work has only considered negation as a side problem. For this reason, no actual analysis on the type of errors"
W15-1301,vilar-etal-2006-error,0,0.159901,"aining data sparsity; finally Baker et al. (2012) and Fancellu and Webber (2014) consider it as a model problem, where the system needs enhancement with respect to the semantics of negation. Given that all these works assess the quality of translation of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-negation related elements. Evaluating the semantic adequacy of the SMT output has also stimulated interest in recent years. Traditional error categories, such as the ones presented in (Vilar et al., 2006), are mostly based on n-gram overlap between hypothesis and reference and so are the most widely used automatic evaluation metrics used in SMT (e.g. BLEU (Papineni et al., 2002) and TER (Snover et al., 2009)). In contrast, MEANT (Lo and Wu, 2010, 2011) and its human counterpart, HMEANT, attempt to abstract from simple string matching and assess the degree of semantic similarity between machine output and reference sentence. To do so, both sides are annotated using Propbank-like semantic labels, and the fillers matched if both sides contain the same event. To assign a score to the test set eval"
W15-1301,W12-4203,0,0.156257,"on the type of errors involved in translating negation or their causes has been specifically carried out. The standard approach has been to formulate an hypothesis about what can go wrong when translating negation, modify the SMT system in a way aimed at reducing the number of times that happens, and then assume that any increase in BLEU score - the standard automatic evaluation metric used in SMT - confirms the initial hypothesis. Collins et al. (2005) and Li et al. (2009) consider negation, along with other linguistic phenomena, as a problem of structural mismatch between source and target; Wetzel and Bond (2012) consider it instead as a problem of training data sparsity; finally Baker et al. (2012) and Fancellu and Webber (2014) consider it as a model problem, where the system needs enhancement with respect to the semantics of negation. Given that all these works assess the quality of translation of negative sentences using an n-gram overlap metric, there is no certainty whether any improvement derives from a better rendering of negation or from other, non-negation related elements. Evaluating the semantic adequacy of the SMT output has also stimulated interest in recent years. Traditional error cate"
W15-2503,guillou-etal-2014-parcor,1,0.927711,"United Kingdom L.K.Guillou@sms.ed.ac.uk Bonnie Webber School of Informatics University of Edinburgh Scotland, United Kingdom bonnie@inf.ed.ac.uk Abstract scribed planned speech). In the ParCor annotations, each pronoun is marked as being one of eight types: Anaphoric/cataphoric, event reference, extra-textual reference, pleonastic, addressee reference, speaker reference, generic reference, or other function4 . Additional features are recorded for some pronoun types, for example anaphoric/cataphoric pronouns are linked to their antecedents. Full details of the annotation scheme are provided in Guillou et al. (2014). Through analysing similarities and differences in pronoun use in these parallel texts, we hope to better understand the problems of translating different types of pronouns. This knowledge may in turn be used to build discourse-aware SMT systems in the future. In addition, through analysing translations produced by state-of-the art systems, we hope to understand how well current systems translate a range of pronoun types. This information may be used to identify the pronoun types where future efforts would be best directed. The advantage of using the ParCor corpus is that it allows us to cond"
W15-2503,E12-3001,1,0.928811,"analysis of pronouns as a whole, we have analysed a parallel corpus of annotated English-German texts to highlight some of the problems that hinder progress. We combine this with an assessment of the ability of two state-of-the-art systems to translate different pronoun types. 1 Introduction Previous work on the translation of pronouns in Statistical Machine Translation (SMT) has focussed on the specific problem of translating anaphoric pronouns – i.e., ones that co-refer with an antecedent entity previously mentioned in the discourse (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012; Nov´ak et al., 2013; Hardmeier, 2014; Weiner, 2014). This is because languages differ in how an anaphoric pronoun relates to its antecedent, and the relationship does not fit naturally into the SMT pipeline. Some pronoun forms also have non-anaphoric uses, and there are other types of pronouns. Languages also differ as to what types of pronouns are used for what purposes. To investigate similarities and differences in pronoun usage across languages, we conducted an analysis of the ParCor corpus1 of pronoun annotations over a set of parallel English-German texts. The corpus contains a collect"
W15-2503,2010.iwslt-papers.10,0,0.566144,"ther progress requires careful analysis of pronouns as a whole, we have analysed a parallel corpus of annotated English-German texts to highlight some of the problems that hinder progress. We combine this with an assessment of the ability of two state-of-the-art systems to translate different pronoun types. 1 Introduction Previous work on the translation of pronouns in Statistical Machine Translation (SMT) has focussed on the specific problem of translating anaphoric pronouns – i.e., ones that co-refer with an antecedent entity previously mentioned in the discourse (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012; Nov´ak et al., 2013; Hardmeier, 2014; Weiner, 2014). This is because languages differ in how an anaphoric pronoun relates to its antecedent, and the relationship does not fit naturally into the SMT pipeline. Some pronoun forms also have non-anaphoric uses, and there are other types of pronouns. Languages also differ as to what types of pronouns are used for what purposes. To investigate similarities and differences in pronoun usage across languages, we conducted an analysis of the ParCor corpus1 of pronoun annotations over a set of parallel English-German texts. The corpus con"
W15-2503,P14-6007,0,0.393013,"have analysed a parallel corpus of annotated English-German texts to highlight some of the problems that hinder progress. We combine this with an assessment of the ability of two state-of-the-art systems to translate different pronoun types. 1 Introduction Previous work on the translation of pronouns in Statistical Machine Translation (SMT) has focussed on the specific problem of translating anaphoric pronouns – i.e., ones that co-refer with an antecedent entity previously mentioned in the discourse (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012; Nov´ak et al., 2013; Hardmeier, 2014; Weiner, 2014). This is because languages differ in how an anaphoric pronoun relates to its antecedent, and the relationship does not fit naturally into the SMT pipeline. Some pronoun forms also have non-anaphoric uses, and there are other types of pronouns. Languages also differ as to what types of pronouns are used for what purposes. To investigate similarities and differences in pronoun usage across languages, we conducted an analysis of the ParCor corpus1 of pronoun annotations over a set of parallel English-German texts. The corpus contains a collection of texts from two different genres"
W15-2503,W10-1737,0,0.175056,"Missing"
W15-2503,W13-3307,0,0.0252522,"Missing"
W15-2503,P06-1055,0,0.0104197,"tions for number/gender and case combinations. The number/gender options are masculine, feminine, neuter and plural. The case options are: “case unknown”, and three German cases: nominative, accusative and dative. See Figure 1. 4.6 English relativizers may be explicit (that- and whrelativizers), or implicit (null-relativizers). Both may be translated as relative pronouns in German. We randomly selected 50 instances of relativizers from the TED corpus; 25 that- and 25 nullrelativizers. The selection was semi-automatic, based on identifying relative clauses in the output of the Berkeley Parser (Petrov et al., 2006) and manually selecting those that contained a that- or null-relativizer. As null-relativizers are implicit, there are no tokens in the English text to highlight. To keep this task in line with the others, we manually insert symbols for the nulls, i.e. the “;” in “The house ; Jack built”, and (manually) align them to the corresponding token in the SMT output. (Unalignable tokens are left untranslated.) Instead of a pronoun in the English text, the annotator is presented with an instance of “that” or a symbol representing the null-relativizer. Placeholders are included in the translation as nor"
W15-2503,D13-1037,0,\N,Missing
W15-2503,2014.iwslt-evaluation.6,0,\N,Missing
W15-2516,guillou-etal-2014-parcor,1,0.850856,"o the fact that pronouns often refer to entities mentioned in a nonlocal context such as previous clauses or sentences. Furthermore, languages differ with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Port"
W15-2516,E12-3001,0,0.0498972,"with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al., 2008).1 antecedent . . . une symphonie et"
W15-2516,petrov-etal-2012-universal,0,0.0272995,"on as bag-of-words, but separate the feature space by source and target side vocabulary and whether the word occurs before or after the pronoun. Special BOS and EOS markers are included for contexts at the beginning or end of sentence, respectively. We neither remove stopwords nor normalize the tokens. We also include as features, the Part-of-Speech (POS) tags in a 3-word window to each side of source and target pronouns. This gives some abstraction from the lexical surface form. For the source side we use the POS tags from Stanford CoreNLP (Manning et al., 2014) mapped to universal POS tags (Petrov et al., 2012). For the target side we use coarse-grained tags provided by 1 116 https://github.com/gchrupala/morfette antecedentaligned si des while institutions institutions comme . . . such as les ONG peuvent travailler . . . NGOs may work on au social d´eveloppement social development , , elles sont they are under-funded sous-financ´ees antecedentco−ref Figure 2: The antecedentco−ref of they on the English sentence (source language) is determined with a co-reference resolution system. The target-side antecedentaligned is obtained by following the word alignment links. In the shared task, the target pron"
W15-2516,2010.iwslt-papers.10,0,0.0985871,"ious clauses or sentences. Furthermore, languages differ with respect to usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al.,"
W15-2516,D13-1037,0,0.211553,"Missing"
W15-2516,W14-3312,0,0.112768,"usage of pronouns, e.g. how they agree with their antecedent or whether source and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al., 2008).1 antecedent . . . une symphonie et . . . the symphony and La"
W15-2516,W15-2501,0,0.0584592,"e and target language exhibit similar patterns of pronoun usage. Since pronouns contribute an important part to the meaning of an utterance, the meaning can be changed considerably when wrongly resolved and translated. This problem gained recent interest and work has been presented in annotating and analysing translations of pronouns in parallel corpora (Guillou et al., 2014) and MT systems focusing on translation of pronouns have been proposed (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012; Hardmeier et al., 2014). The DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) calls for con2 2.1 Systems for Cross-Lingual Pronoun Prediction Maximum Entropy Classification A MaxEnt classifier can model multinomial dependent variables (discrete class labels) given a set of independent variables (i.e. observations). 115 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 115–121, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. Morfette (Chrupała et al., 2008).1 antecedent . . . une symphonie et . . . the symphony and Language Model Prediction We include a target-side Language Model (LM) predictio"
W15-2516,W11-2123,0,0.0173571,"formance on the test data. Ranks according to each metric are given in parenthesis out of 14 submitted systems (including multiple submissions per submitter and the baseline). 2093 sentences in twelve TED talk documents. 3.2 Classifier Accuracy We extract features from the training and test set and use Mallet (McCallum, 2002) to train the MaxEnt classifier.4 The variance for regularizing the weights is set to 1 (default setting). For the LM component of our system we use the baseline model provided for the pronoun translation subtask. This is a 5-gram modified KneserNey LM trained with KenLM (Heafield, 2011).5 3.3 Table 2: Performance of ALL I N O NE classifier on the test set. Evaluation Metrics 5 Recall F1 78.05 86.96 82.26 cela 9.52 7.41 8.33 elle 49.06 31.33 38.24 elles 80.00 31.37 45.07 il 51.54 64.42 57.26 ils 75.79 90.00 82.29 on 61.90 35.14 44.83 c¸a 64.29 44.12 52.33 OTHER 80.00 88.52 84.04 Macro-averaged 61.13 53.25 54.96 Accuracy 71.40 Table 3: Performance of POST C OMBINED classifier on the test set. 4 Discussion Confusion Matrices Table 5 and Table 6 present confusion matrices on the test set. Divergences from strong diagonal values in both tables derive in part from gender-choice er"
W15-2516,W10-1737,0,0.254363,"Missing"
W15-2516,J13-4004,0,0.377687,"hat maps the i-th observation x and associated label y to a real valued vector. It also consists of a weight vector θ~ of corresponding size, which contains the model parameters that are learned from the training data. The model is of the form p(y|x) = exp θ~ · f (x, y) Z(x) where Z(x) is a normalizing factor ensuring valid probabilities. 2.2 Features Target-side Antecedent The target-side noun antecedent of the pronoun determines the morphological features the pronoun has to agree with, i.e. number and gender. We use the source-side coreference resolution system provided by Stanford CoreNLP (Lee et al., 2013) to determine the coreference chains in each document of the training data. We then project these chains to the target side via word-alignments (cf. Figure 2). The motivation to obtain target-side co-reference chains in that way is three-fold. First, the target side of the training data is missing most of the targetside pronouns since it is the task to predict them. Therefore, relevant parts of co-reference chains are missing and the place-holders for these pronouns will introduce noise to the resolution system. Secondly, we have a statistical machine translation (SMT) scenario in mind as an a"
W15-2516,P14-5010,0,0.00415805,"ze 3 around the pronoun. We integrate this information as bag-of-words, but separate the feature space by source and target side vocabulary and whether the word occurs before or after the pronoun. Special BOS and EOS markers are included for contexts at the beginning or end of sentence, respectively. We neither remove stopwords nor normalize the tokens. We also include as features, the Part-of-Speech (POS) tags in a 3-word window to each side of source and target pronouns. This gives some abstraction from the lexical surface form. For the source side we use the POS tags from Stanford CoreNLP (Manning et al., 2014) mapped to universal POS tags (Petrov et al., 2012). For the target side we use coarse-grained tags provided by 1 116 https://github.com/gchrupala/morfette antecedentaligned si des while institutions institutions comme . . . such as les ONG peuvent travailler . . . NGOs may work on au social d´eveloppement social development , , elles sont they are under-funded sous-financ´ees antecedentco−ref Figure 2: The antecedentco−ref of they on the English sentence (source language) is determined with a co-reference resolution system. The target-side antecedentaligned is obtained by following the word a"
W15-2516,chrupala-etal-2008-learning,0,\N,Missing
W15-2703,A00-3008,0,0.751922,"em, we have embarked on a large crowd-sourcing experiment, the first phase of which is described in Sections 3– 5. Section 6 discusses our results to date, with further phases described in Section 7. 2 Background This is not the first work to call attention to multiple co-occurring connectives. Webber and colleagues (1999) used them to argue that discourse spans could be related by both adjacency relations and anaphoric relations. Similary, in the context of Catalan and Spanish oral narrative, Cuenca and Marin (2009) used them to argue for different patterns and degrees of discourse cohesion. Oates (2000) considers how multiple discourse connectives should be used in Natural Language Generation, noting that the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996). Fraser (2013) considers the order in which multiple CONTRASTIVE connectives co-occur, describing their patterning in terms of general contrastive discourse markers and specific contrastive discourse markers. For Turkish, Zeyrek (2014) describes patterns of multiple co-occuring connectives that signal CON TRASTIVE and/or CONCESSIVE relations. These efforts have all been directed a"
W15-2703,Q14-1025,0,0.0266132,"sage presentation Figure 4: Screen shot of a participant being asked to indicate whether or not their choice of a conjunction that fits with respect to its sense — in this case, “but” — sounds natural 26 with other participants, as well as 3 trials in which a participant selected the response before, which was intended for use in only the catch trials. The resulting dataset of responses from 58 participants comprises 2665 judgments over the 46 target passages (ignoring the four catch trials). The results reported below are raw counts, and do not yet take account of potential participant bias (Passonneau and Carpenter, 2014). Considering the dataset as a whole, we can ask how often a participant’s response matched the author’s original choice. (Note that this can only be assessed on explicit passages – that is, ones where the author expicitly used a pair of co-occurring connectives, cf. Section 3). Table 3 shows the pattern of participant responses for passages for which the authors themselves had included an explicit conjunction before the adverbial. Recall that participants always saw a gap before the discourse adverbial, regardless of the author’s original choice to use or not use a conjunction, meaning the ex"
W15-2703,prasad-etal-2008-penn,1,0.78783,"rhetorical relations (Mann and Thompson, 1988). Such relations between what we will call here discourse spans can be signaled explicitly via discourse connectives or specific lexico-syntactic contructions, or conveyed implicitly, via inference on the part of a comprehender. But when does the latter happen? Previously, it was assumed that relations are conveyed implicitly when they are not signalled explicitly. But consider Ex. 1a-b, each with two explicit connectives conveying distinct relations: 1 The sense labels used here (in small caps) are short forms of the labels used in the PDTB 2.0 (Prasad et al., 2008; Prasad et al., 2014). (1) a. Let’s eat dinner now because otherwise we’ll miss the film. 22 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 22–31, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. gap or choose None.2 In half the passages (10 per adverbial), the author had used one of these conjunctions before the adverbial (which Jiang then removed), and in the other half (including Ex. 3– 4), the author had used no conjunction before the adverbial. The only criteria used in selecting t"
W15-2703,J14-4007,1,\N,Missing
W15-2707,W05-0305,1,0.799224,"Missing"
W15-2707,D12-1083,0,0.012738,"(3) Now, we regard this as a largely phony issue, but the “long term” is nonetheless a big salon topic all around the Beltway. (5) PropBank: Verb = suspend Arg0 = The federal government Arg1 = sales of U.S. savings bonds ARGM-CAU = because Congress hasn’t lifted the ceiling on government debt (4) The U.S. wants the removal of . . .barriers to investment; Japan denies there are real barriers. Researchers working on discourse parsing have commented that intra-sentential (intra-S) discourse relations are, in general, easier to recognize than ones whose arguments are found in separate sentences (Joty et al., 2012; Lin et al., (6) PDTB: Connective = because Arg1 = The federal government suspended sales of U.S. savings bonds Arg2 = Congress hasn’t lifted the ceiling on government debt Sense = Contingency.Cause.Reason 64 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 64–69, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. ARGM - ADV (2235) ARGM - CAU (657) ARGM - TMP (2503) ARGM - PNC (66) ARGM - MNR (13) TOTAL (5475) TEMPORAL CONTINGENCY COMPARISON EXPANSION TOTAL 222 14 2258 0 0 2494 1067 650 523"
W15-2707,J93-2004,0,0.0504287,"Missing"
W15-2707,W04-2705,0,0.14572,"Missing"
W15-2707,J05-1004,0,0.340826,"mproving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same example, shown in (6), marks because as the connective, ‘Contingency.Cause.Reason’ as the sense, the adjunct clause as Arg2 (defined as the argument attached to the connective), and the matrix clause as Arg1 (defined as the non-Arg2 argument)."
W15-2707,N04-1030,1,0.764484,"Missing"
W15-2707,W13-3516,1,0.897991,"Missing"
W15-2707,prasad-etal-2008-penn,1,0.826615,"bber@ed.ac.uk 3 Institute for Research in Cognitive Science, University of Pennsylvania {aleewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clau"
W15-2707,C10-2118,1,0.939227,"Missing"
W15-2707,J14-4007,1,0.857151,"leewk,joshi}@seas.upenn.edu 4 Boulder Language Technologies pradhan@bltek.com Abstract 2012; Feng, 2014). They are also quite useful in Language Technology applications that exploit sentence-level relations. Thus, there is particular value in improving the quality of recognizers capable of determining what, if any, discourse relations hold between intra-S units. Taking abstract objects to be expressed (arguably) typically as clauses headed by verbs or other predicates, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) includes annotations of intra-S discourse relations but, as noted by Prasad et al. (2014), they are significantly underannotated in the corpus. At the same time, Prasad et al. (2014) point to possible overlaps between intra-S discourse relations in the PDTB and a subset of verb-argument annotations in PropBank (Palmer et al., 2005). The PropBank annotations of particular interest here are those in which the arguments are clausal adjuncts, labeled ARGM, and further assigned a semantic role. For example, the PropBank annotation of the verb suspend in Ex. 1 is shown in (5), with the adjunct clause annotated as ARGM and assigned the role CAU (causal). The PDTB annotation for the same"
W15-2707,N07-1069,0,0.060601,"Missing"
W16-1704,P14-1065,0,0.0139892,"Missing"
W16-1704,N09-1064,0,0.0255764,"Missing"
W16-1704,J15-3002,0,0.0272451,"Missing"
W16-1704,W12-3624,0,0.0492071,"Missing"
W16-1704,J93-2004,0,0.0590896,"wsj 1270] (13) Then take the expected return and subtract one standard deviation. [wsj 1564] (14) Be careful boys; use good judgment. [wsj 0596] 2.2 Discourse Adverbials (9) The NAM embraces efforts, which both the adminisAs can be seen from the presence of then in Ex. 9, conjoined VPs can themselves contain discourse adverbials. As with all discourse adverbials, ones that appear in Arg2 of a conjoined VP can link to material elsewhere in the text, as in Ex. 15 (15) Separately, the Federal Energy Regulatory CommisSince these were incorrectly analyzed according to the Penn TreeBank Guidelines (Marcus et al., 1993) and do not actually differ from the tokens already included in the corpus, we decided to include them. On the other hand, we decided to exclude tokens containing conjoined verbs that should possibly have been analyzed as conjoined VPs, such as exist and fight in While the discourse adverbial still shares its Arg2 with the conjoined VP, its Arg1 has been taken to be the FERC turning down its request for approval of its possible purchase of PS of New Hampshire, which appears in the previous sentence. Although such adverbials can link to material in previous sentences, the far more common situat"
W16-1704,prasad-etal-2008-penn,1,0.835795,"Missing"
W16-1704,C10-2118,1,0.597562,"equency with which each PDTB2 sense has been replaced by a specific PDTB3 sense. 4.2 Implicit=instead be part of the world mentality,” declares Charles M. Jordan, GM’s vice president for design . . . (Expansion.Conjunction, Expansion.Substitution.Arg2-as-subst) [wsj 0956] (38) . . . Exxon Corp. built the plant but Implicit=then closed it in 1985. (Comparison.Concession.Arg2-as-denier, Temporal.Asynchronous.Precedence) [wsj 1748] 3. If inserting an implicit connective was perceived as redundant, appropriate material in Arg2 could be annotated as AltLex (Ex. 39), as done elsewhere in the PDTB2 (Prasad et al., 2010). (39) His policies went beyond his control and resulted . . . in riots and disturbances. (Expansion.Conjunction, Contingency.Cause.Result) [wsj 0290] Sense labelling of conjoined VP tokens The second guideline above points to a new feature of our discourse annotation: While multiple relations were annotated in the PDTB2 as holding between identical or overlapping argument spans, all were associated with either multiple explicit connectives or multiple inferred relations. What is new in the annotation of conjoined VPs is the possibility of an explicit relation co-occurring with ones that are i"
W16-1704,J14-4007,1,\N,Missing
W16-1704,W01-1605,0,\N,Missing
W16-1707,J08-4004,0,0.373054,"Missing"
W16-1707,P87-1023,0,0.758335,"ng annotators what additional relation they infer (besides that associated with instead itself), one still needs to ask: • For clauses starting with discourse adverbials other than instead, is the relation signalled by the adverbial all there is, or can an additional relation be inferred with the previous text? In the former case, no additional annotation is required; in the latter, it is. Existing work highlights the importance of understanding discourse relations in context, showing a range of phenomena that are sensitive to the semantic connection that holds between two spans of discourse (Hirschberg and Litman, 1987; Kehler and Rohde, 2013). Such connections can be made explicit in text via an overt connective or marked syntax; otherwise they must be inferred. Various contextual cues have been identified that guide the 49 Proceedings of LAW X – The 10th Linguistic Annotation Workshop, pages 49–58, c Berlin, Germany, August 11, 2016. 2016 Association for Computational Linguistics • If another relation can be inferred, can it be inferred deterministically based on the adverbial alone? If so, no additional work is required, as the relation can be annotated automatically. • If it can’t be inferred based on t"
W16-1707,N13-1132,0,0.0250313,"the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a bo"
W16-1707,N13-1000,0,0.223269,"Missing"
W16-1707,P12-3029,0,0.0207026,"adverbial was replaced with a slash. Each passage contained one of the following discourse adverbials after the gap: actually, after all, first of all, for example, for instance, however, in fact, in general, in other words, indeed, instead, nevertheless, nonetheless, on the one hand, on the other hand, otherwise, specifically, then, therefore, and thus. These represent a sampling of high-frequency adverbials, which belong to a variety of semantic classes and which showed a range of conjunction co-occurrence patterns in counts extracted from the Google Books Ngram Corpus (Michel et al., 2011; Lin et al., 2012). Half the target passages originally contained a conjunction before the adverbial. For those explicit passages, we excised the conjunction and replaced it with a gap. For excerpts that were originally implicit passages, we simply inserted a gap before the adverbial. For each of the 20 adverbials, participants saw 25 explicit passages and 25 implicit passages, with the exception of however, which appeared in 25 implicit passages and 1 explicit passage (due to the rarity of conjunctions that naturally occur directly before however). The distribution of original (author-chosen) conjunctions in t"
W16-1707,A00-3008,0,0.104179,"r and annotate its relation, if any, to the previous sentence. This reflected the common assumption, noted earlier, that the situation is “either/or” – if a discourse relation is marked, there is nothing to infer. With respect to research on explicit multiple cooccurring connectives, over 15 years ago, Webber et al. (1999) used them to argue that discourse spans could be related by both adjacency relations and anaphoric relations. Similary, in the context of Catalan and Spanish oral narrative, Cuenca and Marín (2009) used them to argue for different patterns and degrees of discourse cohesion. Oates (2000) considered how multiple discourse connectives should be used in Natural Language Generation, noting that the order in which they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for probl"
W16-1707,Q14-1025,0,0.10735,"they occur correlates with the hierarchy of discourse connectives presented in (Knott, 1996), while Fraser (2013) offers an account of the order in which multiple contrastive connectives co-occur, in terms of what The other research area that forms the background to the current work is research on acquiring linguistic judgments from a large number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a boxcar has been attached to a train"
W16-1707,W05-0311,0,0.587643,"number of annotators, whether by crowdsourcing or in-house. Here, research has addressed either identifying and correcting for problems arising from judgments from large numbers of unknown, possibly biased and/or inattentive annotators (Hovy et al., 2013; Passonneau and Carpenter, 2014), or identifying benefits that arise from having a large number of annotators (Artstein and Poesio, 2005, 2008). Work in the former area attempts to eliminate judgments that should be treated as noise, while the latter work shows that annotator bias decreases with the number of annotators. In related research, Poesio and Artstein (2005) reflect on the “true ambiguity” of some pronoun tokens and how the presence of these distinct copresent viable interpretations can be brought to light via a sufficiently large number of annotators. In one example they cite, a boxcar has been attached to a train engine. The next sentence specified what should then be done. Over half their participants interpreted the pronoun it in this next 50 sentence as referring to the boxcar, while others interpreted it to refer to the engine. But the situation associated with these two different interpretations was the same in both cases, since the engine"
W16-1707,miltsakaki-etal-2004-penn,1,0.830866,"Missing"
W16-1707,W15-2703,1,0.830948,"lds between two spans of discourse. It may not be simple to identify or infer that relation, but once achieved, the task is taken to be done. But properties of the discourse adverbial instead (Webber, 2013) have challenged this assumption. In particular, sentence-initial instead supports the inference of another discourse relation, with the specific relation depending on properties of the spans. This can be seen through what coordinating conjunction makes the relation explicit—compare: The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn’t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to a larger set of 20 discourse adverbials by eliciting ≈28K conjunction completions via crowdsourcing. Our data replicate and extend Rohde et al.’s findings that discourse adverbials do indeed license inferred conjunctions. Further, the diverse patterns observed for the adverbials include cases in which more than one valid connection can be inferred, each one endorsed by a substantial number of participants; suc"
W16-1707,W13-0124,1,0.96323,"er}@ed.ac.uk, {anna.y.dickinson, chrisclark272}@gmail.com, nschneid@inf.ed.ac.uk, aplouis@essex.ac.uk Abstract establishment of discourse relations (Hirschberg and Litman, 1987; Kehler, 2002; Webber, 2013). When it comes to producing resources annotated with discourse relations—e.g., the Penn Discourse Treebank (PDTB; Prasad et al., 2008)—it is commonly assumed that at most a single discourse relation holds between two spans of discourse. It may not be simple to identify or infer that relation, but once achieved, the task is taken to be done. But properties of the discourse adverbial instead (Webber, 2013) have challenged this assumption. In particular, sentence-initial instead supports the inference of another discourse relation, with the specific relation depending on properties of the spans. This can be seen through what coordinating conjunction makes the relation explicit—compare: The semantic relationship between a sentence and its context may be marked explicitly, or left to inference. Rohde et al. (2015) showed that, contrary to common assumptions, this isn’t exclusive or: a conjunction can often be inferred alongside an explicit discourse adverbial. Here we broaden the investigation to"
W16-1707,prasad-etal-2008-penn,1,\N,Missing
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W17-1804,basile-etal-2012-developing,0,0.423653,"Malta borders no country’ has the UD graph shown in Figure 1(a). When compared to the correct representation given in Figure 1(c), the UDepLambda output shown in Figure 1(b) shows the absence of universal quantification, which in turn leads negation scope to be misrepresented. For this reason, we set the foundation of UDepLambda¬ (UDepLambda-not), an enhanced version of the original framework, whose type theory allows us to jointly handle negation and universal quantification. Moreover, unlike its predecessor, the logical forms are based on the one used in the ‘Groeningen Meaning Bank’ (GMB; (Basile et al., 2012a)), so to allow future comparison to a manually annotated semantic bank. Although the present work shows the conversion process for English, given that the edge labels are universal, our framework could be used to explore the problem of representing the scope of negation in the other 40+ languages universal dependencies are available in. This could also address the problem that all existing resources to represent negation scope as a logical form are limited to English (e.g. GMB and ‘DeepBank’ (Flickinger et al., 2012)) or only to a few other languages (e.g. ‘The Spanish Resource Grammar’ (Mar"
W17-1804,S12-1040,0,0.0947935,"Malta borders no country’ has the UD graph shown in Figure 1(a). When compared to the correct representation given in Figure 1(c), the UDepLambda output shown in Figure 1(b) shows the absence of universal quantification, which in turn leads negation scope to be misrepresented. For this reason, we set the foundation of UDepLambda¬ (UDepLambda-not), an enhanced version of the original framework, whose type theory allows us to jointly handle negation and universal quantification. Moreover, unlike its predecessor, the logical forms are based on the one used in the ‘Groeningen Meaning Bank’ (GMB; (Basile et al., 2012a)), so to allow future comparison to a manually annotated semantic bank. Although the present work shows the conversion process for English, given that the edge labels are universal, our framework could be used to explore the problem of representing the scope of negation in the other 40+ languages universal dependencies are available in. This could also address the problem that all existing resources to represent negation scope as a logical form are limited to English (e.g. GMB and ‘DeepBank’ (Flickinger et al., 2012)) or only to a few other languages (e.g. ‘The Spanish Resource Grammar’ (Mar"
W17-1804,P13-2017,0,0.0583409,"Missing"
W17-1804,morante-daelemans-2012-conandoyle,0,0.230024,"phenomena and be learned automatically, given the link to a manually annotated semantic bank. Related work Available resources that contain a representation of negation scope can be divided in two types: 1) those that represent negation as a FOL (or FOL-translatable) representation (e.g. GMB, ‘DeepBank’), where systems built using these resources are concerned with correctly representing FOL variables and predicates in the scope of negation; and 2) those that try to ground negation at a string-level, where both the negation operator and scope are defined as spans of text (Vincze et al., 2008; Morante and Daelemans, 2012). Systems trained on these resources are then concerned with detecting these spans of text. Resources in 1) are limited in that they are only available in English or for a small number of languages. Moreover no attempt has been made to connect them to more widely-used, cross-linguistic frameworks. On the other hand, grounding a semantic phenomenon to a string-level leads to inevitable simplification. For instance, the interaction between the negation operator and the universal quantifier (e.g. ‘Not every staff member is British’ vs. ‘None of the staff members are British’), along a formal repr"
W17-1804,P14-1007,0,0.0154036,"ator and the universal quantifier (e.g. ‘Not every staff member is British’ vs. ‘None of the staff members are British’), along a formal representation that would allow for inference operations is lost. Furthermore, each corpus is tailored to different applications, making annotation styles across corpora incompatible. Nonetheless these resources have been widely used in the field of Information Extraction and in particular in the Bio-Medical domain. Finally, it is also worth mentioning that there has been some attempts to use formal semantic representations to detect scope at a string level. Packard et al. (2014) used hand-crafted heuristics to traverse the MRS (Minimal Recursion Semantics) structures of negative sentences to then detect which words were in the scope of negation and which were not. Basile et al. (2012b) tried instead to first transform a DRS (Discourse Representation Structure) into graph form and then align this to strings. Whilst the MRS-based system outperformed previous work, mainly due to the fact that MRS structures are closely related to the surface realization, the DRT-based approach performed worse than most systems, mostly given to the fact that the formalism is not easily t"
W17-1804,Q16-1010,1,0.837752,"Missing"
W17-1804,D17-1009,1,0.884082,"Missing"
W17-1804,L16-1376,0,0.0174284,"ues. A FOL representation of the entire input graph can be then obtained by traversing the edges in a given order and combining their semantics. However, in its original formulation, UDeIntroduction Amongst the different challenges around the topic of negation, detecting and representing its scope is one that has been extensively researched in different sub-fields of NLP (e.g. Information Extraction (Velldal et al., 2012; Fancellu et al., 2016)). In particular, recent work have acknowledged the value of representing the scope of negation on top of existing linguistic resources (e.g. AMR – Bos (2016)). Manually annotating the scope of negation is however a time-consuming process, requiring annotators to have some expertise of formal semantics. Our solution to this problem is to automatically convert an available representation that captures negation into a framework that allows a rich variety of semantic phenomena to be represented, in22 Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR), pages 22–32, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 2 pLambda does not handle either universal quantifiers or other scope phen"
W17-1804,J12-2005,0,0.122327,"edited minimally so to yield a more fine-grained description on the phenomena they describe, while lexical information is used only for a very restricted class of lexical items, such as negation cues. A FOL representation of the entire input graph can be then obtained by traversing the edges in a given order and combining their semantics. However, in its original formulation, UDeIntroduction Amongst the different challenges around the topic of negation, detecting and representing its scope is one that has been extensively researched in different sub-fields of NLP (e.g. Information Extraction (Velldal et al., 2012; Fancellu et al., 2016)). In particular, recent work have acknowledged the value of representing the scope of negation on top of existing linguistic resources (e.g. AMR – Bos (2016)). Manually annotating the scope of negation is however a time-consuming process, requiring annotators to have some expertise of formal semantics. Our solution to this problem is to automatically convert an available representation that captures negation into a framework that allows a rich variety of semantic phenomena to be represented, in22 Proceedings of the Workshop Computational Semantics Beyond Events and Rol"
W17-1804,W08-0606,0,0.229036,"Missing"
W17-1804,marimon-2010-spanish,0,\N,Missing
W17-1809,P16-1047,1,0.836482,"gation cues producing tokens in the test set that did not appear in the training set, Chinese-to-English word-alignment was also Introduction Negation cue detection is the task of recognizing the tokens (words, multi-word units or morphemes) inherently expressing negation. For instance, the task in (1) is to detect the negation cue “不(not)”, indicating that the clause as a whole is negative. (1) 所有住客均表示不 不 会追究酒店的这次管 理失职 (All of guests said that they would not investigate the dereliction of hotel.) Previous work has addressed this task in English as a prerequisite for detecting negation scope (Fancellu et al., 2016; Cruz et al., 2015; Zou et al., 2013; Velldal et al., 2012; Zhu et al., 2010). But recently, the release of the CNeSp corpus (Zou et al., 2015) allows allows the task to be addressed in 59 Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR), pages 59–63, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics ot = sigmoid(Wox cht + Woh ht−1 + bo ) taken into account. In contrast, the recent success of Neural Network models for negation scope detection (Fancellu et al., 2016) suggested investigating whether a character-based recurrent"
W17-1809,D13-1099,0,0.0215715,"et that did not appear in the training set, Chinese-to-English word-alignment was also Introduction Negation cue detection is the task of recognizing the tokens (words, multi-word units or morphemes) inherently expressing negation. For instance, the task in (1) is to detect the negation cue “不(not)”, indicating that the clause as a whole is negative. (1) 所有住客均表示不 不 会追究酒店的这次管 理失职 (All of guests said that they would not investigate the dereliction of hotel.) Previous work has addressed this task in English as a prerequisite for detecting negation scope (Fancellu et al., 2016; Cruz et al., 2015; Zou et al., 2013; Velldal et al., 2012; Zhu et al., 2010). But recently, the release of the CNeSp corpus (Zou et al., 2015) allows allows the task to be addressed in 59 Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR), pages 59–63, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics ot = sigmoid(Wox cht + Woh ht−1 + bo ) taken into account. In contrast, the recent success of Neural Network models for negation scope detection (Fancellu et al., 2016) suggested investigating whether a character-based recurrent model can perform on par or better th"
W17-1809,P15-1064,0,0.11899,"ar + Bigram + Transition Financial Article Precision Recall F1 25.09 68.37 36.70 29.82 82.79 43.84 61.94 71.16 66.23 65.15 73.02 68.86 58.57 68.37 63.09 Product Review Precision Recall F1 33.18 76.31 46.25 32.73 75.96 45.75 78.93 87.46 82.98 79.05 86.76 82.72 78.57 86.24 82.23 Scientific Literature Precision Recall F1 12.06 77.42 20.87 14.50 93.55 25.11 64.71 35.48 45.83 25.00 9.68 13.95 47.83 35.48 40.74 Precision 24.01 24.28 69.08 71.70 69.08 All Recall 74.40 76.00 84.00 80.80 82.74 F1 36.31 36.80 75.81 75.98 75.30 Table 2: Results on development set for each of the CNeSp subcorpora. Models Zou et al. (2015) Baseline-Word Baseline-Char BiLSTM-char + Bigram + Transition Financial Article Precision Recall F1 72.77 67.02 69.78 24.76 66.52 36.09 28.66 78.11 41.94 62.92 64.81 63.85 63.41 66.95 65.14 63.08 70.39 66.53 Product Review Precision Recall F1 81.94 89.23 85.43 30.93 72.47 43.36 33.41 78.75 46.91 85.02 91.99 88.37 85.06 91.29 88.07 84.56 89.72 87.07 Scientific Literature Precision Recall F1 75.17 78.91 76.99 12.32 83.33 21.46 12.32 83.33 21.46 20.83 16.67 18.52 7.14 3.33 4.55 14.29 10.00 11.76 Precision 22.13 23.68 70.50 73.83 72.49 All Recall 71.68 77.89 82.24 80.25 82.48 F1 33.82 36.32 75.92"
W17-1809,J12-2005,0,\N,Missing
W17-6814,W15-2703,1,0.745006,"erent discourse connectives are used to realize particular types of coherence relations remains unresolved. While some connectives show nearly one-to-one mappings with individual coherence relations, other connectives permit much more flexible usage across contexts. One early enterprise targeting the above question was Alistair Knott’s systematic assessment of the conditions that permit one connective to substitute for another (Knott, 1996). Substitutability, along with categories of coherence relations, then predicts the behavior of individual connectives. Another such enterprise is our own (Rohde et al., 2015, 2016, 2017) on implicit connectives in the context of explicit discourse adverbials. Using naturally occurring passages, we have gathered judgments from multiple participants as to what connective, if any, they could insert into a particular passage immediately before an existing discourse adverbial, to make explicit the author’s intended message. For example, when shown the passage It’s too far to walk. Instead let’s take the bus., a participant might insert so to express what she takes to be the intended causal reading. Our findings show variation across participant responses. Such diverge"
W17-6814,W16-1707,1,0.859986,"Missing"
W18-4710,afantenos-etal-2012-empirical,0,0.0528617,"Missing"
W18-4710,al-saif-markert-2010-leeds,0,0.026557,"otated relations, making it the largest such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotat"
W18-4710,W17-7404,0,0.0608727,"j 0725] Table 2: New senses in PDTB-3 The response to the question can answer the information need explicitly, as in Exs. (17-18), or implicitly (Ex. 19). And the answer can also indicate that the information need cannot be fulfilled (Ex. 20). (19) So can a magazine survive by downright thumbing its nose at major advertisers? Garbage magazine, billed as ”The Practical Journal for the Environment,” is about to find out. [wsj 0062] (20) With all this, can stock prices hold their own? ”The question is unanswerable at this point” she says. [wsj 0681] Because these relations involve dialogue acts (Bunt et al., 2017), which we treat as distinct from discourse relations, and because they are uninstantiable as connectives, we have added a new coherence relation type for them — called HYPOPHORA. Of course, not all questions in a discourse are dialogue acts. H YPOPHORA does not apply when the subsequent text relates to a question in other ways – for example, with rhetorical questions that are posed for dramatic effect or to make an assertion, rather than to elicit an answer, as in Ex. (21), or if the subsequent text provides an explanation for why the question has been asked, as in Ex. (22). In such cases, an"
W18-4710,F12-2042,0,0.260077,"gely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad"
W18-4710,L16-1629,1,0.825458,"In the PDTB, we have as yet found no evidence for the reverse directionality for either of these senses. 6 Conclusion We have presented highlights from our work on enriching the PDTB with new relations, which has also led to modifications and extensions to the PDTB guidelines. Annotating a further ⇠13K discourse relations and reviewing existing PDTB-2 annotation to bring it in line with the new guidelines has highlighted the importance of assessing consistency across the corpus — that similar tokens are annotated in a similar way, no matter when they were annotated. Such semantic consistency (Hollenstein et al., 2016) is meant to facilitate improvement in all future applications of the PDTB-3. Consistency checks are described in the detailed annotation manual that will accompany the corpus in its LDC distribution, as well as being available at the PDTB website. Acknowledgements Missing from the list of authors is the name of our colleague and dear friend, Aravind K Joshi. Aravind conceived of the Penn Discourse TreeBank and was instrumental in its development from its beginnings in 2005 through the new PDTB-3. Aravind died peacefully on 31 December 2017, and we miss him more than we can say. An obituary wi"
W18-4710,J93-2004,0,0.0649142,"Missing"
W18-4710,W09-3029,1,0.76508,"ed in 2008, contains over 40K tokens of annotated relations, making it the largest such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only thr"
W18-4710,J05-1004,0,0.246178,"Missing"
W18-4710,I13-1011,0,0.0350587,"Missing"
W18-4710,prasad-etal-2008-penn,1,0.84406,"chy, and all changes have been propagated through the rest of the corpus. 1 Introduction The last decade has seen growing interest in enabling language technology and psycholinguistics to move beyond the sentence, to what can be derived from larger units of text. This has led to greater interest in the properties of discourse. One such property is the coherence between clauses and sentences arising from low-level discourse relations. This level of meaning has been made overt through manual annotation in the Penn Discourse TreeBank (PDTB), developed with NSF support.1 Version 2.0. of the PDTB (Prasad et al., 2008), released in 2008, contains over 40K tokens of annotated relations, making it the largest such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013),"
W18-4710,C10-2118,1,0.807988,"t, they were encouraged to identify the non-connective expression in Arg2 that they took as the source of the perceived redundancy as the AltLex, Annotating intra-sentential discourse relations in the PDTB-3 has led to modifying the above convention in two ways — what is annotated as AltLex and where AltLex can be annotated. With respect to what is annotated as AltLex, reliably identifiable AltLex expressions in the PDTB-2 included one part that conveyed the relation and one part that referred anaphorically or elliptically to Arg1, as in “after that” or “a likely reason for the disparity is” (Prasad et al., 2010). To allow for AltLex expressions in the context of intra-sentential discourse relations, we have allowed expressions of any form or syntactic class to be labeled as AltLex, including adjectives and adjective-modifiers such as additional, next, further, and earlier. While these expressions continue to suggest the relation, unlike AltLex expressions in PDTB-2, the reference to Arg1 may be implicit. That is, while next implies next to something, that something may be implicit. One consequence of this new convention is that words such as further and next, that can appear as discourse adverbials,"
W18-4710,J14-4007,1,0.837648,", 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the corpus, adding over 17K new discourse relations. Most of the new relations occur intra-sententially, but there are also ⇠300 inter-sentential implicit relations between adjacent sentences whose annotation is missing from the PDTB-2.2 This paper focuses on the new intrasentential relations annotated in the PDTB-3. We also discuss major modifications and extensions to the PDTB guidelines, including the sense hierarchy, which have resulted from our study of the new relations, and which have been propagated throu"
W18-4710,W15-2707,1,0.887923,"Missing"
W18-4710,W17-5502,1,0.813461,"tential implicit relations between adjacent sentences whose annotation is missing from the PDTB-2.2 This paper focuses on the new intrasentential relations annotated in the PDTB-3. We also discuss major modifications and extensions to the PDTB guidelines, including the sense hierarchy, which have resulted from our study of the new relations, and which have been propagated throughout the corpus. PDTB-3, which we plan to release to the community in Fall 2018, will contain over 53K tokens of discourse relations, and as with PDTB-2, will 1 http://www.seas.upenn.edu/˜pdtb Separate from the PDTB-3, Prasad et al. (2017) address the annotation of cross-paragraph implicit relations that are not annotated in either PDTB-2 or PDTB-3. These annotations are provided for 145 texts from Sections 01, 06, and 23 of the Wall Street Journal corpus, producing a full-text annotated sub-corpus merged with the PDTB-3 annotations for the same texts. However, because the annotation guidelines developed for the cross-paragraph annotation depart in some respects from the PDTB guidelines in ways not incorporated in PDTB-3, these annotations will be released to the community separately, via github (https://github.com/pdtb-upenn/f"
W18-4710,tonelli-etal-2010-annotation,1,0.78837,"fiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the cor"
W18-4710,W16-1704,1,0.918526,"flection, Mr. Oka says, he concluded that Nissan is being prudent in following its slow-startup strategy instead of simply copying Lexus. (EXPANSION . SUBSTITUTION . ARG 1- AS - SUBST ) [ WSJ 0286] Third, the restriction on arguments to clauses (with a small set of specific exceptions) precluded relations between conjoined verb phrases. The PDTB-2 exceptions to clausal realization did allow verb phrases to be valid arguments, but not of the VP conjunction itself. Thus, in Ex. (7), while because was annotated, the VP conjunction and was not. Conjoined VPs have now been annotated in the PDTB-3 (Webber et al., 2016), as in Ex. (8) and Ex. (9). (7) She became an abortionist accidentally, and continued because it enabled her to buy jam, cocoa and other warrationed goodies. (CONTINGENCY. CAUSE . REASON) [wsj 0039] (8) She became an abortionist accidentally, and continued because it enabled her to buy jam, cocoa and other warrationed goodies. (EXPANSION . CONJUNCTION) [wsj 0039] 88 Intra-S Context Free Adjuncts Free TO-infinitives Prep. Clausal Subordination Conjoined VPs S Conjunction Implicits Total ˜Num ˜2200 ˜1500 ˜1600 ˜5800 ˜1800 ˜13000 Table 1: Approximate distribution of new intra-sentential relation"
W18-4710,J05-2005,0,0.140369,"ONDITION) [wsj 0443] Since researchers may be interested in analyzing these constructional AltLex’s further, we have assigned them the relation type A LT L EX C, to indicate that they are a sub-type of Altlex. Tokens of this type have all the same fields as an AltLex. They are just marked for easy identification and review. 94 5 Mapping to ISO-DR-Core Existing annotation frameworks (which, apart from the PDTB, have led to the creation of several other corpora of coherence relations, including Afantenos et al. (2012), Carlson et al. (2003), Reese et al. (2007), Sanders and Scholman (2012), and Wolf and Gibson (2005)) exhibit some major differences in their underlying assumptions, but there are also strong compatibilities. ISO DR-Core (ISO 2476178: 2016) forms part of an effort to develop an international standard for the annotation of discourse relations.4 One of the outcomes of this effort (Bunt and Prasad, 2016) was to provide clear and mutually consistent definitions of a set of core discourse relations (senses) – ISO-DR-Core – many of which have similar definitions in different frameworks, and provide mappings from ISO-DR-Core relations to relations in different frameworks, including the PDTB. With t"
W18-4710,K15-2001,1,0.915269,"Missing"
W18-4710,K16-2001,1,0.85992,"used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the corpus, adding over 17K new discourse relations. Most of the new relations occur intra-sententially, but there are also"
W18-4710,I08-7009,1,0.83098,"st such corpus available to date. Largely because the PDTB was based on the simple idea that discourse relations are grounded in an identifiable set of explicit words or phrases (discourse connectives) or simply in sentence adjacency, it has been taken up and used by many researchers in the NLP community and more recently, by researchers in psycholinguistics as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew tha"
W18-4710,L18-1301,0,0.0149818,"cs as well. It has also stimulated the development of similar resources in other languages (Chinese (Zhou and Xue, 2015), Czech (Pol´akov´a et al., 2013), Hindi (Oza et al., 2009), Modern Standard Arabic (Al-Saif and Markert, 2010), Turkish (Zeyrek and Webber, 2008) and French (Danlos et al., 2012)) and domains (biomedical texts (Prasad et al., 2011), conversational dialogues (Tonelli et al., 2010)), the organization of community-level shared tasks on shallow discourse parsing (Xue et al., 2015; Xue et al., 2016), and a cross-lingual discourse annotation of parallel texts, the TED-MDB corpus (Zeyrek et al., 2018), to support both linguistic understanding of coherence in different languages and improvements in machine translation of discourse connectives. Given only three years in which to develop guidelines, and annotate and release the PDTB, we knew that it would be incomplete (Prasad et al., 2014). With additional support from the NSF, we have now addressed many of the gaps in the corpus, adding over 17K new discourse relations. Most of the new relations occur intra-sententially, but there are also ⇠300 inter-sentential implicit relations between adjacent sentences whose annotation is missing from t"
W19-0411,L16-1629,1,0.892074,"Missing"
W19-0411,D14-1036,0,0.0194314,"Missing"
W19-0411,P09-2004,0,0.594105,"be subject to both usage and sense ambiguity, as has already been discussed in the literature. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four are illustrated and discussed here. 1 Introduction Discourse connectives, like other linguistic expressions, are subject to ambiguity. Two types of ambiguity — usage ambiguity, whether or not a given token is serving as a discourse connective in its context, and sense ambiguity, what discourse relation(s) a given token is signalling — were the subject of a study by Pitler and Nenkova (2009), who showed how syntactic features could help resolve them both. But discourse connectives are no different from other linguistic expressions in being subject to other types of ambiguity as well. Four of them are discussed here, as a way of encouraging researchers to determine whether existing disambiguation methods suffice to handle them or whether the methods need to be extended. Ignoring the full range of ambiguity of discourse connectives can lead to discourse relations being mis-labelled both manually (during annotation) and automatically (during discourse parsing). As background to pres"
W19-0411,prasad-etal-2008-penn,1,0.687912,"ve been discussed in the context of other linguistic forms. Section 3 discusses part-of-speech ambiguity, which can affect how a given token functions as a discourse connective. Section 4 discusses multi-word ambiguity, where a sequence of tokens can be ambiguous between a sequence of separate elements and a single multi-word discourse connective. Section 5 discusses a scope ambiguity that affects the sense of discourse connectives. Finally, Section 6 discusses semantic role ambiguity involving the arguments of certain CONCESSION relations. 2 2.1 Background PDTB-2 The Penn Discourse Treebank (Prasad et al., 2008) was created as the largest public repository of annotated discourse relations (over 43K), including over 18.4K signalled by explicit discourse connectives (coordinating or subordinating conjunctions, or discourse adverbials). All relations in the corpus are labelled with either one or two senses from a three-level sense hierarchy, whose top level comprised four non-terminal senses: E XPANSION, C OMPARISON, C ONTINGENCY and T EMPORAL. Most discourse relations were labelled with terminal senses, except where annotators were unable to decide and backed off to a level-2 (or in some cases, a top-l"
W19-0411,J14-4007,1,0.825043,"implicit connectives that signalled the sense(s) they inferred to hold between the arguments. The approach in the PDTB-2 is agnostic about any higher-level discourse structure, and as such, made no attempt to build a tree or graph structure of relations over the text as a whole. The size and availability of the PDTB-2 spawned the field of shallow discourse parsing, as in the 2015 and 2016 CoNLL shared tasks (Xue et al., 2015, 2016), as well as the development of similar resources for other languages, including Chinese, Hindi, and Turkish. An in-depth discussion of the PDTB-2 can be found in (Prasad et al., 2014). 2.2 Pitler & Nenkova (2009) Pitler and Nenkova (2009) showed how syntactic features could be used in disambiguating both usage ambiguity and sense ambiguity. To understand these types of ambiguity, consider the word since. Ex. 1 illustrates its non-discourse usage, where since is simply a temporal preposition. Both Ex. 2 and Ex. 3 illustrate discourse usages and also the sense ambiguity of since, signalling a purely temporal relation in Ex. 2 and a purely causal relation in Ex. 3. (1) She has been up since 5am. (2) There have been over 100 mergers since the most recent wave of friendly takeo"
W19-0411,K15-2001,1,0.873247,"consisted of two arguments labelled Arg1 and Arg2, with each relation anchored by either an explicit discourse connective or adjacency. In the latter case, annotators inserted one or more implicit connectives that signalled the sense(s) they inferred to hold between the arguments. The approach in the PDTB-2 is agnostic about any higher-level discourse structure, and as such, made no attempt to build a tree or graph structure of relations over the text as a whole. The size and availability of the PDTB-2 spawned the field of shallow discourse parsing, as in the 2015 and 2016 CoNLL shared tasks (Xue et al., 2015, 2016), as well as the development of similar resources for other languages, including Chinese, Hindi, and Turkish. An in-depth discussion of the PDTB-2 can be found in (Prasad et al., 2014). 2.2 Pitler & Nenkova (2009) Pitler and Nenkova (2009) showed how syntactic features could be used in disambiguating both usage ambiguity and sense ambiguity. To understand these types of ambiguity, consider the word since. Ex. 1 illustrates its non-discourse usage, where since is simply a temporal preposition. Both Ex. 2 and Ex. 3 illustrate discourse usages and also the sense ambiguity of since, signall"
W19-0411,K16-2001,1,0.909654,"Missing"
W19-4011,L16-1492,0,0.186174,"er inspection of how authors’ apply these labels, we often find discrepancies that would not work for our purpose. Table 2 provides a discussion of comparisons and similarities of our label schema to those that are most closely related (Fisas et al., 2015, 2016; Teufel, 1999; Teufel et al., 2006b; Angrosh et al., 2012; Teufel et al., 2009). One contributing factor as to why existing labels do not adequately support our goals is that they are designed to look across the whole of a document. As a result, they seek either very general or much finer grained labelling than we require. For example, Fisas et al. (2016) distinguishes between an author using methods, using data or using tools from another cited work. This finer grained approach is not relevant or needed to provide feedback in a Related Work section, we only need to know that the author used the cited work. Cited Works To provide informative feedback, we need to establish the relevance of a cited work to the author’s work or if this cited work is perfunctory in nature. Firstly, we provide a label that accounts for description of a cited work – CWDESC. Our other labels account for contrasting the author’s work to cited work saying: (i) it is si"
W19-4011,W15-1605,0,0.56349,"citation types (Thompson and Tribble, 2001). We also include a background label that relates to when an author says something positive or highlights a strength in the field/general – BG-EVAL-P. Mapping Qualities to the Annotation Schema Looking just at label names, it can seem like our labels (Table 1) are direct replications of other schemas. However, on closer inspection of how authors’ apply these labels, we often find discrepancies that would not work for our purpose. Table 2 provides a discussion of comparisons and similarities of our label schema to those that are most closely related (Fisas et al., 2015, 2016; Teufel, 1999; Teufel et al., 2006b; Angrosh et al., 2012; Teufel et al., 2009). One contributing factor as to why existing labels do not adequately support our goals is that they are designed to look across the whole of a document. As a result, they seek either very general or much finer grained labelling than we require. For example, Fisas et al. (2016) distinguishes between an author using methods, using data or using tools from another cited work. This finer grained approach is not relevant or needed to provide feedback in a Related Work section, we only need to know that the author"
W19-4011,P16-2089,0,0.187839,"grammar and spelling, students must grasp aspects of style and content structure expected within their discipline. Automated recognition of content features in academic writing has become a popular approach to assist students in recent years. Previous work has focused on identifying rhetoric intentions, such as those described by Swales (1981) that can be found in an Introduction (Cotos and Pendar, 2016; Anthony and V. Lashkia, 2003) or in PhD summaries (Feltrim et al., 2006). Other approaches have focused on identifying argument components and relations and how these relate to essay scores (Ghosh et al., 2016). The one aspect that these approaches have in common is the need for annotated data based on taskorientated annotation schemes. Our focus is on building an annotation schema which can help writers recognise appropriate intentions in writing their Related Work section, and indicate when these are missing. Annotating intention in academic writing is challenging as the language and author intentions differ across the typical sections found in a paper 2 Background Our aim is to help authors recognise rhetorical intentions that are present in their writing and highlight those that are missing, usi"
W19-4011,W10-1913,0,0.649697,"seek, such as those that consider citation function (Teufel et al., 2006a; Angrosh et al., 2012) or argument zones reflecting rhetoric intentions (Teufel, 1999; Teufel et al., 2009). However, these are designed for different purposes, such as understanding citation relations, summarisation or information extraction (e.g gene relations, knowledge claims). Thus, they also have labels that are irrelevant to Related Work, e.g. ‘Conclusion’, which may make the annotation task more difficult. Since previous work has shown that annotation schemes benefit from being designed for their specific goal (Guo et al., 2010), we propose a specific annotation framework to support automated writing feedback on Related Work. This paper describes our framework for annotating the discourse of Related Work in such a way that it supports feedback on writing. The framework reflects qualities that both theory and experiments have shown to be important. We discuss how these qualities have motivated our design along with those existing schemes that are most closely related to ours. We report results that show reliable annotation for this framework. Future work will investigate the degree to which such annotation can be auto"
W19-4011,W15-0501,0,0.0181332,"tions that uses Kappa agreement reports agreement in a range of 0.650.78 (Teufel et al., 2006a; Fisas et al., 2015; Teufel et al., 2009) with Liakata et al. (2012) being much lower at 0.55. Teufel et al. (2009) points out that Kappa treats agreement in rare categories as surprising and rewards these more than frequent categories. Although she sees this as an advantage because scientific publications often have these rare categories, others see this as misleading and criticise that chance-corrected measures do this when applied to unbalanced data-sets. Hence, others often report raw agreement (Kirschner et al., 2015). Our data does have rare categories and so we report the raw agreement in addition to the Kappa agreement. The annotators were given 9 pages of guidelines which contained examples and suggested workflow to decide on an annotation label. Initially, the annotators met to discuss the guidelines and ensure their understanding. They trained on the same 10 Related Work sections and compared their results discussing any difference. 6 DESC 8 44 6 Table 3: Author Label Agreement Matrix. The letter A (Author) at the beginning of each entry was omitted for the sake of clarity. Annotator Task The Related"
W19-4011,W06-1312,0,0.166992,"ersity of Edinburgh Edinburgh, UK Dorota Głowacka Dept. of Computer Science University of Helsinki Helsinki, Finland a.j.casey@sms.ed.ac.uk bonnie@inf.ed.ac.uk glowacka@cs.helsinki.fi Abstract (Introduction, Methods, Results, Discussion) and within disciplines (Hyland, 2015). We focus on one section of scientific text that has, for the most part, been ignored in the past — the Related Work section. Currently no annotation schema specifically focuses on Related Work. There are schemas that capture some, but not all, elements of intentions we seek, such as those that consider citation function (Teufel et al., 2006a; Angrosh et al., 2012) or argument zones reflecting rhetoric intentions (Teufel, 1999; Teufel et al., 2009). However, these are designed for different purposes, such as understanding citation relations, summarisation or information extraction (e.g gene relations, knowledge claims). Thus, they also have labels that are irrelevant to Related Work, e.g. ‘Conclusion’, which may make the annotation task more difficult. Since previous work has shown that annotation schemes benefit from being designed for their specific goal (Guo et al., 2010), we propose a specific annotation framework to support"
W19-4011,W06-1613,0,0.133373,"ersity of Edinburgh Edinburgh, UK Dorota Głowacka Dept. of Computer Science University of Helsinki Helsinki, Finland a.j.casey@sms.ed.ac.uk bonnie@inf.ed.ac.uk glowacka@cs.helsinki.fi Abstract (Introduction, Methods, Results, Discussion) and within disciplines (Hyland, 2015). We focus on one section of scientific text that has, for the most part, been ignored in the past — the Related Work section. Currently no annotation schema specifically focuses on Related Work. There are schemas that capture some, but not all, elements of intentions we seek, such as those that consider citation function (Teufel et al., 2006a; Angrosh et al., 2012) or argument zones reflecting rhetoric intentions (Teufel, 1999; Teufel et al., 2009). However, these are designed for different purposes, such as understanding citation relations, summarisation or information extraction (e.g gene relations, knowledge claims). Thus, they also have labels that are irrelevant to Related Work, e.g. ‘Conclusion’, which may make the annotation task more difficult. Since previous work has shown that annotation schemes benefit from being designed for their specific goal (Guo et al., 2010), we propose a specific annotation framework to support"
W19-4011,W04-1205,0,0.745884,"or intention schemas to provide reliability studies of their annotations and to fully automate these. AZ marks zones that identify knowledge claims indicating who these knowledge claims belong to, in addition to providing categories for relationships between the authors or existing works. Teufel et al. (2009) extended the AZ schema from 7 to 15 categories. This extension allowed the authors to then apply their schema to the domain of life sciences in addition to their original domain of Computational Linguistics. The AZ scheme has also been successfully adapted in other domains, e.g. biology (Mizuta and Collier, 2004). The requirement to adapt the schema to new domains supports the idea that different styles of writing across domains may influence recognising intention in writing and our choice to focus on only one domain. Whilst the AZ scheme has proven very successful, it has been applied to capturing intentions across entire documents. The schema was designed to support tasks of summarisation and to improve information access. For a section such as Related Work, which is rarely used in summarisation or information access, this means that its meaningful author intentions may be labelled too generically t"
W19-4011,C12-2103,0,0.427155,"Missing"
W19-4011,D14-1006,0,0.0293576,"an annotation unit will be the most meaningful. One reason for this is that in the next stage of our work (providing feedback), we will need to look at several sentences together to determine relevance, as citation relevance has been shown to require to look beyond just the citing sentence (Teufel et al., 2006a). annotation frameworks which are more directly linked to the Toulmin model of argumentation (Toulmin, 2003) to represent argument structures in a research article. These annotation schemas represent arguments as claims and premises with some including relations of support and attack (Stab and Gurevych, 2014). Whilst this structure has been shown to work well in a persuasive essay scenario, it would not support the types of intentions discussed in the next section that are relevant to Related Work. 2.2 Writing Analytics Tools Using rhetoric intentions to provide writers with feedback has been successful in academic writing. Mover (Anthony and V. Lashkia, 2003), Research Writer Tutor (RWT) (Cotos and Pendar, 2016) and ACAWriter (Abel, 2018) are three tools based on Swales CARS model (Swales, 1990). The first two tools carry out annotation based on their interpretation of the CARS model — the first"
W19-4011,D09-1155,0,0.141262,"Finland a.j.casey@sms.ed.ac.uk bonnie@inf.ed.ac.uk glowacka@cs.helsinki.fi Abstract (Introduction, Methods, Results, Discussion) and within disciplines (Hyland, 2015). We focus on one section of scientific text that has, for the most part, been ignored in the past — the Related Work section. Currently no annotation schema specifically focuses on Related Work. There are schemas that capture some, but not all, elements of intentions we seek, such as those that consider citation function (Teufel et al., 2006a; Angrosh et al., 2012) or argument zones reflecting rhetoric intentions (Teufel, 1999; Teufel et al., 2009). However, these are designed for different purposes, such as understanding citation relations, summarisation or information extraction (e.g gene relations, knowledge claims). Thus, they also have labels that are irrelevant to Related Work, e.g. ‘Conclusion’, which may make the annotation task more difficult. Since previous work has shown that annotation schemes benefit from being designed for their specific goal (Guo et al., 2010), we propose a specific annotation framework to support automated writing feedback on Related Work. This paper describes our framework for annotating the discourse o"
W90-0125,C90-2068,1,\N,Missing
W98-0113,P83-1020,0,0.432737,"one that is preferred. For example, in plan recognition (identifying the structure of goals and subgoals that give rise to what is usually taken to be a sequence of observed actions), Kautz (Kautz, 1990) suggested a ""goal minimization"" bias that preferred a tree with the fewest goals (non-terminal nodes) able to ""explain"" the sequence of actions. Where goal minimization is known to produce the wrong explanation, some other bias is needed to yield the one that is preferred (Gertner and Webber, I 996). Similarly, in associating a preferred reading with a compact underspecified representation, (Marcus et al., 1983) proposed a bias towards a tree that minimised the dominance relation . That is, if two node names stand in a dominance relation, they are taken to refer to one and the same node, provided nothing rules it out. Ofcourse, such a ""min.dom"" bias might yield several trees, each of which are equally minimal. Typically, this is true of global ambiguities as in (6) above, where dominance can be minimised by identifying node 5 either with node 4 or with node 6, each .move resulting in an equally minimal tree. An alternative bias combines ""min.dom"" with ""right-association"" (Frazier, 1995; Chen and Vija"
W98-0113,J92-4004,0,0.478545,"tational Linguistics University of the Saarland Saarbrücken, Gerrnany claire@coli.uni-sb.de Bonnie Webber Computer and Information Science University of Pennsylvania Philadelphia PA 19104-6389 USA bonnie@central.cis.upenn.edu Descriptions. In recent years, both formal and computational linguistics have been exploiting descriptions of structures where previously the structures themselves were used. Tue practice started with (Marcus et al., 1983), who demonstrated the value of (syntactic) tree descriptions for near-deterministic incremental parsing. Vijay-Shankar (Vijay-Shankar and Joshi, 1988; Vijay-Shankar, 1992) used descriptions to maintain the monotonicity of syntactic derivations in the framework ofFeature-Based Tree Adjoining Grammar. In semantics, both (Muskens, 1997) and (Egg et al., 1997) have shown the value of descriptions as an underspecified representation of scope ambiguities. Tue current paper further extends the use of descriptions, from individual sentences to discourse, showing their benefit for incremental, near-detenninistic discourse processing. In particular, we show that using descriptions to desc~be the semantic representation of discourse penn1ts: (1) a monotone treatment of lo"
W98-0113,W98-0315,1,0.794915,"ntity from possible alternatives. We believe it is worth exploring what bias best models the preferences people have in discourse interpretation, and how it resembles their preference at the sentence level. thank Mark Steedman and Aravind Joshi for comments and suggestions. An early draft of this paper was presented at the Workshop on Underspecification, Bad Teinach, Germany, May 1998. Claire Gardent is grateful to the Deutsche Forschungsgesellschaft for financial support within the SFB 378, Comparison with related work. A related approach to discourse structure and semantics is presented in (Webber and Joshi, 1998), where Lexicalised Tree Adjoining Grammar (LTAG) is used to construct the compositional semantics of discourse. Although the basic structures used here are different, we foresee no difficulty in modifiying them in order to integrate the additional information included in the LTAG discourse trees. Essentially, the atomic labels representing the relations should be mapped into the feature structures used in (Webber and Joshi, 1998) and this information used to labe! not the root node of a local tree but its anchor. Second, the LTAG approach has focussed on describing the compositional semantics"
W98-0315,P97-1012,1,0.546272,"s trying to do. I Secondly, there is a single auzil~ary tree whose semantics corresponds simply to continuing the description conveyed by the structure to which it is adjoined. Any additionalinferences that a listener draws from the resulting adjacency are defeasible, Introduction In the past few years, researchers interested in accounting for how elements combine in a discourse, have taken to using the adjoining operation found in Tree-Adjoining G r a m m a r (TAG) (Gardent, 1994; Gardent, 1997; Polanyi and van den Berg, 1996; Schilder, 1997; van den Berg, 1996; Webber, 1991). More recently, Cristea and Webber (1997) have argued that a Tree-Adjoining G r a m m a r for discourse would also need the substitution operation found in a lexicalized TAG (Schabes, 1990). Here we move further and explore a fully lexicalized TAG for discourse, allowing us to examine how the insights of lexicalized g r a m m a r s - that the basic elements of a clause are not simply words, but structures that reflect a word&apos;s role and syntactic/semantic scope carry over to discourse. We show how this suggests explanations for such phenomena as the following: • that arguments of a coherence relation can be stretched &quot;long distance&quot; b"
W98-1419,P85-1008,0,0.511827,"s consideration of syntax and semantics. Reasoningmust enable the generator to assess quickly and reliably at any stage how the hearer will interpret the current sentence, with its -(inc0mplete)syntax,andsemantics. We show that these representational and reasoning requirements are met in the SPUDsystem for sentence planning and realization. 1 Introduction T h e problem we address is that o f producing efficient descriptions of objects, collections, acti0ns, events, etc. (i.e., any generalized individual from a rich ontology for Natural Language such as those described in [2] •and advocated in [9]). We are interested in a particular kind of efficiency that we call textual economy, which presupposes a view of sentence generation as goal-directed activity that has broad support in Natural Language Generation (NLG) research [1, 5, 15, 17]. According to this view, a system has certain communicative intentions that it aims to fulfill in producing a description. For example, the system might have the goal of identifying an individual or action o~ to the hearer, or ensuring that the hearer knows that has property P. Such goals can be satisfied explicitly by assembling appropriate syntactic co"
W98-1419,J88-2003,0,0.0657808,"ucleus(PREP, REMOVING, RESULT) A in(PREP, start(TIME), REMOVED, SOURCE) A (5b) S e m a n t i c s : caused-motion(REMOVING, REMOVER, REMOVED) A away(RESULT, end(TIME), REMOVED, SOURCE) The tree given in (5a) specifies that remove syntactically satisfies a requirement tO include an s, requires a further NP to be included (describing what is removed), and allows the possibility of an explicit vv modifier that describes what the latter has been removed from. 2 The semantics in (5b) consists of a set of features, formulated in an ontologically promiscuous semantics, as advocated in [9]. It follows [14] in viewing events as consisting of a preparatory phase, a transition, and a result state (what is called a nucleus in [14]). The semantics in (5b) describes all parts of a remove event: In the preparatory phase, the object (REMOVED) is in/on SOURCE. It undergoes motion caused by the agent (REMOVER),and ends up away from SOURCE in the result state. Semantic features are used by SPUD in one of two ways. Some make a semantic contribution that specifies n e w information---these add to what new information the speaker can convey with the structure. Others simply impose a semantic requirement that"
W98-1419,J92-4007,0,0.103748,"s of a sentence by incorporating lexico-grammatical entries into a partial sentence one-by-one and incrementally assessing the answers to the questions given above. In this paper, we describe the intermediate representations that allow SPUD to do so, since these representations have been glossed over in earlier presentations [26, 27]. Reasoning in SPUD is performed using a fast modal theorem prover [24, 25] to keep track both of what the sentence entails and what the sentence requires in context. By reasoning about the predicated relationships withinclauses and the informational relationships [16] between clauses, sPUD is able to generate sentences that exhibit two forms of textual economy: referential interdependency among noun phrases within a single clause, and pragmatic overloading of clauses in instructions [7]. For an informal example of the textual economy to be gained by taking advantage of predicated relationships within clauses, consider the scene pictured in Figure 1 and the goal of getting the hearer to take the rabbit currently in the hat out of the hat it&apos;s currently in. Even though there are several rabbits, several hats, and even a rabbit in a bathtub and a flower in a"
W98-1419,J93-4004,0,0.158801,"Missing"
W98-1419,A92-1006,0,0.0291366,"me way: (3a) Hold the cup under the faucet... (3b) ...to wash it. Examples like (1) and• (2) suggest that the natural locality for sentence planning is in a description of a generalized individual. Even though such descriptions may play out over several clauses (or even sentences), the predications within clauses and the informational relations across clauses of a description give rise to similar textual economies, that merit a similar treatment. 2 SPUD• An NLG system must satisfy at least three constraints in mapping the content planned for a sentence onto the string of words that realize it [4, 13, 20]. Any fact to be communicated must be fit into an abstract grammatical structure, including lexical items. Any reference to a domain entity must be elaborated into a description that distinguishes the entity from its distractors--the salient alternatives to it in context. Finally, a surface form must be found for this conceptual material. In one architecture for NLG Systems that is becoming something of a standard [22], these tasks are performed in Separate stages. For example, to refer to a uniquely identifiable entity x from the common ground, first a set of concepts is identified that toget"
W98-1419,W96-0410,1,0.942178,"ses where a single intention to act is used to wholly or partially satisfy several of an agent&apos;s goals simultaneously. i 78 I I il I I il I I I I fl I il Figure 1: &quot;Remove the rabbit from the hat.&quot; • what (generalized) individuals would the hearer take the sentence to refer to? • what would the sentence invite the hearer to conclude about those individuals? • how can this sentence be modified or extended? can the generator recognize and exploit an opportunity for textual economy. These representational and reasoning requirements are met in the SPUD system for sentence planning and realization [26, 27]. SPUD draws on earlier work by Appelt [1] in building sentences using planning techniques, sPUD plans the syntax and semantics of a sentence by incorporating lexico-grammatical entries into a partial sentence one-by-one and incrementally assessing the answers to the questions given above. In this paper, we describe the intermediate representations that allow SPUD to do so, since these representations have been glossed over in earlier presentations [26, 27]. Reasoning in SPUD is performed using a fast modal theorem prover [24, 25] to keep track both of what the sentence entails and what the se"
W98-1419,P97-1026,1,0.939159,"ses where a single intention to act is used to wholly or partially satisfy several of an agent&apos;s goals simultaneously. i 78 I I il I I il I I I I fl I il Figure 1: &quot;Remove the rabbit from the hat.&quot; • what (generalized) individuals would the hearer take the sentence to refer to? • what would the sentence invite the hearer to conclude about those individuals? • how can this sentence be modified or extended? can the generator recognize and exploit an opportunity for textual economy. These representational and reasoning requirements are met in the SPUD system for sentence planning and realization [26, 27]. SPUD draws on earlier work by Appelt [1] in building sentences using planning techniques, sPUD plans the syntax and semantics of a sentence by incorporating lexico-grammatical entries into a partial sentence one-by-one and incrementally assessing the answers to the questions given above. In this paper, we describe the intermediate representations that allow SPUD to do so, since these representations have been glossed over in earlier presentations [26, 27]. Reasoning in SPUD is performed using a fast modal theorem prover [24, 25] to keep track both of what the sentence entails and what the se"
