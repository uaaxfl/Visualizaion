2020.acl-main.62,P19-1469,0,0.0257751,"hese all require a complete state-action annotation for expert demonstrations. We aim to overcome this limitation in this study. Semi-supervised learning aims to utilize unlabeled data to boost model performance, and is studied in computer vision (Iscen et al., 2019), item ranking (Park and Chang, 2019; Huang et al., 2019b), and multi-label classification (Miyato et al., 2015; Wang et al., 2018, 2019b). Many studies apply semi-supervised VAE (Kingma et al., 2014) for different classification tasks, e.g., sentiment analysis (Xu et al., 2017; Li et al., 2019a), text matching (Shen et al., 2018; Choi et al., 2019). While these work focus on prediction accuracies, we aim to enrich expert demonstrations via semi-supervised learning. 6 Conclusions We study the problem of semi-supervised policy learning and propose Act-VRNN to provide more effective and stable rewards estimations. We formulate a generative model to jointly infer action labels and learn action embeddings. We design a novel reward function to first model dialogue progress, and estimate action rewards by determining whether the action leads to similar progress as expert dialogues. The experimental results confirm that Act-VRNN achieves better"
2020.acl-main.62,P18-5002,0,0.145652,"Missing"
2020.acl-main.62,P16-1230,0,0.0607927,"Missing"
2020.acl-main.62,I17-1074,0,0.0315972,"f-the-art approaches, and their variants enhanced by semi-supervised learning techniques (Sec. 4.2). We analyze the effectiveness of action learning and reward estimation of Act-VRNN under different supervision ratios (Sec. 4.3). 4.1 Settings We use MultiWOZ (Budzianowski et al., 2018), a multi-domain human-human conversational dataset in our experiments. It contains in total 8438 dialogues spanning over seven domains, and each dialogue has 13.7 turns on average. MultiWOZ also contains a larger dialogue state and action space compared to former datasets such as movie-ticket booking dialogues (Li et al., 2017), and thus it is a much more challenging environment for policy learning. To use MultiWOZ for policy learning, a user simulator that initializes a user goal at the We use a three-layer transformer (Vaswani et al., 2017) with a hidden size of 128 and 4 heads as our base model for action embedding learning, i.e., g(·) in Eqn. 12. We use grid search to find the best hyperparameters for the models. We choose the action embedding dimensionality among {50, 75, 100, 150, 200}, the stochastic latent state size in VRNN among {16, 32, 64, 128, 256}, and the deterministic latent state size among {25, 50,"
2020.acl-main.62,P19-1186,0,0.177529,"Missing"
2020.acl-main.62,W18-5041,0,0.044364,"Missing"
2020.acl-main.62,P18-1203,0,0.0405383,"ogue State annotation Task-oriented dialogue systems complete tasks for users, such as making a restaurant reservation or finding attractions to visit, in multi-turn dialogues (Gao et al., 2018; Sun et al., 2016, 2017). Dialogue policy is a critical component in both the conventional pipeline approach (Young et al., 2013) and recent end-to-end approaches (Zhao et al., 2019). It decides the next action that a dialogue system should take at each turn. Considering its nature of sequential decision making, dialogue policy is usually learned via reinforcement learning (Su et al., ∗ Utterance 2015; Peng et al., 2018; Zhang et al., 2019). Specifically, dialogue policy is learned by maximizing accumulated rewards over interactions with an environment (i.e., actual users or a user simulator). Handcrafted rewards are commonly used for policy learning in earlier work (Peng et al., 2018), which assigns a small negative penalty at each turn and a large positive/negative reward when the task is successful/failed. However, such reward setting does not provide sufficient supervision signals in each turn other than the last turn, which causes the sparse reward issues and may result in poorly learned policies (Takan"
2020.acl-main.62,W15-4655,0,0.0554714,"Missing"
2020.acl-main.62,D19-1010,0,0.0706838,"2018; Zhang et al., 2019). Specifically, dialogue policy is learned by maximizing accumulated rewards over interactions with an environment (i.e., actual users or a user simulator). Handcrafted rewards are commonly used for policy learning in earlier work (Peng et al., 2018), which assigns a small negative penalty at each turn and a large positive/negative reward when the task is successful/failed. However, such reward setting does not provide sufficient supervision signals in each turn other than the last turn, which causes the sparse reward issues and may result in poorly learned policies (Takanobu et al., 2019). To address this problem, reward function learning that relies on expert demonstrations has been introduced (Takanobu et al., 2019; Li et al., 2019b). Specifically, state-action sequences generated by an optimal policy (i.e., expert demonstrations) are collected, and a reward function is learned to give high rewards to state-action pairs that better resemble the behaviors of the optimal policy. In this way, turn-by-turn rewards estimated by the reward function can be provided to learn dialogue policy. Obtaining expert demonstrations is critical to reward function learning. Since it is impract"
2020.acl-main.62,W17-5509,0,0.0487308,"Missing"
2020.acl-main.62,P19-1364,0,0.0302705,"on Task-oriented dialogue systems complete tasks for users, such as making a restaurant reservation or finding attractions to visit, in multi-turn dialogues (Gao et al., 2018; Sun et al., 2016, 2017). Dialogue policy is a critical component in both the conventional pipeline approach (Young et al., 2013) and recent end-to-end approaches (Zhao et al., 2019). It decides the next action that a dialogue system should take at each turn. Considering its nature of sequential decision making, dialogue policy is usually learned via reinforcement learning (Su et al., ∗ Utterance 2015; Peng et al., 2018; Zhang et al., 2019). Specifically, dialogue policy is learned by maximizing accumulated rewards over interactions with an environment (i.e., actual users or a user simulator). Handcrafted rewards are commonly used for policy learning in earlier work (Peng et al., 2018), which assigns a small negative penalty at each turn and a large positive/negative reward when the task is successful/failed. However, such reward setting does not provide sufficient supervision signals in each turn other than the last turn, which causes the sparse reward issues and may result in poorly learned policies (Takanobu et al., 2019). To"
2020.acl-main.62,N19-1123,0,0.0358657,"e brasserie. does either of them sound good for you? System action annotation restaurant-inform:{name=de luca cucina, name=riverside brasserie} Introduction Rui Zhang is the corresponding author. I would like moderate price range please. Dialogue State annotation Task-oriented dialogue systems complete tasks for users, such as making a restaurant reservation or finding attractions to visit, in multi-turn dialogues (Gao et al., 2018; Sun et al., 2016, 2017). Dialogue policy is a critical component in both the conventional pipeline approach (Young et al., 2013) and recent end-to-end approaches (Zhao et al., 2019). It decides the next action that a dialogue system should take at each turn. Considering its nature of sequential decision making, dialogue policy is usually learned via reinforcement learning (Su et al., ∗ Utterance 2015; Peng et al., 2018; Zhang et al., 2019). Specifically, dialogue policy is learned by maximizing accumulated rewards over interactions with an environment (i.e., actual users or a user simulator). Handcrafted rewards are commonly used for policy learning in earlier work (Peng et al., 2018), which assigns a small negative penalty at each turn and a large positive/negative rewa"
2020.acl-main.706,D19-1052,0,0.0451164,"Missing"
2020.acl-main.706,2020.acl-main.708,0,0.20003,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,2020.acl-main.18,0,0.0282211,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,P17-1025,0,0.0299591,"for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve Commonse"
2020.acl-main.706,W18-6505,0,0.0214463,"Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained"
2020.acl-main.706,P16-1154,0,0.0267129,"sts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be men"
2020.acl-main.706,P16-1014,0,0.0262209,", 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generate"
2020.acl-main.706,P16-1195,0,0.0197135,"generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale"
2020.acl-main.706,P04-1050,0,0.0480877,"Missing"
2020.acl-main.706,N12-1093,0,0.0316404,"e to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have show"
2020.acl-main.706,D16-1128,0,0.0238773,"ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 20"
2020.acl-main.706,D16-1011,0,0.0343869,"which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a"
2020.acl-main.706,P19-1195,0,0.115119,"f variable diameters) in the simulator to achieve a given goal. Figure 1 shows an example of a task with a specified goal. The input to ESPRIT is a sequence of frames from a physics simulation and the output is a natural language narrative that reflects the locations of the objects in the initial scene and a description of the sequence of physical events that would lead to the desired goal state, as shown in Figure 2. The first phase of the framework uses a neural network classifier to identify salient frames from the simulation. For the second phase we experimented with table-to-text models (Puduppully et al., 2019a,b) as well as pre-trained language models (Radford et al., 2018). We evaluated our framework for natural language generated reasoning using several automated and human evaluations with a focus on the understanding of qualitative physics and the ordering of a natural sequence of physical events. We found that our model achieves very high performance for phase one (identifying frames with salient physical events) and that, for phase two, the table-to-text models outperform pre-trained language models on qualitative physics reasoning. 2 2.1 Dataset PHYRE Benchmark We build our dataset by extend"
2020.acl-main.706,P09-1011,0,0.0603867,"et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulceh"
2020.acl-main.706,P19-1487,1,0.734658,"(Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a questi"
2020.acl-main.706,D15-1166,0,0.0153733,"; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that"
2020.acl-main.706,N16-1086,0,0.015485,"ing task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results"
2020.acl-main.706,W00-1418,0,0.219091,"explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While tra"
2020.acl-main.706,N18-1137,0,0.0145506,"entifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al.,"
2020.acl-main.706,P17-2034,0,0.0279544,"hang et al., 2016). Recent papers use neural networks over visual inputs to predict future pixels (Finn et al., 2016; Lerer et al., 2016; Mirza et al., 2016; Du and Narasimhan, 2019) or make qualitative predictions (Groth et al., 2018; Li et al., 2016, 2017; Janner et al., 2019; Wu et al., 2015; Mao et al., 2019). Furthermore, several frameworks and benchmarks have been introduced to test visual reasoning such as PHYRE (Bakhtin et al., 2019), Mujoco (Todorov et al., 2012), and Intphys (Riochet et al., 2018), some of which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al.,"
2020.acl-main.706,P98-2209,0,0.135936,"ining and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention ("
2020.acl-main.706,P18-1151,1,0.861331,"We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use di"
2020.acl-main.706,D15-1199,0,0.0419796,"Missing"
2020.acl-main.706,D17-1239,0,0.0932855,"alls fall. The red ball lands on the ground and the green ball lands on the red ball and rolls to the right over the black vertical bar. Generation (AVG) The red ball lands in the cubby and the green ball lands on top and a little to the right, sending the green ball right. It rolls over the short black wall of the cage and onto the floor, where it keeps rolling right towards the purple goal... The red ball falls and knocks the green ball off of its curved black platform and to the left. It rolls leftwards and continues falling until it lands on the purple floor... Generation (BiLSTM) maries (Wiseman et al., 2017). Second, the output generated by the BiLSTM model predicts the incorrect direction of motion for the green ball, an error that is occasionally seen across generation descriptions of both models. This indicates that a table-to-text paradigm for generating such solution explanations is not adequate for learning the direction of motion for the physical reasoning required for these explanations. Table 6: Example input records, gold annotation, and generated simulation description from the AVG and BiLSTM models, taken from example 00014:394. We show only a short segment of the actual input records"
2020.acl-main.706,2020.acl-main.224,0,0.0218279,"iptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions"
2020.acl-main.706,D14-1179,0,\N,Missing
2020.acl-main.706,P19-1202,0,\N,Missing
2020.acl-main.706,D19-1204,1,\N,Missing
2020.acl-main.706,C98-2204,0,\N,Missing
2020.clssts-1.3,P19-3004,0,0.0613604,"Missing"
2020.clssts-1.3,1998.amta-tutorials.5,0,0.165633,"on Retrieval (CLIR) has repeatedly been a casualty of its own success. Research in the 1970’s focused on extending monolingual thesauri to multilingual thesauri. Although there were some issues to address involving the ways conceptual differences were reflected in different cultures (and thus in different languages), the thesaurus-based retrieval systems of the day proved to be relatively easily extended to include entry vocabulary from different languages. Thus, after publication of an ISO multilingual thesaurus standard in 1986 there was little further research left to do along those lines (Oard and Diekema, 1998). The 1990’s saw the rapid development of a different paradigm for CLIR, one in which queries were expressed in natural language and the system’s goal was to rank, not to select, documents. Much of the initial work focused on dictionary-based techniques and on techniques based on comparable corpora, but it was the introduction of techniques based on parallel text around the turn of the century that essentially solved the crosslanguage ranking problem (Nie, 2010). Of course, ranking is only useful in interactive applications if the searcher can recognize relevant documents, so success with cros"
2020.clssts-1.3,2020.clssts-1.2,0,0.0409311,"amount of Social Media/Blog content. Similarly, the amounts of News Broadcast and Topical Broadcast recordings are similar, and about two times larger than the amount of Conversational Speech. Experiments Corpus Description The IARPA MATERIAL corpus currently consists of document collections in six languages: Swahili, Tagalog, Somali, Lithuanian, Bulgarian, and Pashto. Our analysis is based on the Lithuanian collection, for which we have results from all three participating teams. Collection statistics are given in Table 1. Details of the collection and the annotation process can be found in (Zavorin et al., 2020). 3.2. Official Results We refer to the three participating systems as Teams A, B, and C to preserve anonymity. A comparison of scores for each team from the October 2019 evaluation is shown in Table 3. AQWV is the official program measure (NIST, 2016). Although the program objective is set-based retrieval, documents returned by each team also have a confidence score that can be used as a basis for ranking. This enables us to compute Mean Average Precision (MAP) on the returned list of documents, although we note that different systems return different numbers of relevant documents so the MAP"
2020.clssts-1.3,D19-6129,0,0.0130655,"ents can be translated into English, or queries and documents can be transformed into some other shared space (e.g., using embeddings). Evidence from multiple systems can also be combined by a variety of methods. Available data sources can be combined before retrieval, evidence from different systems can be combined during the matching phase, or the documents retrieved by different systems can be combined after the matching phase. Details on the approaches used by the SARAL team are described in (Boschee et al., 2019), the approaches used by the FLAIR team are described in (Zbib et al., 2019; Zhao et al., 2019), and the approaches used by the SCRIPTS team are described in (Oard et al., 2019). 3. 3.1. Document Genres The corpus contains both text documents and speech recordings, which can be further subdivided by the source. There are a total of 10,203 text documents and 3,297 speech recordings, each modality being broken into 3 different genres. Documents (a term used inclusively in MATERIAL to refer to both text documents and speech recordings) are thus provided in six genres (NIST, 2016): 1. News Text (Text) - newswire or reports. Formal language. 2. Topical Text (Text) - specialty articles or rep"
2020.emnlp-main.147,D18-1547,0,0.15152,"Missing"
2020.emnlp-main.147,P19-1360,0,0.041004,"has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models are proposed (Zhao et al., 2017; Serban et al., 2016). They formulate the response generation problem as a translation task and apply sequence-to-sequence (Seq2Seq) models to generat"
2020.emnlp-main.147,N19-1423,0,0.0423041,"Missing"
2020.emnlp-main.147,W17-5506,0,0.342015,"nment and adaption to new domains. Another popular approach is the end-to-end models (Serban et al., 2016; Wen et al., 2017; Williams et al., 2017; Zhao et al., 2017; Serban et al., 2017), which directly map the dialogue history to the output responses. This approach has attracted more attention in the research community recently as it alleviates the drawbacks of the pipeline approach. However, end-to-end dialogue models usually suffer from ineffective use of knowledge bases due to the lack of appropriate framework to handle KB data. To mitigate this issue, recent end-to-end dialogue studies (Eric et al., 2017; Madotto et al., 2018) employ memory networks (Weston et al., 2015; Sukhbaatar et al., 2015) to support the learning over KB, and have achieved promising results via integrating memory with copy mechanisms (Gulcehre et al., 2016; Eric and Manning, 2017). By using memory, they assume that the underlying structure of KB is linear since memory can be viewed as a 1878 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1878–1888, c November 16–20, 2020. 2020 Association for Computational Linguistics list structure. As a result, the relationships between e"
2020.emnlp-main.147,E17-2075,0,0.0827301,"nses. This approach has attracted more attention in the research community recently as it alleviates the drawbacks of the pipeline approach. However, end-to-end dialogue models usually suffer from ineffective use of knowledge bases due to the lack of appropriate framework to handle KB data. To mitigate this issue, recent end-to-end dialogue studies (Eric et al., 2017; Madotto et al., 2018) employ memory networks (Weston et al., 2015; Sukhbaatar et al., 2015) to support the learning over KB, and have achieved promising results via integrating memory with copy mechanisms (Gulcehre et al., 2016; Eric and Manning, 2017). By using memory, they assume that the underlying structure of KB is linear since memory can be viewed as a 1878 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1878–1888, c November 16–20, 2020. 2020 Association for Computational Linguistics list structure. As a result, the relationships between entities are not captured. However, since KB is naturally a graph structure (nodes are entities and edges are relations between entities). By overlooking such relationships, the model fails to capture substantial information embedded in the KB including t"
2020.emnlp-main.147,P16-1014,0,0.0307633,"ory to the output responses. This approach has attracted more attention in the research community recently as it alleviates the drawbacks of the pipeline approach. However, end-to-end dialogue models usually suffer from ineffective use of knowledge bases due to the lack of appropriate framework to handle KB data. To mitigate this issue, recent end-to-end dialogue studies (Eric et al., 2017; Madotto et al., 2018) employ memory networks (Weston et al., 2015; Sukhbaatar et al., 2015) to support the learning over KB, and have achieved promising results via integrating memory with copy mechanisms (Gulcehre et al., 2016; Eric and Manning, 2017). By using memory, they assume that the underlying structure of KB is linear since memory can be viewed as a 1878 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1878–1888, c November 16–20, 2020. 2020 Association for Computational Linguistics list structure. As a result, the relationships between entities are not captured. However, since KB is naturally a graph structure (nodes are entities and edges are relations between entities). By overlooking such relationships, the model fails to capture substantial information embed"
2020.emnlp-main.147,2020.acl-main.62,1,0.464976,"o taskoriented dialogue systems. • We further propose a novel recurrent cell architecture to exploit the graph structural information in the dialogue history. We also combine the multi-hop reasoning ability with graph to exploit the relationships between entities in the KB. • We evaluate the proposed model on two realworld task-oriented dialogue datasets (i.e., SMD and MultiWOZ 2.1). The results show that our model outperforms the state-of-theart models consistently. 2 Related Work Task-oriented dialogue system has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al.,"
2020.emnlp-main.147,W16-3602,0,0.0284956,"iented dialogue datasets (i.e., SMD and MultiWOZ 2.1). The results show that our model outperforms the state-of-theart models consistently. 2 Related Work Task-oriented dialogue system has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models"
2020.emnlp-main.147,P18-1133,0,0.177239,"can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models are proposed (Zhao et al., 2017; Serban et al., 2016). They formulate the response generation problem as a translation task and apply sequence-to-sequence (Seq2Seq) models to generate responses. Seq2Seq models have shown to be effective in language modeling but they struggle to incorporate"
2020.emnlp-main.147,D15-1166,0,0.142339,"Missing"
2020.emnlp-main.147,P18-1136,0,0.37716,"to new domains. Another popular approach is the end-to-end models (Serban et al., 2016; Wen et al., 2017; Williams et al., 2017; Zhao et al., 2017; Serban et al., 2017), which directly map the dialogue history to the output responses. This approach has attracted more attention in the research community recently as it alleviates the drawbacks of the pipeline approach. However, end-to-end dialogue models usually suffer from ineffective use of knowledge bases due to the lack of appropriate framework to handle KB data. To mitigate this issue, recent end-to-end dialogue studies (Eric et al., 2017; Madotto et al., 2018) employ memory networks (Weston et al., 2015; Sukhbaatar et al., 2015) to support the learning over KB, and have achieved promising results via integrating memory with copy mechanisms (Gulcehre et al., 2016; Eric and Manning, 2017). By using memory, they assume that the underlying structure of KB is linear since memory can be viewed as a 1878 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1878–1888, c November 16–20, 2020. 2020 Association for Computational Linguistics list structure. As a result, the relationships between entities are not capture"
2020.emnlp-main.147,P02-1040,0,0.113397,"ted between [16,512], which is also equivalent to the RNN hidden state (including the encoder and the decoder). We also use dropout for regularization on both the encoder and the decoder to avoid over-fitting and the dropout rate is set between [0.1,0.5]. We use Adam optimizer (Kingma and Ba, 2015) to accelerate the convergence with a learning rate chosen between [1e−3 ,1e−4 ]. We simply use a greedy strategy to search for the target word in the decoder without advanced techniques like beam-search. 4.3 Evaluation Metrics We use two common evaluation metrics in dialogue studies including BLEU (Papineni et al., 2002) (using Moses multi-bleu.perl script) and Entity F1 (Eric et al., 2017; Madotto et al., 2018) for evaluations. 4.4 Effect of Models We compare our model with several existing models: standard sequence-to-sequence (Seq2Seq) models with and without attention (Luong et al., 2015), pointer to unknown (Ptr-Unk, (Gulcehre et al., 2016)), GraphLSTM (Peng et al., 2017), BERT (Devlin et al., 2019), Mem2Seq (Madotto et al., 2018) and GLMP (Wu et al., 2019b). Note that the results we listed in Table 2 for GLMP is different from the original paper, since we reimplement their model in Tensorflow according"
2020.emnlp-main.147,P18-1203,0,0.0206641,"orms the state-of-theart models consistently. 2 Related Work Task-oriented dialogue system has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models are proposed (Zhao et al., 2017; Serban et al., 2016). They formulate the response generatio"
2020.emnlp-main.147,Q17-1008,0,0.0282391,"vember 16–20, 2020. 2020 Association for Computational Linguistics list structure. As a result, the relationships between entities are not captured. However, since KB is naturally a graph structure (nodes are entities and edges are relations between entities). By overlooking such relationships, the model fails to capture substantial information embedded in the KB including the semantics of the entities which may significantly impact the accuracy of results. Moreover, structural knowledge such as dependency relationships has recently been investigated on some tasks (e.g., relation extraction) (Peng et al., 2017; Song et al., 2018) and shown to be effective in the model’s generalizability. However, such dependency relationships (essentially also graph structure) have not been explored in dialogue systems, again missing great potential for improvements. With the above insight, we propose a novel graph-based end-to-end task-oriented dialogue model (GraphDialog) aimed to exploit the graph knowledge both in dialogue history and KBs. Unlike traditional RNNs such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014), we design a novel recurrent unit (Section 3.1.2) that allows multiple hidd"
2020.emnlp-main.147,D18-1246,0,0.024715,"2020 Association for Computational Linguistics list structure. As a result, the relationships between entities are not captured. However, since KB is naturally a graph structure (nodes are entities and edges are relations between entities). By overlooking such relationships, the model fails to capture substantial information embedded in the KB including the semantics of the entities which may significantly impact the accuracy of results. Moreover, structural knowledge such as dependency relationships has recently been investigated on some tasks (e.g., relation extraction) (Peng et al., 2017; Song et al., 2018) and shown to be effective in the model’s generalizability. However, such dependency relationships (essentially also graph structure) have not been explored in dialogue systems, again missing great potential for improvements. With the above insight, we propose a novel graph-based end-to-end task-oriented dialogue model (GraphDialog) aimed to exploit the graph knowledge both in dialogue history and KBs. Unlike traditional RNNs such as LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014), we design a novel recurrent unit (Section 3.1.2) that allows multiple hidden states as inputs"
2020.emnlp-main.147,P16-1230,0,0.0617693,"Missing"
2020.emnlp-main.147,D18-1416,0,0.0234109,"heart models consistently. 2 Related Work Task-oriented dialogue system has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models are proposed (Zhao et al., 2017; Serban et al., 2016). They formulate the response generation problem as a tra"
2020.emnlp-main.147,E17-1042,0,0.0874905,"Missing"
2020.emnlp-main.147,P17-1062,0,0.031744,"he dialogue history, and to retrieve relevant information from the KB is essential in task-oriented dialogue systems. One approach for designing task-oriented dialogue systems is the pipeline approach (Williams Rui Zhang is the corresponding author. Food Efes Restaurant Customer: I’m looking for a moderately priced Polish restaurant. Introduction ∗ 106 Regent Street and Young, 2007; Lee et al., 2009; Young et al., 2013), but it suffers from the difficulty in credit assignment and adaption to new domains. Another popular approach is the end-to-end models (Serban et al., 2016; Wen et al., 2017; Williams et al., 2017; Zhao et al., 2017; Serban et al., 2017), which directly map the dialogue history to the output responses. This approach has attracted more attention in the research community recently as it alleviates the drawbacks of the pipeline approach. However, end-to-end dialogue models usually suffer from ineffective use of knowledge bases due to the lack of appropriate framework to handle KB data. To mitigate this issue, recent end-to-end dialogue studies (Eric et al., 2017; Madotto et al., 2018) employ memory networks (Weston et al., 2015; Sukhbaatar et al., 2015) to support the learning over KB, an"
2020.emnlp-main.147,P19-1078,0,0.114066,"ultiWOZ 2.1). The results show that our model outperforms the state-of-theart models consistently. 2 Related Work Task-oriented dialogue system has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models are proposed (Zhao et al., 2017; Serb"
2020.emnlp-main.147,W17-5505,0,0.281978,"d to retrieve relevant information from the KB is essential in task-oriented dialogue systems. One approach for designing task-oriented dialogue systems is the pipeline approach (Williams Rui Zhang is the corresponding author. Food Efes Restaurant Customer: I’m looking for a moderately priced Polish restaurant. Introduction ∗ 106 Regent Street and Young, 2007; Lee et al., 2009; Young et al., 2013), but it suffers from the difficulty in credit assignment and adaption to new domains. Another popular approach is the end-to-end models (Serban et al., 2016; Wen et al., 2017; Williams et al., 2017; Zhao et al., 2017; Serban et al., 2017), which directly map the dialogue history to the output responses. This approach has attracted more attention in the research community recently as it alleviates the drawbacks of the pipeline approach. However, end-to-end dialogue models usually suffer from ineffective use of knowledge bases due to the lack of appropriate framework to handle KB data. To mitigate this issue, recent end-to-end dialogue studies (Eric et al., 2017; Madotto et al., 2018) employ memory networks (Weston et al., 2015; Sukhbaatar et al., 2015) to support the learning over KB, and have achieved pro"
2020.emnlp-main.147,P18-1135,0,0.0216453,"ets (i.e., SMD and MultiWOZ 2.1). The results show that our model outperforms the state-of-theart models consistently. 2 Related Work Task-oriented dialogue system has been a longstanding studied topic (Williams and Young, 2007; Lee et al., 2009; Huang et al., 2020b) and can be integrated into many practical applications such as virtual assistant (Sun et al., 2016, 2017). Traditionally, task-oriented dialogue systems are built in the pipeline approach, which consists of four essential components: natural language understanding (Chen et al., 2016), dialogue state tracking (Lee and Stent, 2016; Zhong et al., 2018; Wu et al., 2019a), policy learning (Su et al., 2016; Peng et al., 2018; Su et al., 2018) and natural language generation (Sharma et al., 2017; Chen et al., 2019; Huang et al., 2020a). Another recent approach is the end-to-end models (Wu et al., 2018; Lei et al., 2018), which directly map the user utterances to responses without heavy annotations. Bordes et al. (2017) apply endto-end memory networks (Sukhbaatar et al., 2015) for task-oriented dialogues and shown that end-toend models are promising on the tasks. To produce more flexible responses, several generative models are proposed (Zhao e"
2020.findings-emnlp.355,P19-1080,0,0.0232941,"‘request’, ‘departure’} Generated Response i have train leaving after 10:00, where would you like to depart? User Utterance is there a restaurant in the center serving italian then ? Ground-truth Response there are several italian restaurants downtown , would you like me to pick for you ? Selected Action {‘inform’, ‘food’, ‘area’ } Generated Response there are several italian restaurants , do you have a preference to the area ? * The parts highlighted in bold is the error of either content planning or language generation. fine-grained actions (Shu et al., 2019), or encoding syntax attributes (Balakrishnan et al., 2019). Since these approaches often assume expensive action annotations, recent years have seen a growing interest in learning latent actions in an unsupervised way (Zhao et al., 2019; Huang et al., 2020a). These approaches build on either adversarial learning (Hu et al., 2017; Wang et al., 2018; Yang et al., 2018) or variational inference (Kingma and Welling, 2014) and encode all system utterances via a self-reconstruction task or distant supervision (Yarats and Lewis, 2018). Due to their implicit nature, latent actions are difficult to generalize, and we aim to overcome this limitation by learnin"
2020.findings-emnlp.355,D18-1547,0,0.265903,"operty of capturing the intentions of system utterances in the latent space cannot be enforced (Locatello 3981 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3981–3991 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2019), which in turn is due to the implicit nature of latent variables. For example, variational auto-encoder (VAE), which is often used for latent action learning, tends to produce a balanced distribution over the latent variables (Zhao et al., 2018), while the true distribution of system actions is highly imbalanced (Budzianowski et al., 2018). The resulting misaligned action representations would confuse the model of both steps and degenerate the sample efficiency in training. To address the above issues, we propose to learn natural language actions that represent system utterances as a span of words, which explicitly reveal the underlying intentions. Natural language provides unique compositional structure while retaining the representation flexibility. These properties promote model generalization and thus make natural language a flexible representation for capturing characteristics with minimal assumptions (Jiang et al., 2019)."
2020.findings-emnlp.355,P19-1360,0,0.0777922,"ation Task-oriented dialogue systems complete tasks for users, such as making a hotel reservation or finding train routes, in a multi-turn conversation (Gao et al., 2018; Sun et al., 2016, 2017). The generated system utterances should not only be naturally sound, but more importantly be informative, i.e., to proceed the dialogue towards task completion. To fulfill this requirement, conditioned response generation is widely adopted based on system actions ∗ Utterance {‘option’, ‘five’ ‘request’,‘time’ } alright I found five options available. when would you like to leave by? (Wen et al., 2017; Chen et al., 2019). The response generation process is decoupled into two consecutive steps, where an action is first selected and then an utterance is generated conditioned on this action. One can optimize each step towards its goal, i.e., informative and naturally sound, without impinging the other (Yarats and Lewis, 2018). However, such approaches rely on action annotations (as in Table 1), which require domain knowledge and extensive efforts to obtain. To deal with the absence of action annotations, latent action learning has been introduced (Zhao et al., 2018; Yarats and Lewis, 2018). System utterances are"
2020.findings-emnlp.355,P18-5002,0,0.0624582,"Missing"
2020.findings-emnlp.355,2020.acl-main.62,1,0.622068,"hat the task concerns can be preserved by just the salient words. For example, the sentiment of sentence “The movie starts out as competent but turn bland” can be revealed by the word “bland” when it is identified salient by considering the complete context. In our scenarios, we consider measuring word saliency in terms of state transitions. This is because state transitions reflect how the intentions of a system utterance influence the dialogue progress, and action representations that capture such influences can well reveal the intentions (Chandak et al., 2019; Tennenholtz and Mannor, 2019; Huang et al., 2020b). By considering salient words for state tracking tasks as actions, we obtain action representations that enjoy the merits of natural language and indeed capture the characteristics of interest, i.e., intentions of system utterances. Obtaining salient words by applying existing saliency identification approaches (Ribeiro et al., 2018) is, however, unable to produce unified action representations. Specifically, system utterances with the same intention might not share similar wordings, and existing attribution approaches can only identify salient words within utterances. We tackle this challe"
2020.findings-emnlp.355,P19-1546,0,0.0211446,"te for each turn, where bt ∈ {0, 1}Nb and Nb is the number of all slot-value pairs. Dialogue state tracking is usually formulated as a multilabel learning problem where the state at turn t predicted by modeling the conditional distribution p(bt |ct ) = p(bt |ut , xt−1 , bt−1 ), where bt−1 is the dialogue state in the previous turn. To model this conditional distribution, a state tracking model pB (ut , xt−1 , bt−1 ) mainly employs an utterance en3983 coder, a context encoder to work with a slot-value predictor that estimates whether a slot-value pair should be included in the dialogue states (Lee et al., 2019). Specifically, the predictor takes as input a slot-value pair (si , ei ), and the encoded utterances hutt ∈ RD and context hctx ∈ RD from the utterance encoder futt (ut , xt−1 ) and context encoder fctx (bt−1 ) respectively, and D is the hidden dimension. The prediction is then performed by aggregating the results of slot-value predictor fval (hutt , hctx , (si , ei )) for the complete Nb slotvalue pairs. We optimize the state tracking model using the cross-entropy loss: X X L= − log(b&gt; t ·pB (ut , xt−1 , ct−1 )) (4) di t=1:nd where the parameters of pB , which include futt , fctx , and fval"
2020.findings-emnlp.355,P18-1133,0,0.0532458,"Missing"
2020.findings-emnlp.355,P02-1040,0,0.107112,"Missing"
2020.findings-emnlp.355,D19-1130,0,0.037289,"Missing"
2020.findings-emnlp.355,P18-1101,0,0.0673412,"would you like to leave by? (Wen et al., 2017; Chen et al., 2019). The response generation process is decoupled into two consecutive steps, where an action is first selected and then an utterance is generated conditioned on this action. One can optimize each step towards its goal, i.e., informative and naturally sound, without impinging the other (Yarats and Lewis, 2018). However, such approaches rely on action annotations (as in Table 1), which require domain knowledge and extensive efforts to obtain. To deal with the absence of action annotations, latent action learning has been introduced (Zhao et al., 2018; Yarats and Lewis, 2018). System utterances are represented as low-dimensional latent variables by an auto-encoding task (Zhao et al., 2019), and utterances with the same representations are considered to convey similar meanings. Such action representations might be prone to overdependence on the training data, which restricts the model generalization capability, especially when multiple domains are considered. This is because, without explicit supervision, the desired property of capturing the intentions of system utterances in the latent space cannot be enforced (Locatello 3981 Findings of"
2020.findings-emnlp.355,N19-1123,0,0.409395,"where an action is first selected and then an utterance is generated conditioned on this action. One can optimize each step towards its goal, i.e., informative and naturally sound, without impinging the other (Yarats and Lewis, 2018). However, such approaches rely on action annotations (as in Table 1), which require domain knowledge and extensive efforts to obtain. To deal with the absence of action annotations, latent action learning has been introduced (Zhao et al., 2018; Yarats and Lewis, 2018). System utterances are represented as low-dimensional latent variables by an auto-encoding task (Zhao et al., 2019), and utterances with the same representations are considered to convey similar meanings. Such action representations might be prone to overdependence on the training data, which restricts the model generalization capability, especially when multiple domains are considered. This is because, without explicit supervision, the desired property of capturing the intentions of system utterances in the latent space cannot be enforced (Locatello 3981 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3981–3991 c November 16 - 20, 2020. 2020 Association for Computational Lingu"
2020.findings-emnlp.355,P17-1061,0,0.0598783,"fluent utterances, this fine-tuning stage focuses on the content planning model pl (a|c) and keeps the parameters of pr (x|a, c) fixed. The reward Rt at each turn is back-propagated via policy gradients as: X ∇φ J (φ) = Rt · ∇φ log pl (a|ct ) (3) t=1:nd where φ denotes the parameters of model pl . In order to enable conditioned response generation when action annotations are absent, latent action learning is introduced. Given dialogues {(ct , xt )|1 ≤ t ≤ nd }, latent action learning aims to map each utterance to a latent representation zd (x), e.g, one-hot (Wen et al., 2017), or continuous (Zhao et al., 2017). Based on the obtained (ct , zd (xt ), xt ) triples, conditioned response generation is run as mentioned above. Existing latent action learning approaches mostly build on the idea of variational inference, where a latent space is found to reconstruct system utterances and thus encodes the main characteristics of utterances (Zhao et al., 2018; Huang et al., 2020a). The action representations learned from the latent space are, however, difficult to generalize due to the implicit nature and thus cause the sample inefficiency issue. 3 3.1 Proposed Model Overview We study the problem of natural la"
2020.findings-emnlp.355,2020.emnlp-main.147,1,0.763093,"Missing"
2021.acl-long.300,D19-1352,0,0.0174858,"26 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tokens (Somali, Swahili, Tagalog) in the parallel corpus. Lang. Pair Augmented Dataset Size EN-SO EN-SW EN-TL 1,649,484 2,014,838 2,417,448 Table 2: Augmented dataset statistics; “augmented dataset size” refers to total number of positive and negative query-sentence samples in the augmented dataset. bel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019). Given a document D = [S1 , . . . , S|D |], which is a sequence of sentences, and a query Q, following Liu and Croft (2002) we assign a relevance score by: rˆ = max p(r = 1|Q, S; W ) S∈D 4.3 We initialize English word embeddings with word2vec (Mikolov et al., 2013), and initialize SO/SW/TL word embeddings with FastText (Grave et al., 2018). For training we use a SparseAdam (Kingma and Ba, 2015) optimizer with learning rate 0.001. The hyperparameter λ2 in Section 3.3 is set to be 3 so that Lrel and λ2 Lrat are approximately on the same scale during training. More details on experiments are inc"
2021.acl-long.300,P18-1073,0,0.0160745,". In other work, Xu and Weischedel (2000) use a 2-state hidden Markov model (HMM) to estimate the probability that a passage is relevant given the query. Cross-lingual Word Embeddings Crosslingual embedding methods perform cross-lingual relevance prediction by representing query and passage terms of different languages in a shared semantic space (Vuli´c and Moens, 2015; Litschko et al., 2019, 2018; Joulin et al., 2018). Both supervised approaches trained on parallel sentence corpora (Levy et al., 2017; Luong et al., 2015) and unsupervised approaches with no parallel data (Lample et al., 2018; Artetxe et al., 2018) have been proposed to train cross-lingual word embeddings. Our approach differs from previous cross-lingual word embedding methods in two aspects. First, the focus of previous work has mostly been on learning a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work. Second, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approa"
2021.acl-long.300,D18-1216,0,0.0231306,"provide detailed comparisons of performance with other sentence selection approaches. Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat3882 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016). 3 Methods We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT. E 0 are in the same language, so checking whether q or a synon"
2021.acl-long.300,P19-3004,0,0.0145662,"a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work. Second, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approach trains embeddings on an artificial labeled dataset augmented from a parallel corpus and directly represents relevance across languages. Our data augmentation scheme to build a relevance model is inspired by Boschee et al. (2019), but we achieve significant performance improvement by incorporating rationale information into the embedding training process and provide detailed comparisons of performance with other sentence selection approaches. Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorpor"
2021.acl-long.300,P17-1171,0,0.022581,"’s applicability to even lower-resource settings and mitigation of hubness issues (Dinu and Baroni, 2015; Radovanovi´c et al., 2010). These findings are validated by empirical results of experiments in a low-resource sentence selection task, with English queries over sentence collections of text and speech in Somali, Swahili, and Tagalog. 2 Related Work Query-focused Sentence Selection Sentencelevel query relevance prediction is important for various downstream NLP tasks such as queryfocused summarization (Baumel et al., 2016, 2018; Feigenblat et al., 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018). Such applications often depend on a sentence selection system to provide attention signals on which sentences to focus upon to generate a query-focused summary or answer a question. Cross-language Sentence Selection A common approach to cross-language sentence selection is to use MT to first translate either the query or the sentence to the same language and then perform standard monolingual IR (Nie, 2010). The risk of this approach is that errors in translation cascade to the IR system. As an alternative to generating full translations, PSQ (Darwish"
2021.acl-long.300,2016.amta-researchers.10,0,0.0221643,"do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat3882 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016). 3 Methods We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT. E 0 are in the same language, so checking whether q or a synonym can be found in E 0 is a monolingual task. If we can verify that there is no direct match or synonym equivalent of q in E 0 then by transitivity it is unlikely there exists a translat"
2021.acl-long.300,2020.acl-main.747,0,0.0940514,"Missing"
2021.acl-long.300,N19-1423,0,0.0174726,"T) and speech (S) for Tagalog. data as our SECLR models. The 1-best output from each MT system is then scored with Indri (Strohman et al., 2005) to obtain relevance scores. Details of NMT and SMT systems are included in Appendix C.2. PSQ. To implement the PSQ model of Darwish and Oard (2003), we use the same alignment matrix as in rationale training (see Section 3.3) exMultilingual XLM-RoBERTa. We compare our model to the cross-lingual model XLM-RoBERTa (Conneau et al., 2020), which in previous research has been shown to have better performance on lowresource languages than multilingual BERT (Devlin et al., 2019). We use the Hugging Face implementation (Wolf et al., 2019) of XLM-RoBERTa (Base). We fine-tuned the model on the same augmented dataset of labeled query-sentence pairs as the SECLR models, but we apply the XLMRoBERTa tokenizer before feeding examples to the model. We fine-tuned the model for four epochs using an AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 2 × 10−5 . Since XLMRoBERTa is pretrained on Somali and Swahili but not Tagalog, we only compare our models to XLMRoBERTa on Somali and Swahili. 3886 5 Results and Discussion 6 We report Mean Average Precision (MAP) of"
2021.acl-long.300,W11-2123,0,0.217567,"l corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space. While our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired"
2021.acl-long.300,N19-1357,0,0.104894,"rnational Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevance (SECLR) model trained on this data for low-resource sentence selection tasks on text and speech. Additionally, (iii) we propose a rationale training secondary objective to further improve SECLR performance, which we call SECLR-RT. Finally, (iv) we conduct training data ablation and h"
2021.acl-long.300,D18-1330,0,0.0676993,"Missing"
2021.acl-long.300,W18-6478,0,0.0350646,"Missing"
2021.acl-long.300,kamholz-etal-2014-panlex,0,0.0210779,"none of the translations of the query in the matrix are present in the source sentence. 4 Experiments 4.1 Dataset Generation from Parallel Corpus The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A. 1 https://www.iarpa.gov/index.php/ research-programs/material 2 https://dumps.wikimedia.org/ 3884 # sents. EN tkn. LR tkn. EN-SO EN-SW EN-TL 69,818 1,827,826 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tokens (Somali, Swahili, Tagalog) in the parallel corpus."
2021.acl-long.300,P07-2045,0,0.0263233,"these noisy parallel corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space. While our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale tr"
2021.acl-long.300,E17-1072,0,0.303623,"allel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and"
2021.acl-long.300,W15-1521,0,0.157957,"MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with bin"
2021.acl-long.300,J03-1002,0,0.0454723,"n task: Lrel = − log p(r|q, S; W ) 3.3 Guided Alignment with Rationale Training We can improve SECLR by incorporating additional alignment information as a secondary training objective, yielding SECLR-RT. Our intuition is that after training, the word sˆ = arg maxs∈S ws |wq should correspond to a translation of q. However, it is possible that sˆ simply co-occurs frequently with the true translation in our parallel data but its association is coincidental or irrelevant outside the training contexts. We use alignment information to correct for this. We run two SMT word alignment models, GIZA++ (Och and Ney, 2003) and Berkeley Aligner (Haghighi et al., 2009), on the orginal parallel sentence corpus. The two resulting alignments are concatenated as in Zbib et al. (2019) to estimate a unidirectional probabilistic word translation matrix A ∈ [0, 1]|VQ |×|VS |, such that A maps each word in the query language vocabulary to a list of document language words with different probabilities, i.e. P Aq,s is the probability of translating q to s and s∈VS Aq,s = 1. For each relevant training sample, i.e. (q, S, r = 1), we create a rationale distribution ρ ∈ [0, 1]|S| αs = P for s ∈ S. To encourage α to match ρ, we"
2021.acl-long.300,D17-1039,0,0.0752852,"Missing"
2021.acl-long.300,2020.clssts-1.1,0,0.0136766,"system development, and the latter being a larger evaluation corpus. In our main experiments we do not use Analysis or Dev for development and so we report results for all three (the ground truth relevance judgements for the TL Eval collection have not been released yet so we do not report Eval for TL). See Table 3 for evaluation statistics. All queries are text. The speech documents are first transcribed with an ASR system (Ragni and Gales, 2018), and the 1-best ASR output is used in the sentence selection task. Examples of the evaluation datasets are shown in Appendix B. We refer readers to Rubino (2020) for further details about MATERIAL test collections used in this work. While our model and baselines work at the sentence-level, the MATERIAL relevance judgements are only at the document level. Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and ZoExperiment Settings Baselines Cross-Lingual Word Embeddings. We compare our model with three other cross-lingual embedding methods, Bivec (Luong et al., 2015), MUSE (Lample et al., 2018), and SID-SGNS (Levy et al., 2017). Bivec and SID-SGNS are tr"
2021.acl-long.300,tiedemann-2012-parallel,0,0.049924,"he translation matrix, and positive samples where none of the translations of the query in the matrix are present in the source sentence. 4 Experiments 4.1 Dataset Generation from Parallel Corpus The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A. 1 https://www.iarpa.gov/index.php/ research-programs/material 2 https://dumps.wikimedia.org/ 3884 # sents. EN tkn. LR tkn. EN-SO EN-SW EN-TL 69,818 1,827,826 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tok"
2021.acl-long.300,W00-1312,0,0.613969,"on. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevanc"
2021.findings-acl.265,P19-1480,0,0.0201045,"ations besides expanding question answering data, such as initiating a conversation of dialogue systems (Mostafazadeh et al., 2017), providing practice exercises for educational purposes (Jia et al., 2020), and accelerating real-time question answering (Seo et al., 2019). It also has great potential in enriching task-oriented dialogue datasets (Sun et al., 2016, 2017; Huang et al., 2020a; Kim et al., 2020b). Early studies build on encoder-decoder models and utilize different evidence information, e.g., Wikipedia passages (Du and Cardie, 2018), reviews (Yu et al., 2020c), and dialogue history (Gao et al., 2019). These studies often assume that the questions are single-hop which be answered by one piece of evidence. As more high-quality multi-hop question answering datasets become available (e.g., H OTPOT QA (Yang et al., 2018)), recent years have seen a growing interest in multi-hop question generation. Most recent approaches add heuristically extracted features to the encoder-decoder model, which relies on large-scale training data and can still suffer from error propagation (Yu et al., 2020b; Pan et al., 2020). A recent study (Yu et al., 2020a) which also studies low-resource question generation a"
2021.findings-acl.265,2020.findings-emnlp.355,1,0.906379,"uestion generation model with supervision from generative single-hop and multi-hop QG. 5 30 ROUGE-L EM ROUGE-L QA S UPERVISION PLAR (multi only) PLAR (multi+planning) 35 Related Work Question generation has a wide range of applications besides expanding question answering data, such as initiating a conversation of dialogue systems (Mostafazadeh et al., 2017), providing practice exercises for educational purposes (Jia et al., 2020), and accelerating real-time question answering (Seo et al., 2019). It also has great potential in enriching task-oriented dialogue datasets (Sun et al., 2016, 2017; Huang et al., 2020a; Kim et al., 2020b). Early studies build on encoder-decoder models and utilize different evidence information, e.g., Wikipedia passages (Du and Cardie, 2018), reviews (Yu et al., 2020c), and dialogue history (Gao et al., 2019). These studies often assume that the questions are single-hop which be answered by one piece of evidence. As more high-quality multi-hop question answering datasets become available (e.g., H OTPOT QA (Yang et al., 2018)), recent years have seen a growing interest in multi-hop question generation. Most recent approaches add heuristically extracted features to the encode"
2021.findings-acl.265,2020.acl-main.62,1,0.921551,"uestion generation model with supervision from generative single-hop and multi-hop QG. 5 30 ROUGE-L EM ROUGE-L QA S UPERVISION PLAR (multi only) PLAR (multi+planning) 35 Related Work Question generation has a wide range of applications besides expanding question answering data, such as initiating a conversation of dialogue systems (Mostafazadeh et al., 2017), providing practice exercises for educational purposes (Jia et al., 2020), and accelerating real-time question answering (Seo et al., 2019). It also has great potential in enriching task-oriented dialogue datasets (Sun et al., 2016, 2017; Huang et al., 2020a; Kim et al., 2020b). Early studies build on encoder-decoder models and utilize different evidence information, e.g., Wikipedia passages (Du and Cardie, 2018), reviews (Yu et al., 2020c), and dialogue history (Gao et al., 2019). These studies often assume that the questions are single-hop which be answered by one piece of evidence. As more high-quality multi-hop question answering datasets become available (e.g., H OTPOT QA (Yang et al., 2018)), recent years have seen a growing interest in multi-hop question generation. Most recent approaches add heuristically extracted features to the encode"
2021.findings-acl.265,2020.sigdial-1.35,0,0.0998364,"del with supervision from generative single-hop and multi-hop QG. 5 30 ROUGE-L EM ROUGE-L QA S UPERVISION PLAR (multi only) PLAR (multi+planning) 35 Related Work Question generation has a wide range of applications besides expanding question answering data, such as initiating a conversation of dialogue systems (Mostafazadeh et al., 2017), providing practice exercises for educational purposes (Jia et al., 2020), and accelerating real-time question answering (Seo et al., 2019). It also has great potential in enriching task-oriented dialogue datasets (Sun et al., 2016, 2017; Huang et al., 2020a; Kim et al., 2020b). Early studies build on encoder-decoder models and utilize different evidence information, e.g., Wikipedia passages (Du and Cardie, 2018), reviews (Yu et al., 2020c), and dialogue history (Gao et al., 2019). These studies often assume that the questions are single-hop which be answered by one piece of evidence. As more high-quality multi-hop question answering datasets become available (e.g., H OTPOT QA (Yang et al., 2018)), recent years have seen a growing interest in multi-hop question generation. Most recent approaches add heuristically extracted features to the encoder-decoder model, wh"
2021.findings-acl.265,2020.emnlp-main.530,0,0.0331933,"SubQuestions Answer Introduction Rui Zhang is the corresponding author. [1] George ..., known professionally as Dario Franchitti, is a retired Scottish racing driver. [2] After Franchitti did not secure a single-seater drive in 1995, he was contracted by the AMG team to compete in touring cars in the DTM and its successor — the International Touring Car Championship. Paragraph B. Mercedes-AMG Question generation aims to automatically generate valid and coherent questions based on given context, which is widely applied to enrich question answering (QA) datasets, facilitate text comprehension (Ko et al., 2020), seek clarification in conversation (Rao and Daum´e III, 2019), etc. Recently, neural encoder-decoder based approaches ∗ Paragraph A. Dario Franchitti &lt; Dario Franchitti , contracted by , AMG > &lt; Dario Franchitti , competed in , DTM > &lt; AMG , headquartered in , Affalterbach, Baden W¨urttemberg, Germany > After he was contracted by the team that is headquartered in Affalterbach, Baden W¨urttemberg, Germany, Dario Franchitti competed in what series? Which team is headquartered in Affalterbach, Baden W¨urttemberg, Germany? After contracted by AMG, Dario Franchitti competed in what series? DTM ha"
2021.findings-acl.265,2020.acl-main.20,0,0.0359068,"des the information of the other sub-question. Thus, this formulation also alleviates the high variance issue commonly encountered in hierarchical variational training (Vahdat and Kautz, 2020). We also consider the planning guided mechanism in the generative modeling of single-hop QG log p(s) = log p(s, z, q)dz z ≥ Eqω (z|q) [p(s|z)] − KL(qω (z|s)||pω (z)) There is a challenge under the generative formulation: the diversification of feasible generated questions can impinge the model training. Given the same evidence set and pre-selected answer, there can be multiple ways to raise a questions (Lee et al., 2020). However, not every potential single-hop question is qualified as a sub-question to form the target multi-hop question, as illustrated in Table. 2. To address this challenge, we propose to learn a latent planning variable which serves as a generation planning to guide the generation process. The latent planning variable aims to capture the high-level reasoning required to answer the multihop questions, which is abstracted as a reasoning path in existing studies. In order to model decision making of the reasoning path, we define the latent variable z as a discrete variable. We now incorporate"
2021.findings-acl.265,P19-1612,0,0.0277509,"ulti-hop questions are available, which is also difficult to obtain. We aim to overcome these limitations in this study. Our study is also related to generative modeling which treats unobserved variables (e.g., features or labels) as latent variables, and approximates the distribution through variational inference (Kingma and Welling, 2014). Generative modeling has been applied to dialogue response generation (Zhao et al., 2019; Huang et al., 2020b; Yang et al., 2020), policy learning (Huang et al., 2019, 2020c), 3015 sentiment analysis (Xu et al., 2017; Li et al., 2019), knowledge retrieval (Lee et al., 2019; Kim et al., 2020a; Su et al., 2021; Tan et al., 2021), and text style transfer (He et al., 2020). While these works focus on utilizing unlabeled data to boost model performance, we aim to unify non-parallel question corpuses to enable joint learning. 6 Conclusions We proposed a jointly optimized two-phase model named PLAR for low-resource question generation. PLAR effectively utilizes non-parallel singlehop and multi-hop question answering data to perform optimization. We further designed a planning mechanism to guide the generation process of subquestions so that the generation results are"
2021.findings-acl.265,2020.acl-main.703,0,0.0118328,". This is because it is labour-intensive to obtain the extra question decomposition annotations, which are not present in HotpotQA. Thus, it is not practical to assume such decomposition annotations would be available in different QA tasks. We aim to tackle this challenge by utilizing non-parallel single-hop questions, which is relatively easy to acquire and do not require extra task-specific annotations. We compare with three baselines that are based on seq-to-seq models and are competitive in singlehop question generation tasks: ASs2s (Kim et al., 2019), Maxout-QG (Zhao et al., 2018), BART (Lewis et al., 2020). We compare with two baselines that consume auxiliary reasoning path features for multi-hop QG: RC-QG (Yu et al., 2020b) uses reasoning chains built via named entity recognition and relation extraction; and SG-DQG (Pan et al., 2020) adopts semantic role labeling techniques to build semantic graphs. We also compare the full PLAR with its two variants: Pipeline individually trains a single-hop QG model and a question composing model using synthetic question decomposition data obtained as Perez et al. (2020); and PLAR w/o plan uses the generative objectives as PLAR without the planning mechanism"
2021.findings-acl.265,P19-1186,0,0.0283507,"Missing"
2021.findings-acl.265,W02-0406,0,0.0887847,"ia. The question-relevant sentences within these paragraphs are further annotated as supporting facts. We follow the original data split of H OTPOT QA, which includes 90,440 / 6,072 examples for training and evaluation, respectively. We further hold out 6,072 examples from the training data as the validation set. We use SQuAD (Rajpurkar et al., 2016) as the single-hop QA dataset, which has over 100K questions also crowd-sourced based on Wikipedia articles. Following the conventional evaluation metrics, we use n-gram BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE-L (Lin and Hovy, 2002) to evaluate the question generation quality. We consider two input settings to thoroughly evaluate the multi-hop question generation (QG) performance: sentence-level and paragraph-level. In the first setting, following the existing multi-hop QG task formulation (Pan et al., 2020; Yu et al., 2020b), we take the question-relevant sentences (i.e., supporting facts) along with the answer as inputs to generate the question. However, human annotated supporting facts are not always available, while identifying two relevant paragraphs is relatively achievable. Thus, we further consider a paragraph-le"
2021.findings-acl.265,P19-1613,0,0.0229559,"amed entity recognition (NER), and find relations via relation extraction. The extracted structural reasoning path, in the form of subject-predicate-object triples as illustrated in Table 1, is then fed to the generation model as auxiliary features (Yu et al., 2020b). However, the reasoning capability is constrained by the off-the-shelf extraction tools which cannot be extended to arbitrary context (Yang et al., 2018; Dhingra et al., 2020). Another line of recent studies on multi-hop question answering models the reasoning process by decomposing a multi-hop question into several subquestions (Min et al., 2019; Wolfson et al., 2020). As illustrated in Table 1, the answer of the multihop question can be derived by answering a series of single-hop sub-questions. Ideally, question generation can also adopt this two-phase strategy which first generates sub-questions and then composes the sub-questions into a multi-hop question. However, this strategy requires a parallel corpus that annotates each multi-hop question to its corresponding sub-questions, and obtaining such annotations still requires extensive efforts and costs. To address these issues, we propose to jointly optimize the two-phase model usi"
2021.findings-acl.265,I17-1047,0,0.025966,"at without the generative single-hop objective, training the subquestion generation model heavily relies on the initial fine-tuning step, and is thus prone to singlehop QA data insufficiency. The full PLAR model addresses this limitation by further training the subquestion generation model with supervision from generative single-hop and multi-hop QG. 5 30 ROUGE-L EM ROUGE-L QA S UPERVISION PLAR (multi only) PLAR (multi+planning) 35 Related Work Question generation has a wide range of applications besides expanding question answering data, such as initiating a conversation of dialogue systems (Mostafazadeh et al., 2017), providing practice exercises for educational purposes (Jia et al., 2020), and accelerating real-time question answering (Seo et al., 2019). It also has great potential in enriching task-oriented dialogue datasets (Sun et al., 2016, 2017; Huang et al., 2020a; Kim et al., 2020b). Early studies build on encoder-decoder models and utilize different evidence information, e.g., Wikipedia passages (Du and Cardie, 2018), reviews (Yu et al., 2020c), and dialogue history (Gao et al., 2019). These studies often assume that the questions are single-hop which be answered by one piece of evidence. As more"
2021.findings-acl.265,2020.acl-main.135,0,0.307923,"MG , headquartered in , Affalterbach, Baden W¨urttemberg, Germany > After he was contracted by the team that is headquartered in Affalterbach, Baden W¨urttemberg, Germany, Dario Franchitti competed in what series? Which team is headquartered in Affalterbach, Baden W¨urttemberg, Germany? After contracted by AMG, Dario Franchitti competed in what series? DTM have shown promising results for simple, singlehop question generation (Du and Cardie, 2018). Such approaches directly maps context (e.g., text passages) to questions without reasoning, and thus struggle when generating multi-hop questions (Pan et al., 2020). Here, reasoning refers to identifying and aggregating the relevant information taken from multiple documents to derive the question. Table 1 illustrates the reasoning process of a multihop question; in this example, the entity that links the two passages, i.e., “AMG”, is firstly identified, and the relations around it in the context are transformed into a question. To model such reasoning processes in an end-to-end manner requires extensive training data, and is thus impractical due to the 3008 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3008–3022 August"
2021.findings-acl.265,P02-1040,0,0.1106,"evidence set of each question includes relevant paragraphs from Wikipedia. The question-relevant sentences within these paragraphs are further annotated as supporting facts. We follow the original data split of H OTPOT QA, which includes 90,440 / 6,072 examples for training and evaluation, respectively. We further hold out 6,072 examples from the training data as the validation set. We use SQuAD (Rajpurkar et al., 2016) as the single-hop QA dataset, which has over 100K questions also crowd-sourced based on Wikipedia articles. Following the conventional evaluation metrics, we use n-gram BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE-L (Lin and Hovy, 2002) to evaluate the question generation quality. We consider two input settings to thoroughly evaluate the multi-hop question generation (QG) performance: sentence-level and paragraph-level. In the first setting, following the existing multi-hop QG task formulation (Pan et al., 2020; Yu et al., 2020b), we take the question-relevant sentences (i.e., supporting facts) along with the answer as inputs to generate the question. However, human annotated supporting facts are not always available, while identifying two relevant paragrap"
2021.findings-acl.265,2020.emnlp-main.713,0,0.0138969,"uestion generation tasks: ASs2s (Kim et al., 2019), Maxout-QG (Zhao et al., 2018), BART (Lewis et al., 2020). We compare with two baselines that consume auxiliary reasoning path features for multi-hop QG: RC-QG (Yu et al., 2020b) uses reasoning chains built via named entity recognition and relation extraction; and SG-DQG (Pan et al., 2020) adopts semantic role labeling techniques to build semantic graphs. We also compare the full PLAR with its two variants: Pipeline individually trains a single-hop QG model and a question composing model using synthetic question decomposition data obtained as Perez et al. (2020); and PLAR w/o plan uses the generative objectives as PLAR without the planning mechanism. 3013 Table 3: Question Generation Results (Sentence-Level) H OTPOT-30K M ODEL Seq-to-Seq Application ASs2s Maxout-QG BART Reasoning Path RC-QG Enhanced SG-DQG Proposed 4.2 Pipeline PLAR w/o plan PLAR H OTPOT-10K BLEU-1 BLEU-4 METEOR ROUGE-L BLEU-1 BLEU-4 METEOR ROUGE-L 27.12 28.21 30.52 10.11 10.26 11.15 13.27 13.64 14.87 25.69 25.80 27.22 24.32 25.47 26.10 9.27 9.19 10.12 11.93 11.84 12.46 23.20 23.51 23.63 30.86 32.93 11.36 12.32 15.29 16.40 28.66 29.81 29.31 31.12 11.16 12.27 14.88 15.25 26.89 27.90 3"
2021.findings-acl.265,N19-1013,0,0.0561897,"Missing"
2021.findings-acl.265,P19-1436,0,0.0260128,"prone to singlehop QA data insufficiency. The full PLAR model addresses this limitation by further training the subquestion generation model with supervision from generative single-hop and multi-hop QG. 5 30 ROUGE-L EM ROUGE-L QA S UPERVISION PLAR (multi only) PLAR (multi+planning) 35 Related Work Question generation has a wide range of applications besides expanding question answering data, such as initiating a conversation of dialogue systems (Mostafazadeh et al., 2017), providing practice exercises for educational purposes (Jia et al., 2020), and accelerating real-time question answering (Seo et al., 2019). It also has great potential in enriching task-oriented dialogue datasets (Sun et al., 2016, 2017; Huang et al., 2020a; Kim et al., 2020b). Early studies build on encoder-decoder models and utilize different evidence information, e.g., Wikipedia passages (Du and Cardie, 2018), reviews (Yu et al., 2020c), and dialogue history (Gao et al., 2019). These studies often assume that the questions are single-hop which be answered by one piece of evidence. As more high-quality multi-hop question answering datasets become available (e.g., H OTPOT QA (Yang et al., 2018)), recent years have seen a growin"
2021.findings-acl.265,2020.tacl-1.13,0,0.116217,"ition (NER), and find relations via relation extraction. The extracted structural reasoning path, in the form of subject-predicate-object triples as illustrated in Table 1, is then fed to the generation model as auxiliary features (Yu et al., 2020b). However, the reasoning capability is constrained by the off-the-shelf extraction tools which cannot be extended to arbitrary context (Yang et al., 2018; Dhingra et al., 2020). Another line of recent studies on multi-hop question answering models the reasoning process by decomposing a multi-hop question into several subquestions (Min et al., 2019; Wolfson et al., 2020). As illustrated in Table 1, the answer of the multihop question can be derived by answering a series of single-hop sub-questions. Ideally, question generation can also adopt this two-phase strategy which first generates sub-questions and then composes the sub-questions into a multi-hop question. However, this strategy requires a parallel corpus that annotates each multi-hop question to its corresponding sub-questions, and obtaining such annotations still requires extensive efforts and costs. To address these issues, we propose to jointly optimize the two-phase model using non-parallel single-"
2021.findings-acl.265,2020.emnlp-main.147,1,0.752616,"n et al., 2020). A recent study (Yu et al., 2020a) which also studies low-resource question generation assumes that a large amount of unanswered multi-hop questions are available, which is also difficult to obtain. We aim to overcome these limitations in this study. Our study is also related to generative modeling which treats unobserved variables (e.g., features or labels) as latent variables, and approximates the distribution through variational inference (Kingma and Welling, 2014). Generative modeling has been applied to dialogue response generation (Zhao et al., 2019; Huang et al., 2020b; Yang et al., 2020), policy learning (Huang et al., 2019, 2020c), 3015 sentiment analysis (Xu et al., 2017; Li et al., 2019), knowledge retrieval (Lee et al., 2019; Kim et al., 2020a; Su et al., 2021; Tan et al., 2021), and text style transfer (He et al., 2020). While these works focus on utilizing unlabeled data to boost model performance, we aim to unify non-parallel question corpuses to enable joint learning. 6 Conclusions We proposed a jointly optimized two-phase model named PLAR for low-resource question generation. PLAR effectively utilizes non-parallel singlehop and multi-hop question answering data to pe"
2021.findings-acl.265,D18-1259,0,0.353779,"address this problem, recent studies propose to augment the generation model with an explicit reasoning progress. For example, a straightforward solution is to identify the anchoring entities via named entity recognition (NER), and find relations via relation extraction. The extracted structural reasoning path, in the form of subject-predicate-object triples as illustrated in Table 1, is then fed to the generation model as auxiliary features (Yu et al., 2020b). However, the reasoning capability is constrained by the off-the-shelf extraction tools which cannot be extended to arbitrary context (Yang et al., 2018; Dhingra et al., 2020). Another line of recent studies on multi-hop question answering models the reasoning process by decomposing a multi-hop question into several subquestions (Min et al., 2019; Wolfson et al., 2020). As illustrated in Table 1, the answer of the multihop question can be derived by answering a series of single-hop sub-questions. Ideally, question generation can also adopt this two-phase strategy which first generates sub-questions and then composes the sub-questions into a multi-hop question. However, this strategy requires a parallel corpus that annotates each multi-hop que"
2021.findings-acl.265,2020.acl-main.601,0,0.509557,"ACL-IJCNLP 2021, pages 3008–3022 August 1–6, 2021. ©2021 Association for Computational Linguistics extensive collection effort of multi-hop QA data. To address this problem, recent studies propose to augment the generation model with an explicit reasoning progress. For example, a straightforward solution is to identify the anchoring entities via named entity recognition (NER), and find relations via relation extraction. The extracted structural reasoning path, in the form of subject-predicate-object triples as illustrated in Table 1, is then fed to the generation model as auxiliary features (Yu et al., 2020b). However, the reasoning capability is constrained by the off-the-shelf extraction tools which cannot be extended to arbitrary context (Yang et al., 2018; Dhingra et al., 2020). Another line of recent studies on multi-hop question answering models the reasoning process by decomposing a multi-hop question into several subquestions (Min et al., 2019; Wolfson et al., 2020). As illustrated in Table 1, the answer of the multihop question can be derived by answering a series of single-hop sub-questions. Ideally, question generation can also adopt this two-phase strategy which first generates sub-q"
2021.findings-acl.265,2020.acl-main.26,0,0.407929,"ACL-IJCNLP 2021, pages 3008–3022 August 1–6, 2021. ©2021 Association for Computational Linguistics extensive collection effort of multi-hop QA data. To address this problem, recent studies propose to augment the generation model with an explicit reasoning progress. For example, a straightforward solution is to identify the anchoring entities via named entity recognition (NER), and find relations via relation extraction. The extracted structural reasoning path, in the form of subject-predicate-object triples as illustrated in Table 1, is then fed to the generation model as auxiliary features (Yu et al., 2020b). However, the reasoning capability is constrained by the off-the-shelf extraction tools which cannot be extended to arbitrary context (Yang et al., 2018; Dhingra et al., 2020). Another line of recent studies on multi-hop question answering models the reasoning process by decomposing a multi-hop question into several subquestions (Min et al., 2019; Wolfson et al., 2020). As illustrated in Table 1, the answer of the multihop question can be derived by answering a series of single-hop sub-questions. Ideally, question generation can also adopt this two-phase strategy which first generates sub-q"
2021.findings-acl.265,N19-1123,0,0.026935,"error propagation (Yu et al., 2020b; Pan et al., 2020). A recent study (Yu et al., 2020a) which also studies low-resource question generation assumes that a large amount of unanswered multi-hop questions are available, which is also difficult to obtain. We aim to overcome these limitations in this study. Our study is also related to generative modeling which treats unobserved variables (e.g., features or labels) as latent variables, and approximates the distribution through variational inference (Kingma and Welling, 2014). Generative modeling has been applied to dialogue response generation (Zhao et al., 2019; Huang et al., 2020b; Yang et al., 2020), policy learning (Huang et al., 2019, 2020c), 3015 sentiment analysis (Xu et al., 2017; Li et al., 2019), knowledge retrieval (Lee et al., 2019; Kim et al., 2020a; Su et al., 2021; Tan et al., 2021), and text style transfer (He et al., 2020). While these works focus on utilizing unlabeled data to boost model performance, we aim to unify non-parallel question corpuses to enable joint learning. 6 Conclusions We proposed a jointly optimized two-phase model named PLAR for low-resource question generation. PLAR effectively utilizes non-parallel singlehop an"
2021.findings-acl.265,D18-1424,0,0.0239471,"MR (Wolfson et al., 2020)). This is because it is labour-intensive to obtain the extra question decomposition annotations, which are not present in HotpotQA. Thus, it is not practical to assume such decomposition annotations would be available in different QA tasks. We aim to tackle this challenge by utilizing non-parallel single-hop questions, which is relatively easy to acquire and do not require extra task-specific annotations. We compare with three baselines that are based on seq-to-seq models and are competitive in singlehop question generation tasks: ASs2s (Kim et al., 2019), Maxout-QG (Zhao et al., 2018), BART (Lewis et al., 2020). We compare with two baselines that consume auxiliary reasoning path features for multi-hop QG: RC-QG (Yu et al., 2020b) uses reasoning chains built via named entity recognition and relation extraction; and SG-DQG (Pan et al., 2020) adopts semantic role labeling techniques to build semantic graphs. We also compare the full PLAR with its two variants: Pipeline individually trains a single-hop QG model and a question composing model using synthetic question decomposition data obtained as Perez et al. (2020); and PLAR w/o plan uses the generative objectives as PLAR wit"
2021.findings-acl.388,D19-1501,0,0.028207,"l-transport matching and embedding similarity losses by Wang et al. (2020b) and the content matching task presented by Parikh et al. (2020) are proved to be effective. Nevertheless, to the best of our knowledge, we are the first to bridge the training procedure of evaluator and generator together with the iterative training framework snowball. Furthermore, we attempt to construct a new automatic metric and a new dataset dedicated to evaluating the logic consistency of text generation. The concentration of our work differs from the related highfidelity text generation work (Chen et al., 2020b; Chan et al., 2019; Nie et al., 2018; Tian et al., 2019; Wang et al., 2020a), by attempting to present the panorama of the challenges of logic-consistent text generation instead of focusing on the model-wised modifications. 3 Snowball Framework The S NOWBALL framework addresses the challenge of the complex and intensive inner logic with data sparsity constraint for the high-fidelity textgeneration from semantic parses. As illustrated in Figure 2, S NOWBALL assures the logic consistency with three bases: (1) Iterative training procedure synergistically enhances the generator and evaluator in the adversarial fash"
2021.findings-acl.388,2020.acl-main.708,0,0.110483,"ose automatic metrics including BLEU, ROUGE and, BLEURT. Our data and code are available at https://github.com/ Ciaranshu/relogic. 1 Figure 1: Our data augmentation procedure for the generator and evaluator in the S NOWBALL framework. Introduction Natural language generation (NLG) from semantic parses is to generate the text description for the formal representation input such as logical forms, AMR, and SQL queries. It has drawn widespread attention because of its substantial contributions to the interpretability and usability of the latest natural language interfaces (Gatt and Krahmer, 2018; Chen et al., 2020b; Hu et al., 2020; Mishra et al., 2019; Yu et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types"
2021.findings-acl.388,2020.findings-emnlp.190,0,0.0628061,"ose automatic metrics including BLEU, ROUGE and, BLEURT. Our data and code are available at https://github.com/ Ciaranshu/relogic. 1 Figure 1: Our data augmentation procedure for the generator and evaluator in the S NOWBALL framework. Introduction Natural language generation (NLG) from semantic parses is to generate the text description for the formal representation input such as logical forms, AMR, and SQL queries. It has drawn widespread attention because of its substantial contributions to the interpretability and usability of the latest natural language interfaces (Gatt and Krahmer, 2018; Chen et al., 2020b; Hu et al., 2020; Mishra et al., 2019; Yu et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types"
2021.findings-acl.388,2020.coling-main.218,0,0.0330621,"Missing"
2021.findings-acl.388,P17-1089,0,0.0296698,"et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling work (Iyer et al., 2017; Yaghmazadeh et al., 2017), which potentially leads to the unsatisfied fidelity of remaining the complex and intensive inner logic in the generated text based on our empirical research; (2) The general-purpose automatic metrics (Novikova et al., 2017a) such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and BLEURT (Sellam et al., 4414 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4414–4426 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: Our S NOWBALL framework employs an iterative training procedure over a generator and evalu"
2021.findings-acl.388,2020.emnlp-main.750,0,0.0358291,"8; Ngonga Ngomo et al., 2013; Koutrika et al., 2010b). However, different from these works, our work focuses on the logic consistency generation from parses. So we will mainly discuss and evaluate the model based on the logic between parses and questions. 2.2 High-fidelity Text Generation As for the end-to-end neural-based text generation models, collaborating the auxiliary task during model training is an intuitive method that introduces the logic regulation to the models. For instance, the fidelity classification task proposed by Harkous et al. (2020), the auxiliary span extraction tasks by Kryscinski et al. (2020),the table-text optimal-transport matching and embedding similarity losses by Wang et al. (2020b) and the content matching task presented by Parikh et al. (2020) are proved to be effective. Nevertheless, to the best of our knowledge, we are the first to bridge the training procedure of evaluator and generator together with the iterative training framework snowball. Furthermore, we attempt to construct a new automatic metric and a new dataset dedicated to evaluating the logic consistency of text generation. The concentration of our work differs from the related highfidelity text generation work"
2021.findings-acl.388,2020.acl-main.703,0,0.326324,"mented datai−1 which contains more unseen logic variations uncovered in the seed data; Step 3: The enhanced Generatori predicts the increasingly realistic-like perturbed sentences from the perturbed logical forms, which brings more challenging negative samples to the training set of the Evaluatori . The data augmentation in the first step would be further described in Section 3.2. To be specific, our generator and evaluator in S NOWBALL are described as follows. Generator The generator maps the logical form to the corresponding natural language sentences. We choose the pre-trained BART model (Lewis et al., 2020) following the standard transformer architecture (Vaswani et al., 2017), which contains 4416 the encoder and decoder architecture as the denoising autoencoder pre-trained on the task of corrupted text reconstruction. The input of the encoder is the structure-aware representation of the logic forms (Section 3.2), while the target output of the decoder is the aligned textual description for the input parses. each given logical form could be enumerated exhaustively according to hand-tuned rules to cover the following logic inconsistencies: • Logic shift: The logic shift indicates that the generat"
2021.findings-acl.388,W04-1013,0,0.134728,"o generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling work (Iyer et al., 2017; Yaghmazadeh et al., 2017), which potentially leads to the unsatisfied fidelity of remaining the complex and intensive inner logic in the generated text based on our empirical research; (2) The general-purpose automatic metrics (Novikova et al., 2017a) such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and BLEURT (Sellam et al., 4414 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4414–4426 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: Our S NOWBALL framework employs an iterative training procedure over a generator and evaluator through data augmentation. 2020) are not ideal for explicitly measuring the logic consistency (Wang et al., 2020b; Harkous et al., 2020), because they tend to evenly weight each word in the generated text without fully attending on the fatal logical keywords. To address these two critical problems, we p"
2021.findings-acl.388,P19-4009,0,0.0124953,"ROUGE and, BLEURT. Our data and code are available at https://github.com/ Ciaranshu/relogic. 1 Figure 1: Our data augmentation procedure for the generator and evaluator in the S NOWBALL framework. Introduction Natural language generation (NLG) from semantic parses is to generate the text description for the formal representation input such as logical forms, AMR, and SQL queries. It has drawn widespread attention because of its substantial contributions to the interpretability and usability of the latest natural language interfaces (Gatt and Krahmer, 2018; Chen et al., 2020b; Hu et al., 2020; Mishra et al., 2019; Yu et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling wo"
2021.findings-acl.388,D18-1422,0,0.0273954,"g and embedding similarity losses by Wang et al. (2020b) and the content matching task presented by Parikh et al. (2020) are proved to be effective. Nevertheless, to the best of our knowledge, we are the first to bridge the training procedure of evaluator and generator together with the iterative training framework snowball. Furthermore, we attempt to construct a new automatic metric and a new dataset dedicated to evaluating the logic consistency of text generation. The concentration of our work differs from the related highfidelity text generation work (Chen et al., 2020b; Chan et al., 2019; Nie et al., 2018; Tian et al., 2019; Wang et al., 2020a), by attempting to present the panorama of the challenges of logic-consistent text generation instead of focusing on the model-wised modifications. 3 Snowball Framework The S NOWBALL framework addresses the challenge of the complex and intensive inner logic with data sparsity constraint for the high-fidelity textgeneration from semantic parses. As illustrated in Figure 2, S NOWBALL assures the logic consistency with three bases: (1) Iterative training procedure synergistically enhances the generator and evaluator in the adversarial fashion; (2) Data augm"
2021.findings-acl.388,D17-1238,0,0.0277135,"Missing"
2021.findings-acl.388,W17-5525,0,0.0411058,"Missing"
2021.findings-acl.388,P02-1040,0,0.110262,"2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling work (Iyer et al., 2017; Yaghmazadeh et al., 2017), which potentially leads to the unsatisfied fidelity of remaining the complex and intensive inner logic in the generated text based on our empirical research; (2) The general-purpose automatic metrics (Novikova et al., 2017a) such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and BLEURT (Sellam et al., 4414 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4414–4426 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: Our S NOWBALL framework employs an iterative training procedure over a generator and evaluator through data augmentation. 2020) are not ideal for explicitly measuring the logic consistency (Wang et al., 2020b; Harkous et al., 2020), because they tend to evenly weight each word in the generated text without fully attending on the fatal logical keywords. To address these two crit"
2021.findings-acl.388,2020.emnlp-main.89,0,0.0156512,"ill mainly discuss and evaluate the model based on the logic between parses and questions. 2.2 High-fidelity Text Generation As for the end-to-end neural-based text generation models, collaborating the auxiliary task during model training is an intuitive method that introduces the logic regulation to the models. For instance, the fidelity classification task proposed by Harkous et al. (2020), the auxiliary span extraction tasks by Kryscinski et al. (2020),the table-text optimal-transport matching and embedding similarity losses by Wang et al. (2020b) and the content matching task presented by Parikh et al. (2020) are proved to be effective. Nevertheless, to the best of our knowledge, we are the first to bridge the training procedure of evaluator and generator together with the iterative training framework snowball. Furthermore, we attempt to construct a new automatic metric and a new dataset dedicated to evaluating the logic consistency of text generation. The concentration of our work differs from the related highfidelity text generation work (Chen et al., 2020b; Chan et al., 2019; Nie et al., 2018; Tian et al., 2019; Wang et al., 2020a), by attempting to present the panorama of the challenges of log"
2021.findings-acl.388,D19-1314,0,0.0191727,"omains. E2E is on the restaurant domain, and RotoWire is on the basketball domain. 4415 Moreover, some of them only have loose alignments between input and sentence, e.g., RotoWire. Generating the natural language descriptions for the logic forms or parses as a sub-task of D2T, has been studied in various datasets and tasks, such as GCC grammar to text (White, 2006), and UCC grammar to text (Gardent and Plainfoss´e, 1990). There are a lot of works that leverage the neural networks to conduct the generation on various tasks, for example, generating natural language from AMR (Song et al., 2018; Ribeiro et al., 2019; Damonte and Cohen, 2019), logic forms (Chen et al., 2020b), as well as SQL parses (Xu et al., 2018; Ngonga Ngomo et al., 2013; Koutrika et al., 2010b). However, different from these works, our work focuses on the logic consistency generation from parses. So we will mainly discuss and evaluate the model based on the logic between parses and questions. 2.2 High-fidelity Text Generation As for the end-to-end neural-based text generation models, collaborating the auxiliary task during model training is an intuitive method that introduces the logic regulation to the models. For instance, the fide"
2021.findings-acl.388,2020.acl-main.704,0,0.0332043,"Missing"
2021.findings-acl.388,P18-1150,0,0.148946,"limited number of domains. E2E is on the restaurant domain, and RotoWire is on the basketball domain. 4415 Moreover, some of them only have loose alignments between input and sentence, e.g., RotoWire. Generating the natural language descriptions for the logic forms or parses as a sub-task of D2T, has been studied in various datasets and tasks, such as GCC grammar to text (White, 2006), and UCC grammar to text (Gardent and Plainfoss´e, 1990). There are a lot of works that leverage the neural networks to conduct the generation on various tasks, for example, generating natural language from AMR (Song et al., 2018; Ribeiro et al., 2019; Damonte and Cohen, 2019), logic forms (Chen et al., 2020b), as well as SQL parses (Xu et al., 2018; Ngonga Ngomo et al., 2013; Koutrika et al., 2010b). However, different from these works, our work focuses on the logic consistency generation from parses. So we will mainly discuss and evaluate the model based on the logic between parses and questions. 2.2 High-fidelity Text Generation As for the end-to-end neural-based text generation models, collaborating the auxiliary task during model training is an intuitive method that introduces the logic regulation to the models."
2021.findings-acl.388,2020.acl-main.101,0,0.168529,"ure 1: Our data augmentation procedure for the generator and evaluator in the S NOWBALL framework. Introduction Natural language generation (NLG) from semantic parses is to generate the text description for the formal representation input such as logical forms, AMR, and SQL queries. It has drawn widespread attention because of its substantial contributions to the interpretability and usability of the latest natural language interfaces (Gatt and Krahmer, 2018; Chen et al., 2020b; Hu et al., 2020; Mishra et al., 2019; Yu et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling work (Iyer et al., 2017; Yaghmazadeh et al., 2017), which potentially leads to the unsatisfied fide"
2021.findings-acl.388,W19-1901,0,0.0128424,"ples, we attempt to cover the possible logic perturbations mentioned in section 3.2 with minimum modification to the original [logical form, Text] pairs. For example, a coincident pair [SELECT avg(age) FROM dogs, What is the average age of dogs?] would be corrupted into [SELECT avg(age) FROM dogs, What is the oldest age of dogs?]. Table 1 summarizes the statistics of each dataset for both generator and evaluator, respectively. 4419 5.2 Baselines and Implementation Details The baselines for assessing the performance of S NOWBALL framework are the attention-based LSTM machine translation model (Tao et al., 2019), and the single-pass trained models which are the models trained before performing S NOWBALL iteration. For instance, the BART-large generator trained in the second S NOWBALL iteration would be compared to the identical BART-large generator in the zero S NOWBALL iteration. The hype-parameter settings of the models trained on SQL2Text and Logic2Text, mostly follow the default setting of BART model from Huggingface (Lewis et al., 2020; Wolf et al., 2020). However, the learning rate of evaluator and tokenizer are slightly different, namely the learning rate of evaluator on SQL2Text is 2e-5 for B"
2021.findings-acl.388,D18-1112,0,0.04215,"Missing"
2021.findings-acl.388,W19-8639,0,0.0172534,"or the generator and evaluator in the S NOWBALL framework. Introduction Natural language generation (NLG) from semantic parses is to generate the text description for the formal representation input such as logical forms, AMR, and SQL queries. It has drawn widespread attention because of its substantial contributions to the interpretability and usability of the latest natural language interfaces (Gatt and Krahmer, 2018; Chen et al., 2020b; Hu et al., 2020; Mishra et al., 2019; Yu et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling work (Iyer et al., 2017; Yaghmazadeh et al., 2017), which potentially leads to the unsatisfied fidelity of remaining the complex and"
2021.findings-acl.388,W18-6502,0,0.0195137,"ps://github.com/ Ciaranshu/relogic. 1 Figure 1: Our data augmentation procedure for the generator and evaluator in the S NOWBALL framework. Introduction Natural language generation (NLG) from semantic parses is to generate the text description for the formal representation input such as logical forms, AMR, and SQL queries. It has drawn widespread attention because of its substantial contributions to the interpretability and usability of the latest natural language interfaces (Gatt and Krahmer, 2018; Chen et al., 2020b; Hu et al., 2020; Mishra et al., 2019; Yu et al., 2019; Ngomo et al., 2013; Wang et al., 2018; Gardent et al., 2017; Wang et al., 2020a; *Equal Contribution Wang, 2019; Koutrika et al., 2010a). Recently, pretrained large-scale language models like BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), and GPT3 (Brown et al., 2020) have raised the ability to generate natural language from formal texts to a promising level of fluency and coherence. However, NLG from semantic parses still has suffered from two crucial challenges: (1) the data scarcity constraint due to the bias on certain types of logic forms or expensive labeling work (Iyer et al., 2017; Yaghmazadeh et al., 2017), which"
2021.findings-acl.388,D18-1425,1,0.83479,"Missing"
2021.findings-emnlp.166,D10-1049,0,0.0403045,"ased on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The decoder takes the vector representation as context to generate the target sentence. In both the encoder and the decoder, sequence neural network models, such as LSTM (Hochreiter and Schmidhuber, 1997) and Transforme"
2021.findings-emnlp.166,H05-1042,0,0.0807925,"ated sentences. In early data-to-text genThe grouped-attention lets an attribute represen- eration approaches, content-planning is done by tation captures the relationship between tokens in creating hand-crafted rules (Scott and de Souza, an attribute (i.e., local context). The second is 1990; Hovy, 1993), using template-based models self-attention among attribute level representations. (McKeown, 1992; Reiter et al., 2000), or exploitThis attention updates the attributes’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019,"
2021.findings-emnlp.166,J05-1002,0,0.0157328,"Missing"
2021.findings-emnlp.166,P01-1023,0,0.189222,"e-based models self-attention among attribute level representations. (McKeown, 1992; Reiter et al., 2000), or exploitThis attention updates the attributes’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The d"
2021.findings-emnlp.166,W02-2112,0,0.282995,"Missing"
2021.findings-emnlp.166,W03-1016,0,0.365176,"Missing"
2021.findings-emnlp.166,2020.emnlp-main.351,0,0.0381566,"et al., 2016; Liu et al., 2018), weather forecasting (Mei et al., 2016), and game summarization (Wiseman et al., 2017). Despite their success, these models may generate incoherent sentences since they do not have a proper content-plan. To improve the coherence of the generated text, recent studies re-introduce content-planning mechanism in neural approaches of data-to-text generation. Sha et al. (2018) propose a content-planning mechanism via link-based attention to model relationships among input data. It learns a matrix where each element indicates transition probability of two attributes. Goldfarb-Tarrant et al. (2020) employ the Aristotelian framework and a re-scoring model to find the best story plot in story generation. Other studies along this line propose twostage models, i.e., learning the content-planning and the surface-realization consecutively. For example, Hua and Wang (2019) use LSTM-based content-plan, while Puduppully et al. (2019) exploit Pointer Networks (Vinyals et al., 2015) for content-planning. Both works use LSTM-based surface-realization. Trisedya et al. (2020) propose a content-planning-based attention model, where the content-plan is learned using Pointer Networks. formation networks"
2021.findings-emnlp.166,P16-1014,0,0.0803499,"Missing"
2021.findings-emnlp.166,P82-1020,0,0.754117,"Missing"
2021.findings-emnlp.166,D19-1055,0,0.0169867,"recent studies re-introduce content-planning mechanism in neural approaches of data-to-text generation. Sha et al. (2018) propose a content-planning mechanism via link-based attention to model relationships among input data. It learns a matrix where each element indicates transition probability of two attributes. Goldfarb-Tarrant et al. (2020) employ the Aristotelian framework and a re-scoring model to find the best story plot in story generation. Other studies along this line propose twostage models, i.e., learning the content-planning and the surface-realization consecutively. For example, Hua and Wang (2019) use LSTM-based content-plan, while Puduppully et al. (2019) exploit Pointer Networks (Vinyals et al., 2015) for content-planning. Both works use LSTM-based surface-realization. Trisedya et al. (2020) propose a content-planning-based attention model, where the content-plan is learned using Pointer Networks. formation networks as the input encoder. However, LSTM is sub-optimal to capture the relationships in the input. This is because the input is a set (e.g., a set of attributes), and each attributes may consist of multiple tokens, which form a hierarchical structure among the attributes inste"
2021.findings-emnlp.166,C10-2062,0,0.0220343,"ong attribute level representations. (McKeown, 1992; Reiter et al., 2000), or exploitThis attention updates the attributes’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The decoder takes the vector repr"
2021.findings-emnlp.166,N12-1093,0,0.0282105,"presentations. (McKeown, 1992; Reiter et al., 2000), or exploitThis attention updates the attributes’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The decoder takes the vector representation as context to ge"
2021.findings-emnlp.166,P83-1022,0,0.61173,"an attribute (i.e., local context). The second is 1990; Hovy, 1993), using template-based models self-attention among attribute level representations. (McKeown, 1992; Reiter et al., 2000), or exploitThis attention updates the attributes’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et"
2021.findings-emnlp.166,D16-1128,0,0.0397727,"Missing"
2021.findings-emnlp.166,P09-1011,0,0.0253654,"Missing"
2021.findings-emnlp.166,A97-1041,0,0.360512,"s’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The decoder takes the vector representation as context to generate the target sentence. In both the encoder and the decoder, sequence neural network model"
2021.findings-emnlp.166,N16-1086,0,0.0997086,"ter and Dale, 2000) is and the interpretability of neural data-to-text an important and challenging task in natural lan- generation models (Trisedya et al., 2018). In this guage processing. It aims to produce sentences paper, we study the problem of neural content-plan given structured data. There are many downstream generation. Given a set of attributes of an entity, applications of data-to-text-generation, such as bi- we aim to select the salient attributes (i.e., the ography summarization (Lebret et al., 2016), auto- attributes mentioned) and reorder the selected matic weather forecasting (Mei et al., 2016), etc. attributes such that they follow the common Traditional approaches follow a pipeline frame- attribute mentioning order in natural sentences. For work consisting of three stages: content-selection, example, in Table 1, the input is a set of attributes content-planning, and surface-realization. Content- for the entity Barack Obama (in the form of selection selects the data to be expressed; content- key-value pairs): hname; Barack Hussein planning determines the structure of the sentences Obamai, hbirth_place; Honolulu, ∗ Equal contribution Hawaiii, etc. Suppose that the target descrip1935"
2021.findings-emnlp.166,P02-1040,0,0.109666,". 5.5 Evaluation with Text Generation The primary goal of this paper is content-plan generation, which is an intermediate goal of text generation. In this experiment, we further evaluate the quality of the generated content-plans by using them for text generation. We train a text generation model using an encoder-decoder framework. Both the encoder and the decoder use the LSTM model. We train the model over the gold standard content-plan–target-sentence pairs of the dataset (cf. Section 5.1). For testing, we use the contentplan generated by the tested models as input, and we compute the BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores of the sentence generated by the text generation model. For each model, we take the best variant (i.e., the CS mask variant) to generate the content-plan. Table 3 shows the text generation results. These 6 Conclusions and Future Works results confirm that our proposed model achieves We presented a model for generating a content-plan the best performance. The BLEU scores of the from a set of entity attributes. To capture the local generated text from the content-plan generated by our model on WIKIALL, WIKIBIO, and RO- and global contexts from the input set, we prop"
2021.findings-emnlp.166,W00-1429,0,0.364022,"essential part of data-to-text The first is grouped-attention, a token-level atten- generation to determine the order of data mentioned tion mechanism restricted within each attribute. in generated sentences. In early data-to-text genThe grouped-attention lets an attribute represen- eration approaches, content-planning is done by tation captures the relationship between tokens in creating hand-crafted rules (Scott and de Souza, an attribute (i.e., local context). The second is 1990; Hovy, 1993), using template-based models self-attention among attribute level representations. (McKeown, 1992; Reiter et al., 2000), or exploitThis attention updates the attributes’ representa- ing machine learning models (Duboue and McKtions based on the overall attribute information (i.e., eown, 2003; Barzilay and Lapata, 2005; Liang global context). The third is cross-attention, which et al., 2009). Content-planning is coupled with 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization,"
2021.findings-emnlp.166,2020.acl-main.641,0,0.0217539,"th 1936 content-selection and surface-realization. Contentselection mainly relies on hand-built heuristics (Kukich, 1983; Ehud Reiter, 1997) and shallow statistical machine learning models (Duboue and McKeown, 2001, 2002; Kim and Mooney, 2010; Konstas and Lapata, 2012). For surface-realization, earlier studies exploit template-based models (McKeown et al., 1997; Deemter et al., 2005) and language models (Angeli et al., 2010). As end-to-end training is becoming prevalent (Wang et al., 2019, 2021b), recent data-totext generation approaches employ neural networks which can be trained end-to-end (Shen et al., 2020). These approaches use encoder-decoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The decoder takes the vector representation as context to generate the target sentence. In both the encoder and the decoder, sequence neural network models, such as LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017), are used to process the data. Studies in this line of work include biography summarization (Lebret et al., 2016; Liu et al., 2018), weather forecasting (Mei et al.,"
2021.findings-emnlp.166,D17-1239,0,0.196862,"ecoder frameworks (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). The encoder is used to transform the input into some vector representation. The decoder takes the vector representation as context to generate the target sentence. In both the encoder and the decoder, sequence neural network models, such as LSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017), are used to process the data. Studies in this line of work include biography summarization (Lebret et al., 2016; Liu et al., 2018), weather forecasting (Mei et al., 2016), and game summarization (Wiseman et al., 2017). Despite their success, these models may generate incoherent sentences since they do not have a proper content-plan. To improve the coherence of the generated text, recent studies re-introduce content-planning mechanism in neural approaches of data-to-text generation. Sha et al. (2018) propose a content-planning mechanism via link-based attention to model relationships among input data. It learns a matrix where each element indicates transition probability of two attributes. Goldfarb-Tarrant et al. (2020) employ the Aristotelian framework and a re-scoring model to find the best story plot in"
2021.findings-emnlp.166,P18-1151,1,0.849393,"t al., 2018a). Despite their success, end-to-end models without proper content-planning may generate repetitive, incomplete, and incoherent sentences. Moreover, end-to-end models are less interpretable, making it difficult to perform error analysis for further improvement. It has been shown that explicitly learning content-planning improves the performance (e.g., 1 Introduction reduce repetition or generate a coherent sentence) Data-to-text generation (Reiter and Dale, 2000) is and the interpretability of neural data-to-text an important and challenging task in natural lan- generation models (Trisedya et al., 2018). In this guage processing. It aims to produce sentences paper, we study the problem of neural content-plan given structured data. There are many downstream generation. Given a set of attributes of an entity, applications of data-to-text-generation, such as bi- we aim to select the salient attributes (i.e., the ography summarization (Lebret et al., 2016), auto- attributes mentioned) and reorder the selected matic weather forecasting (Mei et al., 2016), etc. attributes such that they follow the common Traditional approaches follow a pipeline frame- attribute mentioning order in natural sentence"
2021.findings-emnlp.172,2020.wmt-1.8,0,0.11426,"7), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. Our work aims to provide the first benchmark for Authorship Attribution in the form of the Turing Test by including humans and neural language models. Figure 3: The T URING B ENCH Environment. CTRL (Keskar et al., 2019), XLM (Lample and Conneau, 2019), XLNET (Yang et al., 2019), FAIR (Ng et al., 2019; Chen et al., 2020), TRANSFORMERXL (Dai et al., 2019), and PPLM (Dathathri et al., 2020). In addition, some of these language models have multiple pre-trained models and thus, we were able to generate texts with 19 neural machine textgenerators. We choose these 10 language model architectures because they are currently considered as the SOTA text-generators, many of the text-generators on Hugging Face’s model repo are variants of these language models, and both their pre-trained models and codes were publicly available. To generate texts, all 19 neural generators require a short prompt and a specified number of"
2021.findings-emnlp.172,P14-1147,0,0.0110502,"ribution (AA) aims to decide the author of a given text from a set of candidates (Houvardas and Stamatatos, 2006; Stamatatos, 2009b; Zhang et al., 2014). AA has a broad range of applications including author profiling (López-Monroy et al., 2020), computer forensics (Lambers and Veenman, 2009), and plagiarism detection (Stamatatos, 2009a). Previous work on AA has explored and combined various features and representations at different levels including n-grams (Escalante et al., 2011; Sapkota et al., 2015, 2016), POS-tags (Ferracane et al., 2017; Halvani et al., 2020) psycholinguistics features (Li et al., 2014; Uchendu et al., 2019), while recent approaches also build deep neural network based classifiers such as feed-forward NNLMs (Ge et al., 2016), CNNs (Hitschler et al., 2017; Shrestha et al., 2017), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. Our work aims to provide the first benchmark for Authorship Attribution in the form of the Turing Test by inc"
2021.findings-emnlp.172,2021.ccl-1.108,0,0.0767046,"Missing"
2021.findings-emnlp.172,W17-4914,0,0.0364948,"Missing"
2021.findings-emnlp.172,2021.eacl-main.155,0,0.0263927,"rams (Escalante et al., 2011; Sapkota et al., 2015, 2016), POS-tags (Ferracane et al., 2017; Halvani et al., 2020) psycholinguistics features (Li et al., 2014; Uchendu et al., 2019), while recent approaches also build deep neural network based classifiers such as feed-forward NNLMs (Ge et al., 2016), CNNs (Hitschler et al., 2017; Shrestha et al., 2017), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. Our work aims to provide the first benchmark for Authorship Attribution in the form of the Turing Test by including humans and neural language models. Figure 3: The T URING B ENCH Environment. CTRL (Keskar et al., 2019), XLM (Lample and Conneau, 2019), XLNET (Yang et al., 2019), FAIR (Ng et al., 2019; Chen et al., 2020), TRANSFORMERXL (Dai et al., 2019), and PPLM (Dathathri et al., 2020). In addition, some of these language models have multiple pre-trained models and thus, we were able to generate texts with 19 neural machine textgenerators. We choose these 10 lan"
2021.findings-emnlp.172,W19-5333,0,0.134831,"estha et al., 2017), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. Our work aims to provide the first benchmark for Authorship Attribution in the form of the Turing Test by including humans and neural language models. Figure 3: The T URING B ENCH Environment. CTRL (Keskar et al., 2019), XLM (Lample and Conneau, 2019), XLNET (Yang et al., 2019), FAIR (Ng et al., 2019; Chen et al., 2020), TRANSFORMERXL (Dai et al., 2019), and PPLM (Dathathri et al., 2020). In addition, some of these language models have multiple pre-trained models and thus, we were able to generate texts with 19 neural machine textgenerators. We choose these 10 language model architectures because they are currently considered as the SOTA text-generators, many of the text-generators on Hugging Face’s model repo are variants of these language models, and both their pre-trained models and codes were publicly available. To generate texts, all 19 neural generators require a short prompt and a"
2021.findings-emnlp.172,D16-1264,0,0.110755,"Missing"
2021.findings-emnlp.172,N15-1010,0,0.0199269,"generalizable, interpretable, and robust detectors (Jawahar et al., 2020). Authorship Attribution Authorship Attribution (AA) aims to decide the author of a given text from a set of candidates (Houvardas and Stamatatos, 2006; Stamatatos, 2009b; Zhang et al., 2014). AA has a broad range of applications including author profiling (López-Monroy et al., 2020), computer forensics (Lambers and Veenman, 2009), and plagiarism detection (Stamatatos, 2009a). Previous work on AA has explored and combined various features and representations at different levels including n-grams (Escalante et al., 2011; Sapkota et al., 2015, 2016), POS-tags (Ferracane et al., 2017; Halvani et al., 2020) psycholinguistics features (Li et al., 2014; Uchendu et al., 2019), while recent approaches also build deep neural network based classifiers such as feed-forward NNLMs (Ge et al., 2016), CNNs (Hitschler et al., 2017; Shrestha et al., 2017), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. O"
2021.findings-emnlp.172,2020.emnlp-main.673,1,0.832948,"eenman, 2009), and plagiarism detection (Stamatatos, 2009a). Previous work on AA has explored and combined various features and representations at different levels including n-grams (Escalante et al., 2011; Sapkota et al., 2015, 2016), POS-tags (Ferracane et al., 2017; Halvani et al., 2020) psycholinguistics features (Li et al., 2014; Uchendu et al., 2019), while recent approaches also build deep neural network based classifiers such as feed-forward NNLMs (Ge et al., 2016), CNNs (Hitschler et al., 2017; Shrestha et al., 2017), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. Our work aims to provide the first benchmark for Authorship Attribution in the form of the Turing Test by including humans and neural language models. Figure 3: The T URING B ENCH Environment. CTRL (Keskar et al., 2019), XLM (Lample and Conneau, 2019), XLNET (Yang et al., 2019), FAIR (Ng et al., 2019; Chen et al., 2020), TRANSFORMERXL (Dai et al., 2019), and PPLM (Dathathri et al., 2020). In"
2021.findings-emnlp.172,P16-1210,0,0.0219538,"Missing"
2021.findings-emnlp.172,2020.cl-2.8,0,0.0281792,"Given sio, 2014; Adelani et al., 2020), spam emails (Das 19 neural text-generators, there are 19 Turing Test and Verma, 2018). 2002 Automatic Detection of Generated Text Given the potential malicious applications of text generation (Solaiman et al., 2019), it is thus vital to build detectors to distinguish text generated by machines from humans (Gehrmann et al., 2019; Bakhtin et al., 2019; Jawahar et al., 2020; Varshney et al., 2020; Çano and Bojar, 2020). Most current work focus on fake news detection (Rashkin et al., 2017; Zhou et al., 2019; Bhat and Parthasarathy, 2020; Zhong et al., 2020; Schuster et al., 2020; Ippolito et al., 2020). Despite this progress, it remains a challenging task to build generalizable, interpretable, and robust detectors (Jawahar et al., 2020). Authorship Attribution Authorship Attribution (AA) aims to decide the author of a given text from a set of candidates (Houvardas and Stamatatos, 2006; Stamatatos, 2009b; Zhang et al., 2014). AA has a broad range of applications including author profiling (López-Monroy et al., 2020), computer forensics (Lambers and Veenman, 2009), and plagiarism detection (Stamatatos, 2009a). Previous work on AA has explored and combined various featu"
2021.findings-emnlp.172,K19-1079,0,0.0184381,"orld knowledge (Radsite to host leaderboards. This benchmark dataset ford et al., 2018, 2019; Keskar et al., 2019; Zellers is created by collecting 10K news articles (mostly et al., 2019; Deng et al., 2019; Brown et al., 2020). in politics) written by journalists in media outlets such as CNN, Washington Post, etc. Using the Ti- The progress in neural text generation has facilitated a wide range of applications: dialog response tle of each article, we Prompt 19 selected neural generation (Zhang et al., 2020), storytelling (Fan text-generators to generate an article similar to the et al., 2018; See et al., 2019), table-to-text generhuman-written one. This creates 200K articles with 20 labels (or authors). Next, we have two bench- ation (Lebret et al., 2016), code comment genermark tasks - Turing Test and Authorship Attribution. ation (Alon et al., 2018), medical report generation (Liu et al., 2019a). The Turing Test task is modeled after the Turing Test concept (Turing, 2009), where if a machine However, as these language models can genershows intelligent behavior or characteristics usu- ate text indistinguishable from human-written text, ally attributed to a human, then the machine has they can also"
2021.findings-emnlp.172,E17-2106,0,0.0286234,"s including author profiling (López-Monroy et al., 2020), computer forensics (Lambers and Veenman, 2009), and plagiarism detection (Stamatatos, 2009a). Previous work on AA has explored and combined various features and representations at different levels including n-grams (Escalante et al., 2011; Sapkota et al., 2015, 2016), POS-tags (Ferracane et al., 2017; Halvani et al., 2020) psycholinguistics features (Li et al., 2014; Uchendu et al., 2019), while recent approaches also build deep neural network based classifiers such as feed-forward NNLMs (Ge et al., 2016), CNNs (Hitschler et al., 2017; Shrestha et al., 2017), LSTMs (Jafariakinabad and Hua, 2019, 2020), and BERT-based models (Uchendu et al., 2020). However, previous AA work largely focuses on authorship attribution among humans, while only a few papers (Manjavacas et al., 2017; Uchendu et al., 2020; Munir et al., 2021) study neural generated text. Our work aims to provide the first benchmark for Authorship Attribution in the form of the Turing Test by including humans and neural language models. Figure 3: The T URING B ENCH Environment. CTRL (Keskar et al., 2019), XLM (Lample and Conneau, 2019), XLNET (Yang et al., 2019), FAIR (Ng et al., 2019; Ch"
2021.findings-emnlp.172,D18-1294,0,0.0210902,"Missing"
2021.findings-emnlp.172,2020.acl-demos.30,0,0.0418707,", benchmark tasks, and a web- ones, especially in terms of grammar, fluency, coherency, and usage of real world knowledge (Radsite to host leaderboards. This benchmark dataset ford et al., 2018, 2019; Keskar et al., 2019; Zellers is created by collecting 10K news articles (mostly et al., 2019; Deng et al., 2019; Brown et al., 2020). in politics) written by journalists in media outlets such as CNN, Washington Post, etc. Using the Ti- The progress in neural text generation has facilitated a wide range of applications: dialog response tle of each article, we Prompt 19 selected neural generation (Zhang et al., 2020), storytelling (Fan text-generators to generate an article similar to the et al., 2018; See et al., 2019), table-to-text generhuman-written one. This creates 200K articles with 20 labels (or authors). Next, we have two bench- ation (Lebret et al., 2016), code comment genermark tasks - Turing Test and Authorship Attribution. ation (Alon et al., 2018), medical report generation (Liu et al., 2019a). The Turing Test task is modeled after the Turing Test concept (Turing, 2009), where if a machine However, as these language models can genershows intelligent behavior or characteristics usu- ate text"
2021.findings-emnlp.172,2020.emnlp-main.193,0,0.0349176,"and machine labels. Given sio, 2014; Adelani et al., 2020), spam emails (Das 19 neural text-generators, there are 19 Turing Test and Verma, 2018). 2002 Automatic Detection of Generated Text Given the potential malicious applications of text generation (Solaiman et al., 2019), it is thus vital to build detectors to distinguish text generated by machines from humans (Gehrmann et al., 2019; Bakhtin et al., 2019; Jawahar et al., 2020; Varshney et al., 2020; Çano and Bojar, 2020). Most current work focus on fake news detection (Rashkin et al., 2017; Zhou et al., 2019; Bhat and Parthasarathy, 2020; Zhong et al., 2020; Schuster et al., 2020; Ippolito et al., 2020). Despite this progress, it remains a challenging task to build generalizable, interpretable, and robust detectors (Jawahar et al., 2020). Authorship Attribution Authorship Attribution (AA) aims to decide the author of a given text from a set of candidates (Houvardas and Stamatatos, 2006; Stamatatos, 2009b; Zhang et al., 2014). AA has a broad range of applications including author profiling (López-Monroy et al., 2020), computer forensics (Lambers and Veenman, 2009), and plagiarism detection (Stamatatos, 2009a). Previous work on AA has explored and"
2021.findings-emnlp.377,2021.naacl-main.109,0,0.0578886,"Missing"
2021.findings-emnlp.377,N18-2097,0,0.0178644,"equence Summarization Recent summarization models are based on Transformer (Vaswani et al., 2017) that has a quadratic time and memory complexity with respect to the input length, preventing it from being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong e"
2021.findings-emnlp.377,N19-1423,0,0.0170129,"to sliding window attention + global attention, which is more memory efficient. Longformer can accept up to 16K tokens and has shown improvement over long document summarization using its long-encoder-decoder (LED) variant. We allow the maximum input of 4,096 tokens for Longformer and cutoff the rest of the input, as we found further increasing such limit yields no improvements. To incorporate queries in QMSum for these endto-end models, we simply append the queries to the front of the meeting transcripts, as it is a standard practice for query-based summarization and also question answering (Devlin et al., 2019). 3.3 Experiment Setup For a fair comparison between all models, we fit all of the models into the same RTX 8000 GPU with 48 GiB of GPU memory. We adopt the fairseq3 implementation for BART, and the original code base for both Longformer4 and HMNet5 . We inherit the hyperparameters for all those models for fine-tuning in our experiments.6 Our most expensive experiments are fine-tuning for HMNet and Longformer, which take around 8 hours, while the runtime for BART model is less than one hour. We use ROUGE (Lin, 2004) as our main evaluation metric and pyrouge library7 as the ROUGE implementation"
2021.findings-emnlp.377,P19-1102,1,0.852432,"eries at the beginning of the input. For the input to the two models, we use the gold relevant text spans given a query in QMSum to avoid the influences of retrieval models. The results show that encoding queries has a large impact on both types of models, especially for BART, even if the gold utterances are given. 4.4 Transfer Ability between Different Tasks Pretraining has been shown effective for document summarization by introducing external knowledge As we discussed, some dialogues (e.g., QMSum) from other similar tasks (Hermann et al., 2015; contain more than 20k tokens. They exceed the Fabbri et al., 2019). We hypothesize that it is input limitation of most existing summarization especially important for dialogue summarization models. In this section, we further analyze the per- because the dataset size is usually small. Thereformance of summarization models as the input fore, we study the transfer learning between diflength changes. To compare the robustness be- ferent dialogue summarization tasks via pretraintween two types of models (mainly BART and HM- ing. Tab. 3 shows the performance of BART-large Net), we divide the test dialogues by the number of models that are pretrained using differe"
2021.findings-emnlp.377,D19-5409,0,0.102612,"ong et al., 2021; Zhu et al., 2021). Dia- els such as Longformer (Beltagy et al., 2020), and logue summarization aims to generate a short sum- several dialogue utterance retrieval methods for a mary for long dialogues to help the readers capture retrieve-then-summarize pipeline model, as well important information more efficiently. as hierarchical dialogue encoding models. For the A number of existing works on dialogue sum- specific challenges in dialogues, we explore difmarization focus on extracting the main events of ferent datasets for pretraining to test the transfera short conversation (Gliwa et al., 2019; Rohde ability between similar summarization tasks. We et al., 2021). However, unlike the short dialogues evaluate these models on three recent long dia∗ logue summarization datasets: QMSum for meetEqual Contribution. ‡ The work was done when Asli was at MSR. ings (Zhong et al., 2021), MediaSum for inter4426 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4426–4433 November 7–11, 2021. ©2021 Association for Computational Linguistics views (Zhu et al., 2021), SummScreen for TV series transcripts (Chen et al., 2021). In our experiments, we find that the pipeline met"
2021.findings-emnlp.377,D19-5406,0,0.0131131,"017) that has a quadratic time and memory complexity with respect to the input length, preventing it from being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong et al., 2021) is a query-based multi-domain meeting summarization dataset annotated by humans. It contai"
2021.findings-emnlp.377,2020.acl-main.703,0,0.0305473,"contains less than 20 utterances, some tasks Abstract for summarizing much longer dialogues have been proposed recently (Chen et al., 2021; Zhong et al., Dialogue summarization helps readers capture 2021). These datasets are usually derived from salient information from long conversations in meetings and interviews, with hundreds of turns meetings, interviews, and TV series. However, in each dialogue. The length of such dialogues real-world dialogues pose a great challenge typically exceeds the input limits imposed by reto current summarization models, as the diacent transformer-based models (Lewis et al., 2020), logue length typically exceeds the input limmaking it difficult to train an end-to-end summaits imposed by recent transformer-based pretrained models, and the interactive nature of rization model for such tasks. This poses the chaldialogues makes relevant information more lenge: How can we effectively use the current neucontext-dependent and sparsely distributed ral summarization models on dialogues that greatly than news articles. In this work, we perexceed their length limits? form a comprehensive study on long dialogue Additionally, compared with document summasummarization by investigati"
2021.findings-emnlp.377,W04-1013,0,0.0552362,"practice for query-based summarization and also question answering (Devlin et al., 2019). 3.3 Experiment Setup For a fair comparison between all models, we fit all of the models into the same RTX 8000 GPU with 48 GiB of GPU memory. We adopt the fairseq3 implementation for BART, and the original code base for both Longformer4 and HMNet5 . We inherit the hyperparameters for all those models for fine-tuning in our experiments.6 Our most expensive experiments are fine-tuning for HMNet and Longformer, which take around 8 hours, while the runtime for BART model is less than one hour. We use ROUGE (Lin, 2004) as our main evaluation metric and pyrouge library7 as the ROUGE implementation throughout all experiments. 4 Result and Analysis Here we demonstrate our findings in four corresponding subsections. We also show some concrete examples and perform qualitative analysis in § 4.5 4.1 Dealing with Long Dialogues We compare several methods for addressing the long input issue for dialogue summarization, including different utterance retrieval methods describe in § 3.2.1 for a retrieve-then-summarize framework, heuristics for shortening the dialogue 3 https://github.com/pytorch/fairseq https://github.c"
2021.findings-emnlp.377,D18-1206,0,0.0283295,"his work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu et al. Tab. 1 shows the statistics for these three (2020b) described a hierarchical model for both long dialogue datasets. Additionally, we also inner- and cross-utterance attention, while Chen consider CNN/Dailymail (Hermann et al., 2015) and Yang (2020) proposed a multi-view decoder (CNN/DM), XSum (Narayan et al., 2018), and to leverage different extracted views of dialogues, SAMSum (Gliwa et al., 2019) as datasets for presuch as topic view and stage view. training in our experiments. 4427 3.2 Models 3.2.1 Retrieve-then-summarize Pipeline Dialogues tend to be relatively long, and most existing summarization models cannot process such long inputs. The two-stage retrieve-thensummarize pipeline first retrieves the most relevant subtext in the dialogue and then feeds to a summarizer. We experiment with the following retrievers: • TF-IDF (Jones, 1972) Based on bag-of-words representation, TF-IDF measuers term fre"
2021.findings-emnlp.377,P17-1098,0,0.0268736,"., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu et al. Tab. 1 shows the statistics for these three (2020b) described a hierarchical model for both long dialogue datasets. Additionally, we also inner- and cross-utterance attention, while Chen consider CNN/Dailymail (Hermann et al., 2015) and Yang (2020) proposed a multi-view decoder (CNN/DM), XSum (Narayan et al., 2018), and to leverage different extracted views of dialogues,"
2021.findings-emnlp.377,P18-1062,0,0.0550762,"Missing"
2021.findings-emnlp.377,P19-1212,0,0.0228442,"m being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong et al., 2021) is a query-based multi-domain meeting summarization dataset annotated by humans. It contains 1,808 queries together with 232 long meeting transcripts, with topics as software product, a"
2021.findings-emnlp.377,2020.findings-emnlp.19,1,0.927658,"ontains annotated gold spans which could be used as the gold labels for training the retrievers; MediaSum (Zhu et al., 2021) is a large-scale media interview dataset consisting of 463.6K transcripts Dialogue Summarization Dialogue summariza- collected from NPR and CNN. Because MediaSum tion aims to generate concise summaries for dia- contains short summaries, i.e. only a short sentence logues, such as meetings (McCowan et al., 2005; representing the topic, we only use this dataset for Janin et al., 2003; Zhong et al., 2021; Shang et al., pretraining and analysis. Due to the huge size of 2018; Zhu et al., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et"
2021.findings-emnlp.377,2020.coling-main.39,0,0.0374974,"e summariza- collected from NPR and CNN. Because MediaSum tion aims to generate concise summaries for dia- contains short summaries, i.e. only a short sentence logues, such as meetings (McCowan et al., 2005; representing the topic, we only use this dataset for Janin et al., 2003; Zhong et al., 2021; Shang et al., pretraining and analysis. Due to the huge size of 2018; Zhu et al., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu"
2021.findings-emnlp.377,2021.naacl-main.474,1,0.874688,"ty can be further improved with In this paper, we systematically investigate these a stronger retrieval model and pretraining on issues on dialog summarization: we first explore proper external summarization datasets. the various solutions to the lengthy input problem. Then, we analyze and compare the methods to im1 Introduction prove generic summarization models on challengLarge amount of dialogue data have been produced ing dialogue datasets. To address the long input in meetings, TV series, and interviews (Chen et al., issue, we investigate extended transformer mod2021; Zhong et al., 2021; Zhu et al., 2021). Dia- els such as Longformer (Beltagy et al., 2020), and logue summarization aims to generate a short sum- several dialogue utterance retrieval methods for a mary for long dialogues to help the readers capture retrieve-then-summarize pipeline model, as well important information more efficiently. as hierarchical dialogue encoding models. For the A number of existing works on dialogue sum- specific challenges in dialogues, we explore difmarization focus on extracting the main events of ferent datasets for pretraining to test the transfera short conversation (Gliwa et al., 2019; Rohde ability b"
2021.mrl-1.24,N19-1423,0,0.0237212,"r leveraging relevance judgments in English to train dense retrievers for document retrieval in nonEnglish languages. Our experimental results show that combining dense retrieval and term-matching retrieval can obtain effectiveness improvements. Also, weakly-supervised target language transfer yields effectiveness competitive to generationbased target language transfer. This extended abstracted is an abridged version of Shi et al. (2021). 2 Cross-Lingual Relevance Transfer Model-Based Transfer. By exploiting the zeroshot cross-lingual transfer ability of pretrained transformers such as mBERT (Devlin et al., 2019), we train the dense retriever in the source language and apply inference directly to the target languages. Target Language Transfer. To bridge the language gap between training and inference, we explore two techniques for creating a target language transfer set. (1) Generation-based query synthesis, where the goal is to leverage powerful generation models to predict reasonable queries given documents in the target language. We choose mBART (Liu et al., 2020) as our query generation model. The input of the model is the passage and its learning target is the corresponding query. We use the tran"
2021.mrl-1.24,2020.emnlp-main.550,0,0.0465571,"thout manual annotation effort by treating the titles of Wikipedia articles as queries and the corresponding documents as positive candidates. We also retrieve top 1000 documents with BM25 for each query; documents except for the positive candidate are labeled as negative. Two-Stage Training. We apply two-stage training to learn the dense retrieval model. The encoders are first trained on source language (English) annotated data which are available in larger quantities; then the models are fine-tuned on the synthesized query– document pairs in the target language. We leverage the DPR model of Karpukhin et al. (2020), but using mBERT as the backbone model. During inference, we apply both bag-of-words exact term matching and dense retrieval. The rel- 3 Experimental Setup evance score of each document combines termmatching scores with dense retrieval similarity via We conduct experiments on six test collections: NTSdoc “ α ¨ Sterm ` p1 ´ αq ¨ Sdense , where α is CIR 8 in Chinese, TREC 2002 in Arabic, CLEF tuned via cross-validation. 2006 in French, FIRE 2012 in Hindi, FIRE 2012 251 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 251–253 November 11, 2021. ©2021 Association for"
2021.mrl-1.24,2020.tacl-1.47,0,0.0413747,"ce Transfer Model-Based Transfer. By exploiting the zeroshot cross-lingual transfer ability of pretrained transformers such as mBERT (Devlin et al., 2019), we train the dense retriever in the source language and apply inference directly to the target languages. Target Language Transfer. To bridge the language gap between training and inference, we explore two techniques for creating a target language transfer set. (1) Generation-based query synthesis, where the goal is to leverage powerful generation models to predict reasonable queries given documents in the target language. We choose mBART (Liu et al., 2020) as our query generation model. The input of the model is the passage and its learning target is the corresponding query. We use the translate–train technique to obtain the generation models. More specifically, we leverage Google Translate to translate English query– document pairs into the target languages. Then, we use passages in the target language collections as input and generate corresponding queries in the same language. (2) Weakly-supervised query synthesis. We can automatically build the target language transfer set without manual annotation effort by treating the titles of Wikipedia"
2021.mrl-1.24,2021.mrl-1.24,1,0.0530913,"Missing"
2021.naacl-main.37,D19-1052,0,0.0377301,"Missing"
2021.naacl-main.37,2020.acl-main.708,0,0.0149657,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,2020.findings-emnlp.190,0,0.0356274,"Missing"
2021.naacl-main.37,2020.acl-main.18,0,0.0285505,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,P19-1483,0,0.0424379,"Missing"
2021.naacl-main.37,W19-8652,0,0.0305433,"Missing"
2021.naacl-main.37,L18-1544,0,0.0274201,"e structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defined in the domain specific ontologies E2E and WebNLG were built on (Table 2). We also intro1 Introduction duce a novel tree ontology annotation approach on Automatically generating textual descriptions from tables, which converts a flat table schema into a"
2021.naacl-main.37,D19-1428,0,0.0202842,"generating textual descriptions from tables, which converts a flat table schema into a structured data improves the accessibility of knowl- tree structured semantic frame. The tree ontology edge bases to lay users. Such applications include reflects the core and auxiliary relations in the table explaining data records to non-experts (Cawsey schema, and naturally occurs across many domains. et al., 1997), writing sports news (Chen and As a result, DART provides high-quality sentence Mooney, 2008), summarizing information in mul- annotations to tree structured semantic frames extiple documents (Fan et al., 2019), and generating tracted from various data sources, including Wikdialogue responses (Wen et al., 2015). iSQL (Zhong et al., 2017) and WikiTableQuestions While significant progress has been made in this (Pasupat and Liang, 2015), two open-domain quesfield, there are still several issues with existing tion answering datasets, as well as E2E (Novikova Data-to-Text datasets. First, they adopt a flat ontol- et al., 2017b) and WebNLG (Gardent et al., 2017) ogy structure of the data, such as slot-value pairs (Figure 1). We evaluated several state-of-the-art for data records (Lebret et al., 2016; Novi"
2021.naacl-main.37,W17-3518,0,0.177339,"itates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defi"
2021.naacl-main.37,2020.acl-main.703,0,0.0345274,"Missing"
2021.naacl-main.37,P09-1011,0,0.106929,"Missing"
2021.naacl-main.37,2020.findings-emnlp.165,0,0.0746934,"Missing"
2021.naacl-main.37,N19-1236,0,0.0269971,"Missing"
2021.naacl-main.37,D17-1238,0,0.0117771,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P19-1195,0,0.0491104,"Missing"
2021.naacl-main.37,W07-2315,0,0.135635,"Missing"
2021.naacl-main.37,D19-1314,0,0.0450819,"Missing"
2021.naacl-main.37,2020.acl-main.704,0,0.020086,"Missing"
2021.naacl-main.37,W17-5525,0,0.0286406,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P17-2002,0,0.0678008,"Missing"
2021.naacl-main.37,2020.emnlp-main.89,0,0.0169662,"ork effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the natu"
2021.naacl-main.37,T87-1019,0,0.795136,"Missing"
C14-1016,J81-4005,0,0.763958,"Missing"
C14-1016,D08-1094,0,0.0315364,"Missing"
C14-1016,P12-1092,0,0.659285,"et al., 2013b). While word embedding plays an increasingly important role in many tasks, most of word embedding models, which assume one embedding vector for each individual word, suffer from a critical limitation for modeling tremendous polysemous words (e.g. bank, left, doctor). Using the same embedding vector to represent the different meanings (we will call prototype of a word in the rest of the paper) of a polysemous word is somehow unreasonable and sometimes it even hurts the model’s expression ability. To address this problem, some recent efforts, such as (Reisinger and Mooney, 2010) (Huang et al., 2012), have investigated how to obtain multi embedding vectors for the respective different prototypes of a polysemous word. Specifically, these works usually take a two-step approach: they first train single prototype word representations through a multi-layer neural network with the assumption that one word only yields single word embedding; then, they identify multi word embeddings for each polysemous word by clustering all its context window features, which are usually computed as the average of single prototype embeddings of its neighboring words in the context window. Compared with traditiona"
C14-1016,W13-3512,0,0.016965,"on between the ranking of ground truth similarity scores (given by human labeling) and the ranking based on the similarity scores produced by the model. Traditional word similarity tasks such as WordSim353 (Finkelstein et al., 2001) and RG (Rubenstein and Goodenough, 1965) are not suitable for evaluating multi-prototype models since there is neither enough number of polysemous words in these datasets nor context information to infer the prototype index. To address this issue, a new word similarity benchmark dataset including context information was released in (Huang et al., 2012). Following (Luong et al., 2013), we use SCWS to denote this dataset. Similar to WordSim353, SCWS contains some word pairs (concretely, 2003 pairs), together with human labeled similarity scores for these word pairs. What makes SCWS different from WS353 is that the words in SCWS are contained in sentences, i.e., there are 2003 pairs of sentences containing these words, while words in WS353 are not associated with sentences. Therefore, the human labeled scores are based on the meanings of the words in the context. Given the presence of the context, the word similarity scores, especially those scores depending on polysemous wo"
C14-1016,I11-1079,0,0.00868633,"importance of considering multiprototype models. Note that (Reisinger and Mooney, 2010) also proposes to deal with the word polysemy problem by assigning to each prototype a real value vector. However their embedding vectors are obtained through a tf-idf counting model, which is usually called as distributional representations (Turian et al., 2010), rather than through a neural network. Therefore, we do not regard their paper as very related to our work. The similar statement holds for other works on vector model for word meaning in context such as (Erk and Pad´o, 2008) (Thater et al., 2011) (Reddy et al., 2011) (Van de Cruys et al., 2011). Our model is mainly based on the recent proposed Word2Vec model, more concretely, the continuous Skip-Gram model (Mikolov et al., 2013a) (Mikolov et al., 2013b). The continuous Skip-Gram model specifies the probability of observing the context words conditioned on the central word wI in the window via a three-layer neural network. With less parameters to train (thus higher scalability), Word2Vec discovers interesting analogical semantic relations between words like Japan - Tokyo = France - Paris. 152 3 Model Description In this section, we introduce our algorithm"
C14-1016,N10-1013,0,0.814936,"ikolov et al., 2010) (Mikolov et al., 2013b). While word embedding plays an increasingly important role in many tasks, most of word embedding models, which assume one embedding vector for each individual word, suffer from a critical limitation for modeling tremendous polysemous words (e.g. bank, left, doctor). Using the same embedding vector to represent the different meanings (we will call prototype of a word in the rest of the paper) of a polysemous word is somehow unreasonable and sometimes it even hurts the model’s expression ability. To address this problem, some recent efforts, such as (Reisinger and Mooney, 2010) (Huang et al., 2012), have investigated how to obtain multi embedding vectors for the respective different prototypes of a polysemous word. Specifically, these works usually take a two-step approach: they first train single prototype word representations through a multi-layer neural network with the assumption that one word only yields single word embedding; then, they identify multi word embeddings for each polysemous word by clustering all its context window features, which are usually computed as the average of single prototype embeddings of its neighboring words in the context window. Com"
C14-1016,I11-1127,0,0.0244953,"Missing"
C14-1016,P10-1040,0,0.162466,"word’s all context words’ features in the corpus. The features are the embedding vectors trained previously via a three-layer neural network. Each cluster’s centroid is regarded as the embedding vector for each prototype. Their reported experimental results verify the importance of considering multiprototype models. Note that (Reisinger and Mooney, 2010) also proposes to deal with the word polysemy problem by assigning to each prototype a real value vector. However their embedding vectors are obtained through a tf-idf counting model, which is usually called as distributional representations (Turian et al., 2010), rather than through a neural network. Therefore, we do not regard their paper as very related to our work. The similar statement holds for other works on vector model for word meaning in context such as (Erk and Pad´o, 2008) (Thater et al., 2011) (Reddy et al., 2011) (Van de Cruys et al., 2011). Our model is mainly based on the recent proposed Word2Vec model, more concretely, the continuous Skip-Gram model (Mikolov et al., 2013a) (Mikolov et al., 2013b). The continuous Skip-Gram model specifies the probability of observing the context words conditioned on the central word wI in the window vi"
C14-1016,D11-1094,0,0.0168259,"Missing"
D18-1123,councill-etal-2008-parscit,0,0.0377989,"ublication list. We make the following contributions: (i) We create a dataset of 2,500 homepages1 , in which each publication string is labeled. (ii) We address the problem of publication string extraction by end-to-end learning, without feature engineering. (iii) We propose a model that can learn the structures of publication lists and the styles of publication strings. We also propose an alternating training method that can reduce overfitting and further improve the prediction accuracy. 2 Related Work Earlier studies extract publication strings from research papers (Peng and McCallum, 2006; Councill et al., 2008; Tkaczyk et al., 2015). Such a problem is simpler for two reasons: (i) The reference list of a research paper usually appears at a fixed position (e.g., end of paper) and is continuous. (ii) The references are usually well-formatted and have few format variations since they may be generated by software such as LATEX. For extracting publication strings from academic homepages, previous studies use either rule-based (Hong et al., 2009; Yang and Ho, 2010) or a hybrid of machine learning and rule-based methods (Chung et al., 2012). For example, Chung et al. (2012) develop a system named PRM that"
D18-1123,D17-1256,0,0.0273563,"Missing"
D18-1123,W09-3609,0,0.151801,"earchers like to list some of their publications with more details, such as full venue names, volume and page information, while listing the other publication in a concise way. Also, some researchers like to group their publication by year or by topic. (ii) A publication string may contain multiple lines of text (cf. Figure 1), and there may not be a clear boundary between two publications strings. (iii) There may be strings in an academic homepage that share very similar structures and styles with publication strings, such as records of conference presentations (cf. Figure 1). Previous work (Hong et al., 2009; Chung et al., 2012) focuses on feature and rule engineering and cannot accommodate the above challenges. To address these challenges, we propose a model named PubSE to extract every publication string from an academic homepage. PubSE has two characteristics: (i) The model structure reflects the structure of a list of publications, by its loss-functions at both text line-level and webpage-level. (ii) The training process of the 1005 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1005–1010 c Brussels, Belgium, October 31 - November 4, 2018. 2018 A"
D18-1123,E17-2068,0,0.0321192,"Missing"
D18-1123,D14-1181,0,0.00509166,"Missing"
D18-1123,N16-1030,0,0.11026,"Missing"
D18-1123,I17-1056,0,0.029789,"Missing"
D18-1123,P16-1101,0,0.0514327,"Missing"
D18-1123,N16-1174,0,0.0467258,"Missing"
D18-1193,Q13-1005,0,0.076636,"Missing"
D18-1193,W13-2322,0,0.0317059,"Missing"
D18-1193,P14-1133,0,0.030183,"edicts NONE instead, it will be pushed into the stack. The stack pops NONE at next step. For example, in Figure 2, the current popped token is SELECT, which is a instance of keyword (KW) type. It calls the COL module to predict a column name, which will be pushed to the stack. 4.6 Data Augmentation Even though Spider already has a significantly larger number of complex queries than existing datasets, the number of training examples for some complex SQL components is still limited. A widely used way is to conduct data augmentation to generate more training examples automatically. Many studies (Berant and Liang, 2014; Iyer et al., 2017; Su and Yan, 2017) have shown that data augmentation can bring significant improvement in performance. In prior work, data augmentation was typically performed within a single domain dataset. We propose a cross-domain data augmentation method to expand our training data for complex queries. Cross-domain data augmentation is more difficult than the in-domain setting because question-program pairs tend to have domain specific words and phrases. To tackle this issue, we first create a list of universal patterns for questionSQL pairs, based on the human labeled pairs from all t"
D18-1193,N10-1138,0,0.0462004,"Missing"
D18-1193,P16-1004,0,0.137291,"Berant and Liang, 2014; Pasupat and Liang, 2015). As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the"
D18-1193,P18-1068,0,0.531592,"ems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the target program. Then the model fills in missing details in the sketch. Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rule"
D18-1193,C12-2040,0,0.0386433,"th the previous best models. 2 Related Work Semantic parsing maps natural language to formal meaning representations. There are a range of representations, such as logic forms and executable programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq"
D18-1193,P17-1089,0,0.406209,"Missing"
D18-1193,P17-1105,0,0.171224,"q2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the target program. Then the model fills in missing details in the sketch. Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many"
D18-1193,Q14-1030,0,0.0397021,"Missing"
D18-1193,D17-1127,0,0.0638207,"the stack. The stack pops NONE at next step. For example, in Figure 2, the current popped token is SELECT, which is a instance of keyword (KW) type. It calls the COL module to predict a column name, which will be pushed to the stack. 4.6 Data Augmentation Even though Spider already has a significantly larger number of complex queries than existing datasets, the number of training examples for some complex SQL components is still limited. A widely used way is to conduct data augmentation to generate more training examples automatically. Many studies (Berant and Liang, 2014; Iyer et al., 2017; Su and Yan, 2017) have shown that data augmentation can bring significant improvement in performance. In prior work, data augmentation was typically performed within a single domain dataset. We propose a cross-domain data augmentation method to expand our training data for complex queries. Cross-domain data augmentation is more difficult than the in-domain setting because question-program pairs tend to have domain specific words and phrases. To tackle this issue, we first create a list of universal patterns for questionSQL pairs, based on the human labeled pairs from all the different training databases in Spi"
D18-1193,P11-1060,0,0.113297,"Missing"
D18-1193,D15-1166,0,0.0322332,"ral previous state-of-the-art models in the text-to-SQL task. As the dataset and task definition used in this work are fundamentally different from prior work using datasets such as GeoQuery, WikiSQL, we adapted these models to our task in the same way as (Yu et al., 2018b). Specifically: Seq2Seq with Attention or Copying In order to make the models aware of the table schema information, Yu et al. (2018b) pass the models with a vocabulary that contains SQL keywords and column names of the given database. (Iyer et al., 2017) Iyer et al. (2017) apply an attention based seq2seq model similar to (Luong et al., 2015) to text-to-SQL tasks. Yu et al. (2018b) adapt their model without user interaction to the task. SQLNet & TypeSQL Xu et al. (2017) introduce SQLNet, which employs a column attention mechanism and a sketch-based method to generates SQL queries as a slot-filling task. Yu et al. (2018a) improves SQLNet by utilizing word types extracted from a knowledge graph or table content 1659 Method Seq2Seq Seq2Seq+Attention (Dong and Lapata, 2016) Seq2Seq+Copying Iyer et al. (2017) SQLNet (Xu et al., 2017) TypeSQL (Yu et al., 2018a) SyntaxSQLNet -augment -wikiSQL -augment -table -wikiSQL -augment -history -t"
D18-1193,P15-1142,0,0.255475,"Missing"
D18-1193,D14-1162,0,0.081044,"ve 5 clauses. Exact matching score is 1 if the model predicts all clauses correctly for a given example. To better understand model performance on different queries, (Yu et al., 2018b) divide SQL queries into 4 levels: easy, medium, hard, extra hard. The definition of difficulty is based on the number of SQL components, selections, and conditions. Queries that contain more SQL keywords are considered harder. 5.2 Experimental Settings Our model is implemented in PyTorch (Paszke et al., 2017). We build each module based on the TypeSQL (Yu et al., 2018a) implementation. We use pre-trained GloVe (Pennington et al., 2014) embeddings for all question, SQL history, and schema tokens. All word embeddings are fixed. For each experiment, the dimension and dropout rate of all hidden layers is set to 120 and 0.3 respectively. We use Adam (Kingma and Ba, 2015) with the default hyperparameters for optimization, with a batch size of 64. The same loss functions in (Xu et al., 2017) are used. • Training data: Spider (plus examples from 6 existing datasets) + WikiSQL + data augmentation • Model architecture: history path + tableaware column encoding We will conduct ablation studies to analyze the effect of each of the prop"
D18-1193,C04-1021,0,0.849862,"Missing"
D18-1193,J82-3002,0,0.499483,"Missing"
D18-1193,P07-1121,0,0.200758,"Missing"
D18-1193,W16-0105,0,0.0488845,"; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-t"
D18-1193,P17-1041,0,0.186096,"taset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the target program. Then the model fills in missing details in the sketch. Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based mo"
D18-1193,N18-2093,1,0.222984,"split (Zettlemoyer and Collins, 2005), most queries in the test set also appear in the train set. The WikiSQL dataset recently developed by (Zhong et al., 2017) is much larger and does use different databases for training and testing, but it only contains very simple SQL queries and 1653 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1653–1663 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics OP ROOT Action Modules database schemas. To address those issues in the current semantic parsing datasets, Yu et al. (2018b) have developed a large-scale human labeled text-to-SQL dataset consisting of about 6,000 complex SQL queries and 200 databases with multiple tables. This dataset defines a new complex and cross-domain text-to-SQL task that requires models to generalize well to both new SQL queries and databases. The task cannot be solved easily without truly understanding the semantic meanings of the input questions. In this paper, we propose SyntaxSQLNet, a SQL specific syntax tree network to address the aforementioned task. Specifically, to generate complex SQL queries with multiple clauses, selections an"
D18-1425,Q13-1005,0,0.0373234,"oreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2"
D18-1425,W13-2322,0,0.031249,"ins 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advis"
D18-1425,P14-1133,0,0.0534807,"x SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). Thes"
D18-1425,H94-1010,0,0.896485,"ry given the input question, models need to understand both the natural language question and relationships between tables and columns in the database schema. In addition, we also propose a new task for text-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATI"
D18-1425,N10-1138,0,0.0214231,"ext-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp"
D18-1425,P16-1004,0,0.159092,"hat no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been stu"
D18-1425,P18-1068,0,0.473951,"ets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to new domains. Most importantly, these datasets have a limited number of labeled logic forms or SQL queries. In order to expand the size of these datasets and apply neural network approaches, each logic form or SQL query has about 4"
D18-1425,P11-1060,0,0.0682285,". Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazad"
D18-1425,L18-1491,0,0.0935667,"Missing"
D18-1425,P16-1057,0,0.118169,"Missing"
D18-1425,D15-1166,0,0.0429896,"Missing"
D18-1425,C12-2040,0,0.0602249,"semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to new domains. Most importantly, these datasets have a limited number of label"
D18-1425,C04-1021,0,0.615026,"Missing"
D18-1425,P17-1089,0,0.429303,"Missing"
D18-1425,P16-1002,0,0.0994049,"Missing"
D18-1425,H90-1020,0,0.075333,"e the SQL query given the input question, models need to understand both the natural language question and relationships between tables and columns in the database schema. In addition, we also propose a new task for text-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled"
D18-1425,P15-1085,0,0.0422304,"opment sets. Also, the task needs to take different table schemas as inputs. Therefore, the model has to generalize to new databases. However, in order to generate about 90,000 questions and SQL pairs for about 26,000 databases, Zhong et al. (2017) made simplified assumptions about the SQL queries and databases. Their SQL labels only cover single SELECT column and aggregation, and WHERE conditions. Moreover, all the databases only contain single tables. No JOIN, GROUP BY, and ORDER BY, etc. are included. Recently, researchers have constructed some datasets for code generation including IFTTT (Quirk et al., 2015), DJANGO (Oda et al., 2015), HEARTHSTONE (Ling et al., 2016), NL2Bash (Lin et al., 2018), and CoNaLa (Yin et al., 2018). Database Collection & Creation 200 databases (DB) 150 man-hours SQL Review 150 man-hours Question Review and Paraphrase 150 man-hours Question and SQL Annotation 20-50 examples per DB 500 man-hours Question Review & Paraphrase 150 man-hours Final Review & Processing 150 man-hours Figure 2: The annotation process of our Spider corpus. These tasks parse natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling e"
D18-1425,P17-1105,0,0.0897459,"et al., 2015), HEARTHSTONE (Ling et al., 2016), NL2Bash (Lin et al., 2018), and CoNaLa (Yin et al., 2018). Database Collection & Creation 200 databases (DB) 150 man-hours SQL Review 150 man-hours Question Review and Paraphrase 150 man-hours Question and SQL Annotation 20-50 examples per DB 500 man-hours Question Review & Paraphrase 150 man-hours Final Review & Processing 150 man-hours Figure 2: The annotation process of our Spider corpus. These tasks parse natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). 3 Corpus Construction All questions and SQL queries were written and reviewed by 11 computer science students who were native English speakers.As illustrated in Figure 2, we develop our dataset in five steps, spending around 1,000 hours of human labor in total: §3.1 Database Collection and Creation, §3.2 Question and SQL Annotation, §3.3 SQL Review, §3.4 Question Review and Paraphrase, §3.5 Final Question and SQL Review. 3.1 Database Collection and Creation Collecting databases with complex schemas is hard. Although relational databases are widely used in industry and"
D18-1425,Q14-1030,0,0.0204325,"dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Z"
D18-1425,P17-1041,0,0.248117,"NE (Ling et al., 2016), NL2Bash (Lin et al., 2018), and CoNaLa (Yin et al., 2018). Database Collection & Creation 200 databases (DB) 150 man-hours SQL Review 150 man-hours Question Review and Paraphrase 150 man-hours Question and SQL Annotation 20-50 examples per DB 500 man-hours Question Review & Paraphrase 150 man-hours Final Review & Processing 150 man-hours Figure 2: The annotation process of our Spider corpus. These tasks parse natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). 3 Corpus Construction All questions and SQL queries were written and reviewed by 11 computer science students who were native English speakers.As illustrated in Figure 2, we develop our dataset in five steps, spending around 1,000 hours of human labor in total: §3.1 Database Collection and Creation, §3.2 Question and SQL Annotation, §3.3 SQL Review, §3.4 Question Review and Paraphrase, §3.5 Final Question and SQL Review. 3.1 Database Collection and Creation Collecting databases with complex schemas is hard. Although relational databases are widely used in industry and academia, most of them"
D18-1425,N18-2093,1,0.771968,"(2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to new domains. Most importantly, these datasets have a limited number of labeled logic forms or SQL queries. In order to expand the size of these datasets and apply neura"
D18-1425,J82-3002,0,0.411092,"e is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to"
D18-1425,P07-1121,0,0.0807639,"ropose a new task for text-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jag"
D19-1537,W10-2903,0,0.0605657,"lumn) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-drive"
D19-1537,H94-1010,0,0.712423,"xample. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction goal, where the ann"
D19-1537,N19-1423,0,0.0255756,"of user utterance and column headers. dorms have a TV louge (b) Utterance Encoder. Bi LSTM Concatennation Attention over Utterance Tokens Self-Attention Among Table Columns Bi LSTM Bi LSTM Bi LSTM Bi LSTM Bi LSTM dorm . id dorm . name has . dorm id has . amenity id amenity Bi LSTM . id amenity . name (c) Table Encoder. Figure 2: Utterance-Table Encoder for the example in (a). Utterance-Table BERT Embedding. We consider two options as the input to the first layer biLSTM. The first choice is the pretrained word embedding. Second, we also consider the contextualized word embedding based on BERT (Devlin et al., 2019). To be specific, we follow Hwang et al. (2019) to concatenate the user utterance and all the column headers in a single sequence separated by the [SEP] token: [CLS], Xi , [SEP], c1 , [SEP], . . . , cm , [SEP] This sequence is fed into the pretrained BERT model whose hidden states at the last layer is used as the input embedding. 3.2 The hidden state of this interaction encoder hI encodes the history as the interaction proceeds. Turn Attention When issuing the current utterance, the user may omit or explicitly refer to the previously mentioned information. To this end, we adopt the turn attent"
D19-1537,P16-1004,0,0.0241913,"2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Frie"
D19-1537,P18-1068,0,0.1126,"meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-attention between the two as illustrated in Figure 2. Figure 2b shows the utterance encoder. For the user utterance at each turn, we first use a bi-LSTM to encode utterance tokens. The bi-LSTM hidden state is fed into a dot-product attention layer (Luong et al., 2015) over the column header embeddings. For each utterance token embedding, we get an attention weighted average of the column header embeddings to obtain the most relevant columns (Dong and Lapata, 2018). We then concatenate the bi-LSTM hidden state and the column attention vector, and use a second layer biLSTM to generate the utterance token embedding hE . Figure 2c shows the table encoder. For each column header, we concatenate its table name and its column name separated by a special dot token (i.e., table name . column name). Each column header is processed by a bi-LSTM layer. To better capture the internal structure of the table schemas (e.g., foreign key), we then employ a selfattention (Vaswani et al., 2017) among all column headers. We then use an attention layer to capture the relati"
D19-1537,P18-1033,1,0.853357,"L queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et a"
D19-1537,N18-1177,0,0.0515028,"Missing"
D19-1537,P16-1154,0,0.0390467,", we predict a switch pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; A"
D19-1537,D18-1188,0,0.0130342,"r et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to"
D19-1537,P19-1444,0,0.46871,"Missing"
D19-1537,P18-1124,0,0.0623869,"Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) m"
D19-1537,P17-1097,0,0.0224668,"(Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an i"
D19-1537,H90-1021,0,0.51482,"unge’) Table 3: SParC example. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction"
D19-1537,N18-2115,0,0.0394036,"017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) an"
D19-1537,P17-1089,0,0.0436623,"sing executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018;"
D19-1537,D18-1192,0,0.0123607,"while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 201"
D19-1537,P17-1167,0,0.0294541,"al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE"
D19-1537,D19-1624,0,0.0439311,"r which takes bag-of-words representations of column headers as input. They also modify the decoder to select between a SQL keyword or a column header. (2) SyntaxSQL-con: This is adapted from the original context-agnostic SyntaxSQLNet (Yu et al., 2018b) by using bi-LSTMs to encode the interaction history including the utterance and the associated SQL query response. It also employs a column attention mechanism to compute representations of the previous question and SQL query. Spider. We compare with the results as reported in Yu et al. (2018b). Furthermore, we also include recent results from Lee (2019) who propose to use recursive decoding procedure, Bogin SQLNet (Xu et al., 2017) SyntaxSQLNet (Yu et al., 2018b) +data augmentation (Yu et al., 2018b) Lee (2019) GNN (Bogin et al., 2019) IRNet (Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate repr"
D19-1537,P16-1138,0,0.0348913,"promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating"
D19-1537,D15-1166,0,0.0400847,"), (X2 , Y2 ), . . . , (Xt−1 , Yt−1 )] Furthermore, in the cross-domain setting, each interaction is grounded to a different database. Therefore, the model is also given the schema of the current database as an input. We consider relational databases with multiple tables, and each table contains multiple column headers: T = [c1 , c2 , . . . , cl , . . . , cm ] where m is the number of column headers, and each cl consists of multiple words including its table name and column name (§ 3.1). 3 Methodology We employ an encoder-decoder architecture with attention mechanisms (Sutskever et al., 2014; Luong et al., 2015) as illustrated in Figure 1. The framework consists of (1) an utterance-table encoder to explicitly encode the user utterance and table schema at each turn, (2) A turn attention incorporating the recent history for decoding, (3) a table-aware decoder taking into account the context of the utterance, the table schema, and the previously generated query to make editing decisions. 3.1 Utterance-Table Encoder An effective encoder captures the meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-att"
D19-1537,P96-1008,0,0.628325,"ations by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Y"
D19-1537,P15-1142,0,0.0267292,"ith context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015). Since both SCONE and SequentialQA are annotated with only denotations but not query labels, they don’t include many questions with rich semantic and contextual types. For example, SequentialQA (Iyyer et al., 2017) requires that the answer to follow-up questions must be a subset of previous answers, and most of the questions can be answered by simple SQL queries with SELECT and WHERE clauses. Concurrent with our work, Yu et al. (2019a) introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting. Each dialogue in CoSQL simulates a DB"
D19-1537,D17-1127,0,0.0360468,"oyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Su"
D19-1537,P18-1193,0,0.0113637,"17; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segme"
D19-1537,N18-1203,0,0.0305507,"Missing"
D19-1537,P18-1034,0,0.0408402,"languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Mill"
D19-1537,P15-1129,0,0.0350818,"tences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions int"
D19-1537,D14-1162,1,0.106791,"(Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries. 5.3 Implementation Details Our model is implemented in PyTorch (Paszke et al., 2017). We use pretrained 300-dimensional GloVe (Pennington et al., 2014) word embedding. All LSTM layers have 300 hidden size, and we use 1 layer for encoder LSTMs, and 2 layers for decoder LSTMs. We use the ADAM optimizer (Kingma and Ba, 2015) to minimize the tokenlevel cross-entropy loss with a batch size of 16. Model parameters are randomly initialized from a uniform distribution U [−0.1, 0.1]. The main model has an initial learning rate of 0.001 and it will be multiplied by 0.8 if the validation loss increases compared with the previous epoch. When using BERT instead of GloVe, we use the pretrained small uncased BERT model with 768 hidden size5 , and we fine t"
D19-1537,P17-1099,0,0.0417505,"pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer,"
D19-1537,D18-1197,0,0.0120536,"ost of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames whic"
D19-1537,P17-1041,0,0.060978,"iously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2"
D19-1537,N18-2093,1,0.932754,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,P09-1110,0,0.0185736,"still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQu"
D19-1537,D18-1193,1,0.950706,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,D18-1425,1,0.89856,"Missing"
D19-1537,P19-1443,1,0.875363,"Missing"
I11-1088,D10-1027,0,0.0182218,"pplications, the head of a hyperedge is usually a single node. This allows a single hyperedge to be semantically equivalent to a tree node with its children. Once a source language sentence is parsed into a packed forest and pruned, the next step is to find trees in the forest that have matching rules in the translation rule dictionary. Matching rules are used to produce a translation forest for decoding into a target language string. The forest is a hypergraph made up of hyperedges where each hyperedge has a single source node, while the rules in the rule dictionary are trees. Recent work by Huang and Mi (2010) has shown that forest expansion can be done in decoders using beam search (Koehn, 2004). However, to the best of our knowledge, no index-based search structures for incrementally finding trees have been proposed to date. 786 forest is a hypergraph made up of hyperedges, the left-hand-sides of rules in the rule dictionary are actually trees. Each node in a forest can have zero or more outgoing hyperedges. A hyperedge is be treated as a tree node with its children. Source sentence trees are constructed by recursively expanding nodes in the forest. Figure 2a shows a tree constructed with three h"
I11-1088,W06-3601,0,0.0176102,"st collections are extracted from NIST (2010a; 2010b) data. 2 Figure 1: A translation rule reproduced from (Mi and Huang, 2008) 2.2 Forest Based Translation Forest-based SMT overcomes the limitations of parse errors in tree-based translation and has been shown to be faster than k-best tree-based translation (Mi et al., 2008). For translating a sentence using the forest-based translation technique, we require a parser that can process a source language sentence and produce a packed forest and a translation rule dictionary, or database, which is a collection of tree-to-string translation rules (Huang et al., 2006; Liu et al., 2006). Background A translation rule is a mapping from a source language tree to a string in the target language. An example translation rule from Chinese to English is shown in Figure 1. The left-hand-side is the source language tree. The Chinese word yˇu is translated to with in the target side on the right. The xi variables on the right-hand-side are placeholders for the corresponding elements in the tree. Other numerical parameters associated with each translation rule are not shown here. Note that there may be several rules in the rule dictionary that have identical source l"
I11-1088,W01-1812,0,0.0425496,"are placeholders for the corresponding elements in the tree. Other numerical parameters associated with each translation rule are not shown here. Note that there may be several rules in the rule dictionary that have identical source language trees but different translations. In this section we present the fundamentals of packed forests and forest-based translation before briefly describing the B-tree index structure. 2.1 Packed Forest Packed forests or forests are directed hypergraphs and have been used to model and represent several applications in computer science and discrete mathematics (Klein and Manning, 2001). Directed hypergraphs can be defined as a pair: H = (V, E) where V = {v1 , v2 , · · · , vn } is the set of nodes and E = {E1 , E2 , · · · , Em } is the set of hyperedges. Each hyperedge can be defined as a pair: Ei = (Xi , Yi ) |Xi , Yi ⊆ V, i = 1, 2, · · · , m. Packed forests have been used in NLP in the area of sentence parsing (Gallo et al., 1993) where the propositions of a parse analysis correspond to the nodes in the hypergraph and rules are represented as hyperedges. A similar model is used in forest based machine translation, where multiple parses of the input sentence are modelled as"
I11-1088,koen-2004-pharaoh,0,0.0318684,"o be semantically equivalent to a tree node with its children. Once a source language sentence is parsed into a packed forest and pruned, the next step is to find trees in the forest that have matching rules in the translation rule dictionary. Matching rules are used to produce a translation forest for decoding into a target language string. The forest is a hypergraph made up of hyperedges where each hyperedge has a single source node, while the rules in the rule dictionary are trees. Recent work by Huang and Mi (2010) has shown that forest expansion can be done in decoders using beam search (Koehn, 2004). However, to the best of our knowledge, no index-based search structures for incrementally finding trees have been proposed to date. 786 forest is a hypergraph made up of hyperedges, the left-hand-sides of rules in the rule dictionary are actually trees. Each node in a forest can have zero or more outgoing hyperedges. A hyperedge is be treated as a tree node with its children. Source sentence trees are constructed by recursively expanding nodes in the forest. Figure 2a shows a tree constructed with three hyperedges, having head nodes A, B and F. Figure 2b shows the typical string representati"
I11-1088,P06-1077,0,0.0364607,"xtracted from NIST (2010a; 2010b) data. 2 Figure 1: A translation rule reproduced from (Mi and Huang, 2008) 2.2 Forest Based Translation Forest-based SMT overcomes the limitations of parse errors in tree-based translation and has been shown to be faster than k-best tree-based translation (Mi et al., 2008). For translating a sentence using the forest-based translation technique, we require a parser that can process a source language sentence and produce a packed forest and a translation rule dictionary, or database, which is a collection of tree-to-string translation rules (Huang et al., 2006; Liu et al., 2006). Background A translation rule is a mapping from a source language tree to a string in the target language. An example translation rule from Chinese to English is shown in Figure 1. The left-hand-side is the source language tree. The Chinese word yˇu is translated to with in the target side on the right. The xi variables on the right-hand-side are placeholders for the corresponding elements in the tree. Other numerical parameters associated with each translation rule are not shown here. Note that there may be several rules in the rule dictionary that have identical source language trees but d"
I11-1088,D08-1022,0,0.0473526,"Missing"
I11-1088,P08-1023,0,0.106786,"describe an algorithm that allows incremental search for trees in a forest and show that its performance is orders of magnitude faster than iterative search. A B-tree index is used to store the rule dictionaries. Prefix-compressed indexes with a large page size are found to provide a balance of fast search and disk space utilisation. 1 Introduction Statistical machine translation (SMT) uses machine learning and parallel corpora to perform translations automatically. Syntax based SMT systems can be broadly classified into two types based on the input to the system: tree-based and string-based (Mi et al., 2008). In a tree-based system, the input is a parse tree of the source language, whereas in the latter, the input is a sequence of words that is simultaneously parsed and translated. Forest-based translation employs multiple parse trees for each source language sentence. Forest-based translation can be performed in three main steps. First, the input sentence is parsed into a packed forest, which is then pruned. Next, for each tree in the packed forest, all matching translation rules are found. These translation rules are then combined to form a translation forest. The translation forest is finally"
K17-1045,W09-1802,0,0.254656,"Missing"
K17-1045,H01-1065,0,0.128073,"Dragomir Radev1 1 Department of Computer Science, Yale University 2 The LNM Institute of Information Technology {michihiro.yasunaga,r.zhang,kshitijh.meelu}@yale.edu {ayush.original}@gmail.com {krishnan.srinivasan,dragomir.radev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the re"
K17-1045,N09-1041,0,0.62727,"Missing"
K17-1045,C16-1053,0,0.0291462,"summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006; Hong and Nenkova, 2014), among others, to rank sentence 452 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, thanks to the large scale news article datasets (Hermann et al., 2015), Cheng and Lapata (2016) train an extractive summarization system with attention-based encoder-decoder RNNs to sequentially label summary-worth sentences in single documents. See et al. (2017), adopting an abstractive approach, augment the standard attention-based encoder-decoder RNNs with the ability to copy words from the source text via pointing and to kee"
K17-1045,hong-etal-2014-repository,0,0.102031,"repeated trials. Note that the axis is only displaying the interval - 7.0. thevertical four experiments. The vertical axis 4.0 displays the validation costs in the interval 4.0 - 7.0. art systems related to our regression method. We 1 Introduction compute ROUGE scores from the actual output Cosine PDG ADG Similarity summary of each system. We run the G-Flow code released by Christensen et al. (2013) to get Number of nodes 265 265 265 the output summary of the G-Flow system. The Number of edges 1023 1050 884 output summary of other systems are compiled in Average edge weight 0.075 0.295 0.359 Hong et al. (2014). To ensure fair comparison, we Average node degree 0.171 5.136 2.260 use ROUGE-1.5.5 with the same parameters as in Hong et al. (2014) across all methods: -n 2 -m -l ρ of degree and salience 0.136 0.113 0.093 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0. Table 5: Characteristics of the three graph repreFrom Table 3, we observe that our GCN syssentations, averaged over the clusters (i.e. graphs) tem significantly outperforms the commonly used in DUC 2004. Note that max edge weight in all baselines and traditional graph approaches such three representations is 1.0 due to rescaling for as Centroid, Lex"
K17-1045,P16-1046,0,0.319592,"mmary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. This work proposes a multi-document summarization system that exploits the rep"
K17-1045,E14-1075,0,0.456827,"idocument summarization systems. 1 Introduction Document summarization aims to produce fluent and coherent summaries covering salient information in the documents. Many previous summarization systems employ an extractive approach by identifying and concatenating the most salient text units (often whole sentences) in the document. Traditional extractive summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006; Hong and Nenkova, 2014), among others, to rank sentence 452 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, thanks to the large scale news article datasets (Hermann et al., 2015), Cheng and L"
K17-1045,W14-1504,0,0.0931152,"Missing"
K17-1045,N13-1136,0,0.641372,"nks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. This work proposes a multi-document summarization system that exploits the representational power of deep neural networks and the sentence relation information encoded in graph representations of document clusters. Specifically, we apply Graph Convolutional Networks (Kipf and Welling, 2017) on sentence relation graphs. First, we discuss three different techniques to produce sentence relation graphs, where nodes represent sentences in a cluster and edges capture the connections betwe"
K17-1045,P03-1069,0,0.0387029,"n Srinivasan1 Dragomir Radev1 1 Department of Computer Science, Yale University 2 The LNM Institute of Information Technology {michihiro.yasunaga,r.zhang,kshitijh.meelu}@yale.edu {ayush.original}@gmail.com {krishnan.srinivasan,dragomir.radev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between s"
K17-1045,P13-1099,0,0.0286741,"based Neural Multi-Document Summarization Michihiro Yasunaga1 Rui Zhang1 Kshitijh Meelu1 Ayush Pareek2 Krishnan Srinivasan1 Dragomir Radev1 1 Department of Computer Science, Yale University 2 The LNM Institute of Information Technology {michihiro.yasunaga,r.zhang,kshitijh.meelu}@yale.edu {ayush.original}@gmail.com {krishnan.srinivasan,dragomir.radev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all"
K17-1045,P06-2020,0,0.0973184,"Missing"
K17-1045,I05-2004,0,0.0184367,", and extract salient sentences in a greedy manner while avoiding redundancy. We evaluate our model on the DUC 2004 multidocument summarization (MDS) task. Our model shows a clear advantage over traditional graphbased extractive summarizers, as well as a baseline GRU model that does not use any graph, and achieves competitive results with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices"
K17-1045,D15-1044,0,0.497344,"adev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. This work proposes"
K17-1045,P17-1099,0,0.0387102,", August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, thanks to the large scale news article datasets (Hermann et al., 2015), Cheng and Lapata (2016) train an extractive summarization system with attention-based encoder-decoder RNNs to sequentially label summary-worth sentences in single documents. See et al. (2017), adopting an abstractive approach, augment the standard attention-based encoder-decoder RNNs with the ability to copy words from the source text via pointing and to keep track of what has been summarized. These models (Cheng and Lapata, 2016; See et al., 2017) achieve state-of-the-art performance on the DUC 2002 single-document summarization task. However, scaling up these RNN sequence-to-sequence approaches to the multidocument summarization task has not been successful, 1) due to the lack of large multi-document summarization datasets needed to train the computationally expensive sequence-t"
K17-1045,N06-2046,0,0.0602558,"ences in a greedy manner while avoiding redundancy. We evaluate our model on the DUC 2004 multidocument summarization (MDS) task. Our model shows a clear advantage over traditional graphbased extractive summarizers, as well as a baseline GRU model that does not use any graph, and achieves competitive results with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices in information networ"
K17-1045,N16-1007,0,0.0128408,"nt ) (Cho et al., 2014; Chung et al., 2014) and extract the last hidden state as the sentence embedding. We then apply Graph Convolutional Networks (Kipf and Welling, 2017) on the sentence relation graph with the sentence embeddings as the input node features, to produce final sentence embeddings that reflect the graph representation. Thereafter, a second level GRU (GRUdoc ) produces the entire cluster embedding Summarization Using Neural Networks Neural networks have recently been popular for text summarization (K˚ageb¨ack et al., 2014; Rush et al., 2015; Yin and Pei, 2015; Cao et al., 2016; Wang and Ling, 2016; Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017). For example, Rush et al. (2015) introduce a neural attention feed-forward network-based model for sentence compression. Wang and Ling (2016) 453 Sentences d1s1 doc1 GRUdoc Sentence Relation Graph h0 h1 GRUdoc h2 h0 h1 h2 d1s2 Cluster doc2 d2s2 C Sentence Embedding d2s1 h0 h1 h2 h3 h4 w1 w2 w3 . GRUsent Cluster Embedding Estimated Scores Graph Convolutional Networks Salience Estimation Figure 1: Illustration of our architecture for sentence salience estimation. In this example, there are two documents in the cluster and"
K17-1045,K16-1028,0,0.0389677,"nd extract the last hidden state as the sentence embedding. We then apply Graph Convolutional Networks (Kipf and Welling, 2017) on the sentence relation graph with the sentence embeddings as the input node features, to produce final sentence embeddings that reflect the graph representation. Thereafter, a second level GRU (GRUdoc ) produces the entire cluster embedding Summarization Using Neural Networks Neural networks have recently been popular for text summarization (K˚ageb¨ack et al., 2014; Rush et al., 2015; Yin and Pei, 2015; Cao et al., 2016; Wang and Ling, 2016; Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017). For example, Rush et al. (2015) introduce a neural attention feed-forward network-based model for sentence compression. Wang and Ling (2016) 453 Sentences d1s1 doc1 GRUdoc Sentence Relation Graph h0 h1 GRUdoc h2 h0 h1 h2 d1s2 Cluster doc2 d2s2 C Sentence Embedding d2s1 h0 h1 h2 h3 h4 w1 w2 w3 . GRUsent Cluster Embedding Estimated Scores Graph Convolutional Networks Salience Estimation Figure 1: Illustration of our architecture for sentence salience estimation. In this example, there are two documents in the cluster and each document has two sentences. Sentences are p"
K17-1045,W12-2601,0,0.0111739,"how mean (and standard deviation for R-1) over 10 repeated trials for each of our experiments. We use the benchmark data sets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. Table 2 shows the statistics of the data sets. We use DUC 2001, 2002, 2003 and 2004 containing 30, 59, 30 and 50 clusters of nearly 10 documents each respectively. Our model is trained on DUC 2001 and 2002, validated on 2003, and tested on 2004. For evaluation, we use the ROUGE-1,2 metric, with stemming and stop words not removed as suggested by Owczarzak et al. (2012). 4.2 38.57 GRU layers (L = 3). The hidden states in GRUsent , GCN hidden layers, and GRUdoc are all 300dimensional vectors (D = F = 300). The rescaling factor α in the objective function (Eq 13) is chosen as 40 from {10, 20, 30, 40, 50, 100} based on the validation performance. The objective function is optimized using Adam (Kingma and Ba, 2015) stochastic gradient descent with a learning rate of 0.001 and a batch size of 1. We use gradient clipping with a maximum gradient norm of 1.0. The model is validated every 10 iterations, and the training is stopped early if the validation performance"
K17-1045,W00-1009,1,0.606346,"etitive results with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices in information networks and achieve improved results on MDS. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences (Mann and Thompson, 1988). In our work, we build on the Approximate Dis"
K17-1045,radev-etal-2004-mead,1,0.810013,"ph, and it achieves competitive results against other state-of-the-art multidocument summarization systems. 1 Introduction Document summarization aims to produce fluent and coherent summaries covering salient information in the documents. Many previous summarization systems employ an extractive approach by identifying and concatenating the most salient text units (often whole sentences) in the document. Traditional extractive summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006; Hong and Nenkova, 2014), among others, to rank sentence 452 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, tha"
K17-1045,H01-1056,1,0.539887,"ts with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices in information networks and achieve improved results on MDS. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences (Mann and Thompson, 1988). In our work, we build on the Approximate Discourse Graph (ADG) mo"
N16-1177,P15-1162,0,0.0761047,"Missing"
N16-1177,P14-1062,0,0.794263,"e models and Convolutional Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows. Introduction Sentence and document modeling systems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats However, despite their ability to account for word orders, order-sensitive"
N16-1177,D14-1181,0,0.412854,"Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows. Introduction Sentence and document modeling systems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats However, despite their ability to account for word orders, order-sensitive models base"
N16-1177,C02-1150,0,0.891809,"st applies independent LSTM networks to each subsentence. Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences. We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio e"
N16-1177,P15-1107,0,0.0409797,"Missing"
N16-1177,P15-2029,0,0.368428,"stems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats However, despite their ability to account for word orders, order-sensitive models based on neural networks still suffer from several disadvantages. First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015). Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem (Iyyer et al., 2015), recursive models require heavy label1512 Proceedings of NAACL-HLT 2016, pages 1512–1521, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ing on phrases to add supervisions on internal nodes. Furthermore, parsing is restricted to sentences and it is unclear how to model paragraphs and documents using recursive neural networks. In CNN models, convolutional operators process word vectors sequentially using small windows. Thus sentences a"
N16-1177,P11-1015,0,0.707441,"013) NB (Socher et al., 2013) NBSVM-bi (Wang and Manning, 2012) SVMS (Silva et al., 2011) Standard-RNN (Socher et al., 2013) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) Standard-LSTM (Tai et al., 2015) bi-LSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) SA-LSTM (Dai and Le, 2015) DCNN (Kalchbrenner et al., 2014) CNN-MC (Kim, 2014) MVCNN (Yin and Sch¨utze, 2015) Dep-CNN (Ma et al., 2015) Neural-BoW (Kalchbrenner et al., 2014) DAN (Iyyer et al., 2015) Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc) (Dahl et al., 2012) Full+Unlabeled+BoW(bnc) (Maas et al., 2011) DSCNN DSCNN-Pretrain MR — — 79.4 — — 79.0 — — — — — 80.7 — 81.1 — 81.9 — 80.3 — — — 81.5 82.2 SST-2 79.4 81.8 — — 82.4 82.9 85.4 86.6 86.7 86.8 88.0 — 86.8 88.1 89.4 — 80.5 86.3 87.8 — — 89.1 88.7 SST-5 40.7 41.0 — — 43.2 44.4 45.7 49.8 45.8 49.1 51.0 — 48.5 47.4 49.6 49.5 42.4 47.7 48.7 — — 49.7 50.6 TREC — — — 95.0 — — — — — — — — 93.0 92.2 — 95.4 88.2 — — — — 95.4 95.6 SUBJ — — 93.2 — — — — — — — — — — 93.2 93.9 — — — — — 88.2 93.2 93.9 IMDB — — 91.2 — — — — — — — — 92.8 — — — — — 89.4 92.6 89.2 88.9 90.2 90.7 Table 1: Experiment results of DSCNN compared with other models. Performance is"
N16-1177,P04-1035,0,0.656454,"a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences. We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), whe"
N16-1177,P05-1015,0,0.769896,"ses. As for document modeling (Figure 2), DSCNN first applies independent LSTM networks to each subsentence. Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences. We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word"
N16-1177,D14-1162,0,0.0996574,"ask SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding. In this formulation, instead of using one-hot vectors by indexing words into a vocabulary, word embeddings are learned by projecting words onto a low dimensional and dense vector space that encodes both semantic and syntactic features of words. Given word embeddings, different models have been proposed to learn the composition of words to build up phrase and sentence representations. Most methods fall into three types: unordered models, sequence models, and Convolutional Neural Networks models. In unordered models,"
N16-1177,D12-1110,0,0.485927,"ives the schematic for the hierarchy. 5 5.1 Experiments Datasets Movie Review Data (MR) proposed by (Pang and Lee, 2005) is a dataset for sentiment analysis of movie reviews. The dataset consists of 5,331 positive and 5,331 negative reviews, mostly in one sentence. We follow the practice of using 10-fold cross validation to report results. Stanford Sentiment Treebank (SST) is another popular sentiment classification dataset introduced Method SVM (Socher et al., 2013) NB (Socher et al., 2013) NBSVM-bi (Wang and Manning, 2012) SVMS (Silva et al., 2011) Standard-RNN (Socher et al., 2013) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) Standard-LSTM (Tai et al., 2015) bi-LSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) SA-LSTM (Dai and Le, 2015) DCNN (Kalchbrenner et al., 2014) CNN-MC (Kim, 2014) MVCNN (Yin and Sch¨utze, 2015) Dep-CNN (Ma et al., 2015) Neural-BoW (Kalchbrenner et al., 2014) DAN (Iyyer et al., 2015) Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc) (Dahl et al., 2012) Full+Unlabeled+BoW(bnc) (Maas et al., 2011) DSCNN DSCNN-Pretrain MR — — 79.4 — — 79.0 — — — — — 80.7 — 81.1 — 81.9 — 80.3 — — — 81.5 82.2 SST-2 79.4 81.8 — — 82.4 82.9 85.4 86.6"
N16-1177,D13-1170,0,0.538442,"pproach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification. 1 By contrast, order-sensitive models based on neural networks are becoming increasingly popular thanks to their ability to capture word order information. Many prevalent order-sensitive neural models can be categorized into two classes: Recursive models and Convolutional Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows. Introduction Sentence"
N16-1177,P15-1150,0,0.216506,"ns in an order-sensitive way. For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al., 2015). Besides, RNN and LSTM can be both converted to tree-structured networks by using parsing information. For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the standard RNN structured by syntactic trees to the sentiment analysis task. (Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units. Recently, CNN-based models have demonstrated remarkable performances on sentence modeling and classification tasks. Leveraging convolution operators, these models can extract features from variablelength phrases corresponding to different filters. For example, DCNN in (Kalchbrenner et al., 2014) constructs hierarchical features of sentences by onedimensional convolution and dynamic k-max pooling. (Yin and Sch¨utze, 2015) further utilizes multichannel embeddings and unsupervised pretraining to"
N16-1177,P12-2018,0,0.224402,"Missing"
N16-1177,D15-1199,0,0.0120628,"4) adds an additional hidden layer on top of the averaged word embeddings before the softmax layer for classification purposes. In contrast, sequence models, such as standard Recurrent Neural Networks (RNN) and Long ShortTerm Memory networks, construct phrase and sentence representations in an order-sensitive way. For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al., 2015). Besides, RNN and LSTM can be both converted to tree-structured networks by using parsing information. For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the standard RNN structured by syntactic trees to the sentiment analysis task. (Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units. Recently, CNN-based models have demonstrated remarkable performances on sentence modeling and classification tasks. Leveraging convolution operators, these models can extract features from variablelength phrases"
N16-1177,K15-1021,0,0.120126,"Missing"
N18-2093,Q13-1005,0,0.0901207,"k Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012;"
N18-2093,W13-2322,0,0.0388136,"in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; G"
N18-2093,P14-1133,0,0.0450292,"ngful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Data"
N18-2093,N10-1138,0,0.0312634,"igure 2). By grouping different slots in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Po"
N18-2093,P17-1089,0,0.160066,"Missing"
N18-2093,P17-1105,0,0.105436,". We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al.,"
N18-2093,Q14-1030,0,0.0326972,"al language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b)."
N18-2093,P11-1060,0,0.0463556,"ing different slots in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003,"
N18-2093,P16-1057,0,0.112214,"Missing"
N18-2093,J82-3002,0,0.779252,"oney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequenceto-sequence approach to converting text to logical forms. Most of previous work focus on specific table schemas, which means they us"
N18-2093,P15-1142,0,0.269664,"s. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jag"
N18-2093,P07-1121,0,0.157012,"slot filling problem (Figure 2). By grouping different slots in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and"
N18-2093,D14-1162,0,0.0805578,"Missing"
N18-2093,C04-1021,0,0.729361,"Missing"
N18-2093,W16-0105,0,0.0419702,"2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequenceto-sequence approach to converting text to logical forms. Most of previous work focus on specific table schemas, which means they use a single database in both train and test. Thus, they don’t generalize to new databases. Zhong et al. (2017) publish the WikiSQL dataset and propose a sequence-to-sequence model with reinforcement learning to generate SQL queries. In the problem definition of the WikiSQL task, the databases in the test set do not appear in the train and develo"
N18-2093,P17-1041,0,0.245392,"on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 201"
N18-2093,P16-1004,0,\N,Missing
N18-2093,C12-2040,0,\N,Missing
P16-1062,P13-2045,1,0.926109,"arget” or “Sky light.” In contrast, people writing a descriptive caption for a photograph can adopt a less creative style. Corpora may also differ on how similar texts within a particular 654 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 654–665, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics metric for short text clustering without varying the clustering method. Yan et al. (2012) proposed an alternative term weighting scheme to use in place of tf-idf when clustering using non-negative matrix factorization. King et al. (2013) used the cosine similarity between feature vectors that included context word and part-of-speech features and spelling features and applied Louvain clustering to the resulting graph. Xu et al. (2015) used a convolutional neural network to represent short texts and found that, when used with the k-means clustering algorithm, this deep semantic representation outperformed tf-idf, Laplacian eigenmaps, and average embeddings for clustering. Other papers focused on choosing the best clustering method for short texts, but kept the similarity metric constant. Rangrej et al. (2011) compared k-means,"
P16-1062,D15-1177,0,0.0149647,"a wide variety of ways, leading to data that can benefit from different similarity metrics than less creative texts. At the same time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and"
P16-1062,W05-0904,0,0.0523056,"ought vectors. In each case, we represent texts as vectors and find their cosine similarities; if cosine similarity can be negative, we add one and normalize by two to ensure similarity in the range [0, 1]. N -Gram Counts. First we consider n-gram count vectors. We use three variations: (1) unigrams, (2) unigrams and bigrams, and (3) unigrams, bigrams, and trigrams. N -Gram tf-idf. We also consider weighting n-grams by tf-idf, as calculated by sklearn (Pedregosa et al., 2011). Dependency Counts. Grammatical information has been found to be useful in text, particularly short text, similarity. (Liu and Gildea, 2005; Zhang et al., 2005; Wang et al., 2009b; Heilman ˇ c et al., and Smith, 2010; Tian et al., 2010; Sari´ 2012; Tai et al., 2015). To leverage this information, previous work has used dependency kernels (Tian et al., 2010), which measure similarity by the fraction of identical dependency parse segments between two sentences. Here, we accomplish the same effect using a count vector for each sentence, with the dependency parse segments as the vocabulary. We define the set of segments for a dependency parse to consist of, for each word, the word, its parent, and the dependency relation that connect"
P16-1062,W15-1505,0,0.0275372,"an less creative texts. At the same time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short"
P16-1062,D15-1181,0,0.0149005,"ing to data that can benefit from different similarity metrics than less creative texts. At the same time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and how tightly rela"
P16-1062,N10-1145,0,0.0603791,"Missing"
P16-1062,P82-1020,0,0.494212,"Missing"
P16-1062,P15-1150,0,0.0740451,"Missing"
P16-1062,P11-1110,1,0.883281,"Missing"
P16-1062,W10-0721,0,0.0173689,"ting texts in response to the same stimulus. In a corpus of texts relating to several stimuli, it may be desirable to cluster according to which stimulus each text relates to—for instance, grouping all of the news headlines about the same event together. Here, we consider texts triggered by several types of stimuli: photographs that need descriptive captions, cartoons that need humorous captions, and crossword answers that need original clues. Each need shapes the properties of the texts. Pascal and Flickr Captions. The Pascal Captions dataset (hereinafter PAS) and the 8K ImageFlickr dataset (Rashtchian et al., 2010) are sets of captions solicited from Mechanical Turkers for photographs from Flickr and from the PatRelated Work The most similar work to the present paper is Shrestha et al. (2012), which acknowledged that the similarity metric and the clustering method could both contribute to clustering results. It compared four similarity methods and also tested four clustering methods. Unlike the present work, it did not consider distributional semantics-based similarity measures or similarity measures that incorporated deep learning. In addition, it reported that the characteristics of the corpora “overs"
P16-1062,P15-2058,0,0.0308967,"time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts. We examine several such propert"
P16-1062,S12-1060,0,0.0737265,"Missing"
P16-1062,W15-1509,0,0.032151,"s of the 54th Annual Meeting of the Association for Computational Linguistics, pages 654–665, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics metric for short text clustering without varying the clustering method. Yan et al. (2012) proposed an alternative term weighting scheme to use in place of tf-idf when clustering using non-negative matrix factorization. King et al. (2013) used the cosine similarity between feature vectors that included context word and part-of-speech features and spelling features and applied Louvain clustering to the resulting graph. Xu et al. (2015) used a convolutional neural network to represent short texts and found that, when used with the k-means clustering algorithm, this deep semantic representation outperformed tf-idf, Laplacian eigenmaps, and average embeddings for clustering. Other papers focused on choosing the best clustering method for short texts, but kept the similarity metric constant. Rangrej et al. (2011) compared k-means, singular value decomposition, and affinity propagation for tweets, finding affinity propagation the most effective, using tf-idf with cosine similarity or Jaccard for a similarity measure. Errecalde e"
P16-1062,I05-1034,0,0.0378551,"case, we represent texts as vectors and find their cosine similarities; if cosine similarity can be negative, we add one and normalize by two to ensure similarity in the range [0, 1]. N -Gram Counts. First we consider n-gram count vectors. We use three variations: (1) unigrams, (2) unigrams and bigrams, and (3) unigrams, bigrams, and trigrams. N -Gram tf-idf. We also consider weighting n-grams by tf-idf, as calculated by sklearn (Pedregosa et al., 2011). Dependency Counts. Grammatical information has been found to be useful in text, particularly short text, similarity. (Liu and Gildea, 2005; Zhang et al., 2005; Wang et al., 2009b; Heilman ˇ c et al., and Smith, 2010; Tian et al., 2010; Sari´ 2012; Tai et al., 2015). To leverage this information, previous work has used dependency kernels (Tian et al., 2010), which measure similarity by the fraction of identical dependency parse segments between two sentences. Here, we accomplish the same effect using a count vector for each sentence, with the dependency parse segments as the vocabulary. We define the set of segments for a dependency parse to consist of, for each word, the word, its parent, and the dependency relation that connects them as shown in E"
P16-1062,W10-0707,0,\N,Missing
P16-1062,I13-1085,0,\N,Missing
P18-1033,W14-2402,0,0.0135844,"methodology apply broadly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from"
P18-1033,P14-1133,0,0.0222958,"dly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research"
P18-1033,D17-1151,0,0.0125685,"data. We call this a questionbased data split. However, many English questions may correspond to the same SQL query. If at least one copy of every SQL query appears in training, then the task evaluated is classification, not true semantic parsing, of the English questions. We can increase the number of distinct SQL queries by varying 5.1 Systems Recently, a great deal of work has used variations on the seq2seq model. We compare performance of a basic seq2seq model (Sutskever et al., 2014), and seq2seq with attention over the input (Bahdanau et al., 2015), implemented with TensorFlow seq2seq (Britz et al., 2017). We also extend that model to include an attention-based copying option, similar to Jia and Liang (2016). Our output vocabulary for the decoder includes a special token, COPY. If COPY has the highest probability at step t, we replace it with the input token with the 355 O O city0 O city1 Flight from Denver to Boston bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot. The hidden states of the LSTM at each end of the sentence are passed through a small feed-forward network to det"
P18-1033,P16-1004,0,0.431621,"aluation methodology for this task. In the process, we (1) introduce a new, challenging dataset, (2) standardize and fix many errors in existing datasets, and (3) propose a simple yet effective baseline system.1 ∗ The first two authors contributed equally to this work. Code and data is available at https://github. com/jkkummerfeld/text2sql-data/ 1 351 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 351–360 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics sumptions about the output structure (Dong and Lapata, 2016). One challenge for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-en"
P18-1033,C12-2040,0,0.151263,"Missing"
P18-1033,P17-1089,0,0.251203,"mputational Linguistics sumptions about the output structure (Dong and Lapata, 2016). One challenge for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-end approaches (Yin et al., 2016; Neelakantan et al., 2017), which we do not consider here. One key contribution of this paper is standardization of a range of datasets, to help address the challenge of limited data resources. challenging form of ambiguity from the task. In the process, we apply extensive effort to standardize datasets and fix a range of errors. Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses. In this paper, we provide new a"
P18-1033,P16-1002,0,0.420886,"me SQL query. If at least one copy of every SQL query appears in training, then the task evaluated is classification, not true semantic parsing, of the English questions. We can increase the number of distinct SQL queries by varying 5.1 Systems Recently, a great deal of work has used variations on the seq2seq model. We compare performance of a basic seq2seq model (Sutskever et al., 2014), and seq2seq with attention over the input (Bahdanau et al., 2015), implemented with TensorFlow seq2seq (Britz et al., 2017). We also extend that model to include an attention-based copying option, similar to Jia and Liang (2016). Our output vocabulary for the decoder includes a special token, COPY. If COPY has the highest probability at step t, we replace it with the input token with the 355 O O city0 O city1 Flight from Denver to Boston bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot. The hidden states of the LSTM at each end of the sentence are passed through a small feed-forward network to determine the SQL template to use. This architecture is simple and enables a joint choice of the tags and th"
P18-1033,P17-2017,1,0.806873,"hers were written by CS students with knowledge of the database who were instructed to write questions they might ask in an academic advising appointment. The authors manually labeled the initial set of questions with SQL. To ensure high quality, at least two annotators scored each questionquery pair on a two-point scale for accuracy— did the query generate an accurate answer to the question?—and a three-point scale for helpfulness—did the answer provide the information the asker was probably seeking? Cases with low scores were fixed or removed from the dataset. We collected paraphrases using Jiang et al. (2017)’s method, with manual inspection to ensure accuracy. For a given sentence, this produced paraphrases with the same named entities (e.g. course number EECS 123). To add variation, we annotated entities in the questions and queries with their types—such as course name, department, or instructor—and substituted randomly-selected values of each type into each paraphrase and its corresponding query. This combination of paraphrasing and entity replacement means an original question of “For next semester, who is teaching EECS 123?” can give rise to “Who teaches MATH 456 next semester?” as well as “W"
P18-1033,W00-1317,0,0.498716,"text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food types, and locations. Scholar (Iyer et al., 2017) User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct. Academic (Li and Jagadish, 2014) Questions about the Microsoft Academic Search (MAS) database, derived by enumerating every logical query that could be expressed using the search page of the MAS website and writing sentences to match them. The domain is similar to that of Scholar, but their schemas differ. 352 Yelp and IMDB (Yaghmazadeh et"
P18-1033,P11-1060,0,0.0827939,"ns about evaluation methodology apply broadly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and ap"
P18-1033,P15-1128,0,0.0259465,"pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly avai"
P18-1033,W16-0105,0,0.0149045,"e for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-end approaches (Yin et al., 2016; Neelakantan et al., 2017), which we do not consider here. One key contribution of this paper is standardization of a range of datasets, to help address the challenge of limited data resources. challenging form of ambiguity from the task. In the process, we apply extensive effort to standardize datasets and fix a range of errors. Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses. In this paper, we provide new and improved data, a new baseline, and guidelines that complement existing metrics, supporting fu"
P18-1033,D07-1071,0,0.0602937,"as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food types, and locations. Scholar (Iyer et al., 2017) User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct. Academic (Li and Jagadish, 2014)"
P18-1033,P13-1092,0,0.0364262,"while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly available, and some required extensive engineering effort for each domain, such as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking ta"
P18-1033,C04-1021,0,0.784096,"Missing"
P18-1033,H90-1080,0,0.212278,"12) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly available, and some required extensive engineering effort for each domain, such as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food t"
P18-1151,D14-1067,0,0.0283216,"RDF triples via the predicate. This representation allows easy data share between KBs. However, usually the elements of a triple are stored as Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans. Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q&A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about “John Doe”. By querying a KB, a Q&A system retrieves three triples “hJohn Doe,birth place,Londoni”, “hJohn Doe,birth date,1967-01-10i”, and “hLondon,capital of,Englandi.” We aim"
P18-1151,E17-1060,0,0.0315995,"s. However, usually the elements of a triple are stored as Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans. Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q&A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about “John Doe”. By querying a KB, a Q&A system retrieves three triples “hJohn Doe,birth place,Londoni”, “hJohn Doe,birth date,1967-01-10i”, and “hLondon,capital of,Englandi.” We aim to generate a natural sentence that incorporates the information of the triples and is"
P18-1151,D14-1179,0,0.0866571,"Missing"
P18-1151,W13-2102,0,0.115121,"Missing"
P18-1151,P11-2031,0,0.0494468,"Missing"
P18-1151,W11-2107,0,0.0463865,"Missing"
P18-1151,W13-0108,0,0.084656,"a and Wilks (2004) follow a 1628 traditional NLG approach to generate sentences from RDF data in the medical domain. They start with filtering repetitive RDF data (document planning) and then group coherent triples (microplanning). After that, they aggregate the sentences generated for coherent triples to produce the final sentences (aggregation and realization). Cimiano et al. (2013) generate cooking recipes from semantic web data. They focus on using a large corpus to extract lexicon in the cooking domain. The lexicon is then used with a traditional NLG approach to generate cooking recipes. Duma and Klein (2013) learn a sentence template from a parallel RDF data and text corpora. They first align entities in RDF triples with entities mentioned in sentences. Then, they extract templates from the aligned sentences by replacing the entity mention with a unique token. This method works well on RDF triples in a seen domain but fails on RDF triples in a previously unseen domain. Recently, several methods using neural networks are proposed. Lebret et al. (2016) generate the first sentence of a biography using a conditional neural language model. This model is trained to predict the next word of a sentence n"
P18-1151,P17-1017,0,0.261835,"coder model by first concatenating the elements of the RDF triples into a linear sequence and then feeding the sequence as the model input to learn the corresponding natural sentence. We implemented such a model (detailed in Section 3.2) that ranked top in the WebNLG Challenge 20172 . This Challenge has a primary objective of generating syntactically correct natural sentences from a set of RDF triples. Our model achieves the highest global scores on the automatic evaluation, outperforming competitors that use rule-based methods, statistical machine translation, and neural machine translation (Gardent et al., 2017b). While our previous model achieves a good result, simply concatenating the elements in the RDF triples may lose the relationship between entities that affects the semantics of the resulting sentence (cf. Table 3). To address this issue, in this paper, we propose a novel graph-based triple encoder model that maintain the structure of RDF triples as a small knowledge graph named the GTR-LSTM model. This model computes the hidden state of each entity in a graph to preserve the relationships between entities in a triple (intra-triple relationships) and the relationships between entities in rela"
P18-1151,W17-3518,0,0.449094,"coder model by first concatenating the elements of the RDF triples into a linear sequence and then feeding the sequence as the model input to learn the corresponding natural sentence. We implemented such a model (detailed in Section 3.2) that ranked top in the WebNLG Challenge 20172 . This Challenge has a primary objective of generating syntactically correct natural sentences from a set of RDF triples. Our model achieves the highest global scores on the automatic evaluation, outperforming competitors that use rule-based methods, statistical machine translation, and neural machine translation (Gardent et al., 2017b). While our previous model achieves a good result, simply concatenating the elements in the RDF triples may lose the relationship between entities that affects the semantics of the resulting sentence (cf. Table 3). To address this issue, in this paper, we propose a novel graph-based triple encoder model that maintain the structure of RDF triples as a small knowledge graph named the GTR-LSTM model. This model computes the hidden state of each entity in a graph to preserve the relationships between entities in a triple (intra-triple relationships) and the relationships between entities in rela"
P18-1151,W08-0510,0,0.0386482,"implement the existing models, the adapted model, and the proposed model using Keras3 . We use three common evaluation metrics including BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006). For the metric computation and significance testing, we use MultEval (Clark et al., 2011). 4.1 Tested Models We compare our proposed graph-based triple encoder (GTR-LSTM, Section 3.4) with three existing model including the adapted standard BLSTM encoder (BLSTM, Section 3.2), Neural Wikipedian (Vougiouklis et al., 2017) (TFF), and statistical machine translation (Hoang and Koehn, 2008) (SMT) trained on a 6-gram language model. We also compare with the adapted standard triple encoder (TLSTM, Section 3.3). 4.2 Hyperparameters We use grid search to find the best hyperparameters for the neural networks. We use GloVe (Pennington et al., 2014) trained on the GKB and WebNLG training data and full English Wikipedia data dump to get 300-dimension word embeddings. We use 512 hidden units for both encoder and decoder. We use a 0.5 dropout rate for regularization on both encoder and decoder to avoid overfitting. We train our model on NVIDIA Tesla K40c. We find that using adaptive learn"
P18-1151,D16-1128,0,0.114517,"Missing"
P18-1151,D15-1166,0,0.0247375,"ph can contain cycles that cause difficulty in determining the starting and ending vertices. Our traversal procedure ensures that the hidden states of all vertices are updated based on their adjacent vertices (local neighbors). To further capture the global information of the graph, we apply an attention model on the GTR-LSTM triple encoder. The attention model takes the hidden states of all vertices computed by the encoder and the previous hidden state of the decoder to compute the final input vector of each decoder time-step. Figure 5 illustrates the attention model of GTR-LSTM. Inspired by Luong et al. (2015), we adapt the following equation to compute the weight of each vertex. T exp(hd t W xn ) αn = P|X| |X| X d T j=1 exp(h t W xj ) (8) Decoder The decoder of the proposed framework is a standard LSTM. It is trained to generate the output sequence by predicting the next output word wt conditioned on the hidden state hd t . The current hidden state hd t is conditioned on the hidden state of the previous time-step hd t−1 , the output of the previous time-step wt−1 , and input vector representation hT . The hidden state and the output of the decoder at time-step t are computed as: hd t = f (hd t−1 ,"
P18-1151,N16-1086,0,0.421463,"Uniform Resource Identifiers (URIs), and many predicates (words or phrases) are not intuitive; this representation is difficult to comprehend by humans. Translating RDF triples into natural sentences helps humans to comprehend the knowledge embedded in the triples, and building a natural language based user interface is an important task in user interaction studies (Damljanovic et al., 2010). This task has many applications, such as question answering (Bordes et al., 2014; Fader et al., 2014), profile summarizing (Lebret et al., 2016; Chisholm et al., 2017), and automatic weather forecasting (Mei et al., 2016). For example, the SPARQL inference of a Q&A system (Unger et al., 2012) returns a set of RDF triples which need to be translated into natural sentences to provide a more readable answer for the users. Table 1 illustrates such an example. Suppose a user is asking a question about “John Doe”. By querying a KB, a Q&A system retrieves three triples “hJohn Doe,birth place,Londoni”, “hJohn Doe,birth date,1967-01-10i”, and “hLondon,capital of,Englandi.” We aim to generate a natural sentence that incorporates the information of the triples and is easier to be understood by the user. In this example,"
P18-1151,P02-1040,0,0.105728,"Missing"
P18-1151,D14-1162,0,0.0878415,"ation and significance testing, we use MultEval (Clark et al., 2011). 4.1 Tested Models We compare our proposed graph-based triple encoder (GTR-LSTM, Section 3.4) with three existing model including the adapted standard BLSTM encoder (BLSTM, Section 3.2), Neural Wikipedian (Vougiouklis et al., 2017) (TFF), and statistical machine translation (Hoang and Koehn, 2008) (SMT) trained on a 6-gram language model. We also compare with the adapted standard triple encoder (TLSTM, Section 3.3). 4.2 Hyperparameters We use grid search to find the best hyperparameters for the neural networks. We use GloVe (Pennington et al., 2014) trained on the GKB and WebNLG training data and full English Wikipedia data dump to get 300-dimension word embeddings. We use 512 hidden units for both encoder and decoder. We use a 0.5 dropout rate for regularization on both encoder and decoder to avoid overfitting. We train our model on NVIDIA Tesla K40c. We find that using adaptive learning rates for the optimization is efficient and leads the model to converge faster. Thus, we use Adam (Kingma and Ba, 2015) with a learning rate of 0.0002 instead of stochastic gradient descent. The update of parameters in training is computed using a mini"
P18-1151,2006.amta-papers.25,0,0.0590521,"Missing"
P18-1151,P15-1150,0,0.150393,"Missing"
P18-2017,P15-1136,0,0.125868,"three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of ha"
P18-2017,D16-1245,0,0.116186,"chieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achie"
P18-2017,P16-1061,0,0.124566,"chieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achie"
P18-2017,H05-1013,0,0.138808,"Missing"
P18-2017,N07-1030,0,0.170921,"are considered as mentions. We show the mention detection accuracy breakdown by span widths in Figure 2. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words. In addition, it is important to note that our model can detect mentions that do not exist in the training data. While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de105 the Entity Detection and Tracking task simultaneously. Denis and Baldridge (2007a) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task. tect 1048 mentions which are not detected by Lee et al. (2017), consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memori"
P18-2017,P10-1142,0,0.0608018,"S Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is core"
P18-2017,P02-1014,0,0.0458906,"in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj"
P18-2017,D13-1203,0,0.0437191,"Missing"
P18-2017,Q14-1037,0,0.0418307,"Missing"
P18-2017,D14-1162,0,0.0817454,"which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document FFNNanaphora and FFNNantecedent reduce span representation dimensions and only keep information relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of si and sj by ˆs|j Ub"
P18-2017,W12-4501,0,0.2554,"ine attention model to produce scores c(i, j): where yˆi = sigmoid(m(i)), yi = 1 if and only if si is in one of the gold mention clusters. Our final loss combines mention detection and clustering: Lloss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe"
P18-2017,D09-1120,0,0.0634763,"grouping mentions in a text such that all mentions in a cluster refer to the same entity. An example is given below (Bj¨orkelund and Kuhn, 2014) where mentions for two entities are labeled in two clusters: [Drug Emporium Inc.]a1 said [Gary Wilber]b1 was named CEO of [this drugstore chain]a2 . [He]b2 succeeds his father, Philip T. Wilber, who founded [the company]a3 and remains chairman. Robert E. Lyons III, who headed the [company]a4 ’s Philadelphia region, was appointed president and chief operating officer, succeeding [Gary Wilber]b3 . Many traditional coreference systems, either rulebased (Haghighi and Klein, 2009; Lee et al., 2011) ∗ Dragomir R. Radev Yale University dragomir.radev@yale.edu Work done during the internship at IBM Watson. 102 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 102–107 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans (“Drug Emporium Inc.”, “Gary Wilber”, “was named CEO”) for the current span “this drugstore chain”. 3"
P18-2017,J01-4004,0,0.634055,"ing to new mentions in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Ferna"
P18-2017,P10-1040,0,0.0385321,"ns 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document FFNNanaphora and FFNNantecedent reduce span representation dimensions and only keep information relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of si and sj by ˆs|j Ubiˆsi and the prior | ˆsi . likelihood of si having an antecedent"
P18-2017,N06-2015,0,0.0404729,"moid(m(i)), yi = 1 if and only if si is in one of the gold mention clusters. Our final loss combines mention detection and clustering: Lloss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embed"
P18-2017,M95-1005,0,0.59745,"loss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs ha"
P18-2017,W04-3250,0,0.110477,"Missing"
P18-2017,N16-1114,0,0.121226,"-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. F"
P18-2017,W11-1902,0,0.0332067,"t such that all mentions in a cluster refer to the same entity. An example is given below (Bj¨orkelund and Kuhn, 2014) where mentions for two entities are labeled in two clusters: [Drug Emporium Inc.]a1 said [Gary Wilber]b1 was named CEO of [this drugstore chain]a2 . [He]b2 succeeds his father, Philip T. Wilber, who founded [the company]a3 and remains chairman. Robert E. Lyons III, who headed the [company]a4 ’s Philadelphia region, was appointed president and chief operating officer, succeeding [Gary Wilber]b3 . Many traditional coreference systems, either rulebased (Haghighi and Klein, 2009; Lee et al., 2011) ∗ Dragomir R. Radev Yale University dragomir.radev@yale.edu Work done during the internship at IBM Watson. 102 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 102–107 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans (“Drug Emporium Inc.”, “Gary Wilber”, “was named CEO”) for the current span “this drugstore chain”. 3 tion model instead"
P18-2017,P15-1137,0,0.652723,"detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width). The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent scores for a pair of spans. Furthermore, one major challenge of coreference resolution is that most mentions in the document are singleton or non-anaphoric, i.e., not coreferent with any previous mention (Wiseman et al., 2015). Since the data set only have annotations for mention clusters, the end-to-end coreference resolution system needs to detect mentions, detect anaphoricity, and perform coreference linking. Therefore, research questions still remain on good designs of the scoring architecture and the learning strategy for both mention detection and antecedent scoring given only the gold cluster labels. To this end, we propose to use a biaffine attenCoreference resolution aims to identify in a text all mentions that refer to the same real-world entity. The state-of-the-art endto-end neural coreference model con"
P18-2017,D17-1018,0,0.2209,"tson and Roth, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014), usually solve the problem in two separate stages: (1) a mention detector to propose entity mentions from the text, and (2) a coreference resolver to cluster proposed mentions. At both stages, they rely heavily on complicated, fine-grained, conjoined features via heuristics. This pipeline approach can cause cascading errors, and in addition, since both stages rely on a syntactic parser and complicated handcraft features, it is difficult to generalize to new data sets and languages. Very recently, Lee et al. (2017) proposed the first state-of-the-art end-to-end neural coreference resolution system. They consider all text spans as potential mentions and therefore eliminate the need of carefully hand-engineered mention detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width). The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent"
P18-2017,H05-1004,0,0.153336,"where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layer"
P18-2017,Q15-1029,0,0.0391508,". (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. For example, Wiseman et al. (2015) propose the first neural coreference resolution system by training a deep feed-forward neural network for mention ranking. How"
P18-2017,P17-2003,0,0.0117705,"ts have contributions and when they work together the total gain is even higher. Mention Detection Subtask To further understand our model, we perform a mention detection subtask where spans with mention scores higher than 0 are considered as mentions. We show the mention detection accuracy breakdown by span widths in Figure 2. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words. In addition, it is important to note that our model can detect mentions that do not exist in the training data. While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de105 the Entity Detection and Tracking task simultaneously. Denis and Baldridge (2007a) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task. tect 1048 mentions which are not detected by Lee et al. (2017), consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airfor"
P18-2017,D08-1031,0,\N,Missing
P19-1023,P15-1034,0,0.0239598,"of Open Information Extraction (Open IE) and proposed a pipeline that consists of three stages: learner, extractor, and assessor. The learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. The assessor assigns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of each token of the input. The work most 231 D"
P19-1023,P18-2065,0,0.0183315,", 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB predicates necessitates another mapping step, typically aided by paraphrase dictionaries. This two-stage architecture is inherently prone to error propagation across its two stages: NED errors may cause extraction errors (and vice versa) that lead to inaccurate relationships being • We propose an end-to-end model for extract230 related to ours is Neural Open IE (Cui et al., 2018), which proposed an encoder-decoder with attention model to extract triples. However, this work is not geared for extracting relations of canonicalized entities. Another line of studies use neural learning for semantic role labeling (He et al., 2018), but the goal here is to recognize the predicate-argument structure of a single input sentence – as opposed to extracting relations from a corpus. All of these methods generate triples where the head and tail entities and the predicate stay in their surface forms. Therefore, different names and phrases for the same entities result in multiple trip"
P19-1023,D11-1142,0,0.0476115,"nko et al. (2007) introduced the paradigm of Open Information Extraction (Open IE) and proposed a pipeline that consists of three stages: learner, extractor, and assessor. The learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. The assessor assigns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of ea"
P19-1023,N13-1092,0,0.0649546,"Missing"
P19-1023,D17-1278,0,0.142936,"ntities and predicate of the first extracted triple, including NYU, instance of, and Private University, are mapped to their unique IDs Q49210, P31, and Q902104, respectively, to comply with the semantic space of the KB. Previous studies on relation extraction have employed both unsupervised and supervised approaches. Unsupervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted e"
P19-1023,D16-1236,1,0.942465,"fold. First, the embeddings capture the relationship between words and entities, which is essential for named entity disambiguation. Second, the entity embeddings preserve the relationships between entities, which help to build a highly accurate classifier to filter the invalid extracted triples. To cope with the lack of fully labeled training data, we adapt distant supervision to generate aligned pairs of sentence and triple as the training data. We augment the process with co-reference resolution (Clark and Manning, 2016) and dictionary-based paraphrase detection (Ganitkevitch et al., 2013; Grycner and Weikum, 2016). The co-reference resolution helps extract sentences with implicit entity names, which enlarges the set of candidate sentences to be aligned with existing triples in a KB. The paraphrase detection helps filter sentences that do not express any relationships between entities. The main contributions of this paper are: also the predicate &quot;instance of&quot; is in the set of predefined predicates we are interested in, but the relationship of hNYU, instance of, Private Universityi does not exist in the KB. We aim to add this relationship to our KB. This is the typical situation for KB enrichment (as opp"
P19-1023,P18-2058,0,0.0250184,"sitates another mapping step, typically aided by paraphrase dictionaries. This two-stage architecture is inherently prone to error propagation across its two stages: NED errors may cause extraction errors (and vice versa) that lead to inaccurate relationships being • We propose an end-to-end model for extract230 related to ours is Neural Open IE (Cui et al., 2018), which proposed an encoder-decoder with attention model to extract triples. However, this work is not geared for extracting relations of canonicalized entities. Another line of studies use neural learning for semantic role labeling (He et al., 2018), but the goal here is to recognize the predicate-argument structure of a single input sentence – as opposed to extracting relations from a corpus. All of these methods generate triples where the head and tail entities and the predicate stay in their surface forms. Therefore, different names and phrases for the same entities result in multiple triples, which would pollute the KG if added this way. The only means to map triples to uniquely identified entities in a KG is by post-processing via entity linking (NED) methods (Shen et al., 2015) or by clustering with subsequent mapping (Gal´arraga e"
P19-1023,P82-1020,0,0.77071,"Missing"
P19-1023,D11-1072,1,0.814752,"Missing"
P19-1023,P10-1030,0,0.0897818,"Missing"
P19-1023,D16-1245,0,0.161323,"013) and TransE (Bordes et al., 2013). The advantages of our jointly learned embeddings are twofold. First, the embeddings capture the relationship between words and entities, which is essential for named entity disambiguation. Second, the entity embeddings preserve the relationships between entities, which help to build a highly accurate classifier to filter the invalid extracted triples. To cope with the lack of fully labeled training data, we adapt distant supervision to generate aligned pairs of sentence and triple as the training data. We augment the process with co-reference resolution (Clark and Manning, 2016) and dictionary-based paraphrase detection (Ganitkevitch et al., 2013; Grycner and Weikum, 2016). The co-reference resolution helps extract sentences with implicit entity names, which enlarges the set of candidate sentences to be aligned with existing triples in a KB. The paraphrase detection helps filter sentences that do not express any relationships between entities. The main contributions of this paper are: also the predicate &quot;instance of&quot; is in the set of predefined predicates we are interested in, but the relationship of hNYU, instance of, Private Universityi does not exist in the KB. We"
P19-1023,N18-2053,0,0.0517372,"Missing"
P19-1023,K18-1050,0,0.0272964,"ord and entity embeddings to capture the relationship between words and entities for named entity disambiguation. We further propose a modified beam search and a triple classifier to generate high-quality triples. • We evaluate the proposed model over two real-world datasets. We adapt distant supervision with co-reference resolution and paraphrase detection to obtain high-quality training data. The experimental results show that our model consistently outperforms a strong baseline for neural relation extraction (Lin et al., 2016) coupled with state-of-the-art NED models (Hoffart et al., 2011; Kolitsas et al., 2018). 2 2.1 2.2 Entity-aware Relation Extraction Inspired by the work of Brin (1998), state-of-theart methods employ distant supervision by leveraging seed facts from an existing KG (Mintz et al., 2009; Suchanek et al., 2009; Carlson et al., 2010). These methods learn extraction patterns from seed facts, apply the patterns to extract new fact candidates, iterate this principle, and finally use statistical inference (e.g., a classifier) for reducing the false positive rate. Some of these methods hinge on the assumption that the co-occurrence of a seed fact’s entities in the same sentence is an indi"
P19-1023,P17-1004,0,0.0380074,"Missing"
P19-1023,W15-1506,0,0.0685961,"Missing"
P19-1023,P16-1200,0,0.565133,"typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities (e.g., (Suchanek et al., 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB predicates necessitates another mapping step, typically aided by para"
P19-1023,W18-6501,0,0.0677595,"Missing"
P19-1023,N13-1008,0,0.144814,"Missing"
P19-1023,D12-1048,0,0.0271704,"troduced the paradigm of Open Information Extraction (Open IE) and proposed a pipeline that consists of three stages: learner, extractor, and assessor. The learner uses dependency-parsing information to learn patterns for extraction, in an unsupervised way. The extractor generates candidate triples by identifying noun phrases as arguments and connecting phrases as predicates. The assessor assigns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of each token of the input"
P19-1023,D17-1188,0,0.0765332,"our problem. man (2015) proposed Convolution Networks with multi-sized window kernel. Zeng et al. (2015) proposed Piecewise Convolution Neural Networks (PCNN). Lin et al. (2016, 2017) improved this approach by proposing PCNN with sentence-level attention. This method performed best in experimental studies; hence we choose it as the main baseline against which we compare our approach. Follow-up studies considered further variations: Zhou et al. (2018) proposed hierarchical attention, Ji et al. (2017) incorporated entity descriptions, Miwa and Bansal (2016) incorporated syntactic features, and Sorokin and Gurevych (2017) used background knowledge for contextualization. None of these neural models is geared for KG enrichment, as the canonicalization of entities is out of their scope. 3 3.1 Solution Framework Figure 1 illustrates the overall solution framework. Our framework consists of three components: data collection module, embedding module, and neural relation extraction module. In the data collection module (detailed in Section 3.2), we align known triples in an existing KB with sentences that contain such triples from a text corpus. The aligned pairs of sentences and triples will later be used as the tra"
P19-1023,P09-1113,0,0.858189,"th unsupervised and supervised approaches. Unsupervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities (e.g., (Suchanek et al., 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB pred"
P19-1023,N18-1081,0,0.016511,"igns a probability to each candidate triple based on statistical evidence. This approach was prone to extracting incorrect, verbose and uninformative triples. Various followup studies (Fader et al., 2011; Mausam et al., 2012; Angeli et al., 2015; Mausam, 2016) improved the accuracy of Open IE, by adding handcrafted patterns or by using distant supervision. Corro and Gemulla (2013) developed ClausIE, a method that analyzes the clauses in a sentence and derives triples from this structure. Gashteovski et al. (2017) developed MinIE to advance ClausIE by making the resulting triples more concise. Stanovsky et al. (2018) proposed a supervised learner for Open IE by casting relation extraction into sequence tagging. A bi-LSTM model is trained to predict the label (entity, predicate, or other) of each token of the input. The work most 231 Dataset Collection Module Wikipedia article Embedding Module Neural Relation Extraction Module Joint learning skip-gram & TransE Expected output: <Q49210,P31,Q902104&gt;; <Q387638,P161,Q40026&gt; Triple classifier Wikidata Word Embeddings Distant supervision Entity Embeddings 0.2 0.4 0.1 0.2 0.1 0.1 0.5 0.1 0.4 0.2 0.1 0.5 0.1 0.1 0.2 0.1 0.5 0.1 0.5 0.1 0.2 0.3 0.3 0.3 0.2 0.2 0.3"
P19-1023,P16-1105,0,0.0159232,"d rj ∈ R. Table 1 illustrates the input and target output of our problem. man (2015) proposed Convolution Networks with multi-sized window kernel. Zeng et al. (2015) proposed Piecewise Convolution Neural Networks (PCNN). Lin et al. (2016, 2017) improved this approach by proposing PCNN with sentence-level attention. This method performed best in experimental studies; hence we choose it as the main baseline against which we compare our approach. Follow-up studies considered further variations: Zhou et al. (2018) proposed hierarchical attention, Ji et al. (2017) incorporated entity descriptions, Miwa and Bansal (2016) incorporated syntactic features, and Sorokin and Gurevych (2017) used background knowledge for contextualization. None of these neural models is geared for KG enrichment, as the canonicalization of entities is out of their scope. 3 3.1 Solution Framework Figure 1 illustrates the overall solution framework. Our framework consists of three components: data collection module, embedding module, and neural relation extraction module. In the data collection module (detailed in Section 3.2), we align known triples in an existing KB with sentences that contain such triples from a text corpus. The ali"
P19-1023,D12-1104,1,0.779299,"ining data in the form of sentence-triple pairs. Following Sorokin and Gurevych (2017), we use distant supervision (Mintz et al., 2009) to align sentences in Wikipedia1 with triples in Wikidata2 (Vrandecic and Kr¨otzsch, 2014). 1 https://dumps.wikimedia.org/enwiki/latest/enwikilatest-pages-articles.xml.bz2 2 https://dumps.wikimedia.org/wikidatawiki/entities/latestall.ttl.gz 233 In Step (2), we use a dictionary based paraphrase detection to capture relationships between entities in a sentence. First, we create a dictionary by populating predicate paraphrases from three sources including PATTY (Nakashole et al., 2012), POLY (Grycner and Weikum, 2016), and PPDB (Ganitkevitch et al., 2013) that yield 540 predicates and 24, 013 unique paraphrases. For example, predicate paraphrases for the relationAll (WIKI) Train+val Test (WIKI) Test (GEO) #pairs 255,654 225,869 29,785 1,000 #triples 330,005 291,352 38,653 1,095 #entities 279,888 249,272 38,690 124 #predicates 158 157 109 11 Table 2: Statistics of the dataset. ship &quot;place of birth&quot; are {born in, was born in, ...}. Then, we use this dictionary to filter sentences that do not express any relationships between entities. We use exact string matching to find verb"
P19-1023,D12-1042,0,0.060714,"ntz et al., 2009; Suchanek et al., 2009; Carlson et al., 2010). These methods learn extraction patterns from seed facts, apply the patterns to extract new fact candidates, iterate this principle, and finally use statistical inference (e.g., a classifier) for reducing the false positive rate. Some of these methods hinge on the assumption that the co-occurrence of a seed fact’s entities in the same sentence is an indicator of expressing a semantic relationship between the entities. This is a potential source of wrong labeling. Follow-up studies (Hoffmann et al., 2010; Riedel et al., 2010, 2013; Surdeanu et al., 2012) overcome this limitation by various means, including the use of relation-specific lexicons and latent factor models. Still, these methods treat entities by their surface forms and disregard their mapping to existing entities in the KG. Suchanek et al. (2009) and Sa et al. (2017) used probabilistic-logical inference to eliminate false positives, based on constraint solving or Monte Carlo sampling over probabilistic graphical models, respectively. These methods integrate entity linking (i.e., NED) into their models. However, both have high computational complexity and rely on modeling constrain"
P19-1023,P18-1151,1,0.857438,"Missing"
P19-1023,K16-1025,0,0.117063,"d model on a different style of text than the training data), we also collect another test dataset outside Wikipedia. We apply the same procedure to the user reviews of a travel website. First, we collect user reviews on 100 popular landmarks in Australia. Then, we apply the adapted distant supervision to the reviews and collect 1,000 sentence-triple pairs (we call it the GEO test dataset). Table 2 summarizes the statistics of our datasets. 3.3 a joint learning of word and entity embeddings that is effective to capture the similarity between words and entities for named entity disambiguation (Yamada et al., 2016). Note that our method differs from that of Yamada et al. (2016). We use joint learning by combining skip-gram (Mikolov et al., 2013) to compute the word embeddings and TransE (Bordes et al., 2013) to compute the entity embeddings (including the relationship embeddings), while Yamada et al. (2016) use Wikipedia Link-based Measure (WLM) (Milne and Witten, 2008) that does not consider the relationship embeddings. Our model learns the entity embeddings by minimizing a margin-based objective function JE : JE = X X   max 0, γ + f (tr ) − f (t0r ) (1) tr ∈Tr t0r ∈Tr0 Tr = {hh, r, ti|hh, r, ti ∈ G"
P19-1023,D15-1203,0,0.522191,"pervised approaches typically start with a small set of manually defined extraction patterns to detect entity names and phrases about relationships in an input text. This paradigm is known as Open Information Extraction (Open IE) (Banko et al., 2007; Corro and Gemulla, 2013; Gashteovski et al., 2017). In this line of approaches, both entities and predicates are captured in their surface forms without canonicalization. Supervised approaches train statistical and neural models for inferring the relationship between two known entities in a sentence (Mintz et al., 2009; Riedel et al., 2010, 2013; Zeng et al., 2015; Lin et al., 2016). Most of these studies employ a preprocessing step to recognize the entities. Only few studies have fully integrated the mapping of extracted triples onto uniquely identified KB entities by using logical reasoning on the existing KB to disambiguate the extracted entities (e.g., (Suchanek et al., 2009; Sa et al., 2017)). Most existing methods thus entail the need for Named Entity Disambiguation (NED) (cf. the survey by Shen et al. (2015)) as a separate processing step. In addition, the mapping of relationship phrases onto KB predicates necessitates another mapping step, typi"
P19-1043,W17-2408,0,0.158932,"velop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation. 1 Table 1: An email with three possible subject lines. 2018), and email classification (Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017), to our knowledge there is no previous work on generating email subjects. In this paper, we propose the task of Subject Line Generation (SLG): automatically producing email subjects given the email body. While this is similar to email summarization, the two tasks serve different purposes in the process of email composition and consumption. A subject line is required when the sender writes the email, while a summary is more useful for long emails to benefit the recipient. An automatically generated email subject can also be used for downstream applications such as email triaging to help people"
P19-1043,D14-1181,0,0.00235813,"ion or METEOR for machine translation, there is no available automatic metric designed for email subject generation. Motivated by recent work on regression-based metrics for machine translation (Shimanaka et al., 2018) and dialog response generation (Lowe et al., 2017), we build a neural network (ESQE) to estimate the quality of an email subject given the email body (§3.3). The estimator is pretrained and fixed during RL training phase to provide rewards for the extractor agent. While our model is based on Chen and Bansal D = [d1 , d2 , . . . , dj , . . . , d|D |] We first use a temporal CNN (Kim, 2014) to build individual sentence representations. For each sentence, we feed the sequence of its word vectors into 1-D convolutional filters with various window sizes. We then apply ReLU activation and then max-over-time pooling. The sentence representation is a concatenation of activations from all filters cj = CNN(dj ), j = 1, . . . , |D| (1) Then we use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to capture documentlevel inter-sentence information over CNN outputs: → − → − d j = LSTMforward ( d j−1 , cj ) ← − ← − d j = LSTMbackward ( d j+1 , cj ) → − ← − dj = [ d j , d j ] (2) For"
P19-1043,P08-1041,0,0.0246458,"ard. Our model outperforms several competitive baselines and approaches human-level performance. In the future, we would like to generalize it to multiple domains and datasets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal w"
P19-1043,P18-1063,0,0.0119666,"he email body which contain the necessary information for writing a subject. This task can be formulated as a sequence-to-sequence learning problem where the output sequence corresponds to the position of “positive” sentences in the input email body. Therefore, we use a pointer network (Vinyals et al., 2015) to first build hierarchical sentence representations during encoding and then extract “positive” sentences during decoding. Suppose our input is an email body D which consists of |D |sentences: Our Model Our model is illustrated in Figure 1. Based on recent progress in news summarization (Chen and Bansal, 2018), our model generates email subjects in two stages: (1) The extractor selects multiple sentences containing salient information for writing a subject (§3.1). (2) The abstractor rewrites multiple selected sentences into a succinct subject line while preserving key information (§3.2). We employ a multi-stage training strategy (§3.4) including a Reinforcement Learning (RL) phase because of its usefulness for text generation tasks (Ranzato et al., 2016; Bahdanau et al., 2017) to optimize the non-differentiable metrics such as ROUGE and METEOR. However, unlike ROUGE for summarization or METEOR for"
P19-1043,P16-1046,0,0.0785311,"Missing"
P19-1043,W04-1000,0,0.533926,"Missing"
P19-1043,P17-1103,0,0.0210667,"ject line while preserving key information (§3.2). We employ a multi-stage training strategy (§3.4) including a Reinforcement Learning (RL) phase because of its usefulness for text generation tasks (Ranzato et al., 2016; Bahdanau et al., 2017) to optimize the non-differentiable metrics such as ROUGE and METEOR. However, unlike ROUGE for summarization or METEOR for machine translation, there is no available automatic metric designed for email subject generation. Motivated by recent work on regression-based metrics for machine translation (Shimanaka et al., 2018) and dialog response generation (Lowe et al., 2017), we build a neural network (ESQE) to estimate the quality of an email subject given the email body (§3.3). The estimator is pretrained and fixed during RL training phase to provide rewards for the extractor agent. While our model is based on Chen and Bansal D = [d1 , d2 , . . . , dj , . . . , d|D |] We first use a temporal CNN (Kim, 2014) to build individual sentence representations. For each sentence, we feed the sequence of its word vectors into 1-D convolutional filters with various window sizes. We then apply ReLU activation and then max-over-time pooling. The sentence representation is a"
P19-1043,W14-3348,0,0.00867843,", negative otherwise. The multi-sentence extractor is trained to predict “positive” sentences by minimizing the cross-entropy loss. For the multi-sentence abstractor, we create training examples by pairing the “positive” sentences and the 3.3 Email Subject Quality Estimator Since there is no established automatic metric for SLG, we build our own Email Subject Quality Estimator (ESQE). Given an email body D and a potential subject for the subject s, our quality estimator outputs a real-valued Subject Quality score 449 (Lin, 2004) including F1 scores of ROUGE1, ROUGE-2, and ROUGE-L. (2) METEOR (Denkowski and Lavie, 2014). They all rely on one or more references and measure the similarity between the output and the reference. In addition, we include ESQE, which is a reference-less metric. Human Evaluation. While those automatic scores are quick and inexpensive to calculate, only our quality estimator is designed for evaluation of subject line generation. Therefore, we also conduct an extensive human evaluation on the overall score and two aspects of email quality: informativeness and fluency. An email subject is informative if it contains accurate and consistent details with the body, and it is fluent if free"
P19-1043,loza-etal-2014-building,0,0.0177156,"sets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal with the problem of email thread summarization by the sentence extraction approach. Acknowledgements We would like to thank Jimmy Nguyen and Vipul Raheja for their help"
P19-1043,D15-1166,0,0.0145755,"t sentences: SQ(D, s) = FFNN([D, s]) oˆtj = vo |tanh(Wo dj + Uo et ) P (ot |o1 , o2 , . . . , ot−1 ) = softmax(ˆ ot ) (4) where {v, W, U} are trainable parameters. We also add a trainable “stop” vector with the same dimension as the sentence representation. The decoder can choose to stop by pointing to this “stop” sentence. 3.2 Multi-sentence Abstractor In the second stage, the abstractor takes the selected sentences from the extractor and rewrites them into an email subject. We implement the abstractor as a sequence-to-sequence encoderdecoder model with the bilinear multiplicative attention (Luong et al., 2015) and copy mechanism (See et al., 2017). The copy mechanism enables the decoder to copy words directly from the input document, which is helpful to generate accurate information verbatim even for out-of-vocabulary words. (6) To train the estimator, we collect human evaluations on 3,490 email subjects. In order to expose the estimator to both good and bad examples, 2,278 of the 3,490 are the original subjects and the remaining 1,212 subjects are generated by an existing summarization system. Each subject has 3 human evaluation scores (the same human evaluation as explained in §4.1) and we train"
P19-1043,W04-3252,0,0.257472,"by the extractor: r(a1:T ) = SQ(D, s) (8) For training, we maximize the expected reward: 4.2 L(θ) = Ea1:T ∼πθ [r(a1:T )] (9) To benchmark our method, we use several methods from the summarization field, including some recent state-of-the-art systems, because the email subject line can be viewed as a short summary of the email content. They can be clustered into two groups. (1) Unsupervised extractive or/and abstractive summarization. LEAD-2 directly uses the first two sentences as the subject line. We choose lead2 to include both the greeting and the first sentence of main content. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) are two graph-based ranking models to extract the most salient sentence as the subject line. Shang et al. (2018) use a graph-based framework to extract topics and then generate a single abstractive sentence for each topic under a budget constraint. (2) Neural summarization using encoderdecoder networks with attention mechanisms. (Sutskever et al., 2014; Bahdanau et al., 2015). The Pointer-Generator Network from See et al. (2017) augments the standard encoder-decoder network by adding the ability to copy words from the source text and using the coverage loss"
P19-1043,C10-1037,0,0.0312835,"ole training takes about 4 hours. 4.3 Baselines. For TextRank and LexRank, we use the sumy2 implementation which uses the snowball stemmer, the sentence and word tokenizer from NLTK3 . For Shang et al. (2018), we Implementation Details Our Model. We pretrain 128-dimensional word2vec (Mikolov et al., 2013) on our corpus as initialization and update word embeddings during training. We use single layer bidirectional LSTMs with 256 hidden units in all models. The 2 3 451 https://github.com/miso-belica/sumy https://www.nltk.org/ use their extension of the Multi-Sentence Compression Graph (MSCG) of Filippova (2010) and a budget of 10 words in the submodular maximization. We choose the number of communities from [1,2,3,4,5] based on the dev set and we find that 1 works best. For the Pointer-Generator Network from See et al. (2017), we follow their implementation4 and use a batch size 16. For Paulus et al. (2018), we use an implementation from Keneshloo et al. (2018)5 . We did not include the intra-temporal attention and the intra-decoder attention because they hurt the performance. For Hsu et al. (2018), we follow their code6 with a batch size 16. All training is early stopped based on the dev set perfor"
P19-1043,P18-1013,0,0.233096,"ients. The baseline network has the same architecture as the decoder of the extractor. But it has another set of trainable parameters θb and predicts the reward by minimizing the following mean squared error: L(θb ) = (bt − r)2 4 4.1 Baselines (11) Experimental Setup Evaluation Automatic Evaluation. Since SLG is a new task, we analyze the usefulness of automatic metrics from sister tasks, and also use human evaluation. We first use automatic metrics from text summarization and machine translation: (1) ROUGE 450 LEAD-2 TextRank LexRank Shang et al. (2018) See et al. (2017) Paulus et al. (2018) Hsu et al. (2018) Narayan et al. (2018a) Our System Human Annotation R-1 11.28 11.12 13.02 10.56 18.02 14.08 16.59 13.52 25.41 23.43∗ R-2 4.61 3.75 4.96 3.28 5.73 5.09 4.67 3.27 11.34 9.71∗ Dev R-L 10.48 10.15 11.89 9.92 16.63 13.36 15.12 13.33 25.07 22.17 METEOR 10.76 9.19 10.84 6.17 10.83 9.07 13.22∗ 4.64 9.83 10.87∗ R-1 11.00 11.32 12.46 10.40 17.02 13.49 15.75 12.60 23.67 23.90∗ R-2 4.33 3.88 4.62 3.09 5.45 4.55 4.54 3.09 10.29 10.09∗ Test R-L 10.20 10.14 11.37 9.77 15.78 12.83 14.41 12.52 23.44 22.75∗ METEOR 11.27∗ 10.64∗ 11.56∗ 6.15 10.31 8.65 12.49∗ 4.66 9.37 11.04∗ Test R-L 16.62 16.00 18.68 14.88 20.8"
P19-1043,W01-0719,0,0.456874,"tion Email is a ubiquitous form of online communication. An email message consists of two basic elements: an email subject line and an email body. The subject line, which is displayed to the recipient in the list of inbox messages, should tell what the email body is about and what the sender wants to convey. An effective email subject line becomes essential since it can help people manage a large number of emails. Table 1 shows an email body with three possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,"
P19-1043,N18-4015,0,0.0145992,"rewrites multiple selected sentences into a succinct subject line while preserving key information (§3.2). We employ a multi-stage training strategy (§3.4) including a Reinforcement Learning (RL) phase because of its usefulness for text generation tasks (Ranzato et al., 2016; Bahdanau et al., 2017) to optimize the non-differentiable metrics such as ROUGE and METEOR. However, unlike ROUGE for summarization or METEOR for machine translation, there is no available automatic metric designed for email subject generation. Motivated by recent work on regression-based metrics for machine translation (Shimanaka et al., 2018) and dialog response generation (Lowe et al., 2017), we build a neural network (ESQE) to estimate the quality of an email subject given the email body (§3.3). The estimator is pretrained and fixed during RL training phase to provide rewards for the extractor agent. While our model is based on Chen and Bansal D = [d1 , d2 , . . . , dj , . . . , d|D |] We first use a temporal CNN (Kim, 2014) to build individual sentence representations. For each sentence, we feed the sequence of its word vectors into 1-D convolutional filters with various window sizes. We then apply ReLU activation and then max-"
P19-1043,D18-1206,0,0.36017,"ree possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588/394,622/381,197 14,436/1,960/1,906 avg doc words 760 431 31 75 avg summary words 46 23 8 4 Table 2: Annotated Enron Subject Line Corpus compared with other datasets. to properly evaluate the subject, we use a combination of automatic metrics from the text summarization and machine translation fields, in addition to building our own regression-based Email Subject Quality Estimator (ESQE). Third, to gene"
P19-1043,N18-1158,0,0.19802,"ree possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588/394,622/381,197 14,436/1,960/1,906 avg doc words 760 431 31 75 avg summary words 46 23 8 4 Table 2: Annotated Enron Subject Line Corpus compared with other datasets. to properly evaluate the subject, we use a combination of automatic metrics from the text summarization and machine translation fields, in addition to building our own regression-based Email Subject Quality Estimator (ESQE). Third, to gene"
P19-1043,W17-4503,0,0.0470077,"Missing"
P19-1043,P14-2056,0,0.121458,"ment summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation. 1 Table 1: An email with three possible subject lines. 2018), and email classification (Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017), to our knowledge there is no previous work on generating email subjects. In this paper, we propose the task of Subject Line Generation (SLG): automatically producing email subjects given the email body. While this is similar to email summarization, the two tasks serve different purposes in the process of email composition and consumption. A subject line is required when the sender writes the email, while a summary is more useful for long emails to benefit the recipient. An automatically generated email subject can also be used for downstream applications such as"
P19-1043,D14-1211,0,0.0207832,"cts by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal with the problem of email thread summarization by the sentence extraction approach. Acknowledgements We would like to thank Jimmy Nguyen and Vipul Raheja for their help in the data creation process. We also thank Dragomir Radev, Courtney Napoles, Dimitri"
P19-1043,C04-1079,0,0.102346,"Selection and Rewriting with Email Subject Quality Estimation Reward. Our model outperforms several competitive baselines and approaches human-level performance. In the future, we would like to generalize it to multiple domains and datasets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and"
P19-1043,N04-4027,0,0.359232,"nication. An email message consists of two basic elements: an email subject line and an email body. The subject line, which is displayed to the recipient in the list of inbox messages, should tell what the email body is about and what the sender wants to convey. An effective email subject line becomes essential since it can help people manage a large number of emails. Table 1 shows an email body with three possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588"
P19-1043,D15-1044,0,0.485597,"eral research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588/394,622/381,197 14,436/1,960/1,906 avg doc words 760 431 31 75 avg summary words 46 23 8 4 Table 2: Annotated Enron Subject Line Corpus compared with other datasets. to properly evaluate the subject, we use a combination of automatic metrics from the text summarization and machine translation fields, in addition to building our own regression-based Email Subject Quality Estimator (ESQE). Third, to generate effective email subjects, we propose a m"
P19-1043,I17-2062,0,0.0543644,"Missing"
P19-1043,D17-1062,0,0.014181,"document summarization because the email subject line can be viewed as a short summary of the email content. Therefore, we use different summarization models as baselines with techniques such as graph-based extraction and compression, sequence-to-sequence neural abstractive summarization with the hierarchical attention, copy, and coverage mechanisms. In addition, RL has become increasingly popular for text generation to optimize the non-differentiable metrics and to reduce the exposure bias in the traditional “teaching forcing” supervised training (Ranzato et al., 2016; Bahdanau et al., 2017; Zhang and Lapata, 2017; Sakaguchi et al., 2017). For example, Narayan et al. (2018b) use RL for ranking sentences in pure extractive summarization. Furthermore, current methods on news headline generation (Lopyrev, 2015; Tilk and Alum¨ae, 2017; Kiyono et al., 2017; Tan et al., 2017; Shen et al., 2017) most follow the encoder-decoder model, while our model uses a multi-sentence selection and rewriting framework. different automatic scores and the average human rating. We also report the inter-rater agreement in the last row by checking the correlation between the third human rating and the average of the other two."
P19-1043,scerri-etal-2010-classifying,0,0.0203545,"iple domains and datasets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal with the problem of email thread summarization by the sentence extraction approach. Acknowledgements We would like to thank Jimmy Nguyen and Vipul Ra"
P19-1043,P17-1099,0,0.693041,"j = vo |tanh(Wo dj + Uo et ) P (ot |o1 , o2 , . . . , ot−1 ) = softmax(ˆ ot ) (4) where {v, W, U} are trainable parameters. We also add a trainable “stop” vector with the same dimension as the sentence representation. The decoder can choose to stop by pointing to this “stop” sentence. 3.2 Multi-sentence Abstractor In the second stage, the abstractor takes the selected sentences from the extractor and rewrites them into an email subject. We implement the abstractor as a sequence-to-sequence encoderdecoder model with the bilinear multiplicative attention (Luong et al., 2015) and copy mechanism (See et al., 2017). The copy mechanism enables the decoder to copy words directly from the input document, which is helpful to generate accurate information verbatim even for out-of-vocabulary words. (6) To train the estimator, we collect human evaluations on 3,490 email subjects. In order to expose the estimator to both good and bad examples, 2,278 of the 3,490 are the original subjects and the remaining 1,212 subjects are generated by an existing summarization system. Each subject has 3 human evaluation scores (the same human evaluation as explained in §4.1) and we train our estimator to regress the average."
P19-1043,W04-1013,0,\N,Missing
P19-1043,W04-1008,0,\N,Missing
P19-1306,D17-1110,0,0.0683054,"Missing"
P19-1306,P18-4020,0,0.0274498,"Missing"
P19-1306,P07-2045,0,0.00605898,"report the Actual Query Weighted Value (AQWV) (NIST, 2017), a set-based metric with penalty for both missing relevant and returning irrelevant documents. We use β = 40.0 and find the best global fixed cutoff over all queries. Baselines. For traditional CLIR approaches, we use query translation and document translation with the Indri system. For query translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ). For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For SMT, we use the moses system (Koehn et al., 2007) with word alignments using mGiza and 5-gram KenLM language model (Heafield, 2011). For NMT, we use sequence-to4 2 github.com/facebookresearch/MUSE 3 https://www.wiktionary.org/ www.iarpa.gov/index.php/ research-programs/material 5 https://trec.nist.gov/trec_eval/ 3176 sequence model with attention (Bahdanau et al., 2015; Miceli Barone et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018). For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM. These models and our methods all use an SMTbased document transla"
P19-1306,W11-2123,0,0.0274541,"nalty for both missing relevant and returning irrelevant documents. We use β = 40.0 and find the best global fixed cutoff over all queries. Baselines. For traditional CLIR approaches, we use query translation and document translation with the Indri system. For query translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ). For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For SMT, we use the moses system (Koehn et al., 2007) with word alignments using mGiza and 5-gram KenLM language model (Heafield, 2011). For NMT, we use sequence-to4 2 github.com/facebookresearch/MUSE 3 https://www.wiktionary.org/ www.iarpa.gov/index.php/ research-programs/material 5 https://trec.nist.gov/trec_eval/ 3176 sequence model with attention (Bahdanau et al., 2015; Miceli Barone et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018). For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM. These models and our methods all use an SMTbased document translation as input. Implementation Details. For POSIT-DRMM and Bilingual POSIT-DRMM, we"
P19-1306,P99-1027,0,0.908895,"s of query and documents independently, and the matching is performed at the final stage. The latter explicitly encodes the interaction between terms to direct capture word-level interaction patterns. For example, the DRMM (Guo et al., 2016) first compares the term embeddings of each pair of terms within the query and the document and then generates fixedlength matching histograms. 3 4 Related Work Cross-lingual Information Retrieval. Traditional CLIR approaches include document translation and query translation, and more research efforts are on the latter (Oard and Hackett, 1997; Oard, 1998; McCarley, 1999; Franz et al., 1999). Early methods use the dictionary to translate the user query (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996; Pirkola, 1998). Other methods include the single best SMT query translation (Chin et al., 2014) and the weighted SMT translation alternatives known as the probabilistic structured query (PSQ) (Darwish and Oard, 2003; Ture et al., 2012). Recently, Bai et al. (2010) and Sokolov et al. (2013) propose methods to learn the sparse query-document associations from supervised ranking signals on cross-lingual Wikipedia and patent data, respectively. Furthermore,"
P19-1306,D18-1211,0,0.510747,"slation direction, it can be further categorized into the document translation and the query translation approaches (Nie, 2010). In both cases, we first solve the translation problem, and the task is transformed to the monolingual setting. However, while conceptually simple, the performance of this modular approach is fundamentally limited by the quality of machine translation. Recently, many deep neural IR models have shown promising results on monolingual data sets (Huang et al., 2013; Guo et al., 2016; Pang et al., 2016; Mitra et al., 2016, 2017; Xiong et al., 2017; Hui et al., 2017, 2018; McDonald et al., 2018). They learn a scoring function directly from the relevance label of query-document pairs. However, it is not clear how to use them when documents and queries are not in the same language. Furthermore, those deep neural networks need a large amount of training data. This is expensive to get for lowresource language pairs in our cross-lingual case. In this paper, we propose a cross-lingual deep relevance ranking architecture based on a bilingual view of queries and documents. As shown in Figure 1, our model first translates queries and documents and then uses four components to match them in bo"
P19-1306,W17-4710,0,0.0312461,"ry translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ). For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For SMT, we use the moses system (Koehn et al., 2007) with word alignments using mGiza and 5-gram KenLM language model (Heafield, 2011). For NMT, we use sequence-to4 2 github.com/facebookresearch/MUSE 3 https://www.wiktionary.org/ www.iarpa.gov/index.php/ research-programs/material 5 https://trec.nist.gov/trec_eval/ 3176 sequence model with attention (Bahdanau et al., 2015; Miceli Barone et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018). For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM. These models and our methods all use an SMTbased document translation as input. Implementation Details. For POSIT-DRMM and Bilingual POSIT-DRMM, we use the k-maxpooling with k = 5 and 0.3 dropout of the BiLSTM output. For PACRR, PACRR-DRMM and their bilingual counterparts, we use convolutional filter sizes with [1,2,3], and each filter size has 32 filters. We use k = 2 in the k-max-pooling. The loss function is m"
P19-1306,W17-2328,0,0.0243248,"rmore, Vuli´c Experiments Training and Inference. We first use the Indri1 system which uses query likelihood with Dirichlet Smoothing (Zhai and Lafferty, 2004) to preselect the documents from the collection. To build the training dataset, for each positive example in the returned list, we randomly sample one negative example from the documents returned by Indri. The model is then trained with a binary crossentropy loss. On validation or testing set, we use our prediction scores to rerank the documents returned by Indri. Extra Features. Following the previous work (Severyn and Moschitti, 2015; Mohan et al., 2017; McDonald et al., 2018), we compute the final relevance score by a linear model to combine the model output with the following set of extra fea3175 1 www.lemurproject.org/indri.php MAP P@20 Query Translation and Document Translation with Indri Dictionary-Based Query Translation (DBQT) 20.93 4.86 Probabilistic Structured Query (PSQ) 27.16 5.81 Statistical MT (SMT) 26.30 5.28 Neural MT (NMT) 26.54 5.26 Deep Relevance Ranking PACRR 24.69 5.24 PACRR-DRMM 22.15 5.14 POSIT-DRMM 23.91 6.04 Deep Relevance Ranking with Extra Features in Section 4 PACRR 27.03 5.34 PACRR-DRMM 25.46 5.50 POSIT-DRMM 26.10"
P19-1306,N18-2073,0,0.100832,"Missing"
P19-1306,D13-1175,0,0.0219708,"n Retrieval. Traditional CLIR approaches include document translation and query translation, and more research efforts are on the latter (Oard and Hackett, 1997; Oard, 1998; McCarley, 1999; Franz et al., 1999). Early methods use the dictionary to translate the user query (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996; Pirkola, 1998). Other methods include the single best SMT query translation (Chin et al., 2014) and the weighted SMT translation alternatives known as the probabilistic structured query (PSQ) (Darwish and Oard, 2003; Ture et al., 2012). Recently, Bai et al. (2010) and Sokolov et al. (2013) propose methods to learn the sparse query-document associations from supervised ranking signals on cross-lingual Wikipedia and patent data, respectively. Furthermore, Vuli´c Experiments Training and Inference. We first use the Indri1 system which uses query likelihood with Dirichlet Smoothing (Zhai and Lafferty, 2004) to preselect the documents from the collection. To build the training dataset, for each positive example in the returned list, we randomly sample one negative example from the documents returned by Indri. The model is then trained with a binary crossentropy loss. On validation o"
P19-1306,C12-1164,0,0.385793,"Missing"
P19-1306,N15-1104,0,0.0863141,"Missing"
P19-1306,P14-2080,0,0.277727,"-gram matching features. We then use maxpooling and k-max-pooling to produce the feature matrix where each row is a document-aware encoding of a query term. The final step computes the relevance score: Bilingual PACRR uses an MLP on the whole feature matrix to get the relevance score, while Bilingual PACRR-DRMM first uses an MLP on individual rows to get query term scores and then use a second layer to combine them. and Moens (2015) and Litschko et al. (2018) use cross-lingual word embeddings to represent both queries and documents as vectors and perform IR by computing the cosine similarity. Schamoni et al. (2014) and Sasaki et al. (2018) also use an automatic process to build CLIR datasets from Wikipeida articles. Neural Learning to Rank. Most of neural learning to rank models can be categorized in two groups: representation based (Huang et al., 2013; Shen et al., 2014) and interaction based (Pang et al., 2016; Guo et al., 2016; Hui et al., 2017; Xiong et al., 2017; McDonald et al., 2018). The former builds representations of query and documents independently, and the matching is performed at the final stage. The latter explicitly encodes the interaction between terms to direct capture word-level inte"
P19-1306,oard-1998-comparative,0,\N,Missing
P19-1443,W06-3000,0,0.242624,"he development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database dialogue/QA systems, which we use as references when constructing SParC. 3 Data Collection We create the SParC dataset in four stages: selecting interaction goals, creating questions, annotating SQL representations, and reviewing. Interaction goal selection To ensure thematic relevance within each question sequence, we use questions in the original Spider dataset as the thematic guidance for constructing meaningful query interactions, i.e. the interaction goal. Each sequence is based on a question in Spider a"
P19-1443,W04-2504,0,0.426463,"atabase (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL queries) querying 200 co"
P19-1443,H94-1010,0,0.882449,"ese work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The us"
P19-1443,P16-1004,0,0.0694107,"atabases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and test sets. We provide det"
P19-1443,P18-1068,0,0.0403454,"at there is plenty of room for advancement in modeling and learning on the SParC dataset. 2 Related Work Context-independent semantic parsing Early studies in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Li and Jagadish, 2014; Pasupat and Liang, 2015; Dong and Lapata, 2016; Iyer et al., 2017) were based on small and singledomain datasets such as ATIS (Hemphill et al., 1990; Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996). Recently, an increasing number of neural approaches (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018a; Dong and Lapata, 2018; Yu et al., 2018b) have started to use large and crossdomain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c). Most of them focus on converting stand-alone natural language questions to executable queries. Table 1 compares SParC with other semantic parsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is ab"
P19-1443,H90-1021,0,0.896741,"Missing"
P19-1443,W14-4337,0,0.0264241,"Missing"
P19-1443,P17-1089,0,0.361004,"Missing"
P19-1443,P17-1167,0,0.14015,"Missing"
P19-1443,W04-2509,0,0.345013,"contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL quer"
P19-1443,P16-1138,0,0.462801,"rsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is able to consider ordering issues in SQL evaluation. See more evaluation details in section 6.1. 4512 Dataset SParC ATIS (Hemphill et al., 1990; Dahl et al., 1994) Spider (Yu et al., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL lo"
P19-1443,P96-1008,0,0.654289,"Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to ac"
P19-1443,P17-1163,0,0.0488151,"Missing"
P19-1443,P15-1142,0,0.0459989,"asets used in recovering context-dependent meaning (including SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017)) contain no logical form annotations but only denotation (Berant and Liang, 2014) instead. SCONE (Long et al., 2016) contains some instructions in limited domains such as chemistry experiments. The formal representations in the dataset are world states representing state changes after each instruction instead of programs or logical forms. SequentialQA (Iyyer et al., 2017) was created by asking crowd workers to decompose some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015) into sequences of inner-related simple questions. As shown in Table 1, neither of the two datasets were annotated with query labels. Thus, to make the tasks feasible, SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017) exclude many questions with rich semantic and contextual types. For example, (Iyyer et al., 2017) requires that the answers to the questions in SequentialQA must appear in the table, and most of them can be solved by simple SQL queries with SELECT and WHERE clauses. Such direct mapping without formal query labels becomes unfeasible for complex questions. Furthermore"
P19-1443,D14-1162,1,0.110555,"ts corresponding table name and column name separated by a special dot token (i.e., table name.column name), and use the average word embedding10 of tokens in this sequence as the column header embedding hC . Decoder The decoder is implemented with another LSTM (LSTMD ) with attention to the LSTME representations of the questions in η previous turns. At each decoding step, the decoder chooses to generate either a SQL keyword (e.g., select, where, group by) or a column header. To achieve this, we use separate layers to score SQL keywords and column headers, 10 We use the 300-dimensional GloVe (Pennington et al., 2014) pretrained word embeddings. 4517 Model and finally use the softmax operation to generate the output probability distribution over both categories. 5.2 SyntaxSQLNet with history input (SyntaxSQL-con) SyntaxSQLNet is a syntax tree based neural model for the complex and cross-domain contextindependent text-to-SQL task introduced by Yu et al. (2018b). The model consists of a table-aware column attention encoder and a SQL-specific syntax tree-based decoder. The decoder adopts a set of inter-connected neural modules to generate different SQL syntax components. We extend this model by providing the"
P19-1443,N18-1203,0,0.332817,"Missing"
P19-1443,D07-1071,0,0.146156,"atural language interfaces to databases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and t"
P19-1443,P09-1110,0,0.281655,"l., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL logic it contains. In contrast to ATIS, SParC consists of a large number of complex SQL queries (with most SQL syntax components) inquiring 200 databases in 138 different domains, which contributes to its diversity in query semantics and contextual dependencies. Similar to Spider, the databases in the train, development and test sets of SParC do not overlap. Context-dependent semantic parsing with denotations Some datasets used in recovering context-dependent meaning (including SCONE (Long et"
P19-1443,P18-1135,1,0.81917,"uery labels becomes unfeasible for complex questions. Furthermore, SequentialQA contains questions based only on a single Wikipedia tables at a time. In contrast, SParC contains 200 significantly larger databases, and complex query labels with all common SQL key components. This requires a system developed for SParC to handle information needed over larger databases in different domains. Conversational QA and dialogue system Language understanding in context is also studied for dialogue and question answering systems. The development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database di"
P19-1443,N18-2093,1,0.869956,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,D18-1193,1,0.919435,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,W06-3001,0,\N,Missing
P19-1443,Q13-1005,0,\N,Missing
P19-1443,P14-1133,0,\N,Missing
P19-1443,P18-1033,1,\N,Missing
P19-1443,D18-1241,0,\N,Missing
U16-1008,P14-5010,0,0.00600708,"lations into binary relations which are represented as a graph. This graph is then used to reconstruct the complex relations by constructing tuples from selected maximal cliques scored on the graph. Li et al. (2015) make use of lexical semantics to train a model based on distant-supervision for n-ary relation extraction. However, both these systems are computationally expensive and do not scale up efficiently. Bunescu and Mooney (2005a) advocate the use of shortest path between the entities in a de4 4.1 Methodology Shortest path dependency: binary to n-ary relations We use dependency parsing (Manning et al., 2014) to help extract n-ary relations. Dependency parse provides information about word-word dependencies in the form of directed links. These dependencies capture the predicate-argument relations present in the sentence. The finite verb is taken to be the structural centre of the clause structure. All other syntactic units (words) are connected either directly (to the predicate) or indirectly (through a preposition or infinitive particle) to the verb using directed links, which are called dependencies. Each dependency consists of a head from where the directed link originates and a dependent where"
U16-1008,A97-1029,0,0.0455069,"POS tags. This method has been proven to be the best among all kernel methods to extract binary relations. However, it is yet to be confirmed if it works for extracting complex n-ary relations. Information extraction is a sequential confluence of two processes - entity extraction and relation extraction. Entity extraction refers to the task of NER wherein the task is to correctly classify an entity (like person, location, organization, etc.) out of a given sentence in a textual document. Past two decades have seen a massive body of work which aimed to improvise the entity extraction systems (Bikel et al., 1997), (Cunningham et al., 2002) and (Alfonseca and Manandhar, 2002). It is a well-explored research area which has reached maturity (Finkel et al., 2005). Most NER systems are domain dependent and require training with a new annotated corpus for a new task. Relation extraction refers to the task of finding relations among the entities which were obtained during entity extraction. A huge body of work addresses the task of extracting binary relations wherein a relation exists between two entities only. Feature-based supervised learning methods like (Kambhatla, 2004) and (Zhao and Grishman, 2005) lev"
U16-1008,P05-1061,0,0.500234,"xtraction systems utilize the identified entities to extract relations among them. Past two decades have witnessed a significant advancement in extracting binary domain-dependent relations (Kambhatla, 2004), (Zhao and Grishman, 2005) and (Bunescu and Mooney, 2005a). However, modern question answering and summarizing systems have triggered an interest in capturing detailed information in a structured and semantically coherent fashion, thus motivating the need for complex n-ary relation extraction systems (where the number of entities, n ≥ 2). Some notable n-ary relation extraction systems are (McDonald et al., 2005) and (Li et al., 2015). McDonald et al. (2005) factorized complex n-ary relation into binary relations, representing them in a graph and tried to reconstruct the complex relation by making tuples from selected maximal cliques in the graph. While they obtained reasonable precision and recall using a maximum entropy binary classifier on a corpus of 447 selected abstracts from MEDLINE, they have not explored the constituency and dependency parse features which have been proven to be efficient in relation extraction. Li et al. (2015) make use of lexical semantics to train a model based on distant-"
U16-1008,H05-1091,0,0.677727,"nd together form the backbone of a classic IE system. Entity extraction systems have achieved a high accuracy in identifying certain entities such as mention of people, places and organizations (Finkel et al., 2005). However, such named entity recognition (NER) systems are domain-dependent and do not scale up well to generalize across all entities. Relation extraction systems utilize the identified entities to extract relations among them. Past two decades have witnessed a significant advancement in extracting binary domain-dependent relations (Kambhatla, 2004), (Zhao and Grishman, 2005) and (Bunescu and Mooney, 2005a). However, modern question answering and summarizing systems have triggered an interest in capturing detailed information in a structured and semantically coherent fashion, thus motivating the need for complex n-ary relation extraction systems (where the number of entities, n ≥ 2). Some notable n-ary relation extraction systems are (McDonald et al., 2005) and (Li et al., 2015). McDonald et al. (2005) factorized complex n-ary relation into binary relations, representing them in a graph and tried to reconstruct the complex relation by making tuples from selected maximal cliques in the graph. W"
U16-1008,E12-2021,0,0.0854726,"Missing"
U16-1008,P04-1054,0,0.109381,"with a new annotated corpus for a new task. Relation extraction refers to the task of finding relations among the entities which were obtained during entity extraction. A huge body of work addresses the task of extracting binary relations wherein a relation exists between two entities only. Feature-based supervised learning methods like (Kambhatla, 2004) and (Zhao and Grishman, 2005) leverage the syntactic and semantic features. Exploration of a large feature space in polynomial computational time motivated the development of kernel based methods like tree kernels (Zelenko et al., 2003) and (Culotta and Sorensen, 2004), subsequence kernels (Bunescu and Mooney, 2005b) and dependency tree kernel (Bunescu and Mooney, 2005a). Open IE system (Banko et al., 2007) gives a sound method to generalize the relation extraction process, however the system does not give any insights to extract complex n-ary relations. With advances in biomedical text mining and modern question answering systems, complex nary relation extraction is gaining attention wherein the task is to detect and extract relations existing between two or more entities in a given sentence. McDonald et al. (2005) attempt to solve this problem by factoriz"
U16-1008,P05-1052,0,0.273833,"se components are sequential and together form the backbone of a classic IE system. Entity extraction systems have achieved a high accuracy in identifying certain entities such as mention of people, places and organizations (Finkel et al., 2005). However, such named entity recognition (NER) systems are domain-dependent and do not scale up well to generalize across all entities. Relation extraction systems utilize the identified entities to extract relations among them. Past two decades have witnessed a significant advancement in extracting binary domain-dependent relations (Kambhatla, 2004), (Zhao and Grishman, 2005) and (Bunescu and Mooney, 2005a). However, modern question answering and summarizing systems have triggered an interest in capturing detailed information in a structured and semantically coherent fashion, thus motivating the need for complex n-ary relation extraction systems (where the number of entities, n ≥ 2). Some notable n-ary relation extraction systems are (McDonald et al., 2005) and (Li et al., 2015). McDonald et al. (2005) factorized complex n-ary relation into binary relations, representing them in a graph and tried to reconstruct the complex relation by making tuples from selected m"
U16-1008,P05-1045,0,0.600541,"works for extracting complex n-ary relations. Information extraction is a sequential confluence of two processes - entity extraction and relation extraction. Entity extraction refers to the task of NER wherein the task is to correctly classify an entity (like person, location, organization, etc.) out of a given sentence in a textual document. Past two decades have seen a massive body of work which aimed to improvise the entity extraction systems (Bikel et al., 1997), (Cunningham et al., 2002) and (Alfonseca and Manandhar, 2002). It is a well-explored research area which has reached maturity (Finkel et al., 2005). Most NER systems are domain dependent and require training with a new annotated corpus for a new task. Relation extraction refers to the task of finding relations among the entities which were obtained during entity extraction. A huge body of work addresses the task of extracting binary relations wherein a relation exists between two entities only. Feature-based supervised learning methods like (Kambhatla, 2004) and (Zhao and Grishman, 2005) leverage the syntactic and semantic features. Exploration of a large feature space in polynomial computational time motivated the development of kernel"
U16-1021,U16-1020,0,0.0853703,"orm of unstructured natural language text containing ambiguous name entities. A name entity mention may relate to multiple known entities. For example, the entity mention “New York” may refer to the city of New York or the movie New York which was released in 2009. Entity linking (EL) is the process of resolving disambiguity between textual entity mentions and the correct entity node in the knowledge base (KB). EL systems usually rely on semantic resources like Wikipedia as endpoints for disambiguation (Shen et al., 2015), however, 1 https://inclass.kaggle.com/c/alta-2016challenge/leaderboard Chisholm et al. (2016) provide a relaxed definition of a KB as any uniform resource locator (URL) which reliably disambiguates linked mentions on the web (Chisholm et al., 2016a). This relaxed definition has motivated the shared task of ALTA 2016 (Chisholm et al., 2016b). The task organizers provided manually selected URL pairs from a heterogenous collection of websites including popular social networking websites like LinkedIn, Twitter, ResearchGate; knowledge bases like Wikipedia, IMDB and news websites like NDTV and Economic Times. The participants are asked to classify whether a given pair of URLs refer to the"
U16-1021,D07-1074,0,0.269983,"nts are asked to classify whether a given pair of URLs refer to the same underlying entity. For example, in Figure 1, URLs in the pair &lt; UA1 , UA2 > refer to the same entity “Barack Obama” whereas URLs in the pair &lt; UB1 , UB2 > refer to two different entities “Donald Trump” and “Ivanka Trump”. Figure 1: Example of URL pairs Considerable research has been done in the field of EL using existing KB like DBpedia (Auer et al., 2007), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2008) and KnowItAll (Etzioni et al., 2004). Wikipedia has proven to be a great resource in solving EL tasks (Cucerzan, 2007); (Milne and Witten, 2008) where dictionary-based techniques, contextual features and entity references have been used to train classifiers. Chisholm et al. (2016) study link behaviour and propose a KB discovery method using URL path features by inferring endpoints via logistic regression. We adopt a two stage approach to solve this problem. First, our system determines the possible underlying entities for a given URL using entity features obtained from the crawled text with the Gitansh Khirbat, Jianzhong Qi and Rui Zhang. 2016. Disambiguating Entities Referred by Web Endpoints using Tree Ense"
U16-1021,P05-1045,0,0.0212133,"both URLs refer to the same underlying entity. Contextual features in and around the entities are exploited and a tree ensemble model is trained for this task. The rest of the paper is organized as follows. Section 2 describes the methodology in detail. Section 3 describes the experiments and results. Section 4 discusses the error analysis of the obtained results and Section 5 concludes the paper. 2.1.2 Named Entity Recognition The next step is to make use of a named entity recognition (NER) system to identify all the entities present in the crawled text. We make use of Stanford’s NER system (Finkel et al., 2005) which uses a model trained on MUC6, MUC7 and ACE 2002 datasets to classify words into three categories namely Location, Person and Organization. The details about this NER system is beyond the scope of this paper and can be obtained from Finkel et al. (2005). 2 2.1.3 Entity Ranking Entity ranking is the key step in Stage 1. It trains a logistic regression model using the features obtained in Sections 2.1.1 and 2.1.2 to assign a score for each entity identified in the crawled text. We consider four main features: Methodology The goal of ALTA 2016 shared task is to determine if a given pair of"
U16-1021,N13-1122,0,0.0137207,"iguation The second stage of our system solves the problem of determining whether a given pair of URLs refer to the same underlying entity. It makes use of the output of Stage 1 and involves two components as described below. Figure 2: System pipeline 2.2.1 Feature Extraction This module makes use of contextual features in and around the identified entities. A concept vector is created to represent the semantic content of the crawled text from the URL. This concept vector contains TF-IDF of URL path, page title and top-3 entity mentions obtained from Stage 1 and adds features of bag of words (Guo et al., 2013); (Ratinov et al., 2011) and anchor texts (Kulkarni et al., 2009) as described below. • Bag of words - TF-IDF summary of the entire crawled text is generated and top-20 words after removal of stopwords are chosen as the representative bag of words. • Anchor texts - The URLs referred in all the anchor texts are preprocessed according to Section 2.1.1 to obtain the URL endpoint. A vector containing all such endpoints and anchor texts is used to define a TF-IDF vector for the given URL pair. 2.2.2 XGBoost The features defined in Section 2.2.1 are used to train a supervised tree ensemble classifie"
U16-1021,P11-1138,0,0.0362458,"stage of our system solves the problem of determining whether a given pair of URLs refer to the same underlying entity. It makes use of the output of Stage 1 and involves two components as described below. Figure 2: System pipeline 2.2.1 Feature Extraction This module makes use of contextual features in and around the identified entities. A concept vector is created to represent the semantic content of the crawled text from the URL. This concept vector contains TF-IDF of URL path, page title and top-3 entity mentions obtained from Stage 1 and adds features of bag of words (Guo et al., 2013); (Ratinov et al., 2011) and anchor texts (Kulkarni et al., 2009) as described below. • Bag of words - TF-IDF summary of the entire crawled text is generated and top-20 words after removal of stopwords are chosen as the representative bag of words. • Anchor texts - The URLs referred in all the anchor texts are preprocessed according to Section 2.1.1 to obtain the URL endpoint. A vector containing all such endpoints and anchor texts is used to define a TF-IDF vector for the given URL pair. 2.2.2 XGBoost The features defined in Section 2.2.1 are used to train a supervised tree ensemble classifier called extreme gradien"
U16-1021,W16-1302,0,0.188623,"orm of unstructured natural language text containing ambiguous name entities. A name entity mention may relate to multiple known entities. For example, the entity mention “New York” may refer to the city of New York or the movie New York which was released in 2009. Entity linking (EL) is the process of resolving disambiguity between textual entity mentions and the correct entity node in the knowledge base (KB). EL systems usually rely on semantic resources like Wikipedia as endpoints for disambiguation (Shen et al., 2015), however, 1 https://inclass.kaggle.com/c/alta-2016challenge/leaderboard Chisholm et al. (2016) provide a relaxed definition of a KB as any uniform resource locator (URL) which reliably disambiguates linked mentions on the web (Chisholm et al., 2016a). This relaxed definition has motivated the shared task of ALTA 2016 (Chisholm et al., 2016b). The task organizers provided manually selected URL pairs from a heterogenous collection of websites including popular social networking websites like LinkedIn, Twitter, ResearchGate; knowledge bases like Wikipedia, IMDB and news websites like NDTV and Economic Times. The participants are asked to classify whether a given pair of URLs refer to the"
