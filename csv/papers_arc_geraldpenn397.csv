2021.textgraphs-1.11,Structural Realization with {GGNN}s,2021,-1,-1,2,0,738,jinman zhao,Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15),0,"In this paper, we define an abstract task called structural realization that generates words given a prefix of words and a partial representation of a parse tree. We also present a method for solving instances of this task using a Gated Graph Neural Network (GGNN). We evaluate it with standard accuracy measures, as well as with respect to perplexity, in which its comparison to previous work on language modelling serves to quantify the information added to a lexical selection task by the presence of syntactic knowledge. That the addition of parse-tree-internal nodes to this neural model should improve the model, with respect both to accuracy and to more conventional measures such as perplexity, may seem unsurprising, but previous attempts have not met with nearly as much success. We have also learned that transverse links through the parse tree compromise the model{'}s accuracy at generating adjectival and nominal parts of speech."
2021.smm4h-1.1,Statistically Evaluating Social Media Sentiment Trends towards {COVID}-19 Non-Pharmaceutical Interventions with Event Studies,2021,-1,-1,4,1,1133,jingcheng niu,Proceedings of the Sixth Social Media Mining for Health ({\\#}SMM4H) Workshop and Shared Task,0,"In the midst of a global pandemic, understanding the public{'}s opinion of their government{'}s policy-level, non-pharmaceutical interventions (NPIs) is a crucial component of the health-policy-making process. Prior work on CoViD-19 NPI sentiment analysis by the epidemiological community has proceeded without a method for properly attributing sentiment changes to events, an ability to distinguish the influence of various events across time, a coherent model for predicting the public{'}s opinion of future events of the same sort, nor even a means of conducting significance tests. We argue here that this urgently needed evaluation method does already exist. In the financial sector, event studies of the fluctuations in a publicly traded company{'}s stock price are commonplace for determining the effects of earnings announcements, product placements, etc. The same method is suitable for analysing temporal sentiment variation in the light of policy-level NPIs. We provide a case study of Twitter sentiment towards policy-level NPIs in Canada. Our results confirm a generally positive connection between the announcements of NPIs and Twitter sentiment, and we document a promising correlation between the results of this study and a public-health survey of popular compliance with NPIs."
2021.iwpt-1.2,Proof Net Structure for Neural {L}ambek Categorial Parsing,2021,-1,-1,2,1,5810,aditya bhargava,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"In this paper, we present the first statistical parser for Lambek categorial grammar (LCG), a grammatical formalism for which the graphical proof method known as *proof nets* is applicable. Our parser incorporates proof net structure and constraints into a system based on self-attention networks via novel model elements. Our experiments on an English LCG corpus show that incorporating term graph structure is helpful to the model, improving both parsing accuracy and coverage. Moreover, we derive novel loss functions by expressing proof net constraints as differentiable functions of our model output, enabling us to train our parser without ground-truth derivations."
2021.eacl-main.294,Reanalyzing the Most Probable Sentence Problem: A Case Study in Explicating the Role of Entropy in Algorithmic Complexity,2021,-1,-1,2,1,10948,eric corlett,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"When working with problems in natural language processing, we can find ourselves in situations where the traditional measurements of descriptive complexity are ineffective at describing the behaviour of our algorithms. It is easy to see why {---} the models we use are often general frameworks into which difficult-to-define tasks can be embedded. These frameworks can have more power than we typically use, and so complexity measures such as worst-case running time can drastically overestimate the cost of running our algorithms. In particular, they can make an apparently tractable problem seem NP-complete. Using empirical studies to evaluate performance is a necessary but incomplete method of dealing with this mismatch, since these studies no longer act as a guarantee of good performance. In this paper we use statistical measures such as entropy to give an updated analysis of the complexity of the NP-complete Most Probable Sentence problem for pCFGs, which can then be applied to word sense disambiguation and inference tasks. We can bound both the running time and the error in a simple search algorithm, allowing for a much faster search than the NP-completeness of this problem would suggest."
2021.eacl-main.306,"The {C}hinese Remainder Theorem for Compact, Task-Precise, Efficient and Secure Word Embeddings",2021,-1,-1,2,0,10966,patricia thaine,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"The growing availability of powerful mobile devices and other edge devices, together with increasing regulatory and security concerns about the exchange of personal information across networks of these devices has challenged the Computational Linguistics community to develop methods that are at once fast, space-efficient, accurate and amenable to secure encoding schemes such as homomorphic encryption. Inspired by recent work that restricts floating point precision to speed up neural network training in hardware-based SIMD, we have developed a method for compressing word vector embeddings into integers using the Chinese Reminder Theorem that speeds up addition by up to 48.27{\%} and at the same time compresses GloVe word embedding libraries by up to 25.86{\%}. We explore the practicality of this simple approach by investigating the trade-off between precision and performance in two NLP tasks: compositional semantic relatedness and opinion target sentiment classification. We find that in both tasks, lowering floating point number precision results in negligible changes to performance."
2020.repl4nlp-1.23,Supertagging with {CCG} primitives,2020,-1,-1,2,1,5810,aditya bhargava,Proceedings of the 5th Workshop on Representation Learning for NLP,0,"In CCG and other highly lexicalized grammars, supertagging a sentence{'}s words with their lexical categories is a critical step for efficient parsing. Because of the high degree of lexicalization in these grammars, the lexical categories can be very complex. Existing approaches to supervised CCG supertagging treat the categories as atomic units, even when the categories are not simple; when they encounter words with categories unseen during training, their guesses are accordingly unsophisticated. In this paper, we make use of the primitives and operators that constitute the lexical categories of categorial grammars. Instead of opaque labels, we treat lexical categories themselves as linear sequences. We present an LSTM-based model that replaces standard word-level classification with prediction of a sequence of primitives, similarly to LSTM decoders. Our model obtains state-of-the-art word accuracy for single-task English CCG supertagging, increases parser coverage and F1, and is able to produce novel categories. Analysis shows a synergistic effect between this decomposed view and incorporation of prediction history."
2020.lrec-1.271,Temporal Histories of Epidemic Events ({THEE}): A Case Study in Temporal Annotation for Public Health,2020,-1,-1,3,1,1133,jingcheng niu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a new temporal annotation standard, THEE-TimeML, and a corpus TheeBank enabling precise temporal information extraction (TIE) for event-based surveillance (EBS) systems in the public health domain. Current EBS must estimate the occurrence time of each event based on coarse document metadata such as document publication time. Because of the complicated language and narration style of news articles, estimated case outbreak times are often inaccurate or even erroneous. Thus, it is necessary to create annotation standards and corpora to facilitate the development of TIE systems in the public health domain to address this problem.We will discuss the adaptations that have proved necessary for this domain as we present THEE-TimeML and TheeBank. Finally, we document the corpus annotation process, and demonstrate the immediate benefit to public health applications brought by the annotations."
2020.lrec-1.815,{FAB}: The {F}rench Absolute Beginner Corpus for Pronunciation Training,2020,-1,-1,3,0,18249,sean robertson,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We introduce the French Absolute Beginner (FAB) speech corpus. The corpus is intended for the development and study of Computer-Assisted Pronunciation Training (CAPT) tools for absolute beginner learners. Data were recorded during two experiments focusing on using a CAPT system in paired role-play tasks. The setting grants FAB three distinguishing features from other non-native corpora: the experimental setting is ecologically valid, closing the gap between training and deployment; it features a label set based on teacher feedback, allowing for context-sensitive CAPT; and data have been primarily collected from absolute beginners, a group often ignored. Participants did not read prompts, but instead recalled and modified dialogues that were modelled in videos. Unable to distinguish modelled words solely from viewing videos, speakers often uttered unintelligible or out-of-L2 words. The corpus is split into three partitions: one from an experiment with minimal feedback; another with explicit, word-level feedback; and a third with supplementary read-and-record data. A subset of words in the first partition has been labelled as more or less native, with inter-annotator agreement reported. In the explicit feedback partition, labels are derived from the experiment{'}s online feedback. The FAB corpus is scheduled to be made freely available by the end of 2020."
2020.eval4nlp-1.11,Grammaticality and Language Modelling,2020,-1,-1,2,1,1133,jingcheng niu,Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems,0,"Ever since Pereira (2000) provided evidence against Chomsky{'}s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as {``}psycholinguistic subjects{''} and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al. (2017) and Sprouse et al. (2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coefficient (MCC), although probably because CoLA{'}s seminal publication was using it to compute inter-rater reliabilities. Researchers working in this area have used other accuracy and correlation scores, often driven by a need to reconcile and compare various discrete and continuous variables with each other. The score that we will advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that we are aware of is Sprouse et al. (2018a), and that paper actually applied it backwards (with some justification) so that the language model probability was treated as the discrete binary variable by setting a threshold. With the PBC in mind, we will first reappraise some recent work in syntactically targeted linguistic evaluations (Hu et al., 2020), arguing that while their experimental design sets a new high watermark for this topic, their results may not prove what they have claimed. We then turn to the task-independent assessment of language models as grammaticality classifiers. Prior to the introduction of the GLUE leaderboard, the vast majority of this assessment was essentially anecdotal, and we find the use of the MCC in this regard to be problematic. We conduct several studies with PBCs to compare several popular language models. We also study the effects of several variables such as normalization and data homogeneity on PBC."
P19-1550,Rationally Reappraising {ATIS}-based Dialogue Systems,2019,0,0,2,1,1133,jingcheng niu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The Air Travel Information Service (ATIS) corpus has been the most common benchmark for evaluating Spoken Language Understanding (SLU) tasks for more than three decades since it was released. Recent state-of-the-art neural models have obtained F1-scores near 98{\%} on the task of slot filling. We developed a rule-based grammar for the ATIS domain that achieves a 95.82{\%} F1-score on our evaluation set. In the process, we furthermore discovered numerous shortcomings in the ATIS corpus annotation, which we have fixed. This paper presents a detailed account of these shortcomings, our proposed repairs, our rule-based grammar and the neural slot-filling architectures associated with ATIS. We also rationally reappraise the motivations for choosing a neural architecture in view of this account. Fixing the annotation errors results in a relative error reduction of between 19.4 and 52{\%} across all architectures. We nevertheless argue that neural models must play a different role in ATIS dialogues because of the latter{'}s lack of variety."
W17-4112,Vowel and Consonant Classification through Spectral Decomposition,2017,0,0,2,0,10966,patricia thaine,Proceedings of the First Workshop on Subword and Character Level Models in {NLP},0,"We consider two related problems in this paper. Given an undeciphered alphabetic writing system or mono-alphabetic cipher, determine: (1) which of its letters are vowels and which are consonants; and (2) whether the writing system is a vocalic alphabet or an abjad. We are able to show that a very simple spectral decomposition based on character co-occurrences provides nearly perfect performance with respect to answering both question types."
P16-1197,Evaluating Sentiment Analysis in the Context of Securities Trading,2016,24,3,3,0,34536,siavash kazemian,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"There are numerous studies suggesting that published news stories have an important effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks mentioned in the news. There is even some published research suggesting that automated sentiment analysis of news documents, quarterly reports, blogs and/or twitter data can be productively used as part of a trading strategy. This paper presents just such a family of trading strategies, and then uses this application to re-examine some of the tacit assumptions behind how sentiment analyzers are generally evaluated, in spite of the contexts of their application. This discrepancy comes at a cost."
W14-2620,Evaluating Sentiment Analysis Evaluation: A Case Study in Securities Trading,2014,22,2,3,0,34536,siavash kazemian,"Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"There are numerous studies suggesting that published news stories have an important effect on the direction of the stock market, its volatility, the volume of trades, and the value of individual stocks mentioned in the news. There is even some published research suggesting that automated sentiment analysis of news documents, quarterly reports, blogs and/or Twitter data can be productively used as part of a trading strategy. This paper presents just such a family of trading strategies, and then uses this application to re-examine some of the tacit assumptions behind how sentiment analyzers are generally evaluated, in spite of the contexts of their application. This discrepancy comes at a cost."
D14-1085,Unsupervised Sentence Enhancement for Automatic Summarization,2014,24,23,2,1,3669,jackie cheung,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"We present sentence enhancement as a novel technique for text-to-text generation in abstractive summarization. Compared to extraction or previous approaches to sentence fusion, sentence enhancement increases the range of possible summary sentences by allowing the combination of dependency subtrees from any sentence from the source text. Our experiments indicate that our approach yields summary sentences that are competitive with a sentence fusion baseline in terms of content quality, but better in terms of grammaticality, and that the benefit of sentence enhancement relies crucially on an event coreference resolution algorithm using distributional semantics. We also consider how text-to-text generation approaches to summarization can be extended beyond the source text by examining how human summary writers incorporate source-text-external elements into their summary sentences."
W13-3009,Why Letter Substitution Puzzles are Not Hard to Solve: A Case Study in Entropy and Probabilistic Search-Complexity,2013,6,0,2,1,10948,eric corlett,Proceedings of the 13th Meeting on the Mathematics of Language ({M}o{L} 13),0,"In this paper we investigate the theoretical causes of the disparity between the theoretical and practical running times for the A algorithm proposed in Corlett and Penn (2010) for deciphering letter-substitution ciphers. We argue that the difference seen is due to the relatively low entropies of the probability distributions of character transitions seen in natural language, and we develop a principled way of incorporating entropy into our complexity analysis. Specifically, we find that the low entropy of natural languages can allow us, with high probability, to bound the depth of the heuristic values expanded in the search. This leads to a novel probabilistic bound on search depth in these tasks."
P13-5007,The mathematics of language learning,2013,0,0,2,0,5843,andras kornai,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials),0,"Over the past decade, attention has gradually shifted from the estimation of parameters to the learning of linguistic structure (for a survey see Smith 2011). The Mathematics of Language (MOL) SIG put together this tutorial, composed of three lectures, to highlight some alternative learning paradigms in speech, syntax, and semantics in the hopes of accelerating this trend. Compounding the enormous variety of formal models one may consider is the bewildering range of ML techniques one may bring to bear. In addition to the surprisingly useful classical techniques inherited from multivariate statistics such as Principal Component Analysis (PCA, Pearson 1901) and Linear Discriminant Analysis (LDA, Fisher 1936), computational linguists have experimented with a broad range of neural net, nearest neighbor, maxent, genetic/evolutionary, decision tree, max margin, boost, simulated annealing, and graphical model learners. While many of these learners became standard in various domains of ML, within CL the basic HMM approach proved surprisingly resilient, and it is only very recently that deep learning techniques from neural computing are becoming competitive not just in speech, but also in OCR, paraphrase, sentiment analysis, parsing and vector-based semantic representations. The first lecture will provide a mathematical introduction to some of the fundamental techniques that lie beneath these linguistic applications of neural networks, such as: BFGS optimization, finite difference approximations of Hessians and Hessianfree optimization, contrastive divergence and variational inference."
P13-1039,Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors,2013,29,6,2,1,3669,jackie cheung,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization."
P13-1121,Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain,2013,28,20,2,1,3669,jackie cheung,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed."
W12-2604,Ecological Validity and the Evaluation of Speech Summarization Quality,2012,14,2,3,0,42300,anthony mccallum,Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization,0,"There is little evidence of widespread adoption of speech summarization systems. This may be due in part to the fact that the natural language heuristics used to generate summaries are often optimized with respect to a class of evaluation measures that, while computationally and experimentally inexpensive, rely on subjectively selected gold standards against which automatically generated summaries are scored. This evaluation protocol does not take into account the usefulness of a summary in assisting the listener in achieving his or her goal.n n In this paper we study how current measures and methods for evaluating summarization systems compare to human-centric evaluation criteria. For this, we have designed and conducted an ecologically valid evaluation that determines the value of a summary when embedded in a task, rather than how closely a summary resembles a gold standard. The results of our evaluation demonstrate that in the domain of lecture summarization, the wellknown baseline of maximal marginal relevance (Carbonell and Goldstein, 1998) is statistically significantly worse than human-generated extractive summaries, and even worse than having no summary at all in a simple quiz-taking task. Priming seems to have no statistically significant effect on the usefulness of the human summaries. In addition, ROUGE scores and, in particular, the contextfree annotations that are often supplied to ROUGE as references, may not always be reliable as inexpensive proxies for ecologically valid evaluations. In fact, under some conditions, relying exclusively on ROUGE may even lead to scoring human-generated summaries that are inconsistent in their usefulness relative to using no summaries very favourably."
E12-1005,Evaluating Distributional Models of Semantics for Syntactically Invariant Inference,2012,27,4,2,1,3669,jackie cheung,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"A major focus of current work in distributional models of semantics is to construct phrase representations compositionally from word representations. However, the syntactic contexts which are modelled are usually severely limited, a fact which is reflected in the lexical-level WSD-like evaluation methods used. In this paper, we broaden the scope of these models to build sentence-level representations, and argue that phrase representations are best evaluated in terms of the inference decisions that they support, invariant to the particular syntactic constructions used to guide composition. We propose two evaluation methods in relation classification and QA which reflect these goals, and apply several recent compositional distributional models to the tasks. We find that the models outperform a simple lemma overlap baseline slightly, demonstrating that distributional approaches can already be useful for tasks requiring deeper inference."
E12-1071,Unsupervised Detection of Downward-Entailing Operators By Maximizing Classification Certainty,2012,9,2,2,1,3669,jackie cheung,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose an unsupervised, iterative method for detecting downward-entailing operators (DEOs), which are important for deducing entailment relations between sentences. Like the distillation algorithm of Danescu-Niculescu-Mizil et al. (2009), the initialization of our method depends on the correlation between DEOs and negative polarity items (NPIs). However, our method trusts the initialization more and aggressively separates likely DEOs from spurious distractors and other words, unlike distillation, which we show to be equivalent to one iteration of EM prior re-estimation. Our method is also am enable to a bootstrapping method that co-learns DEOs and NPIs, and achieves the best results in identifying DEOs in two corpora."
C12-2092,On {P}anini and the Generative Capacity of Contextualized Replacement Systems,2012,5,0,1,1,739,gerald penn,Proceedings of {COLING} 2012: Posters,0,"This paper re-examines the widely held belief that the formalism underlying the rule system propounded by the ancient Indian grammarian, Pxc4x81n. ini (ca. 450xe2x80x93350 BCE), either anticipates or converges upon the same expressive power found in finite state control systems or the context-free languages that are used in programming language theory and computational linguistics. While there is indeed a striking but cosmetic resemblance to the contextualized rewriting systems used by modern morphologists and phonologists, a subtle difference in how rules are prevented from applying cyclically leads to a massive difference in generative capacity. The formalism behind Pxc4x81n. inian grammar, in fact, generates string languages not even contained within any of the multiple-component tree-adjoining languages, MCTAL(k), for any k. There is ample evidence, nevertheless, that Pxc4x81n. inixe2x80x99s grammar itself judiciously avoided the potential pitfalls of this unconstrained formalism to articulate a large-coverage, but seemingly very tractable grammar of the Sanskrit language."
C12-1051,Flexible Structural Analysis of Near-Meet-Semilattices for Typed Unification-Based Grammar Design,2012,9,0,2,0,43749,rouzbeh farahmand,Proceedings of {COLING} 2012,0,"We present a new method for directly working with typed unification grammars in which type unification is not well-defined. This is often the case, as large-scale HPSG grammars now usually have type systems for which many pairs do not have least upper bounds. Our method yields a unification algorithm that compiles quickly and yet is nearly as fast during parsing as one that requires least upper bounds. The method also provides a natural naming convention for unification results in cases where no user-defined type exists."
I11-1057,Indexing Spoken Documents with Hierarchical Semantic Structures: Semantic Tree-to-string Alignment Models,2011,27,2,3,1,1624,xiaodan zhu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper addresses a semantic tree-tostring alignment problem: indexing spoken documents with known hierarchical semantic structures, with the goal to help index and access such archives. We propose and study a number of alignment models of different modeling capabilities and time complexities to provide a comprehensive understanding of these unsupervised models and hence the problem itself."
P10-1020,Entity-Based Local Coherence Modelling Using Topological Fields,2010,24,7,2,1,3669,jackie cheung,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly."
P10-1035,Accurate Context-Free Parsing with {C}ombinatory {C}ategorial {G}rammar,2010,11,36,2,0,44746,timothy fowler,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCG-bank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy."
P10-1106,An Exact {A}* Method for Deciphering Letter-Substitution Ciphers,2010,7,14,2,1,10948,eric corlett,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment problems."
P10-1153,A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices,2010,13,4,4,0,45714,matthew skala,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques."
D10-1003,Utilizing Extra-Sentential Context for Parsing,2010,20,3,2,1,3669,jackie cheung,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Syntactic consistency is the preference to reuse a syntactic construction shortly after its appearance in a discourse. We present an analysis of the WSJ portion of the Penn Tree-bank, and show that syntactic consistency is pervasive across productions with various left-hand side nonterminals. Then, we implement a reranking constituent parser that makes use of extra-sentential context in its feature set. Using a linear-chain conditional random field, we improve parsing accuracy over the generative baseline parser on the Penn Treebank WSJ corpus, rivalling a similar model that does not make use of context. We show that the context-aware and the context-ignorant rerankers perform well on different subsets of the evaluation data, suggesting a combined approach would provide further improvement. We also compare parses made by models, and suggest that context can be useful for parsing by capturing structural dependencies between sentences as opposed to lexically governed dependencies."
C10-2177,Imposing Hierarchical Browsing Structures onto Spoken Documents,2010,26,3,3,1,1624,xiaodan zhu,Coling 2010: Posters,0,"This paper studies the problem of imposing a known hierarchical structure onto an unstructured spoken document, aiming to help browse such archives. We formulate our solutions within a dynamic-programming-based alignment framework and use minimum error-rate training to combine a number of global and hierarchical constraints. This pragmatic approach is computationally efficient. Results show that it outperforms a baseline that ignores the hierarchical and global features and the improvement is consistent on transcripts with different WERs. Directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results."
2010.jeptalnrecital-invite.3,The Quantitative Study of Writing Systems,2010,-1,-1,1,1,739,gerald penn,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Conf{\\'e}rences invit{\\'e}es,0,
P09-1008,Topological Field Parsing of {G}erman,2009,16,9,2,1,3669,jackie cheung,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Freer-word-order languages such as German exhibit linguistic phenomena that present unique challenges to traditional CFG parsing. Such phenomena produce discontinuous constituents, which are not naturally modelled by projective phrase structure trees. In this paper, we examine topological field parsing, a shallow form of parsing which identifies the major sections of a sentence in relation to the clausal main verb and the subordinating heads. We report the results of topological field parsing of German using the unlexicalized, latent variable-based Berkeley parser (Petrov et al., 2006) Without any language- or model-dependent adaptation, we achieve state-of-the-art results on the TuBa-D/Z corpus, and a modified NE-GRA corpus that has been automatically annotated with topological fields (Becker and Frank, 2002). We also perform a qualitative error analysis of the parser output, and discuss strategies to further improve the parsing results."
P09-1062,Summarizing multiple spoken documents: finding evidence from untranscribed audio,2009,23,30,2,1,1624,xiaodan zhu,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"This paper presents a model for summarizing multiple untranscribed spoken documents. Without assuming the availability of transcripts, the model modifies a recently proposed unsupervised algorithm to detect re-occurring acoustic patterns in speech and uses them to estimate similarities between utterances, which are in turn used to identify salient utterances and remove redundancies. This model is of interest due to its independence from spoken language transcription, an error-prone and resource-intensive process, its ability to integrate multiple sources of information on the same topic, and its novel use of acoustic patterns that extends previous work on low-level prosodic feature detection. We compare the performance of this model with that achieved using manual and automatic transcripts, and find that this new approach is roughly equivalent to having access to ASR transcripts with word error rates in the 33--37% range without actually having to do the ASR, plus it better handles utterances with out-of-vocabulary words."
P09-1086,Improving Automatic Speech Recognition for Lectures through Transformation-based Rules Learned from Minimal Data,2009,27,9,2,1,18250,cosmin munteanu,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"We demonstrate that transformation-based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9%. Our method is distinguished from earlier related work by its robustness to small amounts of training data, and its resulting efficiency, in spite of its use of true word error rate computations as a rule scoring function."
P08-5006,Interactive Visualization for Computational Linguistics,2008,1,8,2,0,47856,christopher collins,Tutorial Abstracts of ACL-08: HLT,0,"Interactive information visualization is an emerging and powerful research technique that can be used to understand models of language and their abstract representations. Much of what computational linguists fall back upon to improve NLP applications and to model language understanding is structure that has, at best, only an indirect attestation in observable data. An important part of our research progress thus depends on our ability to fully investigate, explain, and explore these structures, both empirically and relative to accepted linguistic theory. The sheer complexity of these abstract structures, and the observable patterns on which they are based, usually limits their accessibility --- often even to the researchers creating or attempting to learn them."
P08-1054,A Critical Reassessment of Evaluation Baselines for Speech Summarization,2008,13,66,1,1,739,gerald penn,Proceedings of ACL-08: HLT,1,"We assess the current state of the art in speech summarization, by comparing a typical summarizer on two different domains: lecture data and the SWITCHBOARD corpus. Our results cast significant doubt on the merits of this areaxe2x80x99s accepted evaluation standards in terms of: baselines chosen, the correspondence of results to our intuition of what xe2x80x9csummariesxe2x80x9d should be, and the value of adding speechrelated features to summarizers that already use transcripts from automatic speech recognition (ASR) systems. 1 Problem definition and related literature Speech is arguably the most basic, most natural form of human communication. The consistent demand for and increasing availability of spoken audio content on web pages and other digital media should therefore come as no surprise. Along with this availability comes a demand for ways to better navigate through speech, which is inherently more linear or sequential than text in its traditional delivery. Navigation connotes a number of specific tasks, including search, but also browsing (Hirschberg et al., 1999) and skimming, which can involve far more analysis and manipulation of content than the spoken document retrieval tasks of recent NIST fame (1997 2000). These would include time compression of the speech signal and/or xe2x80x9cdichoticxe2x80x9d presentations of speech, in which a different audio track is presented to either ear (Cherry and Taylor, 1954; Ranjan et al., 2006). Time compression of speech, on the other hand, excises small slices of digitized speech data out of the signal so that the voices speak all of the content but more quickly. The excision can either be fixed rate, for which there have been a number of experiments to detect comprehension limits, or variable rate, where the rate is determined by pause detection and shortening (Arons, 1992), pitch (Arons, 1994) or longer-term measures of linguistic salience (Tucker and Whittaker, 2006). A very short-term measure based on spectral entropy can also be used (Ajmal et al., 2007), which has the advantage that listeners cannot detect the variation in rate, but they nevertheless comprehend better than fixed-rate baselines that preserve pitch periods. With or without variable rates, listeners can easily withstand a factor of two speed-up, but Likert response tests definitively show that they absolutely hate doing it (Tucker and Whittaker, 2006) relative to word-level or utterance-level excisive methods, which would include the summarization-based strategy that we pursue in this paper. The strategy we focus on here is summarization, in its more familiar construal from computational linguistics and information retrieval. We view it as an extension of the text summarization problem in which we use automatically prepared, imperfect textual transcripts to summarize speech. Other details are provided in Section 2.2. Early work on speech summarization was either domainrestricted (Kameyama and Arima, 1994), or prided itself on not using ASR at all, because of its unreliability in open domains (Chen and Withgott, 1992). Summaries of speech, however, can still be delivered audially (Kikuchi et al., 2003), even when (noisy) transcripts are used."
W06-0402,Control Strategies for Parsing with Freer Word-Order Languages,2006,12,0,1,1,739,gerald penn,Proceedings of the Third Workshop on Constraints and Language Processing,0,"We provide two different methods for bounding search when parsing with freer word-order languages. Both of these can be thought of as exploiting alternative sources of constraints not commonly used in CFGs, in order to make up for the lack of more rigid word-order and the standard algorithms that use the assumption of rigid word-order implicitly. This work is preliminary in that it has not yet been evaluated on a large-scale grammar/corpus for a freer word-order language."
N06-2030,Quantitative Methods for Classifying Writing Systems,2006,7,2,1,1,739,gerald penn,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"We describe work in progress on using quantitative methods to classify writing systems according to Sproat's (2000) classification grid using unannotated data. We specifically propose two quantitative tests for determining the type of phonography in a writing system, and its degree of logography, respectively."
N06-2050,"Comparing the roles of textual, acoustic and spoken-language features on spontaneous-conversation summarization",2006,7,11,2,1,1624,xiaodan zhu,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"This paper is concerned with the summarization of spontaneous conversations. Compared with broadcast news, which has received intensive study, spontaneous conversations have been less addressed in the literature. Previous work has focused on textual features extracted from transcripts. This paper explores and compares the effectiveness of both textual features and speech-related features. The experiments show that these features incrementally improve summarization performance. We also find that speech disfluencies, which have been removed as noise in previous work, help identify important utterances, while the structural feature is less effective than it is in broadcast news."
P04-1029,Optimizing Typed Feature Structure Grammar Parsing through Non-Statistical Indexing,2004,9,3,2,1,18250,cosmin munteanu,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs). The static analysis tries to predict at compile-time which feature paths will cause unification failure during parsing at run-time. To support the static analysis, we introduce a new classification of the instances of variables used in TFSGs, based on what type of structure sharing they create. The indexing actions that can be performed during parsing are also enumerated. Non-statistical indexing has the advantage of not requiring training, and, as the evaluation using large-scale HPSGs demonstrates, the improvements are comparable with those of statistical optimizations. Such statistical optimizations rely on data collected during training, and their performance does not always compensate for the training costs."
P04-1030,Head-Driven Parsing for Word Lattices,2004,22,12,3,0,47856,christopher collins,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large-vocabulary speech recognition. The model is adapted to an online left to right chart-parser for word lattices, integrating acoustic, n-gram, and parser probabilities. The parser uses structural and lexical dependencies not considered by n-gram models, conditioning recognition on more linguistically-grounded relationships. Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding."
P04-1031,Balancing Clarity and Efficiency in Typed Feature Logic Through Delaying,2004,15,25,1,1,739,gerald penn,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"The purpose of this paper is to re-examine the balance between clarity and efficiency in HPSG design, with particular reference to the design decisions made in the English Resource Grammar (LinGO, 1999, ERG). It is argued that a simple generalization of the conventional delay statements used in logic programming is sufficient to restore much of the functionality and concomitant benefit that the ERG elected to forego, with an acceptable although still perceptible computational cost."
P03-1026,A Tabulation-Based Parsing Method that Reduces Copying,2003,9,10,1,1,739,gerald penn,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a new bottom-up chart parsing algorithm for Prolog along with a compilation procedure that reduces the amount of copying at run-time to a constant number (2) per edge. It has applications to unification-based grammars with very large partially ordered categories, in which copying is expensive, and can facilitate the use of more sophisticated indexing strategies for retrieving such categories that may otherwise be overwhelmed by the cost of such copying. It also provides a new perspective on quick-checking and related heuristics, which seems to confirm that forcing an early failure (as opposed to seeking an early guarantee of success) is in fact the best approach to use. A preliminary empirical evaluation of its performance is also provided."
J03-3007,Book Reviews: Linguistic Evolution through Language Acquisition: Formal and Computational Models edited by Ted Briscoe; Implementing Typed Feature Structure Grammars by Ann Copestake,2003,0,0,2,0,52902,michael arbib,Computational Linguistics,0,None
E03-1028,{AVM} Description Compilation using Types as Modes,2003,5,0,1,1,739,gerald penn,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper provides a method for generating compact and efficient code to implement the enforcement of a description in typed feature logic. It does so by viewing information about types through the course of code generation as modes of instantiation --- a generalization of the common practice in logic programming of the binary instantiated/variable mode declarations that advanced Prolog compilers use. Section 1 introduces the description language. Sections 2 and 3 motivate the view of mode and compilation taken here, and outline a mode declaration language for typed feature logic. Sections 4 through 7 then present the compiler. An evaluation on two grammars is presented at the end."
E03-1039,Topological Parsing,2003,7,11,1,1,739,gerald penn,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a new grammar formalism for parsing with freer word-order languages, motivated by recent linguistic research in German and the Slavic languages. Unlike CFGs, these grammars contain two primitive notions of constituency that are used to preserve the semantic or interpretational aspects of phrase structure, while at the same time providing a more efficient backbone for parsing based on word-order and contiguity constraints. A simple parsing algorithm is presented, and compilation of grammars into Constraint Handling Rules is also discussed."
W02-0103,A Web-based Instructional Platform for Contraint-Based Grammar Formalisms and Parsing,2002,4,45,2,0,3013,detmar meurers,Proceedings of the {ACL}-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics,0,"We propose the creation of a web-based training framework comprising a set of topics that revolve around the use of feature structures as the core data structure in linguistic theory, its formal foundations, and its use in syntactic processing."
P02-1009,Generalized Encoding of Description Spaces and its Application to Typed Feature Structures,2002,6,4,1,1,739,gerald penn,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a new formalization of a unification- or join-preserving encoding of partially ordered sets that more essentially captures what it means for an encoding to preserve joins, generalizing the standard definition in AI research. It then shows that every statically typable ontology in the logic of typed feature structures can be encoded in a data structure of fixed size without the need for resizing or additional union-find operations. This is important for any grammar implementation or development system based on typed feature structures, as it significantly reduces the overhead of memory management and reference-pointer-chasing during unification."
P01-1054,Tractability and Structural Closures in Attribute Logic Type Signatures,2001,11,3,1,1,739,gerald penn,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semi-latticehood, unique feature introduction, and the absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist."
J00-2010,Book Reviews: The Mathematics of Syntactic Structure: Trees and their Logics,2000,0,0,1,1,739,gerald penn,Computational Linguistics,0,None
P98-2169,Parametric Types for Typed Attribute-Value Logic,1998,8,2,1,1,739,gerald penn,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Parametric polymorphism has been combined with inclusional polymorphism to provide natural type systems for Prolog (DH88), HiLog (YFS92), and constraint resolution languages (Smo89), and, in linguistics, by HPSG-like grammars to classify lists and sets of linguistic objects (PS94), and by phonologists in representations of hierarchical structure (Kle91). This paper summarizes the incorporation of parametric types into the typed attribute-value logic of (Car92), thus providing a natural extension to the type system for ALE (CP96). Following (Car92), the concern here is not with models of feature terms themselves, but with how to compute with parametric types, and what diferent kinds of information one can represent relative to a signature with parametric types, than relative to a signature without them. This enquiry has yielded a more flexible interpretation of parametric types with several specific properties necessary to conform to their current usage by linguists and implementors who work with feature-based formalisms."
C98-2164,Parametric Types for Typed Attribute-Value Logic,1998,8,2,1,1,739,gerald penn,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Parametric polymorphism has been combined with inclusional polymorphism to provide natural type systems for Prolog (DH88), HiLog (YFS92), and constraint resolution languages (Smo89), and, in linguistics, by HPSG-like grammars to classify lists and sets of linguistic objects (PS94), and by phonologists in representations of hierarchical structure (Kle91). This paper summarizes the incorporation of parametric types into the typed attribute-value logic of (Car92), thus providing a natural extension to the type system for ALE (CP96). Following (Car92), the concern here is not with models of feature terms themselves, but with how to compute with parametric types, and what diferent kinds of information one can represent relative to a signature with parametric types, than relative to a signature without them. This enquiry has yielded a more flexible interpretation of parametric types with several specific properties necessary to conform to their current usage by linguists and implementors who work with feature-based formalisms."
W97-1509,Head-Driven Generation and Indexing in {ALE},1997,-1,-1,1,1,739,gerald penn,Computational Environments for Grammar Development and Linguistic Engineering,0,None
W94-0204,Default Finite State Machines and Finite State Phonology,1994,4,4,1,1,739,gerald penn,Computational Phonology,0,None
