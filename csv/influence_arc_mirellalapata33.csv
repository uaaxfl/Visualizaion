2020.acl-main.174,W14-0907,0,0.0188617,"2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment contains protagonist or location information); they create summaries to help readers decide whether they are interested in reading the whole story, without revealing its plot. Mihalcea and Ceylan (2007) summarize books with an unsupervised graph-based approach operating over segments (i.e., topical units)."
2020.acl-main.174,P13-1035,0,0.0570598,"Missing"
2020.acl-main.174,P14-1035,0,0.0202398,"ysis shows that key events identified in the latent space correlate with important summary content. 2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment contains protagonist or location information); they create summaries to help readers decide whether they are interested in reading the whole story, without revealing its plot. Mihalcea and Ceylan (2007) summ"
2020.acl-main.174,D18-2029,0,0.0268876,"roduced in Zheng and Lapata (2019) takes directed edges into account, capturing the intuition that the centrality of any two nodes is influenced by their relative position. Also note that the edges of preceding and following scenes are differentially weighted by λ1 and λ2 . Although earlier implementations of T EXT R ANK (Mihalcea and Tarau, 2004) compute node similarity based on symbolic representations such as tf*idf, we adopt a neural approach. Specifically, we obtain sentence representations based on a pretrained encoder. In our experiments, we rely on the Universal Sentence Encoder (USE; Cer et al. 2018), however, other embeddings are possible.1 We represent a scene by the mean of its sentence representations and measure scene similarity ei j using cosine.2 As in the original T EXT R ANK algorithm (Mihalcea and Tarau, 2004), scenes are ranked based on their centrality and the M most central ones are selected to appear in the summary. 3.2 Supervised Screenplay Summarization Most extractive models frame summarization as a classification problem. Following a recent approach (S UMMA RU NN ER; Nallapati et al. 2017), we use a neural network-based encoder to build representations for scenes and app"
2020.acl-main.174,P09-1068,0,0.0461756,"ervised summarization algorithms; (b) we provide a new layer of annotations for the CSI corpus, which can be used for research in long-form summarization; and (c) we demonstrate that narrative structure can facilitate screenplay summarization; our analysis shows that key events identified in the latent space correlate with important summary content. 2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importanc"
2020.acl-main.174,P16-1046,1,0.924798,"rtant aspects of a CSI episode and improve summarization performance over general extractive algorithms, leading to more complete and diverse summaries. 1 Setup Opportunity New Situation Change of Plans Progress Point of no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterward"
2020.acl-main.174,P10-1015,0,0.0685763,"Missing"
2020.acl-main.174,P19-1102,0,0.0428377,"nal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the structure itself can be explicitly taken i"
2020.acl-main.174,Q18-1001,1,0.942042,"our method does not involve manually annotating turning points in CSI episodes. Instead, we approximate narrative structure automatically by pretraining on the annotations of the TRIPOD dataset of Papalampidi et al. (2019) and employing a variant of their model. We find that narrative structure representations learned on their dataset (which was created for feature-length films), transfer well across cinematic genres and computational tasks. We propose a framework for end-to-end training in which narrative structure is treated as a latent variable for summarization. We extend the CSI dataset (Frermann et al., 2018) with binary labels indicating whether a scene should be included in the summary and present experiments with both supervised and unsupervised summarization models. An overview of our approach is shown in Figure 2. Our contributions can be summarized as follows: (a) we develop methods for instilling knowledge about narrative structure into generic su1921 pervised and unsupervised summarization algorithms; (b) we provide a new layer of annotations for the CSI corpus, which can be used for research in long-form summarization; and (c) we demonstrate that narrative structure can facilitate screenp"
2020.acl-main.174,N15-1113,1,0.950991,"onding summaries (e.g., by mining IMDb or Wikipedia), the size of such a corpus would be at best in the range of a few hundred examples not hundreds of thousands. Also note that genre differences might render transfer learning (Pan and Yang, 2010) difficult, e.g., a model trained on movie screenplays might not generalize to sitcoms or soap operas. Given the above challenges, we introduce a number of assumptions to make the task feasible. Firstly, our goal is to produce informative summaries, which serve as a surrogate to reading the full script or watching the entire film. Secondly, we follow Gorinski and Lapata (2015) in conceptualizing screenplay summarization as the task of identifying a sequence of informative scenes. Thirdly, we focus on summarizing television programs such as CSI: Crime Scene Investigation (FrIn this work, we adapt general-purpose extractive summarization algorithms (Nallapati et al., 2017; Zheng and Lapata, 2019) to identify informative scenes in screenplays and instill in them knowledge about narrative film structure (Hauge, 2017; Cutting, 2016; Freytag, 1896). Specifically, we adopt a scheme commonly used by screenwriters as a practical guide for producing successful screenplays. A"
2020.acl-main.174,D10-1008,0,0.0770998,"Missing"
2020.acl-main.174,N18-1065,0,0.0230746,"no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the"
2020.acl-main.174,J97-1003,0,0.466094,"subsequently project them via distant supervision onto screenplays, thereby creating silver-standard labels. We utilize this silver-standard dataset in order to pretrain a network which performs TP identification. TP Identification Network We first encode screenplay scenes via a BiLSTM equipped with an attention mechanism. We then contextualize them with respect to the whole screenplay via a second BiLSTM. Next, we compute topic-aware scene representations ti via a context interaction layer (CIL) as proposed in Papalampidi et al. (2019). CIL is inspired by traditional segmentation approaches (Hearst, 1997) and measures the semantic similarity of the current scene with a preceding and following context window in the screenplay. Hence, the topic-aware scene representations also encode the degree to which each scene acts as a topic boundary in the screenplay. In the final layer, we employ TP-specific attention mechanisms to compute the probability pi j that scene ti represents the jth TP in the screenplay. Note that we expect the TP-specific attention distributions to be sparse, as there are only a few scenes which are relevant for a TP (recall that TPs are boundary scenes between sections). To en"
2020.acl-main.174,P84-1044,0,0.285232,"Missing"
2020.acl-main.174,J10-1003,0,0.0254828,"sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment contains protagonist or location information); they create summaries to help readers decide whether they are interested in reading the whole story, without revealing its plot. Mihalcea and Ceylan (2007) summarize books with an unsupervised graph-based approach operating over segments (i.e., topical units). Their algorithm first generates a summary for each segment and then an overall summary by collecting sentences from the individual segment summaries. Focusing on screenplays, Gorinski and Lapata (2015) generate a"
2020.acl-main.174,P19-1500,1,0.847489,"math Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the structure itself can be explicitly taken into account in model de"
2020.acl-main.174,P10-1158,1,0.724258,"rovide a new layer of annotations for the CSI corpus, which can be used for research in long-form summarization; and (c) we demonstrate that narrative structure can facilitate screenplay summarization; our analysis shows that key events identified in the latent space correlate with important summary content. 2 Related Work A large body of previous work has focused on the computational analysis of narratives (Mani, 2012; Richards et al., 2009). Attempts to analyze how stories are written have been based on sequences of events (Schank and Abelson, 1975; Chambers and Jurafsky, 2009), plot units (McIntyre and Lapata, 2010; Goyal et al., 2010; Finlayson, 2012) and their structure (Lehnert, 1981; Rumelhart, 1980), as well as on characters or personas in a narrative (Black and Wilensky, 1979; Propp, 1968; Bamman et al., 2014, 2013; Valls-Vargas et al., 2014) and their relationships (Elson et al., 2010; Agarwal et al., 2014; Srivastava et al., 2016). As mentioned earlier, work on summarization of narratives has had limited appeal, possibly due to the lack of annotated data for modeling and evaluation. Kazantseva and Szpakowicz (2010) summarize short stories based on importance criteria (e.g., whether a segment con"
2020.acl-main.174,W04-3252,0,0.0576735,"quence of scenes D = {s1 , s2 , . . . , sn }. Our aim is to select a subset D 0 = {si , . . . , sk } consisting of the most informative scenes (where k < n). Note that this definition produces extractive summaries; we further assume that selected scenes are presented according to their order in the screenplay. We next discuss how summaries can be created using both unsupervised and supervised approaches, and then move on to explain how these are adapted to incorporate narrative structure. 3.1 Unsupervised Screenplay Summarization Our unsupervised model is based on an extension of T EXT R ANK (Mihalcea and Tarau, 2004; Zheng and Lapata, 2019), a well-known algorithm for extractive single-document summarization. In our setting, a screenplay is represented as a graph, in which nodes correspond to scenes and edges between scenes si and s j are weighted by their simi1922 larity ei j . A node’s centrality (importance) is measured by computing its degree: centrality(si ) = λ1 ∑ ei j + λ2 ∑ ei j j<i (1) j&gt;i where λ1 +λ2 = 1. The modification introduced in Zheng and Lapata (2019) takes directed edges into account, capturing the intuition that the centrality of any two nodes is influenced by their relative position"
2020.acl-main.174,K16-1028,0,0.0301533,"pisode and improve summarization performance over general extractive algorithms, leading to more complete and diverse summaries. 1 Setup Opportunity New Situation Change of Plans Progress Point of no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure o"
2020.acl-main.174,D18-1206,1,0.875357,"ns Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient since important passages can be identified in predictable locations (e.g., by performing a “smart selection” of sentences from the beginning of the document) and the structure itself can"
2020.acl-main.174,D19-1180,1,0.888013,"cessful screenplays. According to this scheme, wellstructured stories consist of six basic stages which are defined by five turning points (TPs), i.e., events which change the direction of the narrative, and determine the story’s progression and basic thematic units. In Figure 1, TPs are highlighted for a CSI episode. Although the link between turning points and summarization has not been previously made, earlier work has emphasized the importance of narrative structure for summarizing books (Mihalcea and Ceylan, 2007) and social media content (Kim and Monroy-Hern´andez, 2015). More recently, Papalampidi et al. (2019) have shown how to identify turning points in feature-length screenplays by projecting synopsis-level annotations. Crucially, our method does not involve manually annotating turning points in CSI episodes. Instead, we approximate narrative structure automatically by pretraining on the annotations of the TRIPOD dataset of Papalampidi et al. (2019) and employing a variant of their model. We find that narrative structure representations learned on their dataset (which was created for feature-length films), transfer well across cinematic genres and computational tasks. We propose a framework for e"
2020.acl-main.174,P19-1628,1,0.910543,"on performance over general extractive algorithms, leading to more complete and diverse summaries. 1 Setup Opportunity New Situation Change of Plans Progress Point of no Return Complications Major Setback The ﬁnal push Climax Aftermath Figure 1: Example of narrative structure for episode “Burden of Proof” from TV series Crime Scene Investigation (CSI); turning points are highlighted in color. Introduction Automatic summarization has enjoyed renewed interest in recent years thanks to the popularity of modern neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; Zheng and Lapata, 2019) and the availability of large-scale datasets containing hundreds of thousands of document–summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019). Most efforts to date have concentrated on the summarization of news articles which tend to be relatively short and formulaic following an “inverted pyramid” structure which places the most essential, novel and interesting elements of a story in the beginning and supporting material and secondary details afterwards. The rigid structure of news articles is expedient si"
2020.acl-main.174,P18-1061,0,0.0155492,"ataset, which also uses character-centered graphs to describe the content of movie video clips. Our work synthesizes various strands of research on narrative structure analysis (Cutting, 2016; Hauge, 2017), screenplay summarization (Gorinski and Lapata, 2015), and neural network modeling (Dong, 2018). We focus on extractive summarization and our goal is to identify an optimal sequence of key events in a narrative. We aim to create summaries which re-tell the plot of a story in a concise manner. Inspired by recent neural network-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018; Zheng and Lapata, 2019), we develop supervised and unsupervised models for our summarization task based on neural representations of scenes and how these relate to the screenplay’s narrative structure. Contrary to most previous work which has focused on characters, we select summary scenes based on events and their importance in the story. Our definition of narrative structure closely follows Papalampidi et al. (2019). However, the model architectures we propose are general and could be adapted to different plot analysis schemes (Field, 2005; Vogler, 2007). To overcome the difficulties in ev"
2020.acl-main.175,N19-4010,0,0.0480271,"Missing"
2020.acl-main.175,D18-1403,1,0.644549,"ins (movies vs businesses) and summarization requirements (short vs longer summaries). Results based on automatic and human evaluation show that our method outperforms previous unsupervised summarization models, including the state-of-the-art abstractive system of Chu and Liu (2019) and is on the same par with a state-of-the-art supervised model (Wang and Ling, 2016) trained on a small sample of (genuine) review-summary pairs. 2 Related Work Most previous work on unsupervised opinion summarization has focused on extractive approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) where a clustering model groups opinions of the same aspect, and a sentence extraction model identifies text representative of each cluster. Ganesan et al. (2010) propose a graph-based abstractive framework for generating concise opinion summaries, while Di Fabbrizio et al. (2014) use an extractive system to first select salient sentences and then generate an abstractive summary based on hand-written templates (Carenini and Moore, 2006). As mentioned earlier, we follow the setting of Chu and Liu (2019) in assuming that we have access to reviews but no gold-standard summaries. Their model lear"
2020.acl-main.175,K16-1002,0,0.0525868,"individual reviews. As a result, it may not be able to generate meaningful text when the number of reviews is large. Furthermore, autoencoders are constrained to use simple decoders lacking attention (Bahdanau et al., 2014) and copy (Vinyals et al., 2015) mechanisms which have proven useful in the supervised setting leading to the generation of informative and detailed summaries. Problematically, a powerful decoder might be detrimental to the reconstruction objective, learning to express arbitrary distributions of the output sequence while ignoring the encoded input (Kingma and Welling, 2014; Bowman et al., 2016). In this paper, we enable the use of supervised techniques for unsupervised summarization. Specifically, we automatically generate a synthetic training dataset from a corpus of product reviews, and use this dataset to train a more powerful neural model with supervised learning. The synthetic data is created by selecting a review from the corpus, pretending it is a summary, generating multiple noisy versions thereof and treating these as pseudoreviews. The latter are obtained with two noise generation functions targeting textual units of different granularity: segment noising introduces noise"
2020.acl-main.175,E06-1039,0,0.435974,"eriments on two review datasets representing different domains (movies vs businesses) and summarization requirements (short vs longer summaries). Results based on automatic and human evaluation show that our method outperforms previous unsupervised summarization models, including the state-of-the-art abstractive system of Chu and Liu (2019) and is on the same par with a state-of-the-art supervised model (Wang and Ling, 2016) trained on a small sample of (genuine) review-summary pairs. 2 Related Work Most previous work on unsupervised opinion summarization has focused on extractive approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) where a clustering model groups opinions of the same aspect, and a sentence extraction model identifies text representative of each cluster. Ganesan et al. (2010) propose a graph-based abstractive framework for generating concise opinion summaries, while Di Fabbrizio et al. (2014) use an extractive system to first select salient sentences and then generate an abstractive summary based on hand-written templates (Carenini and Moore, 2006). As mentioned earlier, we follow the setting of Chu and Liu (2019) in assuming that we have a"
2020.acl-main.175,N18-1150,0,0.0332334,"a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the pr"
2020.acl-main.175,W14-3348,0,0.0423026,"Missing"
2020.acl-main.175,W14-4408,0,0.233403,"Missing"
2020.acl-main.175,N18-1065,0,0.0395419,"Missing"
2020.acl-main.175,P19-1102,0,0.0204957,"nctions and a summarization model which learns to denoise the input and generate the original review. At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided st"
2020.acl-main.175,K18-1040,0,0.114069,"pled making use of linguistically motivated noise functions. Our work relates to denoising autoencoders (DAEs; Vincent et al., 2008), which have been effectively used as unsupervised methods for various NLP tasks. Earlier approaches have shown that DAEs can be used to learn high-level text representations for domain adaptation (Glorot et al., 2011) and multimodal representations of textual and visual input (Silberer and Lapata, 2014). Recent 1935 work has applied DAEs to text generation tasks, specifically to data-to-text generation (Freitag and Roy, 2018) and extractive sentence compression (Fevry and Phang, 2018). Our model differs from these approaches in two respects. Firstly, while previous work has adopted trivial noising methods such as randomly adding or removing words (Fevry and Phang, 2018) and randomly corrupting encodings (Silberer and Lapata, 2014), our noise generators are more linguistically informed and suitable for the opinion summarization task. Secondly, while in Freitag and Roy (2018) the decoder is limited to vanilla RNNs, our noising method enables the use of more complex architectures, enhanced with attention and copy mechanisms, which are known to improve the performance of summa"
2020.acl-main.175,D18-1426,0,0.142004,"sed training, our dataset construction method is more principled making use of linguistically motivated noise functions. Our work relates to denoising autoencoders (DAEs; Vincent et al., 2008), which have been effectively used as unsupervised methods for various NLP tasks. Earlier approaches have shown that DAEs can be used to learn high-level text representations for domain adaptation (Glorot et al., 2011) and multimodal representations of textual and visual input (Silberer and Lapata, 2014). Recent 1935 work has applied DAEs to text generation tasks, specifically to data-to-text generation (Freitag and Roy, 2018) and extractive sentence compression (Fevry and Phang, 2018). Our model differs from these approaches in two respects. Firstly, while previous work has adopted trivial noising methods such as randomly adding or removing words (Fevry and Phang, 2018) and randomly corrupting encodings (Silberer and Lapata, 2014), our noise generators are more linguistically informed and suitable for the opinion summarization task. Secondly, while in Freitag and Roy (2018) the decoder is limited to vanilla RNNs, our noising method enables the use of more complex architectures, enhanced with attention and copy mec"
2020.acl-main.175,C10-1039,0,0.86213,"ractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentiment identification (Pang et al., 2002; Pang and Lee, 2004), and summary creation based on extractive (Radev et al., 2000; Lu et al., 2009) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014; Di Fabbrizio et al., 2014). Although poThe supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. For instance, manually writing opinion summaries is practically impossible since an annotator must read all available reviews for a given product or service which can be prohibitivel"
2020.acl-main.175,D18-1443,0,0.021528,"those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarizati"
2020.acl-main.175,D14-1168,0,0.0629742,"on of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentiment identification (Pang et al., 2002; Pang and Lee, 2004), and summary creation based on extractive (Radev et al., 2000; Lu et al., 2009) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014; Di Fabbrizio et al., 2014). Although poThe supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. For instance, manually writing opinion summaries is practically impossible since an annotator must read all available reviews for a given product or service which can be prohibitively many. Moreover, different types of product"
2020.acl-main.175,P17-2074,0,0.0818293,"Missing"
2020.acl-main.175,W04-1013,0,0.102335,"ersion x(c) (see Figure 1 for an example). Repeating the process N times produces the noisy set X(c) . We describe this process step-by-step in the Appendix. Document Noising Given candidate summary y = {w1 , ..., wL }, we also create another set of document-level noisy versions X(d) = (d) (d) {x1 , ..., xN }. Instead of manipulating parts of the summary, we altogether replace it with a similar review from the corpus and treat it as a noisy version. Specifically, we select N reviews that are most similar to y and discuss the same product. To measure similarity, we use IDF-weighted ROUGE-1 F1 (Lin, 2004), where we calculate the lexical overlap between the review and the candidate summary, weighted by token importance: X  overlap = IDF(wj ) ∗ 1(wj ∈ y) wj ∈x P = overlap/|x| R = overlap/|y| F1 = (2 ∗ P ∗ R)/(P + R) where x is a review in the corpus, 1(·) is an indicator function, and P, R, and F1 are the ROUGE-1 precision, recall, and F1 , respectively. The reviews with the highest F1 are selected as noisy versions of y, resulting in the noisy set X(d) (see Figure 1). We create a total of 2 ∗ N noisy versions of y, i.e., X = X(c) ∪X(d) and obtain our synthetic training data D = {(X, y)} by gen"
2020.acl-main.175,P19-1500,1,0.871382,"n evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee"
2020.acl-main.175,P12-1036,0,0.0836517,"pata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentiment identification (Pang et al., 2002; Pang and Lee, 2004), and summary creation based on extractive (Radev et al., 2000; Lu et al., 2009) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014; Di Fabbrizio et al., 2014). Although poThe supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily"
2020.acl-main.175,P04-1035,0,0.182217,"ence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentiment identification (Pang et al., 2002; Pang and Lee, 2004), and summary creation based on extractive (Radev et al., 2000; Lu et al., 2009) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014; Di Fabbrizio et al., 2014). Although poThe supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. For instance, manually writing opinion summaries is pract"
2020.acl-main.175,W02-1011,0,0.0467991,"g results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentiment identification (Pang et al., 2002; Pang and Lee, 2004), and summary creation based on extractive (Radev et al., 2000; Lu et al., 2009) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014; Di Fabbrizio et al., 2014). Although poThe supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. For instance, manually writing opini"
2020.acl-main.175,D10-1007,0,0.120679,"ting different domains (movies vs businesses) and summarization requirements (short vs longer summaries). Results based on automatic and human evaluation show that our method outperforms previous unsupervised summarization models, including the state-of-the-art abstractive system of Chu and Liu (2019) and is on the same par with a state-of-the-art supervised model (Wang and Ling, 2016) trained on a small sample of (genuine) review-summary pairs. 2 Related Work Most previous work on unsupervised opinion summarization has focused on extractive approaches (Carenini et al., 2006; Ku et al., 2006; Paul et al., 2010; Angelidis and Lapata, 2018) where a clustering model groups opinions of the same aspect, and a sentence extraction model identifies text representative of each cluster. Ganesan et al. (2010) propose a graph-based abstractive framework for generating concise opinion summaries, while Di Fabbrizio et al. (2014) use an extractive system to first select salient sentences and then generate an abstractive summary based on hand-written templates (Carenini and Moore, 2006). As mentioned earlier, we follow the setting of Chu and Liu (2019) in assuming that we have access to reviews but no gold-standar"
2020.acl-main.175,P19-1504,1,0.850018,"se. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect"
2020.acl-main.175,N18-1202,0,0.0149527,"corpus C which display an excess of these characteristics based on a list of domain-specific constraints (detailed in Section 4). We sample a review y from the filtered corpus, which we use as the candidate summary. Segment Noising Given candidate summary y = {w1 , ..., wL }, we create a set of segment-level (c) (c) noisy versions X(c) = {x1 , ..., xN }. Previous work has adopted noising techniques based on random n-gram alterations (Fevry and Phang, 2018), however, we instead rely on two simple, linguistically informed noise functions. Firstly, we train a bidirectional language model (BiLM; Peters et al., 2018) on the review corpus C. For each word in y, the BiLM predicts a softmax word distribution which can be used to replace words. Secondly, we utilize FLAIR1 (Akbik et al., 2019), an off-theshelf state-of-the-art syntactic chunker that leverages contextual embeddings, to shallow parse each review r in corpus C. This results in a list of chunks Cr = {c1 , ..., cK } with corresponding syntactic labels Gr = {g1 , ..., gK } for each review r, which we use for replacing and rearranging chunks. Segment-level noise involves token- and chunk1 https://github.com/zalandoresearch/ flair 1936 level alteratio"
2020.acl-main.175,W00-0403,0,0.4594,"ents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentiment identification (Pang et al., 2002; Pang and Lee, 2004), and summary creation based on extractive (Radev et al., 2000; Lu et al., 2009) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014; Di Fabbrizio et al., 2014). Although poThe supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization. Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced. For instance, manually writing opinion summaries is practically impossible since an annotator must read all available r"
2020.acl-main.175,W17-1003,0,0.129597,"Missing"
2020.acl-main.175,D15-1044,0,0.0704362,"s from these approaches in two respects. Firstly, while previous work has adopted trivial noising methods such as randomly adding or removing words (Fevry and Phang, 2018) and randomly corrupting encodings (Silberer and Lapata, 2014), our noise generators are more linguistically informed and suitable for the opinion summarization task. Secondly, while in Freitag and Roy (2018) the decoder is limited to vanilla RNNs, our noising method enables the use of more complex architectures, enhanced with attention and copy mechanisms, which are known to improve the performance of summarization systems (Rush et al., 2015; See et al., 2017). 3 Modeling Approach Let X = {x1 , ..., xN } denote a set of reviews about a product (e.g., a movie or business). Our aim is to generate a summary y of the opinions expressed in X. We further assume access to a corpus C = {X1 , ..., XM } containing multiple reviews about M products without corresponding opinion summaries. Our method consists of two parts. We first create a synthetic dataset D = {(X, y)} consisting of summary-review pairs. Specifically, we sample review xi from C, pretend it is a summary, and generate multiple noisy versions thereof (i.e., pseudoreviews). At"
2020.acl-main.175,P17-1099,0,0.664929,"ews and generates a summary containing salient opinions, treating those that do not reach consensus as noise. Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu"
2020.acl-main.175,P14-1068,1,0.814267,"rarchical variational autoencoder to learn a latent code of the summary. While they also use randomly sampled reviews for supervised training, our dataset construction method is more principled making use of linguistically motivated noise functions. Our work relates to denoising autoencoders (DAEs; Vincent et al., 2008), which have been effectively used as unsupervised methods for various NLP tasks. Earlier approaches have shown that DAEs can be used to learn high-level text representations for domain adaptation (Glorot et al., 2011) and multimodal representations of textual and visual input (Silberer and Lapata, 2014). Recent 1935 work has applied DAEs to text generation tasks, specifically to data-to-text generation (Freitag and Roy, 2018) and extractive sentence compression (Fevry and Phang, 2018). Our model differs from these approaches in two respects. Firstly, while previous work has adopted trivial noising methods such as randomly adding or removing words (Fevry and Phang, 2018) and randomly corrupting encodings (Silberer and Lapata, 2014), our noise generators are more linguistically informed and suitable for the opinion summarization task. Secondly, while in Freitag and Roy (2018) the decoder is li"
2020.acl-main.175,N16-1007,0,0.607789,"t our model brings substantial improvements over both abstractive and extractive baselines. 1 Abstractive summarization has enjoyed renewed interest in recent years thanks to the availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Liu et al., 2018; Fabbri et al., 2019) which have driven the development of neural architectures for summarizing single and multiple documents. Several approaches (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018; Liu et al., 2018; Perez-Beltrachini et al., 2019; Liu and Lapata, 2019; Wang and Ling, 2016) have shown promising results with sequence-to-sequence models that encode one or several source documents and then decode the learned representations into an abstractive summary. Introduction The proliferation of massive numbers of online product, service, and merchant reviews has provided strong impetus to develop systems that perform opinion mining automatically (Pang and Lee, 2008). The vast majority of previous work (Hu and Liu, 2006) breaks down the problem of opinion aggregation and summarization into three interrelated tasks involving aspect extraction (Mukherjee and Liu, 2012), sentim"
2020.acl-main.416,E17-2039,0,0.0421148,"Missing"
2020.acl-main.416,W13-2322,0,0.0171379,"cross sentences. The basic meaning-carrying units in DRT are Discourse Representation Structures (DRSs). They consist of discourse referents (e.g., x1 , x2 ) representing entities in the discourse and conditions (e.g., male.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b"
2020.acl-main.416,W13-2101,0,0.0301587,"more consistent with human evaluation than S MATCH (Cai and Knight, 2013b), an AMR metric which is the basis of C OUNTER. S EM B LEU cannot be directly used on DRS graphs due to the large amount of indexed variables and the fact that the graphs are not explicitly given; moreover, our metric outputs F1 scores instead of precision only. Opitz et al. (2020) propose a set of principles for AMR-related metrics, showing the advantages and drawbacks of alignment- and BLEU-based AMR metrics. However, efficiency of the metric is crucial for the development of document-level models of semantic parsing. Basile and Bos (2013) propose to represent DRSs via Discourse Representation Graphs (DRGs) which are acyclic and directed. However, DRGs are similar to flattened trees, and not able to capture clause-level information (e.g., b1 Agent e1 x1 ) required for evaluation (van Noord et al., 2018a). 5 Conclusions In this work we proposed D SCORER, as a DRS evaluation metric alternative to C OUNTER. Our metric is significantly more efficient than C OUNTER and considers high-order DRSs. D SCORER allows to speed up model selection and development removing the bottleneck of evaluation time. Acknowledgments We thank the anonym"
2020.acl-main.416,W15-1841,0,0.0243534,"hboring nodes and connecting edges. For example, the two E nodes are differExperiments In our experiments, we investigate the correlation between D SCORER and C OUNTER, and the efficiency of the two metrics. We present results on two datasets, namely the Groningen Meaning Bank (GMB; Bos et al. 2017) and the Parallel Meaning Bank (PMB; Abzianidze et al. 2017). We compare two published systems on the GMB: DRTS-sent which is a sentence-level parser (Liu et al., 2018) and DRTS-doc which is a documentlevel parser (Liu et al., 2019a). On the PMB, we compare seven systems: Boxer, a CCG-based parser (Bos, 2015), AMR2DRS, a rule-based parser that converts AMRs to DRSs, SIM-SPAR giving the DRS in the training set most similar to the current DRS, SPAR giving a fixed DRS for each sentence, seq2seq-char, a character-based sequence-tosequence clause parser (van Noord et al., 2018b), seq2seq-word, a word-based sequence-to-sequence clause parser, and a transformer-based clause parser (Liu et al., 2019b). 3.1 Metric Settings C OUNTER takes 100 hill-climbing restarts to search for the best variable mappings on PMB and 10 restarts on GMB. Both D SCORER and C OUNTER are computed on one CPU (2.10GHz). The weight"
2020.acl-main.416,P13-2131,0,0.317625,"automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-complete, similar to other metrics such as S MATCH (Cai and Knight, 2013a) for Abstract Meaning Representation. C OUNTER uses a greedy hill-climbing algorithm to obtain one-to-one variable mappings, and then computes precision, recall, and F1 scores according to the overlap of clauses between two DRSs. To get around the problem of search errors, the hill-climbing search implementation applies several random restarts. This incurs unacceptable runtime, especially when evaluating document-level DRSs with a large number of variables. Another problem with the current evaluation is that C OUNTER only considers local clauses without taking larger window sizes into accoun"
2020.acl-main.416,W19-1202,0,0.0120492,"ions (e.g., male.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this ali"
2020.acl-main.416,P18-1040,1,0.931081,"ucted recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-complete, similar to other metrics such as S MATCH (Cai and Knight, 2013a) for Abstract Meaning Representation. C OUNTER uses a greedy hill-climbing algorithm to obtain one-to-one variable mappings, and then computes precision, recall, and F1 sc"
2020.acl-main.416,P19-1629,1,0.800877,"ale.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-compl"
2020.acl-main.416,W19-1203,1,0.905421,"ale.n.02(x1 ), Agent(e1 , x1 )) representing information about discourse referents. Every variable and condition are bounded by a box label (e.g., b1 ) which implies that the variable or condition are interpreted in that box. DRSs are constructed recursively. An example of a DRS in boxstyle notation is shown in Figure 1(a). DRS parsing differs from related parsing tasks (e.g., Banarescu et al. 2013) in that it can create representations that go beyond individual sentences. Despite the large amount of recently developed DRS parsing models (van Noord et al., 2018b; van Noord, 2019; Evang, 2019; Liu et al., 2019b; Fancellu et al., 2019; Le et al., 2019), the automatic evaluation of DRSs is not straightforward due to the non-standard DRS format shown in Figure 1(a). It is neither a tree (although a DRS-to-tree conversion exists; see Liu et al. 2018, 2019a for details) nor a graph. Evaluation so far relied on COUNTER (van Noord et al., 2018a) which converts DRSs to clauses shown in Figure 1(b). Given two DRSs with n and m (n ≥ m) varin! ables each, C OUNTER has to consider (n−m)! possible variable mappings in order to find an optimal one for evaluation. The problem of finding this alignment is NP-compl"
2020.acl-main.416,W19-1204,0,0.0208265,"Missing"
2020.acl-main.416,L18-1267,0,0.0327133,"Missing"
2020.acl-main.416,Q18-1043,0,0.32164,"Missing"
2020.acl-main.416,2020.tacl-1.34,0,0.0135934,"as well. And the mismatch of the second path reduces the final score. 4 Related Work The metric S EM B LEU (Song and Gildea, 2019) is most closely related to ours. It evaluates AMR graphs by calculating precision based on n-gram overlap. S EM B LEU yields scores more consistent with human evaluation than S MATCH (Cai and Knight, 2013b), an AMR metric which is the basis of C OUNTER. S EM B LEU cannot be directly used on DRS graphs due to the large amount of indexed variables and the fact that the graphs are not explicitly given; moreover, our metric outputs F1 scores instead of precision only. Opitz et al. (2020) propose a set of principles for AMR-related metrics, showing the advantages and drawbacks of alignment- and BLEU-based AMR metrics. However, efficiency of the metric is crucial for the development of document-level models of semantic parsing. Basile and Bos (2013) propose to represent DRSs via Discourse Representation Graphs (DRGs) which are acyclic and directed. However, DRGs are similar to flattened trees, and not able to capture clause-level information (e.g., b1 Agent e1 x1 ) required for evaluation (van Noord et al., 2018a). 5 Conclusions In this work we proposed D SCORER, as a DRS evalu"
2020.acl-main.416,P02-1040,0,0.107989,"ntire documents. In order to address the above issues, we propose D SCORER, a highly efficient metric for the evalu4547 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4547–4554 c July 5 - 10, 2020. 2020 Association for Computational Linguistics He didn’t play the piano. But she sang. ation of DRS parsing on texts of arbitrary length. D SCORER converts DRSs (predicted and gold) to graphs from which it extracts n-grams, and then computes precision, recall and F1 scores between them. The algorithm operates over n-grams in a fashion similar to BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), which are metrics widely used for evaluating the output of machine translation and summarization systems. While BLEU only calculates precision with a brevity penalty (it is not straightforward to define recall given the wide range of possible translations for a given input), ROUGE is a recall-oriented metric since the summary length is typically constrained by a prespecified budget.1 However, in DRS parsing, there is a single correct semantic representation (goldstandard reference) and no limit on the maximum size of DRSs. Our proposed metric, D SCORER, converts box-sty"
2020.acl-main.416,P19-1446,0,0.0367455,"Missing"
2020.acl-main.461,P99-1071,0,0.258324,"oceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169 c July 5 - 10, 2020. 2020 Association for Computational Linguistics approaches have primarily focused on extractive summarization, i.e., producing summaries by copying parts of the input reviews. In this work, we instead consider abstractive summarization which involves generating new phrases, possibly rephrasing or using words that were not in the original text. Abstractive summaries are often preferable to extractive ones as they can synthesize content across documents avoiding redundancy (Barzilay et al., 1999; Carenini and Cheung, 2008; Di Fabbrizio et al., 2014). In addition, we focus on the unsupervised setting and do not use any summaries for training. Unlike aspect-based summarization (Liu, 2012), which rewards the diversity of opinions, we aim to generate summaries that represent consensus (i.e., dominant opinons in reviews). We argue that such summaries can be useful for quick decision making, and to get an overall feel for a product or business (see the example in Table 1). More specifically, we assume we are provided with a large collection of reviews for various products and businesses an"
2020.acl-main.461,D17-1223,0,0.0313091,"lidis and Lapata, 2018; Medhat et al., 2014). Although there has been significant progress recently in summarizing non-subjective context (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018), modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. Moreover, annotation efforts would have to be undertaken for multiple domains as online reviews are inherently multi-domain (Blitzer et al., 2007) and summarization systems highly domain-sensitive (Isonuma et al., 2017). Thus, perhaps unsurprisingly, there is a long history of applying unsupervised and weakly-supervised methods to opinion summarization (e.g., Mei et al. 2007; Titov and McDonald 2008; Angelidis and Lapata 2018), however, these 5151 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5151–5169 c July 5 - 10, 2020. 2020 Association for Computational Linguistics approaches have primarily focused on extractive summarization, i.e., producing summaries by copying parts of the input reviews. In this work, we instead consider abstractive summarization which"
2020.acl-main.461,K16-1028,0,0.0981339,"Missing"
2020.acl-main.461,E17-2025,0,0.017462,"chanical Turk (AMT) workers, who summarized 8 input reviews. We created a new test for Amazon reviews following a similar procedure (see Appendix A.6 for details). We sampled 60 products and 8 reviews for each product, and they were shown to AMT workers who were asked to write a summary. We collected three summaries per product, 28 products were used for development and 32 for testing. 4.2 Experimental Details We used GRUs (Cho et al., 2014) for sequential encoding and decoding we used GRUs. We randomly initialized word embeddings that were shared across the model as a form of regularization (Press and Wolf, 2017). Further, optimization was performed using Adam (Kingma and Ba, 2014). In order to overcome the “posterior collapse” (Bowman et al., 2016), both for our model and the vanilla VAE baseline, we applied cyclical annealing (Fu et al., 2019). The reported ROUGE scores are based on F1 (see Appendix A.3 for details on hyperparameters). 5155 Copycat MeanSum LexRank Opinosis VAE Clustroid Lead Random Oracle R1 0.2947 0.2846 0.2501 0.2488 0.2542 0.2628 0.2634 0.2304 0.2907 R2 0.0526 0.0366 0.0362 0.0278 0.0311 0.0348 0.0372 0.0244 0.0527 RL 0.1809 0.1557 0.1467 0.1409 0.1504 0.1536 0.1386 0.1344 0.1863"
2020.emnlp-main.245,P19-1285,0,0.0491807,"Missing"
2020.emnlp-main.245,N19-1423,0,0.0346298,"bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage (background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension model that performs chai"
2020.emnlp-main.245,N19-1246,1,0.875678,"Missing"
2020.emnlp-main.245,P19-1222,0,0.0170661,"omprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differ"
2020.emnlp-main.245,N16-1181,0,0.225016,"he variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to understand and chain together free-form predicates and logical connectives. The proposed model is inspired by neural module networks (NMNs), which were proposed for visual question answering (Andreas et al., 2016b,a). NMNs assemble a network from a collection of specialized modules where each module performs some learnable function, such as locating a question word in an image, or recognizing relationships between objects in the image. The modules are composed together specific to what is asked in the question, then executed to obtain an answer. We design general modules that are targeted at the reasoning necessary for ROPES and compose them together to answer questions. We design three kinds of basic modules to learn the neuro-symbolic multi-step inference over questions, situations, and background p"
2020.emnlp-main.245,D19-5817,1,0.83692,"19; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to d"
2020.emnlp-main.245,D19-1455,0,0.0745637,"tion, then successively chains them, making a prediction after including the situation in the chaining. ∗ Work done during an internship at Allen Institute for Artificial Intelligence. Prior work addressing this problem has largely 3040 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3040–3050, c November 16–20, 2020. 2020 Association for Computational Linguistics either used symbolic reasoning, such as markov logic networks (Khot et al., 2015) and integer linear programming (Khashabi et al., 2016), or blackbox neural networks (Jiang et al., 2019; Jiang and Bansal, 2019). Symbolic methods give some measure of interpretability and the ability to handle logical operators to track polarity, but they are brittle, unable to handle the variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to understand and chain together fr"
2020.emnlp-main.245,P19-1261,0,0.0780502,"background and question, then successively chains them, making a prediction after including the situation in the chaining. ∗ Work done during an internship at Allen Institute for Artificial Intelligence. Prior work addressing this problem has largely 3040 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3040–3050, c November 16–20, 2020. 2020 Association for Computational Linguistics either used symbolic reasoning, such as markov logic networks (Khot et al., 2015) and integer linear programming (Khashabi et al., 2016), or blackbox neural networks (Jiang et al., 2019; Jiang and Bansal, 2019). Symbolic methods give some measure of interpretability and the ability to handle logical operators to track polarity, but they are brittle, unable to handle the variability of language. Neural networks often perform better on practical datasets, as they are more robust to paraphrase, but they lack any explicit notion of reasoning and are hard to interpret. We present a model that is a middle ground between these two approaches: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. The proposed model is able to underst"
2020.emnlp-main.245,D15-1080,0,0.0761646,"Missing"
2020.emnlp-main.245,P18-1249,0,0.0998005,"tial experiments, and we performed an analysis of the data to figure out the cause. ROPES used an annotator split to separate the train, dev, and test sets in order to avoid annotator bias (Geva et al., 2019), but we discovered that this led to a large distributional shift between train/dev and test, which we explore in this section. In light of this analysis, we recommend treating the dev set as an in-domain test set, and the original test set as an out-of-domain test. Answer types Our analysis is based on looking at the syntactic category of the answer phrase. We use the syntactic parser of Kitaev and Klein (2018) to obtain constituent trees for the passages in ROPES. We take the constituent label of the lowest subtree that covers the answer span3 as the answer type. The four most frequent answer types in ROPES are noun phrase (NP), verb phrase (VP), adjective phrase (ADJP) and adverb phrase (ADVP). Table 1 shows examples for each type. Most NP answers 3 The passages could have more than one span that matches the answer; we use the last occurrence of the answer span for our analysis. 3043 Type NP VP ADJP ADVP Others Passage ...The child poured two spoonfuls of sugar into cup A and three spoonfuls of su"
2020.emnlp-main.245,D19-5808,1,0.903659,"ion: Which category of flowers would be more likely to have brightly colored petals? Introduction Answer: category B Performing chained inference over natural language text is a long-standing goal in artificial intelligence (Grosz et al., 1986; Reddy, 2003). This kind of inference requires understanding how natural language statements fit together in a way that permits drawing conclusions. This is very challenging without a formal model of the semantics underlying the text, and when polarity needs to be tracked across many statements. For instance, consider the example in Figure 1 from ROPES (Lin et al., 2019), a recently released reading comprehension dataset that requires applying information contained in a background paragraph to a new situation. To answer the question, one must associate each category of flowers with a polarity for having brightly colored petals, which must be done by going through the information about pollinators given in the situation and linking it to what was said about pollinators and brightly colored petals in the background paragraph, along with tracking the polarity of those statements. (a) background question situation S ELECT C HAIN C HAIN P REDICT category B (b) Fig"
2020.emnlp-main.245,2021.ccl-1.108,0,0.180833,"Missing"
2020.emnlp-main.245,P19-1613,0,0.0172757,"several reading comprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker"
2020.emnlp-main.245,N18-1202,1,0.510184,", where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage (background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension mod"
2020.emnlp-main.245,D16-1264,0,0.121792,"Missing"
2020.emnlp-main.245,D19-1374,0,0.01434,"background and situation passage) and then give the answer to the question in the specific situation, without retrieval on the background passage. Models beyond Pre-trained Transformer As the emergence of fully pre-trained transformer (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Radford et al.; Dai et al., 2019; Yang et al., 2019), most of NLP benchmarks got new state-of-the-art results by the models built beyond the pre-trained transformer on specific tasks (e.g. syntactic parsing, semantic parsing and GLUE) (Wang et al., 3047 2018; Kitaev and Klein, 2018; Zhang et al., 2019; Tsai et al., 2019). Our work is in the same line to adopt the advantages of pre-trained transformer, which has already collected contextualized word representation from a large amount of data. 6 Conclusion We propose a multi-step reading comprehension model that performs chained inference over natural language text. We have demonstrated that our model substantially outperforms prior work on ROPES, a challenging new reading comprehension dataset. We have additionally presented some analysis of ROPES that should inform future work on this dataset. While our model is not a neural module network, as our model uses"
2020.emnlp-main.245,P19-1260,0,0.0183596,"ize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the effects of a passage"
2020.emnlp-main.245,W18-5446,0,0.0697258,"Missing"
2020.emnlp-main.245,Q18-1021,0,0.0301516,"Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning"
2020.emnlp-main.245,D18-1259,0,0.0516665,"to say, the crash rate per cyclist goes down as the cycle volume increases... Situation: ...Day 1 had 500 cyclists left. Day 2 had Related Work 400 cyclists left. Day 3 had 300 cyclists left. Day 4 had Neural Module Networks were originally proposed for visual question answering tasks (Andreas et al., 2016b,a), and recently have been used on several reading comprehension tasks (Jiang et al., 2019; Jiang and Bansal, 2019; Gupta et al., 2020), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the docu"
2020.emnlp-main.245,P19-1009,0,0.0506588,"Missing"
2020.emnlp-main.245,P19-1218,0,0.021383,"20), where they specialize the module functions such as F IND and C OMPARE to retrieve the relevant entities with or without supervised signals for HotpotQA (Yang et al., 2018) or DROP (Dua et al., 2019). As ROPES is quite different from these datasets, the modules that we choose to use are also different, focusing on chained inference. Multi-Hop Reasoning There are several datasets constructed for multi-hop reasoning e.g. H OTPOT QA (Yang et al., 2018; Jiang et al., 2019; Jiang and Bansal, 2019; Min et al., 2019; Feldman and El-Yaniv, 2019), QA NGAROO (Welbl et al., 2018; Chen et al., 2019b; Zhuang and Wang, 2019; Tu et al., 2019) and W IKI H OP (Welbl et al., 2018; Song et al., 2018; Das et al., 2019; Asai et al., 2019) which aims to get the answer across the documents. The term “multi-hop” reasoning on these datasets is similar to relative information retrieval, where one entity is bridged to another 200 cyclists left.... Question: What day had a lower crash rate per cyclist: Day 1 or Day 2 ? Answer: Day 1 Ours: Day 2 Table 10: The examples of the answers to the questions by the multi-step reranker. entity with one hop. Differently, the multi-step reasoning on ROPES aims to do reasoning over the eff"
2020.emnlp-main.296,D19-1352,0,0.0226774,"ery. Specifically, for the ith query-cluster pair, we first rank all segments in the cluster based on term frequency with respect to the query, and determine kiIR such that it reaches a fixed threshold θ ∈ [0, 1]. Formally, kiIR , the number of retrieved segments, is given by: kiIR = max k k X ri,j &lt; θ (1) j=1 where ri,j is the relevance score for segment j (normalized over segments in the ith cluster). Although we adopt term frequency as our relevance estimator, there is nothing in our framework which precludes the use of more sophisticated retrieval methods (Dai and Callan, 2019; Akkalyoncu Yilmaz et al., 2019). We investigated approaches based on term frequency-inverse sentence frequency (Allan et al., 2003) and BM25 (Robertson et al., 2009), however, we empirically found that they are inferior, having a bias towards shorter segments which are potentially less informative for summarization. 3634 3.2 Evidence Estimator We argue that relevance matching is not sufficient to capture the semantics expressed in the query narrative and its relationship to the documents in the cluster. We therefore leverage distant supervision signals from existing QA datasets to train our evidence estimator and use the tr"
2020.emnlp-main.296,N19-1071,0,0.0211258,"tasets show that the proposed model yields results superior to competitive baselines contributing to summaries which are more Figure 3: Performance (ROUGE-2 Recall) over k QA best evidence sentences selected by estimators trained on sentences and passages (development set). relevant and less redundant. We have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query. In the future, we would like to generate abstractive summaries following an unsupervised approach (Baziotis et al., 2019; Chu and Liu, 2019) and investigate how recent advances in open domain QA (Wang et al., 2019; Qi et al., 2019) can be adapted for query focused summarization. Acknowledgments The authors would like to thank the anonymous reviewers for their valuable feedback. We acknowledge the financial support of the European Research Council (Lapata; award number 681760). This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract FA865017-C-9118. The views and conclusions contained"
2020.emnlp-main.296,W06-0707,0,0.0818068,"kely to contain an answer, and central. The modules can be independently developed and leverage training data if available. We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary. Our framework1 is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets. 1 Introduction Query Focused Multi-Document Summarization (QFS; Dang 2006) aims to create a short summary from a set of documents that answers a specific query. It has various applications in personalized information retrieval and recommendation engines where search results can be tailored to an information need (e.g., a user might be looking for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to"
2020.emnlp-main.296,N19-1423,0,0.114563,"queries, but are less appropriate in QFS settings where query narratives can be long and complex. We argue that a trained evidence estimator might be better at performing semantic matching (Guo et al., 2016) between queries and document segments. To this effect, we experiment with two popular QA settings, namely answer sentence selection (Heilman and Smith, 2010; Yang et al., 2015) and machine reading comprehension (Rajpurkar et al., 2016) which operates over passages than isolated sentences. In both cases, our evidence estimators take advantage of powerful pre-trained encoders such as BERT (Devlin et al., 2019), to better capture semantic interactions between queries and text units. Our contributions in this work are threefold: we propose a coarse-to-fine model for QFS which we argue allows to introduce trainable components taking advantage of existing datasets and pre-trained models; we capitalize on the connections of QFS with question answering and propose different ways to effectively estimate the query-segment relationship; we provide experimental results on several benchmarks which show that our model consistently outperforms strong comparison systems across domains (news articles vs. medical"
2020.emnlp-main.296,D18-1443,0,0.0330022,"ent Summarization (QFS; Dang 2006) aims to create a short summary from a set of documents that answers a specific query. It has various applications in personalized information retrieval and recommendation engines where search results can be tailored to an information need (e.g., a user might be looking for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to the representational power afforded by deeper architectures and the availability of large-scale datasets containing 1 Our code can be downloaded from github.com/ yumoxu/querysum. hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Unfortunately, such datasets do not exist in QFS, and one might argue it is unrealistic they will ever be created for millions of queries, across different domains, and languages. In addition to the difficulties in obtaining training data, another obstacle to the application of end-to-end neural mod"
2020.emnlp-main.296,N18-1065,0,0.0405724,"king for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to the representational power afforded by deeper architectures and the availability of large-scale datasets containing 1 Our code can be downloaded from github.com/ yumoxu/querysum. hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Unfortunately, such datasets do not exist in QFS, and one might argue it is unrealistic they will ever be created for millions of queries, across different domains, and languages. In addition to the difficulties in obtaining training data, another obstacle to the application of end-to-end neural models is the size and number of source documents which can be very large. It is practically unfeasible (given memory limitations of current hardware) to train a model which encodes all of them into vectors and subsequently generates a summary from them. In this paper we propose a coarse-to-fine mode"
2020.emnlp-main.296,N10-1145,0,0.564176,"y problem in QFS. Existing QFS systems (Wan et al., 2007; Wan, 2008; Wan and Xiao, 2009; Wan and Zhang, 2014) employ classic retrieval techniques (such as TF-IDF) to estimate the affinity between query-sentence pairs. Such techniques can handle short keyword queries, but are less appropriate in QFS settings where query narratives can be long and complex. We argue that a trained evidence estimator might be better at performing semantic matching (Guo et al., 2016) between queries and document segments. To this effect, we experiment with two popular QA settings, namely answer sentence selection (Heilman and Smith, 2010; Yang et al., 2015) and machine reading comprehension (Rajpurkar et al., 2016) which operates over passages than isolated sentences. In both cases, our evidence estimators take advantage of powerful pre-trained encoders such as BERT (Devlin et al., 2019), to better capture semantic interactions between queries and text units. Our contributions in this work are threefold: we propose a coarse-to-fine model for QFS which we argue allows to introduce trainable components taking advantage of existing datasets and pre-trained models; we capitalize on the connections of QFS with question answering a"
2020.emnlp-main.296,P17-1147,0,0.0232614,"ire documents), and as a result is insensitive to the original input size and more scalable. Our key insight is to treat evidence estimation as a question answering task where a cluster of po3632 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3632–3645, c November 16–20, 2020. 2020 Association for Computational Linguistics tentially relevant documents provides support for answering a query (Baumel et al., 2016). Advantageously, we are able to train the evidence estimator on existing large-scale question answering datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), alleviating the data paucity problem in QFS. Existing QFS systems (Wan et al., 2007; Wan, 2008; Wan and Xiao, 2009; Wan and Zhang, 2014) employ classic retrieval techniques (such as TF-IDF) to estimate the affinity between query-sentence pairs. Such techniques can handle short keyword queries, but are less appropriate in QFS settings where query narratives can be long and complex. We argue that a trained evidence estimator might be better at performing semantic matching (Guo et al., 2016) between queries and document segments. To this effect, we experiment with two popula"
2020.emnlp-main.296,P09-2027,0,0.0279261,"which segments to include in the summary. The vast majority of previous work (Wan et al., 2007; Wan, 2008; Wan and Xiao, 2009; Wan and Zhang, 2014) creates summaries by ranking textual segments (usually sentences) according to their relationship (e.g., similarity) to other segments and their relevance to the query. In other words, relevance and evidence estimation are subservient to estimating the centrality of a segment (e.g., with a graph-based model). We argue that disentangling these subtasks allows us to better model the query and specialize the summaries to specific questions or topics (Katragadda and Varma, 2009). A coarse-to-fine approach is also expedient from a computational perspective; at each step the model processes a decreasing number of segments (rather than entire documents), and as a result is insensitive to the original input size and more scalable. Our key insight is to treat evidence estimation as a question answering task where a cluster of po3632 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3632–3645, c November 16–20, 2020. 2020 Association for Computational Linguistics tentially relevant documents provides support for answering a query"
2020.emnlp-main.296,D18-1055,0,0.0539568,"Missing"
2020.emnlp-main.296,D17-1221,0,0.358122,"nchmark datasets. 1 Introduction Query Focused Multi-Document Summarization (QFS; Dang 2006) aims to create a short summary from a set of documents that answers a specific query. It has various applications in personalized information retrieval and recommendation engines where search results can be tailored to an information need (e.g., a user might be looking for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to the representational power afforded by deeper architectures and the availability of large-scale datasets containing 1 Our code can be downloaded from github.com/ yumoxu/querysum. hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Unfortunately, such datasets do not exist in QFS, and one might argue it is unrealistic they will ever be created for millions of queries, across different domains, and languages. In addition to the difficulties in obtaining training d"
2020.emnlp-main.296,N03-1020,0,0.148329,"Missing"
2020.emnlp-main.296,K16-1028,0,0.0442119,"d outperforms strong comparison systems on benchmark datasets. 1 Introduction Query Focused Multi-Document Summarization (QFS; Dang 2006) aims to create a short summary from a set of documents that answers a specific query. It has various applications in personalized information retrieval and recommendation engines where search results can be tailored to an information need (e.g., a user might be looking for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to the representational power afforded by deeper architectures and the availability of large-scale datasets containing 1 Our code can be downloaded from github.com/ yumoxu/querysum. hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Unfortunately, such datasets do not exist in QFS, and one might argue it is unrealistic they will ever be created for millions of queries, across different domains, and languages. In addition to th"
2020.emnlp-main.296,D19-1261,0,0.0124094,"are more Figure 3: Performance (ROUGE-2 Recall) over k QA best evidence sentences selected by estimators trained on sentences and passages (development set). relevant and less redundant. We have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query. In the future, we would like to generate abstractive summaries following an unsupervised approach (Baziotis et al., 2019; Chu and Liu, 2019) and investigate how recent advances in open domain QA (Wang et al., 2019; Qi et al., 2019) can be adapted for query focused summarization. Acknowledgments The authors would like to thank the anonymous reviewers for their valuable feedback. We acknowledge the financial support of the European Research Council (Lapata; award number 681760). This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract FA865017-C-9118. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies"
2020.emnlp-main.296,P18-2124,0,0.0245778,"ntence selection and span selection. Red denotes answers while blue denotes a plausible answer to the question that cannot be answered from the given context. We use the union of WikiQA (Yang et al., 2015) and TrecQA (Heilman and Smith, 2010) for answer sentence selection and SQuAD 2.0 (Rajpurkar et al., 2016) for span selection. SQuAD 2.0 contains both answerable and unanswerable questions and we show one example for each of them. is provided in Table 1. We used three datasets for training our evidence estimator, including WikiQA (Yang et al., 2015), TrecQA (Yao et al., 2013), and SQuAD 2.0 (Rajpurkar et al., 2018). WikiQA and TrecQA are benchmarks for answer sentence selection while SQuAD 2.0 is a popular machine reading comprehension dataset (which we used for span selection). Compared to SQuAD, WikiQA and TrecQA are smaller and we therefore integrate them for model training (Yang et al., 2019). We show statistics for QA datasets in Table 2 and examples in Table 3. Implementation Details We used the publicly released BERT model2 and fine-tuned it on our QA tasks. Considering the maximum input length BERT allows (512 tokens) and the query narrative (which in DUC is fairly long), we set the maximum pass"
2020.emnlp-main.296,D16-1264,0,0.680478,"egments (rather than entire documents), and as a result is insensitive to the original input size and more scalable. Our key insight is to treat evidence estimation as a question answering task where a cluster of po3632 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3632–3645, c November 16–20, 2020. 2020 Association for Computational Linguistics tentially relevant documents provides support for answering a query (Baumel et al., 2016). Advantageously, we are able to train the evidence estimator on existing large-scale question answering datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), alleviating the data paucity problem in QFS. Existing QFS systems (Wan et al., 2007; Wan, 2008; Wan and Xiao, 2009; Wan and Zhang, 2014) employ classic retrieval techniques (such as TF-IDF) to estimate the affinity between query-sentence pairs. Such techniques can handle short keyword queries, but are less appropriate in QFS settings where query narratives can be long and complex. We argue that a trained evidence estimator might be better at performing semantic matching (Guo et al., 2016) between queries and document segments. To this effect, we experi"
2020.emnlp-main.296,P17-1099,0,0.0304551,"1 Introduction Query Focused Multi-Document Summarization (QFS; Dang 2006) aims to create a short summary from a set of documents that answers a specific query. It has various applications in personalized information retrieval and recommendation engines where search results can be tailored to an information need (e.g., a user might be looking for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to the representational power afforded by deeper architectures and the availability of large-scale datasets containing 1 Our code can be downloaded from github.com/ yumoxu/querysum. hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Unfortunately, such datasets do not exist in QFS, and one might argue it is unrealistic they will ever be created for millions of queries, across different domains, and languages. In addition to the difficulties in obtaining training data, another obstac"
2020.emnlp-main.296,N18-1158,1,0.836524,"ry Focused Multi-Document Summarization (QFS; Dang 2006) aims to create a short summary from a set of documents that answers a specific query. It has various applications in personalized information retrieval and recommendation engines where search results can be tailored to an information need (e.g., a user might be looking for an overview summary or a more detailed one which would allow them to answer a specific question). Neural approaches have become increasingly popular in single-document text summarization (Nallapati et al., 2016; Paulus et al., 2018; Li et al., 2017b; See et al., 2017; Narayan et al., 2018; Gehrmann et al., 2018), thanks to the representational power afforded by deeper architectures and the availability of large-scale datasets containing 1 Our code can be downloaded from github.com/ yumoxu/querysum. hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). Unfortunately, such datasets do not exist in QFS, and one might argue it is unrealistic they will ever be created for millions of queries, across different domains, and languages. In addition to the difficulties in obtaining training data, another obstacle to the application"
2020.emnlp-main.296,D08-1080,0,0.0605627,"hes which have generally shown strong performance in QFS. Under this framework, all sentences within a document cluster, together with their query relevance, are jointly considered in estimating centrality. A variety of approaches have been proposed to enhance the way relevance and centrality are estimated ranging from incorporating topic-sensitive information (Wan, 2008; Badrinath et al., 2011; Xu and Lapata, 2019), predictions about information certainty (Wan and Zhang, 2014), manifold-ranking algorithms (Wan et al., 2007; Wan and Xiao, 2009; Wan, 2009), and Wikipedia-based query expansion (Nastase, 2008). More recently, Li et al. (2015) estimate the salience of text units within a sparsecoding framework by additionally taking into account reader comments (associated with news reports). Li et al. (2017a) use a cascaded neural attention model to find salient sentences, whereas in follow-on work Li et al. (2017b) employ a generative model which maps sentences to a latent semantic space while a reconstruction model estimates sentence salience. There are also feature-based approaches achieving good results by optimizing sentence selection under a summary length constraint (Feigenblat et al., 2017)"
2020.emnlp-main.296,D19-1599,0,0.0182566,"to summaries which are more Figure 3: Performance (ROUGE-2 Recall) over k QA best evidence sentences selected by estimators trained on sentences and passages (development set). relevant and less redundant. We have also shown that disentangling the tasks of relevance, evidence, and centrality estimation is beneficial allowing us to progressively specialize the summaries to the semantics of the query. In the future, we would like to generate abstractive summaries following an unsupervised approach (Baziotis et al., 2019; Chu and Liu, 2019) and investigate how recent advances in open domain QA (Wang et al., 2019; Qi et al., 2019) can be adapted for query focused summarization. Acknowledgments The authors would like to thank the anonymous reviewers for their valuable feedback. We acknowledge the financial support of the European Research Council (Lapata; award number 681760). This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract FA865017-C-9118. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the"
2020.emnlp-main.296,Q18-1021,0,0.0184561,"include in the summary, making model performance relatively insensitive to the number and size of input documents. Drawing inspiration from recent work on QA, we take advantage of existing datasets in order to reliably estimate the relationship between the query and candidate segments. We focus on two QA subtasks which have attracted considerable attention in the literature, namely answer sentence selection which aims to extract answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Yang et al., 2015) and machine reading comprehension (Rajpurkar et al., 2016; Welbl et al., 2018; Yang et al., 2018), which aims at answering a question after processing a short text passage (Chen, 2018). QA and QFS are related but ultimately different tasks. QA aims at finding the best answer in a span or sentence, while QFS extracts a set of sentences based on user preferences and the content of the input documents under a length budget (Wan, 2008; Wan and Zhang, 2014). QA questions are often short and fact-based while QFS narratives can be longer and more complex (see the example in Section 3) and as a result simply localizing an answer within a cluster is not optimal. 3633 Figure 1:"
2020.emnlp-main.296,Q19-1037,1,0.90709,"ystems usually take as input a set of documents and select the sentences most relevant to the query for inclusion in the summary. In Figure 1(a), we provide a sketch of classic centrality-based approaches which have generally shown strong performance in QFS. Under this framework, all sentences within a document cluster, together with their query relevance, are jointly considered in estimating centrality. A variety of approaches have been proposed to enhance the way relevance and centrality are estimated ranging from incorporating topic-sensitive information (Wan, 2008; Badrinath et al., 2011; Xu and Lapata, 2019), predictions about information certainty (Wan and Zhang, 2014), manifold-ranking algorithms (Wan et al., 2007; Wan and Xiao, 2009; Wan, 2009), and Wikipedia-based query expansion (Nastase, 2008). More recently, Li et al. (2015) estimate the salience of text units within a sparsecoding framework by additionally taking into account reader comments (associated with news reports). Li et al. (2017a) use a cascaded neural attention model to find salient sentences, whereas in follow-on work Li et al. (2017b) employ a generative model which maps sentences to a latent semantic space while a reconstruc"
2020.emnlp-main.296,D15-1237,0,0.0742458,"Missing"
2020.emnlp-main.296,D18-1259,0,0.119285,"as a result is insensitive to the original input size and more scalable. Our key insight is to treat evidence estimation as a question answering task where a cluster of po3632 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3632–3645, c November 16–20, 2020. 2020 Association for Computational Linguistics tentially relevant documents provides support for answering a query (Baumel et al., 2016). Advantageously, we are able to train the evidence estimator on existing large-scale question answering datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Yang et al., 2018), alleviating the data paucity problem in QFS. Existing QFS systems (Wan et al., 2007; Wan, 2008; Wan and Xiao, 2009; Wan and Zhang, 2014) employ classic retrieval techniques (such as TF-IDF) to estimate the affinity between query-sentence pairs. Such techniques can handle short keyword queries, but are less appropriate in QFS settings where query narratives can be long and complex. We argue that a trained evidence estimator might be better at performing semantic matching (Guo et al., 2016) between queries and document segments. To this effect, we experiment with two popular QA settings, namel"
2020.emnlp-main.296,N13-1106,0,0.0343316,"Missing"
2020.emnlp-main.319,N19-1253,0,0.0365747,"Missing"
2020.emnlp-main.319,N19-1000,0,0.247589,"Missing"
2020.emnlp-main.319,D16-1102,0,0.0280507,"Missing"
2020.emnlp-main.319,C16-1058,0,0.0149357,"cy than a monolingual labeler. An obstacle for developing cross-lingual SRL models is the absence of a unified annotation scheme for all languages. Although the CoNLL-09 shared task (Hajiˇc et al., 2009) provides annotations for seven languages, the labeling schemes and role sets are not shared. To this end, van der Plas et al. (2010) build a French SRL dataset, following an annotation scheme similar to CoNLL-09 for English. Some recent cross-lingual SRL models (Aminian et al., 2017, 2019; Fei et al., 2020) make use of the publicly available Universal Proposition Bank (UPB; Akbik et al. 2015; Akbik and Li 2016), which annotates predicates and semantic roles following the English Proposition Bank 3.0 (Palmer et al., 2005). Since annotation projection is involved in the construction of UPB, the quality of UPB is also influenced by the quality of the parallel data, the performance of the source-language SRL model, and the accuracy of alignment tools. 3890 5 Conclusions In this paper we developed a cross-lingual SRL model and demonstrated it can effectively leverage unlabeled parallel data without relying on word alignments or any other external tools. We have also contributed two quality controlled dat"
2020.emnlp-main.319,I17-2003,0,0.212902,"our experiments. 3.2 Model Configuration Our model was implemented in PyTorch and optimized using the Adam optimizer (Kingma and Ba, 2014). Word embeddings were initialized using the officially released multilingual BERT (base; cased version; Devlin et al. 2019). The parameters of BERT are fixed during training in order to preserve the cross-lingual nature of the embeddings. Hyperparameter values (for all languages) are shown in Table 2. 3.3 Results on Universal Proposition Bank We compared our model against several baselines on the UPB test set. These include two transfer methods: Bootstrap (Aminian et al., 2017) and CModel (Aminian et al., 2019), which perform annotation projection through parallel data and filter Hyperparameters multilingual BERT embeddings size predicate indicator embeddings size batch size learning rate Bi-LSTM hidden states size BiLSTM depth hidden feature size in biaffine scorer Bi-LSTM hidden states size BiLSTM depth compressed role representation size hidden feature size in biaffine scorer value 768 16 30 0.001 400 3 300 256 2 30 30 Table 2: Hyperparameter settings for input and training (first block), semantic role labeler (second block) and semantic role compressor (third bl"
2020.emnlp-main.319,W19-0417,0,0.122697,"l learning, a perennial problem with building SLR systems lies in the paucity of training data since semantic role annotations are available for only a handful of the world’s languages. As a result, much previous work has focused on cross-lingual SRL which aims at leveraging existing resources in a source language to minimize the effort required to construct a model or annotations for a new target language. Annotation projection is a popular approach which transfers annotations from a source to a target language via automatic word alignments (Pad´o and Lapata, 2005; van der Plas et al., 2011; Aminian et al., 2019). Although very intuitive, it is sensitive to the quality of the parallel data, the performance of the source-language SRL model, and the accuracy of alignment tools, all of which introduce noise. Translation-based approaches (T¨ackstr¨om et al., 2012; Fei et al., 2020; Rasooli and Collins, 2015) aim to alleviate the noise brought by the the source-side labeler by directly translating the gold-standard data into the target language. A third alternative is model transfer where a source-language model is modified in a way that it can be directly applied to a new language, e.g., by employing cros"
2020.emnlp-main.319,W11-2136,0,0.037185,"and provides direct supervision for predicting semantic roles in the target language. Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.1 1 Introduction Semantic role labeling (SRL) is the task of identifying the arguments of semantic predicates in a sentence and labeling them with a set of predefined relations (e.g., “who” did “what” to “whom,” “when,” and “where”). It has emerged as an important technology for a wide spectrum of applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marche"
2020.emnlp-main.319,Q19-1022,1,0.844039,"are the output of the compressor taking SS and ST as input, respectively. Best viewed in color. 2.1 Semantic Role Labeler Input Layer and Encoder For each sentence, the representation of i-th word wi is the concatenation of multilingual contextualized word embeddings ewwi and predicate indicator embedding ewp i . The former are pretrained on a large-scale unlabeled corpus and their parameters stay frozen during the training of our model. Predicate embeddings are randomly initialized and updated constantly during model training. Unlike previous supervised SRL approaches (Roth and Lapata, 2016; Cai and Lapata, 2019; He et al., 2019), our model does not make use of any syntactic information (e.g., POS-tags, dependency relations) since we cannot assume it will be available for low-resource languages. Following Marcheggiani et al. (2017), sentences are represented using a multi-layer bi-directional LSTM (Hochreiter and Schmidhuber, 1997); the BiLSTM receives at time step t representation x for each word and recursively computes two hidden → − states, one for the forward pass ( h t ), and another ← − one for the backward pass ( h t ). Each word is the concatenation of its forward and backward LSTM → − ← − s"
2020.emnlp-main.319,W05-0620,0,0.359501,"Missing"
2020.emnlp-main.319,K19-1035,0,0.112751,"notated test sets for German, French, and Chinese. Pairwise differences between our model and previous systems are all statistically significant (p < 0.05) using stratified shuffling (Noreen, 1989). our model against manual annotations on French, German, and Chinese (see Table 1). Since previous models have not provided results on these datasets, we re-implemented three strong comparison systems, i.e., CModel, MAN-MOE, and PGN. Details on our implementation are in the Appendix. Our results are summarized in Table 4, where languages are ordered in terms of their word order distance to English (Ahmad et al., 2019a). We note that our approach significantly outperforms previously published models on these three languages. All systems perform best on French which is perhaps unsurprising given that it is closest to English and worst on Chinese which is least related to English. This suggests that transferring SRL annotations between languages with similar word orders could be an easier task. 3.5 Ablation Study and Analysis To investigate the contribution of the semantic role compressor and cross-lingual training, we conducted a series of ablation studies on the manually annotated DE, FR, and ZH datasets."
2020.emnlp-main.319,P19-1299,0,0.290646,"e BiLSTM depth compressed role representation size hidden feature size in biaffine scorer value 768 16 30 0.001 400 3 300 256 2 30 30 Table 2: Hyperparameter settings for input and training (first block), semantic role labeler (second block) and semantic role compressor (third block). word alignments empirically. We also report the results of two strong mixture-of-experts models which focus on combining language specific features automatically (MOE; Guo et al. 2018), and also on learning language-invariant features with a multinomial adversarial network as a shared feature extractor (MAN-MOE; Chen et al. 2019). We also include a recently proposed translation-based model (PGN; Fei et al. 2020) which performs competitively on UPB; this system directly translates the source annotated corpus into the target language, and then performs annotation projection and filtering similar to Bootstrap and CModel. Table 3 shows labeled F-scores (using automatically predicted predicate senses) on the test portion of the Universal Proposition Bank. The various languages are ordered according to their typological distance to English based on word order (Ahmad et al., 2019a) with Portuguese being closest and Finnish f"
2020.emnlp-main.319,D18-1269,0,0.022156,"tion density. Model transfer does not require parallel corpora or word alignment tools; nevertheless, it relies on accurate features such as POS tags (McDonald et al., 2013) or syntactic parse trees (Kozhevnikov and Titov, 2013) to enhance the ability to generalize across languages. Adversarial training is commonly used to extract language-agnostic features thereby improving the performance of cross-lingual systems (Chen et al., 2019; Ahmad et al., 2019b). Translation-based approaches have been gaining popularity in cross-lingual dependency parsing (Rasooli and Collins, 2015; Tiedemann, 2015; Conneau et al., 2018) and have recently been applied to SRL (Fei et al., 2020). Daza and Frank (2019b) propose a cross-lingual encoder-decoder model that simultaneously translates and generates sentences with semantic role annotations in a resource-poor target language. Rather than creating annotations or models for a target language, other work aims to exploit the similarities between languages. Mulcaire et al. (2018) combine resources for multiple languages to create polyglot semantic role labelers and show that polyglot training can result in better labeling accuracy than a monolingual labeler. An obstacle for"
2020.emnlp-main.319,D19-1056,0,0.26608,"performance of the source-language SRL model, and the accuracy of alignment tools, all of which introduce noise. Translation-based approaches (T¨ackstr¨om et al., 2012; Fei et al., 2020; Rasooli and Collins, 2015) aim to alleviate the noise brought by the the source-side labeler by directly translating the gold-standard data into the target language. A third alternative is model transfer where a source-language model is modified in a way that it can be directly applied to a new language, e.g., by employing cross-lingual word representations (T¨ackstr¨om et al., 2012; Swayamdipta et al., 2016; Daza and Frank, 2019a) and universal POS tags (McDonald et al., 2013). Word alignment noise poses serious problems for both annotation-projection and translationbased methods (the latter still rely on alignment tools to transfer word-level labels from source to 3883 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3883–3894, c November 16–20, 2020. 2020 Association for Computational Linguistics target). For example, there could be many-to-one alignments, leading to semantic role conflicts in the target language. Some form of filtering is often introduced to reduce the"
2020.emnlp-main.319,N19-1423,0,0.0121768,"d it is far from trivial to unify them. To this end, we created two manual resources, by randomly sampling 258 German and 304 Chinese sentences from UPB. The manual annotation was performed by native speakers following the annotation guidelines of UPB which in turn follows the English Proposition Bank. Table 1 provides a breakdown of labeled data used in our experiments. 3.2 Model Configuration Our model was implemented in PyTorch and optimized using the Adam optimizer (Kingma and Ba, 2014). Word embeddings were initialized using the officially released multilingual BERT (base; cased version; Devlin et al. 2019). The parameters of BERT are fixed during training in order to preserve the cross-lingual nature of the embeddings. Hyperparameter values (for all languages) are shown in Table 2. 3.3 Results on Universal Proposition Bank We compared our model against several baselines on the UPB test set. These include two transfer methods: Bootstrap (Aminian et al., 2017) and CModel (Aminian et al., 2019), which perform annotation projection through parallel data and filter Hyperparameters multilingual BERT embeddings size predicate indicator embeddings size batch size learning rate Bi-LSTM hidden states siz"
2020.emnlp-main.319,2020.acl-main.627,0,0.0257476,"Missing"
2020.emnlp-main.319,D18-1498,0,0.0596318,"Missing"
2020.emnlp-main.319,P17-1044,0,0.0135982,"tion (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) which forego the need for extensive feature engineering. Despite recent advances in representational learning, a perennial problem with building SLR systems lies in the paucity of training data since semantic role annotations are available for only a handful of the world’s languages. As a result, much previous work has focused on cross-lingual SRL which aims at leveraging existing resources in a source language to minimize the effort required to construct a model or annotations for a new target language. Annotation projection is a popular approach which transfers a"
2020.emnlp-main.319,2005.mtsummit-papers.11,0,0.0296827,"ST ), Pˆθˆ (r|wTi , RT )) (13) i=1 The final training loss during cross-lingual training Lcross is the sum of above losses: S T S T Lcross = Lcross + Lcross + Lcom + Lcom 3 3.1 (14) Experiments Datasets We trained our model using English as the source language and obtained semantic role labelers in German (DE), Spanish (ES), Finish (FI), French (FR), Italian (IT), Portuguese (PT), and Chinese (ZH). For English, we used the Proposition Bank (v3; Palmer et al. 2005) and the annotations provided as part of the CoNLL-09 shared task (Hajiˇc 3887 et al., 2009). We used the Europarl parallel corpus (Koehn, 2005) for the European languages and a large-scale EN-ZH parallel corpus (Xu, 2019) for Chinese. We provide details regarding the size of the parallel corpora in the Appendix. We compared our model against previous methods on the Universal Proposition Bank (UPB, v1.0; Akbik et al. 2016), which is built upon the Universal Dependency Treebank (UDT, v1.4) and the Proposition Bank (PB, v3.0). All languages in the UBP follow a unified dependency-based SRL annotation scheme. In order to comply with this scheme, we converted argument spans in the English Proposition Bank to dependency-based arguments by l"
2020.emnlp-main.319,P13-1117,0,0.0961071,"introduced to reduce the impact of this noise, e.g., parallel sentence pairs are discarded according to projection density (Aminian et al., 2019) or alignment confidence (Fei et al., 2020). In addition, translation-based approaches rely on high performance translation engines, which are often trained on large-scale parallel corpora. Unfortunately, neither adequate MT nor high-quality parallel data can be guaranteed when dealing with low-resource languages. Model transfer is an appealing alternative, however, it relies on accurate features based on lemmas, POS tags, and syntactic parse trees (Kozhevnikov and Titov, 2013; Fei et al., 2020) which are themselves obtained with access to additional annotation. It is not realistic to assume that treebank-style resources will be available for low-resource languages. In this paper, we propose a novel method for cross-lingual SRL which does not rely on word alignments, machine translation or pre-processing tools such as parsers or taggers. Aside from semantic role annotations in the source language, we only assume access to raw text in the form of a parallel corpus. The backbone of our model is an LSTM-based semantic role labeler jointly trained with multi-lingual wo"
2020.emnlp-main.319,D19-1129,0,0.0598006,"Missing"
2020.emnlp-main.319,N18-2078,0,0.0205649,"t supervision for predicting semantic roles in the target language. Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.1 1 Introduction Semantic role labeling (SRL) is the task of identifying the arguments of semantic predicates in a sentence and labeling them with a set of predefined relations (e.g., “who” did “what” to “whom,” “when,” and “where”). It has emerged as an important technology for a wide spectrum of applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) which f"
2020.emnlp-main.319,K17-1041,0,0.11766,", 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) which forego the need for extensive feature engineering. Despite recent advances in representational learning, a perennial problem with building SLR systems lies in the paucity of training data since semantic role annotations are available for only a handful of the world’s languages. As a result, much previous work has focused on cross-lingual SRL which aims at leveraging existing resources in a source language to minimize the effort required to construct a model or annotations for a new target language. Annotation projection is a popular approach which transfers annotations from a source to"
2020.emnlp-main.319,P13-2017,0,0.0566898,"Missing"
2020.emnlp-main.319,P18-2106,0,0.0372279,"Missing"
2020.emnlp-main.319,H05-1108,1,0.707675,"Missing"
2020.emnlp-main.319,J05-1004,0,0.770406,"rvised features.1 1 Introduction Semantic role labeling (SRL) is the task of identifying the arguments of semantic predicates in a sentence and labeling them with a set of predefined relations (e.g., “who” did “what” to “whom,” “when,” and “where”). It has emerged as an important technology for a wide spectrum of applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) which forego the need for extensive feature engineering. Despite recent advances in representational learning, a perennial problem with building SLR systems lies in the paucity of training data since semantic role annot"
2020.emnlp-main.319,P11-2052,0,0.0816956,"Missing"
2020.emnlp-main.319,W10-1814,0,0.0547513,"Missing"
2020.emnlp-main.319,W13-3516,0,0.037585,"Missing"
2020.emnlp-main.319,D15-1039,0,0.0574489,"Missing"
2020.emnlp-main.319,P16-1113,1,0.792755,"orpus, where RS and RT are the output of the compressor taking SS and ST as input, respectively. Best viewed in color. 2.1 Semantic Role Labeler Input Layer and Encoder For each sentence, the representation of i-th word wi is the concatenation of multilingual contextualized word embeddings ewwi and predicate indicator embedding ewp i . The former are pretrained on a large-scale unlabeled corpus and their parameters stay frozen during the training of our model. Predicate embeddings are randomly initialized and updated constantly during model training. Unlike previous supervised SRL approaches (Roth and Lapata, 2016; Cai and Lapata, 2019; He et al., 2019), our model does not make use of any syntactic information (e.g., POS-tags, dependency relations) since we cannot assume it will be available for low-resource languages. Following Marcheggiani et al. (2017), sentences are represented using a multi-layer bi-directional LSTM (Hochreiter and Schmidhuber, 1997); the BiLSTM receives at time step t representation x for each word and recursively computes two hidden → − states, one for the forward pass ( h t ), and another ← − one for the backward pass ( h t ). Each word is the concatenation of its forward and b"
2020.emnlp-main.319,K16-1019,0,0.0177244,"of the parallel data, the performance of the source-language SRL model, and the accuracy of alignment tools, all of which introduce noise. Translation-based approaches (T¨ackstr¨om et al., 2012; Fei et al., 2020; Rasooli and Collins, 2015) aim to alleviate the noise brought by the the source-side labeler by directly translating the gold-standard data into the target language. A third alternative is model transfer where a source-language model is modified in a way that it can be directly applied to a new language, e.g., by employing cross-lingual word representations (T¨ackstr¨om et al., 2012; Swayamdipta et al., 2016; Daza and Frank, 2019a) and universal POS tags (McDonald et al., 2013). Word alignment noise poses serious problems for both annotation-projection and translationbased methods (the latter still rely on alignment tools to transfer word-level labels from source to 3883 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3883–3894, c November 16–20, 2020. 2020 Association for Computational Linguistics target). For example, there could be many-to-one alignments, leading to semantic role conflicts in the target language. Some form of filtering is often int"
2020.emnlp-main.319,N12-1052,0,0.0948773,"Missing"
2020.emnlp-main.319,W15-1824,0,0.134068,"A1, and A2, the improvements on AM-* (modifiers for current predicate) are modest for both French and Chinese. One possible reason is that the head words of A0, A1 and A2 are usually nouns or adjectives, which tend to have fixed positions in parallel sentence pairs. However, modifiers can be optional and have more varied positions within and across languages, which increases the difficulty for cross-lingual learning. 4 Related Work There has been a great deal of interest in crosslingual transfer learning for SRL (Pad´o and Lapata, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013; Tiedemann, 2015; Zhao et al., 2018; Chen et al., 2019; Aminian et al., 2019; Fei et al., 2020). The majority of previous work has focused on two types of approaches, namely annotation projection and model transfer. A variety of methods have been proposed to improve the quality of annotation projections due to alignment noise. These range from word and argument filtering techniques (Pad´o and Lapata, 2005, 2009), to learning syntax and semantics jointly (van der Plas et al., 2011), and iterative bootstrap4 The proportion of A2 in Chinese is higher than in French, as the two languages follow different annotati"
2020.emnlp-main.319,W10-1836,0,0.0106157,"ntroduction Semantic role labeling (SRL) is the task of identifying the arguments of semantic predicates in a sentence and labeling them with a set of predefined relations (e.g., “who” did “what” to “whom,” “when,” and “where”). It has emerged as an important technology for a wide spectrum of applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) which forego the need for extensive feature engineering. Despite recent advances in representational learning, a perennial problem with building SLR systems lies in the paucity of training data since semantic role annotations are available for"
2020.emnlp-main.319,P15-1109,0,0.0122156,"rom machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). There have been considerable efforts on developing annotated resources for semantic role labeling (Palmer et al., 2005; Zaghouani et al., 2010) which 1 Our code and data can be downloaded from https:// github.com/RuiCaiNLP/SRL_CPS. in turn have greatly facilitated the development of the various models designed to automatically predict semantic roles. Recent years have seen the successful application of neural network models to SRL (Zhou and Xu, 2015; He et al., 2017; Marcheggiani et al., 2017) which forego the need for extensive feature engineering. Despite recent advances in representational learning, a perennial problem with building SLR systems lies in the paucity of training data since semantic role annotations are available for only a handful of the world’s languages. As a result, much previous work has focused on cross-lingual SRL which aims at leveraging existing resources in a source language to minimize the effort required to construct a model or annotations for a new target language. Annotation projection is a popular approach"
2020.emnlp-main.337,D17-1223,0,0.0604145,"Missing"
2020.emnlp-main.337,W04-1013,0,0.0893905,"ator can directly attend to the word vacuum in the source reviews to increase its prediction probability. Additionally, we condition on partial information about the target review ri using an oracle 2 Both the code and datasets are available at: https:// github.com/abrazinskas/FewSum q(ri , r−i ) as shown in Eq. 2. M N 1 XX j j log Gθ (rij |Eθ (r−i ), q(rij , r−i )) (2) MN j=1 i=1 We refer to this partial information as properties (Ficler and Goldberg, 2017), which correspond to text characteristics of ri or relations between ri and r−i . For example, one such property can be the ROUGE score (Lin, 2004) between ri and r−i , which indicates the degree of overlap between ri and r−i . In Fig. 1, a high ROUGE value can signal to the generator to attend the word vacuum in the source reviews instead of predicting it based on language statistics. Intuitively, while the model observes a wide distribution of ROUGE scores during training on reviews, during summarization in test time we can achieve a high degree of input-output text overlap by setting the property to a high value. We considered three types of properties. Content Coverage: ROUGE-1, ROUGE-2, and ROUGE-L between ri and r−i signals to Gθ h"
2020.emnlp-main.337,K16-1028,0,0.0518863,"ment to the input reviews. The reviews are truncated, and delimited with the symbol ‘||’. generation (Hu and Liu, 2004; Medhat et al., 2014; Angelidis and Lapata, 2018). Introduction Summarization of user opinions expressed in online resources, such as blogs, reviews, social media, or internet forums, has drawn much attention due to its potential for various information access applications, such as creating digests, search, and report Although significant progress has been observed in supervised summarization in non-subjective single-document context, such as news articles (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018), modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. A key obstacle making data annotation expensive is that annotators need to consider multiple input texts when writing a summary, which is 4119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4119–4135, c November 16–20, 2020. 2020 Association for Computational Linguistics time-consuming. Moreover, annotation would have to be under"
2020.emnlp-main.337,P17-1099,0,0.0665184,"uncated, and delimited with the symbol ‘||’. generation (Hu and Liu, 2004; Medhat et al., 2014; Angelidis and Lapata, 2018). Introduction Summarization of user opinions expressed in online resources, such as blogs, reviews, social media, or internet forums, has drawn much attention due to its potential for various information access applications, such as creating digests, search, and report Although significant progress has been observed in supervised summarization in non-subjective single-document context, such as news articles (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018), modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. A key obstacle making data annotation expensive is that annotators need to consider multiple input texts when writing a summary, which is 4119 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4119–4135, c November 16–20, 2020. 2020 Association for Computational Linguistics time-consuming. Moreover, annotation would have to be undertaken for multiple domains as online re"
2020.emnlp-main.337,P16-1162,0,0.00773267,"480 human-written summaries (180 for Amazon and 300 for Yelp) for 8 reviews each, using Amazon Mechanical Turk (AMT). Each product/business received 3 summaries, and averaged ROUGE scores are reported in the following sections. Also, we reserved approximately 13 for testing and the rest for training and validation. The details are in Appendix 9.2. 4.2 Experimental Details For the main model, we used the Transformer architecture (Vaswani et al., 2017) with trainable length embeddings and shared parameters between the encoder and generator (Raffel et al., 2019). Subwords were obtained with BPE (Sennrich et al., 2016) using 32000 merges. Subword embeddings were shared across the model as a form of regularization (Press and Wolf, 2017). For a fair comparison, we approximately matched the number of parameters to C OPYCAT (Braˇzinskas et al., 2020). We randomly initialized all parameters with Glorot (Glorot and Bengio, 2010). For the plug-in network, we employed a multi-layer feed-forward network with multi-head attention modules over encoded states of the source review. After the last layer, we performed a linear projection to compute property values. Further, parameter optimization was performed using Adam"
2020.emnlp-main.337,P07-1056,0,\N,Missing
2020.emnlp-main.337,P19-1213,0,\N,Missing
2020.emnlp-main.337,N16-1095,0,\N,Missing
2020.emnlp-main.415,D11-1038,1,0.750876,"all experiments we assume that English is the high-resource language and German is WikiLarge WMT19 GeoLino Wikipedia Source EnglishC EnglishC — — Target EnglishS GermanC GermanS EnglishS Size 300K 6.0M 200K 1.4M Table 2: Training data used in our experiments; monolingual corpora shown under Target; indices are shorthands for Complex and Simple language. the low-resource language. Simplification data in English is taken from WikiLarge (Zhang and Lapata, 2017), a fairly large corpus which consists of a mixture of three automatically-collated Wikipedia simplification datasets (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). EnglishGerman bilingual data is taken from the WMT19 news translation task. Complex monolingual nonparallel data uses one side of the WMT19 translation data. Simple English non-parallel data uses sentences extracted from simple Wikipedia, a simplified version of Wikipedia. Simple German nonparallel data uses sentences scraped from GEOLino (Hancke et al., 2012), a German general-interest magazine for children aged between 8–14. Test Set We evaluated our model on two German simplification datasets, each targeting different users. TextComplexityDE (Naderi et al., 2019) consists"
2020.emnlp-main.415,Q15-1021,0,0.155218,"et al., 2015; Sutskever et al., 2014). In contrast to traditional methods, which target individual aspects of the simplification task, such as sentence splitting (Carroll et al. 1999; Chandrasekar et al. 1996, inter alia) or the substitution of complex words with simpler ones (Devlin, 1999; Kaji et al., 2002), neural models have no special purpose mechanisms for ensuring how to best simplify text. They rely on representation learning to implicitly capture simplification rewrites from data, i.e., examples of complex-simple sentence pairs. While large-scale parallel datasets exist for English (Xu et al., 2015; Zhang and Lapata, 2017) and Spanish (Agrawal and Carpuat, 2019), there is a limited amount of simplification data for other languages. For example, Klaper et al. (2013) automatically aligned 7,000 complex-simple German sentences,1 and Brunato et al. (2015) released 1,000 complex-simple Italian sentences. But datadriven approaches to simplification, in particular popular neural models, require significantly more training data to achieve good performance, making these datasets better suited for testing or development purposes. Unsupervised approaches (Surya et al., 2019; Artetxe et al., 2018)"
2020.emnlp-main.415,Q16-1029,0,0.116949,"s. As we wished to do equally well with all tasks we select a minibatch from a task with a probability inversely proportional to the training loss of the task. One model was selected using the average FRE-BLEU score across both development sets. All text was preprocessed using the UDPipe tokEvaluation As there is no single agreed-upon metric for simplification (Alva-Manchego et al., 2020; Sulem et al., 2018), we evaluate model output using a combination of four automaticallygenerated scores.5 These metrics have been previously shown to correlate with human judgments of simplification quality (Xu et al., 2016) and essentially quantify: a) whether the output is similar to the gold standard reference (Target-based, T); b) whether the output is similar to the source (Sourcebased, S); and c) whether the output is simple on its own, with no regard to preserving the meaning of the original sentence (Readability-based, R). We indicate the type of each metric using superscripts. BLEUT (Papineni et al., 2002) assesses the degree to which generated simplifications agree with the gold standard references.6 I-BLEUT,S (Sun and Zhou, 2012) combines self-BLEU and BLEU to reward systems with high overlap with the"
2020.emnlp-main.415,D17-1062,1,0.905926,"d and pivot-based methods. 1 Introduction Sentence simplification aims to reduce the linguistic complexity of a text whilst retaining most of its meaning. It has been the subject of several modeling efforts in recent years due to its relevance to various applications (Siddharthan, 2014; Shardlow, 2014). Examples include the development of reading aids for individuals with autism (Evans et al., 2014), aphasia (Carroll et al., 1999), dyslexia (Rello et al., 2013), and population groups with low-literacy skills (Watanabe et al., 2009), such as children and non-native speakers. Modern approaches (Zhang and Lapata, 2017; Mallinson and Lapata, 2019; Nishihara et al., 2019; Dong et al., 2019) view the simplification task as monolingual text-to-text rewriting and employ the very successful encoder-decoder neural architecture (Bahdanau et al., 2015; Sutskever et al., 2014). In contrast to traditional methods, which target individual aspects of the simplification task, such as sentence splitting (Carroll et al. 1999; Chandrasekar et al. 1996, inter alia) or the substitution of complex words with simpler ones (Devlin, 1999; Kaji et al., 2002), neural models have no special purpose mechanisms for ensuring how to be"
2020.emnlp-main.415,N18-1063,0,0.100233,"out set to 0.1. The networks were optimized using Adam (Kingma and Ba, 2014). Multi-tasking was performed by alternating batches of different tasks. Tasks varied in dataset sizes and had different difficulties. As we wished to do equally well with all tasks we select a minibatch from a task with a probability inversely proportional to the training loss of the task. One model was selected using the average FRE-BLEU score across both development sets. All text was preprocessed using the UDPipe tokEvaluation As there is no single agreed-upon metric for simplification (Alva-Manchego et al., 2020; Sulem et al., 2018), we evaluate model output using a combination of four automaticallygenerated scores.5 These metrics have been previously shown to correlate with human judgments of simplification quality (Xu et al., 2016) and essentially quantify: a) whether the output is similar to the gold standard reference (Target-based, T); b) whether the output is similar to the source (Sourcebased, S); and c) whether the output is simple on its own, with no regard to preserving the meaning of the original sentence (Readability-based, R). We indicate the type of each metric using superscripts. BLEUT (Papineni et al., 20"
2020.emnlp-main.415,D18-1355,0,0.0272328,"tasets for training their model. Palmero Aprosio et al. (2019) explore different ways to to incorporate non-parallel simplification data to expand small scale training data, including autoencoding and backtranslation. Translation data, in the form of paraphrases, has also been incorporated into simplification models leading to significant improvements. Guo et al. (2018) use multi-task learning to augment the limited amount of simplification training data. In addition to training on complex-simple sentence pairs, their model employs paraphrases, created automatically using machine translation. Zhao et al. (2018) augment a Transformer-based simplification model with lexical rules obtained from Simple PPDB (Pavlick and Callison-Burch, 2016), a database of paraphrase rules, automatically annotated with simplicity scores. Unlike previous approaches, we do not train models to create training data, either via backtranslation or extracting paraphrases. Instead, our model is able to train directly on existing datasets, saving computation power and time. In the future, it would be interesting to explore whether additional datasets or tasks improve simplification performance. Crosslingual Generation Cross-ling"
2020.emnlp-main.415,P12-2008,0,0.0304857,"reviously shown to correlate with human judgments of simplification quality (Xu et al., 2016) and essentially quantify: a) whether the output is similar to the gold standard reference (Target-based, T); b) whether the output is similar to the source (Sourcebased, S); and c) whether the output is simple on its own, with no regard to preserving the meaning of the original sentence (Readability-based, R). We indicate the type of each metric using superscripts. BLEUT (Papineni et al., 2002) assesses the degree to which generated simplifications agree with the gold standard references.6 I-BLEUT,S (Sun and Zhou, 2012) combines self-BLEU and BLEU to reward systems with high overlap with the reference, and penalize those with high overlap to the source. Self-BLEU computes the BLEU score between the output and the source. It allows us to examine whether the models are making trivial changes to the input. Following Xu et al. (2016), we set the parameter which balances the contribution of the two metrics to α = 0.9. SARIT,S (Xu et al., 2016) is calculated using the average of three rewrite operation scores: addition, copying, and deletion. It rewards addition operations when the system’s output is not in the in"
2020.emnlp-main.415,P19-1198,0,0.255011,"atasets exist for English (Xu et al., 2015; Zhang and Lapata, 2017) and Spanish (Agrawal and Carpuat, 2019), there is a limited amount of simplification data for other languages. For example, Klaper et al. (2013) automatically aligned 7,000 complex-simple German sentences,1 and Brunato et al. (2015) released 1,000 complex-simple Italian sentences. But datadriven approaches to simplification, in particular popular neural models, require significantly more training data to achieve good performance, making these datasets better suited for testing or development purposes. Unsupervised approaches (Surya et al., 2019; Artetxe et al., 2018) which forgo the use of parallel corpora are an appealing solution to overcoming the paucity of data. However, in this paper we argue that better simplification models can be obtained by taking advantage of existing complex-simple data in a high-resource language, and bilingual data in a low-resource language (i.e., a language for which no parallel simplification corpus exists). Drawing inspiration from the success of machine translation (Firat et al., 2016b; Blackwood et al., 2018; Johnson et al., 2017), we propose a modeling framework which transfers simplification kno"
2020.emnlp-main.415,C10-1152,0,0.0534272,"ed in Table 2. For all experiments we assume that English is the high-resource language and German is WikiLarge WMT19 GeoLino Wikipedia Source EnglishC EnglishC — — Target EnglishS GermanC GermanS EnglishS Size 300K 6.0M 200K 1.4M Table 2: Training data used in our experiments; monolingual corpora shown under Target; indices are shorthands for Complex and Simple language. the low-resource language. Simplification data in English is taken from WikiLarge (Zhang and Lapata, 2017), a fairly large corpus which consists of a mixture of three automatically-collated Wikipedia simplification datasets (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). EnglishGerman bilingual data is taken from the WMT19 news translation task. Complex monolingual nonparallel data uses one side of the WMT19 translation data. Simple English non-parallel data uses sentences extracted from simple Wikipedia, a simplified version of Wikipedia. Simple German nonparallel data uses sentences scraped from GEOLino (Hancke et al., 2012), a German general-interest magazine for children aged between 8–14. Test Set We evaluated our model on two German simplification datasets, each targeting different users. TextComplexityDE (Nad"
2020.emnlp-main.454,Q18-1002,1,0.821936,"ciently capture various important story aspects in long synopses and reviews, we model our task from the perspective of Multiple Instance Learning (MIL). We assume that each synopsis and review is a bag of instances (i.e., sentences in our task), where labels are assigned at the bag level. In such cases, a prediction is made for the bag by either learning to aggregate the instance level predictions (Keeler and Rumelhart, 1992; Dietterich et al., 1997; Maron and Ratan, 1998) or jointly learning the labels for instances and the bag (hua Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015; Angelidis and Lapata, 2018; Xu and Lapata, 2019). In our setting, we choose the latter; i.e., we aggregate P (YP ) for each sentence with the combined representation of XP S and XR to compute P (YP |X). As we will show later, MIL improves prediction performance and promotes interpretability. We represent a synopsis XP S consisting of L sentences (s1 , ..., sL ) in a hierarchical manner instead of a long sequence of words. At first, for a sentence si = (w1 , ..., wT ) having T words, we create a matrix Ei where Eit is the vector representation for word wt in si . We use pre-trained Glove embeddings (Pennington et al., 2"
2020.emnlp-main.454,L18-1274,1,0.811738,"Missing"
2020.emnlp-main.454,C18-1244,1,0.688704,"eal life. This is a great mix, and the artistic style make the film memorable. violence action murder atmospheric revenge mafia family loyalty greed relationship artistic Figure 1: Example snippets from plot synopsis and review of The Godfather and tags that can be generated from these. Introduction A high-level description of stories represented by a tagset can assist consumers of story-based media (e.g., movies, books) during the selection process. Although collecting tags from users is timeconsuming and often suffers from coverage issues (Katakis et al., 2008), NLP techniques like those in Kar et al. (2018b) and Gorinski and Lapata (2018) can be employed to generate tags automatically from written narratives such as synopses. However, existing supervised approaches suffer from two significant weaknesses. Firstly, the accuracy of the extracted tags is subject to the quality of the synopses. Secondly, the tagset is predefined by what was present in the training and development sets and thus is brittle; story attributes are unbounded in principle and grow with the underlying vocabulary. To address the weaknesses presented above, we propose to exploit user reviews. We have found that movie reviews"
2020.emnlp-main.454,P97-1005,0,0.424897,"pproached as supervised learning, we push this task to an unsupervised direction to avoid the annotation burden. We verify our proposed method against multiple competitive baselines and conduct a human evaluation to confirm our tags’ effectiveness for a set of movies. 2 Background Prior art related to this paper’s work includes story analysis of movies and mining opinions from movie reviews. In this section, we briefly discuss these lines of work. Story Analysis of Movies Over the years, highlevel story characterization approaches evolved around the problem of identifying genres (Biber, 1992; Kessler et al., 1997; Petrenz, 2012; Worsham and Kalita, 2018). Genre information is helpful but not very expressive most of the time as it is a broad way to categorize items. Recent work (Gorinski and Lapata, 2018; Kar et al., 2018b) retrieves other Instances Tags per instance Reviews per movie S Sentence per document S Words per sentence R Sentence per document R Words per sentence Train 9, 746 3 72 50 21 117 27 Val 2, 437 3 74 53 21 116 27 Test 3, 046 3 72 51 21 116 27 Table 1: Statistics of the dataset. S denotes synopses and R denotes review summaries. attributes of movie storylines like mood, plot type, and"
2020.emnlp-main.454,C10-1074,0,0.0469024,"subtle distinction between the reviews of typical material products (e.g. phone, TV, furniture) and story-based items (e.g. literature, film, blog). In contrast to the usual aspect based opinions (e.g. battery, resolution, color), reviews of story-based items often contain end users’ feelings, important events of stories, or genre related information, which are abstract in nature (e.g. heart-warming, slasher, melodramatic) and do not have a very specific target aspect. Extraction of such opinions about stories has been approached by previous work using reviews of movies (Zhuang et al., 2006; Li et al., 2010) and books (Lin et al., 2013). Such attempts are broadly divided into two categories. The first category deals with spotting words or phrases (excellent, fantastic, boring) used by people to express how they feel about the story. And the second category focuses on extracting important opinionated sentences from reviews and generating a summary. In our work, while the primary task is to retrieve relevant tags from a pre-defined tagset by supervised learning, our model provides the ability to mine story aspects from reviews without any direct supervision. 3 Dataset Our starting data set is the M"
2020.emnlp-main.454,C18-1167,0,0.0152374,"push this task to an unsupervised direction to avoid the annotation burden. We verify our proposed method against multiple competitive baselines and conduct a human evaluation to confirm our tags’ effectiveness for a set of movies. 2 Background Prior art related to this paper’s work includes story analysis of movies and mining opinions from movie reviews. In this section, we briefly discuss these lines of work. Story Analysis of Movies Over the years, highlevel story characterization approaches evolved around the problem of identifying genres (Biber, 1992; Kessler et al., 1997; Petrenz, 2012; Worsham and Kalita, 2018). Genre information is helpful but not very expressive most of the time as it is a broad way to categorize items. Recent work (Gorinski and Lapata, 2018; Kar et al., 2018b) retrieves other Instances Tags per instance Reviews per movie S Sentence per document S Words per sentence R Sentence per document R Words per sentence Train 9, 746 3 72 50 21 117 27 Val 2, 437 3 74 53 21 116 27 Test 3, 046 3 72 51 21 116 27 Table 1: Statistics of the dataset. S denotes synopses and R denotes review summaries. attributes of movie storylines like mood, plot type, and possible feeling of consumers in a superv"
2020.emnlp-main.454,D14-1162,0,0.0871978,"dis and Lapata, 2018; Xu and Lapata, 2019). In our setting, we choose the latter; i.e., we aggregate P (YP ) for each sentence with the combined representation of XP S and XR to compute P (YP |X). As we will show later, MIL improves prediction performance and promotes interpretability. We represent a synopsis XP S consisting of L sentences (s1 , ..., sL ) in a hierarchical manner instead of a long sequence of words. At first, for a sentence si = (w1 , ..., wT ) having T words, we create a matrix Ei where Eit is the vector representation for word wt in si . We use pre-trained Glove embeddings (Pennington et al., 2014) to initialize E. Then, we encode the sentences using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015). It helps the model to create a sentence representation shi for the ith sentence in XP S by learning to put a higher weight on the words that correlate more with the target tags. The transformation is as follows: → − −−−−→ h wit = LST M (Eit ), t ∈ [1, T ] ← − ←−−−− h wit = LST M (Eit ), t ∈ [T, 1] → − ← − uit = tanh(Wwt .[ h wit , h wit ] + bw ) exp(rt ) rit = u> αit = P it vt ; t exp(rt ) Learning the Predefined Tagset Different words and senten"
2020.emnlp-main.454,E12-3002,0,0.0288464,"d learning, we push this task to an unsupervised direction to avoid the annotation burden. We verify our proposed method against multiple competitive baselines and conduct a human evaluation to confirm our tags’ effectiveness for a set of movies. 2 Background Prior art related to this paper’s work includes story analysis of movies and mining opinions from movie reviews. In this section, we briefly discuss these lines of work. Story Analysis of Movies Over the years, highlevel story characterization approaches evolved around the problem of identifying genres (Biber, 1992; Kessler et al., 1997; Petrenz, 2012; Worsham and Kalita, 2018). Genre information is helpful but not very expressive most of the time as it is a broad way to categorize items. Recent work (Gorinski and Lapata, 2018; Kar et al., 2018b) retrieves other Instances Tags per instance Reviews per movie S Sentence per document S Words per sentence R Sentence per document R Words per sentence Train 9, 746 3 72 50 21 117 27 Val 2, 437 3 74 53 21 116 27 Test 3, 046 3 72 51 21 116 27 Table 1: Statistics of the dataset. S denotes synopses and R denotes review summaries. attributes of movie storylines like mood, plot type, and possible feeli"
2020.emnlp-main.454,D19-1410,0,0.0123854,"e stories (Kar et al., 2018b). To our knowledge, this method is currently the best-performing system on our task. Pre-trained language models Large pre-trained language models (LM) built with Transformers (Vaswani et al., 2017) have shown impressive performance in a wide range of natural language understanding (NLU) tasks like natural language inference, sentiment analysis, and question-answering in the GLUE benchmark (Wang et al., 2019). However, directly fine-tuning such models for long texts like synopses and reviews is extremely memory expensive. Therefore, we employ Sentence-BERT (SBERT; Reimers and Gurevych, 2019) in our work, which is a state-of-the-art universal sentence encoder built with pre-trained BERT (Devlin et al., 2019). We use SBERT encoded sentence representations with our proposed model in Section 4 instead of training the Bi-LSTM with a word-level attention based sentence encoder. Then we use these representations to create a document representation using Bi-LSTM with sentence-level attention, keeping the rest of the model unchanged. 6 Results Quantitative Results We report the results of our experiments on the test5 set in Table 2. We mainly discuss the top-3 setting, where three tags ar"
2020.emnlp-main.703,D16-1125,1,0.840058,"space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller,"
2020.emnlp-main.703,2020.acl-main.463,0,0.195512,"s and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current evaluation paradigms for generalization. Yet, this is the structure of generalization in human development: drawing analogies to episodic mem"
2020.emnlp-main.703,J92-4003,0,0.0781903,"Missing"
2020.emnlp-main.703,P12-1015,0,0.0358063,"., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current"
2020.emnlp-main.703,2020.acl-main.407,0,0.0170751,"ng (She et al., 2014) in the real world face challenging, continuous perception and control (Tellex et al., 2020). Consequently, research in this space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et"
2020.emnlp-main.703,P14-1113,0,0.0214947,"Missing"
2020.emnlp-main.703,W17-2810,0,0.0700925,"Missing"
2020.emnlp-main.703,J93-2004,0,0.0733849,"al These World Scopes go beyond text to consider the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and ongoing progression of how contextual information can factor into representations and tasks. We conclude with a discussion of how 8718 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8718–8735, c November 16–20, 2020. 2020 Association for Computational Linguistics 1 WS1: Corpora and Representations The story of data-driven language research begins with the corpus. The Penn Treebank (Marcus et al., 1993) is the canonical example of a clean subset of naturally generated language, processed and annotated for the purpose of studying representations. Such corpora and the model representations built from them exemplify WS1. Community energy was initially directed at finding formal linguistic structure, such as recovering syntax trees. Recent success on downstream tasks has not required such explicitly annotated signal, leaning instead on unstructured fuzzy representations. These representations span from dense word vectors (Mikolov et al., 2013) to contextualized pretrained representations (Peters"
2020.emnlp-main.703,E12-1076,0,0.0176926,"r new research into more challenging world modeling. Mottaghi et al. (2016) predicts the effects of forces on objects in images. Bakhtin et al. (2019) extends this physical reasoning to complex puzzles of cause and effect. Sun et al. (2019b,a) models scripts and actions, and alternative unsupervised training regimes (Bachman et al., 2019) open up research towards automatic concept formation. Advances in computer vision have enabled building semantic representations rich enough to interact with natural language. In the last decade of work descendant from image captioning (Farhadi et al., 2010; Mitchell et al., 2012), a myriad of tasks on visual question answering (Antol et al., 2015; Das et al., 2018; Yagcioglu et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretrain"
2020.emnlp-main.703,1985.tmi-1.17,0,0.223965,"achs et al., 1981; O’Grady, 2005; Vigliocco et al., 2014). Perception includes auditory, tactile, and visual input. Even restricted to purely linguistic signals, sarcasm, stress, and meaning can be implied through prosody. Further, tactile senses lend meaning, both physical (Sinapov et al., 2014; Thomason et al., 2016) and abstract, to concepts like heavy and soft. Visual perception is a rich signal for modeling a vastness of experiences in the world that cannot be documented by text alone (Harnad, 1990). For example, frames and scripts (Schank and Abelson, 1977; Charniak, 1977; Dejong, 1981; Mooney and Dejong, 1985) require understanding often unstated sets of pre- and post-conditions about the world. To borrow from Charniak (1977), how should we learn the meaning, method, and implications of painting? A web crawl of knowledge Eugene Charniak (A Framed PAINTING: The Representation of a Common Sense Knowledge Fragment 1977) from an exponential number of possible how-to, text-only guides and manuals (Bisk et al., 2020) is misdirected without some fundamental referents to which to ground symbols. Models must be able to watch and recognize objects, people, and activities to understand the language describing"
2020.emnlp-main.703,N18-1202,0,0.171411,"1993) is the canonical example of a clean subset of naturally generated language, processed and annotated for the purpose of studying representations. Such corpora and the model representations built from them exemplify WS1. Community energy was initially directed at finding formal linguistic structure, such as recovering syntax trees. Recent success on downstream tasks has not required such explicitly annotated signal, leaning instead on unstructured fuzzy representations. These representations span from dense word vectors (Mikolov et al., 2013) to contextualized pretrained representations (Peters et al., 2018; Devlin et al., 2019). Word representations have a long history predating the recent success of deep learning methods. Outside of NLP, philosophy (Austin, 1975) and linguistics (Lakoff, 1973; Coleman and Kay, 1981) recognized that meaning is flexible yet structured. Early experiments on neural networks trained with sequences of words (Elman, 1990; Bengio et al., 2003) suggested that vector representations could capture both syntax and semantics. Subsequent experiments with larger models, documents, and corpora have demonstrated that representations learned from text capture a great deal of in"
2020.emnlp-main.703,D18-1261,0,0.0266104,"perimentation with language starkly contrasts with the disembodied chat bots that are the focus of the current dialogue community (Roller et al., 2020; Adiwardana et al., 2020; Zhou et al., 2020; Chen et al., 2018; Serban et al., 2017), which often do not learn from individual experiences and whose environments are not persistent enough to learn the effects of actions. Theory of Mind When attempting to get what we want, we confront people who have their own desires and identities. The ability to consider the feelings and knowledge of others is now commonly referred to as the “Theory of Mind” (Nematzadeh et al., 2018). This paradigm has also been described under the “Speaker-Listener” model (Stephens et al., 2010), and a rich theory to describe this computationally is being actively developed under the Rational Speech Act Model (Frank and Goodman, 2012; Bergen et al., 2016). A series of challenges that attempt to address this fundamental aspect of communication have been introduced (Nematzadeh et al., 2018; Sap et al., 2019). These works are a great start towards deeper understanding, but static datasets can be problematic due to the risk of embedding spurious patterns and bias (de Vries et al., 2020; Le e"
2020.emnlp-main.703,P19-1506,0,0.0297579,"s rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world face challenging, continuous perception and cont"
2020.emnlp-main.703,P16-1144,1,0.890919,"Missing"
2020.emnlp-main.703,N15-1082,0,0.0556621,"Missing"
2020.emnlp-main.703,D14-1162,0,0.0857475,"s in deep models. Traditionally, transfer learning relied on our understanding of model classes, such as English grammar. Domain adaptation simply required sufficient data to capture lexical variation, by assuming most higherlevel structure would remain the same. Unsupervised representations today capture deep associations across multiple domains, and can be used successfully transfer knowledge into surprisingly diverse contexts (Brown et al., 2020). These representations require scale in terms of both data and parameters. Concretely, Mikolov et al. (2013) trained on 1.6 billion tokens, while Pennington et al. (2014) scaled up to 840 billion tokens from Common Crawl. Recent approaches 1 A parallel discussion would focus on the hardware required to enable advances to higher World Scopes. Playstations (Pinto et al., 2009) and then GPUs (Krizhevsky et al., 2012) made many WS2 advances possible. Perception, interaction, and robotics leverage other new hardware. have made progress by substantially increasing the number of model parameters to better consume these vast quantities of data. Where Peters et al. (2018) introduced ELMo with ∼108 parameters, Transformer models (Vaswani et al., 2017) have continued to"
2020.emnlp-main.703,P18-2124,0,0.0547463,"Missing"
2020.emnlp-main.703,P19-1534,0,0.0307256,"rate language that does something to the world. Passive creation and evaluation of generated language separates generated utterances from their effects on other people, and while the latter is a rich learning signal it is inherently difficult to annotate. In order to learn the effects language has on the world, an agent must participate in linguistic activity, such as negotiation (Yang et al., 2019a; He et al., 2018; Lewis et al., 2017), collaboration (Chai et al., 2017), visual disambiguation (Anderson et al., 2018; Lazaridou et al., 2017; Liu and Chai, 2015), or providing emotional support (Rashkin et al., 2019). These activities require inferring mental states and social outcomes—a key area of interest in itself (Zadeh et al., 2019). What “lame” means in terms of discriminative information is always at question: it can be defined as “undesirable,” but what it tells one about the processes operating in the environment requires social context to determine (Bloom, 2002). It is the toddler’s social experimentation with “You’re so lame!” that gives the word weight and definite intent (Ornaghi et al., 2011). In other words, the discriminative signal for the most foundational part of a word’s meaning can o"
2020.emnlp-main.703,Q13-1003,0,0.0607394,"fool!” can be hurtful, while for others it may seem playful. Social knowledge is requisite for realistic understanding of sentiment in situated human contexts. 8725 Relevant recent work The move from WS2 to WS3 requires rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces c"
2020.emnlp-main.703,P19-1180,0,0.0412109,"ent work The move from WS2 to WS3 requires rethinking existing tasks and investigating where their semantics can be expanded and grounded. This idea is not new (Chen and Mooney, 2008; Feng and Lapata, 2010; Bruni et al., 2014; Lazaridou et al., 2016) and has accelerated in the last few years. Elliott et al. (2016) reframes machine translation with visual observations, a trend extended into videos (Wang et al., 2019b). Regneri et al. (2013) introduce a foundational dataset aligning text descriptions and semantic annotations of actions with videos. Vision can even inform core tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world fac"
2020.emnlp-main.703,D19-1454,0,0.0599647,"Missing"
2020.emnlp-main.703,P14-1068,1,0.867068,"al., 2018; Chaabouni et al., 2020). Suhr et al. (2019a) introduced an environment for evaluating language understanding in the service of a shared goal, and Andreas and Klein (2016) use a visual paradigm for studying pragmatics. Such efforts help us examine how inductive biases and environmental pressures build towards socialization (WS5), even if full social context is still too difficult and expensive to be practical. Most of this research provides resources such as data, code, simulators and methodology for evaluating the multimodal content of linguistic representations (Schlangen, 2019b; Silberer and Lapata, 2014; Bruni et al., 2012). Moving forward, we encourage a broad re-examination of how NLP frames the relationship between meaning and context (Bender and Koller, 2020) and how pretraining obfuscates our ability to measure generalization (Linzen, 2020). 7 Conclusions Our World Scopes are steep steps. WS5 implies a persistent agent experiencing time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult"
2020.emnlp-main.703,D19-1592,0,0.0554394,"Missing"
2020.emnlp-main.703,P18-1238,0,0.112804,"Missing"
2020.emnlp-main.703,W14-4313,1,0.780432,"re tasks like syntax (Shi et al., 2019) and language modeling (Ororbia et al., 2019). Careful design is key, as visually augmented tasks can fail to require sensory perception (Thomason et al., 2019a). Language-guided, embodied agents invoke many of the challenges of WS4. Language-based navigation (Anderson et al., 2018) and task completion (Shridhar et al., 2020) in simulation environments ground language to actions, but even complex simulation action spaces can be discretized and enumerated. By contrast, language-guided robots that perform task completion (Tellex et al., 2014) and learning (She et al., 2014) in the real world face challenging, continuous perception and control (Tellex et al., 2020). Consequently, research in this space effectively restricts understanding to small grammars (Paul et al., 2018; Walter et al., 2013) or controlled dialog responses (Thomason et al., 2020). These efforts to translate language instructions to actions build towards using language for end-to-end, continuous control (WS4). Collaborative games have long served as a testbed for studying language (Werner and Dyer, 1991) and emergent communication (Schlangen, 2019a; Lazaridou et al., 2018; Chaabouni et al., 202"
2020.emnlp-main.703,J08-1008,0,0.0602314,"time and a personalized set of experiences. confined to IID datasets that lack the structure in time from which humans draw correlations about long-range causal dependencies. What happens if a machine is allowed to participate consistently? This is difficult to test under current evaluation paradigms for generalization. Yet, this is the structure of generalization in human development: drawing analogies to episodic memories and gathering new data through non-independent experiments. As with many who have analyzed the history of NLP, its trends (Church, 2007), its maturation toward a science (Steedman, 2008), and its major challenges (Hirschberg and Manning, 2015; McClelland et al., 2019), we hope to provide momentum for a direction many are already heading. We call for and embrace the incremental, but purposeful, contextualization of language in human experience. With all that we have learned about what words can tell us and what they keep implicit, now is the time to ask: What tasks, representations, and inductive-biases will fill the gaps? Computer vision and speech recognition are mature enough for investigation of broader linguistic contexts (WS3). The robotics industry is rapidly developing"
2020.emnlp-main.703,D19-1218,0,0.0512018,"Missing"
2020.emnlp-main.703,P19-1644,0,0.137634,"easoning to complex puzzles of cause and effect. Sun et al. (2019b,a) models scripts and actions, and alternative unsupervised training regimes (Bachman et al., 2019) open up research towards automatic concept formation. Advances in computer vision have enabled building semantic representations rich enough to interact with natural language. In the last decade of work descendant from image captioning (Farhadi et al., 2010; Mitchell et al., 2012), a myriad of tasks on visual question answering (Antol et al., 2015; Das et al., 2018; Yagcioglu et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretraining (e.g. via conceptual captions (Sharma et al., 2018)) or further broadened to include audio (Tsai et al., 2019). Vision can also help ground speech signals (Srinivasa"
2020.emnlp-main.703,P06-1124,0,0.0482713,"ardware. have made progress by substantially increasing the number of model parameters to better consume these vast quantities of data. Where Peters et al. (2018) introduced ELMo with ∼108 parameters, Transformer models (Vaswani et al., 2017) have continued to scale by orders of magnitude between papers (Devlin et al., 2019; Radford et al., 2019; Zellers et al., 2019b) to ∼1011 (Brown et al., 2020). Current models are the next (impressive) step in language modeling which started with Good (1953), the weights of Kneser and Ney (1995); Chen and Goodman (1996), and the power-law distributions of Teh (2006). Modern approaches to learning dense representations allow us to better estimate these distributions from massive corpora. However, modeling lexical co-occurrence, no matter the scale, is still modeling the written world. Models constructed this way blindly search for symbolic co-occurences void of meaning. How can models yield both “impressive results” and “diminishing returns”? Language modeling— the modern workhorse of neural NLP systems—is a canonical example. Recent pretraining literature has produced results that few could have predicted, crowding leaderboards with “super-human"" accurac"
2020.emnlp-main.703,P19-1452,0,0.0131052,"e, in the limit, to everything humanity has ever written.1 We are no longer constrained to a single author or source, and the temptation for NLP is to believe everything that needs knowing can be learned from the written world. But, a large and noisy text corpus is still a text corpus. This move towards using large scale raw data has led to substantial advances in performance on existing and novel community benchmarks (Devlin et al., 2019; Brown et al., 2020). Scale in data and modeling has demonstrated that a single representation can discover both rich syntax and semantics without our help (Tenney et al., 2019). This change is perhaps best seen in transfer learning enabled by representations in deep models. Traditionally, transfer learning relied on our understanding of model classes, such as English grammar. Domain adaptation simply required sufficient data to capture lexical variation, by assuming most higherlevel structure would remain the same. Unsupervised representations today capture deep associations across multiple domains, and can be used successfully transfer knowledge into surprisingly diverse contexts (Brown et al., 2020). These representations require scale in terms of both data and pa"
2020.emnlp-main.703,N19-1197,1,0.927858,"the basis of action-oriented categories (Thelen and Smith, 1996) as children learn how to manipulate their perception by manipulating their environment. Language grounding enables an agent to connect words to these actionoriented categories for communication (Smith and Gasser, 2005), but requires action to fully discover such connections. Embodiment—situated action taking—is therefore a natural next broader context. An embodied agent, whether in a virtual world, such as a 2D Maze (MacMahon et al., 2006), a grid world (Chevalier-Boisvert et al., 2019), a simulated house (Anderson et al., 2018; Thomason et al., 2019b; Shridhar et al., 2020), or the real world (Tellex et al., 2011; Matuszek, 2018; Thomason et al., 2020; Tellex et al., 2020) must translate from language to action. Control and action taking open several new dimensions to understanding and actively learning about the world. Queries can be resolved via dialog-based exploration with a human interlocutor (Liu and Chai, 2015), even as new object properties, like texture and weight (Thomason et al., 2017), or feedback, like muscle activations (Moro and Kennington, 2018), become available. We see the need for embodied language with complex meaning"
2020.emnlp-main.703,P19-1656,0,0.0233734,"u et al., 2018), natural language and visual reasoning (Suhr et al., 2019b), visual commonsense (Zellers et al., 2019a), 8721 2 3 Or the 1,600 classes of Anderson et al. (2017). Torchvision/Detectron2 include dozens of trained models. and multilingual captioning/translation via video (Wang et al., 2019b) have emerged. These combined text and vision benchmarks are rich enough to train large-scale, multimodal transformers (Li et al., 2019a; Lu et al., 2019; Zhou et al., 2019) without language pretraining (e.g. via conceptual captions (Sharma et al., 2018)) or further broadened to include audio (Tsai et al., 2019). Vision can also help ground speech signals (Srinivasan et al., 2020; Harwath et al., 2019) to facilitate discovery of linguistic concepts (Harwath et al., 2020). At the same time, NLP resources contributed to the success of these vision backbones. Hierarchical semantic representations emerge from ImageNet classification pretraining partially due to class hypernyms owed to that dataset’s WordNet origins. For example, the person class sub-divides into many professions and hobbies, like firefighter, gymnast, and doctor. To differentiate such sibling classes, learned vectors can also encode lowe"
2020.emnlp-main.703,P10-1040,1,0.164233,"the recent success of deep learning methods. Outside of NLP, philosophy (Austin, 1975) and linguistics (Lakoff, 1973; Coleman and Kay, 1981) recognized that meaning is flexible yet structured. Early experiments on neural networks trained with sequences of words (Elman, 1990; Bengio et al., 2003) suggested that vector representations could capture both syntax and semantics. Subsequent experiments with larger models, documents, and corpora have demonstrated that representations learned from text capture a great deal of information about meaning in and out of context (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013; McCann et al., 2017). The intuition of such embedding representations, that context lends meaning, has long been acknowledged (Firth, 1957; Turney and Pantel, 2010). Earlier on, discrete, hierarchical representations, such as agglomerative clustering guided by mutual information (Brown et al., 1992), were constructed with some innate interpretability. A word’s position in such a hierarchy captures semantic and syntactic distinctions. When the Baum–Welch algorithm (Welch, 2003) is applied to unsupervised Hidden Markov Models, it assigns a class distribution to every word"
2020.emnlp-main.703,2020.cl-1.2,0,0.0416453,"Missing"
2020.emnlp-main.703,N19-1364,0,0.123027,"chniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication. Improvements in hardware and data collection have galvanized progress in NLP across many benchmark tasks. Impressive performance has been achieved in language modeling (Radford et al., 2019; Zellers et al., 2019b; Keskar et al., 2019) and span-selection question answering (Devlin et al., 2019; Yang et al., 2019b; Lan et al., 2020) through massive data and massive models. With models exceeding human performance on such tasks, now is an excellent time to reflect on a key question: Where is NLP going? In this paper, we consider how the data and world a language learner is exposed to define and constrains the scope of that learner’s semantics. Meaning does not arise from the statistical distribution of words, but from their use by people to communicate. Many of the assumptions and understandings on which communication relies lie outside of text. We must consider what is missing from models trained solel"
2020.emnlp-main.703,N10-1125,1,\N,Missing
2020.emnlp-main.703,D12-1110,0,\N,Missing
2020.emnlp-main.703,D08-1094,0,\N,Missing
2020.emnlp-main.703,P15-1135,1,\N,Missing
2020.emnlp-main.703,D16-1230,0,\N,Missing
2020.emnlp-main.703,P18-2103,0,\N,Missing
2020.emnlp-main.703,W18-5446,0,\N,Missing
2020.emnlp-main.703,P19-1388,0,\N,Missing
2020.emnlp-main.703,D17-1259,0,\N,Missing
2020.emnlp-main.703,D18-1256,0,\N,Missing
2020.emnlp-main.703,N19-1423,0,\N,Missing
2020.emnlp-main.703,D19-1598,0,\N,Missing
2020.findings-emnlp.45,D13-1160,0,0.143133,"Missing"
2020.findings-emnlp.45,P14-1133,0,0.0384888,"SQL queries). Early experiments considering multilingual word representations (Conneau et al., 2017; Song et al., 2018) yielded no significant improvement and these results are omitted for brevity. Multilingual “Shared” Encoder Following Duong et al. (2017) and Susanto and Lu (2017a), we experiment with an encoder trained with batches from multiple languages as input. Errors in the MT data are purportedly mitigated through the 2 deepset.ai/german-bert Machine Translation as Paraphrasing Paraphrasing is a common augmentation for semantic parsers to improve generalization to unseen utterances (Berant and Liang, 2014; Dong et al., 2017; Iyer et al., 2017; Su and Yan, 2017; Utama et al., 2018). While there has been some study of multilingual paraphrase systems (Ganitkevitch and Callison-Burch, 2014), we instead use MT as a paraphrase resource, similar to Mallinson et al. (2017). Each MT system will have have different outputs from different language models and therefore we hypothesize that an ensemble of multiple systems, (J1 , . . . JN ), will provide greater linguistic diversity to better approximate L. Whereas prior work uses back-translation or beam search, a developer in our scenario lacks the resourc"
2020.findings-emnlp.45,P04-3031,0,0.291427,"Missing"
2020.findings-emnlp.45,P19-1448,0,0.0576953,"table query to retrieve an answer, or denotation, from a knowledge base. Sequence-to-sequence neural networks (Sutskever et al., 2014) are a popular approach to semantic parsing, framing the task as sequence transduction from natural to formal languages (Jia and Liang, 2016; Dong and Lapata, 2016). 1 Our code and data can be found at github.com/ tomsherborne/bootstrap. Recent proposals include learning intermediate logic representations (Dong and Lapata, 2018; Guo et al., 2019), constrained decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019), and graph-based parsing (Bogin et al., 2019; Shaw et al., 2019). Given recent interest in semantic parsing and the data requirements of neural methods, it is unsurprising that many challenging datasets have been released in the past decade (Wang et al., 2015; Zhong et al., 2017; Iyer et al., 2017; Yu et al., 2018, 2019). However, these widely use English as synonymous for natural language. English is neither linguistically typical (Dryer and Haspelmath, 2013) nor the most widely spoken language worldwide (Eberhard et al., 2019), but is presently the lingua franca of both utterances and knowledge bases in semantic parsing. Natural langu"
2020.findings-emnlp.45,D09-1030,0,0.174627,"Missing"
2020.findings-emnlp.45,P19-1007,0,0.0425877,"Missing"
2020.findings-emnlp.45,2020.acl-main.608,0,0.0647974,"Missing"
2020.findings-emnlp.45,D18-1269,0,0.100792,"Missing"
2020.findings-emnlp.45,H94-1010,0,0.564391,"ut is presently the lingua franca of both utterances and knowledge bases in semantic parsing. Natural language interfaces intended for international deployment must be adaptable to multiple locales beyond prototypes for English. However, it is uneconomical to create brand new datasets for every new language and domain. In this regard, most previous work has focused on multilingual semantic parsing i.e., learning from multiple natural languages in parallel assuming the availability of multilingual training data. Examples of multilingual datasets include GeoQuery (Zelle and Mooney, 1996), ATIS (Dahl et al., 1994) and NLMaps (Haas and Riezler, 2016) but each is limited to one domain. For larger datasets, professional translation can be prohibitively expensive and require many man-hours from experts and native speakers. Recently, Min et al. (2019) reproduced the public partitions of the SPIDER dataset (Yu et al., 2018) into Chinese, but this required three expert annotators for verification and agreement. We posit there exists a more efficient strategy for expanding semantic parsing to a new language. In this work, we consider crosslingual semantic parsing, adapting a semantic parser trained on English,"
2020.findings-emnlp.45,D17-1091,1,0.909367,"Missing"
2020.findings-emnlp.45,K17-1038,0,0.537727,"neration in 45 languages. Susanto and Lu (2017a) explore a multilingual neural architecture in four languages for GeoQuery and three languages for ATIS by extending Dong and Lapata (2016) with multilingual encoders. Other work focuses on multilingual representations for semantic parsing based on universal dependencies (Reddy et al., 2017) or embeddings of logical forms (Zou and Lu, 2018). We capitalize on existing semantic parsing datasets to bootstrap from English to another language, and therefore, do not assume that multiple languages are available as parallel input. Our work is closest to Duong et al. (2017), however they explore how to parse both English and German simultaneously using a multilingual corpus. In contrast, we consider English data only as an augmentation to improve parsing in Chinese and German and do not use “real” utterances during training. Recently, Artetxe et al. (2020) studied MT for crosslingual entailment, however, our results in Section 5 suggest these prior findings may not extend to semantic parsing, owing to the heightened requirement for factual consistency across translations. Our work complements recent efforts in crosslingual language understanding such as XNLI for"
2020.findings-emnlp.45,P18-1033,0,0.0614992,"Missing"
2020.findings-emnlp.45,D16-1026,0,0.0601139,"Missing"
2020.findings-emnlp.45,ganitkevitch-callison-burch-2014-multilingual,0,0.0222572,"ults are omitted for brevity. Multilingual “Shared” Encoder Following Duong et al. (2017) and Susanto and Lu (2017a), we experiment with an encoder trained with batches from multiple languages as input. Errors in the MT data are purportedly mitigated through the 2 deepset.ai/german-bert Machine Translation as Paraphrasing Paraphrasing is a common augmentation for semantic parsers to improve generalization to unseen utterances (Berant and Liang, 2014; Dong et al., 2017; Iyer et al., 2017; Su and Yan, 2017; Utama et al., 2018). While there has been some study of multilingual paraphrase systems (Ganitkevitch and Callison-Burch, 2014), we instead use MT as a paraphrase resource, similar to Mallinson et al. (2017). Each MT system will have have different outputs from different language models and therefore we hypothesize that an ensemble of multiple systems, (J1 , . . . JN ), will provide greater linguistic diversity to better approximate L. Whereas prior work uses back-translation or beam search, a developer in our scenario lacks the resources to train a NMT system for such techniques. As a shortcut, we input the same English sentence into m public APIs for MT to retrieve a set of candidate paraphrases in the language of i"
2020.findings-emnlp.45,W18-2501,0,0.04889,"Missing"
2020.findings-emnlp.45,C16-1133,0,0.0147808,"r in our scenario lacks the resources to train a NMT system for such techniques. As a shortcut, we input the same English sentence into m public APIs for MT to retrieve a set of candidate paraphrases in the language of interest (we use three APIs in experiments). We experiment with two approaches to utilising these pseudo-paraphrases. The first, MT-Paraphrase, aims to learn a single, robust language model for L by uniformly sampling one paraphrase from (J1 , . . . JN ) as input to the model during each epoch of training. The second approach, MT-Ensemble, is an ensemble architecture similar to Garmash and Monz (2016) and Firat et al. (2016) combining attention over each paraphrase in a single decoder. For N paraphrases, we train N parallel encoder models, {en }Nn=1 , and ensemble across each paraphrase by combining N sets of encoder-decoder attention heads. For each encoder output, En = en (Xn ), we compute multi-head attention, zi in Equation 2, with the decoder state, D, as the query and En as the key and value (Equation 5). Attention heads are combined through a combination function (Equation 6) and output miε replaces zi in Equation 3. 503 We compare ensemble strategies using two combination functions"
2020.findings-emnlp.45,P19-1444,0,0.0136787,"i et al., 2013; Berant et al., 2013; Liang, 2016; Kollar et al., 2018), where a semantically complex question is translated to an executable query to retrieve an answer, or denotation, from a knowledge base. Sequence-to-sequence neural networks (Sutskever et al., 2014) are a popular approach to semantic parsing, framing the task as sequence transduction from natural to formal languages (Jia and Liang, 2016; Dong and Lapata, 2016). 1 Our code and data can be found at github.com/ tomsherborne/bootstrap. Recent proposals include learning intermediate logic representations (Dong and Lapata, 2018; Guo et al., 2019), constrained decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019), and graph-based parsing (Bogin et al., 2019; Shaw et al., 2019). Given recent interest in semantic parsing and the data requirements of neural methods, it is unsurprising that many challenging datasets have been released in the past decade (Wang et al., 2015; Zhong et al., 2017; Iyer et al., 2017; Yu et al., 2018, 2019). However, these widely use English as synonymous for natural language. English is neither linguistically typical (Dryer and Haspelmath, 2013) nor the most widely spoken language worldwi"
2020.findings-emnlp.45,D13-1161,0,0.0800756,"Missing"
2020.findings-emnlp.45,D10-1119,0,0.0801368,"ior findings may not extend to semantic parsing, owing to the heightened requirement for factual consistency across translations. Our work complements recent efforts in crosslingual language understanding such as XNLI for entailment (Conneau et al., 2018), semantic textual similarity (Cer et al., 2017) or the XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) benchmarks. There has also been interest in parsing into interlingual graphical meaning representations (Damonte and Cohen, 2018; Zhang et al., 2018), spoken language understanding (Upadhyay et al., 2018) and λ-calculus expressions (Kwiatkowski et al., 2010; Lu and Ng, 2011; Lu, 2014). In contrast, we focus on logical forms grounded in knowledge-bases and therefore do not consider these approaches further. 2 Throughout this work, we consider the real-world scenario where a typical developer wishes to develop a semantic parser to facilitate question answering from an existing commercial database to Related Work Across logical formalisms, there have been several proposals for multilingual semantic parsing 3 500 Problem Formulation Noun/Adjective Ambiguity (“first-class fares” is a noun object) EN DEMT DEH Show me the first class fares from Baltimo"
2020.findings-emnlp.45,D11-1140,0,0.219338,"Missing"
2020.findings-emnlp.45,W12-3152,0,0.0665502,"Missing"
2020.findings-emnlp.45,D17-1009,1,0.874066,"Missing"
2020.findings-emnlp.45,N18-1066,0,0.0184862,"roposed method overcomes the paucity of gold-standard training data using pre-trained models, joint training with English, and paraphrasing through MT engines; and (4) an investigation into practical minimum gold-standard translation requirements for a fixed performance penalty when MT is unavailable. which employ multiple natural languages in parallel (Jones et al., 2012; Andreas et al., 2013; Lu, 2014; Susanto and Lu, 2017b; Jie and Lu, 2018). Jie and Lu (2014) ensemble monolingual parsers to generate a single parse from < 5 source languages for GeoQuery (Zelle and Mooney, 1996). Similarly, Richardson et al. (2018) propose a polyglot automaton decoder for source-code generation in 45 languages. Susanto and Lu (2017a) explore a multilingual neural architecture in four languages for GeoQuery and three languages for ATIS by extending Dong and Lapata (2016) with multilingual encoders. Other work focuses on multilingual representations for semantic parsing based on universal dependencies (Reddy et al., 2017) or embeddings of logical forms (Zou and Lu, 2018). We capitalize on existing semantic parsing datasets to bootstrap from English to another language, and therefore, do not assume that multiple languages"
2020.findings-emnlp.45,P19-1010,0,0.287562,"eve an answer, or denotation, from a knowledge base. Sequence-to-sequence neural networks (Sutskever et al., 2014) are a popular approach to semantic parsing, framing the task as sequence transduction from natural to formal languages (Jia and Liang, 2016; Dong and Lapata, 2016). 1 Our code and data can be found at github.com/ tomsherborne/bootstrap. Recent proposals include learning intermediate logic representations (Dong and Lapata, 2018; Guo et al., 2019), constrained decoding (Yin and Neubig, 2017; Krishnamurthy et al., 2017; Lin et al., 2019), and graph-based parsing (Bogin et al., 2019; Shaw et al., 2019). Given recent interest in semantic parsing and the data requirements of neural methods, it is unsurprising that many challenging datasets have been released in the past decade (Wang et al., 2015; Zhong et al., 2017; Iyer et al., 2017; Yu et al., 2018, 2019). However, these widely use English as synonymous for natural language. English is neither linguistically typical (Dryer and Haspelmath, 2013) nor the most widely spoken language worldwide (Eberhard et al., 2019), but is presently the lingua franca of both utterances and knowledge bases in semantic parsing. Natural language interfaces inten"
2020.findings-emnlp.45,J84-3009,0,0.161594,"Missing"
2020.findings-emnlp.45,N18-2028,0,0.0153823,"rce language tabula rasa, we experiment with using pretrained 768-dimensional inputs from BERT-base in English, Chinese and German2 , as well as the multilingual model trained on 104 languages. To account for rare entities which may be absent from pre-trained vocabularies, we append these representations to learnable embeddings. Representations for logical form tokens are trained from a random initialisation, as we lack a BERT-style pre-trained model for meaning representations (i.e., λ−DCS or SQL queries). Early experiments considering multilingual word representations (Conneau et al., 2017; Song et al., 2018) yielded no significant improvement and these results are omitted for brevity. Multilingual “Shared” Encoder Following Duong et al. (2017) and Susanto and Lu (2017a), we experiment with an encoder trained with batches from multiple languages as input. Errors in the MT data are purportedly mitigated through the 2 deepset.ai/german-bert Machine Translation as Paraphrasing Paraphrasing is a common augmentation for semantic parsers to improve generalization to unseen utterances (Berant and Liang, 2014; Dong et al., 2017; Iyer et al., 2017; Su and Yan, 2017; Utama et al., 2018). While there has bee"
2020.findings-emnlp.45,P11-1122,0,0.0303798,"Missing"
2020.findings-emnlp.45,D18-1194,0,0.0629358,"Missing"
2020.findings-emnlp.45,P18-2107,0,0.0171261,"and Lu (2014) ensemble monolingual parsers to generate a single parse from < 5 source languages for GeoQuery (Zelle and Mooney, 1996). Similarly, Richardson et al. (2018) propose a polyglot automaton decoder for source-code generation in 45 languages. Susanto and Lu (2017a) explore a multilingual neural architecture in four languages for GeoQuery and three languages for ATIS by extending Dong and Lapata (2016) with multilingual encoders. Other work focuses on multilingual representations for semantic parsing based on universal dependencies (Reddy et al., 2017) or embeddings of logical forms (Zou and Lu, 2018). We capitalize on existing semantic parsing datasets to bootstrap from English to another language, and therefore, do not assume that multiple languages are available as parallel input. Our work is closest to Duong et al. (2017), however they explore how to parse both English and German simultaneously using a multilingual corpus. In contrast, we consider English data only as an augmentation to improve parsing in Chinese and German and do not use “real” utterances during training. Recently, Artetxe et al. (2020) studied MT for crosslingual entailment, however, our results in Section 5 suggest"
2020.findings-emnlp.45,P13-2009,0,\N,Missing
2020.findings-emnlp.45,P06-4018,0,\N,Missing
2020.findings-emnlp.45,P12-1051,0,\N,Missing
2020.findings-emnlp.45,P11-1060,0,\N,Missing
2020.findings-emnlp.45,D11-1149,0,\N,Missing
2020.findings-emnlp.45,Q14-1007,0,\N,Missing
2020.findings-emnlp.45,D14-1137,0,\N,Missing
2020.findings-emnlp.45,C14-1122,0,\N,Missing
2020.findings-emnlp.45,P15-1129,0,\N,Missing
2020.findings-emnlp.45,N16-1088,0,\N,Missing
2020.findings-emnlp.45,P17-2098,0,\N,Missing
2020.findings-emnlp.45,N18-1104,0,\N,Missing
2020.findings-emnlp.45,P17-1089,0,\N,Missing
2020.findings-emnlp.45,S17-2001,0,\N,Missing
2020.findings-emnlp.45,P17-2007,0,\N,Missing
2020.findings-emnlp.45,E17-1083,1,\N,Missing
2020.findings-emnlp.45,D17-1160,0,\N,Missing
2020.findings-emnlp.45,N18-3022,0,\N,Missing
2020.findings-emnlp.45,L18-1528,0,\N,Missing
2020.findings-emnlp.45,N19-1423,0,\N,Missing
2020.findings-emnlp.45,D19-1377,0,\N,Missing
2021.acl-long.112,C18-1139,0,0.021802,"s input or by directly predicting the latent codes. The output should have a different surface form to the input but remain fluent. Exemplar Construction During training, we retrieve exemplars Xsyn from the training data following a process which first identifies the underlying syntax of Y, and finds a question with the same syntactic structure but a different, arbitrary meaning. We use a shallow approximation of syntax, to ensure the availability of equivalent exemplars in the training data. An example of the exemplar retrieval process is shown in Table 2; we first apply a chunker (FlairNLP, Akbik et al., 2018) to Y, then extract the chunk label for each tagged span, ignoring stopwords. This gives us the template that Y follows. We then select a question at random from the training data with the same template to give Xsyn . If no other questions in the dataset use this template, we create an exemplar by replacing each chunk with a random sample of the same type. We experimented with a range of approaches to determining question templates, including using part-of-speech tags and (truncated) constituency parses. We found that using chunks and preserving stopwords gave a reasonable level of granularity"
2021.acl-long.112,P05-1074,0,0.247573,"ble to generate question 2 3 Head # 4 Figure 3: Predictive entropy by head for various question properties - lower entropy indicates higher predictive power. paraphrases with a better balance of diversity and intent preservation compared to prior work. Although we are able to identify some high-level properties encoded by each of the syntactic latent variables, further work is needed to learn interpretable syntactic encodings. 5 Related Work Paraphrasing Prior work on generating paraphrases has looked at extracting sentences with similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011). More recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al., 2017; Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019). Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into Engli"
2021.acl-long.112,P19-1602,0,0.0247224,". Syntactic Templates The idea of generating paraphrases by controlling the structure of the output has seen recent interest, but most work so far has assumed access to a template oracle. Iyyer et al. 1412 (2018) use linearized parse trees as a template, then sample paraphrases by using multiple templates and reranking the output. Chen et al. (2019a) use a multi task objective to train a model to generate output that follows an input template. Their approach is limited by their use of automatically generated paraphrases for training, and their reliance on the availability of oracle templates. Bao et al. (2019) use a discriminator to separate spaces, but rely on noising the latent space to induce variation in the output form. Their results show good fidelity to the references, but low variation compared to the input. Goyal and Durrett (2020) use the artifically generated dataset ParaNMT-50m (Wieting and Gimpel, 2018) for their training and evaluation, which displays low output variation according to our results. Kumar et al. (2020) show strong performance using full parse trees as templates, but focus on generating output with the correct parse and do not consider the problem of template prediction."
2021.acl-long.112,P01-1008,0,0.470918,"ng and form. S EPARATOR is able to generate question 2 3 Head # 4 Figure 3: Predictive entropy by head for various question properties - lower entropy indicates higher predictive power. paraphrases with a better balance of diversity and intent preservation compared to prior work. Although we are able to identify some high-level properties encoded by each of the syntactic latent variables, further work is needed to learn interpretable syntactic encodings. 5 Related Work Paraphrasing Prior work on generating paraphrases has looked at extracting sentences with similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011). More recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al., 2017; Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019). Other work has relied on the strength of neural machine translation models, translating an input into a pivot"
2021.acl-long.112,P19-1599,0,0.28224,"plicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori. In this paper, we propose S EPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent. Our key innovations are: (a) to train a model to reconstruct a target question from an input paraphrase with the same meaning, and an exemplar with the same surface form, and (b) to separately encode the form and meaning of questions as discrete and continuous latent variables respectively, enabling us to modify the output surface form"
2021.acl-long.112,N19-1254,0,0.380327,"plicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori. In this paper, we propose S EPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent. Our key innovations are: (a) to train a model to reconstruct a target question from an input paraphrase with the same meaning, and an exemplar with the same surface form, and (b) to separately encode the form and meaning of questions as discrete and continuous latent variables respectively, enabling us to modify the output surface form"
2021.acl-long.112,W14-4012,0,0.0137888,"Missing"
2021.acl-long.112,P11-2117,0,0.0369626,"ive power. paraphrases with a better balance of diversity and intent preservation compared to prior work. Although we are able to identify some high-level properties encoded by each of the syntactic latent variables, further work is needed to learn interpretable syntactic encodings. 5 Related Work Paraphrasing Prior work on generating paraphrases has looked at extracting sentences with similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011). More recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al., 2017; Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019). Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). Kumar et al. (2019) use submodular function maximisation to improve"
2021.acl-long.112,C04-1051,0,0.257827,"cates higher predictive power. paraphrases with a better balance of diversity and intent preservation compared to prior work. Although we are able to identify some high-level properties encoded by each of the syntactic latent variables, further work is needed to learn interpretable syntactic encodings. 5 Related Work Paraphrasing Prior work on generating paraphrases has looked at extracting sentences with similar meaning from large corpora (Barzilay and McKeown, 2001; Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011). More recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al., 2017; Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019). Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). Kumar et al. (2019) use submodular functi"
2021.acl-long.112,D17-1091,1,0.93523,"n word choice, syntactic structure and even question type. Our task is to generate these different surface forms, using only a single example as input. Introduction A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Table 1. Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al.,"
2021.acl-long.112,P13-1158,0,0.238899,"rm, leading to separated encoding spaces. We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time. Crucially, our method does not require access to an external source of target exemplars. Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods. 1 Table 1: Examples of question paraphrase clusters, drawn from Paralex (Fader et al., 2013). Each member of the cluster has essentially the same semantic intent, but a different surface form. Each cluster exhibits variation in word choice, syntactic structure and even question type. Our task is to generate these different surface forms, using only a single example as input. Introduction A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Tab"
2021.acl-long.112,N13-1092,0,0.0970975,"Missing"
2021.acl-long.112,2020.acl-main.22,0,0.0820715,"nslates input sentences into a pivot language (Czech), then back into English. Although this system was trained on high volumes of data (including Common Crawl), the training data contains relatively few questions, and we would not expect it to perform well in the domain under consideration. ‘Diverse Paraphraser using Submodularity’ (DiPS; Kumar et al. 2019) uses submodular optimisation to increase the diversity of samples from a standard encode-decoder model. Latent bag-of-words (BoW; Fu et al. 2019) uses an encoder-decoder model with a discrete bag-of-words as the latent encoding. SOW/REAP (Goyal and Durrett, 2020) uses a two stage approach, deriving a set of feasible syntactic rearrangements that is used to guide a second encoder-decoder model. We additionally implement a simple tf-idf baseline (Jones, 1972), retrieving the question from the training set with the highest similarity to the input. Finally, we include a basic copy baseline as a lower bound, that simply uses the input question as the output. 4 Results Our experiments were designed to answer three questions: (a) Does S EPARATOR effectively factorize meaning and form? (b) Does S EPARATOR 1409 Model Copy VAE AE tf-idf VQ-VAE ParaNMT DiPS SOW/"
2021.acl-long.112,2021.eacl-main.88,0,0.0179003,"use a discriminator to separate spaces, but rely on noising the latent space to induce variation in the output form. Their results show good fidelity to the references, but low variation compared to the input. Goyal and Durrett (2020) use the artifically generated dataset ParaNMT-50m (Wieting and Gimpel, 2018) for their training and evaluation, which displays low output variation according to our results. Kumar et al. (2020) show strong performance using full parse trees as templates, but focus on generating output with the correct parse and do not consider the problem of template prediction. Huang and Chang (2021) independently and concurrently propose training a model with a similar ‘split training’ approach to ours, but using constituency parses instead of exemplars, and a ‘bagof-words’ instead of reference paraphrases. Their approach has the advantage of not requiring paraphrase clusters during training, but they do not attempt to solve the problem of template prediction and rely on the availability of oracle target templates. Russin et al. (2020) modify the architecture of an encoder-decoder model, introducing an inductive bias to encode the structure of inputs separately from the lexical items to"
2021.acl-long.112,N18-1170,0,0.0193488,"orm. Each cluster exhibits variation in word choice, syntactic structure and even question type. Our task is to generate these different surface forms, using only a single example as input. Introduction A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Table 1. Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surfac"
2021.acl-long.112,W18-6319,0,0.0236992,"VAE baseline; paraphrase retrieval performance using zsem for the separated model is comparable to using z for the VAE. 3 z refers to the combined encoding, i.e., [zsem ; zsyn ]. 4.2 Paraphrase Generation Automatic Evaluation While we have shown that our approach leads to disentangled representations, we are ultimately interested in generating diverse paraphrases for unseen data. That is, given some input question, we want to generate an output question with the same meaning but different form. We use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al., 2002; Post, 2018) that is penalized by the similarity between the output and the input, iBLEU = αBLEU(output, ref erences) −(1 − α)BLEU(output, input), (9) where α = 0.7 is a constant that weights the tradeoff between fidelity to the references and variation from the input. We also report the usual BLEU(output, ref erences) as well as Self-BLEU(output, input). The latter allows us to examine whether the models are making trivial changes to the input. The Paralex test set contains 5.6 references on average per cluster, while QQP contains only 1.3. This leads to lower BLEU scores for QQP in general, since the mo"
2021.acl-long.112,2020.tacl-1.22,0,0.0643994,"ection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori. In this paper, we propose S EPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the original intent. Our key innovations are: (a) to train a model to reconstruct a target question from an input paraphrase with the same meaning, and an exemplar with the same surface form, and (b) to separately encode the form and meaning of questions as discrete and continuous latent variables respectively, enabling us to modify the output surface form while preserving the"
2021.acl-long.112,P19-1605,0,0.0493627,"include a model which reconstructs Y only from Xsem , with no signal for the desired form of the output. In other words, we derive both zsem and zsyn from Xsem , and no separation between meaning and form is learned. This model uses a continuous Gaussian latent variable for both zsyn and zsem , but is otherwise equivalent in architecture to S EPARATOR. We refer to this as the VAE baseline. We also experiment with a vanilla autoencoder or AE baseline by removing the variational component, such that zsem , zsyn = ˜esem , ˜esyn . We include our own implementation of the VQ-VAE model described in Roy and Grangier (2019). They use a quantized bottleneck for both zsem and zsyn , with a large codebook K = 64, 000, H = 8 heads and a residual connection within the quantizer. For QQP, containing only 55,611 trainEncoding zsem zsyn z Cluster type Paraphrase Template 0.943 0.096 0.952 0.092 0.960 0.096 (a) VAE Baseline Encoding zsem zsyn z Cluster type Paraphrase Template 0.944 0.053 0.065 0.866 0.307 0.849 (b) S EPARATOR Table 3: Retrieval accuracies for each encoding for each cluster type. The VAE baseline is trained only on paraphrase pairs and receives no signal for the desired form of the output. S EPARATOR is"
2021.acl-long.112,N19-1363,0,0.0456002,"Missing"
2021.acl-long.112,P19-1500,1,0.80563,"is induced by our proposed training scheme, shown in Figure 1 and described in detail in Section 2.2. 2.1 Model Architecture While the encoder and decoder used by the model are standard Transformer modules, our bottleneck is more complex and we now describe it in more detail. Let the encoder output be {eh,1 , . . . , eh,|X |} = E NCODER(X), where eh,t ∈ RD/HT , h ∈ 1, ..., HT with HT the number of transformer heads, |X |the length of the input sequence and D the dimension of the transformer. We first pool this sequence of encodings to a single vector, using the multi-head pooling described in Liu and Lapata (2019). For each head h, we calculate a distribution over time indexes αh,t using attention: exp ah,t , t0 ∈|X |exp ah,t0 αh,t = P (1) ah,t = kTh eh,t , (2) with kh ∈ RD/H a learned parameter. 1406 We then take a weighted average of a linear projection of the encodings, to give pooled output ˜eh , X ˜eh = αh,t0 Vh eh,t0 , (3) t0 ∈|X| RD/H×D/H with Vh ∈ a learned parameter. Transformer heads are assigned either to a semantic group Hsem , that will be trained to encode the intent of the input, ˜esem = [. . . ; ˜eh ; . . .], h ∈ Hsem , or to a syntactic group Hsyn , that will be trained to represent th"
2021.acl-long.112,P12-3005,0,0.0140138,"ork with respect to these codes. At test time, we set qh = argmaxq0 [p(qh0 |Xtest )]. h 3 Experimental Setup Datasets We evaluate our approach on two datasets: Paralex (Fader et al., 2013), a dataset of question paraphrase clusters scraped from WikiAnswers; and Quora Question Pairs (QQP)2 sourced from the community question answering forum Quora. We observed that a significant fraction of the questions in Paralex included typos or were ungrammatical. We therefore filter out any questions marked as non-English by a language detection 1408 2 https://www.kaggle.com/c/quora-question-pairs script (Lui and Baldwin, 2012), then pass the questions through a simple spellchecker. While this destructively edited some named entities in the questions, it did so in a consistent way across the whole dataset. There is no canonical split for Paralex, so we group the questions into clusters of paraphrases, and split these clusters into train/dev/test partitions with weighting 80/10/10. Similarly, QQP does not have a public test set. We therefore partitioned the clusters in the validation set randomly in two, to give us our dev/test splits. Summary statistics of the resulting datasets are given in Appendix B. All scores r"
2021.acl-long.112,J10-3003,0,0.0399359,"c preservation and syntactic novelty compared to previous methods. 1 Table 1: Examples of question paraphrase clusters, drawn from Paralex (Fader et al., 2013). Each member of the cluster has essentially the same semantic intent, but a different surface form. Each cluster exhibits variation in word choice, syntactic structure and even question type. Our task is to generate these different surface forms, using only a single example as input. Introduction A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Table 1. Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of a"
2021.acl-long.112,E17-1083,1,0.831039,"nitkevitch et al., 2013), or identifying paraphrases from sources that are weakly aligned (Dolan et al., 2004; Coster and Kauchak, 2011). More recently, neural approaches to paraphrasing have shown promise. Several models have used an information bottleneck to try to encode the semantics of the input, including VAEs (Bowman et al., 2016), VQ-VAEs (van den Oord et al., 2017; Roy and Grangier, 2019), and a latent bag-of-words model (Fu et al., 2019). Other work has relied on the strength of neural machine translation models, translating an input into a pivot language and then back into English (Mallinson et al., 2017; Wieting and Gimpel, 2018; Hu et al., 2019). Kumar et al. (2019) use submodular function maximisation to improve the diversity of paraphrases generated by an encoder-decoder model. Dong et al. (2017) use an automatic paraphrasing system to rewrite inputs to a question answering system at inference time, reducing the sensitivity of the system to the specific phrasing of a query. Syntactic Templates The idea of generating paraphrases by controlling the structure of the output has seen recent interest, but most work so far has assumed access to a template oracle. Iyyer et al. 1412 (2018) use lin"
2021.acl-long.112,P02-1040,0,0.112637,"ce loss compared to the VAE baseline; paraphrase retrieval performance using zsem for the separated model is comparable to using z for the VAE. 3 z refers to the combined encoding, i.e., [zsem ; zsyn ]. 4.2 Paraphrase Generation Automatic Evaluation While we have shown that our approach leads to disentangled representations, we are ultimately interested in generating diverse paraphrases for unseen data. That is, given some input question, we want to generate an output question with the same meaning but different form. We use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al., 2002; Post, 2018) that is penalized by the similarity between the output and the input, iBLEU = αBLEU(output, ref erences) −(1 − α)BLEU(output, input), (9) where α = 0.7 is a constant that weights the tradeoff between fidelity to the references and variation from the input. We also report the usual BLEU(output, ref erences) as well as Self-BLEU(output, input). The latter allows us to examine whether the models are making trivial changes to the input. The Paralex test set contains 5.6 references on average per cluster, while QQP contains only 1.3. This leads to lower BLEU scores for QQP in general,"
2021.acl-long.112,D18-1131,0,0.0129662,"type. Our task is to generate these different surface forms, using only a single example as input. Introduction A paraphrase of an utterance is “an alternative surface form in the same language expressing the same semantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Table 1. Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but h"
2021.acl-long.112,P19-1177,0,0.0167237,"e paraphrases. Their approach has the advantage of not requiring paraphrase clusters during training, but they do not attempt to solve the problem of template prediction and rely on the availability of oracle target templates. Russin et al. (2020) modify the architecture of an encoder-decoder model, introducing an inductive bias to encode the structure of inputs separately from the lexical items to improve compositional generalisation on an artificial semantic parsing task. Chen et al. (2019b) use a multi-task setup to generate separated encodings, but do not experiment with generation tasks. Shu et al. (2019) learn discrete latent codes to introduce variation to the output of a machine translation system. 6 Conclusion We present S EPARATOR, a method for generating paraphrases that balances high variation in surface form with strong intent preservation. Our approach consists of: (a) a training scheme that causes an encoder-decoder model to learn separated latent encodings, (b) a vector-quantized bottleneck that results in discrete variables for the syntactic encoding, and (c) a simple model to predict different yet valid surface forms for the output. Extensive experiments and a human evaluation sho"
2021.acl-long.112,P12-2008,0,0.0268934,"essfully factorise meaning and form, with negligible performance loss compared to the VAE baseline; paraphrase retrieval performance using zsem for the separated model is comparable to using z for the VAE. 3 z refers to the combined encoding, i.e., [zsem ; zsyn ]. 4.2 Paraphrase Generation Automatic Evaluation While we have shown that our approach leads to disentangled representations, we are ultimately interested in generating diverse paraphrases for unseen data. That is, given some input question, we want to generate an output question with the same meaning but different form. We use iBLEU (Sun and Zhou, 2012) as our primary metric, a variant of BLEU (Papineni et al., 2002; Post, 2018) that is penalized by the similarity between the output and the input, iBLEU = αBLEU(output, ref erences) −(1 − α)BLEU(output, input), (9) where α = 0.7 is a constant that weights the tradeoff between fidelity to the references and variation from the input. We also report the usual BLEU(output, ref erences) as well as Self-BLEU(output, input). The latter allows us to examine whether the models are making trivial changes to the input. The Paralex test set contains 5.6 references on average per cluster, while QQP contai"
2021.acl-long.112,P18-1042,0,0.203,"emantic content as the original form” (Madnani and Dorr, 2010). For questions, a paraphrase should have the same intent, and should lead to the same answer as the original, as in the examples in Table 1. Question paraphrases are of significant interest, with applications in data augmentation (Iyyer et al., 2018), query rewriting (Dong et al., 2017) and duplicate question detection (Shah et al., 2018), as they allow a system to better identify the underlying intent of a user query. Recent approaches to paraphrasing use information bottlenecks with VAEs (Bowman et al., 2016) or pivot languages (Wieting and Gimpel, 2018) to try to extract the semantics of an input utterance, before projecting back to a (hopefully different) surface form. However, these methods have little to no control over the preservation of the input meaning or variation in the output surface form. Other work has specified the surface form to be generated (Iyyer et al., 2018; Chen et al., 2019a; Kumar et al., 2020), but has so far assumed that the set of valid surface forms is known a priori. In this paper, we propose S EPARATOR, a method for generating paraphrases that exhibit high variation in surface form while still retaining the origi"
2021.acl-long.475,P19-1612,0,0.129262,"this work we do not assume access to any resources other than those available for generic summarization. We further decompose abstractive QFS into two subtasks: (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., generating an abstractive summary based on found evidence). Under this formulation, we use generic summarization data not only for conditional language modeling, but also for learning an evidence ranking model. Inspired by the Cloze task and its applications in NLP (Taylor, 1953; Lewis et al., 2019; Lee et al., 2019), we propose M ARGE, a Masked ROUGE regression framework for evidence estimation and ranking. M ARGE intro6096 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6096–6109 August 1–6, 2021. ©2021 Association for Computational Linguistics Training: Generic Summary - The Da Vinci Code was published in 2003, and within six years Brown had booted John Grisham f rom t he No. 1 sl ot on t he l ist of writ ers whose books were most often donated t o Oxf am's 700 shops. - The Independ"
2021.acl-long.475,2020.acl-main.703,0,0.102666,"., 2017a,b). More recently Xu and Lapata (2020) propose a coarse-to-fine framework that leverages distant supervision from question answering to extract summary-worthy content. Abstractive QFS has received significantly less attention. This is due to generation models being particularly data-hungry (Lebanoff et al., 2018; Liu and Lapata, 2019a) and the scarcity of QFS training data. The increasing availability of pretrained models has prompted the development of pipeline-style frameworks for QFS which use resources from a wider range of NLP tasks. For example, Su et al. (2020) fine-tune BART (Lewis et al., 2020) on CNN/DailyMail (Hermann et al., 2015), a single-document summarization dataset, and generate abstracts for QFS by iteratively summarizing paragraphs to a budget. They learn a query model for paragraph selection based on a plethora of QA and machine reading datasets (Su et al., 2019; Rajpurkar et al., 2016). Similarly, Laskar et al. (2020) fine-tune B ERT S UM on CNN/DailyMail, and propose a three-stage system which uses supervision from QFS data (typically reserved for evaluation) and related QA and paraphrase identification tasks. We also focus on abstractive QFS, however, we do not assume"
2021.acl-long.475,P19-1484,0,0.110392,"ains and topics. In this work we do not assume access to any resources other than those available for generic summarization. We further decompose abstractive QFS into two subtasks: (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., generating an abstractive summary based on found evidence). Under this formulation, we use generic summarization data not only for conditional language modeling, but also for learning an evidence ranking model. Inspired by the Cloze task and its applications in NLP (Taylor, 1953; Lewis et al., 2019; Lee et al., 2019), we propose M ARGE, a Masked ROUGE regression framework for evidence estimation and ranking. M ARGE intro6096 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6096–6109 August 1–6, 2021. ©2021 Association for Computational Linguistics Training: Generic Summary - The Da Vinci Code was published in 2003, and within six years Brown had booted John Grisham f rom t he No. 1 sl ot on t he l ist of writ ers whose books were most often donated t o Oxf am's 700 sh"
2021.acl-long.475,D17-1221,0,0.0279737,"benchmarks, and show that across query types and domains our system achieves state-of-the-art results on both evidence ranking and abstractive QFS. 2 Related Work The majority of previous QFS approaches have been extractive, operating over queries and document clusters from which they select query-relevant sentences to compose a summary. They mostly differ in the way centrality and relevance are estimated and incorporated, e.g., via manifold ranking (Wan et al., 2007), using a look-ahead strategy (Badrinath et al., 2011), uncertainty prediction (Wan and Zhang, 2014), or attention mechanisms (Li et al., 2017a,b). More recently Xu and Lapata (2020) propose a coarse-to-fine framework that leverages distant supervision from question answering to extract summary-worthy content. Abstractive QFS has received significantly less attention. This is due to generation models being particularly data-hungry (Lebanoff et al., 2018; Liu and Lapata, 2019a) and the scarcity of QFS training data. The increasing availability of pretrained models has prompted the development of pipeline-style frameworks for QFS which use resources from a wider range of NLP tasks. For example, Su et al. (2020) fine-tune BART (Lewis e"
2021.acl-long.475,P19-1500,1,0.834177,"guage modeling (i.e., summary generation). We introduce M ARGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.1 1 Introduction The neural encoder-decoder framework has become increasingly popular in generic summarization (See et al. 2017; Gehrmann et al. 2018; Liu and Lapata 2019a; Fabbri et al. 2019, inter alia) thanks to the availability of large-scale datasets containing hundreds of thousands of document-summary pairs. Training data of this magnitude is not readily available for query focused summarization (QFS; Dang 2005) which aims to create a short summary from a set of documents that answers a specific query. Existing corpora (Nema et al., 2017; Dang, 2005; Hoa, 2006; Baumel et al., 2016) are relatively small for modern data-hungry neural architectures and have been mostly used for evaluation purposes. 1 Our code and data is available at https://github. com/yum"
2021.acl-long.475,D19-1387,1,0.849469,"guage modeling (i.e., summary generation). We introduce M ARGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.1 1 Introduction The neural encoder-decoder framework has become increasingly popular in generic summarization (See et al. 2017; Gehrmann et al. 2018; Liu and Lapata 2019a; Fabbri et al. 2019, inter alia) thanks to the availability of large-scale datasets containing hundreds of thousands of document-summary pairs. Training data of this magnitude is not readily available for query focused summarization (QFS; Dang 2005) which aims to create a short summary from a set of documents that answers a specific query. Existing corpora (Nema et al., 2017; Dang, 2005; Hoa, 2006; Baumel et al., 2016) are relatively small for modern data-hungry neural architectures and have been mostly used for evaluation purposes. 1 Our code and data is available at https://github. com/yum"
2021.acl-long.475,P17-1098,0,0.0222987,"state-of-the-art performance despite learning from weak supervision.1 1 Introduction The neural encoder-decoder framework has become increasingly popular in generic summarization (See et al. 2017; Gehrmann et al. 2018; Liu and Lapata 2019a; Fabbri et al. 2019, inter alia) thanks to the availability of large-scale datasets containing hundreds of thousands of document-summary pairs. Training data of this magnitude is not readily available for query focused summarization (QFS; Dang 2005) which aims to create a short summary from a set of documents that answers a specific query. Existing corpora (Nema et al., 2017; Dang, 2005; Hoa, 2006; Baumel et al., 2016) are relatively small for modern data-hungry neural architectures and have been mostly used for evaluation purposes. 1 Our code and data is available at https://github. com/yumoxu/marge. A major bottleneck in leveraging generic summarization data for QFS is the absence of queries (Nema et al., 2017); the majority of existing datasets consist of document-summary pairs, while QFS summaries are expected to answer specific queries. Recent work (Xu and Lapata, 2020; Su et al., 2020; Laskar et al., 2020) sidesteps this problem by resorting to distant supe"
2021.acl-long.475,D16-1264,0,0.326209,"odern data-hungry neural architectures and have been mostly used for evaluation purposes. 1 Our code and data is available at https://github. com/yumoxu/marge. A major bottleneck in leveraging generic summarization data for QFS is the absence of queries (Nema et al., 2017); the majority of existing datasets consist of document-summary pairs, while QFS summaries are expected to answer specific queries. Recent work (Xu and Lapata, 2020; Su et al., 2020; Laskar et al., 2020) sidesteps this problem by resorting to distant supervision from query-relevant NLP resources including question answering (Rajpurkar et al., 2016; Chakraborty et al., 2020) and paraphrase identification (Dolan and Brockett, 2005). Such approaches incorporate query modeling in the summarization process but are even more data hungry compared to generic summarization ones, since they additionally require access to QA datasets which can be extremely costly to create (Bajaj et al., 2016; Kwiatkowski et al., 2019). Moreover, there is often a mismatch between queries in QA datasets and those in QFS scenarios (Xu and Lapata, 2020); the two types of queries are not identically distributed and it is practically infeasible to find appropriate que"
2021.acl-long.475,D15-1237,0,0.0822853,"Missing"
2021.acl-long.475,P17-1099,0,0.0296717,"ts for a query) and (2) conditional language modeling (i.e., summary generation). We introduce M ARGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model. Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.1 1 Introduction The neural encoder-decoder framework has become increasingly popular in generic summarization (See et al. 2017; Gehrmann et al. 2018; Liu and Lapata 2019a; Fabbri et al. 2019, inter alia) thanks to the availability of large-scale datasets containing hundreds of thousands of document-summary pairs. Training data of this magnitude is not readily available for query focused summarization (QFS; Dang 2005) which aims to create a short summary from a set of documents that answers a specific query. Existing corpora (Nema et al., 2017; Dang, 2005; Hoa, 2006; Baumel et al., 2016) are relatively small for modern data-hungry neural architectures and have been mostly used for evaluation purposes. 1 Our code and d"
2021.acl-long.475,N13-1106,0,0.0917323,"Missing"
2021.acl-long.475,N18-1081,0,0.0290184,"K] Unified Masked Representation The intuition behind UMR is that a summary will encapsulate most salient information a user needs, while a query typically covers only a small fraction. We thus add one or more “placeholders” to the query to represent missing information the user actually seeks. We also identify such information in generic summaries for selective masking, to reduce the distributional shift during training. The UMR for a summary is the concatenation of its sentential UMRs. To convert a sentence from natural language to UMR, we parse it with Open Information Extraction (Open IE; Stanovsky et al. 2018) to a set of propositions consisting of verbs and their arguments. The latter are considered candidate information slots I. We initialize Algorithm 1, by replacing all such slots with a [MASK] token. We subsequently sample and reveal a set of slots subject to a budget constraint. We define the budget as B = γ ∗ |I |where γ ∈ [0, 1] modulates the proportion of tokens to be revealed within I slots (and is optimized on the development set). Finally, in order to keep the representation of UMRS and UMRQ consistent (see next paragraph), we merge adjacent [MASK] tokens to one [MASK] resulting in a pa"
2021.acl-long.475,W18-6545,0,0.0230888,"ining loss is computed as LAE + LPAR . At inference, U NI LM V 2 operates over sentences deemed relevant by the query model and decodes summaries autoregressively (see Figure 1(b) left). Synthetic MDS Data The pre-trained language model can be fine-tuned on MDS datasets (e.g., Multi-News; Fabbri et al. 2019) which are perhaps better aligned with the QFS task since both MDS and QFS operate over document clusters. We additionally propose a way to create synthetic MDS datasets based on SDS data. This is advantageous for two reasons. Firstly, MDS resources are fairly limited compared to SDS data (Zhang et al., 2018; Lebanoff et al., 2018). And secondly, by construction, we can ensure various data characteristics which might be desirable (e.g., the number of topics represented in the document collection). A challenge with leveraging SDS for QFS is the summary length (Lebanoff et al., 2018). Summaries in SDS datasets such as CNN/DailyMail (Hermann et al., 2015), are on average 30 tokens long. In contrast, query focused summaries can be as long as 250 tokens. We sidestep this problem by adopting a retrieval-based solution. Specifically, we first build a database with all summaries in the 6099 Dataset 2005"
2021.acl-long.475,D19-5827,0,0.0201277,"ata-hungry (Lebanoff et al., 2018; Liu and Lapata, 2019a) and the scarcity of QFS training data. The increasing availability of pretrained models has prompted the development of pipeline-style frameworks for QFS which use resources from a wider range of NLP tasks. For example, Su et al. (2020) fine-tune BART (Lewis et al., 2020) on CNN/DailyMail (Hermann et al., 2015), a single-document summarization dataset, and generate abstracts for QFS by iteratively summarizing paragraphs to a budget. They learn a query model for paragraph selection based on a plethora of QA and machine reading datasets (Su et al., 2019; Rajpurkar et al., 2016). Similarly, Laskar et al. (2020) fine-tune B ERT S UM on CNN/DailyMail, and propose a three-stage system which uses supervision from QFS data (typically reserved for evaluation) and related QA and paraphrase identification tasks. We also focus on abstractive QFS, however, we do not assume access to any additional training resources over and above generic summarization datasets, even for query modeling. Moreover, our system is able to generate long QFS abstracts all at once, instead of iteratively creating bullet-style summaries which often lack coherence. 3 Problem Fo"
2021.acl-long.475,2020.nlpcovid19-2.14,0,0.105371,"rom a set of documents that answers a specific query. Existing corpora (Nema et al., 2017; Dang, 2005; Hoa, 2006; Baumel et al., 2016) are relatively small for modern data-hungry neural architectures and have been mostly used for evaluation purposes. 1 Our code and data is available at https://github. com/yumoxu/marge. A major bottleneck in leveraging generic summarization data for QFS is the absence of queries (Nema et al., 2017); the majority of existing datasets consist of document-summary pairs, while QFS summaries are expected to answer specific queries. Recent work (Xu and Lapata, 2020; Su et al., 2020; Laskar et al., 2020) sidesteps this problem by resorting to distant supervision from query-relevant NLP resources including question answering (Rajpurkar et al., 2016; Chakraborty et al., 2020) and paraphrase identification (Dolan and Brockett, 2005). Such approaches incorporate query modeling in the summarization process but are even more data hungry compared to generic summarization ones, since they additionally require access to QA datasets which can be extremely costly to create (Bajaj et al., 2016; Kwiatkowski et al., 2019). Moreover, there is often a mismatch between queries in QA data"
2021.eacl-main.229,2020.acl-main.175,1,0.838761,"r et al., 2014; Bahdanau et al., 2014) to various abstractive summarization tasks including headline generation (Rush et al., 2015), single- (See et al., 2017; Nallapati et al., 2016), and multi-document summarization (Wang and Ling, 2016; Liu et al., 2018; Liu and Lapata, 2019). Closest to our approach is the work of Wang and Ling (2016) who generate opinion summaries following a two-stage process which first selects/extracts reviews bearing pertinent information, and then generates the summary by conditioning on these reviews. More recent models (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020) perform opinion summarization in an unsupervised way. However, these are mostly done on toy datasets (Chu and Liu, 2019), typically with a small number of reviews per target entity. Our proposed framework works better on realworld datasets with a large number of reviews, since it eliminates the need to rely only on preselected salient reviews which we argue leads to information loss and subsequently less customizable generation. Instead, our model first condenses the source reviews into multiple dense vectors which serve as input to a decoder to generate an abstractive summary. Beyond produci"
2021.eacl-main.229,D18-1403,1,0.842002,"ion while leveraging the full spectrum of opinions available for a specific target. We perform experiments on a dataset consisting of movie reviews and opinion summaries elicited from the Rotten Tomatoes website (Wang and Ling, 2016; see Figure 1). Our proposed approach outperforms state-of-the-art models by a large margin using automatic metrics and in a judgment elicitation study. We also verify that our zero-shot customization technique can effectively generate need-specific summaries. 2663 2 Related Work Most opinion summarization models follow extractive methods (see Kim et al., 2011 and Angelidis and Lapata, 2018 for overviews), with the exception of a few systems which are able to generate novel words and phrases not featured in the source text. Ganesan et al. (2010) propose a graph-based framework for generating concise opinion summaries, while Gerani et al. (2014) represent reviews as discourse trees which they aggregate to a global graph to generate a summary. Other work (Carenini et al., 2013; Mukherjee and Joshi, 2013) takes the distribution of opinions and their aspects into account so as to generate more readable summaries. Di Fabbrizio et al. (2014) present a hybrid system which uses extracti"
2021.eacl-main.229,2020.acl-main.461,1,0.736516,"Missing"
2021.eacl-main.229,W14-3348,0,0.0219838,"thods include (g) R EGRESS +S2S (Wang and Ling, 2016), an instantiation of the EA framework where a ridge regression model with hand-engineered features implements the E X TRACT model, while an attention-based sequenceto-sequence neural network is the A BSTRACT model; (h) B ERT C ENT +S2S, our implementation of an EA-based system which uses B ERT C ENT instead of R EGRESS as the E XTRACT model; and 3 Our code can be downloaded from https://github. com/rktamplayo/CondaSum. Results Automatic Evaluation We considered two evaluation metrics which are also reported in Wang and Ling (2016): METEOR (Denkowski and Lavie, 2014), a recall-oriented metric that rewards matching stems, synonyms, and paraphrases, and ROUGE-SU4 (Lin, 2004) which is calculated as the recall of unigrams and skip-bigrams up to four words. We also report F1 -scores for ROUGE1/2/L (Lin, 2004). Unigram and bigram overlap (ROUGE-1 and ROUGE-2) are a proxy for assessing informativenes while the longest common subsequence (ROUGE-L) measures fluency. Our results are presented in Table 2. Among one-pass systems, the extractive model B ERT C ENT performs the best; despite being unsupervised and extractive, it benefits from the ability of large neural"
2021.eacl-main.229,N19-1423,0,0.0149876,"[st ; ct ] + bg ) σt = pc (yt0 ) = p(yt0 ) = σ(vs> st + vc> ct + vy> yt0 ) X ai i:yi0 =yt0 t σt ∗ pg (yt0 ) + (14) (15) (16) (1 − σt ) ∗ pc (yt0 ) (17) where W , v, and b are learned parameters, and t is the current timestep. Salience-biased Extracts The model presented so far has no explicit mechanism to encourage salience among reviews. We direct the decoder towards salient reviews by incorporating information from an extractive step. Specifically, we use B ERT C ENT, a centroid-based (Radev et al., 2000) document extraction method that obtains document representations by resorting to BERT (Devlin et al., 2019). B ERT C ENT can be simply described as follows. Firstly, given a review, we obtain its encoding as the 2665 average of its token encodings obtained from BERT. We then take the average of the review encodings and treat it as the centroid of the input reviews, which approximately represents the information that is considered salient. We select the top k reviews whose encodings are the nearest neigbors to the centroid. The selected reviews are concatenated into a long sequence and encoded using a separate BiLSTM whose output serves as input to an LSTM decoder. This decoder generates a salience-"
2021.eacl-main.229,W14-4408,0,0.0583485,"Missing"
2021.eacl-main.229,W18-2706,0,0.0141542,"ince it eliminates the need to rely only on preselected salient reviews which we argue leads to information loss and subsequently less customizable generation. Instead, our model first condenses the source reviews into multiple dense vectors which serve as input to a decoder to generate an abstractive summary. Beyond producing more informative summaries, we demonstrate that our approach also allows to customize them. Recent conditional generation models have focused on controlling various aspects of the output such as politeness (Sennrich et al., 2016), length (Kikuchi et al., 2016), content (Fan et al., 2018), or style (Ficler and Goldberg, 2017). In contrast, our zero-shot customization technique requires neither training examples of documents and corresponding (customized) summaries nor specialized pre-processing to encode which tokens in the input might give rise to customization. 3 C ONDENSE-A BSTRACT Framework We propose an alternative to the E XTRACTA BSTRACT (EA) approach which enables the use of all input reviews when generating the summary. Figure 2b illustrates our proposed C ONDENSE A BSTRACT (CA) framework. In lieu of an integrated encoder-decoder, we generate summaries using two separ"
2021.eacl-main.229,W17-4912,0,0.133735,"o rely only on preselected salient reviews which we argue leads to information loss and subsequently less customizable generation. Instead, our model first condenses the source reviews into multiple dense vectors which serve as input to a decoder to generate an abstractive summary. Beyond producing more informative summaries, we demonstrate that our approach also allows to customize them. Recent conditional generation models have focused on controlling various aspects of the output such as politeness (Sennrich et al., 2016), length (Kikuchi et al., 2016), content (Fan et al., 2018), or style (Ficler and Goldberg, 2017). In contrast, our zero-shot customization technique requires neither training examples of documents and corresponding (customized) summaries nor specialized pre-processing to encode which tokens in the input might give rise to customization. 3 C ONDENSE-A BSTRACT Framework We propose an alternative to the E XTRACTA BSTRACT (EA) approach which enables the use of all input reviews when generating the summary. Figure 2b illustrates our proposed C ONDENSE A BSTRACT (CA) framework. In lieu of an integrated encoder-decoder, we generate summaries using two separate models. The C ONDENSE model return"
2021.eacl-main.229,C10-1039,0,0.587449,"ct representative segments (usually sentences) from the source text (Popescu and Etzioni, 2662 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2662–2672 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2005; Blair-Goldensohn et al., 2008; Lerman et al., 2009). Despite being less popular, abstractive approaches seem more appropriate for the task at hand as they attempt to generate summaries which are maximally informative and minimally redundant without simply rearranging passages from the original opinions (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014). General-purpose summarization approaches have recently shown promising results with end-toend models which are data-driven and take advantage of the success of sequence-to-sequence neural network architectures. Most approaches (Rush et al., 2015; See et al., 2017) encode documents and then decode the learned representations into an abstractive summary, often by attending to the source input (Bahdanau et al., 2014) and copying words from it (Vinyals et al., 2015). Under this modeling paradigm, it is no longer necessary to identify aspects and their"
2021.eacl-main.229,D14-1168,0,0.123275,") from the source text (Popescu and Etzioni, 2662 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2662–2672 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2005; Blair-Goldensohn et al., 2008; Lerman et al., 2009). Despite being less popular, abstractive approaches seem more appropriate for the task at hand as they attempt to generate summaries which are maximally informative and minimally redundant without simply rearranging passages from the original opinions (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014). General-purpose summarization approaches have recently shown promising results with end-toend models which are data-driven and take advantage of the success of sequence-to-sequence neural network architectures. Most approaches (Rush et al., 2015; See et al., 2017) encode documents and then decode the learned representations into an abstractive summary, often by attending to the source input (Bahdanau et al., 2014) and copying words from it (Vinyals et al., 2015). Under this modeling paradigm, it is no longer necessary to identify aspects and their sentiment for the opinion summarization task"
2021.eacl-main.229,D16-1140,0,0.0202138,"ith a large number of reviews, since it eliminates the need to rely only on preselected salient reviews which we argue leads to information loss and subsequently less customizable generation. Instead, our model first condenses the source reviews into multiple dense vectors which serve as input to a decoder to generate an abstractive summary. Beyond producing more informative summaries, we demonstrate that our approach also allows to customize them. Recent conditional generation models have focused on controlling various aspects of the output such as politeness (Sennrich et al., 2016), length (Kikuchi et al., 2016), content (Fan et al., 2018), or style (Ficler and Goldberg, 2017). In contrast, our zero-shot customization technique requires neither training examples of documents and corresponding (customized) summaries nor specialized pre-processing to encode which tokens in the input might give rise to customization. 3 C ONDENSE-A BSTRACT Framework We propose an alternative to the E XTRACTA BSTRACT (EA) approach which enables the use of all input reviews when generating the summary. Figure 2b illustrates our proposed C ONDENSE A BSTRACT (CA) framework. In lieu of an integrated encoder-decoder, we genera"
2021.eacl-main.229,P17-2074,0,0.0283219,"ativeness (Inf), correctness (Corr) and grammaticality (Gram). All pairwise systems differences between C ONDA S UM and other system summaries are significant, except the values marked with asterisk (*), based on a one-way ANOVA with posthoc Tukey HSD tests (p < 0.05). S UM, respectively. As an upper bound, we also included G OLD standard summaries. The study was conducted on the Amazon Mechanical Turk platform using Best-Worst Scaling (BWS; Louviere et al., 2015), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Specifically, participants were shown the movie title and basic background information (i.e., synopsis, release year, genre, director, and cast). They were also presented with three system summaries and asked to select the best and worst among them according to three criteria: Informativeness (i.e., does the summary convey opinions about specific aspects of the movie in a concise manner?), Correctness (i.e., is the information in the summary factually accurate and corresponding to the information given about the movie?), and Grammaticality (i.e., is the summary fluent and grammatical?). Exam"
2021.eacl-main.229,E09-1059,0,0.0462075,"ttery life or sound quality); (2) sentiment prediction (i.e., determining the sentiment of the extracted aspects); and (3) summary generation (i.e., presenting the identified opinions to the user). Textual summaries are created following mostly extractive methods which select representative segments (usually sentences) from the source text (Popescu and Etzioni, 2662 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2662–2672 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2005; Blair-Goldensohn et al., 2008; Lerman et al., 2009). Despite being less popular, abstractive approaches seem more appropriate for the task at hand as they attempt to generate summaries which are maximally informative and minimally redundant without simply rearranging passages from the original opinions (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014). General-purpose summarization approaches have recently shown promising results with end-toend models which are data-driven and take advantage of the success of sequence-to-sequence neural network architectures. Most approaches (Rush et al., 2015; See et al., 2017) encode documen"
2021.eacl-main.229,W04-1013,0,0.0215012,"ith hand-engineered features implements the E X TRACT model, while an attention-based sequenceto-sequence neural network is the A BSTRACT model; (h) B ERT C ENT +S2S, our implementation of an EA-based system which uses B ERT C ENT instead of R EGRESS as the E XTRACT model; and 3 Our code can be downloaded from https://github. com/rktamplayo/CondaSum. Results Automatic Evaluation We considered two evaluation metrics which are also reported in Wang and Ling (2016): METEOR (Denkowski and Lavie, 2014), a recall-oriented metric that rewards matching stems, synonyms, and paraphrases, and ROUGE-SU4 (Lin, 2004) which is calculated as the recall of unigrams and skip-bigrams up to four words. We also report F1 -scores for ROUGE1/2/L (Lin, 2004). Unigram and bigram overlap (ROUGE-1 and ROUGE-2) are a proxy for assessing informativenes while the longest common subsequence (ROUGE-L) measures fluency. Our results are presented in Table 2. Among one-pass systems, the extractive model B ERT C ENT performs the best; despite being unsupervised and extractive, it benefits from the ability of large neural language models to learn general-purpose representations. When used in EA-based systems, B ERT C ENT also i"
2021.eacl-main.229,P19-1500,1,0.931191,"and their sentiment for the opinion summarization task, as these are learned indirectly from training data (i.e., sets of opinions and their corresponding summaries). These models are usually tested on domains where the input is either one document or a small set of documents. However, the number of input reviews for each target entity tends to be very large (150 for the example in Figure 1). It is therefore practically unfeasible to train a model in an end-to-end fashion, given the memory limitations of modern hardware. As a result, current approaches (Wang and Ling, 2016; Liu et al., 2018; Liu and Lapata, 2019) sacrifice end-to-end elegance in favor of a two-stage framework which we call E XTRACT-A BSTRACT (EA): an extractive model first selects a subset of opinions and an abstractive model then generates the summary while conditioning on the extracted subset (see Figure 2a). The extractive pass unfortunately has two drawbacks. Firstly, on account of having access to only a small subset of reviews, the summaries can be less informative and inaccurate, as shown in Figure 1. And secondly, user preferences cannot be easily taken into account (e.g., a user may wish to obtain a summary focusing on the ac"
2021.eacl-main.229,I13-1065,0,0.0339652,"customization technique can effectively generate need-specific summaries. 2663 2 Related Work Most opinion summarization models follow extractive methods (see Kim et al., 2011 and Angelidis and Lapata, 2018 for overviews), with the exception of a few systems which are able to generate novel words and phrases not featured in the source text. Ganesan et al. (2010) propose a graph-based framework for generating concise opinion summaries, while Gerani et al. (2014) represent reviews as discourse trees which they aggregate to a global graph to generate a summary. Other work (Carenini et al., 2013; Mukherjee and Joshi, 2013) takes the distribution of opinions and their aspects into account so as to generate more readable summaries. Di Fabbrizio et al. (2014) present a hybrid system which uses extractive techniques to select salient quotes from the input reviews and embeds them into an abstractive summary to provide evidence for positive or negative opinions. More recent work has seen the effective application of sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014) to various abstractive summarization tasks including headline generation (Rush et al., 2015), single- (See et al., 2017; Nallapa"
2021.eacl-main.229,K16-1028,0,0.0621237,"Missing"
2021.eacl-main.229,D14-1162,0,0.0851986,"uwang/ publications.html 2666 #movies #reviews/movie #tokens/review #tokens/summary Train 2,458 100.0 23.6 23.8 Dev 536 98.0 23.5 23.6 Test 737 100.3 23.6 23.8 (i) B ERT C ENT +P T G EN, the same model as (h) but enhanced with a copy mechanism (Vinyals et al., 2015). For all extractive steps, we set k = 5, which is tuned on the development set. 5 Table 1: Dataset statistics of Rotten Tomatoes. titles during training which we replace with the original titles during inference. Training Configuration For all experiments, our model used word embeddings with 128 dimensions, pretrained using GloVe (Pennington et al., 2014). We set the dimensions of all hidden vectors to 256 and the batch size to 8. For decoding summaries, we use a length-normalized beam search with beam size of 5. We applied dropout (Srivastava et al., 2014) at a rate of 0.5. The model was trained using the Adam optimizer (Kingma and Ba, 2015) with default parameters and l2 constraint (Hinton et al., 2012) of 2. We performed early stopping based on model performance on the development set. Our model is implemented in PyTorch3 . Comparison Systems We compare our approach against two types of methods: one-pass methods and methods that use the EA"
2021.eacl-main.229,H05-1043,0,0.551816,"Missing"
2021.eacl-main.229,W00-0403,0,0.097723,"d sum over the generation probability pg (yt0 ) and the copy probability pc (yt0 ): pg (yt0 ) = softmax(Wg [st ; ct ] + bg ) σt = pc (yt0 ) = p(yt0 ) = σ(vs> st + vc> ct + vy> yt0 ) X ai i:yi0 =yt0 t σt ∗ pg (yt0 ) + (14) (15) (16) (1 − σt ) ∗ pc (yt0 ) (17) where W , v, and b are learned parameters, and t is the current timestep. Salience-biased Extracts The model presented so far has no explicit mechanism to encourage salience among reviews. We direct the decoder towards salient reviews by incorporating information from an extractive step. Specifically, we use B ERT C ENT, a centroid-based (Radev et al., 2000) document extraction method that obtains document representations by resorting to BERT (Devlin et al., 2019). B ERT C ENT can be simply described as follows. Firstly, given a review, we obtain its encoding as the 2665 average of its token encodings obtained from BERT. We then take the average of the review encodings and treat it as the centroid of the input reviews, which approximately represents the information that is considered salient. We select the top k reviews whose encodings are the nearest neigbors to the centroid. The selected reviews are concatenated into a long sequence and encoded"
2021.eacl-main.229,D15-1044,0,0.221025,"Blair-Goldensohn et al., 2008; Lerman et al., 2009). Despite being less popular, abstractive approaches seem more appropriate for the task at hand as they attempt to generate summaries which are maximally informative and minimally redundant without simply rearranging passages from the original opinions (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014). General-purpose summarization approaches have recently shown promising results with end-toend models which are data-driven and take advantage of the success of sequence-to-sequence neural network architectures. Most approaches (Rush et al., 2015; See et al., 2017) encode documents and then decode the learned representations into an abstractive summary, often by attending to the source input (Bahdanau et al., 2014) and copying words from it (Vinyals et al., 2015). Under this modeling paradigm, it is no longer necessary to identify aspects and their sentiment for the opinion summarization task, as these are learned indirectly from training data (i.e., sets of opinions and their corresponding summaries). These models are usually tested on domains where the input is either one document or a small set of documents. However, the number of"
2021.eacl-main.229,P17-1099,0,0.261402,"al., 2008; Lerman et al., 2009). Despite being less popular, abstractive approaches seem more appropriate for the task at hand as they attempt to generate summaries which are maximally informative and minimally redundant without simply rearranging passages from the original opinions (Ganesan et al., 2010; Carenini et al., 2013; Gerani et al., 2014). General-purpose summarization approaches have recently shown promising results with end-toend models which are data-driven and take advantage of the success of sequence-to-sequence neural network architectures. Most approaches (Rush et al., 2015; See et al., 2017) encode documents and then decode the learned representations into an abstractive summary, often by attending to the source input (Bahdanau et al., 2014) and copying words from it (Vinyals et al., 2015). Under this modeling paradigm, it is no longer necessary to identify aspects and their sentiment for the opinion summarization task, as these are learned indirectly from training data (i.e., sets of opinions and their corresponding summaries). These models are usually tested on domains where the input is either one document or a small set of documents. However, the number of input reviews for e"
2021.eacl-main.229,N16-1005,0,0.025993,"s better on realworld datasets with a large number of reviews, since it eliminates the need to rely only on preselected salient reviews which we argue leads to information loss and subsequently less customizable generation. Instead, our model first condenses the source reviews into multiple dense vectors which serve as input to a decoder to generate an abstractive summary. Beyond producing more informative summaries, we demonstrate that our approach also allows to customize them. Recent conditional generation models have focused on controlling various aspects of the output such as politeness (Sennrich et al., 2016), length (Kikuchi et al., 2016), content (Fan et al., 2018), or style (Ficler and Goldberg, 2017). In contrast, our zero-shot customization technique requires neither training examples of documents and corresponding (customized) summaries nor specialized pre-processing to encode which tokens in the input might give rise to customization. 3 C ONDENSE-A BSTRACT Framework We propose an alternative to the E XTRACTA BSTRACT (EA) approach which enables the use of all input reviews when generating the summary. Figure 2b illustrates our proposed C ONDENSE A BSTRACT (CA) framework. In lieu of an integr"
2021.eacl-main.229,N16-1007,0,0.222967,"no longer necessary to identify aspects and their sentiment for the opinion summarization task, as these are learned indirectly from training data (i.e., sets of opinions and their corresponding summaries). These models are usually tested on domains where the input is either one document or a small set of documents. However, the number of input reviews for each target entity tends to be very large (150 for the example in Figure 1). It is therefore practically unfeasible to train a model in an end-to-end fashion, given the memory limitations of modern hardware. As a result, current approaches (Wang and Ling, 2016; Liu et al., 2018; Liu and Lapata, 2019) sacrifice end-to-end elegance in favor of a two-stage framework which we call E XTRACT-A BSTRACT (EA): an extractive model first selects a subset of opinions and an abstractive model then generates the summary while conditioning on the extracted subset (see Figure 2a). The extractive pass unfortunately has two drawbacks. Firstly, on account of having access to only a small subset of reviews, the summaries can be less informative and inaccurate, as shown in Figure 1. And secondly, user preferences cannot be easily taken into account (e.g., a user may wi"
2021.emnlp-main.528,2020.acl-main.175,1,0.908077,"leanliness, Location, Room, and Service The staff was very friendly and helpful. The room was very clean, and the bathroom was very nice. It was a great location, right on the beach. Table 1: General and aspect-specific summaries generated by our model for a hotel from the S PACE dataset. Aspects and aspect-specific sentences are color-coded. relevant (Erkan and Radev, 2004) and should be presented in the summary. Opinion summarization is no exception, focusing on creating summaries based on opinions that are popular or redundant across reviews (Angelidis and Lapata, 2018b; Chu and Liu, 2019; Amplayo and Lapata, 2020; Bražinskas et al., 2020; Amplayo et al., 2021). Consumers oftentimes resort to review websites to inform their decision making (e.g., whether to buy a product or use a service). The proliferation of online reviews has accelerated research on opinion mining (Pang and Lee, 2008), where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sentiment analysis (Pang et al., 2002), as- one m"
2021.emnlp-main.528,Q18-1002,1,0.798873,"throom was very nice and the shower was great. Cleanliness, Location, Room, and Service The staff was very friendly and helpful. The room was very clean, and the bathroom was very nice. It was a great location, right on the beach. Table 1: General and aspect-specific summaries generated by our model for a hotel from the S PACE dataset. Aspects and aspect-specific sentences are color-coded. relevant (Erkan and Radev, 2004) and should be presented in the summary. Opinion summarization is no exception, focusing on creating summaries based on opinions that are popular or redundant across reviews (Angelidis and Lapata, 2018b; Chu and Liu, 2019; Amplayo and Lapata, 2020; Bražinskas et al., 2020; Amplayo et al., 2021). Consumers oftentimes resort to review websites to inform their decision making (e.g., whether to buy a product or use a service). The proliferation of online reviews has accelerated research on opinion mining (Pang and Lee, 2008), where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sen"
2021.emnlp-main.528,D18-1403,1,0.841721,"throom was very nice and the shower was great. Cleanliness, Location, Room, and Service The staff was very friendly and helpful. The room was very clean, and the bathroom was very nice. It was a great location, right on the beach. Table 1: General and aspect-specific summaries generated by our model for a hotel from the S PACE dataset. Aspects and aspect-specific sentences are color-coded. relevant (Erkan and Radev, 2004) and should be presented in the summary. Opinion summarization is no exception, focusing on creating summaries based on opinions that are popular or redundant across reviews (Angelidis and Lapata, 2018b; Chu and Liu, 2019; Amplayo and Lapata, 2020; Bražinskas et al., 2020; Amplayo et al., 2021). Consumers oftentimes resort to review websites to inform their decision making (e.g., whether to buy a product or use a service). The proliferation of online reviews has accelerated research on opinion mining (Pang and Lee, 2008), where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sen"
2021.emnlp-main.528,2020.acl-main.461,1,0.897154,", and Service The staff was very friendly and helpful. The room was very clean, and the bathroom was very nice. It was a great location, right on the beach. Table 1: General and aspect-specific summaries generated by our model for a hotel from the S PACE dataset. Aspects and aspect-specific sentences are color-coded. relevant (Erkan and Radev, 2004) and should be presented in the summary. Opinion summarization is no exception, focusing on creating summaries based on opinions that are popular or redundant across reviews (Angelidis and Lapata, 2018b; Chu and Liu, 2019; Amplayo and Lapata, 2020; Bražinskas et al., 2020; Amplayo et al., 2021). Consumers oftentimes resort to review websites to inform their decision making (e.g., whether to buy a product or use a service). The proliferation of online reviews has accelerated research on opinion mining (Pang and Lee, 2008), where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sentiment analysis (Pang et al., 2002), as- one might only care about the"
2021.emnlp-main.528,W14-4408,0,0.0378587,"Missing"
2021.emnlp-main.528,W18-2706,0,0.0294325,"ffectively groups opinion sentences into clusters and extracts those capturing aspect-relevant information. We employ multi-instance learning to identify aspect-bearing elements in reviews with varying degrees of granularity (e.g., words, sentences, documents) which we argue affords greater flexibility and better control of the output summaries. In doing so, we also introduce an effective method to create synthetic datasets for aspect-guided opinion summarization. Our work also relates to approaches which attempt to control summarization output based on length (Kikuchi et al., 2016), content (Fan et al., 2018), style (Cao and Wang, 2021), or textual queries (Dang, 2006). Although we focus solely on aspect, our method is general and could be used to adjust additional properties of a summary such as sentiment (e.g., positive vs. negative) or style (e.g., formal vs. colloquial). 3 Problem Formulation Let C denote a corpus of reviews about entities (e.g., products, hotels). Let Re = {r1 , r2 , ..., rN } denote a set of reviews for entity e and Ae = {a1 , a2 , ..., aM } a set of aspects that are relevant for the entity (e.g., cleanliness, location). Each review ri is a sequence of tokens {w1 , w2 , ...}"
2021.emnlp-main.528,C10-1039,0,0.0548144,"idis and Lapata, 2018b), a dataset with product reviews from multiple domains (e.g., “laptop bags”, “boots”). Automatic and human evaluation show that our model outperforms previous approaches on both tasks of general and aspect-specific summarization. We also demonstrate that it can effectively generate multi-aspect summaries based on user preferences. We make our code and data publicly available.1 2 Related Work Earlier work on opinion summarization has focused on general summarization using extractive (Hu and Liu, 2006; Kim et al., 2011; Angelidis and Lapata, 2018b) or abstractive methods (Ganesan et al., 2010; Carenini et al., 2013; Fabbrizio et al., 2014). Due to the absence of opinion summaries in review websites and the difficulty of annotating them on a large scale, more recent methods consider an unsupervised learning setting where there are only reviews available without corresponding summaries (Chu and Liu, 2019; Bražinskas et al., 2020). They make use of autoencoders (Kingma and Welling, 2014) and variants thereof to learn a review decoder through reconstruction, and use it to generate summaries conditioned on averaged representations of the inputs. A more successful approach to opinion su"
2021.emnlp-main.528,2021.ccl-1.108,0,0.0822768,"Missing"
2021.emnlp-main.528,P12-1036,0,0.046679,"se a service). The proliferation of online reviews has accelerated research on opinion mining (Pang and Lee, 2008), where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sentiment analysis (Pang et al., 2002), as- one might only care about the connectivity of a pect extraction (Hu and Liu, 2004; He et al., 2017), television product, an aspect which might be uncombinations thereof (Mukherjee and Liu, 2012; popular amongst reviews. As a result, models Pontiki et al., 2016), and notably opinion summa- that create general opinion summaries may not rization (Hu and Liu, 2006; Wang and Ling, 2016), satisfy the needs of all users, limiting their ability whose aim is to create a textual summary of opin- to make decisions. Angelidis et al. (2021) mitiions found in multiple reviews. gate this problem with an extractive approach that Text summarization models, both extractive produces both general and aspect-specific opinion (Narayan et al., 2018; Zheng and Lapata, 2019; Ca- summaries. They achieve this"
2021.emnlp-main.528,W02-1011,0,0.0411308,"019; Amplayo and Lapata, 2020; Bražinskas et al., 2020; Amplayo et al., 2021). Consumers oftentimes resort to review websites to inform their decision making (e.g., whether to buy a product or use a service). The proliferation of online reviews has accelerated research on opinion mining (Pang and Lee, 2008), where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sentiment analysis (Pang et al., 2002), as- one might only care about the connectivity of a pect extraction (Hu and Liu, 2004; He et al., 2017), television product, an aspect which might be uncombinations thereof (Mukherjee and Liu, 2012; popular amongst reviews. As a result, models Pontiki et al., 2016), and notably opinion summa- that create general opinion summaries may not rization (Hu and Liu, 2006; Wang and Ling, 2016), satisfy the needs of all users, limiting their ability whose aim is to create a textual summary of opin- to make decisions. Angelidis et al. (2021) mitiions found in multiple reviews. gate this problem with a"
2021.emnlp-main.528,N18-1158,1,0.831835,"duct, an aspect which might be uncombinations thereof (Mukherjee and Liu, 2012; popular amongst reviews. As a result, models Pontiki et al., 2016), and notably opinion summa- that create general opinion summaries may not rization (Hu and Liu, 2006; Wang and Ling, 2016), satisfy the needs of all users, limiting their ability whose aim is to create a textual summary of opin- to make decisions. Angelidis et al. (2021) mitiions found in multiple reviews. gate this problem with an extractive approach that Text summarization models, both extractive produces both general and aspect-specific opinion (Narayan et al., 2018; Zheng and Lapata, 2019; Ca- summaries. They achieve this essentially by cluschola et al., 2020) and abstractive (See et al., 2017; tering opinions through a discrete latent variable Gehrmann et al., 2018; Liu and Lapata, 2019), op- model (van den Oord et al., 2017) and extracting erate under the assumption that salient content is sentences based on popular aspects or a particular 6578 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6578–6593 c November 7–11, 2021. 2021 Association for Computational Linguistics aspect. By virtue of being extractiv"
2021.emnlp-main.528,N16-1007,0,0.0189164,"make decisions more efficiently. Opinion However, the notion of salience in reviews mining has assumed several guises in the literature largely depends on user interest. For example, such as sentiment analysis (Pang et al., 2002), as- one might only care about the connectivity of a pect extraction (Hu and Liu, 2004; He et al., 2017), television product, an aspect which might be uncombinations thereof (Mukherjee and Liu, 2012; popular amongst reviews. As a result, models Pontiki et al., 2016), and notably opinion summa- that create general opinion summaries may not rization (Hu and Liu, 2006; Wang and Ling, 2016), satisfy the needs of all users, limiting their ability whose aim is to create a textual summary of opin- to make decisions. Angelidis et al. (2021) mitiions found in multiple reviews. gate this problem with an extractive approach that Text summarization models, both extractive produces both general and aspect-specific opinion (Narayan et al., 2018; Zheng and Lapata, 2019; Ca- summaries. They achieve this essentially by cluschola et al., 2020) and abstractive (See et al., 2017; tering opinions through a discrete latent variable Gehrmann et al., 2018; Liu and Lapata, 2019), op- model (van den"
2021.emnlp-main.528,Q19-1037,1,0.907812,"g., we produce a general summary using all aspect codes, or an aspect-specific one based on a subset thereof (Section 3.3). to be controllable. We induce these controllers using a multiple instance learning (MIL) model, illustrated in Figure 1. MIL is a machine learning framework where labels are associated with groups of instances (i.e., bags), while instance labels are unobserved (Keeler and Rumelhart, 1991). The goal is then to infer labels for bags (Dietterich et al., 1997; Maron and Ratan, 1998) or jointly for instances and bags (Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015; Xu and Lapata, 2019; Angelidis and Lapata, 2018a). Our MIL model is an example of the latter variant. In our setting, documents are bags of sentences and sentences are bags of tokens. We further assume that only documents have aspect labels. Given review r with tokens {wk }, we obtain token encodings e = {ek } from a pretrained language model (PLM; Liu et al. 2019) which uses the popular Transformer architecture (Vaswani et al., 2017). We use a non-linear transformation to obtain tokenlevel aspect predictions zT : e = PLM({wk }) (1) zT = tanh(W e + b) (2) where zT ∈ RN ×M , and N and M are the number of tokens a"
2021.emnlp-main.528,P19-1628,1,0.844879,"might be uncombinations thereof (Mukherjee and Liu, 2012; popular amongst reviews. As a result, models Pontiki et al., 2016), and notably opinion summa- that create general opinion summaries may not rization (Hu and Liu, 2006; Wang and Ling, 2016), satisfy the needs of all users, limiting their ability whose aim is to create a textual summary of opin- to make decisions. Angelidis et al. (2021) mitiions found in multiple reviews. gate this problem with an extractive approach that Text summarization models, both extractive produces both general and aspect-specific opinion (Narayan et al., 2018; Zheng and Lapata, 2019; Ca- summaries. They achieve this essentially by cluschola et al., 2020) and abstractive (See et al., 2017; tering opinions through a discrete latent variable Gehrmann et al., 2018; Liu and Lapata, 2019), op- model (van den Oord et al., 2017) and extracting erate under the assumption that salient content is sentences based on popular aspects or a particular 6578 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6578–6593 c November 7–11, 2021. 2021 Association for Computational Linguistics aspect. By virtue of being extractive, their summaries can b"
2021.emnlp-main.742,2020.acl-main.554,0,0.0426258,"marisation systems for different languages) has been gaining momentum (Chi et al., 2020b; Scialom et al., 2020). While creating large-scale multi-lingual summarisation datasets has proven feasible (Straka et al., 2018; Scialom et al., 2020), at least for the news domain, cross-lingual datasets are more difficult to obtain. In contrast to monolingual summarisation, naturally occurring documents in a source language paired with summaries in different target languages are rare. For this reason, existing approaches either create large-scale synthetic data using back-translation (Zhu et al., 2019; Cao et al., 2020), translate the input documents (Ouyang et al., 2019), or build document-summary pairs from social media annotations and crowd-sourcing (Nguyen and Daumé III, 2019). Recent efforts (Ladhak et al., 2020) have been directed at the creation of a large-scale cross-lingual dataset in the domain of how-to guides. Despite being a valuable resource, how-to guides are by nature relatively short documents (391 tokens on average) and their summaries limited to brief instructional sentences (mostly commands). Given a document in a source language (e.g., French), cross-lingual summarisation aims to produce"
2021.emnlp-main.742,2020.acl-main.493,0,0.103849,"and validate it with a human study. To illustrate the utility of our dataset we report experiments with multi-lingual pretrained models in supervised, zero- and fewshot, and out-of-domain scenarios. 1 Introduction document-summary pairs. Although initial efforts have overwhelmingly focused on English, more recently, with the advent of cross-lingual representations (Ruder et al., 2019) and large pre-trained models (Devlin et al., 2019; Liu et al., 2020), research on multi-lingual summarisation (i.e., building monolingual summarisation systems for different languages) has been gaining momentum (Chi et al., 2020b; Scialom et al., 2020). While creating large-scale multi-lingual summarisation datasets has proven feasible (Straka et al., 2018; Scialom et al., 2020), at least for the news domain, cross-lingual datasets are more difficult to obtain. In contrast to monolingual summarisation, naturally occurring documents in a source language paired with summaries in different target languages are rare. For this reason, existing approaches either create large-scale synthetic data using back-translation (Zhu et al., 2019; Cao et al., 2020), translate the input documents (Ouyang et al., 2019), or build docume"
2021.emnlp-main.742,D18-1217,0,0.0221954,"nstances SX→en . We perform a single outer loop iteration and instead of taking a copy of the (meta) parameters and updating them after the inner loop, we combine the support set with a monolingual sample of similar size. We call this method light-weight First Order MAML (LF-MAML). We also observe that in a real-world scenario, in addition to the small set with cross-lingual examples SX→en , there may exist documents in the source language DocX without corresponding summaries in English. To further train the model with additional unlabelled data, we apply a Cross-View Training technique (CVT; Clark et al. 2018). We exploit the fact that our fine-tuning does not start from scratch but rather from a pre-trained model which already generates output sequences of at least minimal quality. We augment the set of document summary pairs x, y in SX→en with instances x ˆ, yˆ where yˆ is generated by the current model and x ˆ is a different view of x. We cheaply create different views from input x by taking different layers from the encoder. 4 Experimental Setup Datasets and Splits We work with the Dde→en , Df r→en , and Dcs→en directions of our XWikis corpus (i.e., first column in Table 1) and evaluate model p"
2021.emnlp-main.742,N18-1065,0,0.0343978,"Missing"
2021.emnlp-main.742,2020.findings-emnlp.360,0,0.493828,"traka et al., 2018; Scialom et al., 2020), at least for the news domain, cross-lingual datasets are more difficult to obtain. In contrast to monolingual summarisation, naturally occurring documents in a source language paired with summaries in different target languages are rare. For this reason, existing approaches either create large-scale synthetic data using back-translation (Zhu et al., 2019; Cao et al., 2020), translate the input documents (Ouyang et al., 2019), or build document-summary pairs from social media annotations and crowd-sourcing (Nguyen and Daumé III, 2019). Recent efforts (Ladhak et al., 2020) have been directed at the creation of a large-scale cross-lingual dataset in the domain of how-to guides. Despite being a valuable resource, how-to guides are by nature relatively short documents (391 tokens on average) and their summaries limited to brief instructional sentences (mostly commands). Given a document in a source language (e.g., French), cross-lingual summarisation aims to produce a summary in a different target language (e.g., English). The practical benefits of this task are twofold: it not only provides rapid access to salient content, but also enables the dissemination of re"
2021.emnlp-main.742,2020.acl-main.703,0,0.0220657,", we assume we have access to monolingual English data (Docen , Sumen ) to learn an English summariser, and we study the zero- and few-shot cross-lingual scenarios when the input to this model is in a language other than English (i.e., German, French, and Czech). We further exploit the fact that our XWikis corpus allows us to learn cross-lingual summarisation models in a fully supervised setting, and establish comparisons against models with weaker supervision signals. Our fully supervised models follow state-of-the-art approaches based on Transformers and pre-training (Liu and Lapata, 2019b; Lewis et al., 2020). We simulate zero- and few- shot scenarios by considering subsets of the available data instances. To further complement automatic evaluation, we carried out a human evaluation study to assess the quality of cross-lingual data instances (DocX , SumY ). In other words, we validate the assumption that given a pair of aligned titles tX − tY , the lead paragraph in language Y is a valid overview summary of the document body in language X. As this evaluation requires bilingual judges, we selected three language pairs, namely Dde→en , Df r→en and Dcs→en and recruited three judges per pair, i.e., bi"
2021.emnlp-main.742,W04-1013,0,0.145278,"Missing"
2021.emnlp-main.742,P19-1500,1,0.922681,"ary descriptions of goods, services, or knowledge To further push research on cross-lingual sumavailable online in foreign languages. Figure 1 marisation, we propose a large dataset with shows an example of an input document in French document-summary pairs in four languages: Czech, (left) and its summary in English and other lan- English, French, and German.1 Inspired by past reguages (right). search on monolingual descriptive summarisation Recent years have witnessed increased interest (Sauper and Barzilay, 2009; Zopf, 2018; Liu et al., in abstractive summarisation (Rush et al., 2015; 2018; Liu and Lapata, 2019a; Perez-Beltrachini Zhang et al., 2020) thanks to the popularity of neu- et al., 2019; Hayashi et al., 2021), we derive crossral network models and the availability of datasets 1 (Sandhaus, 2008; Hermann et al., 2015; Grusky Although we focus on this language subset in this paper, et al., 2018) containing hundreds of thousands of we plan to release further languages in the future. 9408 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9408–9423 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: Example source document in"
2021.emnlp-main.742,D19-1387,1,0.937545,"ary descriptions of goods, services, or knowledge To further push research on cross-lingual sumavailable online in foreign languages. Figure 1 marisation, we propose a large dataset with shows an example of an input document in French document-summary pairs in four languages: Czech, (left) and its summary in English and other lan- English, French, and German.1 Inspired by past reguages (right). search on monolingual descriptive summarisation Recent years have witnessed increased interest (Sauper and Barzilay, 2009; Zopf, 2018; Liu et al., in abstractive summarisation (Rush et al., 2015; 2018; Liu and Lapata, 2019a; Perez-Beltrachini Zhang et al., 2020) thanks to the popularity of neu- et al., 2019; Hayashi et al., 2021), we derive crossral network models and the availability of datasets 1 (Sandhaus, 2008; Hermann et al., 2015; Grusky Although we focus on this language subset in this paper, et al., 2018) containing hundreds of thousands of we plan to release further languages in the future. 9408 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9408–9423 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: Example source document in"
2021.emnlp-main.742,N19-1204,0,0.0199843,"en gaining momentum (Chi et al., 2020b; Scialom et al., 2020). While creating large-scale multi-lingual summarisation datasets has proven feasible (Straka et al., 2018; Scialom et al., 2020), at least for the news domain, cross-lingual datasets are more difficult to obtain. In contrast to monolingual summarisation, naturally occurring documents in a source language paired with summaries in different target languages are rare. For this reason, existing approaches either create large-scale synthetic data using back-translation (Zhu et al., 2019; Cao et al., 2020), translate the input documents (Ouyang et al., 2019), or build document-summary pairs from social media annotations and crowd-sourcing (Nguyen and Daumé III, 2019). Recent efforts (Ladhak et al., 2020) have been directed at the creation of a large-scale cross-lingual dataset in the domain of how-to guides. Despite being a valuable resource, how-to guides are by nature relatively short documents (391 tokens on average) and their summaries limited to brief instructional sentences (mostly commands). Given a document in a source language (e.g., French), cross-lingual summarisation aims to produce a summary in a different target language (e.g., Engl"
2021.emnlp-main.742,P19-1504,1,0.902817,"Missing"
2021.emnlp-main.742,2020.acl-demos.14,0,0.0596993,"Missing"
2021.emnlp-main.742,D15-1044,0,0.0464521,"r enabling access to summary descriptions of goods, services, or knowledge To further push research on cross-lingual sumavailable online in foreign languages. Figure 1 marisation, we propose a large dataset with shows an example of an input document in French document-summary pairs in four languages: Czech, (left) and its summary in English and other lan- English, French, and German.1 Inspired by past reguages (right). search on monolingual descriptive summarisation Recent years have witnessed increased interest (Sauper and Barzilay, 2009; Zopf, 2018; Liu et al., in abstractive summarisation (Rush et al., 2015; 2018; Liu and Lapata, 2019a; Perez-Beltrachini Zhang et al., 2020) thanks to the popularity of neu- et al., 2019; Hayashi et al., 2021), we derive crossral network models and the availability of datasets 1 (Sandhaus, 2008; Hermann et al., 2015; Grusky Although we focus on this language subset in this paper, et al., 2018) containing hundreds of thousands of we plan to release further languages in the future. 9408 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9408–9423 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1"
2021.emnlp-main.742,P09-1024,0,0.0261585,"articles from French or German newspapers to non-French or non-German speakers; or enabling access to summary descriptions of goods, services, or knowledge To further push research on cross-lingual sumavailable online in foreign languages. Figure 1 marisation, we propose a large dataset with shows an example of an input document in French document-summary pairs in four languages: Czech, (left) and its summary in English and other lan- English, French, and German.1 Inspired by past reguages (right). search on monolingual descriptive summarisation Recent years have witnessed increased interest (Sauper and Barzilay, 2009; Zopf, 2018; Liu et al., in abstractive summarisation (Rush et al., 2015; 2018; Liu and Lapata, 2019a; Perez-Beltrachini Zhang et al., 2020) thanks to the popularity of neu- et al., 2019; Hayashi et al., 2021), we derive crossral network models and the availability of datasets 1 (Sandhaus, 2008; Hermann et al., 2015; Grusky Although we focus on this language subset in this paper, et al., 2018) containing hundreds of thousands of we plan to release further languages in the future. 9408 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9408–9423 c Nov"
2021.emnlp-main.743,P18-1063,0,0.0193774,"spark of interest in unsupervised abstractive opinion summarization. Such models include M EAN S UM (Chu and Liu, 2019), C OPYCAT (Bražinskas et al., 2020b), D E An alternative to review subsets selection are NOISE S UM (Amplayo and Lapata, 2020), O PIN more memory and computationally efficient attenION D IGEST (Suhara et al., 2020), and C ONDA tion mechanisms (Beltagy et al., 2020; Pasunuru S UM (Amplayo and Lapata, 2021b). et al., 2021). However, it is unclear what relationOur work is related to the extractive-abstractive ship exists between attention weights and model summarization model (Chen and Bansal, 2018) that outputs (Jain and Wallace, 2019), thus, making it selects salient sentences from an input document harder to offer evidence for generated summaries. using reinforcement learning. They assume one-to- In our case, the summarizer relies only on a seone mapping between extracted and summary sen- lected subset and generates summaries faithful to 9431 its content. In general, in news summarization, which is a more mature branch, large datasets are commonly obtained from online resources (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019). The"
2021.emnlp-main.743,W14-4408,0,0.0606172,"Missing"
2021.emnlp-main.743,P19-1102,0,0.0192641,"el (Chen and Bansal, 2018) that outputs (Jain and Wallace, 2019), thus, making it selects salient sentences from an input document harder to offer evidence for generated summaries. using reinforcement learning. They assume one-to- In our case, the summarizer relies only on a seone mapping between extracted and summary sen- lected subset and generates summaries faithful to 9431 its content. In general, in news summarization, which is a more mature branch, large datasets are commonly obtained from online resources (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019). The most relevant dataset is M ULTI N EWS Fabbri et al. (2019), where journalist-written summaries are linked to multiple news articles. The most similar opinion summarization dataset S PACE (Angelidis et al., 2020) contains 1050 summaries produced for 50 hotels by crowdsourcing. 7 Conclusions In this work, we introduce the largest multidocument abstractive dataset for opinion summarization. The dataset consists of verdicts, pros and cons, written by professional writers for more than 31,000 Amazon products. Each product is linked to more than 320 customer reviews, on average. As standard en"
2021.emnlp-main.743,2020.emnlp-main.751,0,0.0394514,"Missing"
2021.emnlp-main.743,W18-2706,0,0.0207669,"tails can be found in Appendix 8.4. Hardware All experiments were conducted on 4 x GeForce RTX 2080 Ti. 5 5.1 Results Automatic Evaluation The results in Table 4 suggest that the supervised models substantially outperform the unsupervised ones. Also, all supervised abstractive summarizers outperform E XT S UM, suggesting recombining information from reviews into fluent text is beneficial. Among the summarizers with the review selectors, S EL S UM yields the best results on verdicts and cons. Although, we noticed that S EL S UM generates shorter pros than R1 TOP - K, which may harm its scores (Fan et al., 2018)5 . Further, when random reviews were used both in training and testing (R AND S EL), the results are substantially lower. On the other hand, when review subsets were produced by S EL S UM and summarized by R AND S EL (marked with ‘*’), we observed a substantial increase in all the scores. This suggests the importance of deliberate review selection in test time. In general, all models yield higher scores on pros than cons, which is expected as most reviews are positive (on average 4.32/5) and it is harder for the model to find negative points in input reviews. 5.2 Content Support Generating in"
2021.emnlp-main.743,C10-1039,0,0.0583512,"estimated the mutual information (MI) (Kraskov et al., 2004; Ross, 2014) between the posterior input features and the binary decision to select a review, as in Sec. 3.3. We found that besides review-vssummary ROUGE-1 and -2 scores, the posterior uses fine-grained aspect features, and review-vs-allreviews ROUGE scores (quantifying the uniqueness of each review). See also Appendix 8.7. 6 Related Work Due to a lack of annotated data, extractive weaklysupervised opinion summarization has been the dominant paradigm. L EX R ANK (Erkan and Radev, 2004) is an unsupervised extractive model. O PINOSIS (Ganesan et al., 2010) does not use any supervision and relies on POS tags and redundancies to generate short opinions. Although, it can 5 recombine fragments of input text, it cannot genR1 TOP - K and S EL S UM generate 31.95 and 27.14 words on average, respectively. erate novel words and phrases and thus produce 9430 O RACLE R ANDOM L EX R ANK M EAN S UM C OPYCAT E XT S UM R AND S EL R AND S EL * R1 TOP - K S EL S UM R1 38.14 13.12 15.12 13.78 17.05 18.74 23.25 23.95 23.43 24.33 Verdict R2 11.76 0.82 1.84 0.93 1.78 3.01 4.75 5.16 4.94 5.29 RL 31.50 10.85 12.60 11.70 14.50 15.74 17.82 18.49 18.52 18.84 R1 37.22 14"
2021.emnlp-main.743,D18-1443,0,0.0560584,"Missing"
2021.emnlp-main.743,D14-1168,0,0.0273338,"g a differentiable loss. Also, our model is related to the unsupervised paraphrasing MARGE model (Lewis et al., 2020a), where the decoder has a modified attention mechanism accounting for the target-source document similarity. However, in their approach, the actual selection of relevant documents is performed offline via heuristics. This, in turn, makes it non-differentiable and over-reliant on the modified attention mechanism. We, however, learn the selector (posterior) jointly with summarizer, and select reviews in the online regime. coherent abstractive summaries. Other earlier approaches (Gerani et al., 2014; Di Fabbrizio et al., 2014) relied on text planners and templates, which restrict the output text. A more recent method of Angelidis and Lapata (2018) applies multiple specialized models to produce extractive summaries. More recently, there has been a spark of interest in unsupervised abstractive opinion summarization. Such models include M EAN S UM (Chu and Liu, 2019), C OPYCAT (Bražinskas et al., 2020b), D E An alternative to review subsets selection are NOISE S UM (Amplayo and Lapata, 2020), O PIN more memory and computationally efficient attenION D IGEST (Suhara et al., 2020), and C ONDA"
2021.emnlp-main.743,N18-1065,0,0.0479485,"Missing"
2021.emnlp-main.743,2020.acl-main.703,0,0.159914,"AND S EL). Here, review subsets were re-sampled at each training epoch. ROUGE-1 top-k We produced review subsets based on review-summary ROUGE-1 R scores (R1 TOP - K ) for training.3 Specifically, we computed the scores for each pair, and then selected K reviews with highest scores to form the subset. To select reviews in test time, we trained a selector as in Sec. 3.3. 4.4 Experimental details Below we briefly describe model details; more information can be found in Appendix 8.5. Summarizer We used the Transformer encoderdecoder architecture (Vaswani et al., 2017) initialized with base BART (Lewis et al., 2020b), 140M parameters in total. Reviews were independently encoded and concatenated states of product reviews were attended by the decoder to predict the summary as in Bražinskas et al. (2020a). We used ROUGE-L as the stopping criterion. Summary generation was performed via the beam search of size 5 and with 3-gram blocking (Paulus et al., 2017). Posterior For the inference network in Sec. 3.2.1, we used a simple non-linear two-layer feed-forward network with 250 hidden dimensions. The model consisted of 95k parameters. The network inputs 23 pre-computed features. For instance, ROUGE-1 and -2 sc"
2021.emnlp-main.743,2020.acl-main.45,0,0.0629,"Missing"
2021.emnlp-main.743,W04-1013,0,0.0205053,"esigned for pros and cons generation. Therefore, we used a separately trained classifier to split each summary to pros and cons. Extractive summarizer We used a pre-trained BART encoder, and 100 hidden states for 1 layer score feed-forward network with ReLU, and 0.1 dropout. The contextualizer had one layer, and the final score feed-forward had 100 hidden dimensions, 0.1 dropout, with layer normalization before logits are computed. We trained the model for 5 epochs, with 1e-05 learning rate. Automatic evaluation We separately evaluated verdicts, pros, and cons with the standard ROUGE package (Lin, 2004)4 , and report F1 scores. Human evaluation To assess content support, we randomly sampled 50 products, generated summaries, and hired 3 workers on Amazon Mechanical Turk (AMT) for each HIT. To ensure high qual3 We tried but were not able to obtain better results by turning the scores into a distribution and sampling from it, so we used the deterministic strategy in the main experiments. 4 We used a wrapper over the package https:// github.com/pltrdy/files2rouge. 9429 ity submissions, we used qualification tasks and filters. More details can be found in Appendix 8.4. Hardware All experiments we"
2021.emnlp-main.743,D19-1387,1,0.851138,"rvised abstractive summarization model which treats a summary as a structured latent state of an autoencoder trained to reconstruct reviews of a product. C OPYCAT (Bražinskas et al., 2020b) is the state-of-the-art unsupervised abstractive summarizer with hierarchical continuous latent representations to model products and individual reviews. R ANDOM: here we split all N reviews by sentences, and randomly selected 3, 7, 4 sentences for verdicts, pros, and cons, respectively. E XT S UM: we created an extractive summarizer trained on our data. First, we used the same ROUGE greedy heuristic as in Liu and Lapata (2019) to sequentially select summarizing verdict, pro, and con sentences from the full set of reviews using the actual gold summary (O RACLE). Further, we trained a model, with the same architecture as the prior in Sec. 3.3, to predict sentence classes. More details can be found in Appendix 8.2. 4.3 Alternative Review Selectors To better understand the role of review selection, we trained the same encoder-decoder summarizer as in S EL S UM but with two alternative selectors. Random reviews We trained and tested on random review subsets (R AND S EL). Here, review subsets were re-sampled at each trai"
2021.emnlp-main.743,2020.acl-main.173,0,0.0807035,". via Monte Carlo (MC). log E rˆ1:K ∼p(ˆ r1:K |r1:N ) E rˆ1:K ∼p(ˆ r1:K |r1:N ) [pθ (s|ˆ r1:K )] ≥ [log pθ (s|ˆ r1:K )] (2) Here the latent subset rˆ1:K is sampled from a prior categorical distribution agnostic of the summary. From the theoretical perspective, it can lead to a large gap between the log-likelihood and the lower bound, contributing to poor performance (Deng et al., 2018). From the practical perspective, it can result in the input reviews not covering the summary content, thus forcing the decoder in training to predict ‘novel’ content. Consequently, this leads to hallucinations (Maynez et al., 2020) in test time, as we empirically demonstrate in Sec. 5.2. 3.2 Model 3.2.1 To address the previously mentioned problems, we leverage amortized inference reducing the gap (Kingma and Welling, 2013; Cremer et al., 2018). And re-formulate the lower bound using weighted sampling as shown in Eq. 3. log view subsets selected by the approximate posterior qφ (ˆ r1:K |r1:N , s). Unlike the prior, it selects reviews relevant to the summary s, thus providing a better content coverage of the summary. Hence, it reduces the amount of ‘novel’ content the decoder needs to predict. As we empirically demonstrate"
2021.emnlp-main.743,K16-1028,0,0.0612886,"Missing"
2021.emnlp-main.743,D18-1206,1,0.837831,"odel summarization model (Chen and Bansal, 2018) that outputs (Jain and Wallace, 2019), thus, making it selects salient sentences from an input document harder to offer evidence for generated summaries. using reinforcement learning. They assume one-to- In our case, the summarizer relies only on a seone mapping between extracted and summary sen- lected subset and generates summaries faithful to 9431 its content. In general, in news summarization, which is a more mature branch, large datasets are commonly obtained from online resources (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018; Fabbri et al., 2019). The most relevant dataset is M ULTI N EWS Fabbri et al. (2019), where journalist-written summaries are linked to multiple news articles. The most similar opinion summarization dataset S PACE (Angelidis et al., 2020) contains 1050 summaries produced for 50 hotels by crowdsourcing. 7 Conclusions In this work, we introduce the largest multidocument abstractive dataset for opinion summarization. The dataset consists of verdicts, pros and cons, written by professional writers for more than 31,000 Amazon products. Each product is linked to more than 320 customer reviews, on a"
2021.emnlp-main.743,D19-1018,0,0.0488964,"Missing"
2021.emnlp-main.743,2021.naacl-main.380,0,0.0407238,"Missing"
2021.emnlp-main.743,E17-2025,0,0.0486887,"Missing"
2021.findings-emnlp.88,W17-6901,0,0.0227271,"representations, extensions to decoder attention, and downsampling examples from frequent templates. We decompose decoding in two stages where the input is first tagged with semantic symbols which are then subsequently used to predict the final meaning representation. These semantic tags are automatically induced from logical forms without any extra annotation and vary depending on the meaning representation at hand (e.g., λ-calculus, SQL). They serve the goal of injecting inductive bias for compositional generalization rather than expressing general semantic information across languages (see Abzianidze and Bos 2017 for a proposal to develop a universal semantic tagset for nonexecutable semantic parsing). Our framework can be applied to different sequence-to-sequence models, domains, and semantic formalisms. It does not require manual task-specific engineering (Herzig and Berant, 2021) and is orthogonal to data augmentation methods (Andreas, 2020; Aky) and other extensions (Oren et al., 2020) which we could also incorporate. Figure 1: We first tag natural language input x with semantic symbols (e.g., predicates) and predict tag sequence z. We generate the final semantic representation y, given x and z as"
2021.findings-emnlp.88,2020.acl-main.676,0,0.46948,"hat it could incorporate any sequenceto-sequence model as the base model and augment it with semantic tagging. We evaluate the proposed approach on querybased splits of three semantic parsing benchmarks: ATIS, G EO Q UERY, and a subset of W IKI SQL covering different semantic formalisms (λ-calculus and SQL). We report experiments with LSTMand Transformer-based models (Dong and Lapata, 2016, 2018; Vaswani et al., 2017) demonstrating that our framework improves compositional generation across datasets and model architectures. Our approach is also superior to a recent data augmentation proposal (Andreas, 2020), specifically designed to enhance compositional generalization. 2 Related Work The realization that neural sequence models perform poorly in settings requiring compositional generalization has led to several research efforts aiming to study the extent of this problem and how to handle it. For instance, recent studies have proposed benchmarks which allow to measure different aspects of compositional generalization. Lake and Baroni (2018) introduce SCAN, a grounded navigation task where a learner must translate natural language commands into a sequence of actions in a synthetic language. Bahdan"
2021.findings-emnlp.88,H94-1010,0,0.235396,"f ˆ word aligner adopted in their paper. θ = arg max log p(y|x, zˆ; θ) (19) θ For W IKI SQL, our baseline model follows the COARSE 2 FINE approach put forward in Dong 5 Experimental Setup and Lapata (2018) which is well suited to Datasets Our experiments evaluate the proposed the formulaic nature of the queries, takes the framework on compositional generalization. We table schema into account, and performs on present results on query-based splits for three par with some more sophisticated models (Mcwidely used semantic parsing benchmarks, namely Cann et al., 2018; Yu et al., 2018). They ATIS (Dahl et al., 1994), G EO Q UERY (Zelle and predict select and where SQL clauses sepaMooney, 1996), and W IKI SQL (Zhong et al., rately (all queries in W IKI SQL follow the same 2017). For G EO Q UERY (880 language queries format, i.e., &quot;SELECT agg_op agg_col where to a database of U.S. geography) and ATIS (5,410 (cond_col cond_op cond AND)...&quot;, which is a queries to a flight booking system) meaning repre- small subset of the SQL syntax). The select sentations are in λ-calculus and SQL. We adopt the clause is predicted via two independent classifiers, split released by Finegan-Dollak et al. (2018) for while the"
2021.findings-emnlp.88,P16-1004,1,0.90063,"ances to machine-interpretable meaning rep- with human language learners who are able to sysresentations such as executable queries or logi- tematically generalize to such compositions (Fodor cal forms. Sequence-to-sequence neural networks and Pylyshyn, 1988; Lake et al., 2019). (Sutskever et al., 2014) have emerged as a general Previous work (Finegan-Dollak et al., 2018) has modeling framework for semantic parsing, achiev- exposed the inability of semantic parsers to gening impressive results across different domains and eralize compositionally simply by evaluating their semantic formalisms (Dong and Lapata 2016; Jia performance on different dataset splits. Existing and Liang 2016; Iyer et al. 2017; Wang et al. 2020, datasets commonly adopt question-based splits inter alia). Despite recent success, there has been where many examples in the test set have the same mounting evidence (Finegan-Dollak et al., 2018; query templates (induced by anonymizing named Keysers et al., 2020; Herzig and Berant, 2021; Lake entities) as examples in the training. As a result, and Baroni, 2018) that these models fail at com- many of the queries in the test set are seen in trainpositional generalization, i.e, they are una"
2021.findings-emnlp.88,P18-1068,1,0.845928,"g_col where to a database of U.S. geography) and ATIS (5,410 (cond_col cond_op cond AND)...&quot;, which is a queries to a flight booking system) meaning repre- small subset of the SQL syntax). The select sentations are in λ-calculus and SQL. We adopt the clause is predicted via two independent classifiers, split released by Finegan-Dollak et al. (2018) for while the where clause is generated via a sequence SQL. We create query-based splits for λ-calculus, model with a sketch as an intermediate outcome. as we use the preprocessed versions provided in Their encoder augments question representations Dong and Lapata (2018), where natural language with table information by computing attention over 1027 table column vectors and deriving a context vector to summarize the relevant columns for each word. Our tagger uses COARSE 2 FINE’s table-aware encoder to predict tags. Our parser diverges slightly from their model: while for each word the context vector is originally computed by the attention mechanism, we replace it with the column vector specified by the corresponding tag. Configuration We implemented the base semantic parsers (LSTM and T RANSFORMER) with fairseq (Ott et al., 2019). As far as GECA is concerned,"
2021.findings-emnlp.88,P18-1033,0,0.0436694,"rnia? ” and “How many people live in the capital of Georgia? ” fails to generalize 1 Introduction to questions such as “How many people live in the Semantic parsing aims at mapping natural language capital of California?”. This is in stark contrast utterances to machine-interpretable meaning rep- with human language learners who are able to sysresentations such as executable queries or logi- tematically generalize to such compositions (Fodor cal forms. Sequence-to-sequence neural networks and Pylyshyn, 1988; Lake et al., 2019). (Sutskever et al., 2014) have emerged as a general Previous work (Finegan-Dollak et al., 2018) has modeling framework for semantic parsing, achiev- exposed the inability of semantic parsers to gening impressive results across different domains and eralize compositionally simply by evaluating their semantic formalisms (Dong and Lapata 2016; Jia performance on different dataset splits. Existing and Liang 2016; Iyer et al. 2017; Wang et al. 2020, datasets commonly adopt question-based splits inter alia). Despite recent success, there has been where many examples in the test set have the same mounting evidence (Finegan-Dollak et al., 2018; query templates (induced by anonymizing named Keys"
2021.findings-emnlp.88,W05-0602,0,0.126116,"$(*|() 3 #&quot; $(&|(, *) remain the same between train and test sets. Other work proposes data augmentation as a way of injecting a compositional inductive bias into neural sequence models. Under this protocol, synthetic examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. Recombination operations can be performed by applying rules (Andreas, 2020) or learned using a generative model (Aky). Herzig and Berant (2021) follow a more traditional approach (Zelle and Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011) and develop a spanbased parser which predicts a tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. Finally, Oren et al. (2020) improve compositional generalization with the use of contextual representations, extensions to decoder attention, and downsampling examples from frequent templates. We decompose decoding in two stages where the input is first tagged with semantic symbols which are then subse"
2021.findings-emnlp.88,2021.acl-long.74,0,0.376639,"ework for semantic parsing, achiev- exposed the inability of semantic parsers to gening impressive results across different domains and eralize compositionally simply by evaluating their semantic formalisms (Dong and Lapata 2016; Jia performance on different dataset splits. Existing and Liang 2016; Iyer et al. 2017; Wang et al. 2020, datasets commonly adopt question-based splits inter alia). Despite recent success, there has been where many examples in the test set have the same mounting evidence (Finegan-Dollak et al., 2018; query templates (induced by anonymizing named Keysers et al., 2020; Herzig and Berant, 2021; Lake entities) as examples in the training. As a result, and Baroni, 2018) that these models fail at com- many of the queries in the test set are seen in trainpositional generalization, i.e, they are unable to ing, and parsers are being evaluated for their ability 1 Our code and data can be found at https://github. to generalize to questions with different surface com/mswellhao/Semantic-Tagging. forms but the same meaning. In contrast, when 1022 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1022–1032 November 7–11, 2021. ©2021 Association for Computational Ling"
2021.findings-emnlp.88,P82-1020,0,0.772971,"Missing"
2021.findings-emnlp.88,P17-1089,0,0.106437,"resentations such as executable queries or logi- tematically generalize to such compositions (Fodor cal forms. Sequence-to-sequence neural networks and Pylyshyn, 1988; Lake et al., 2019). (Sutskever et al., 2014) have emerged as a general Previous work (Finegan-Dollak et al., 2018) has modeling framework for semantic parsing, achiev- exposed the inability of semantic parsers to gening impressive results across different domains and eralize compositionally simply by evaluating their semantic formalisms (Dong and Lapata 2016; Jia performance on different dataset splits. Existing and Liang 2016; Iyer et al. 2017; Wang et al. 2020, datasets commonly adopt question-based splits inter alia). Despite recent success, there has been where many examples in the test set have the same mounting evidence (Finegan-Dollak et al., 2018; query templates (induced by anonymizing named Keysers et al., 2020; Herzig and Berant, 2021; Lake entities) as examples in the training. As a result, and Baroni, 2018) that these models fail at com- many of the queries in the test set are seen in trainpositional generalization, i.e, they are unable to ing, and parsers are being evaluated for their ability 1 Our code and data can be"
2021.findings-emnlp.88,P16-1002,0,0.0735122,"( h i−1 , wi ) ← − ← − h i = fLSTM ( h i+1 , wi ) → − ← − hi = [ h i , h i ] (2) (3) (4) → − where hi is the concatenation of vectors h i and ← − h i , and fLSTM refers to the LSTM function. We (1) feed both hi and wi to the final output layer in 1024 order to predict tags z: p(z|x; θ) = = n Y i=1 n Y p(zi |x; θ) (5) softmax(Whi + Uwi + b) (6) i=1 W, U, and b are parameters in the output layer. 3.2 Meaning Representation Generation LSTM-based encoder-decoder models with an attention mechanism have been successfully applied to a wide range of semantic parsing benchmarks (Dong and Lapata, 2016; Jia and Liang, 2016; Iyer et al., 2017), while Transformers have been rapidly gaining popularity for various NLP tasks including semantic parsing (Wang et al., 2020; Sherborne et al., 2020). Our approach is model-agnostic in that it could be combined with any type of sequence-to-sequence model; to highlight this versatility, we present experiments with both LSTMand Transformer-based models. We first embed the predicted tag and word sequences, obtaining tag embeddings eg1 , eg2 , .., egn and word embeddings w w ew 1 , e2 , ..., en . Then, we concatenate the two types of embeddings at each time step and feed them"
2021.findings-emnlp.88,D10-1119,0,0.0494878,"ay of injecting a compositional inductive bias into neural sequence models. Under this protocol, synthetic examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. Recombination operations can be performed by applying rules (Andreas, 2020) or learned using a generative model (Aky). Herzig and Berant (2021) follow a more traditional approach (Zelle and Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011) and develop a spanbased parser which predicts a tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. Finally, Oren et al. (2020) improve compositional generalization with the use of contextual representations, extensions to decoder attention, and downsampling examples from frequent templates. We decompose decoding in two stages where the input is first tagged with semantic symbols which are then subsequently used to predict the final meaning representation. These semantic tags are automatically induced from logical"
2021.findings-emnlp.88,D11-1140,0,0.0369213,"ional inductive bias into neural sequence models. Under this protocol, synthetic examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. Recombination operations can be performed by applying rules (Andreas, 2020) or learned using a generative model (Aky). Herzig and Berant (2021) follow a more traditional approach (Zelle and Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011) and develop a spanbased parser which predicts a tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. Finally, Oren et al. (2020) improve compositional generalization with the use of contextual representations, extensions to decoder attention, and downsampling examples from frequent templates. We decompose decoding in two stages where the input is first tagged with semantic symbols which are then subsequently used to predict the final meaning representation. These semantic tags are automatically induced from logical forms without any extra ann"
2021.findings-emnlp.88,J13-2005,0,0.0218061,"ociation for Computational Linguistics: EMNLP 2021, pages 1022–1032 November 7–11, 2021. ©2021 Association for Computational Linguistics adopting a query-based split, the structure of the queries in the test set is unobserved at training time, and parsers therefore must generalize to questions with different meanings. Table 1 illustrates the difference between question- and query-based splits on G EO Q UERY (Zelle and Mooney, 1996). On the contrary, compositional generalization poses no problem for traditional semantic parsers (Zettlemoyer and Collins, 2005, 2007; Wong and Mooney, 2006, 2007; Liang et al., 2013) which typically use a (probabilistic) grammar; the latter defines the meaning of individual words and phrases and how to best combine them in order to obtain meaning representations for entire utterances. Neural semantic parsers do away with representing symbolic structure explicitly in favor of a more general approach which directly transduces the utterance into a logical form, avoiding domainspecific assumptions and grammar learning. Nonetheless, the symbolic paradigm provides two important insights that could serve as a guide in designing neural semantic parsers with better compositional g"
2021.findings-emnlp.88,2021.ccl-1.108,0,0.0373138,"Missing"
2021.findings-emnlp.88,2020.findings-emnlp.225,0,0.161477,"with other fragments that appear in at least one similar environment. Recombination operations can be performed by applying rules (Andreas, 2020) or learned using a generative model (Aky). Herzig and Berant (2021) follow a more traditional approach (Zelle and Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011) and develop a spanbased parser which predicts a tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. Finally, Oren et al. (2020) improve compositional generalization with the use of contextual representations, extensions to decoder attention, and downsampling examples from frequent templates. We decompose decoding in two stages where the input is first tagged with semantic symbols which are then subsequently used to predict the final meaning representation. These semantic tags are automatically induced from logical forms without any extra annotation and vary depending on the meaning representation at hand (e.g., λ-calculus, SQL). They serve the goal of injecting inductive bias for compositional generalization rather th"
2021.findings-emnlp.88,N19-4009,0,0.0290064,"question representations Dong and Lapata (2018), where natural language with table information by computing attention over 1027 table column vectors and deriving a context vector to summarize the relevant columns for each word. Our tagger uses COARSE 2 FINE’s table-aware encoder to predict tags. Our parser diverges slightly from their model: while for each word the context vector is originally computed by the attention mechanism, we replace it with the column vector specified by the corresponding tag. Configuration We implemented the base semantic parsers (LSTM and T RANSFORMER) with fairseq (Ott et al., 2019). As far as GECA is concerned, we have a different setting from Andreas (2020): we use the preprocessed versions provided by Dong and Lapata (2018) for ATIS and G EO Q UERY, while they report experiments on G EO Q UERY only, with different preprocessing. We used their open-sourced code to generate synthetic data for our setting in order to make experiments comparable. For COARSE 2 FINE (Dong and Lapata, 2018), we used the code released by the authors. Hyperparameters for the semantic taggers were validated on the development split of ATIS and were directly copied for G EO Q UERY because of its"
2021.findings-emnlp.88,2020.findings-emnlp.45,1,0.867923,"u et al. (2019) use a synthetic VQA task to In this paper, we devise a new decoding frame- evaluate whether models can reason about all possiwork that preserves the expressivity and generality ble object pairs after training only on a small subset. of sequence-to-sequence models while featuring They show that modular structured models are best lexicon-style alignments and disentangled informa- in terms of systematic generalization, while endtion processing. Specifically, we decompose de- to-end versions do not generalize as well. Keysers coding into two phases. Given a natural language et al. (2020) introduce a method to systematically utterance, each word is first labeled with a seman- construct benchmarks for evaluating compositional tic symbol representing its meaning via a tagger. generalization. Using Freebase as an example, they Semantic symbols are atomic units like predicates create questions which maximize compound diver(in λ-calculus) or columns (in SQL). The tagger ex- gence (e.g., combinations of entities and relations) plicitly aligns semantic symbols to tokens or token while guaranteeing that the atoms (aka the primspans in the utterance. Moreover, the prediction of itive e"
2021.findings-emnlp.88,2020.acl-main.677,0,0.172012,"as executable queries or logi- tematically generalize to such compositions (Fodor cal forms. Sequence-to-sequence neural networks and Pylyshyn, 1988; Lake et al., 2019). (Sutskever et al., 2014) have emerged as a general Previous work (Finegan-Dollak et al., 2018) has modeling framework for semantic parsing, achiev- exposed the inability of semantic parsers to gening impressive results across different domains and eralize compositionally simply by evaluating their semantic formalisms (Dong and Lapata 2016; Jia performance on different dataset splits. Existing and Liang 2016; Iyer et al. 2017; Wang et al. 2020, datasets commonly adopt question-based splits inter alia). Despite recent success, there has been where many examples in the test set have the same mounting evidence (Finegan-Dollak et al., 2018; query templates (induced by anonymizing named Keysers et al., 2020; Herzig and Berant, 2021; Lake entities) as examples in the training. As a result, and Baroni, 2018) that these models fail at com- many of the queries in the test set are seen in trainpositional generalization, i.e, they are unable to ing, and parsers are being evaluated for their ability 1 Our code and data can be found at https://"
2021.findings-emnlp.88,N06-1056,0,0.244927,"when 1022 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1022–1032 November 7–11, 2021. ©2021 Association for Computational Linguistics adopting a query-based split, the structure of the queries in the test set is unobserved at training time, and parsers therefore must generalize to questions with different meanings. Table 1 illustrates the difference between question- and query-based splits on G EO Q UERY (Zelle and Mooney, 1996). On the contrary, compositional generalization poses no problem for traditional semantic parsers (Zettlemoyer and Collins, 2005, 2007; Wong and Mooney, 2006, 2007; Liang et al., 2013) which typically use a (probabilistic) grammar; the latter defines the meaning of individual words and phrases and how to best combine them in order to obtain meaning representations for entire utterances. Neural semantic parsers do away with representing symbolic structure explicitly in favor of a more general approach which directly transduces the utterance into a logical form, avoiding domainspecific assumptions and grammar learning. Nonetheless, the symbolic paradigm provides two important insights that could serve as a guide in designing neural semantic parsers"
2021.findings-emnlp.88,P07-1121,0,0.220935,"Missing"
2021.findings-emnlp.88,N18-2093,0,0.0133516,"er instead of an off-the-shelf ˆ word aligner adopted in their paper. θ = arg max log p(y|x, zˆ; θ) (19) θ For W IKI SQL, our baseline model follows the COARSE 2 FINE approach put forward in Dong 5 Experimental Setup and Lapata (2018) which is well suited to Datasets Our experiments evaluate the proposed the formulaic nature of the queries, takes the framework on compositional generalization. We table schema into account, and performs on present results on query-based splits for three par with some more sophisticated models (Mcwidely used semantic parsing benchmarks, namely Cann et al., 2018; Yu et al., 2018). They ATIS (Dahl et al., 1994), G EO Q UERY (Zelle and predict select and where SQL clauses sepaMooney, 1996), and W IKI SQL (Zhong et al., rately (all queries in W IKI SQL follow the same 2017). For G EO Q UERY (880 language queries format, i.e., &quot;SELECT agg_op agg_col where to a database of U.S. geography) and ATIS (5,410 (cond_col cond_op cond AND)...&quot;, which is a queries to a flight booking system) meaning repre- small subset of the SQL syntax). The select sentations are in λ-calculus and SQL. We adopt the clause is predicted via two independent classifiers, split released by Finegan-Doll"
2021.findings-emnlp.88,D07-1071,0,0.0625551,"oposes data augmentation as a way of injecting a compositional inductive bias into neural sequence models. Under this protocol, synthetic examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. Recombination operations can be performed by applying rules (Andreas, 2020) or learned using a generative model (Aky). Herzig and Berant (2021) follow a more traditional approach (Zelle and Mooney, 1996; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006, 2007; Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011) and develop a spanbased parser which predicts a tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. Finally, Oren et al. (2020) improve compositional generalization with the use of contextual representations, extensions to decoder attention, and downsampling examples from frequent templates. We decompose decoding in two stages where the input is first tagged with semantic symbols which are then subsequently used to predict the final meaning representation. These semantic tags are automatic"
2021.naacl-main.219,D13-1160,0,0.0392973,"select restaurant where star_rating = 3 select restaurant where star_rating = 3 and cuisine = thai Gold Exe 7 7 7 7 7 3 3 3 Figure 1: Candidate programs for an utterance can be classified by executability (Exe); note that the gold program is always in the set of executable programs. We propose to ultilize the weak yet freely available signal of executablility for learning. ronment against which they are executed (e.g., a knowledge base, a relational database). An alternative to annotation is to collect answers (or denotations) of programs, rather than programs themselves (Liang et al., 2013; Berant et al., 2013). In this work, we focus on the more extreme setting where there are no annotations available for a large number of utterances. This setting resembles a common real-life scenario where massive numbers of user utterances can be collected when deploying a semantic parser (Iyer et al., 2017). Effectively utilizing the unlabeled data makes it possible for a semantic parser to improve over time without human involvement. Our key observation is that not all candidate programs for an utterance will be semantically valid. This implies that only some candidate programs can be executed and obtain non-em"
2021.naacl-main.219,N18-1016,0,0.0652319,"Missing"
2021.naacl-main.219,P16-1004,1,0.885665,"Missing"
2021.naacl-main.219,P17-1097,0,0.015149,"ograms. The objective to minimize consists of two parts: N M 1 X 1 X l l Lsup (xi , yi ) + λ Lunsup (xi ) J = N M i=1 j=1 grams. Specifically, they are defined as follows: Lsup (x, y) = − log p(y|x, θ) X Lunsup (x) = − log R(y)p(y|x, θ) (2) (3) y where R(y) is a binary reward function that returns 1 if y is executable and 0 otherwise. In practice, this function is implemented by running a task-specific executor, e.g., a SQL executor. Another alternative to unsupervised loss is REINFORCE (Sutton et al., 1999), i.e., maximize the expected R(y) with respect to p(y|x, θ). However, as presented in Guu et al. (2017), this objective usually underperforms MML, which is consistent with our initial experiments.2 3.2 Self-Training and Top-K MML MML in Equation (3) requires marginalizing over all executable programs which is intractable. Conventionally, we resort to beam search to explore the space of programs and collect executable ones. To illustrate, we can divide the space of programs into four parts based on whether they are executable and observed, as shown in Figure 2a. For example, programs in PSE ∪ PSN are seen in the sense that they are retrieved by beam search. Programs in PSE ∪ PUE are all executab"
2021.naacl-main.219,P17-2098,0,0.0388284,"Missing"
2021.naacl-main.219,P17-1089,0,0.0183457,"pose to ultilize the weak yet freely available signal of executablility for learning. ronment against which they are executed (e.g., a knowledge base, a relational database). An alternative to annotation is to collect answers (or denotations) of programs, rather than programs themselves (Liang et al., 2013; Berant et al., 2013). In this work, we focus on the more extreme setting where there are no annotations available for a large number of utterances. This setting resembles a common real-life scenario where massive numbers of user utterances can be collected when deploying a semantic parser (Iyer et al., 2017). Effectively utilizing the unlabeled data makes it possible for a semantic parser to improve over time without human involvement. Our key observation is that not all candidate programs for an utterance will be semantically valid. This implies that only some candidate programs can be executed and obtain non-empty execution results.1 As illustrated in Figure 1, executability is a weak signal that can differentiate between semantically valid and invalid programs. On unlabeled utterances, we can encourage a parser to only focus on executable programs ignoring non-executable ones. Moreover, the ex"
2021.naacl-main.219,P16-1002,0,0.366429,"mpty execution results.1 As illustrated in Figure 1, executability is a weak signal that can differentiate between semantically valid and invalid programs. On unlabeled utterances, we can encourage a parser to only focus on executable programs ignoring non-executable ones. Moreover, the executability of a program Semantic parsing is the task of mapping natural language (NL) utterances to meaning representations (aka programs) that can be executed against a real-world environment such as a knowledge base or a relational database. While neural sequence-to-sequence models (Dong and Lapata, 2016; Jia and Liang, 2016a) have achieved much success in this task in recent years, they usually require a large amount of labeled data (i.e., utteranceprogram pairs) for training. However, annotating utterances with programs is expensive as it 1 In the rest of this paper, we extend the meaning of ‘exerequires expert knowledge of meaning representa- cutability’, and use it to refer to the case where a program is tions (e.g., lambda calculus, SQLs) and the envi- executable and obtains non-empty results. 2747 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguisti"
2021.naacl-main.219,D16-1116,0,0.0409265,"Missing"
2021.naacl-main.219,D17-1160,0,0.0169834,"pproxima- and Lapata, 2016; Jia and Liang, 2016a) treat protions of MML for learning from executions. They grams as sequences, ignoring their internal strucare designed to complement Self-Training and Top- ture. As a result, the well-formedness of generated K MML via discouraging seen non-executable pro- programs cannot be guaranteed. Grammar-based grams and introducing sparsity. In the following decoders aim to remedy this issue. For text-to-LF sections, we will empirically show that they are parsing, we use the type-constrained decoder prosuperior to Self-Training and Top-K MML for posed by Krishnamurthy et al. (2017); for text-tosemi-supervised semantic parsing. The approxi- SQL parsing, we use an AST (abstract syntax tree) mations we proposed may also be beneficial for based decoder following Yin and Neubig (2018). learning from denotations (Liang et al., 2013; Be- Note that grammar-based decoding can only ensure rant et al., 2013) and weakly supervised question the syntactic correctness of generated programs. answering (Min et al., 2019), but we leave this to Executable programs are additionally semantically future work. correct. For example, all programs in Figure 1 are 2752 Model Lower bound Self-Trai"
2021.naacl-main.219,J13-2005,0,0.168547,"nt where cuisine > 3 select restaurant where star_rating = 3 select restaurant where star_rating = 3 and cuisine = thai Gold Exe 7 7 7 7 7 3 3 3 Figure 1: Candidate programs for an utterance can be classified by executability (Exe); note that the gold program is always in the set of executable programs. We propose to ultilize the weak yet freely available signal of executablility for learning. ronment against which they are executed (e.g., a knowledge base, a relational database). An alternative to annotation is to collect answers (or denotations) of programs, rather than programs themselves (Liang et al., 2013; Berant et al., 2013). In this work, we focus on the more extreme setting where there are no annotations available for a large number of utterances. This setting resembles a common real-life scenario where massive numbers of user utterances can be collected when deploying a semantic parser (Iyer et al., 2017). Effectively utilizing the unlabeled data makes it possible for a semantic parser to improve over time without human involvement. Our key observation is that not all candidate programs for an utterance will be semantically valid. This implies that only some candidate programs can be exec"
2021.naacl-main.219,D19-1284,0,0.0874913,"ly seen nonexecutable (PSN ) programs are discouraged. 4.3 Sparse MML Sparse MML is based on the intuition that in most cases there is only one or few correct pro4.2 Repulsion MML and Gentle MML grams among all executable programs. As menAs mentioned previously, Self-Training and Toptioned in Section 2, spurious programs that are exeK MML should be reasonable approximations in cutable, but do not reflect the semantics of an uttercases where gold programs are retrieved, i.e., they ance are harmful. One empirical evidence from preare in the seen executable subset (PSE in Figure 2a). vious work (Min et al., 2019) is that Self-Training However, if a parser is uncertain, i.e., beam search outperforms Top-K MML for weakly-supervised cannot retrieve the gold programs, exclusively exquestion answering. Hence, exploiting all seen ploiting PSE programs is undesirable. Hence, we executable programs can be sub-optimal. Followconsider ways of taking unseen executable proing recent work on sparse distributions (Martins grams (PUE in Figure 2a) into account. Since we and Astudillo, 2016; Niculae et al., 2018), we pronever directly observe unseen programs (PUE or pose to encourage sparsity of the ‘soft label’ q. P"
2021.naacl-main.219,D17-1127,0,0.0347042,"Missing"
2021.naacl-main.219,2020.acl-main.677,1,0.801936,"s defined as follows:  t+1 qsparse = SparseMaxy∈PSE log p(y|x, θ t ) t+1 occupies the middle ground beIntuitively, qsparse tween Self-Training (uses y ∗ only) and Top-K MML (uses all PSE programs). With the help of sparsity of q introduced by SparseMax, the M-step will only promote a subset of PSE programs. 5 Semantic Parsers In principle, our X-PR framework is modelagnostic, i.e., it can be coupled with any semantic parser for semi-supervised learning. In this work, we use a neural parser that achieves state-of-the-art performance across semantic parsing tasks. Specifically, we use RAT-SQL (Wang et al., 2020) which features a relation-aware encoder and a grammarbased decoder. The parser was originally developed for text-to-SQL parsing, and we adapt it to text-to-LF parsing. In this section, we briefly review the encoder and decoder of this parser. For more details, please refer to Wang et al. (2020). 5.1 Relation-Aware Encoding Relation-aware encoding is originally designed to handle schema encoding and schema linking for text-to-SQL parsing. We generalize these two notions for both text-to-LF and text-to-SQL parsing as follows: • enviroment encoding: encoding enviroments, i.e., a knowledge base c"
2021.naacl-main.33,P19-1444,0,0.386011,"zation. earlier work has primarily focused on evaluating parsers in-domain (e.g., tables or databases) and Conventional supervised learning simply asoften with the same programs as those provided sumes that source- and target-domain data origin training (Finegan-Dollak et al., 2018). A much inate from the same distribution, and as a result more challenging goal is achieving domain gener- struggles to capture this notion of domain generalization, i.e., building parsers which can be suc- alization for zero-shot semantic parsing. Previous cessfully applied to new domains and are able approaches (Guo et al., 2019b; Wang et al., 2020; to produce complex unseen programs. Achiev- Herzig and Berant, 2018) facilitate domain genering this generalization goal would, in principle, alization by incorporating inductive biases in the let users query arbitrary (semi-)structured data on model, e.g., designing linking features or functions the Web and reduce the annotation effort required which should be invariant under domain shifts. In to build multi-domain NL interfaces (e.g., Apple this work, we take a different direction and improve 366 Proceedings of the 2021 Conference of the North American Chapter of the As"
2021.naacl-main.33,D18-1190,0,0.0766582,"tables or databases) and Conventional supervised learning simply asoften with the same programs as those provided sumes that source- and target-domain data origin training (Finegan-Dollak et al., 2018). A much inate from the same distribution, and as a result more challenging goal is achieving domain gener- struggles to capture this notion of domain generalization, i.e., building parsers which can be suc- alization for zero-shot semantic parsing. Previous cessfully applied to new domains and are able approaches (Guo et al., 2019b; Wang et al., 2020; to produce complex unseen programs. Achiev- Herzig and Berant, 2018) facilitate domain genering this generalization goal would, in principle, alization by incorporating inductive biases in the let users query arbitrary (semi-)structured data on model, e.g., designing linking features or functions the Web and reduce the annotation effort required which should be invariant under domain shifts. In to build multi-domain NL interfaces (e.g., Apple this work, we take a different direction and improve 366 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366–379 June 6"
2021.naacl-main.33,2020.acl-main.398,0,0.0158909,"ing training. One line of work tries to incorporate inductive biases, e.g., domain-invariant n-gram matching features (Guo et al., 2019b; Wang et al., 2020), cross-domain alignment functions (Herzig and Berant, 2018), or auxiliary linking tasks (Chang et al., 2020) to improve schema linking. However, in the cross-lingual setting of Chinese Spider (Min et al., 2019), where questions and schemas are not in the same language, it is not obvious how to design such inductive biases like n-gram matching features. Another line of work relies on large-scale unsupervised pre-training on massive tables (Herzig et al., 2020; Yin et al., 2020) to obtain better representations for both questions and database schemas. Our work is orthogonal to these approaches and can be easily coupled with them. As an example, • We handle zero-shot semantic parsing by apwe show in Section 5 that our training procedure plying a meta-learning objective that directly can improve the performance of a parser already enoptimizes for domain generalization. hanced with n-gram matching features (Guo et al., • We propose an approximation of the meta- 2019b; Wang et al., 2020). Our work is similar in spirit to Givoli and Relearning objective"
2021.naacl-main.33,2020.findings-emnlp.438,0,0.350531,"Missing"
2021.naacl-main.33,D19-1377,0,0.0459501,"d in a question. For example, a parser would decide to select the column Status because of the word statuses in Figure 1. However, in the setting of zero-shot parsing, columns or tables might be mentioned in a question without ever being observed during training. One line of work tries to incorporate inductive biases, e.g., domain-invariant n-gram matching features (Guo et al., 2019b; Wang et al., 2020), cross-domain alignment functions (Herzig and Berant, 2018), or auxiliary linking tasks (Chang et al., 2020) to improve schema linking. However, in the cross-lingual setting of Chinese Spider (Min et al., 2019), where questions and schemas are not in the same language, it is not obvious how to design such inductive biases like n-gram matching features. Another line of work relies on large-scale unsupervised pre-training on massive tables (Herzig et al., 2020; Yin et al., 2020) to obtain better representations for both questions and database schemas. Our work is orthogonal to these approaches and can be easily coupled with them. As an example, • We handle zero-shot semantic parsing by apwe show in Section 5 that our training procedure plying a meta-learning objective that directly can improve the per"
2021.naacl-main.33,P19-1589,0,0.152758,"Missing"
2021.naacl-main.33,P15-1142,0,0.0817549,"Missing"
2021.naacl-main.33,D14-1162,0,0.0876107,"ial encoder provides initial representations, denoted as Qinit and Sinit for the question and the schema, respectively. A relation-aware transformer (RAT) module then takes the initial representations and further computes context-aware representations Qenc and Senc for the question and the schema, respectively. Finally, a decoder generates a sequence of production rules that constitute the abstract syntax tree T based on Qenc and Senc . To obtain Qinit and Sinit , the initial encoder could either be 1) LSTMs (Hochreiter and Schmidhuber, 1997) on top of pre-trained word embeddings, like GloVe (Pennington et al., 2014), or 2) pre-trained contextual embeddings like BERT (Devlin et al., 2 We re-implemented RAT-SQL, and added a component for value prediction so that our base parsers can be evaluated by execution accuracy. 370 2019). In our work, we will test the effectiveness of our method for both variants. As shown in Wang et al. (2020), the encodings Qenc and Senc , which are the output of the RAT module, heavily rely on schema-linking features. These features are extracted from a heuristic function that links question words to columns and tables based on n-gram matching, and they are readily available in t"
2021.naacl-main.33,2020.acl-demos.14,0,0.0271746,"Missing"
2021.naacl-main.33,N18-2028,0,0.034793,"Missing"
2021.naacl-main.33,2020.acl-main.742,0,0.146303,"n n-gram matching, and they are readily available in the conventional mono-lingual setting of the Spider dataset. However, we hypothesize that the parser’s over-reliance on these features is specific to Spider, where annotators were shown the database schema and asked to formulate queries. As a result, they were prone to re-using terms from the schema verbatim in their questions. This would not be the case in a real-world application where users are unfamiliar with the structure of the underlying database and free to use arbitrary terms which would not necessarily match column or table names (Suhr et al., 2020). Hence, we will also evaluate our parser in the cross-lingual setting where Q and S are not in the same language, and such features would not be available. 5 Experiments To evaluate DG-MAML, we integrate it with a base parser and test it on zero-shot text-to-SQL tasks. By designing an in-domain benchmark, we also show that the out-of-domain improvement does not come at the cost of in-domain performance. We also present some analysis to show how DGMAML affects domain generalization. 5.1 Datasets and Metrics We evaluate DG-MAML on two zero-shot textto-SQL benchmarks, namely, (English) Spider (Y"
2021.naacl-main.33,W00-1317,0,0.491671,"Missing"
2021.naacl-main.33,2020.acl-main.677,1,0.36471,"k has primarily focused on evaluating parsers in-domain (e.g., tables or databases) and Conventional supervised learning simply asoften with the same programs as those provided sumes that source- and target-domain data origin training (Finegan-Dollak et al., 2018). A much inate from the same distribution, and as a result more challenging goal is achieving domain gener- struggles to capture this notion of domain generalization, i.e., building parsers which can be suc- alization for zero-shot semantic parsing. Previous cessfully applied to new domains and are able approaches (Guo et al., 2019b; Wang et al., 2020; to produce complex unseen programs. Achiev- Herzig and Berant, 2018) facilitate domain genering this generalization goal would, in principle, alization by incorporating inductive biases in the let users query arbitrary (semi-)structured data on model, e.g., designing linking features or functions the Web and reduce the annotation effort required which should be invariant under domain shifts. In to build multi-domain NL interfaces (e.g., Apple this work, we take a different direction and improve 366 Proceedings of the 2021 Conference of the North American Chapter of the Association for Comput"
2021.naacl-main.33,P15-1129,0,0.0624881,"Missing"
2021.naacl-main.33,D18-2002,0,0.0544053,"n be computed as: ∇θ Lτ (θ) =∇θ θ 0 ∇θ0 LBt (θ 0 ) + ∇θ LBs (θ)  = I − α∇2θ LBs (θ) ∇θ0 LBt (θ 0 ) (5) + ∇θ LBs (θ) Semantic Parser In general, DG-MAML is model-agnostic and can be coupled with any semantic parser to improve its domain generalization. In this work, we use a base parser that is based on RAT-SQL (Wang et al., 2020), which currently achieves state-of-theart performance on Spider.2 Formally, RAT-SQL takes as input question Q and schema S of its corresponding database. Then it produces a program which is represented as an abstract syntax tree T in the context-free grammar of SQL (Yin and Neubig, 2018). RAT-SQL adopts the encoder-decoder framework for text-to-SQL parsing. It has three components: an initial encoder, a transformer-based encoder and an LSTM-based decoder. The initial encoder provides initial representations, denoted as Qinit and Sinit for the question and the schema, respectively. A relation-aware transformer (RAT) module then takes the initial representations and further computes context-aware representations Qenc and Senc for the question and the schema, respectively. Finally, a decoder generates a sequence of production rules that constitute the abstract syntax tree T base"
2021.naacl-main.33,2020.acl-main.745,0,0.0107787,"e of work tries to incorporate inductive biases, e.g., domain-invariant n-gram matching features (Guo et al., 2019b; Wang et al., 2020), cross-domain alignment functions (Herzig and Berant, 2018), or auxiliary linking tasks (Chang et al., 2020) to improve schema linking. However, in the cross-lingual setting of Chinese Spider (Min et al., 2019), where questions and schemas are not in the same language, it is not obvious how to design such inductive biases like n-gram matching features. Another line of work relies on large-scale unsupervised pre-training on massive tables (Herzig et al., 2020; Yin et al., 2020) to obtain better representations for both questions and database schemas. Our work is orthogonal to these approaches and can be easily coupled with them. As an example, • We handle zero-shot semantic parsing by apwe show in Section 5 that our training procedure plying a meta-learning objective that directly can improve the performance of a parser already enoptimizes for domain generalization. hanced with n-gram matching features (Guo et al., • We propose an approximation of the meta- 2019b; Wang et al., 2020). Our work is similar in spirit to Givoli and Relearning objective that is more effic"
2021.naacl-main.33,D18-1193,0,0.0964776,"). Hence, we will also evaluate our parser in the cross-lingual setting where Q and S are not in the same language, and such features would not be available. 5 Experiments To evaluate DG-MAML, we integrate it with a base parser and test it on zero-shot text-to-SQL tasks. By designing an in-domain benchmark, we also show that the out-of-domain improvement does not come at the cost of in-domain performance. We also present some analysis to show how DGMAML affects domain generalization. 5.1 Datasets and Metrics We evaluate DG-MAML on two zero-shot textto-SQL benchmarks, namely, (English) Spider (Yu et al., 2018b) and Chinese Spider (Min et al., 2019). Chinese Spider is a Chinese version of Spider that translates all NL questions from English to Chinese and keeps the original English database. It introduces the additional challenge of encoding crosslingual correspondences between Chinese and English.3 In both datasets, we report exact set match accuracy, following Yu et al. (2018b). We also report execution accuracy in the Spider dataset. schema-linking features (as mentioned in Section 4) and pre-trained emebddings such as BERT. To show that our method can still achieve additional improvements, we c"
2021.naacl-main.33,D18-1425,0,0.0902285,"Missing"
2021.naacl-main.33,2020.emnlp-main.558,0,0.318575,"Missing"
2021.naacl-main.35,P14-1041,0,0.589919,") b1 b9 : x 2 , b2 : e 9 , b2 : x3 , b8 : t4 b2 b9 : Pred(x2 , male.n.02) b2 : Pred(e1 , play.v.03) b2 : Agent(e9 , x2 ) b2 : Theme(e9 , x3 ) b2 : Pred(x3 , piano.n.01) b8 : Pred(t1 , now.n.01) b2 : temp after(e9 , t1 ) (b) b3 b3 :  : b4 : x1 , b4 : e1 b4 b4 : Named(x1 , “tom”) b4 : Pred(e1 , stop.v.05) b4 : Agent(e1 , x1 ) b2 : Pred(x2 , male.n.02) b4 : Patient(e1 , x2 ) b4 : temp before(e1 , e9 ) CONTRAST(b2 , b3 ) Figure 2: DRS from Figure 1 with (a) shuffled conditions and (b) different variable names. to generate text from DRSs have been few and far between (however see Basile 2015 and Narayan and Gardent 2014 for notable exceptions). This is primarily due to two properties of DRS-based semantic representations which render generation from them challenging. Firstly, DRS conditions are unordered representing a set (rather than a list).2 A hypothetical generator would have to produce the same output text for any DRSs which convey the same meaning but appear different due to their conditions having a different order (see Figures 1 and 2a which are otherwise identical but the order of conditions in boxes b1 and b4 varies). The second challenge concerns variables and their prominent status in DRSs. Vari"
2021.naacl-main.35,P17-1014,0,0.0463635,"odels which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of t"
2021.naacl-main.35,P03-1069,1,0.608649,"n, and scope marked in brown. first performs sentence splitting and deletion operations over DRSs and then uses a phrase-based machine translation model for surface realization. Our work is closest to Basile (2015); we share the same goal of generating from DRSs, however, our model is trained end-to-end and can perform long-form generation for documents and sentences alike. We also adopt an ordering component, but we order DRS conditions rather than lexical items, and propose a model capable of inferring a global order. There has been long-standing interest in information ordering within NLP (Lapata, 2003; Abend et al., 2015; Chen et al., 2016; Gong et al., 2016; Logeswaran et al., 2018; Cui et al., 2018; Yin et al., 2019; Honovich et al., 2020). Our innovation lies in conceptualizing ordering as a graph scoring task which can be further realized with graph neural network models (Wu et al., 2020). 5 Conclusions the generation task. We have introduced a novel sibling treeLSTM for encoding DRSs rendered as trees and shown it is particularly suited to trees with wide branches. We have experimentally demonstrated that our encoder coupled with a graph-based condition ordering model outperforms stro"
2021.naacl-main.35,P18-1040,1,0.501651,"Pred(E0 , step.v.01) B0 : Agent(E0 , X0 ) B−2 : Pred(X−2 , male.n.01) B0 : Patient(E0 , X−2 ) B0 : temp before(E0 , E−1 ) Figure 4: DRSs with relative variables. merging variables in the top layer with variables in the bottom layer via introducing special conditions. We collect variables in top layers of DRS boxes to construct a dictionary d = {v : b}, where v denotes a variable and b is a presupposition box label (e.g., x1 : b1 ). We then move variables 2.1 DRS-to-Tree Conversion from the top to the bottom layer by expressing them as special conditions b : Ref(v) and placing The algorithm of Liu et al. (2018) renders DRSs them before conditions on variable v. For example, in a tree-style format. It constructs trees based on DRS conditions in the bottom box layers, with- b6 : x1 in Figure 1 becomes special condition b6 : Ref(x1 ) and is placed before condition b6 : Pred(x1 , out considering variables in the top layer. This male.n.02) in Figure 3(a). results in oversimplified semantic representations Once top variables have been rewritten as speand information loss (e.g., presuppositions cannot cial conditions, the resulting DRSs are converted be handled). We improve upon their approach by into tree"
2021.naacl-main.35,P19-1629,1,0.791072,"Boxes b1 and b2 are DRSs, the top layers contain variables (e.g., x1 , x2 ) indicating discourse referents and the bottom layers contain conditions (e.g., Named(x3 , “tom”)) representing information about discourse referents. Variables and conditions have pointers (denoted by b in the figure) pointing to the boxes where they should be interpreted.1 Predicates are disambiguated to their Wordnet (Fellbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et"
2021.naacl-main.35,W19-1203,1,0.826837,"Boxes b1 and b2 are DRSs, the top layers contain variables (e.g., x1 , x2 ) indicating discourse referents and the bottom layers contain conditions (e.g., Named(x3 , “tom”)) representing information about discourse referents. Variables and conditions have pointers (denoted by b in the figure) pointing to the boxes where they should be interpreted.1 Predicates are disambiguated to their Wordnet (Fellbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et"
2021.naacl-main.35,2020.acl-main.416,1,0.776526,"s a key role in the generation: both Seq and Sibling models improve when ordering of conditions is explicitly incorporated (either with Counting or GraphOrder). We observe that the combination of Sibling with GraphOrder achieves the best results (58.73 BLEU). Table 5 presents our results on the test set. We compare our Sibling encoder against a sequential one. Both models are interfaced with GraphOrder. We also compare to a previous graph-to-text model (Song et al., 2018; Damonte and Cohen, 2019) which has been used for generating from AMRs. We converted DRSs to graphs following the method of Liu et al. (2020); graphs were encoded with a GCRN (Seo et al., 2018) and decoded with an LSTM. As can be seen, Sibling+GraphOrder outperforms all comparison systems achieving a BLEU of 59.26. However, compared to ideal-world generation (see Table 3) there is still considerable room for improvement. somewhat problematic in our case as it merely calculates word overlap between generated and goldstandard text without assessing whether model output is faithful to the semantics of the input (i.e., the DRS meaning representations). To this effect, we present examples of text generated by our model, demonstrating ho"
2021.naacl-main.35,D11-1149,0,0.0397722,"y.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositi"
2021.naacl-main.35,D19-1314,0,0.0943767,"t al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the context of the two-sentence discourse. 397 Proceedings of the 2021"
2021.naacl-main.35,P17-1099,0,0.046218,"d generation (dev set); improvements compared to Seq shown in parentheses. 3.2 [h0 , h1 , ..., hn−1 ] = BiLSTM([x0 , x1 , ..., xn ]) In addition, we included various models with treebased encoders: ChildSum, is the bidirectional childsum-treeLSTM encoder of Tai et al. (2015); it operates over right-branch binarized trees; Nary, is the bidirectional Nary-TreeLSTM of Tai et al. (2015), again over right-branch binarized trees;5 and Sibling is our bidirectional sibling-TreeLSTM. All models were equipped with the same LSTM decoder, global attention (Bahdanau et al., 2015), and the copy strategy of See et al. (2017). The embedding dimension was 300 and the hidden dimension 512. All encoders and decoders have 2 layers. The detailed settings are shown in BLEU Seq+Naive Seq+Random Seq+Counting Seq+GraphOrder 4.61 24.34 (16.77) 45.17 55.57 Sibling+Naive Sibling+Random Sibling+Counting Sibling+GraphOrder 6.98 43.43 (0.26) 49.54 58.73 Table 4: Real-world generation (dev set). For Random, we report average results after shuffling 5 times (variance shown in parentheses). Models BLEU Parameters Graph Seq+GraphOrder Sibling+GraphOrder 45.72 55.28 59.26 30.1M 32.4M + 6.1M 34.5M + 6.1M Ideal-World Generation Models"
2021.naacl-main.35,P18-1150,0,0.441532,"ext in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the cont"
2021.naacl-main.35,P15-1150,0,0.0972604,"Missing"
2021.naacl-main.35,Q18-1043,0,0.127145,"Missing"
2021.naacl-main.35,W19-0504,0,0.040473,"Missing"
2021.naacl-main.35,D18-1112,0,0.0212898,"(P (x) → Q(x))). 4 Related Work Much previous work has focused on text generation from formal representations of meaning focusing exclusively on isolated sentences or queries. The literature offers a collection of approaches to generating from AMRs most of which employ neural models and structured encoders (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020). Other work generates text from structured query language (SQL) adopting either sequence-to-sequence (Iyer et al., 2016) or graph-to-sequence models (Xu et al., 2018). Basile (2015) was the first to attempt generation from DRT-based meaning representations. He 3.4 Analysis proposes a pipeline system which operates over Figure 7 shows model performance on test set graphs and consists of three components: an alignagainst DRS size (i.e., the number of nodes in a ment module learns the correspondence between DRS tree). Perhaps unsurprisingly, we see that gen- surface text and DRS structure, an ordering moderation quality deteriorates with bigger DRSs (i.e., ule determines the relative position of words and with &gt;1,600 nodes). phrases in the surface form and a"
2021.naacl-main.35,D19-1548,0,0.211358,"ancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the context of the two-sentence discourse. 397 Proceedings of the 2021 Conference of the"
2021.naacl-main.35,C80-1061,0,0.616058,"preted.1 Predicates are disambiguated to their Wordnet (Fellbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Re"
2021.naacl-main.35,2020.tacl-1.2,0,0.197659,"is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carrying units In Figure 1, b6 is a presuppositional box for the interprein Discourse Representation Theory (DRT; Kamp tation of the man in the context of the two-sentence discourse. 397 Proceedings of the 2021 Conference of the North American Chapter of the Associ"
2021.naacl-main.35,D19-1098,0,0.0181705,"the encoder- fjs (Equation (3)) and fjp (Equation (4)) for its decoder framework, where an encoder is used to neighbor cell and the last child cell, respectively. encode input DRS trees and a decoder outputs a The memory of the current cell cj (Equation (5)) is sequence of words. A limitation of sequential en- updated by the gated sum of its cell input represencoders is that they only allow sequential informa- tation and the memories of its neighbor and child tion propagation without considering the structure cells. The hidden representation of current node hj of the input (Tai et al., 2015; Wang et al., 2019). In is computed with its output gate oj (Equation (6)). 400 Pred “male.n.02” a0 - of Agent of a1 temp after a0-of a0 f (b) a 1-o R∗ = arg max S COREK (R|Rset ), Pred “now.n.01” Pred “play.v.03” a0 a1 As discussed previously, DRSs at test time may exhibit an arbitrary order of conditions, which our model should be able to handle. Our solution is to to reorder conditions prior to generation by learning a latent canonical order from training data (e.g., to recover boxes b1 and b3 in Figure 1 from boxes b1 and b3 in Figure 2). More formally, given a set of conditions Rset , we obtain an optimal o"
2021.naacl-main.35,2007.mtsummit-ucnlg.4,0,0.095697,"lbaum, 1998) senses (e.g., male.n.02 and play.v.03). Although there has been considerable activity recently in developing models which analyze text in the style of DRT (van Noord et al., 2018, 2019; Liu et al., 2019a, 2018; Fancellu et al., 2019), attempts It is not uncommon for text generation systems to produce natural language output from intermediate semantic representations (Yao et al., 2012; Takase et al., 2016). The literature presents several examples of generating text from logical forms underlying various grammar formalisms (Wang, 1980; Shieber et al., 1990; Carroll and Oepen, 2005; White et al., 2007), typed lambda calculus (Lu and Ng, 2011), Abstract Meaning Representations (AMR; Flanigan et al. 2016; Konstas et al. 2017; Song et al. 2018; Beck et al. 2018; Damonte and Cohen 2019; Ribeiro et al. 2019; Zhu et al. 2019; Cai and Lam 2020; Wang et al. 2020), Discourse Representation Theory (DRT; Basile and Bos 2011; Basile 2015), and Minimal Recursion Semantics (MRS; Horvat et al. 2015; Hajdik et al. 2019). In this work, we propose neural models to generate high-quality text from semantic representations based on Discourse Representation Structures 1 (DRSs). DRSs are the basic meaning-carryin"
2021.naacl-main.56,N18-1150,0,0.0166175,"mated by matching the student’s predictions to the teacher. generator network which allows to copy words from the source text, and a coverage mechanism Let T and S denote teacher and student models, which keeps track of words that have been sum- respectively. Let fT and fS be functions of the marized. Other work develops abstractive mod- teacher and student. The models are typically neuels trained end-to-end with reinforcement learning ral networks and function f can be in principle debased on multiple encoders and hierarchical atten- fined using the output of any network layer (e.g., a tion (Celikyilmaz et al., 2018) or a coverage mech- hidden or softmax layer). Knowledge distillation anism where the decoder attends over previously methods are commonly expressed as minimizing 693 an objective function over training set X : X LKD = l(fT (xi ), fS (xi )) (1) xi ∈X where l() is a loss function that penalizes the difference between the teacher and the student. Specific instantiations of this general framework include minimizing the teacher/student difference based on output logits, intermediate hidden representations, attention maps, and derivatives of the loss to the input (Ba and Caruana, 2014; Romero et al"
2021.naacl-main.56,P19-1595,0,0.011335,"r (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins in several computer vision and language modeling tasks. Recent efforts have also focused on understanding why this happens, e.g., by observing that knowledge transferred by"
2021.naacl-main.56,N19-1423,0,0.0254685,"lthough the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani et al., 2017), pretrained by reconstructing a text corrupted with several arbitrary noising functions. Bao et al. (2020) design U NILMv2, a Transformer-based neural network pretrained as a pseudo-masked language model. Qi et al. (2020) introduce thei"
2021.naacl-main.56,P18-1195,0,0.0218311,"interpreting the teacher’s knowledge as importance weighting (Furlanello et al., 2018), by showing that early-stopping is crucial (Dong et al., 2019), and by studying how selfdistillation modifies regularization (Mobahi et al., 2020). For text summarization, we argue that selfknowledge distillation can potentially alleviate problems in conventional maximum likelihood training. Summarization models are typically trained on single reference document-summary pairs, however considering a single summary as the only correct reference during maximum likelihood training can harm model generalization (Elbayad et al., 2018) and is counter-intuitive. There can be multiple valid summaries for a source input (Harman and Over, 2004; Nenkova, 2006) and even the single reference summaries available are not entirely goldstandard due to the inherent noise in the automatic construction of large-scale summarization datasets (Kry´sci´nski et al., 2019). With self-knowledge distillation, teacher outputs provide softened distributions of the reference summaries, which can be viewed as an enrichment of the single reference setting and a reweighting of gold summaries to prevent the student from becoming overconfident in its pr"
2021.naacl-main.56,D18-1443,0,0.0393716,"ther with knowledge distillation improve model generalization and performance. We present experiments on several summarization benchmarks (Narayan et al., 2018; PerezBeltrachini et al., 2019; Hermann et al., 2015) covering single- and multi-document summarization settings as well as different types of summaries (e.g., verbose or more telegraphic). Across datasets, the proposed framework boosts the performance of pretrained and non-pretrained abstractive summarizers, achieving new state-of-the-art results. 2 2.1 Background Neural Abstractive Summarization generated words (Paulus et al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2"
2021.naacl-main.56,N18-1065,0,0.0265958,"Missing"
2021.naacl-main.56,R19-1050,0,0.0623232,"Missing"
2021.naacl-main.56,W04-1003,0,0.150298,"rly-stopping is crucial (Dong et al., 2019), and by studying how selfdistillation modifies regularization (Mobahi et al., 2020). For text summarization, we argue that selfknowledge distillation can potentially alleviate problems in conventional maximum likelihood training. Summarization models are typically trained on single reference document-summary pairs, however considering a single summary as the only correct reference during maximum likelihood training can harm model generalization (Elbayad et al., 2018) and is counter-intuitive. There can be multiple valid summaries for a source input (Harman and Over, 2004; Nenkova, 2006) and even the single reference summaries available are not entirely goldstandard due to the inherent noise in the automatic construction of large-scale summarization datasets (Kry´sci´nski et al., 2019). With self-knowledge distillation, teacher outputs provide softened distributions of the reference summaries, which can be viewed as an enrichment of the single reference setting and a reweighting of gold summaries to prevent the student from becoming overconfident in its predictions. The standard objective for an abstractive summarization model is negative log likelihood: LNLL"
2021.naacl-main.56,P84-1044,0,0.399948,"Missing"
2021.naacl-main.56,D16-1139,0,0.187887,"ination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowledge distillader it ill-suited to standard sequence-to-sequence tion transfers knowledge from a larger “teacher” training. For instance, maximum-likelihood train- network to a smaller “student” model by training ing on single reference datasets might not be opti- the student to imitate the teacher’s outputs (in admal for summarization which is subject to a great dition to learning from the training data set). In 1 “born-again networks”, (Furlanello et al., 2018) the Our code is available at https://github.com/ nlpyang/NoisySumm. teacher and student have the same neural archi692 Proceedi"
2021.naacl-main.56,D19-1051,0,0.0744586,"Missing"
2021.naacl-main.56,D16-1180,0,0.109748,"lanello et al., 2018; Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins i"
2021.naacl-main.56,2020.acl-main.703,0,0.348304,"al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani"
2021.naacl-main.56,W04-1013,0,0.0372504,"d on noisy data (+Noisy S). The second and third blocks in Table 1 include the results of pretrained models. To make comparisons fairer, we separate LARGE- (second block) from BASE-size (third block) pretrained models based on parameter size (shown within parentheses). With regard to LARGE-size models, we re5 Results port the results of three very strong summarization systems finetuned with U NI LMLARGE (Bao 5.1 Automatic Evaluation et al., 2020), BARTLARGE (Lewis et al., 2020), and We evaluated summarization quality automatically T511B (Raffel et al., 2019). Our BASE-size models using ROUGE (Lin, 2004). We report unigram and include BERTS UMBASE (Liu and Lapata, 2019), a 697 Models T RANSFORMER A BS +Noisy SKD U NI LMv2BASE +Noisy SKD CNN/DailyMail 20.8 21.4 23.7 24.8 XSum 32.7 33.6 38.7 39.9 Table 3: Factual correctness on CNN/DailyMail and XSum test set. +Noisy SKD are students trained on noisy signals and noisy data. CNN/DailyMail U NI LMv2BASE +Noisy SKD Succinct 0.47 0.53 Inform 0.40 0.60 Fluent 0.54 0.46 XSum U NI LMv2BASE +Noisy SKD Succinct 0.46 0.54 Inform 0.36 0.64 Fluent 0.53 0.47 WikiCatSum U NI LMv2BASE +Noisy SKD Company 0.62 0.38 Film 0.47 0.53 Animal 0.45 0.55 summarizer bas"
2021.naacl-main.56,2020.acl-main.537,0,0.0237099,"ls into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins in several computer vision and language modeling tasks. Recent efforts have also focused on understanding why this happens, e.g., by observing that knowledge transferred by the teacher is localized mainly in higher layers and does not affect early (feature extraction) layers much (Gotmare et al., 2019), by interpreting the teacher’s knowledge as importance weighting (Furlanello et al., 2018), by showing that early-stopping is crucial (Dong et al., 2019), and by studying how selfdistillation"
2021.naacl-main.56,P19-1441,0,0.0194071,"Hahn and Choi, 2019) also sheds light on leveraging knowledge distillation for training a highperforming student model with the same size as the teacher (see the discussion in the next section). Knowledge distillation has been also shown to improve results for various NLP tasks. Tan et al. (2019) use it to transfer knowledge from BERT to smaller models, helping them approach or exceed the quality of much larger pretrained neural networks. Aside from distilling large models into smaller ones (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019), knowledge distillation has been further used in multi-task learning, e.g., to teach a multi-task student from single-task teachers (Clark et al., 2019). 3 Self-Knowledge Distillation for Text Summarization Self-knowledge distillation refers to the special case where the teacher and student have identical neural network architectures. Surprisingly, perhaps, it has been consistently observed (Furlanello et al., 2018; Yang et al., 2019; Ahn et al., 2019; Liu et al., 2020) that students trained with self-knowledge distillation outperform their teachers by significant margins in several computer"
2021.naacl-main.56,D19-1387,1,0.813833,"ated words (Paulus et al., 2018). Gehrmann et al. (2018) follow a bottom-up approach where a content selector first determines which phrases in a source document should be part of the summary, and a copy mechanism is applied only to preselected phrases during decoding. Although the majority of summarization systems are composed of LSTM units, Narayan et al. (2018) and (PerezBeltrachini et al., 2019) propose abstractive models based on convolutional neural networks. Pretrained language models have recently emerged as a key technology for achieving impressive gains in abstractive summarization (Liu and Lapata, 2019; Lewis et al., 2020; Song et al., 2019). These models first pretrain a language model with self-supervised objectives on large corpora and then fine-tune it on summarization datasets. Liu and Lapata (2019) combine a pretrained encoder based on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder"
2021.naacl-main.56,P14-5010,0,0.00321573,"blicly released software. datasets (XSum and WikiCatSum) were created automatically following various assumptions about the correspondence of purported summaries to the source input. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points written by journalists which give a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and the dataset was pre-processed following See et al. (2017). Input documents were truncated to 512 tokens. 2018). The target summary is the lead section of a Wikipedia article, and the source input are webpages related to this article. WikiCatSum (PerezBeltrachini et al., 2019) represents three domains from the original Wikisum dataset under the assumption that these vary in terms of the topics the summaries discuss and their linguistic characteristics. Aside from the summaries, the dataset contains the input webpages whose length is truncated to the first 800 tokens. WikiCatSum contains 62"
2021.naacl-main.56,2020.acl-main.173,0,0.0184305,"datasets (Sandhaus, 2008; et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by Hermann et al., 2015; Grusky et al., 2018; Narayan viewing lead sections in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowledge distillader it il"
2021.naacl-main.56,K16-1028,0,0.028725,"am prediction. Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations 2.2 Knowledge Distillation z = [z1 , ..., zn ], and the decoder autoregressively generates the target summary y = (y1 , ..., ym ) Knowledge Distillation refers to a class of methods token-by-token, hence modeling the conditional for training a new smaller student network by learnprobability p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) and Nallapati et al. (2016) ing from a teacher network (in addition to learning were among the first to apply the neural encoder- from the training data). It is generally assumed that the teacher has been previously trained, and the padecoder architecture to text summarization. See et al. (2017) enhance this model with a pointer- rameters for the student are estimated by matching the student’s predictions to the teacher. generator network which allows to copy words from the source text, and a coverage mechanism Let T and S denote teacher and student models, which keeps track of words that have been sum- respectively. Le"
2021.naacl-main.56,D18-1206,1,0.898344,"multiple target referularize training. Furthermore, to better model ences, it is unrealistic to expect that multi-reference uncertainty during training, we introduce muldatasets can be created at scale for neural network tiple noise signals for both teacher and student models. We demonstrate experimentally on training. In fact, most popular benchmarks are colthree benchmarks that our framework boosts lated opportunistically, based on summaries which the performance of both pretrained and nononly loosely correspond to the source input. pretrained summarizers achieving state-of-theFor example, Narayan et al. (2018) create a art results.1 dataset by pairing the first sentence of a news article 1 Introduction with the rest of the document under the assumpAutomatic summarization has enjoyed renewed in- tion that the introductory sentence expresses the gist of the article. Grusky et al. (2018) pair artiterest in recent years, thanks to the popularity of cles with metadata available in HTML pages under neural network models and their ability to learn continuous representations without recourse to pre- the assumption that HTML tags (e.g., description) denote summary-like content. In other work (Liu processing"
2021.naacl-main.56,P19-1504,1,0.903175,"st of the document under the assumpAutomatic summarization has enjoyed renewed in- tion that the introductory sentence expresses the gist of the article. Grusky et al. (2018) pair artiterest in recent years, thanks to the popularity of cles with metadata available in HTML pages under neural network models and their ability to learn continuous representations without recourse to pre- the assumption that HTML tags (e.g., description) denote summary-like content. In other work (Liu processing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by Hermann et al., 2015; Grusky et al., 2018; Narayan viewing lead sections in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and"
2021.naacl-main.56,2020.findings-emnlp.217,0,0.0107355,"d on BERT (Devlin et al., 2019) with a randomly initialized decoder, demonstrating substantial gains on summarization performance. Song et al. (2019) pretrain an encoder-decoder framework to reconstruct (masked) fragments within a sentence and then fine-tune it on summarization datasets. In the same vein, Lewis et al. (2020) present BART, an encoder-decoder Transformer (Vaswani et al., 2017), pretrained by reconstructing a text corrupted with several arbitrary noising functions. Bao et al. (2020) design U NILMv2, a Transformer-based neural network pretrained as a pseudo-masked language model. Qi et al. (2020) introduce their own novel self-supervised task based on future n-gram prediction. Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations 2.2 Knowledge Distillation z = [z1 , ..., zn ], and the decoder autoregressively generates the target summary y = (y1 , ..., ym ) Knowledge Distillation refers to a class of methods token-by-token, hence modeling the conditional for training a new smaller student network by lear"
2021.naacl-main.56,D15-1044,0,0.0409884,"sk based on future n-gram prediction. Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where the encoder maps the sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations 2.2 Knowledge Distillation z = [z1 , ..., zn ], and the decoder autoregressively generates the target summary y = (y1 , ..., ym ) Knowledge Distillation refers to a class of methods token-by-token, hence modeling the conditional for training a new smaller student network by learnprobability p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) and Nallapati et al. (2016) ing from a teacher network (in addition to learning were among the first to apply the neural encoder- from the training data). It is generally assumed that the teacher has been previously trained, and the padecoder architecture to text summarization. See et al. (2017) enhance this model with a pointer- rameters for the student are estimated by matching the student’s predictions to the teacher. generator network which allows to copy words from the source text, and a coverage mechanism Let T and S denote teacher and student models, which keeps track of words that hav"
2021.naacl-main.56,P17-1099,0,0.634007,"ns in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowledge distillader it ill-suited to standard sequence-to-sequence tion transfers knowledge from a larger “teacher” training. For instance, maximum-likelihood train- network to a smaller “student” model by training in"
2021.naacl-main.56,C18-1146,0,0.018774,"lity of large-scale datasets (Sandhaus, 2008; et al., 2018; Perez-Beltrachini et al., 2019), multidocument summarization datasets are created by Hermann et al., 2015; Grusky et al., 2018; Narayan viewing lead sections in Wikipedia articles as sumet al., 2018) containing hundreds of thousands of document-summary pairs has driven the develop- maries of documents cited therein. The inherent ment of neural architectures for summarization. noise in the data collection process further hampers training with models often being prone to halluSeveral approaches have been proposed, in the vast cination (Song et al., 2018; Maynez et al., 2020), majority sequence-to-sequence models which are and struggling to identify which content units are trained in an end-to-end fashion with a maximum likelihood estimation loss (See et al., 2017; Celiky- salient (Tan et al., 2017). ilmaz et al., 2018; Paulus et al., 2018; Gehrmann In this paper, we propose to alleviate these probet al., 2018). lems by turning to knowledge distillation (Bucilu Despite promising results, there are specific char- et al., 2006; Ba and Caruana, 2014; Hinton et al., acteristics of the summarization task which ren- 2015; Kim and Rush, 2016). Knowl"
2021.naacl-main.56,P17-1108,0,0.0575179,"Missing"
C08-1009,J98-1006,0,0.802741,"g the distributional similarity. Related Work The data requirements for supervised WSD and the current paucity of suitably annotated corpora for many languages and text genres, has sparked considerable interest in unsupervised methods. These typically come in two flavors: (1) developing algorithms that assign word senses without relying on a sense-labeled corpus (Lesk, 1986; Galley and McKeown, 2003) and (2) making use of pseudolabels, i.e., labelled data that has not been specifically annotated for sense disambiguation purposes but contains some form of sense distinctions (Gale et al., 1992; Leacock et al., 1998). We briefly discuss representative examples of both approaches, with a bias to those closely related to our own work. Unsupervised Algorithms One of the first approaches to unsupervised WSD, and the foundation of many algorithms to come, was originally introduced by Lesk (1986). The method assigns a sense to a target ambiguous word by comparing the dictionary definitions of each of its senses with the words in the surrounding context. The sense whose definition has the highest overlap (i.e., words in common) with the context is assumed to be the correct one. Despite its simplicity, the algori"
C08-1009,W02-1006,0,0.0385114,"y available maximum entropy classifier (Daum´e III, 2004) with the default parameters. Table 1: Example sentence and extracted features for the word sense; X denotes the target word. or only those which are monosemous and hopefully less noisy. In all cases we used 50 neighbors, the most similar nouns to the target. 4.2 Features We used a rich feature space based on lemmas, part-of-speech (POS) tags and a variety of positional and syntactic relationships of the target word capturing both immediate local context and wider context. These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). Their use is illustrated on a sample English sentence for the target word sense in Table 1. Label Propagation The basic Label Propagation algorithm (Zhu and Ghahramani, 2002) represents labeled and unlabeled instances as nodes in an undirected graph with weighted edges. Initially only the known data nodes are labeled. The goal is to propagate labels from labeled to unlabeled points along the weighted edges. The weights are based on distance in a high-dimensional space. At each iteration, only the original labels are fixed, whereas the propagated labe"
C08-1009,P98-2127,0,0.227696,"Missing"
C08-1009,P04-1036,0,0.476528,"k (1986). The method assigns a sense to a target ambiguous word by comparing the dictionary definitions of each of its senses with the words in the surrounding context. The sense whose definition has the highest overlap (i.e., words in common) with the context is assumed to be the correct one. Despite its simplicity, the algorithm provides a good baseline for comparison. Coverage can be increased by augmenting the dictionary definition (gloss) of each sense with the glosses of related words and senses (Banerjee and Pedersen, 2003). Although most algorithms disambiguate word senses in context, McCarthy et al. (2004) propose a method that does not rely on contextual cues. Their algorithm capitalizes on the fact that the distribution of word senses is highly skewed. A large number of frequent words is often associated with one dominant sense. Indeed, current supervised methods rarely outperform the simple heuristic of choosing the most common sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. Rather than obtaining the first sense via annotating word senses manually, McCarthy et al. propose to acquire first senses automatically and use them for di"
C08-1009,P03-1058,0,0.212245,"e training data (henceforth “the first sense heuristic”), despite taking local context into account. Rather than obtaining the first sense via annotating word senses manually, McCarthy et al. propose to acquire first senses automatically and use them for disambiguation. Thus, by design, their algorithm assigns the same sense to all instances of a polysemous word. Pseudo-labels as Training Instances Gale et al. (1992) pioneered the use of parallel corpora as a source of sense-tagged data. Their key insight is that different translations of an ambiguous word can serve to distinguish its senses. Ng et al. (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. They gather examples from English-Chinese parallel corpora and use automatic word alignment as a means of obtaining a translation dictionary. Translations are next assigned to senses of English ambiguous words. English instances corresponding to these translations serve as training data. It has become common to use related words from a dictionary to learn contextual cues for WSD (Mihalcea, 2002). Perhaps the first incarnation of this idea is found in Leacock et al. (1998), who describe a system for acquiring"
C08-1009,W97-0201,0,0.161126,"Missing"
C08-1009,P05-1049,0,0.022391,"further, and gives the algorithm a global aspect. We used SemiL5 , a publicly available implementation of label propagation (all parameters were set to default values). 4.3 Supervised Classifiers One of our evaluation goals was to examine the effect of our training-data creation procedure on different types of classifiers and determine which ones are most suited for use with our method. We therefore chose three supervised classifiers (support vector machines, maximum entropy, and label propagation) which are based on different learning paradigms and have shown competitive performance in WSD (Niu et al., 2005; Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004). We summarize below their main characteristics and differences. 4.4 Comparison with State-of-the-art As an upper bound, we considered the accuracy of our classifiers when trained on the manuallylabeled Senseval data (using the same experimental settings and 5-fold crossvalidation). This can be used to estimate the expected decrease in accuracy caused solely by the use of our automatic sense labeling method. We also compared our approach to other unsupervised ones. These include McCarthy Support Vector Machines SVMs model classification a"
C08-1009,W03-1201,0,0.0309298,"tasets show that our approach yields significant improvements over state-of-the-art unsupervised methods, and is competitive with supervised ones, while eliminating the annotation cost. 1 Introduction Word sense disambiguation (WSD), the task of identifying the intended meaning (sense) of words in context, is a long-standing problem in Natural Language Processing. Sense disambiguation is often characterized as an intermediate task, which is not an end in itself, but has the potential to improve many applications. Examples include summarization (Barzilay and Elhadad, 1997), question answering (Ramakrishnan et al., 2003) and machine translation (Chan and Ng, 2007). WSD is commonly treated as a supervised classification task. Assuming we have access to data that has been hand-labeled with correct word senses, we can train a classifier to assign senses to unseen words in context. While this approach often achieves high accuracy, adequately large sense labeled data sets are unfortunately difficult to obtain. For many words, domains, languages, and sense inventories they are unavailable, and c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecomm"
C08-1009,D07-1107,0,0.0459222,"Missing"
C08-1009,C92-2070,0,0.276176,"extual cues for WSD (Mihalcea, 2002). Perhaps the first incarnation of this idea is found in Leacock et al. (1998), who describe a system for acquiring topical contexts that can be used to distinguish between senses. For each sense, related monosemous words are extracted from WordNet using the various relationship connections between sense entries (e.g., hyponymy, hypernymy). Their system then queries the Web with these related words. The contexts surrounding the relatives of a specific sense are presumed to be indicators of that sense, and used for disambiguation. A similar idea, proposed by Yarowsky (1992), is to use a thesaurus and acquire informative contexts from words in the same category as the target. Our own work uses insights gained from unsupervised methods with the aim of creating large datasets of sense-labeled instances without explicit manual coding. Unlike Ng et al. (2003) our algorithm works on monolingual corpora, which are 66 much more abundant than parallel ones, and is fully automatic. In their approach translations and their English senses must be associated manually. Similarly to McCarthy et al. (2004), we assume that words related to the target word are useful indicators o"
C08-1009,W97-0703,0,0.114257,"ental results on the Senseval-2 and Senseval-3 datasets show that our approach yields significant improvements over state-of-the-art unsupervised methods, and is competitive with supervised ones, while eliminating the annotation cost. 1 Introduction Word sense disambiguation (WSD), the task of identifying the intended meaning (sense) of words in context, is a long-standing problem in Natural Language Processing. Sense disambiguation is often characterized as an intermediate task, which is not an end in itself, but has the potential to improve many applications. Examples include summarization (Barzilay and Elhadad, 1997), question answering (Ramakrishnan et al., 2003) and machine translation (Chan and Ng, 2007). WSD is commonly treated as a supervised classification task. Assuming we have access to data that has been hand-labeled with correct word senses, we can train a classifier to assign senses to unseen words in context. While this approach often achieves high accuracy, adequately large sense labeled data sets are unfortunately difficult to obtain. For many words, domains, languages, and sense inventories they are unavailable, and c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share"
C08-1009,briscoe-carroll-2002-robust,0,0.0825931,"Missing"
C08-1009,P07-1005,0,0.0212863,"rovements over state-of-the-art unsupervised methods, and is competitive with supervised ones, while eliminating the annotation cost. 1 Introduction Word sense disambiguation (WSD), the task of identifying the intended meaning (sense) of words in context, is a long-standing problem in Natural Language Processing. Sense disambiguation is often characterized as an intermediate task, which is not an end in itself, but has the potential to improve many applications. Examples include summarization (Barzilay and Elhadad, 1997), question answering (Ramakrishnan et al., 2003) and machine translation (Chan and Ng, 2007). WSD is commonly treated as a supervised classification task. Assuming we have access to data that has been hand-labeled with correct word senses, we can train a classifier to assign senses to unseen words in context. While this approach often achieves high accuracy, adequately large sense labeled data sets are unfortunately difficult to obtain. For many words, domains, languages, and sense inventories they are unavailable, and c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights"
C08-1009,H05-1097,0,\N,Missing
C08-1009,C98-2122,0,\N,Missing
C08-1013,P05-1074,1,0.9157,"tion Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a number of existing paraphrasing methods. 2 Related Work"
C08-1013,N03-1003,0,0.555789,"orrespondences between words in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMe"
C08-1013,P01-1008,0,0.389996,"are created by annotating correspondences between words in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the ali"
C08-1013,J93-2003,0,0.0107748,"veres El mar arroja tantos cadáveres de inmigrantes ilegales ahogados playa ....73 Align .62a la .65 P rec corpses of drowned illegals get washed.14 up on .33 beaches.68 ... LB-P recision So many AlignRecall .11 .10 .46 Rel-Recall .07 .03 .01 Figure 3: Bannard and Callison-Burch (2005) extracted paraphrases by equating English phrases that share a common translation. Table 2: Summary results for scoring the different paraphrasing techniques using our proposed automatic evaluations. Where p(e1 |e2 ) is estimated by training word alignment models over the “parallel corpus” as in the IBM Models (Brown et al., 1993), and phrase translations are extracted from word alignments as in the Alignment Template Model (Och, 2002). Bannard and Callison-Burch (2005) also used techniques from statistical machine translation to identify paraphrases. Rather than drawing pairs of English sentences from a comparable corpus, Bannard and Callison-Burch (2005) used bilingual parallel corpora. They identified English paraphrases by pivoting through phrases in another language. They located foreign language translations of an English phrase, and treated the other English translations of those foreign phrases as potential par"
C08-1013,N06-1003,1,0.731127,"Missing"
C08-1013,W03-1608,0,0.602423,"from the alignments; • Report ParaMetric scores for a number of existing paraphrasing methods. 2 Related Work No consensus has been reached with respect to the proper methodology to use when evaluating paraphrase quality. This section reviews past methods for paraphrase evaluation. Researchers usually present the quality of their automatic paraphrasing technique in terms of a subjective manual evaluation. These have used a variety of criteria. For example, Barzilay and McKeown (2001) evaluated their paraphrases by asking judges whether paraphrases were “approximately conceptually equivalent.” Ibrahim et al. (2003) asked judges whether their paraphrases were “roughly interchangeable given the genre.” Bannard and Callison-Burch (2005) replaced phrases with paraphrases in a number of c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 97 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 97–104 Manchester, August 2008 3 sentences and asked judges whether the substitutions “preserved meaning and remained grammatical.” These subjective"
C08-1013,J03-1002,0,0.00444764,"gure 1. For the results reported in this paper, annotators aligned 50 groups of 10 pairs of equivalent sentences, for a total of 500 sentence pairs. These were assembled by pairing the first of the LDC translations with the other ten (i.e. 1-2, 1-3, 1-4, ..., 1-11). The choice of pairing one sentence with the others instead of doing all pairwise combinations was made simply because the latter would not seem to add much information. However, the choice of using the first translator as the key was arbitrary. Annotators corrected a set of automatic word alignments that were created using Giza++ (Och and Ney, 2003), which was trained on a total of 109,230 sentence pairs created from all pairwise combinations of the eleven translations of 993 Chinese sentences. The average amount of time spent on each of the sentence pairs was 77 seconds, with just over eleven hours spent to annotate all 500 sentence 5 ParaMetric Scores We can exploit the manually aligned data to compute scores in two different fashions. First, we can calculate how well an automatic paraphrasing technique is able to align the paraphrases in a sentence pair. Second, we can calculate the lowerbound on precision for a paraphrasing technique"
C08-1013,J04-4002,0,0.0204135,"rase. Figure 1 shows the alignments that were created between one sentence and three of its ten corresponding translations. Table 1 gives a list of non-identical words and phrases that can be paired by way of the word alignments. These are the basic paraphrases contained within the three sentence pairs. Each phrase has up to three paraphrases. The maximum number of paraphrases for a given span in each sentence is bounded by the number of equivalent sentences that it is paired with. In addition to these basic paraphrases, longer paraphrases can also be obtained using the heuristic presented in Och and Ney (2004) for extracting phrase pairs (PP) from word alignments A, between a foreign sentence f1J and an English sensome want to impeach him and others expect him to step down . there are those who propose impeaching him and those who want him to tender his resignation . some are proposing an indictment against him and some want him to leave office voluntarily . Figure 1: Pairs of English sentences were aligned by hand. Black squares indicate paraphrase correspondences. 1 99 See LDC catalog number 2002T01. some want to impeach and others expect step down some people, there are those who propose, are pr"
C08-1013,N03-1024,0,0.768729,"ords in multiple translations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a n"
C08-1013,P02-1040,0,0.116829,"erence paraphrases can be extracted from the gold standard alignments. While these sets will obviously be fragmentary, we attempt to make them more complete by aligning groups of equivalent sentences rather than only pairs. The paraphrase sets that we extract are appropriate for the particular contexts. Moreover they may potentially be used to study structural paraphrases, although we do not examine that Others evaluate paraphrases in terms of whether they improve performance on particular tasks. Callison-Burch et al. (2006b) measure improvements in translation quality in terms of Bleu score (Papineni et al., 2002) and in terms of subjective human evaluation when paraphrases are integrated into a statistical machine translation system. Lin and Pantel (2001) manually judge whether a paraphrase might be used to answer questions from the TREC question-answering track. To date, no one has used task-based evaluation to compare different paraphrasing methods. Even if such an evaluation were performed, it is unclear whether the results would hold for a different task. Because of this, we strive for a general evaluation rather than a task-specific one. Dolan et al. (2004) create a set of manual word alignments"
C08-1013,W04-3219,0,0.80361,"anslations; Introduction Paraphrasing is useful in a variety of natural language processing applications including natural language generation, question answering, multidocument summarization and machine translation evaluation. These applications require paraphrases for a wide variety of domains and language usage. Therefore building hand-crafted lexical resources such as WordNet (Miller, 1990) would be far too laborious. As such, a number of data-driven approaches to paraphrasing have been developed (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Bannard and Callison-Burch, 2005). Despite this spate of research, no objective evaluation metric has been proposed. In absence of a repeatable automatic evaluation, the quality of these paraphrasing techniques was gauged using subjective manual evaluations. Section 2 gives a survey of the various evaluation methodologies used in previous research. It has not been possible to directly compare paraphrasing • Show how phrase extraction heuristics from statistical machine translation can be used to enumerate paraphrases from the alignments; • Report ParaMetric scores for a number of existing pa"
C08-1013,P07-1058,0,0.0301292,"If a list of reference paraphrases is incomplete, then using it to calculate precision will give inaccurate numbers. Precision will be falsely low if the system produces correct paraphrases which are not in the reference list. Additionally, recall is indeterminable because there is no way of knowing how many correct paraphrases exist. There are further impediments to automatically evaluating paraphrases. Even if we were able to come up with a reasonably exhaustive list of paraphrases for a phrase, the acceptability of each paraphrase would vary depending on the context of the original phrase (Szpektor et al., 2007). While lexical and phrasal paraphrases can be evaluated by comparing them against a list of known paraphrases (perhaps customized for particular contexts), this cannot be naturally done for structural paraphrases which may transform whole sentences. We attempt to resolve these problems by having annotators indicate correspondences in pairs of equivalent sentences. Rather than having people enumerate paraphrases, we asked that they perform the simper task of aligning paraphrases. After developing these manual “gold standard alignments” we can gauge how well different automatic paraphrases are"
C08-1013,C04-1051,0,\N,Missing
C08-1013,J08-4005,1,\N,Missing
C08-1018,P05-1074,0,0.0722015,"n and Lapata (2007) (see Section 5 for details) and augment it with a larger grammar obtained from a parallel bilingual corpus. Crucially, our second grammar will not contain compression rules, just paraphrasing ones. We leave it to the model to learn which rules serve the compression objective. Our paraphrase grammar extraction method uses bilingual pivoting to learn paraphrases over syntax tree fragments, i.e., STSG rules. Pivoting treats the paraphrasing problem as a two-stage translation process. Some English text is translated to a foreign language, and then translated back into English (Bannard and Callison-Burch, 2005): X p(e0 |e) = p(e0 |f )p(f |e) (4) f where p(f |e) is the probability of translating an English string e into a foreign string f and p(e0 |f ) the probability of translating the same foreign string into some other English string e0 . We thus obtain English-English translation probabilities p(e0 |e) by marginalizing out the foreign text. Instead of using strings (Bannard and CallisonBurch, 2005), we use elementary trees on the English side, resulting in a monolingual STSG. We obtain the elementary trees and foreign strings using the GKHM algorithm (Galley et al., 2004). This takes as input a b"
C08-1018,E99-1042,0,0.214303,"is trained discriminatively. Specifically, we generalise the model of Cohn and Lapata (2007) to our abstractive task. We present a novel tree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature. We also develop a number of loss functions suited to the abstractive compression task. We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al., 1999). 2 Abstractive Compression Corpus A stumbling block to studying abstractive sentence compression is the lack of widely available corpora for training and testing. Previous work has been conducted almost exclusively on Ziff-Davis, a corpus derived automatically from document abstract pairs (Knight and Marcu, 2002), or on humanauthored corpora (Clarke, 2008). Unfortunately, none of these data sources are suited to our problem since they have been produced with a single rewriting operation, namely word deletion. Although there is a greater supply of paraphrasing corpora, such as the Multiple-Tra"
C08-1018,J07-2003,0,0.0157973,"target ngrams: Ψ(y) = X r∈y hφ(r, S(y)), λi + X hψ(m, S(y)), λi m∈T (y) (5) where m are the ngrams and ψ is a new feature function over these ngrams (we use only one ngram feature: the trigram log-probability). Sadly, the scoring function in (5) renders the chart-based search used for training and decoding intractable. In order to provide sufficient context to the chartbased algorithm, we must also store in each chart cell the n − 1 target tokens at the left and right edges of its yield. This is equivalent to using as our grammar the intersection between the original grammar and the ngram LM (Chiang, 2007), and increases the decoding complexity to an infeasible O(SRL2(n−1)V ) where L is the size of the lexicon. We adopt a popular approach in syntax-inspired machine translation to address this problem (Chiang, 2007). The idea is to use a beam-search over the intersection grammar coupled with the cubepruning heuristic. The beam limits the number of items in a given chart cell to a fixed constant, regardless of the number of possible LM contexts and non-terminal categories. Cube-pruning further limits the number of items considered for inclusion in the beam, reducing the time complexity to a more"
C08-1018,D07-1008,1,0.545564,"org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and insertion. In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). Therefore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts. In this framework, the goal is to find a sum"
C08-1018,P02-1057,0,0.0528497,"poken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 200"
C08-1018,P03-2041,0,0.0182914,"oncerns the modeling task itself. Ideally, our learning framework should handle structural mismatches and complex rewriting operations. In what follows, we first present a new corpus for abstractive compression which we created by having annotators compress sentences while rewriting them. Besides obtaining useful data for modeling purposes, we also demonstrate that abstractive compression is a meaningful task. We then present a tree-to-tree transducer capable of transforming an input parse tree into a compressed parse tree. Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. Specifically, we generalise the model of Cohn and Lapata (2007) to our abstractive task. We present a novel tree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature. We also develop a number of loss functions suited to the abstractive compression task. We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-docu"
C08-1018,N07-1023,0,0.0450448,"tive Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and insertion. In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). Therefore, in this paper we consider sentence compression from a more general perspec"
C08-1018,N04-1035,0,0.008486,"to English (Bannard and Callison-Burch, 2005): X p(e0 |e) = p(e0 |f )p(f |e) (4) f where p(f |e) is the probability of translating an English string e into a foreign string f and p(e0 |f ) the probability of translating the same foreign string into some other English string e0 . We thus obtain English-English translation probabilities p(e0 |e) by marginalizing out the foreign text. Instead of using strings (Bannard and CallisonBurch, 2005), we use elementary trees on the English side, resulting in a monolingual STSG. We obtain the elementary trees and foreign strings using the GKHM algorithm (Galley et al., 2004). This takes as input a bilingual word-aligned corpus with trees on one side, and finds the minimal set of tree fragments and their corresponding strings which is consistent with the word alignment. This process is illustrated in Figure 2 where the aligned pair on the left gives rise to the rules shown on the right. Note that the English rules and foreign strings shown include variable indices where they have been generalised. We estimate p(f |e) and p(e0 |f ) from the set of tree-to-string rules and then then pivot each tree fragment to produce STSG rules. Figure 3 illustrates the process for"
C08-1018,A00-1043,0,0.141015,"s a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. 1 Introduction Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005"
C08-1018,N06-1014,0,0.0239638,"(T), pivoted set (P) or generated from the source (S). 5 Experimental Design In this section we present our experimental setup for assessing the performance of our model. We give details on the corpora and grammars we used, model parameters and features,6 the baseline used for comparison with our approach, and explain how our system output was evaluated. Grammar Extraction Our grammar used rules extracted directly from our compression corpus (the training partition, 480 sentences) and a bilingual corpus (see Table 2 for examples). The former corpus was word-aligned using the Berkeley aligner (Liang et al., 2006) initialised with a lexicon of word identity mappings, and parsed with Bikel’s (2002) parser. From this we extracted grammar rules following the technique described in Cohn and Lapata (2007). For the pivot grammar we use the French-English Europarl v2 which contains approximately 688K sentences. Again, the corpus was aligned using the Berkeley aligner and the English side was parsed with Bikel’s parser. We extracted tree-to-string rules using our implementation of the GHKM method. To ameliorate the effects of poor alignments on the grammar, we removed singleton rules before pivoting. In additi"
C08-1018,W03-1101,0,0.0165856,"model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. 1 Introduction Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald,"
C08-1018,E06-1038,0,0.595873,"Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, including reordering, su"
C08-1018,P02-1040,0,0.0786148,"Missing"
C08-1018,W04-3219,0,0.252886,"efore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts. In this framework, the goal is to find a summary of the original sentence which is grammatical and conveys the most important information without necessarily using the same words in the same order. Our task is related to, but different from, paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them in order to generate new text, while attempting to produce a shorter resulting string. Quirk et al. (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both acquire paraphrases and use them to generate new strings. However, their model is limited to lexical substitution — no reordering takes place — and is 137 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137–144 Manchester, August 2008 lacking the compression objective. Once we move away from extractive compression we are faced with two problems. First, we must find an appropriate training set for our abstractive task. Compression corpora are n"
C08-1018,P05-1036,0,0.0191217,"summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the task computationally feasible, allowing efficient decoding using a dynamic program (Knight and Marcu, 2002; Turner and Charniak, 2005; McDonald, 2006). Furthermore, constraining the problem to word deletion affords substantial modeling flexibility. Indeed, a variety of models have been successfully developed for this task ranging from instantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007; Turner and Charniak, 2005), to large-margin learning (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008). However, the simplification also renders the task somewhat artificial. There are many rewrite operations that could compress a sentence, besides deletion, includi"
C08-1018,W04-1015,0,0.0145789,"ee transduction model that can naturally account for structural and lexical mismatches. The model incorporates a novel grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression specific loss functions. 1 Introduction Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss. It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003). Most prior work has focused on a specific instantiation of sentence compression, namely word deletion. Given an input sentence of words, w1 , w2 . . . wn , a compression is formed by dropping any subset of these words (Knight c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. and Marcu, 2002). The simplification renders the t"
C10-2029,S07-1053,0,0.06841,"Missing"
C10-2029,D08-1094,0,0.0626708,"Missing"
C10-2029,D07-1109,0,0.0340874,"Missing"
C10-2029,S07-1009,0,0.025182,"gned a representation as a point in a highdimensional space, where the dimensions represent contextual features such as co-occurring words. Following this, meaning relatedness scores are computed by using various similarity measures on the vector representations. One of the major issues that all distributional methods have to face is sense ambiguity. Since vector representations reflect mixtures of uses additional methods have to be employed in order to capture specific meanings of a word in context. Consider the occurrence of verb shed in the following SemEval 2007 Lexical Substitution Task (McCarthy and Navigli, 2007) example: Cats in the latent phase only have the virus internally , but feel normal and do not shed the virus to other cats and the environment . In this paper we present a method for computing similarity which builds vector representations for words in context. Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pad´o, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al., 2007)). Our ap"
C10-2029,P08-1028,1,0.716826,"shed will not capture this infrequent sense. Recent work on distributional methods for similarity focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. In this paper we present a method for computing similarity which builds vector representations for words in context by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that our model significantly outperforms typical distributional methods. 1 For these reasons, recent work on distributional methods for similarity such as (Mitchell and Lapata, 2008) (Erk and Pad´o, 2008) (Thater et al., 2009) focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. Introduction Distributional methods for word similarity ((Landauer and Dumais, 1997), (Schuetze, 1998)) are based on co-occurrence statistics extracted from large amounts of text. Typically, each word is assigned a representation as a point in a highdimensional space, where the dimensions represent contextual features such as co-occurring words. Following this, meaning relatedness scores are computed by using various similarity measures on"
C10-2029,N07-1071,0,0.109096,"(McCarthy and Navigli, 2007) example: Cats in the latent phase only have the virus internally , but feel normal and do not shed the virus to other cats and the environment . In this paper we present a method for computing similarity which builds vector representations for words in context. Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pad´o, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al., 2007)). Our approach differs from this as we assume ambiguity of words at the, initial, acquisition step, by encoding senses of words as a hidden variable in the text we process. In this paper we focus on a particular distributional representation inspired by (Lin and Pantel, 2001a) and induce context-sensitive similarity between phrases represented as paths in dependency graphs. It is inspired by recent work on topic models and it deals with sense-ambiguity in a natural manner by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that"
C10-2029,J98-1004,0,0.156989,"which builds vector representations for words in context by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that our model significantly outperforms typical distributional methods. 1 For these reasons, recent work on distributional methods for similarity such as (Mitchell and Lapata, 2008) (Erk and Pad´o, 2008) (Thater et al., 2009) focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. Introduction Distributional methods for word similarity ((Landauer and Dumais, 1997), (Schuetze, 1998)) are based on co-occurrence statistics extracted from large amounts of text. Typically, each word is assigned a representation as a point in a highdimensional space, where the dimensions represent contextual features such as co-occurring words. Following this, meaning relatedness scores are computed by using various similarity measures on the vector representations. One of the major issues that all distributional methods have to face is sense ambiguity. Since vector representations reflect mixtures of uses additional methods have to be employed in order to capture specific meanings of a word"
C10-2029,P08-1078,0,0.133102,"Missing"
C10-2029,W09-2506,1,0.899921,"nt work on distributional methods for similarity focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. In this paper we present a method for computing similarity which builds vector representations for words in context by modeling senses as latent variables in a large corpus. We apply this to the Lexical Substitution Task and we show that our model significantly outperforms typical distributional methods. 1 For these reasons, recent work on distributional methods for similarity such as (Mitchell and Lapata, 2008) (Erk and Pad´o, 2008) (Thater et al., 2009) focuses on using the context in which a target word occurs to derive context-sensitive similarity computations. Introduction Distributional methods for word similarity ((Landauer and Dumais, 1997), (Schuetze, 1998)) are based on co-occurrence statistics extracted from large amounts of text. Typically, each word is assigned a representation as a point in a highdimensional space, where the dimensions represent contextual features such as co-occurring words. Following this, meaning relatedness scores are computed by using various similarity measures on the vector representations. One of the majo"
C10-2029,E09-1013,1,\N,Missing
D07-1001,W97-0703,0,0.238038,"rom previous approaches in two key respects. First, we present a compression model that is contextually aware; decisions on whether to remove or retain a word (or phrase) are informed by its discourse properties (e.g., whether it introduces a new topic, whether it is semantically related to the previous sentence). Second, we apply our compression model to entire documents rather than isolated sentences. This is more in the spirit of real-world applications where the goal is to generate a condensed and coherent text. Previous work on summarisation has also utilised discourse information (e.g., Barzilay and Elhadad 1997; Daum´e III and Marcu 2002; Marcu 2000; Teufel and Moens 2002). However, its application to document compression is novel to our knowledge. 3 Discourse Representation Obtaining an appropriate representation of discourse is the first step towards creating a compression model that exploits contextual information. In this work we focus on the role of local coherence as this is prerequisite for maintaining global coherence. Ideally, we would like our compressed document to maintain the discourse flow of the original. For this reason, we automatically annotate the source document with discourse-le"
D07-1001,briscoe-carroll-2002-robust,0,0.0563862,"d to the C f list. Entity matching between sentences is required to determine the Cb of a sentence. This is done using the named entity’s unique identifier (as provided by LingPipe) or by the entity’s surface form in the case of nouns not classified as named entities. Entities are ranked according to their grammatical roles; subjects are ranked more highly than objects, which are in turn ranked higher than other grammatical roles (Grosz et al. 1995); ties are broken using left-to-right ordering of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles with RASP (Briscoe and Carroll 2002). Formally, our centering algorithm is as follows (where Ui corresponds to sentence i): 1 LingPipe can be downloaded alias-i.com/lingpipe/. from http://www. 3 1. Extract entities from Ui . 2. Create C f (Ui ) by ranking the entities in Ui according to their grammatical role (subjects > objects > others). 3. Find the highest ranked entity in C f (Ui−1 ) which occurs in C f (Ui ), set the entity to be Cb (Ui ). The above procedure involves several automatic steps (named entity recognition, coreference resolution, identification of grammatical roles) and will unavoidably produce some noisy annota"
D07-1001,P06-2019,1,0.861622,"Missing"
D07-1001,P06-1048,1,0.623971,"Missing"
D07-1001,P02-1057,0,0.342278,"Missing"
D07-1001,N07-1023,0,0.280547,"ulated within the framework of Integer Linear Programming. Experimental results show significant improvements over a stateof-the-art discourse agnostic approach. Most work to date has focused on a rather simple formulation of sentence compression that does not allow any rewriting operations, besides word removal. Moreover, compression is performed on isolated sentences without taking into account their surrounding context. An advantage of this simple view is that it renders sentence compression amenable to a variety of learning paradigms ranging from instantiations of the noisy-channel model (Galley and McKeown 2007; Knight and Marcu 2002; Turner and Charniak 2005) to Integer Linear Programming (Clarke and Lapata 2006a) and large-margin online learning (McDonald 2006). 1 Introduction The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatically correct (Jing 2000). Sentence compression is commonly expressed as a word deletion problem: given an input sentence of words W = w1 , w2 , . . . , wn , the aim is to produce a compre"
D07-1001,J95-2003,0,0.884482,"owing how the discourse progresses from sentence to sentence. To give a simple example, a contextually aware compression system could drop a word or phrase from the current sentence, simply because it is not mentioned anywhere else in the document and is therefore deemed unimportant. Or it could decide to retain it for the sake of topic continuity. We are interested in creating a compression model that is appropriate for documents and sentences. To this end, we assess whether discourse-level information is helpful. Our analysis is informed by two popular models of discourse, Centering Theory (Grosz et al. 1995) and lexical chains (Morris and Hirst 1991). Both approaches model local coherence — the way adjacent sentences bind together to form a larger discourse. Our compression model is an extension of the integer programming formulation proposed by Clarke and Lapata (2006a). Their approach is conceptually simple: it consists of a scoring function coupled with a small number of syntactic and semantic constraints. Discourse-related information can be easily incorporated in the form of additional constraints. We employ our model to perform sentence compression throughout a whole document (by compressin"
D07-1001,A00-1043,0,0.147779,"mple view is that it renders sentence compression amenable to a variety of learning paradigms ranging from instantiations of the noisy-channel model (Galley and McKeown 2007; Knight and Marcu 2002; Turner and Charniak 2005) to Integer Linear Programming (Clarke and Lapata 2006a) and large-margin online learning (McDonald 2006). 1 Introduction The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatically correct (Jing 2000). Sentence compression is commonly expressed as a word deletion problem: given an input sentence of words W = w1 , w2 , . . . , wn , the aim is to produce a compression by removing any subset of these words (Knight and Marcu 2002). Sentence compression can potentially benefit many applications. For example, in summarisation, a compression mechanism could improve the conciseness of the generated summaries (Jing 2000; Lin 2003). Sentence compression could be also used to automatically generate subtitles for television programs; the transcripts cannot usually be In this paper we take a closer loo"
D07-1001,W03-1101,0,0.066772,"tion in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatically correct (Jing 2000). Sentence compression is commonly expressed as a word deletion problem: given an input sentence of words W = w1 , w2 , . . . , wn , the aim is to produce a compression by removing any subset of these words (Knight and Marcu 2002). Sentence compression can potentially benefit many applications. For example, in summarisation, a compression mechanism could improve the conciseness of the generated summaries (Jing 2000; Lin 2003). Sentence compression could be also used to automatically generate subtitles for television programs; the transcripts cannot usually be In this paper we take a closer look at one of the simplifications associated with the compression task, namely that sentence reduction can be realised in isolation without making use of discourse-level information. This is clearly not true — professional abstracters often rely on contextual cues while creating summaries (Endres-Niggemeyer 1998). Furthermore, determining what information is important in a sentence is influenced by a variety of contextual facto"
D07-1001,E06-1038,0,0.553885,"st work to date has focused on a rather simple formulation of sentence compression that does not allow any rewriting operations, besides word removal. Moreover, compression is performed on isolated sentences without taking into account their surrounding context. An advantage of this simple view is that it renders sentence compression amenable to a variety of learning paradigms ranging from instantiations of the noisy-channel model (Galley and McKeown 2007; Knight and Marcu 2002; Turner and Charniak 2005) to Integer Linear Programming (Clarke and Lapata 2006a) and large-margin online learning (McDonald 2006). 1 Introduction The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatically correct (Jing 2000). Sentence compression is commonly expressed as a word deletion problem: given an input sentence of words W = w1 , w2 , . . . , wn , the aim is to produce a compression by removing any subset of these words (Knight and Marcu 2002). Sentence compression can potentially benefit many applications. For example, in summar"
D07-1001,P00-1052,0,0.0257798,"s discourse, but it does so locally since Cb (Ui ) is chosen from Ui−1 . Centering Algorithm So far we have presented centering without explicitly stating how the concepts “utterance”, “entities” and “ranking” are instantiated. A great deal of research has been devoted into fleshing these out and many different instantiations have been developed in the literature (see Poesio et al. 2004 for details). Since our aim is to identify centers in discourse automatically, our parameter choice is driven by two considerations, robustness and ease of computation. We therefore follow previous work (e.g., Miltsakaki and Kukich 2000) in assuming that the unit of an utterance is the sentence (i.e., a main clause with accompanying subordinate and adjunct clauses). This is in line with our compression task which also operates over sentences. We determine which entities are invoked by a sentence using two methods. First, we perform named entity identification and coreference resolution on each document using LingPipe1 , a publicly available system. Named entities and all remaining nouns are added to the C f list. Entity matching between sentences is required to determine the Cb of a sentence. This is done using the named enti"
D07-1001,J91-1002,0,0.158814,"sentence to sentence. To give a simple example, a contextually aware compression system could drop a word or phrase from the current sentence, simply because it is not mentioned anywhere else in the document and is therefore deemed unimportant. Or it could decide to retain it for the sake of topic continuity. We are interested in creating a compression model that is appropriate for documents and sentences. To this end, we assess whether discourse-level information is helpful. Our analysis is informed by two popular models of discourse, Centering Theory (Grosz et al. 1995) and lexical chains (Morris and Hirst 1991). Both approaches model local coherence — the way adjacent sentences bind together to form a larger discourse. Our compression model is an extension of the integer programming formulation proposed by Clarke and Lapata (2006a). Their approach is conceptually simple: it consists of a scoring function coupled with a small number of syntactic and semantic constraints. Discourse-related information can be easily incorporated in the form of additional constraints. We employ our model to perform sentence compression throughout a whole document (by compressing sentences sequentially) and evaluate whet"
D07-1001,J04-3003,0,0.0168071,"Missing"
D07-1001,N03-1026,0,0.604268,"able and informative using a question-answering task. Our method yields significant improvements over a discourse agnostic state-of-the-art compression model (McDonald 2006). 2 Related Work Sentence compression has been extensively studied across different modelling paradigms and has received both generative and discriminative formulations. Most generative approaches (Galley and McKeown 2007; Knight and Marcu 2002; Turner and Charniak 2005) are instantiations of the noisychannel model, whereas discriminative formulations include decision-tree learning (Knight and Marcu 2002), maximum entropy (Riezler et al. 2003), support vector machines (Nguyen et al. 2004), and large-margin learning (McDonald 2006). These models are trained on a parallel corpus of long source sentences and their target compressions. Using a rich feature set derived from parse trees, the 2 models learn either which constituents to delete or which words to place adjacently in the compression output. Relatively few approaches dispense with the parallel corpus and generate compressions in an unsupervised manner using either a scoring function (Clarke and Lapata 2006a; Hori and Furui 2004) or compression rules that are approximated from"
D07-1001,J01-4003,0,0.0130511,"le system. Named entities and all remaining nouns are added to the C f list. Entity matching between sentences is required to determine the Cb of a sentence. This is done using the named entity’s unique identifier (as provided by LingPipe) or by the entity’s surface form in the case of nouns not classified as named entities. Entities are ranked according to their grammatical roles; subjects are ranked more highly than objects, which are in turn ranked higher than other grammatical roles (Grosz et al. 1995); ties are broken using left-to-right ordering of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles with RASP (Briscoe and Carroll 2002). Formally, our centering algorithm is as follows (where Ui corresponds to sentence i): 1 LingPipe can be downloaded alias-i.com/lingpipe/. from http://www. 3 1. Extract entities from Ui . 2. Create C f (Ui ) by ranking the entities in Ui according to their grammatical role (subjects > objects > others). 3. Find the highest ranked entity in C f (Ui−1 ) which occurs in C f (Ui ), set the entity to be Cb (Ui ). The above procedure involves several automatic steps (named entity recognition, coreference resolution, identification"
D07-1001,J02-4002,0,0.0275638,"ompression model that is contextually aware; decisions on whether to remove or retain a word (or phrase) are informed by its discourse properties (e.g., whether it introduces a new topic, whether it is semantically related to the previous sentence). Second, we apply our compression model to entire documents rather than isolated sentences. This is more in the spirit of real-world applications where the goal is to generate a condensed and coherent text. Previous work on summarisation has also utilised discourse information (e.g., Barzilay and Elhadad 1997; Daum´e III and Marcu 2002; Marcu 2000; Teufel and Moens 2002). However, its application to document compression is novel to our knowledge. 3 Discourse Representation Obtaining an appropriate representation of discourse is the first step towards creating a compression model that exploits contextual information. In this work we focus on the role of local coherence as this is prerequisite for maintaining global coherence. Ideally, we would like our compressed document to maintain the discourse flow of the original. For this reason, we automatically annotate the source document with discourse-level information which is subsequently used to inform our compre"
D07-1001,P05-1036,0,0.29012,"ogramming. Experimental results show significant improvements over a stateof-the-art discourse agnostic approach. Most work to date has focused on a rather simple formulation of sentence compression that does not allow any rewriting operations, besides word removal. Moreover, compression is performed on isolated sentences without taking into account their surrounding context. An advantage of this simple view is that it renders sentence compression amenable to a variety of learning paradigms ranging from instantiations of the noisy-channel model (Galley and McKeown 2007; Knight and Marcu 2002; Turner and Charniak 2005) to Integer Linear Programming (Clarke and Lapata 2006a) and large-margin online learning (McDonald 2006). 1 Introduction The computational treatment of sentence compression has recently attracted much attention in the literature. The task can be viewed as producing a summary of a single sentence that retains the most important information and remains grammatically correct (Jing 2000). Sentence compression is commonly expressed as a word deletion problem: given an input sentence of words W = w1 , w2 , . . . , wn , the aim is to produce a compression by removing any subset of these words (Knigh"
D07-1001,W04-1015,0,0.072187,"Missing"
D07-1001,W04-2401,0,\N,Missing
D07-1001,N06-1046,1,\N,Missing
D07-1001,C04-1197,0,\N,Missing
D07-1001,C04-1107,0,\N,Missing
D07-1001,N07-1030,0,\N,Missing
D07-1001,W05-0618,0,\N,Missing
D07-1001,W06-1616,1,\N,Missing
D07-1002,W02-1033,0,0.00885346,"dency relations provided by MiniPar (42 in total). In order to increase coverage, we combine all relation paths for predicates 17 Preprocessing Here we summarize the steps of our QA system preceding the assignment of semantic structure and answer extraction. For each question, we recognize its expected answer type (e.g., in Q: Which record company is Fred Durst with? we would expect the answer to be an ORGANIZATION ). Answer types are determined using classification rules similar to Li and Roth (2002). We also reformulate questions into declarative sentences following the strategy proposed in Brill et al. (2002). The reformulated sentences are submitted as queries to an IR engine for retrieving sentences with relevant answers. Specifically, we use the Lemur Toolkit3 , a state-of-the-art language model-driven search engine. We work only with the 50 top-ranked sentences as this setting performed best in previous experiments of our QA system. We also add to Lemur’s output gold standard sentences, which contain and support an answer for each question. Specifically, documents relevant for each question are retrieved from the AQUAINT Corpus4 according to TREC supplied judgments. Next, sentences which match"
D07-1002,J02-3001,0,0.733989,"or answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models. 1 Introduction Recent years have witnessed significant progress in developing methods for the automatic identification and labeling of semantic roles conveyed by sentential constituents.1 The success of these methods, often referred to collectively as shallow semantic parsing (Gildea and Jurafsky, 2002), is largely due to the availability of resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005), which document the surface realization of semantic roles in real world corpora. More concretely, in the FrameNet paradigm, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations. Semantic roles (or frame 1 The approaches are too numerous to list; we refer the interested reader to Carreras and M`arquez (2005) for an overview. (1) a. b. c. d. e. [Lee]Seller sold a textbook [to Abby]Buyer . [Kim]Seller s"
D07-1002,W06-1601,0,0.186973,"Missing"
D07-1002,C02-1150,0,0.0383054,"ion paths by traversing the dependency tree from the frame element node to the predicate node. We used all dependency relations provided by MiniPar (42 in total). In order to increase coverage, we combine all relation paths for predicates 17 Preprocessing Here we summarize the steps of our QA system preceding the assignment of semantic structure and answer extraction. For each question, we recognize its expected answer type (e.g., in Q: Which record company is Fred Durst with? we would expect the answer to be an ORGANIZATION ). Answer types are determined using classification rules similar to Li and Roth (2002). We also reformulate questions into declarative sentences following the strategy proposed in Brill et al. (2002). The reformulated sentences are submitted as queries to an IR engine for retrieving sentences with relevant answers. Specifically, we use the Lemur Toolkit3 , a state-of-the-art language model-driven search engine. We work only with the 50 top-ranked sentences as this setting performed best in previous experiments of our QA system. We also add to Lemur’s output gold standard sentences, which contain and support an answer for each question. Specifically, documents relevant for each"
D07-1002,C94-1079,0,0.048558,"imental Setup Data All our experiments were performed on the TREC02–05 factoid questions. We excluded NIL questions since TREC doesn’t supply an answer for them. We used the FrameNet V1.3 lexical database. It contains 10,195 predicates grouped into 795 semantic frames and 141,238 annotated sentences. Figure 4 shows the number of annotated sentences available for different predicates. As can be seen, there are 3,380 predicates with no annotated sentences and 1,175 predicates with less than 5 annotated sentences. All FrameNet sentences, questions, and answer sentences were parsed using MiniPar (Lin, 1994), a robust dependency parser. As mentioned in Section 4 we extract dependency relation paths by traversing the dependency tree from the frame element node to the predicate node. We used all dependency relations provided by MiniPar (42 in total). In order to increase coverage, we combine all relation paths for predicates 17 Preprocessing Here we summarize the steps of our QA system preceding the assignment of semantic structure and answer extraction. For each question, we recognize its expected answer type (e.g., in Q: Which record company is Fred Durst with? we would expect the answer to be an"
D07-1002,N03-1022,0,0.0293745,". Then we define our learning task and introduce our approach to semantic role assignment and answer extraction in the context of QA. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Question answering systems have traditionally depended on a variety of lexical resources to bridge surface differences between questions and potential answers. WordNet (Fellbaum, 1998) is perhaps the most popular resource and has been employed in a variety of QA-related tasks ranging from query expansion, to axiom-based reasoning (Moldovan et al., 2003), passage scoring (Paranjpe et al., 2003), and answer filtering (Leidner et al., 2004). Besides WordNet, recent QA systems increasingly rely on syntactic information as a means of abstracting over word order differences and structural alternations (e.g., passive vs. active voice). Most syntax-based QA systems (Wu et al., 2005) incorporate some means of comparison between the tree representing the question with the subtree surrounding the answer candidate. The assumption here is that appropriate answers are more likely to have syntactic relations in common with their corresponding question. Syn"
D07-1002,C04-1100,0,0.0529725,"is rock]Goods . By abstracting over surface syntactic configurations, semantic roles offer an important first step towards deeper text understanding and hold promise for a range of applications requiring broad coverage semantic processing. Question answering (QA) is often cited as an obvious beneficiary of semantic 12 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 12–21, Prague, June 2007. 2007 Association for Computational Linguistics role labeling (Gildea and Jurafsky, 2002; Palmer et al., 2005; Narayanan and Harabagiu, 2004). Faced with the question Q: What year did the U.S. buy Alaska? and the retrieved sentence S: . . .before Russia sold Alaska to the United States in 1867, a hypothetical QA system must identify that United States is the Buyer despite the fact that it is attested in one instance as a subject and in another as an object. Once this information is known, isolating the correct answer (i.e., 1867 ) can be relatively straightforward. Although conventional wisdom has it that semantic role labeling ought to improve answer extraction, surprising little work has been done to this effect (see Section 2 fo"
D07-1002,P06-1146,1,0.422642,"Missing"
D07-1002,J05-1004,0,0.935161,"ptimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models. 1 Introduction Recent years have witnessed significant progress in developing methods for the automatic identification and labeling of semantic roles conveyed by sentential constituents.1 The success of these methods, often referred to collectively as shallow semantic parsing (Gildea and Jurafsky, 2002), is largely due to the availability of resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005), which document the surface realization of semantic roles in real world corpora. More concretely, in the FrameNet paradigm, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations. Semantic roles (or frame 1 The approaches are too numerous to list; we refer the interested reader to Carreras and M`arquez (2005) for an overview. (1) a. b. c. d. e. [Lee]Seller sold a textbook [to Abby]Buyer . [Kim]Seller sold [the sweater]Goods . [My company]Seller has sold [more than three million copies]Goods . [Abby]Seller sold [the car]Go"
D07-1002,W03-1201,0,0.00532578,"troduce our approach to semantic role assignment and answer extraction in the context of QA. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work Question answering systems have traditionally depended on a variety of lexical resources to bridge surface differences between questions and potential answers. WordNet (Fellbaum, 1998) is perhaps the most popular resource and has been employed in a variety of QA-related tasks ranging from query expansion, to axiom-based reasoning (Moldovan et al., 2003), passage scoring (Paranjpe et al., 2003), and answer filtering (Leidner et al., 2004). Besides WordNet, recent QA systems increasingly rely on syntactic information as a means of abstracting over word order differences and structural alternations (e.g., passive vs. active voice). Most syntax-based QA systems (Wu et al., 2005) incorporate some means of comparison between the tree representing the question with the subtree surrounding the answer candidate. The assumption here is that appropriate answers are more likely to have syntactic relations in common with their corresponding question. Syntactic structure matching has been applie"
D07-1002,N04-1030,0,0.00527607,"hing has been applied to passage retrieval (Cui et al., 2005) and answer extraction (Shen and Klakow, 2006). Narayanan and Harabagiu (2004) were the first to stress the importance of semantic roles in answering complex questions. Their system identifies predicate argument structures by merging semantic role information from PropBank and FrameNet. Expected answers are extracted by performing probabilistic inference over the predicate argument structures in conjunction with a domain specific topic model. Sun et al. (2005) incorporate semantic analysis in their TREC05 QA system. They use ASSERT (Pradhan et al., 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. They find that semantic analysis does not boost performance due to the low recall of the semantic parser. Kaisser (2006) proposes a Q Model I SemStruc q SemStruc ac1 Sent. Model I Model II Answer SemStruc ac2 SemStruc aci Figure 1: Architecture of answer extraction question paraphrasing method based on FrameNet. Questions are assigned semantic roles by matching their dependency relations with those a"
D07-1002,P06-1112,1,0.478377,"ecent QA systems increasingly rely on syntactic information as a means of abstracting over word order differences and structural alternations (e.g., passive vs. active voice). Most syntax-based QA systems (Wu et al., 2005) incorporate some means of comparison between the tree representing the question with the subtree surrounding the answer candidate. The assumption here is that appropriate answers are more likely to have syntactic relations in common with their corresponding question. Syntactic structure matching has been applied to passage retrieval (Cui et al., 2005) and answer extraction (Shen and Klakow, 2006). Narayanan and Harabagiu (2004) were the first to stress the importance of semantic roles in answering complex questions. Their system identifies predicate argument structures by merging semantic role information from PropBank and FrameNet. Expected answers are extracted by performing probabilistic inference over the predicate argument structures in conjunction with a domain specific topic model. Sun et al. (2005) incorporate semantic analysis in their TREC05 QA system. They use ASSERT (Pradhan et al., 2004), a publicly available shallow semantic parser trained on PropBank, to generate predic"
D07-1002,H05-1010,0,0.00924554,"le assignment as a global optimization problem. Specifically, we model the interaction between all pairwise labeling decisions as a minimum weight bipartite edge cover problem (Eiter and Mannila, 1997; Cormen et al., 1990). An edge cover is a subgraph of a bipartite graph so that each node is linked to at least one node of the other partition. This yields a semantic role assignment for all frame elements (see Figure 2b where frame elements and roles are adjacent to an edge). Edge covers have been successfully applied in several natural language processing tasks, including machine translation (Taskar et al., 2005) and annotation projection (Pad´o and Lapata, 2006). Formally, optimal edge cover assignments are solutions of following optimization problem: (4) where, max E is edge cover s(nd w , nd SR ) ∏ tween the frame element node nd w and semantic role node nd SR . Edge covers can be computed efficiently in cubic time using algorithms for the equivalent linear assignment problem. Our experiments used Jonker and Volgenant’s (1987) solver.2 Figure 3 shows the semantic role assignments generated by our model for the question Q: Who discovered prions? and the candidate answer sentence S: 1997: Stanley B."
D07-1008,P01-1008,0,0.0985742,"ases or sentences in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervise"
D07-1008,E99-1042,0,0.0558307,"sion bring significant improvements over a state-of-the-art model. 1 Introduction Recent years have witnessed increasing interest in text-to-text generation methods for many natural language processing applications ranging from text summarisation to question answering and machine translation. At the heart of these methods lies the ability to perform rewriting operations according to a set of prespecified constraints. For example, text simplification identifies which phrases or sentences in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on"
D07-1008,P05-1033,0,0.0360254,"urage or discourage compression (see Figure 4), but admittedly in other frameworks (e.g., Clarke and Lapata (2006a)) the length of the compression can be influenced more naturally. In our formulation of the compression problem, a derivation is characterised by a single inventory of features. This entails that the feature space cannot in principle distinguish between derivations that use the same rules, applied in a different order. Although, this situation does not arise often in our dataset, we believe that it can be ameliorated by intersecting a language model with our generation algorithm (Chiang, 2005). 81 Conclusions and Future Work In this paper we have presented a novel method for sentence compression cast in the framework of structured learning. We develop a system that generates compressions using a synchronous tree substitution grammar whose weights are discriminatively trained within a large margin model. We also describe an appropriate algorithm than can be used in both training (i.e., learning the model weights) and decoding (i.e., finding the most plausible compression under the model). The proposed formulation allows us to capture rewriting operations that go beyond word deletion"
D07-1008,P06-2019,1,0.920713,"Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long sentences and their compressions are isomorphic. Consequently, the models are not generally applicable to other text rewriting problems since they cannot readily handle structural mismatches and more complex rewriting operations such as substitutions or insertions. A related iss"
D07-1008,P06-1048,1,0.760964,"Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long sentences and their compressions are isomorphic. Consequently, the models are not generally applicable to other text rewriting problems since they cannot readily handle structural mismatches and more complex rewriting operations such as substitutions or insertions. A related iss"
D07-1008,D07-1001,1,0.84516,"ond, it also uses large margin learning. Sentence compression is formulated as a string-to-substring mapping problem with a deletion-based Hamming loss. Recall that our formulation involves a tree-to-tree mapping. Third, it uses a feature space complementary to ours. For example features are defined between adjacent words, and syntactic evidence is incorporated indirectly into the model. In contrast our model relies on synchronous rules to generate valid compressions and does not explicitly incorporate adjacency features. We used an implementation of McDonald (2006) for comparison of results (Clarke and Lapata, 2007). Evaluation Measures In line with previous work we assessed our model’s output by eliciting human judgements. Participants were presented with an original sentence and its compression and asked to rate the latter on a five point scale based on the information retained and its grammaticality. We conducted two separate elicitation studies, one for the 4 The corpus can be downloaded from http://homepages. inf.ed.ac.uk/s0460084/data/. 79 O: I just wish my parents and my other teachers could be like this teacher, so we could communicate. M: I wish my teachers could be like this teacher. S: I wish"
D07-1008,P03-2041,0,0.575422,"e compressions will have well-formed syntactic structures. And it will not be easy to process them for subsequent generation or analysis tasks. In this paper we present a text-to-text rewriting 73 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 73–82, Prague, June 2007. 2007 Association for Computational Linguistics model that scales to non-isomorphic cases and can thus naturally account for structural and lexical divergences. Our approach is inspired by synchronous tree substitution grammar (STSG, Eisner (2003)) a formalism that allows local distortion of the tree topology. We show how such a grammar can be induced from a parallel corpus and propose a large margin model for the rewriting task which can be viewed as a weighted tree-to-tree transducer. Our learning framework makes use of the algorithm put forward by Tsochantaridis et al. (2005) which efficiently learns a prediction function to minimise a given loss function. Experiments on sentence compression show significant improvements over the state-of-the-art. Beyond sentence compression and related text-to-text generation problems (e.g., paraph"
D07-1008,N07-1023,0,0.173654,"t is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structure"
D07-1008,N04-1014,0,0.0390924,"Missing"
D07-1008,A00-1043,0,0.379828,"methods for many natural language processing applications ranging from text summarisation to question answering and machine translation. At the heart of these methods lies the ability to perform rewriting operations according to a set of prespecified constraints. For example, text simplification identifies which phrases or sentences in a document will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly de"
D07-1008,N03-1017,0,0.0150456,"ld06 STSG Gold standard CompR 68.6 73.7 76.1 RelF1 47.6 53.4∗ — 45 50 F1 ● Ziff-Davis McDonald06 STSG Gold standard ● 60 65 70 75 80 Prec Prec.BP1 Prec.BP2 85 compression rate Figure 4: Compression rate vs. grammatical relations F1 using unigram precision alone and in combination with two brevity penalties. a parallel corpus of syntax trees. We obtained syntactic analyses for source and target sentences with Bikel’s (2002) parser. Our corpora were automatically aligned with Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the intersection heuristic (Koehn et al., 2003). Each word in the lexicon was also aligned with itself. This was necessary in order to inform Giza++ about word identity. Unparseable sentences and those longer than 50 tokens were removed from the data set. We induced a synchronous tree substitution grammar from the Ziff-Davis and Broadcast news corpora using the method described in Section 3.2. We extracted all maximally general synchronous rules. These were complemented with more specific rules from conjoining pairs of general rules. The specific rules were pruned to remove singletons and those rules with more than 3 variables. Grammar rul"
D07-1008,E06-1038,0,0.105015,"corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long sentences and their"
D07-1008,P04-1083,0,0.0322744,"Missing"
D07-1008,J04-4002,0,0.128935,"Missing"
D07-1008,W99-0604,0,0.0869948,"fferent compressions of the same sentence. 60 55 ● ● CompR 66.2 56.8 57.2 RelF1 45.8 54.3 — Broadcast News McDonald06 STSG Gold standard CompR 68.6 73.7 76.1 RelF1 47.6 53.4∗ — 45 50 F1 ● Ziff-Davis McDonald06 STSG Gold standard ● 60 65 70 75 80 Prec Prec.BP1 Prec.BP2 85 compression rate Figure 4: Compression rate vs. grammatical relations F1 using unigram precision alone and in combination with two brevity penalties. a parallel corpus of syntax trees. We obtained syntactic analyses for source and target sentences with Bikel’s (2002) parser. Our corpora were automatically aligned with Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the intersection heuristic (Koehn et al., 2003). Each word in the lexicon was also aligned with itself. This was necessary in order to inform Giza++ about word identity. Unparseable sentences and those longer than 50 tokens were removed from the data set. We induced a synchronous tree substitution grammar from the Ziff-Davis and Broadcast news corpora using the method described in Section 3.2. We extracted all maximally general synchronous rules. These were complemented with more specific rules from conjoining pairs of general"
D07-1008,N03-1024,0,0.0794727,"ent will pose reading difficulty for a given user and substitutes them with simpler alternatives (Carroll et al., 1999). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting"
D07-1008,P02-1040,0,0.084125,"nes as the tree derivation.3 Given this restriction, we define a loss based on position-independent unigram precision (Prec) which penalises errors in the yield independently for each word. Although fairly intuitive, this loss is far from ideal. First, it maximally rewards repeatedly predicting the same word if the latter is in the reference target tree. Secondly, it may bias towards overly short output which drops core information — one-word compressions will tend to have higher precision than longer output. To counteract this, we introduce two brevity penalty measures (BP) inspired by BLEU (Papineni et al., 2002) which we incorporate into the loss function, using a product, loss = 1 − Prec · BP: r BP1 = exp(1 − max(1, )) c c r BP2 = exp(1 − max( , )) r c value one when c = r and decays towards zero for c < r and c > r. In both cases, brevity is assessed against the gold standard target (not the source) to allow the system to learn the correct degree of compression from the training data. Maximisation Algorithm Our algorithm finds the maximising derivation for H(y) in (5). This derivation will have a high loss and a high score under the model, and therefore represents the most-violated constraint which"
D07-1008,W04-3219,0,0.0819526,"ces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). Ideally, we would like a text-to-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a"
D07-1008,N03-1026,0,0.0603333,"fic. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption is that the tree structures representing long se"
D07-1008,P05-1036,0,0.40567,"o-text rewriting system that is not application specific. Given a parallel corpus of training examples, we should be able to learn rewrite rules and how to combine them in order to generate new text. A great deal of previous work has focused on the rule induction problem (Barzilay and McKeown, 2001; Pang et al., 2003; Lin and Pantel, 2001; Shinyama et al., 2002), whereas relatively little emphasis has been placed on the actual generation task (Quirk et al., 2004). A notable exception is sentence compression for which end-to-end rewriting systems are commonly developed (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Riezler et al., 2003; McDonald, 2006). The appeal of this task lies in its simplified formulation as a single rewrite operation, namely word deletion (Knight and Marcu, 2002). Solutions to the compression task have been cast mostly in a supervised learning setting (but see Clarke and Lapata (2006a), Hori and Furui (2004), and Turner and Charniak (2005) for unsupervised methods). Rewrite rules are learnt from a parsed parallel corpus and subsequently used to find the best compression from the set of all possible compressions for a given sentence. A common assumption"
D07-1008,J08-3004,0,\N,Missing
D09-1002,E09-1026,1,0.646527,"Missing"
D09-1002,furstenau-2008-enriching,1,0.862647,"Missing"
D09-1002,P06-2057,0,0.0220432,"on semantic and structural similarity. We evaluate our algorithm in two ways. We assess how accurate it is in predicting the frame for an unknown verb and also evaluate whether the annotations we produce are useful for semantic role labeling. In the following section we provide an overview of related work. Next, we describe our graphalignment model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results. English sentences onto their translations (Pad´o and Lapata, 2006; Johansson and Nugues, 2006). Other work attempts to automatically augment the English FrameNet in a monolingual setting either by extending its coverage or by creating additional training data. 2 In this paper we generalize the proposals of Pennacchiotti et al. (2008) and F¨urstenau and Lapata (2009) in a unified framework. We create training data for semantic role labeling of unknown predicates by projection of annotations from labeled onto unlabeled data. This projection is conThere has been growing interest recently in determining the frame membership for unknown predicates. This is a challenging task, FrameNet curre"
D09-1002,andersen-etal-2008-bnc,0,0.0322566,"Missing"
D09-1002,C08-1050,0,0.0464358,"Missing"
D09-1002,S07-1018,0,0.155779,"mantic roles for the frame Daring are Agent and Manner, whereas for Commerce buy these are Buyer and Goods. A system trained on large amounts of such hand-annotated sentences typically learns to identify the boundaries of the arguments of the verb predicate (argument identification) and label them with semantic roles (argument classification). A variety of methods have been developed for semantic role labeling with reasonably good performance (F1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al., 2007) for an overview of the state-of-the-art). Unfortunately, the reliance on training data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres. The English FrameNet (version 1.3) is not Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over"
D09-1002,C04-1100,0,0.0224065,"semantic role labeling across different languages and text genres. The English FrameNet (version 1.3) is not Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually 11 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11–20, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP a small resource — it contains 502 frames covering 5,866 lexical entries and 135,000"
D09-1002,P06-1146,1,0.893766,"Missing"
D09-1002,D08-1048,0,0.0811666,"Missing"
D09-1002,P03-1002,0,0.158792,"or obstacle to the widespread application of semantic role labeling across different languages and text genres. The English FrameNet (version 1.3) is not Introduction Semantic role labeling, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention in the literature. The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al., 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003), which document the surface realization of semantic roles in real world corpora. Such data is paramount for developing semantic role labelers which are usually 11 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 11–20, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP a small resource — it contains 50"
D09-1002,P06-4020,0,\N,Missing
D09-1045,N03-1008,0,0.161837,"history and use it to modify the probabilities from an n-gram model. In this way, the n-gram’s sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a “pseudodocument”, which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(wi |hi ), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow normalized in order to yield well-formed probability estimates. The approach of Gildea and Hofmann (1999) overcomes this difficulty by using representations constructed with pLSA, which have a direct probabilistic interpretation. As a result, the probability of an upcoming word given the history can be derived naturally and dire"
D09-1045,P01-1017,0,0.0230928,"ur experiments, the hyperparameter α was initialized to 0.5, and the β word probabilities were initialized randomly. We integrated our compositional models with a trigram model which we also trained on BLLIP. The model was built using the SRILM toolkit (Stolcke, 2002) with backoff and Good-Turing smoothing. Ideally, we would have liked to train Roark’s (2001) parser on the same data as that used for the semantic models. However, this would require a gold standard treebank several times larger than those currently available. Following previous work on structured language modeling (Roark, 2001; Charniak, 2001; Chelba and Jelinek, 1998), we therefore trained the parser on sections 2–21 of the Penn Treebank containing 936,017 words. Note that Roark’s (2001) parser produces prefix probabilities for each word of a sentence which we converted to conditional probabilities by dividing each current probability by the previous one. Experimental Setup In this section we discuss our experimental design for assessing the performance of the models presented above. We give details on our training procedure and parameter estimation, and present the methods used for comparison with our approach. Method Following"
D09-1045,C04-1167,0,0.0183125,"stimating the posterior distribution P(θ, z|w, α, β) of the hidden variables given an observed collection of documents w is intractable in general; however, a variety of approximate inference algorithms have been proposed in the literature (e.g., Blei et al. (2003; Griffiths et al. (2007)). 432 models sketched above involve representing the history by multiple LSA models of varying granularity in an attempt to capture topic, subtopic, and local information (Zhang and Rudnicky, 2002); incorporating syntactic information by building the semantic space over words and their syntactic annotations (Kanejiya et al., 2004); and treating the LSA similarity as a feature in a maximum entropy language model (Deng and Khundanpur, 2003). 3 increase the dimensionality of the resulting vector. However, the idea of averaging is somewhat counterintuitive from a linguistic perspective. Composition of simple elements onto more complex ones must allow the construction of novel meanings which go beyond those of the individual elements (Pinker, 1994). In Mitchell and Lapata (2008) we argue that composition models based on multiplication address this problem: Composition Models hi = ui · vi The problem of vector composition ha"
D09-1045,P08-1028,1,0.344988,"chain rule to include only the n-1 preceding words (n is often set within the range of 3–5). The simplification reduces the number of free parameters. However, low values of n impose an artificially local horizon to the language model, 430 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 430–439, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP putes the centroid of the vectors representing the history (e.g., Coccaro and Jurafsky (1998)). This is motivated primarily by mathematical convenience rather than by empirical evidence. In our earlier work (Mitchell and Lapata, 2008) we formulated composition as a function of two vectors and introduced a variety of models based on addition and multiplication. In this paper we apply vector composition to the problem of constructing predictive history representations for language modeling. Besides integrating composition with language modeling, a task which is novel to our knowledge, our approach also serves as a valuable testbed of our earlier framework which we originally evaluated on a small scale verb-subject similarity task. We also investigate how the choice of the underlying semantic representation interacts with the"
D09-1045,J07-2002,1,0.743484,"Missing"
D09-1045,J01-2004,0,0.526925,"nge dependencies, such as syntactic relationships, semantic or thematic constraints. The literature offers many examples of how to overcome this limitation, essentially by allowing the modulation of probabilities by dependencies which extend to words beyond the n-gram horizon. Cache language models (Kuhn and de Mori, 1992) increase the probability of words observed in the history, e.g., by some factor which decays exponentially with distance. Trigger models (Rosenfeld, 1996) go a step further by allowing arbitrary word pairs to be incorporated into the cache. Structured language models (e.g., Roark (2001)) go beyond the representation of history as a linear sequence of words to capture the syntactic constructions in which these words are embedded. It is also possible to build representations of history which are semantic rather than syntactic (Bellegarda (2000; Coccaro and Jurafsky (1998; Gildea and Hofmann (1999)). In this approach, estimates for the probabilities of upcoming words are derived from a comparison of their semantic content with the content of the history so far. The semantic representations, in this case, are vectors derived from the distributional properties of words in a corpu"
D09-1045,D07-1053,0,\N,Missing
D09-1045,D08-1094,0,\N,Missing
D09-1045,P98-1035,0,\N,Missing
D09-1045,C98-1035,0,\N,Missing
D10-1050,P00-1041,0,0.917087,"essions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications. 1 An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and combining them into new sentences guided by a language model (Banko et al., 2000; Soricut and Marcu, 2007). This approach has the potential to work well, breaking out of the single-sentence paradigm. Unfortunately, the resulting summaries are not always coherent — individual constituent phrases are often combined without any semantic constraints — or grammatical beyond the n-gram horizon imposed by the language model. Introduction Summarization is the process of condensing a source text into a shorter version while preserving its information content. Humans summarize on a daily basis and effortlessly, yet the automatic production of high-quality summaries remains a challe"
D10-1050,C08-1018,1,0.785884,"document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1 Approaches to headline generation are too numerous"
D10-1050,P09-1053,0,0.116466,"tion generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to content, style, and the task at hand. 5"
D10-1050,P02-1057,0,0.021816,"Missing"
D10-1050,W03-0501,0,0.0624012,"Missing"
D10-1050,P10-1126,1,0.528706,"that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and E"
D10-1050,N10-1125,1,0.605112,"that are glued together to create a fluent sentence. For example, Banko et al. (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Relatively little work has focused on caption generation, a task related to headline generation. The aim here is to create a short, title-like description of an image embedded in a news article. Like headlines, captions have to be short and informative. In addition, a good caption must clearly identify the subject of the picture and establish its relevance to the article. Feng and Lapata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and E"
D10-1050,A00-1043,0,0.014495,"i-Synchronous Grammar Kristian Woodsend, Yansong Feng and Mirella Lapata School of Informatics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captu"
D10-1050,J02-4006,0,0.0145156,"ormation content. Humans summarize on a daily basis and effortlessly, yet the automatic production of high-quality summaries remains a challenge. Most work today focuses on extractive summarization, where a summary is created by identifying and subsequently concatenating the most important sentences in a document. The advantage of this approach is that it does not require a great deal of linguistic analysis to generate grammatical sentences, Constituent deletion and recombination are merely two of the many rewrite operations professional editors and abstractors employ when creating summaries (Jing, 2002). Additional operations include truncating sentences, aggregating them, and paraphrasing at word or syntax level. Furthermore, professionals write summaries in a task-specific style. News headlines for example are typically short (three to six words), written in the present tense and active voice, and often leave out forms of the verb be. There are also different ways of writing a headline either directly by stating what the docu513 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 513–523, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Associati"
D10-1050,P03-1054,0,0.0419048,"nformation which we obtain by parsing every sentence twice, once with a phrase structure parser and once with a dependency parser. The output from the two representations is combined into a single data structure, by mapping the dependencies to the edges of the phrase structure tree. The procedure is described in detail in Woodsend and Lapata (2010). However, we do not merge the leaf nodes into phrases here, but keep the full tree structure, as we will apply compression to phrases through the QG. In our experiments, we obtain this combined representation from the output of the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. 3.2 Quasi-synchronous grammar Given an input sentence S1 or its parse tree T1, the QG constructs a monolingual grammar for parsing, or generating, the possible translation (or here, paraphrase) trees T2. A grammar node in the target tree T2 is modeled on a subset of nodes in the source tree, with a rather loose alignment between the trees. In our approach, the process of learning the grammar is unsupervised. Each sentence of the source document is compared to each sentence in the target document — headline or caption, depending on th"
D10-1050,W03-1101,0,0.0879938,"Yansong Feng and Mirella Lapata School of Informatics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as parap"
D10-1050,N03-1020,0,0.0251934,"nhances it with topic keywords. For the captions, we compared our model against the highest-scoring document sentence according to the SVM and against the probabilistic model presented in Feng and Lapata (2010a). The latter estimates the probability of a phrase appearing in the caption given the same phrase appearing in the corresponding document and uses a language model to select among many different surface realizations. The language model is adapted with probabilities from an image annotation model (Feng and Lapata, 2010b). Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). The DUC-04 dataset provides four reference headlines per document. We report unigram overlap (ROUGE -1) and bigram overlap (ROUGE -2) as a means of assessing informativeness, and the longest common subsequence (ROUGE -L) as a means of assessing fluency. Original DUC-04 ROUGE parameters were used. We also use ROUGE to evaluate the automatic captions with the original BBC captions as reference. In addition, we evaluated the generated headlines by eliciting human judgments. Participants were presented with a news article and its corresponding headline and were asked to rate the latter along two"
D10-1050,W09-1801,0,0.174402,"atics, University of Edinburgh Edinburgh EH8 9AB, United Kingdom k.woodsend@ed.ac.uk, Y.Feng-4@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract assuming the source document was well written. Unfortunately, extracts generated this way are often documents of low readability and text quality, and contain much redundant information. The conciseness can be improved when sentence extraction is interfaced with sentence compression, where words and clauses are deleted based on rules typically operating over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming"
D10-1050,W06-3104,0,0.271471,"over parsed input (Jing, 2000; Daum´e III and Marcu, 2002; Lin, 2003; Daum´e III, 2006; Zajic et al., 2007; Martins and Smith, 2009). The task of selecting information and rendering it appropriately appears in multiple contexts in summarization. In this paper we present a model that simultaneously optimizes selection and rendering preferences. The model operates over a phrase-based representation of the source document which we obtain by merging PCFG parse trees and dependency graphs. Selection preferences for individual phrases are learned discriminatively, while a quasi-synchronous grammar (Smith and Eisner, 2006) captures rendering preferences such as paraphrases and compressions. Based on an integer linear programming formulation, the model learns to generate summaries that satisfy both types of preferences, while ensuring that length, topic coverage and grammar constraints are met. Experiments on headline and image caption generation show that our method obtains state-of-the-art performance using essentially the same model for both tasks without any major modifications. 1 An alternative abstractive or “bottom-up” approach involves identifying high-interest words and phrases in the source text, and c"
D10-1050,D09-1086,0,0.0242404,"apata (2010a) develop extractive and abstractive caption generation models that operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints re"
D10-1050,D07-1003,0,0.120988,"utput of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Their best model is an extension of Banko et al.’s (2000) word-based model for headline generation to phrases. Our own work develops an ILP-based summarization model with rewrite operations that are not limited to deletion, are defined over phrases, and encoded in quasi-synchronous grammar. The QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), and question answering (Wang et al., 2007); however the use of QG in summarization is novel to our knowledge. Unlike most synchronous grammar formalisms, QG does not posit a strict isomorphism between a source sentence and its target translation; it only loosely links the syntactic structure of the two, and is therefore well suited to describing the relationship between a document and its abstract. We propose an ILP formulation which not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to content, style, and the task at hand. 515 3 Modeling There are three components to"
D10-1050,P10-1058,1,0.791556,"upiec et al., 1995). Given appropriately annotated training data, a binary classifier learns to predict for each document sentence if it is worth extracting. A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint model where words are deleted and sentences selected from a document simultaneously (Daum´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase"
D10-1050,P09-1094,0,0.0181334,"um´e III and Marcu, 2002; Martins and Smith, 2009; Woodsend and Lapata, 2010). ILP models have also been developed for sentence rather than document compression (Clarke and Lapata, 2008). Dras (1999) discusses the application of ILP to reluctant paraphrasing, i.e., the task of choosing between paraphrases while conforming to length, readability, or style constraints. Again, the aim is to rewrite text without, however, content selection. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Notable exceptions are Cohn and Lapata (2008) and Zhao et al. (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Headline generation is a well-studied task within single-document summarization, due to its prominence in the DUC-03 and DUC-04 evaluation competitions.1 Many approaches identify the most informative sentence in a given document (typically the first sentence for the news genre) and subsequently apply a form of sentence compression such that the headline meets some length requirement (Dorr 1 Approaches to headline generation are too numerous to list in detail; see"
D10-1050,A00-2024,0,\N,Missing
D10-1113,W09-0215,0,0.0215078,"d meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is that the similarity of word meanings"
D10-1113,D08-1094,0,0.684663,"Missing"
D10-1113,P10-2017,0,0.49538,"Missing"
D10-1113,P98-2127,0,0.768177,"pose a probabilistic framework for measuring similarity in context. Central to our approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (Landauer and Dumais, 1997; McDonald, 2"
D10-1113,S07-1009,0,0.306038,"n over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity is taken into account directly in the 1162 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics vector construction process, contextualized meaning can be modeled naturally as a change in the original sense distribution. We evaluate our approach on word similarity (Finkelstein et al., 2002) and lexical substitution (McCarthy and Navigli, 2007) and show improvements over competitive baselines. In the remainder of this paper we give a brief overview of related work, emphasizing vector-based approaches that compute word meaning in context (Section 2). Next, we present our probabilistic framework and different instantiations thereof (Sections 3 and 4). Finally, we discuss our experimental results (Sections 5 and 6) and conclude the paper with future work. 2 Related work Vector composition methods construct representations that go beyond individual words (e.g., for phrases or sentences) and thus by default obtain word meanings in contex"
D10-1113,P08-1028,1,0.767463,"ferent senses of words and consequently represent their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overweight person (e.g., She is short and heavy but she has a heart of gold.) or an excessive cannabis user (e.g., Some heavy users develop a psychological dependence on cannabis.). Recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). These methods first extract typical co-occurrence vectors representing a mixture of senses and then use vector operations to either obtain contextualized representations of a target word (Erk and Pad´o, 2008) or a representation for a set of words (Mitchell and Lapata, 2009). In this paper we propose a probabilistic framework for representing word meaning and measuring similarity in context. We model the meaning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likeli"
D10-1113,D09-1045,1,0.949448,"is short and heavy but she has a heart of gold.) or an excessive cannabis user (e.g., Some heavy users develop a psychological dependence on cannabis.). Recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). These methods first extract typical co-occurrence vectors representing a mixture of senses and then use vector operations to either obtain contextualized representations of a target word (Erk and Pad´o, 2008) or a representation for a set of words (Mitchell and Lapata, 2009). In this paper we propose a probabilistic framework for representing word meaning and measuring similarity in context. We model the meaning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity is taken into account directly in the 1162 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1162–1172, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics vector construction process, contex"
D10-1113,J07-2002,1,0.454462,"Missing"
D10-1113,N10-1013,0,0.213729,"words. For example, the meaning of a verb in the presence of its object is modeled as the multiplication of the verb’s vector with the vector capturing the inverse selectional preferences of the object; the latter are computed as the centroid of the verbs that occur with this object. Thater et al. (2009) improve on this model by representing verbs in a second order space, while the representation for objects remains first order. The meaning of a verb boils down to restricting its vector to the features active in the argument noun (i.e., dimensions with value larger than zero). More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. 1163 Specifically, a word’s contexts are clustered to produce groups of similar context vectors. An average prototype vector is then computed separately for each cluster, producing a set of vectors for each word. These cluster vectors can be used to determine the semantic similarity of both isolated words and words in context. In the second case, the distance between prototypes is weighted by the probability that the context belongs to the prototype’s cluster. Erk and Pad´o (2010) propose an exempl"
D10-1113,J98-1004,0,0.677359,"ur approach is the intuition that word meaning is represented as a probability distribution over a set of latent senses and is modulated by context. Experimental results on lexical substitution and word similarity show that our algorithm outperforms previously proposed models. 1 Introduction The computation of meaning similarity as operationalized by vector-based models has found widespread use in many tasks within natural language processing (NLP). These range from the acquisition of synonyms (Grefenstette, 1994; Lin, 1998) and paraphrases (Lin and Pantel, 2001) to word sense disambiguation (Schuetze, 1998), textual entailment (Clarke, 2009), and notably information retrieval (Salton et al., 1975). The popularity of vector-based models lies in their unsupervised nature and ease of computation. In their simplest incarnation, these models represent the meaning of each word as a point in a high-dimensional space, where each component corresponds to some co-occurring contextual element Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (Landauer and Dumais, 1997; McDonald, 2000; Lund and Burgess, 1996). The advantage of taking such a geometric approach is th"
D10-1113,W09-2506,1,0.937616,"t their meaning invariably (i.e., irrespective of cooccurring context). Consider for example the adjective heavy which we may associate with the general meaning of “dense” or “massive”. However, when attested in context, heavy may refer to an overweight person (e.g., She is short and heavy but she has a heart of gold.) or an excessive cannabis user (e.g., Some heavy users develop a psychological dependence on cannabis.). Recent work addresses this issue indirectly with the development of specialized models that represent word meaning in context (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2009). These methods first extract typical co-occurrence vectors representing a mixture of senses and then use vector operations to either obtain contextualized representations of a target word (Erk and Pad´o, 2008) or a representation for a set of words (Mitchell and Lapata, 2009). In this paper we propose a probabilistic framework for representing word meaning and measuring similarity in context. We model the meaning of isolated words as a probability distribution over a set of latent senses. This distribution reflects the a priori, out-of-context likelihood of each sense. Because sense ambiguity"
D10-1113,P10-1097,0,0.467398,"Missing"
D10-1113,C00-2137,0,0.0251152,"their corresponding vector-based similarity values. We report Spearman’s ρ correlations between the similarity values provided by the models and the mean participant similarity ratings in the Finkelstein et al. (2002) data set. For the lexical substitution task, we compare the system ranking with the gold standard ranking using Kendall’s τb rank correlation (which is adjusted for tied ranks). For all contextualized models we defined the context of a target word as the words occurring within a symmetric context window of size 5. We assess differences between models using stratified shuffling (Yeh, 2000).2 2 Given two system outputs, the null hypothesis (i.e., that the two predictions are indistinguishable) is tested by randomly mixing the individual instances (in our case sentences) of the two outputs. We ran a standard number of 10000 iterations. Model SVS LSA NMF LDA LSAMIX NMFMIX LDAMIX Spearman ρ 38.35 49.43 52.99 53.39 49.76 51.62 51.97 Table 2: Results on out of context word similarity using a simple co-occurrence based vector space model (SVS), latent semantic analysis, non-negative matrix factorization and latent Dirichlet allocation as individual models with the best parameter setti"
D10-1113,C98-2122,0,\N,Missing
D11-1038,W03-1004,0,0.274404,"on, we needed to identify and align modified sentences. We first identified modified sections using the Unix diff program, and then individual sentences within the sections were aligned using the program dwdiff5 . This resulted in 14,831 paired sentences. With regard to the aligned simplification corpus, we paired 15,000 articles from SimpleEW and MainEW following the language link within the snapshot files. Within the paired articles, we identified aligned sentences using macro alignment (at paragraph level) then micro alignment (at sentence level), using tf.idf scores to measure similarity (Barzilay and Elhadad, 2003; Nelken and Schieber, 2006). All source-target sentences (resulting from revisions or alignments) were parsed with the Stanford parser (Klein and Manning, 2003) in order to label the text with syntactic information. QG rules were created by aligning nodes in these sentences as described earlier. A breakdown of the number and type of rules we obtained from the revision and aligned corpora (after removing rules appearing only once) is given in Table 2. Examples of the most frequently learned QG rules are shown in Table 3. Rules (1)–(3) involve syntactic simplification and rules (4)–(6) involve"
D11-1038,E99-1042,0,0.972568,"ource sentence (top) and its simplification (bottom). Introduction Sentence simplification is perhaps one of the oldest text rewriting problems. Given a source sentence, the goal is to create a grammatical target that is easier to read with simpler vocabulary and syntactic structure. An example is shown in Table 1 involving a broad spectrum of rewrite operations such as deletion, substitution, insertion, and reordering. The popularity of the simplification task stems from its potential relevance to various applications. Examples include the development of reading aids for people with aphasia (Carroll et al., 1999), non-native speakers (Siddharthan, 2003) and more generally individuals with low literacy (Watanabe et al., 2009). A simplification component could be also used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004) and semantic role labelers (Vickrey and Koller, 2008). Simplification is related to, but different from paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them to generate new text, in a simpler language. The task is al"
D11-1038,C96-2183,0,0.946573,"er vocabulary and syntactic structure. An example is shown in Table 1 involving a broad spectrum of rewrite operations such as deletion, substitution, insertion, and reordering. The popularity of the simplification task stems from its potential relevance to various applications. Examples include the development of reading aids for people with aphasia (Carroll et al., 1999), non-native speakers (Siddharthan, 2003) and more generally individuals with low literacy (Watanabe et al., 2009). A simplification component could be also used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004) and semantic role labelers (Vickrey and Koller, 2008). Simplification is related to, but different from paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them to generate new text, in a simpler language. The task is also distinct from sentence compression as it aims to render a sentence more accessible while preserving its meaning. On the contrary, compression unavoidably leads to some information loss as it creates shorter sentences without necessarily reducing co"
D11-1038,C08-1018,1,0.793695,"ation operations is sentence splitting which usually produces longer rather than shorter output! Moreover, mod409 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409–420, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics els developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al. 2009 for notable exceptions). In this paper we propose a sentence simplification model that is able to handle structural mismatches and complex rewriting operations. Our approach is based on quasi-synchronous grammar (QG, Smith and Eisner 2006), a formalism that is well suited for text rewriting. Rather than postulating a strictly synchronous structure over the source and target sentences, QG identifies a “sloppy” alignment of parse trees assuming that the target tree is in some way “inspired by” the source tree. Specifically, our model is formulated as an integer linear progr"
D11-1038,P09-1053,0,0.0087456,"a. Compared to Zhu et al., our model is conceptually simpler and more general. The proposed ILP formulation not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to grammaticality and the task at hand without the added computational cost of integrating a language model. Furthermore, our learning framework is not limited to simplification and could be easily adapted to other rewriting tasks. Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al., 2007), and title generation (Woodsend et al., 2010). Finally, our work relates to a large body of recent literature on Wikipedia and its potential for a wide range of NLP tasks. Beyond text rewriting, examples include semantic relatedness (Ponzetto and Strube, 2007), information extraction (Wu and Weld, 2010), ontology induction (Nastase and Strube, 2008), and the automatic creation of overview articles (Sauper and Barzilay, 2009). 3 Sentence Simplification Model Our model takes a single sentence as input and creates a version that is simpler to read. This ma"
D11-1038,W03-1602,0,0.80513,"oth grammatical and meaning preserving. 2 Related Work Sentence simplification has attracted a great deal of attention due to its potential impact on society. The literature is rife with attempts to simplify text using mostly hand-crafted syntactic rules aimed at splitting long and complicated sentences into several simpler ones (Carroll et al., 1999; Chandrasekar et al., 1996; Siddharthan, 2004; Vickrey and Koller, 2008). Other work focuses on lexical simplifications and substitutes difficult words by more common WordNet synonyms or paraphrases found in a predefined dictionary (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). More recently, Yatskar et al. (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. A key idea in their work is to utilize SimpleEW edits, while recognizing that these may serve other functions, such as vandalism removal or introduction of new content. Zhu et al. (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. Inspired by syntax-based SMT (Yamada and Knight, 2001), their model consists of three comp"
D11-1038,P02-1028,0,0.0990883,"meaning preserving. 2 Related Work Sentence simplification has attracted a great deal of attention due to its potential impact on society. The literature is rife with attempts to simplify text using mostly hand-crafted syntactic rules aimed at splitting long and complicated sentences into several simpler ones (Carroll et al., 1999; Chandrasekar et al., 1996; Siddharthan, 2004; Vickrey and Koller, 2008). Other work focuses on lexical simplifications and substitutes difficult words by more common WordNet synonyms or paraphrases found in a predefined dictionary (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). More recently, Yatskar et al. (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. A key idea in their work is to utilize SimpleEW edits, while recognizing that these may serve other functions, such as vandalism removal or introduction of new content. Zhu et al. (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. Inspired by syntax-based SMT (Yamada and Knight, 2001), their model consists of three components: a language m"
D11-1038,P03-1054,0,0.0311685,"clauses, each of which is simplified (lexically and structurally) through QG rewrite rules. We generate all possible simplifications for a given input and use the ILP to find the best target subject to grammaticality constraints. In what follows we first detail how we extract QG rewrite rules as these form the backbone of our model and then formulate the ILP proper. 3.1 Quasi-synchronous Grammar Phrase alignment Our model operates on individual sentences annotated with syntactic information i.e., phrase structure trees. In our experiments, we obtain this information from the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. Given an input sentence S1 or its parse tree T1, the QG constructs a monolingual grammar for parsing, or generating, possible translation trees T2. A grammar node in the target tree T2 is modeled on a subset of nodes in the source tree, with a rather loose alignment between the trees. We take aligned sentence pairs represented as phrase structure trees and build up a list of leaf node alignments based on lexical identity. We align direct parent nodes where more than one child node aligns. QG rules are created from aligned nodes above"
D11-1038,E06-1021,0,0.00961054,"d align modified sentences. We first identified modified sections using the Unix diff program, and then individual sentences within the sections were aligned using the program dwdiff5 . This resulted in 14,831 paired sentences. With regard to the aligned simplification corpus, we paired 15,000 articles from SimpleEW and MainEW following the language link within the snapshot files. Within the paired articles, we identified aligned sentences using macro alignment (at paragraph level) then micro alignment (at sentence level), using tf.idf scores to measure similarity (Barzilay and Elhadad, 2003; Nelken and Schieber, 2006). All source-target sentences (resulting from revisions or alignments) were parsed with the Stanford parser (Klein and Manning, 2003) in order to label the text with syntactic information. QG rules were created by aligning nodes in these sentences as described earlier. A breakdown of the number and type of rules we obtained from the revision and aligned corpora (after removing rules appearing only once) is given in Table 2. Examples of the most frequently learned QG rules are shown in Table 3. Rules (1)–(3) involve syntactic simplification and rules (4)–(6) involve sentence splitting. Examples"
D11-1038,P02-1040,0,0.0866269,"ext (MainEW: input source, SimpleEW: simplified target). to the source. We evaluated model output in two ways, using automatic evaluation measures and human judgments. Intuitively, readability measures ought to be suitable for assessing the output of simplification systems. We report results with the well-known FleschKincaid Grade Level index (FKGL). Experiments with other readability measures such as the Flesch Reading Ease and the Coleman-Liau index obtained similar results. In addition, we also assessed how the system output differed from the human SimpleEW gold standard by computing BLEU (Papineni et al., 2002) and TERp (Snover et al., 2009). Both measures are commonly used to automatically evaluate the quality of machine translation output. BLEU9 scores the target output by counting n-gram matches with the reference, whereas TERp is similar to word error rate, the only difference being that it allows shifts and thus can account for word order differences. TERp also allows for stem, synonym, and paraphrase substitutions which are common rewrite operations in simplification. In line with previous work on text rewriting (e.g., Knight and Marcu 2002) we also evaluated system output by eliciting human j"
D11-1038,P09-1024,0,0.00751374,"d, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al., 2007), and title generation (Woodsend et al., 2010). Finally, our work relates to a large body of recent literature on Wikipedia and its potential for a wide range of NLP tasks. Beyond text rewriting, examples include semantic relatedness (Ponzetto and Strube, 2007), information extraction (Wu and Weld, 2010), ontology induction (Nastase and Strube, 2008), and the automatic creation of overview articles (Sauper and Barzilay, 2009). 3 Sentence Simplification Model Our model takes a single sentence as input and creates a version that is simpler to read. This may involve rendering syntactically complex structures simpler (e.g., through sentence splitting), or substituting rare words with more common words or phrases (e.g., such that a second language learner 411 may be familiar with), or deleting elements of the original text in order to produce a relatively simpler and shallower syntactic structure. In addition, the output must be grammatical and coherent. These constraints are global in their scope, and cannot be adequa"
D11-1038,W06-3104,0,0.785809,"1. 2011 Association for Computational Linguistics els developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al. 2009 for notable exceptions). In this paper we propose a sentence simplification model that is able to handle structural mismatches and complex rewriting operations. Our approach is based on quasi-synchronous grammar (QG, Smith and Eisner 2006), a formalism that is well suited for text rewriting. Rather than postulating a strictly synchronous structure over the source and target sentences, QG identifies a “sloppy” alignment of parse trees assuming that the target tree is in some way “inspired by” the source tree. Specifically, our model is formulated as an integer linear program and uses QG to capture the space of all possible rewrites. Given a source tree, it finds the best target tree licensed by the grammar subject to constraints such as sentence length and reading ease. Our model is conceptually simple and computationally effici"
D11-1038,D09-1086,0,0.0122371,"earn appropriate rules that reflect the training data. Compared to Zhu et al., our model is conceptually simpler and more general. The proposed ILP formulation not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to grammaticality and the task at hand without the added computational cost of integrating a language model. Furthermore, our learning framework is not limited to simplification and could be easily adapted to other rewriting tasks. Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al., 2007), and title generation (Woodsend et al., 2010). Finally, our work relates to a large body of recent literature on Wikipedia and its potential for a wide range of NLP tasks. Beyond text rewriting, examples include semantic relatedness (Ponzetto and Strube, 2007), information extraction (Wu and Weld, 2010), ontology induction (Nastase and Strube, 2008), and the automatic creation of overview articles (Sauper and Barzilay, 2009). 3 Sentence Simplification Model Our model takes a single sentence as input and c"
D11-1038,W09-0441,0,0.0110719,"EW: simplified target). to the source. We evaluated model output in two ways, using automatic evaluation measures and human judgments. Intuitively, readability measures ought to be suitable for assessing the output of simplification systems. We report results with the well-known FleschKincaid Grade Level index (FKGL). Experiments with other readability measures such as the Flesch Reading Ease and the Coleman-Liau index obtained similar results. In addition, we also assessed how the system output differed from the human SimpleEW gold standard by computing BLEU (Papineni et al., 2002) and TERp (Snover et al., 2009). Both measures are commonly used to automatically evaluate the quality of machine translation output. BLEU9 scores the target output by counting n-gram matches with the reference, whereas TERp is similar to word error rate, the only difference being that it allows shifts and thus can account for word order differences. TERp also allows for stem, synonym, and paraphrase substitutions which are common rewrite operations in simplification. In line with previous work on text rewriting (e.g., Knight and Marcu 2002) we also evaluated system output by eliciting human judgments. We conducted three ex"
D11-1038,P08-1040,0,0.0897748,"rewrite operations such as deletion, substitution, insertion, and reordering. The popularity of the simplification task stems from its potential relevance to various applications. Examples include the development of reading aids for people with aphasia (Carroll et al., 1999), non-native speakers (Siddharthan, 2003) and more generally individuals with low literacy (Watanabe et al., 2009). A simplification component could be also used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004) and semantic role labelers (Vickrey and Koller, 2008). Simplification is related to, but different from paraphrase extraction (Barzilay, 2003). We must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them to generate new text, in a simpler language. The task is also distinct from sentence compression as it aims to render a sentence more accessible while preserving its meaning. On the contrary, compression unavoidably leads to some information loss as it creates shorter sentences without necessarily reducing complexity. In fact, one of the commonest simplification operations is sentence splitting which usual"
D11-1038,D07-1003,0,0.00707423,"nceptually simpler and more general. The proposed ILP formulation not only allows to efficiently search through the space of many QG rules but also to incorporate constraints relating to grammaticality and the task at hand without the added computational cost of integrating a language model. Furthermore, our learning framework is not limited to simplification and could be easily adapted to other rewriting tasks. Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al., 2007), and title generation (Woodsend et al., 2010). Finally, our work relates to a large body of recent literature on Wikipedia and its potential for a wide range of NLP tasks. Beyond text rewriting, examples include semantic relatedness (Ponzetto and Strube, 2007), information extraction (Wu and Weld, 2010), ontology induction (Nastase and Strube, 2008), and the automatic creation of overview articles (Sauper and Barzilay, 2009). 3 Sentence Simplification Model Our model takes a single sentence as input and creates a version that is simpler to read. This may involve rendering syntactically comple"
D11-1038,D10-1050,1,0.571765,"Missing"
D11-1038,P10-1013,0,0.00571616,"our learning framework is not limited to simplification and could be easily adapted to other rewriting tasks. Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al., 2007), and title generation (Woodsend et al., 2010). Finally, our work relates to a large body of recent literature on Wikipedia and its potential for a wide range of NLP tasks. Beyond text rewriting, examples include semantic relatedness (Ponzetto and Strube, 2007), information extraction (Wu and Weld, 2010), ontology induction (Nastase and Strube, 2008), and the automatic creation of overview articles (Sauper and Barzilay, 2009). 3 Sentence Simplification Model Our model takes a single sentence as input and creates a version that is simpler to read. This may involve rendering syntactically complex structures simpler (e.g., through sentence splitting), or substituting rare words with more common words or phrases (e.g., such that a second language learner 411 may be familiar with), or deleting elements of the original text in order to produce a relatively simpler and shallower syntactic structure."
D11-1038,P01-1067,0,0.0174306,"in a predefined dictionary (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). More recently, Yatskar et al. (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. A key idea in their work is to utilize SimpleEW edits, while recognizing that these may serve other functions, such as vandalism removal or introduction of new content. Zhu et al. (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. Inspired by syntax-based SMT (Yamada and Knight, 2001), their model consists of three components: a language model P(s) whose role is to guarantee that the simplification output is grammatical, a direct translation model P(s|c) capturing the probability that the target sentence s is a simpler version of the source c, and a decoder which searches for the simplification s which maximizes P(s)P(s|c). The translation model is the product of the aforementioned four rewrite operations whose probabilities are estimated from a parallel corpus of MainEW and SimpleEW sentences using an expectation maximization algorithm. Their decoder translates sentences"
D11-1038,P08-2035,0,0.0182487,"t target tree licensed by the grammar subject to constraints such as sentence length and reading ease. Our model is conceptually simple and computationally efficient. Furthermore, it finds globally optimal simplifications without resorting to heuristics or approximations during the decoding process. Contrary to most previous approaches (see the discussion in Section 2) which rely heavily on hand-crafted rules, our model learns simplification rewrites automatically from examples of source-target sentences. Our work joins others in using Wikipedia to extract data appropriate for model training (Yamangil and Nelken, 2008; Yatskar et al., 2010; Zhu et al., 2010). Advantageously, the Simple English Wikipedia (henceforth SimpleEW) provides a large repository of simplified language; it uses fewer words and simpler grammar than the ordinary English Wikipedia (henceforth MainEW) and is aimed at non-native English speakers, children, translators, people with learning disabilities or low reading proficiency. We exploit Wikipedia and create a (parallel) simplification corpus in two ways: by aligning MainEW sentences to their SimpleEW counterparts, and by extracting training instances from SimpleEW revision histories,"
D11-1038,N10-1056,0,0.75983,"he grammar subject to constraints such as sentence length and reading ease. Our model is conceptually simple and computationally efficient. Furthermore, it finds globally optimal simplifications without resorting to heuristics or approximations during the decoding process. Contrary to most previous approaches (see the discussion in Section 2) which rely heavily on hand-crafted rules, our model learns simplification rewrites automatically from examples of source-target sentences. Our work joins others in using Wikipedia to extract data appropriate for model training (Yamangil and Nelken, 2008; Yatskar et al., 2010; Zhu et al., 2010). Advantageously, the Simple English Wikipedia (henceforth SimpleEW) provides a large repository of simplified language; it uses fewer words and simpler grammar than the ordinary English Wikipedia (henceforth MainEW) and is aimed at non-native English speakers, children, translators, people with learning disabilities or low reading proficiency. We exploit Wikipedia and create a (parallel) simplification corpus in two ways: by aligning MainEW sentences to their SimpleEW counterparts, and by extracting training instances from SimpleEW revision histories, thus leveraging Wikipe"
D11-1038,P09-1094,0,0.00871969,"nce splitting which usually produces longer rather than shorter output! Moreover, mod409 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409–420, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics els developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al. 2009 for notable exceptions). In this paper we propose a sentence simplification model that is able to handle structural mismatches and complex rewriting operations. Our approach is based on quasi-synchronous grammar (QG, Smith and Eisner 2006), a formalism that is well suited for text rewriting. Rather than postulating a strictly synchronous structure over the source and target sentences, QG identifies a “sloppy” alignment of parse trees assuming that the target tree is in some way “inspired by” the source tree. Specifically, our model is formulated as an integer linear program and uses QG to cap"
D11-1038,C10-1152,0,0.361267,"constraints such as sentence length and reading ease. Our model is conceptually simple and computationally efficient. Furthermore, it finds globally optimal simplifications without resorting to heuristics or approximations during the decoding process. Contrary to most previous approaches (see the discussion in Section 2) which rely heavily on hand-crafted rules, our model learns simplification rewrites automatically from examples of source-target sentences. Our work joins others in using Wikipedia to extract data appropriate for model training (Yamangil and Nelken, 2008; Yatskar et al., 2010; Zhu et al., 2010). Advantageously, the Simple English Wikipedia (henceforth SimpleEW) provides a large repository of simplified language; it uses fewer words and simpler grammar than the ordinary English Wikipedia (henceforth MainEW) and is aimed at non-native English speakers, children, translators, people with learning disabilities or low reading proficiency. We exploit Wikipedia and create a (parallel) simplification corpus in two ways: by aligning MainEW sentences to their SimpleEW counterparts, and by extracting training instances from SimpleEW revision histories, thus leveraging Wikipedia’s collaborative"
D11-1122,P10-1024,0,0.060663,"labeled instances. Their method starts with a dataset containing no role annotations at all, but crucially relies on VerbNet (Kipper et al., 2000) for identifying the arguments of predicates and making initial role assignments. VerbNet is a manually constructed lexicon of verb classes each of which is explicitly associated with argument realization and semantic role specifications. Subsequent work has focused on unsupervised methods for argument identification and classification. Abend et al. (2009) recognize the arguments of predicates by relying solely on part of speech annotations whereas Abend and Rappoport (2010) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. Grenager and Manning (2006) address the role induction problem and propose a directed graphical model which relates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) formulate role induction as the process of detecting alternations and finding a canonical syntactic form for them. Verbal"
D11-1122,P09-1004,0,0.397308,"rapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method starts with a dataset containing no role annotations at all, but crucially relies on VerbNet (Kipper et al., 2000) for identifying the arguments of predicates and making initial role assignments. VerbNet is a manually constructed lexicon of verb classes each of which is explicitly associated with argument realization and semantic role specifications. Subsequent work has focused on unsupervised methods for argument identification and classification. Abend et al. (2009) recognize the arguments of predicates by relying solely on part of speech annotations whereas Abend and Rappoport (2010) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. Grenager and Manning (2006) address the role induction problem and propose a directed graphical model which relates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) f"
D11-1122,N09-1014,0,0.0319469,"n in a novel way, as a graph partitioning problem. Our method is simple, computationally efficient, and does not rely on hidden variables. Moreover, the graph-based representation for verbs and their arguments affords greater modeling flexibility. A wide range of methods exist for finding partitions in graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. However, we leave this to future work. gation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances. Label information is then propagated between the vertices in such a way that similar instances tend to be assigned the same label. Analogously, Chinese Whispers works by propagating cluster membership information along the edges of a graph, even though the graph"
D11-1122,W06-3812,0,0.685049,"his paper describes an unsupervised method for semantic role induction, i.e., one that does not require any role annotated data or additional semantic resources for training. Contrary to these previous approaches, we conceptualize role induction in a novel way, as a graph partitioning problem. Our method is simple, computationally efficient, and does not rely on hidden variables. Moreover, the graph-based representation for verbs and their arguments affords greater modeling flexibility. A wide range of methods exist for finding partitions in graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. However, we leave this to future work. gation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances."
D11-1122,W10-2301,0,0.0231944,"ment classification is more challenging and must take into account syntactic as well as lexical-semantic information. Both types of information are incorporated into our model through a similarity function that assigns similarity scores to pairs of argument instances. Following previous work (Lang and Lapata, 2010; Grenager and Manning, 2006), our system outputs verb-specific roles by grouping argument instances into clusters and labeling each argument instance with an identifier corGraph-based methods are popular in natural language processing, especially with unsupervised learning problems (Chen and Ji, 2010). The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the 2 For example, Haffari and Sarkar (2007) use label propaname “clustering by propagation”. The term makes gation to analyze other semi-supervised algorithms such as the explicit the algorithm’s connection to label propa- Yarowsky (1995) algorithm. 1322 responding to the cluster it has been as"
D11-1122,D10-1056,0,0.0121453,"ns similarity scores to pairs of argument instances. Following previous work (Lang and Lapata, 2010; Grenager and Manning, 2006), our system outputs verb-specific roles by grouping argument instances into clusters and labeling each argument instance with an identifier corGraph-based methods are popular in natural language processing, especially with unsupervised learning problems (Chen and Ji, 2010). The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the 2 For example, Haffari and Sarkar (2007) use label propaname “clustering by propagation”. The term makes gation to analyze other semi-supervised algorithms such as the explicit the algorithm’s connection to label propa- Yarowsky (1995) algorithm. 1322 responding to the cluster it has been assigned to. Such identifiers are similar to PropBank-style core labels (e.g., A0, A1). After identifying likely arguments for each verb, the next step is to infer a label for each argument instance. Since we aim to induce verb-spec"
D11-1122,D09-1002,1,0.654116,"Missing"
D11-1122,J02-3001,0,0.574114,"e the task as a supervised learning problem and rely on role-annotated data for model training. Most of these systems implement a two-stage architecture consisting of argument identification (determining the arguments of the verbal predicate) and argument classification (labeling these arguments with semantic roles). Despite being relatively shallow, seRecent years have seen increased interest in the shallow semantic analysis of natural language text. The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the semantic relations that hold between a predicate and its arguments (e.g., “who” did “what” to “whom”, “when”, “where”, and “how”) abstracting over surface syntactic configurations. In the example sentences below, window occupies different syntactic positions — it is the object of 1 More precisely, A0 and A1 have a common interpretation broke in sentences (1a,b), and the subject in (1c) — across predicates as proto-agent and proto-patient in the sense while bearing the same semantic role, i.e., the phys- of Dowty (1991). 1320 Proceedings of the 2011 Conference on E"
D11-1122,P07-1025,0,0.0406166,"e bulk of previous work on semantic role labeling has primarily focused on supervised methods (M`arquez et al., 2008), a few semi-supervised and unsupervised approaches have been proposed in the literature. The majority of semi-supervised models have been developed within a framework known as annotation projection. The idea is to combine labeled and unlabeled data by projecting annotations from a labeled source sentence onto an unlabeled target sentence within the same language (F¨urstenau and Lapata, 2009) or across different languages (Pad´o and Lapata, 2009). Outwith annotation projection, Gordon and Swanson (2007) propose to increase the coverage of PropBank to unseen verbs by finding syntactically similar (labeled) verbs and using their annotations as surrogate training data. Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system. Their algorithm induces role labels following a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method starts with a dataset containing no role annotations at all, but crucially relies on VerbNet (Kipper et al., 2000) for identifying"
D11-1122,W06-1601,0,0.68948,"Kipper et al., 2000) for identifying the arguments of predicates and making initial role assignments. VerbNet is a manually constructed lexicon of verb classes each of which is explicitly associated with argument realization and semantic role specifications. Subsequent work has focused on unsupervised methods for argument identification and classification. Abend et al. (2009) recognize the arguments of predicates by relying solely on part of speech annotations whereas Abend and Rappoport (2010) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. Grenager and Manning (2006) address the role induction problem and propose a directed graphical model which relates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) formulate role induction as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific"
D11-1122,D10-1073,0,0.0500291,"Missing"
D11-1122,N10-1137,1,0.895191,"ion. Abend et al. (2009) recognize the arguments of predicates by relying solely on part of speech annotations whereas Abend and Rappoport (2010) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. Grenager and Manning (2006) address the role induction problem and propose a directed graphical model which relates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) formulate role induction as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Lang and Lapata (2011) propose a clustering algorithm which first splits the argument instances of a verb into fine-grained clusters based"
D11-1122,P11-1112,1,0.74935,"rguments and role induction corresponds to inferring the state of these latent variables. Following up on this work, Lang and Lapata (2010) formulate role induction as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Lang and Lapata (2011) propose a clustering algorithm which first splits the argument instances of a verb into fine-grained clusters based on syntactic cues and then executes a series of merge steps (mainly) based on lexical cues. The split phase creates a large number of small clusters with high purity but low collocation, i.e., while the instances in a particular cluster typically belong to the same role the instances for a particular role are commonly scattered amongst many clusters. The subsequent merge phase conflates clusters with the same role in order to increase collocation. Like Grenager and Manning (2006"
D11-1122,J08-2001,0,0.199396,"Missing"
D11-1122,J05-1004,0,0.194372,"odel. Experimental results on the CoNLL 2008 benchmark dataset demonstrate that our model is competitive with other unsupervised approaches in terms of F1 whilst attaining significantly higher cluster purity. 1 Introduction ical object affected by the breaking event. Analogously, ball is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. b. c. [Jim]A0 broke the [window]A1 with a [ball]A2 . The [ball]A2 broke the [window]A1 . The [window]A1 broke [last night]TMP . The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broadcoverage human-annotated corpus of semantic roles and their syntactic realizations. Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles such as location or time whose interpretation is common across predicates (e.g., last night in sentence (1c)). The availability of PropBank and related resources (e.g., FrameNet; Ruppenhofer et al. (2006)) has sparked the development of great many semantic"
D11-1122,J08-2006,0,0.661615,"Missing"
D11-1122,P05-1077,0,0.0259844,"axis) on the auto/auto dataset. Recall that one of the components in our similarity function is lexical similarity which we measure using a vector-based model (see Section 5.4). We created such a model from the Google N-Grams corpus (Brants and Franz, 2006) using a context window of two words on both sides of the target word and co-occurrence frequencies as vector components (no weighting was applied). The large size of this corpus allows us to use bigram frequencies, rather than frequencies of individual words and to distinguish between left and right bigrams. We used randomized algorithms (Ravichandran et al., 2005) to build the semantic space efficiently. Comparison Models We compared our graph partitioning algorithm against three competitive approaches. The first one assigns argument instances to clusters according to their syntactic function (e.g., subject, object) as determined by a parser. This baseline has been previously used as a point of comparison by other unsupervised semantic role induction systems (Grenager and Manning, 2006; Lang and Lapata, 2010) and shown difficult to outperform. 1327 200 300 400 500 Number of iterations Average number of clusters per verb Figure 4: F1 (vertical axis) aga"
D11-1122,D07-1002,1,0.905458,"Missing"
D11-1122,D10-1017,0,0.0163674,"n variables. Moreover, the graph-based representation for verbs and their arguments affords greater modeling flexibility. A wide range of methods exist for finding partitions in graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. However, we leave this to future work. gation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances. Label information is then propagated between the vertices in such a way that similar instances tend to be assigned the same label. Analogously, Chinese Whispers works by propagating cluster membership information along the edges of a graph, even though the graph does not contain any human-labeled instance vertices. 3 Problem Setting We adopt the standard architecture of superv"
D11-1122,P03-1002,0,0.304894,"Missing"
D11-1122,P10-1149,0,0.0328081,"od is simple, computationally efficient, and does not rely on hidden variables. Moreover, the graph-based representation for verbs and their arguments affords greater modeling flexibility. A wide range of methods exist for finding partitions in graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. However, we leave this to future work. gation, a general framework2 for semi-supervised learning (Zhu et al., 2003) with applications to machine translation (Alexandrescu and Kirchhoff, 2009), information extraction (Talukdar and Pereira, 2010) and structured part-of-speech tagging (Subramanya et al., 2010). The basic idea behind label propagation is to represent labeled and unlabeled instances as vertices in an undirected graph with edges whose weights express similarity (and possibly dissimilarity) between the instances. Label information is then propagated between the vertices in such a way that similar instances tend to be assigned the same label. Analogously, Chinese Whispers works by propagating cluster membership information along the edges of a graph, even though the graph does not contain any human-labeled instance vertices"
D11-1122,N09-2004,0,0.195271,"Missing"
D11-1122,P95-1026,0,0.368255,"sing, especially with unsupervised learning problems (Chen and Ji, 2010). The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al., 2010). The same algorithm is also described in Abney (2007, pp. 146-147) under the 2 For example, Haffari and Sarkar (2007) use label propaname “clustering by propagation”. The term makes gation to analyze other semi-supervised algorithms such as the explicit the algorithm’s connection to label propa- Yarowsky (1995) algorithm. 1322 responding to the cluster it has been assigned to. Such identifiers are similar to PropBank-style core labels (e.g., A0, A1). After identifying likely arguments for each verb, the next step is to infer a label for each argument instance. Since we aim to induce verb-specific roles (see Section 3), we construct an undirected, weighted graph for each verb. Vertices correspond to verb argument instances and edge weights quantify the similarities between them. This argument-instance graph is then partitioned into clusters of vertices representing semantic roles and each argument in"
D11-1122,W04-3213,0,\N,Missing
D11-1122,W08-2121,0,\N,Missing
D11-1122,N07-1070,0,\N,Missing
D12-1022,P11-1049,0,0.744386,"(Hochba, 1997). 233 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 233–243, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics sum of redundancy scores of all pairs of selected sentences. Gillick et al. (2008) develop an exact solution for a model similar to Filatova and Hatzivassiloglou (2004) under the assumption that the value of a summary is the sum of values of the unique concepts (approximated by bigrams) it contains. Subsequent work (Gillick et al., 2009; Berg-Kirkpatrick et al., 2011) extends this model to allow sentence compression in the form of word or constituent deletion. In this paper we propose a model for multidocument summarization that attempts to cover many different aspects of the task such as content selection, surface realization, paraphrasing, and stylistic conventions. These aspects are learned separately using specific “expert” predictors, but are optimized jointly using an integer linear programming model (ILP) to generate the output summary.2 All experts are learned from data without requiring additional annotation over and above the summaries written fo"
D12-1022,C08-1018,1,0.77637,"s advantageous as it renders inference simpler and affords flexibility (e.g., additional aspects can be incorporated into the model or trained separately on different datasets). Our work differs from Gillick et al. (2009) and Berg-Kirkpatrick et al. (2011) in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries. Unlike previous applications of STSG to sentence compression (Cohn and Lapata, 2009; Cohn and Lapata, 2008) our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two (Smith and Eisner, 2006), and is therefore well suited to describing the relationship between documents and their abstracts. Secondly, our content selection component extends to features beyond the bigram horizon, as we learn to identify important concepts based on syntactic and positional information. We also learn which words are unlikely to appear in a summary. Thirdly, unlike Berg-Kirkpatrick et al. (2011) our model does not"
D12-1022,P03-2041,0,0.0197953,"the use of unique bigram information to model content and avoid redundancy, positional information to model important and poor locations of content, and language modeling to capture stylistic conventions. Learning each predictor separately gives better generalization, while the ILP framework allows us to combine the decisions of the expert learners through the use of objectives, hard and soft constraints. The experts work collaboratively to rewrite the content using rules extracted from document clusters and model summaries. We adopt the synchronous tree substitution grammar (STSG) formalism (Eisner, 2003) which can model non-isomorphic tree structures (the grammar rules can comprise trees of arbitrary depth) and is thus suited to text-rewriting tasks which typically involve a number of local modifications to the input text. Specifically, we propose quasi-synchronous tree substitution grammar (QTSG) as a flexible formalism to learn general treeedits from loosely-aligned phrase structure trees. We evaluate our model on the 100-word “non2 Our task is standard multi-document summarization and should not be confused with “guided” summarization where system and human summarizers are given a list of"
D12-1022,C04-1057,0,0.145564,"a (i.e., document clusters and their corresponding summaries). This global inference problem is, however, hard — the solution space is large and the lack of easily accessible datasets an obstacle to joint learning. It is thus no surprise that previous work has focused on specific aspects of joint learning. Initial global formulations of the multi-document summarization task focused on extractive summarization and used approximate greedy algorithms for finding the sentences of the summary. Goldstein et al. (2000) search for the set of sentences that are both relevant and non-redundant, whereas Filatova and Hatzivassiloglou (2004) model multi-document summarization as an instance of the maximum coverage set problem.1 More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting. McDonald (2007) proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the 1 Given C, a finite set of weighted elements, a collection T of subsets of C, and an integer k, find those k sets that maximize the total number of elements in the union of T ’s members (Hochba, 1997). 233 Proceedings of the 2012 Joint"
D12-1022,W09-1802,0,0.462298,"gs of summary grammaticality and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008)"
D12-1022,W00-0405,0,0.575101,"ns. An ideal model would learn to output summaries that simultaneously meet all these constraints from data (i.e., document clusters and their corresponding summaries). This global inference problem is, however, hard — the solution space is large and the lack of easily accessible datasets an obstacle to joint learning. It is thus no surprise that previous work has focused on specific aspects of joint learning. Initial global formulations of the multi-document summarization task focused on extractive summarization and used approximate greedy algorithms for finding the sentences of the summary. Goldstein et al. (2000) search for the set of sentences that are both relevant and non-redundant, whereas Filatova and Hatzivassiloglou (2004) model multi-document summarization as an instance of the maximum coverage set problem.1 More recent work improves on the search problem by considering exact solutions and permits a limited amount of rewriting. McDonald (2007) proposes an integer linear programming formulation that maximizes the sum of relevance scores of the selected sentences penalized by the 1 Given C, a finite set of weighted elements, a collection T of subsets of C, and an integer k, find those k sets tha"
D12-1022,J02-4006,0,0.0137295,"ation. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP model for multi-document summarization. A key assumption in their model which we also follow is that input documents contain a variety of concepts, each of which are allocated a value, and the goal of a good summary is to maximize the sum of these values subject to the length constraint. The authors use bigrams as concepts and their frequency in the"
D12-1022,P03-1054,0,0.00282957,"ternative compressions and paraphrases of the source sentences, in the style suitable for the summaries. Finally, an ILP model combines the output of these components into a summary, jointly optimizing content selection and surface realization preferences, and providing the flexibility to treat some components as soft while others as hard constraints. 3.1 Document Representation Given an input sentence, our approach deconstructs it into component phrases and clauses, typical of a phrase structure parser. In our experiments, we obtain this representation from the output of the Stanford parser (Klein and Manning, 2003) but any other broadly similar parser could be used instead. Nodes in the parse tree represent points where QTSG rules can be applied (and paraphrases generated), and they also represent decision points for the ILP. In the following, we will refer to these decision nodes as the set N , and decisions for each node using the binary variable zi , i ∈ N . 3.2 Content Selection Using Bigrams We follow Gillick et al. (2008) in modeling the information content of the summary as the weighted sum of the individual information units it contains. We represent information units as the set of bigrams B see"
D12-1022,N03-1020,0,0.22049,"LR . Alternative sub-trees (b), (c) and (d) are created using QTSG rules (dashed lines). The output sentence (see Table 6) was generated from sub-trees (b) and (d). al. (2011) (henceforth B-K), which has the highest reported ROUGE scores that we are aware of.5 In addition to the full model described in Section 3, we also produced outputs where each of the five components described in Sections 3.2–3.6 were removed, to assess their individual contribution. We evaluated the output summaries in two ways, using automatic measures and human judgements. Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008 parameter settings. We report bigram overlap (ROUGE -2) and skip-bigram (ROUGE -SU4) recall values. We also used Translation Edit Rate (T ER, Snover et al. (2006)) to examine the systems’ rewrite potential. T ER is defined as the minimum number of edits (insertions, deletions, substitutions, and shifts) required to change the system output so that it exactly matches a reference (here, the reference is the most closely aligning source sentence). The perfect TER score is 0, however note that it can be higher than 1 due to insertions. Our judgement elicitation study was conducted"
D12-1022,W01-0100,0,0.769027,"the TAC-08 data set show that our model achieves state-of-the-art performance using ROUGE and significantly improves the informativeness of the summaries. 1 Introduction Automatic summarization has enjoyed wide popularity in natural language processing (see the proceedings of the Document Understanding and Text Analysis conferences) due to its potential for practical applications but also because it incorporates many important aspects of both natural language understanding and generation. Of the many summarization paradigms that have been identified over the years (see Sparck Jones (1999) and Mani (2001) for comprehensive overviews), multi-document summarization — the task of producing summaries from clusters of thematically related documents — has consistently attracted attention. Despite considerable research effort, the automatic generation of multi-document summaries that resemble those written by humans remains challenging. This is primarily due to the task itself which is complex and subject to several constraints: the summary must be maximally informative and minimally redundant, grammatical, coherent, adhere to a pre-specified length and stylistic conventions. An ideal model would lea"
D12-1022,W09-1801,0,0.440368,"of ROUGE and human ratings of summary grammaticality and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest"
D12-1022,E06-1021,0,0.0177901,"Missing"
D12-1022,W06-3104,0,0.0216319,"fers from Gillick et al. (2009) and Berg-Kirkpatrick et al. (2011) in three important respects. Firstly, we develop a genuinely abstractive model that is not limited to deletion. Our rewrite rules are encoded in quasi-synchronous tree substitution grammar and learned automatically from source documents and their summaries. Unlike previous applications of STSG to sentence compression (Cohn and Lapata, 2009; Cohn and Lapata, 2008) our quasi-synchronous TSG does not attempt to learn the complete translation from source to target sentence; it only loosely links the syntactic structure of the two (Smith and Eisner, 2006), and is therefore well suited to describing the relationship between documents and their abstracts. Secondly, our content selection component extends to features beyond the bigram horizon, as we learn to identify important concepts based on syntactic and positional information. We also learn which words are unlikely to appear in a summary. Thirdly, unlike Berg-Kirkpatrick et al. (2011) our model does not try to learn all the parameters (e.g., content, rewrite rules, style) of the summarization problem jointly; although decoupling learning from inference is perhaps less elegant from a modeling"
D12-1022,2006.amta-papers.25,0,0.0126082,"henceforth B-K), which has the highest reported ROUGE scores that we are aware of.5 In addition to the full model described in Section 3, we also produced outputs where each of the five components described in Sections 3.2–3.6 were removed, to assess their individual contribution. We evaluated the output summaries in two ways, using automatic measures and human judgements. Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008 parameter settings. We report bigram overlap (ROUGE -2) and skip-bigram (ROUGE -SU4) recall values. We also used Translation Edit Rate (T ER, Snover et al. (2006)) to examine the systems’ rewrite potential. T ER is defined as the minimum number of edits (insertions, deletions, substitutions, and shifts) required to change the system output so that it exactly matches a reference (here, the reference is the most closely aligning source sentence). The perfect TER score is 0, however note that it can be higher than 1 due to insertions. Our judgement elicitation study was conducted as follows. We randomly selected ten document 5 We are grateful to Taylor Berg-Kirkpatrick for making his system output available to us. 240 clusters from the test set and genera"
D12-1022,P10-1058,1,0.877631,"ity and informativeness. Importantly, there is nothing inherent in our model that is specific to this particular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP mod"
D12-1022,D10-1050,1,0.823061,"ular summarization task. As all of the different experts are learned from data, it could easily adapt to other summarization styles or conventions as needed. 2 Related work Recent years have seen increased interest in global inference methods for summarization. ILP-based models have been developed for several subtasks ranging from sentence compression (Clarke and Lapata, 2008), to single- and multi-document summarization (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), and headline generation (Deshpande et al., 2007; Woodsend et al., 2010). Most of these approaches are either purely extractive or implement a single rewrite operation, namely word deletion. Although it is well-known that hand-written summaries often exhibit additional edits and sentence recombinations (Jing, 2002), the challenges involved in acquiring the rewrite rules, interfacing them with inference, and ensuring grammatical output make the development of abstractive models non-trivial. Our work is closest to Gillick et al. (2008) who also develop an ILP model for multi-document summarization. A key assumption in their model which we also follow is that input d"
D12-1022,N07-1056,0,\N,Missing
D12-1050,J10-4006,0,0.583045,"function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate. In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition. These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), 547 a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008). Word embeddings are distributed representations, low-dimensional and real-valued. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks. We assess the effectiveness of these models using two evaluation protocols. The first one involves modeling similarity judgments for short ph"
D12-1050,D10-1115,0,0.684484,"velop a compositional distributional semantics that brings type-logical and distributional vector space models together. In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces. The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then"
D12-1050,J12-1002,0,0.0653101,"logical and distributional vector space models together. In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces. The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space. Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit). Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008). Socher et al. (2011a) and Socher et al. (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences. The network is given a list of word vectors as input and a binary tree representing their syntactic structure. Then, it computes an n-dimensional representation p o"
D12-1050,P09-1053,0,0.588781,"ved, and their unigram word overlap. Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset. Initial research concentrated on individual words rather than sentential representations. Several approaches used Model Baseline Mihalcea et al. (2006) Rus et al. (2008) Qiu et al. (2006) Islam and Inkpen (2007) Mitchell and Lapata (2010) ( ) Baroni and Lenci (2010) (+) Fernando and Stevenson (2008) Wan et al. (2006) Das and Smith (2009) Socher et al. (2011a) Acc. 66.5 70.3 70.6 72.0 72.6 73.0 73.5 74.1 75.6 76.1 76.8 F1 79.9 81.3 80.5 81.6 81.3 82.3 82.2 82.4 83.0 82.7 83.6 which if added could increase performance. 5 Table 6: Overview of results on the MSRCP (test corpus). Accuracy differences of 3.3 or more are significant at the 0.01 level (using the χ2 statistic). WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008). More recently, the addition of syntactic features based on dependen"
D12-1050,C04-1051,0,0.55834,"Missing"
D12-1050,D11-1129,0,0.352841,"Missing"
D12-1050,W11-2507,0,0.303069,"Missing"
D12-1050,P03-1054,0,0.00410739,"odels can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b). We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011). Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model. They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence. This tree is then used as the basis for a deep recursive autoencoder (RAE). The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors. The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008). Given the binary tree input structure, the model computes parent representations p from their children (c1 , c2 ) using a standard neural network layer: p = f (W ("
D12-1050,W06-1603,0,0.085535,"Missing"
D12-1050,J98-1004,0,0.679473,"Missing"
D12-1050,D11-1014,0,0.0324061,"Missing"
D12-1050,P10-1040,0,0.121509,"Missing"
D12-1050,J06-3003,0,0.058723,"judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL). This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006). While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences. The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995). More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces. This renewed interes"
D12-1050,U06-1019,0,0.413994,"Missing"
D12-1130,W11-2503,0,0.309982,"Missing"
D12-1130,W01-0514,0,0.0239894,"ioral experiments and neuroimaging studies that the perceptual associates of words play an important role in language processing (for a review see Barsalou (2008)). Introduction Distributional models of lexical semantics have seen considerable success at accounting for a wide range of behavioral data in tasks involving semantic cognition (Landauer and Dumais, 1997; Griffiths et al., 2007). These models have also enjoyed lasting popularity in natural language processing. Examples involve information retrieval (Salton et al., 1975), word sense discrimination (Sch¨utze, 1998), text segmentation (Choi et al., 2001), and numerous studies of lexicon acquisition (Grefenstette, 1994; It is thus no surprise that recent years have witnessed the emergence of perceptually grounded distributional models. An important question in the formulation of such models concerns the provenance of perceptual information. A few models use feature norms as a proxy for sensorimotor experience (Howell et al., 2005; Andrews et al., 2009; Steyvers, 2010; Johns and Jones, 2012). These are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represen"
D12-1130,P98-2127,0,0.121348,"Missing"
D12-1130,J98-1004,0,0.235071,"Missing"
D12-1130,N10-1011,1,\N,Missing
D12-1130,C98-2122,0,\N,Missing
D13-1040,D11-1135,0,\N,Missing
D13-1040,W04-2407,0,\N,Missing
D13-1040,D10-1106,0,\N,Missing
D13-1040,S07-1002,0,\N,Missing
D13-1040,P09-1113,0,\N,Missing
D13-1040,P04-1054,0,\N,Missing
D13-1040,P04-1053,0,\N,Missing
D13-1040,P06-1015,0,\N,Missing
D13-1040,D09-1001,0,\N,Missing
D13-1040,P07-1073,0,\N,Missing
D13-1040,N06-1039,0,\N,Missing
D13-1040,P05-1045,0,\N,Missing
D13-1040,D07-1076,0,\N,Missing
D13-1157,D10-1049,0,0.187819,"e of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e.,"
D13-1157,H05-1042,1,0.858125,"lecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content plannin"
D13-1157,P09-1010,0,0.0275449,"learns document plans based on Rhetorical Structure Theory (RST; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser. We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the W EATHER G OV (Liang et al., 2009) and W IN H ELP (Branavan et al., 2009) do1504 mains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plan"
D13-1157,W01-1605,0,0.0155037,"l relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), PRST is the set of production rules of the form PRST ⊆ NRST × {NRST ∪ ΣR } × {NRST ∪ ΣR } associated with a weight for each rule, and D ∈ NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST . Training In order to obtain the weighted productions of GRST , we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporti"
D13-1157,P01-1023,0,0.111229,"planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary algorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al."
D13-1157,W02-2112,0,0.207124,"ents: content planning (selecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by"
D13-1157,P12-1007,0,0.0145589,"is a set of non-terminals corresponding to rhetorical relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), PRST is the set of production rules of the form PRST ⊆ NRST × {NRST ∪ ΣR } × {NRST ∪ ΣR } associated with a weight for each rule, and D ∈ NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST . Training In order to obtain the weighted productions of GRST , we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distri"
D13-1157,W13-0113,0,0.0624369,"Missing"
D13-1157,J98-4004,0,0.0212218,"). The W IN H ELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On W EATHER G OV, we extracted 434 rules for GRST . On W IN H ELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the development set and obtained best results with 60 for W EATHER G OV and 120 for W IN H ELP. We used a trigram model for both domains, trained on each training set. Evaluation We compared two configurations of our system, one with a content planning component based on record type sequences (GRSE ) and 6 When horizontally markovizing, we can encode an arbitrary am"
D13-1157,J04-4001,0,0.0241961,"ing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored"
D13-1157,C10-2062,0,0.200335,"nt (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in"
D13-1157,P03-1054,0,0.0591921,"uk/ikonstas/index.php?page=resources 60 RST-DT W EATHER G OV W IN H ELP 40 20 Topic Change Summary Explanation Temporal Topic-Comment Condition Comparison Evaluation Cause Enablement Background Explanation Contrast Joint Attribution Elaboration 0 Figure 4: Distribution of RST relations on W EATHER G OV, W IN H ELP, and the RST-DT (Williams and Power, 2008). narization). The W IN H ELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On W EATHER G OV, we extracted 434 rules for GRST . On W IN H ELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the deve"
D13-1157,N12-1093,1,0.271765,"ing which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of"
D13-1157,P09-1011,0,0.0814864,"utput text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper"
D13-1157,W98-1411,0,0.352511,"Missing"
D13-1157,P02-1040,0,0.114646,"e h=1 horizontal siblings plus the mother left-hand side (LHS) non-terminal, in order to uniquely identify the Markov chain. For example, A → B C D becomes A → B hA . . . Bi, hA . . . Bi → C hA . . .Ci, hA . . .Ci → D. 1510 another one based on RST (GRST ). In both cases content plans were extracted from (noisy) unsupervised alignments. As a baseline, we used the original model of Konstas and Lapata (2012). We also compared our model to Angeli et al.’s system (2010), which is state of the art on W EATHER G OV. System output was evaluated automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along three dimensions: fluency (is the text grammatical?), semantic correctness (does the meaning conveyed by the text correspond to the database input?) and coherence (is the text comprehensible and logically structured?). Participants used a five point rating scale where a high number indicates better performance. We randomly selected 12 documents from the test s"
D13-1157,W13-2124,0,0.0296492,"Missing"
D13-1157,P04-1011,0,0.0950665,"individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all component"
D13-1157,williams-power-2008-deriving,0,0.0234302,"weighted productions of GRST , we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporting the application of the RST framework to our data. We segment each document in our training set into EDUs based on the record-to-text alignments given by the model of Liang et al. (2009) (see Figure 3c). We then run the discourse parser on the resulting EDUs, and retrieve the corresponding discourse tree; the internal nodes are labelled with one of the RST relations. Finally, we replace the leaf EDUs with their respective terminal symbols R(r.t) ∈ ΣR (Figure 3"
D13-1157,N07-1022,0,0.0304879,"ed to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514, c Seattle, Washington, USA, 18-21 O"
D13-1157,J03-4003,0,\N,Missing
D14-1036,W09-1206,0,0.0606412,"Missing"
D14-1036,W05-0620,0,0.405746,"Missing"
D14-1036,P00-1058,0,0.0769682,"modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree. of non-predictive elementary trees. An example of a PLTAG derivation is given in Figure 2. In step 1, a prediction tree is introduced through substitution, which then allows the adjunction of an adverb in step 2. Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes captu"
D14-1036,J13-4008,1,0.901452,"letions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental dependency parser, do not share this advantage, as we will discuss in Section 4.3. 2 semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This all"
D14-1036,J05-1004,0,0.125834,"ay hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fri"
D14-1036,W07-2416,0,0.0663346,"Missing"
D14-1036,C92-2066,0,0.6767,"edicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. Elementary trees that contain a foot node are called auxiliary trees; those that do not are called initial trees. Examples for TAG elementary trees are given in Figure 1a–c. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the"
D14-1036,P10-2012,1,0.843954,"mantic garden paths occur because 301 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301–312, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics provide semantically informed completions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental depen"
D14-1036,W13-2607,1,0.850665,"sponding nodes in Tv . For simplicity of presentation, we will use a concrete example, see Figure 5. Figure 5a shows the lexicon entries for the words of the sentence Semantic Role Lexicon Recall that Propbank is used to construct the PLTAG treebank, in order to distinguish between arguments and modifiers, which result in elementary trees with substitution nodes, and auxiliary trees, i.e., trees with a foot node, respectively (see Figure 1). Conveniently, we can use the same information to also enrich the extracted lexicon with the semantic role annotations, following the process described by Sayeed and Demberg (2013).1 For arguments, annotations are retained on the substitution node in the parental tree, while for modifiers, the role annotation is displayed on the foot node of the auxiliary tree. Note that we display role annotation on traces that are leaf nodes, 1 Contrary to Sayeed and Demberg (2013) we put role label annotations for PPs on the preposition rather than their NP child, following of the CoNLL 2005 shared task (Carreras and M`arquez, 2005). 2 Prediction tree T in our algorithm is only used during pr verification, so it set to nil for substitution and adjunction operations. 304 Banks refused"
D14-1036,J14-3006,1,0.841387,"nd argument identification. In this respect it is analogous to unlabeled dependency accuracy reported in the parsing literature. We exSystem Comparison We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (MajorityBaseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. 308 System iSRL-Oracle iSRL Majority-Baseline Malt-Baseline Prec 91.00 81.48 71.05 60.90 Rec 80.26 75.51 58.10 46.14 F1 85.29 78.38 63.92 52.50 prefixes (up to word 10), presumably as it does not benefit from syntactic prediction, and thus cannot generate incomplete triples early in the sentence,"
D14-1036,D07-1062,0,0.165244,"essing as the path from an argument to the predicate can be very informative but is often quite complicated, and depends on the syntactic formalism used. Many paths through the parse tree are likely to occur infrequently (or not at all), resulting in very sparse information for the classifier to learn from. Moreover, as we will discuss in Section 4.4, such path information is not always available when the input is processed incrementally. There is previous SRL work employing Tree Adjoining Grammar, albeit in a non-incremental setting, as a means to reduce the sparsity of syntaxbased features. Liu and Sarkar (2007) extract a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental 302 (a) NP (b) S NNS NP↓ Banks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) aux"
D14-1036,W08-2121,0,0.0710313,"Missing"
D14-1036,J93-2004,0,0.0497726,"anks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a p"
D14-1036,J08-2001,0,0.0889078,"Missing"
D14-1036,W00-1307,0,0.0413792,"h with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefi"
D14-1036,W09-1201,0,\N,Missing
D14-1036,Q13-1010,1,\N,Missing
D14-1074,W13-2121,0,0.0117882,"tterns, within each line and across lines. For instance, the final characters in the second, fourth and (optionally) first line must rhyme, 670 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics example, the Haiku poem generator presented in Wu et al. (2009) and Tosa et al. (2008) produces poems by expanding user queries with rules extracted from a corpus and additional lexical resources. Netzer et al. (2009) generate Haiku with Word Association Norms, Agirrezabal et al. (2013) compose Basque poems using patterns based on parts of speech and WordNet (Fellbaum, 1998), and Oliveira (2012) presents a generation algorithm for Portuguese which leverages semantic and grammar templates. whether a candidate poem conforms to these requirements. Poetry generation has received a fair amount of attention over the past years (see the discussion in Section 2), with dozens of computational systems written to produce poems of varying sophistication. Beyond the long-term goal of building an autonomous intelligent system capable of creating meaningful poems, there are potential short"
D14-1074,E14-1003,0,0.0765892,"Missing"
D14-1074,W12-3102,0,0.0607649,"Missing"
D14-1074,D10-1051,0,0.0402015,"entations of individual characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other. Our model generates lines in a poem probabilistically: it estimates the probability of the current line given the probability of all previously generated lines. We use a recurrent neural network to learn the representations of the lines generated so far which in turn serve as input to a recurrent language model (Mikolov et al., 2010; Mikolov et al., 2011b; Mikolov et al., 2011a) which generates the current line. In contrast to previous approaches (Greene et al., 2010; Jiang and Zhou, 2008), our generator makes no Markov assumptions about the dependencies of the words within a line and across lines. We evaluate our approach on the task of quatrain generation (see Table 1 for a human-written example). Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods. 2 A second line of research uses genetic algorithms for poem generation (Manurung, 2003; Manurung et al., 2012; Zhou et al., 2010). Manurung et al. (2012) argue that at a basic level all (machine-generated) poem"
D14-1074,W11-2123,0,0.0149802,"ore this observation. This way, there is no explicitly defined context, and history is captured implicitly by the recurrent nature of the model. This can be problematic for our texts which must obey certain stylistic conventions and sound poetic. In default of a better way of incorporating poeticness into our model, we further interpolate it with a language model feature (i.e., a Kneser-Ney trigram model). Throughout our experiments, we use the RNNLM toolkit to train the character-based recurrent neural network language model (Mikolov et al., 2010). Kneser-Ney n-grams were trained with KenLM (Heafield, 2011). The probability of the ( j + 1)th word given the previous j words and the previous i lines is estimated by a softmax function: j P(w j+1 = k|w1: j , ui ) = exp(y j+1,k ) |V | ∑k=1 exp(y j+1,k ) (8) We obtain P(Si+1 |S1:i ) by multiplying all the terms in the right hand-side of Equation (6). 3.4 Training The objective for training is the cross entropy errors of the predicted character distribution and the actual character distribution in our corpus. An l2 regularization term is also added to the objective. The model is trained with back propagation through time (Rumelhart et al., 1988) with s"
D14-1074,C08-1048,0,0.938481,"al characters, and their combinations into one or more lines as well as how these mutually reinforce and constrain each other. Our model generates lines in a poem probabilistically: it estimates the probability of the current line given the probability of all previously generated lines. We use a recurrent neural network to learn the representations of the lines generated so far which in turn serve as input to a recurrent language model (Mikolov et al., 2010; Mikolov et al., 2011b; Mikolov et al., 2011a) which generates the current line. In contrast to previous approaches (Greene et al., 2010; Jiang and Zhou, 2008), our generator makes no Markov assumptions about the dependencies of the words within a line and across lines. We evaluate our approach on the task of quatrain generation (see Table 1 for a human-written example). Experimental results show that our model outperforms competitive Chinese poetry generation systems using both automatic and manual evaluation methods. 2 A second line of research uses genetic algorithms for poem generation (Manurung, 2003; Manurung et al., 2012; Zhou et al., 2010). Manurung et al. (2012) argue that at a basic level all (machine-generated) poems must satisfy the cons"
D14-1074,W09-2005,0,0.0856058,"aracters long. Characters in turn follow specific phonological patterns, within each line and across lines. For instance, the final characters in the second, fourth and (optionally) first line must rhyme, 670 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 670–680, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics example, the Haiku poem generator presented in Wu et al. (2009) and Tosa et al. (2008) produces poems by expanding user queries with rules extracted from a corpus and additional lexical resources. Netzer et al. (2009) generate Haiku with Word Association Norms, Agirrezabal et al. (2013) compose Basque poems using patterns based on parts of speech and WordNet (Fellbaum, 1998), and Oliveira (2012) presents a generation algorithm for Portuguese which leverages semantic and grammar templates. whether a candidate poem conforms to these requirements. Poetry generation has received a fair amount of attention over the past years (see the discussion in Section 2), with dozens of computational systems written to produce poems of varying sophistication. Beyond the long-term goal of building an autonomous intelligent"
D14-1074,D13-1176,0,0.21433,"acters and their combinations. Secondly, generation proceeds by taking into account multi-sentential context rather than the immediately preceding sentence. Our work joins others in using continuous representations to express the meaning of words and phrases (Socher et al., 2012; Mikolov et al., 2013) and how these may be combined in a language modeling context (Mikolov and Zweig, 2012). More recently, continuous translation models based on recurrent neural networks have been proposed as a means to map a sentence from the source language to sentences in the target language (Auli et al., 2013; Kalchbrenner and Blunsom, 2013). These models are evaluated on the task of rescoring n-best lists of translations. We use neural networks more directly to perform the actual poem generation task. 3 The Poem Generator As common in previous work (Yan et al., 2013; He et al., 2012) we assume that our generator operates in an interactive context. Specifically, the user supplies keywords (e.g., spring, lute, drunk ) highlighting the main concepts around which the poem will revolve. As illustrated in Figure 1, our generator expands these keywords into a set of related phrases. We assume the keywords are restricted to those attest"
D14-1074,P03-1021,0,0.0309552,"se translation model feature and 3.5 Decoding Our decoder is a stack decoder similar to Koehn et al. (2003). In addition, it implements the tonal pattern and rhyming constraints necessary for generating well-formed Chinese quatrains. Once the first line in a poem is generated, its tonal pattern is determined. During decoding, phrases violating this pattern are ignored. As discussed in Section 1, the final characters of the second and the fourth lines must rhyme. We thus remove during decoding fourth lines whose final characters do not rhyme with the second line. Finally, we use MERT training (Och, 2003) to learn feature weights for the decoder. 4 Experimental Design Data We created a corpus of classical Chinese poems by collating several online resources: Tang Poems, Song Poems, Song Ci, Ming Poems, Qing Poems, and Tai Poems. The corpus consists of 284,899 poems in total. 78,859 of these are quatrains and were used for training and evaluating our model.1 Table 2 shows the different partitions of this dataset (P OEMLM) into training (Q TRAIN)2 , validation (Q VALID) and testing (Q TEST). Half of the poems in Q VALID and Q TEST are 5-char quatrains and the other half are 7-char quatrains. All"
D14-1074,N03-1017,0,0.0281297,"classing methods (Zweig and Makarychev, 2013) on our task. Our poem generator models content selection and lexical choice and their interaction, but does not have a strong notion of local coherence, as manifested in poetically felicitous line-to-line transitions. In contrast, machine translation models (Jiang and Zhou, 2008) have been particularly successful at generating adjacent lines (couplets). To enhance coherence, we thus interpolate our model with two machine translation features (i.e., inverted phrase translation model feature and 3.5 Decoding Our decoder is a stack decoder similar to Koehn et al. (2003). In addition, it implements the tonal pattern and rhyming constraints necessary for generating well-formed Chinese quatrains. Once the first line in a poem is generated, its tonal pattern is determined. During decoding, phrases violating this pattern are ignored. As discussed in Section 1, the final characters of the second and the fourth lines must rhyme. We thus remove during decoding fourth lines whose final characters do not rhyme with the second line. Finally, we use MERT training (Och, 2003) to learn feature weights for the decoder. 4 Experimental Design Data We created a corpus of clas"
D14-1074,D12-1110,0,0.136902,"st two lines, and so on. Our generation model computes the probability of line Si+1 = w1 , w2 , . . . , wm , given all previously generated lines S1:i (i ≥ 1) as: using recurrent neural networks. Structural, semantic, and coherence constraints are captured naturally in our framework, through learning the representations of individual characters and their combinations. Secondly, generation proceeds by taking into account multi-sentential context rather than the immediately preceding sentence. Our work joins others in using continuous representations to express the meaning of words and phrases (Socher et al., 2012; Mikolov et al., 2013) and how these may be combined in a language modeling context (Mikolov and Zweig, 2012). More recently, continuous translation models based on recurrent neural networks have been proposed as a means to map a sentence from the source language to sentences in the target language (Auli et al., 2013; Kalchbrenner and Blunsom, 2013). These models are evaluated on the task of rescoring n-best lists of translations. We use neural networks more directly to perform the actual poem generation task. 3 The Poem Generator As common in previous work (Yan et al., 2013; He et al., 2012)"
D14-1074,D13-1106,0,\N,Missing
D15-1295,D13-1106,0,0.0169324,"ntation models. 1 1. [The burglar]A0 [broke]V [the window]A1 . 2. [The window]A1 [broke]V . Introduction In recent years, an increasing body of work has been devoted to learning distributed word representations and their successful usage in numerous tasks and real-world applications. Examples include language modeling (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b; Kalchbrenner et al., 2014), and most notably machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Auli et al., 2013). Distributed word representations (also known as word embeddings) are trained by predicting the contexts in which the words or phrases occur. In this paper, we present a new approach for unsupervised semantic role labeling that leverages distributed representations. The goal of semantic role labeling is to discover the relations that hold between a predicate and its arguments in a given In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the boundaries of the predicate. The semant"
D15-1295,W09-1206,0,0.129534,"Missing"
D15-1295,J92-4003,0,0.237954,"Missing"
D15-1295,W14-4012,0,0.01748,"Missing"
D15-1295,W06-1601,0,0.159049,"n the sentence, given the predicate and a small window of surrounding arguments. Similarly, predicate vectors are learned from the contexts of preceding arguments, and are required to contribute to the prediction of upcoming arguments. Our vectors encode the semantics of arguments, predicates, and their interdependence. Approaches to unsupervised semantic role labeling follow two main modeling paradigms. Under the the first variant, semantic roles are modeled as latent variables in a (directed) graphical model that relates a verb, its semantic roles, and their possible syntactic realizations (Grenager and Manning, 2006; Lang and Lapata, 2010; Garg and Henderson, 2012). Role induction here corresponds to inferring the state of the latent variables representing the semantic roles of arguments. The second approach is similarity-driven and based on clustering. For instance, Lang and Lapata (2014) induce semantic roles via graph partitioning: each vertex in a graph corresponds to an argument instance of a predicate and edges represent features expressing syntactic or semantic similarity. The graph partitioning problem is solved using task-specific adaptations of label propagation and agglomerative clustering. Ti"
D15-1295,D13-1176,0,0.0185329,"ling approaches and other distributed word representation models. 1 1. [The burglar]A0 [broke]V [the window]A1 . 2. [The window]A1 [broke]V . Introduction In recent years, an increasing body of work has been devoted to learning distributed word representations and their successful usage in numerous tasks and real-world applications. Examples include language modeling (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b; Kalchbrenner et al., 2014), and most notably machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Auli et al., 2013). Distributed word representations (also known as word embeddings) are trained by predicting the contexts in which the words or phrases occur. In this paper, we present a new approach for unsupervised semantic role labeling that leverages distributed representations. The goal of semantic role labeling is to discover the relations that hold between a predicate and its arguments in a given In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the b"
D15-1295,P14-1062,0,0.0110802,"improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models. 1 1. [The burglar]A0 [broke]V [the window]A1 . 2. [The window]A1 [broke]V . Introduction In recent years, an increasing body of work has been devoted to learning distributed word representations and their successful usage in numerous tasks and real-world applications. Examples include language modeling (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b; Kalchbrenner et al., 2014), and most notably machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Auli et al., 2013). Distributed word representations (also known as word embeddings) are trained by predicting the contexts in which the words or phrases occur. In this paper, we present a new approach for unsupervised semantic role labeling that leverages distributed representations. The goal of semantic role labeling is to discover the relations that hold between a predicate and its arguments in a given In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the"
D15-1295,N10-1137,1,0.874123,"edicate and a small window of surrounding arguments. Similarly, predicate vectors are learned from the contexts of preceding arguments, and are required to contribute to the prediction of upcoming arguments. Our vectors encode the semantics of arguments, predicates, and their interdependence. Approaches to unsupervised semantic role labeling follow two main modeling paradigms. Under the the first variant, semantic roles are modeled as latent variables in a (directed) graphical model that relates a verb, its semantic roles, and their possible syntactic realizations (Grenager and Manning, 2006; Lang and Lapata, 2010; Garg and Henderson, 2012). Role induction here corresponds to inferring the state of the latent variables representing the semantic roles of arguments. The second approach is similarity-driven and based on clustering. For instance, Lang and Lapata (2014) induce semantic roles via graph partitioning: each vertex in a graph corresponds to an argument instance of a predicate and edges represent features expressing syntactic or semantic similarity. The graph partitioning problem is solved using task-specific adaptations of label propagation and agglomerative clustering. Titov and Klementiev (201"
D15-1295,J14-3006,1,0.312147,"en In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the boundaries of the predicate. The semantic roles in the example are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. In the unsupervised case, the model must induce such labels from data without access to a predefined set of semantic roles. Role induction is commonly treated as a clustering problem (Titov and Klementiev, 2012; Lang and Lapata, 2014). The input to the model are instances of arguments (e.g., window, the burglar in sentence (1)) and the output is a grouping of these instances into clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to exactly one cluster. In other words, the syntactic representations of verbal predicates, and argument positions are observable, whereas the associated semantic roles are latent and need to be inferred. The task is challenging due to its unsupervised nature — it is difficult to define a learning objective function whose optimiza"
D15-1295,J08-2001,0,0.0516815,"Missing"
D15-1295,P12-2029,0,0.220097,"dow of surrounding arguments. Similarly, predicate vectors are learned from the contexts of preceding arguments, and are required to contribute to the prediction of upcoming arguments. Our vectors encode the semantics of arguments, predicates, and their interdependence. Approaches to unsupervised semantic role labeling follow two main modeling paradigms. Under the the first variant, semantic roles are modeled as latent variables in a (directed) graphical model that relates a verb, its semantic roles, and their possible syntactic realizations (Grenager and Manning, 2006; Lang and Lapata, 2010; Garg and Henderson, 2012). Role induction here corresponds to inferring the state of the latent variables representing the semantic roles of arguments. The second approach is similarity-driven and based on clustering. For instance, Lang and Lapata (2014) induce semantic roles via graph partitioning: each vertex in a graph corresponds to an argument instance of a predicate and edges represent features expressing syntactic or semantic similarity. The graph partitioning problem is solved using task-specific adaptations of label propagation and agglomerative clustering. Titov and Klementiev (2012) propose a Bayesian clust"
D15-1295,N13-1090,0,0.789726,"into roles using a linear programming formulation of hierarchical clustering, where we can model task-specific knowledge. Experiments show improved performance over previous unsupervised semantic role labeling approaches and other distributed word representation models. 1 1. [The burglar]A0 [broke]V [the window]A1 . 2. [The window]A1 [broke]V . Introduction In recent years, an increasing body of work has been devoted to learning distributed word representations and their successful usage in numerous tasks and real-world applications. Examples include language modeling (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013), paraphrase detection (Socher et al., 2011a), sentiment analysis (Socher et al., 2011b; Kalchbrenner et al., 2014), and most notably machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Auli et al., 2013). Distributed word representations (also known as word embeddings) are trained by predicting the contexts in which the words or phrases occur. In this paper, we present a new approach for unsupervised semantic role labeling that leverages distributed representations. The goal of semantic role labeling is to discover the relations that hold betwe"
D15-1295,J05-1004,0,0.14219,"ngs) are trained by predicting the contexts in which the words or phrases occur. In this paper, we present a new approach for unsupervised semantic role labeling that leverages distributed representations. The goal of semantic role labeling is to discover the relations that hold between a predicate and its arguments in a given In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the boundaries of the predicate. The semantic roles in the example are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. In the unsupervised case, the model must induce such labels from data without access to a predefined set of semantic roles. Role induction is commonly treated as a clustering problem (Titov and Klementiev, 2012; Lang and Lapata, 2014). The input to the model are instances of arguments (e.g., window, the burglar in sentence (1)) and the output is a grouping of these instances into clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to e"
D15-1295,N15-1001,0,0.0670159,", Lang and Lapata (2014) induce semantic roles via graph partitioning: each vertex in a graph corresponds to an argument instance of a predicate and edges represent features expressing syntactic or semantic similarity. The graph partitioning problem is solved using task-specific adaptations of label propagation and agglomerative clustering. Titov and Klementiev (2012) propose a Bayesian clustering algorithm based on the Chinese Restaurant Process. Their model encourages similar verbs to have similar linking preferences using a distancedependent Chinese Restaurant Process prior. More recently, Titov and Khoddam (2015) propose a reconstruction-error minimization framerwork for unsupervised semantic role induction. Their model consists of two componenets: the encoder (implemented as a log-linear model) predicts roles given syntactic and lexical features, whereas the reconstruction component (implemented as a probabilistic tensor factorization model) recovers argument fillers based on the role predictions, the predicate and other arguments. The two components are estimated jointly to minimize errors in argument reconstruction. Our work follows the similarity-driven modeling paradigm. Rather than engineering r"
D15-1295,E12-1003,0,0.394588,"e and its arguments in a given In sentence (1), A0 represents the Agent of the breaking event, A1 represents the Patient (i.e., the physical object affected by the breaking event) and V determines the boundaries of the predicate. The semantic roles in the example are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. In the unsupervised case, the model must induce such labels from data without access to a predefined set of semantic roles. Role induction is commonly treated as a clustering problem (Titov and Klementiev, 2012; Lang and Lapata, 2014). The input to the model are instances of arguments (e.g., window, the burglar in sentence (1)) and the output is a grouping of these instances into clusters such that each cluster contains arguments corresponding to a specific semantic role and each role corresponds to exactly one cluster. In other words, the syntactic representations of verbal predicates, and argument positions are observable, whereas the associated semantic roles are latent and need to be inferred. The task is challenging due to its unsupervised nature — it is difficult to define a learning objective"
D15-1295,P10-1040,0,0.0615378,"explicit objective function can potentially yield higher quality output compared to greedy algorithms (such as agglomerative clustering). Secondly, through the use of constraints, we can model task-specific knowledge (e.g., semantic roles are unique within a frame). Experimental results show improved performance over both previous unsupervised semantic role labeling approaches and other distributed word representation models. 2 Related Work Our model is inspired by recent work in learning distributed representations of words (Bengio et al., 2006; Mnih and Hinton, 2008; Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013a). In this framework, a neural network is used to predict a word taking into account its context. Words are represented by vectors which are concatenated or averaged in order to form a representation of the context. We induce vector representations to represent each predicate and its argument. As a learning objective, vectors are required to contribute to a prediction task about the target argument in the sentence, given the predicate and a small window of surrounding arguments. Similarly, predicate vectors are learned from the contexts of preceding arguments, and are re"
D15-1295,J08-2005,0,0.0586853,"ppropriate training objective. We are thus able to model complex interactions between arguments and their predicates without making simplifying assumptions (e.g., that arguments are conditionally independent of each other given the predicate). Our embeddings are largely independent of the clustering algorithm used to induce the semantic roles. We advocate the use of linear programming, which supports the incorporation of linguistic and structural constraints during cluster formation. ILP techniques have been previously applied to several supervised NLP tasks, including semantic role labeling (Punyakanok et al., 2008), how2483 START Yesterday Kristina a1 a2 a3 argt−1 argt argt−1 argt+1 argt argt−1 hit Scott predicate a4 argt+1 argt argt−1 with a baseball END a5 a6 Identification argt+1 Window 1 Window 2 Window 3 Window 4 argt+1 argt Figure 1: Symmetric context window from the list of arguments ever their application to unsupervised role induction is novel to our knowledge. 3 Model Unsupervised role induction is commonly modeled after supervised semantic role labeling (M`arquez et al., 2008) and follows a two-stage approach. Given a sentence and a designated verb, the goal is to identify the arguments of th"
D15-1295,D07-1043,0,0.277324,"Missing"
D15-1295,D11-1014,0,0.14487,"Missing"
D15-1295,Q15-1003,0,0.0213401,"Missing"
D15-1295,W08-2121,0,\N,Missing
D16-1053,N16-1181,0,0.0137262,"Missing"
D16-1053,P14-1062,0,0.20293,"Missing"
D16-1053,D15-1075,0,0.119869,"est system T-CNN (Lei et al., 2015). Figure 5 shows examples of intra-attention for sentiment words. Interestingly, the network learns to associate sentiment important words such as though and fantastic or not and good. 5.3 Natural Language Inference The ability to reason about the semantic relationship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premisehypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we end up with 549,367 pairs for training, 9,842 for development and 9,824 for testing. The vocabulary size is 36,809 and the average sentence length is 22. We performed lower-casing and tokenization for the entire dataset. Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logical relationship (Rockt¨aschel et al., 2016; Wang and Jiang, 2016). Furthermore, Rockt¨aschel"
D16-1053,P16-1139,0,0.0886747,"block component which interacts with its hidden state. Kumar et al. (2016) employ a structured neural network with episodic memory modules for natural language and also visual question answering (Xiong et al., 2016). Similar to the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural models (Dyer et al., 2015; Bowman et al., 2016) where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction (Klein and Manning, 2004) where the learned head-modifier relations are directed. 3 The Machine Reader In this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem553 ory (LSTM) unit with an extended memory tape that explicitly simulates the human me"
D16-1053,P15-1033,0,0.0663298,"an external memory block component which interacts with its hidden state. Kumar et al. (2016) employ a structured neural network with episodic memory modules for natural language and also visual question answering (Xiong et al., 2016). Similar to the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural models (Dyer et al., 2015; Bowman et al., 2016) where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction (Klein and Manning, 2004) where the learned head-modifier relations are directed. 3 The Machine Reader In this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem553 ory (LSTM) unit with an extended memory tape that explicitly"
D16-1053,D11-1142,0,0.0100303,"ling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art. 1 Introduction How can a sequence-level network induce relations which are presumed latent during text processing? How can a recurrent network attentively memorize longer sequences in a way that humans do? In this paper we design a machine reader that automatically learns to understand text. The term machine reading is related to a wide range of tasks from answering reading comprehension questions (Clark et al., 2013), to fact and relation extraction (Etzioni et al., 2011; Fader et al., 2011), ontology learning (Poon and Domingos, 2010), and textual entailment (Dagan et al., 2005). Rather than focusing on a specific task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word-by-word basis. In order to understand texts, our machine reader should provide facilities for extracting and representing meaning from natural language text, storing meanings internally, and working with stored meanings to derive further conseq"
D16-1053,D14-1181,0,0.0160223,"Missing"
D16-1053,P04-1061,0,0.0132699,"the above work, we leverage memory and attention in a recurrent neural network for inducing relations between tokens as a module in a larger network responsible for representation learning. As a property of soft attention, all intermediate relations we aim to capture are soft and differentiable. This is in contrast to shift-reduce type neural models (Dyer et al., 2015; Bowman et al., 2016) where the intermediate decisions are hard and induction is more difficult. Finally, note that our model captures undirected lexical relations and is thus distinct from work on dependency grammar induction (Klein and Manning, 2004) where the learned head-modifier relations are directed. 3 The Machine Reader In this section we present our machine reader which is designed to process structured input while retaining the incrementality of a recurrent neural network. The core of our model is a Long Short-Term Mem553 ory (LSTM) unit with an extended memory tape that explicitly simulates the human memory span. The model performs implicit relation analysis between tokens with an attention-based memory addressing mechanism at every time step. In the following, we first review the standard Long Short-Term Memory and then describe"
D16-1053,D15-1180,0,0.0137192,"Missing"
D16-1053,D14-1162,0,0.124032,"Missing"
D16-1053,P10-1031,0,0.00955993,"nguage inference show that our model matches or outperforms the state of the art. 1 Introduction How can a sequence-level network induce relations which are presumed latent during text processing? How can a recurrent network attentively memorize longer sequences in a way that humans do? In this paper we design a machine reader that automatically learns to understand text. The term machine reading is related to a wide range of tasks from answering reading comprehension questions (Clark et al., 2013), to fact and relation extraction (Etzioni et al., 2011; Fader et al., 2011), ontology learning (Poon and Domingos, 2010), and textual entailment (Dagan et al., 2005). Rather than focusing on a specific task, we develop a general-purpose reading simulator, drawing inspiration from human language processing and the fact language comprehension is incremental with readers continuously extracting the meaning of utterances on a word-by-word basis. In order to understand texts, our machine reader should provide facilities for extracting and representing meaning from natural language text, storing meanings internally, and working with stored meanings to derive further consequences. Ideally, such a system should be robu"
D16-1053,D15-1044,0,0.0329954,"supervision signals, achieving performance comparable or better to state-of-the-art models and superior to vanilla LSTMs. 2 Related Work Our machine reader is a recurrent neural network exhibiting two important properties: it is incremental, simulating human behavior, and performs shallow structure reasoning over input streams. Recurrent neural network (RNNs) have been successfully applied to various sequence modeling and sequence-to-sequence transduction tasks. The latter have assumed several guises in the literature such as machine translation (Bahdanau et al., 2014), sentence compression (Rush et al., 2015), and reading comprehension (Hermann et al., 2015). A key contributing factor to their success has been the ability to handle well-known problems with exploding or vanishing gradients (Bengio et al., 1994), leading to models with gated activation functions (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), and more advanced architectures that enhance the information flow within the network (Koutn´ık et al., 2014; Chung et al., 2015; Yao et al., 2015). A remaining practical bottleneck for RNNs is memory compression (Bahdanau et al., 2014): since the inputs are recursively combined into a sin"
D16-1053,D13-1170,0,0.185016,"ich is typically too small in terms of parameters, it becomes difficult to accurately memorize sequences (Zaremba and Sutskever, 2014). In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism which learns soft alignments between the decoding states and the encoded memories (Bahdanau et al., 2014). In our model, memory and attention are added within a sequence encoder allowing the network to uncover lexical relations between tokens. The idea of introducing a structural bias to neural models is by no means new. For example, it is reflected in the work of Socher et al. (2013a) who apply recursive neural networks for learning natural language representations. In the context of recurrent neural networks, efforts to build modular, structured neural models date back to Das et al. (1992) who connect a recurrent neural network with an external memory stack for learning context free grammars. Recently, Weston et al. (2015) propose Memory Networks to explicitly segregate memory storage from the computation of neural networks in general. Their model is trained end-to-end with a memory addressing mechanism closely related to soft attention (Sukhbaatar et al., 2015) and has"
D16-1053,P15-1150,0,0.0628032,"regularization constant was 1E-4 and the mini-batch size was 5. A dropout rate of 0.5 was applied to the neural network classifier. We compared our model with a wide range of topperforming systems. Most of these models (including ours) are LSTM variants (third block in Table 2), recursive neural networks (first block), or convolutional neural networks (CNNs; second block). Recursive models assume the input sentences are represented as parse trees and can take advantage of annotations at the phrase level. LSTM-type models and CNNs are trained on sequential input, with the exception of CT-LSTM (Tai et al., 2015) which operates over tree-structured network topologies such as constituent trees. For comparison, we also report the performance of the paragraph vector model (PV; Le and Mikolov (2014); see Table 2, second block) which neither operates on trees nor sequences but learns distributed document representations parameterized directly. The results in Table 2 show that both 1- and 2-layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art. The number of layers for our models was set to be comparable to previously published results. On the fine-grained and bin"
D16-1053,N16-1170,0,0.45912,"e Inference (SNLI) dataset (Bowman et al., 2015), which contains premisehypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we end up with 549,367 pairs for training, 9,842 for development and 9,824 for testing. The vocabulary size is 36,809 and the average sentence length is 22. We performed lower-casing and tokenization for the entire dataset. Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logical relationship (Rockt¨aschel et al., 2016; Wang and Jiang, 2016). Furthermore, Rockt¨aschel et al. (2016) show that a non-standard encoder-decoder architecture which processes the hypothesis conditioned on 558 it ’s although tough i did to n’t watch hate this but one it , ’s it ’s a fantastic not very good movie either Figure 5: Examples of intra-attention (sentiment analysis). Bold lines (red) indicate attention between sentiment important words. the premise results significantly boosts performance. We use a similar approach to tackle this task with LSTMNs. Specifically, we use two LSTMNs to read the premise and hypothesis, and then match them by comparin"
D17-1009,E17-2039,0,0.0131179,"Missing"
D17-1009,P15-1039,0,0.0304249,"ttempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded repres"
D17-1009,P14-1133,0,0.21499,"Missing"
D17-1009,Q15-1039,0,0.154417,"Missing"
D17-1009,W13-3520,0,0.0208057,"ty linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events wi"
D17-1009,W09-1206,0,0.0126084,"e English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specificity for universality. With both datasets, results"
D17-1009,P02-1041,0,0.341625,"Missing"
D17-1009,C04-1180,1,0.821524,"Missing"
D17-1009,W13-2322,0,0.0191232,", some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English questions and their ans"
D17-1009,W02-1001,0,0.0458946,"problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, G RAPH PARSER uses two graph transformations: CONTRACT and EXPAND . In Figure 3(a) there are two edges between x and Ghana. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. The search space is explored by beam search and model parameters are estimated with the averaged structured perceptron (Collins, 2002) from training data consisting of question-answer pairs, using answer F1 -score as the objective. Other constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD"
D17-1009,W15-0128,0,0.0171085,"2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in"
D17-1009,D15-1127,0,0.0138165,"Missing"
D17-1009,C16-1056,0,0.096081,"ave mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurth"
D17-1009,jakob-etal-2010-mapping,0,0.0578418,"Missing"
D17-1009,D16-1086,0,0.0201594,"emantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) l"
D17-1009,C14-1122,0,0.0632483,"Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit ap"
D17-1009,P14-1134,0,0.024364,"for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to li"
D17-1009,P12-1051,0,0.0794212,"and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this frame"
D17-1009,E03-1030,0,0.105881,"Missing"
D17-1009,P15-1143,0,0.0127531,"tly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common"
D17-1009,N16-1088,0,0.0302269,"motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched"
D17-1009,Q16-1023,0,0.0120801,"stions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing prob"
D17-1009,Q15-1019,0,0.0329059,"d Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012"
D17-1009,D10-1119,1,0.865489,"al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less) directly (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014). 6 Conclusions We introduced UD EP L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to ex"
D17-1009,D13-1161,0,0.0259018,"nde et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches still map to the target meaning representations (more or less)"
D17-1009,levy-andrew-2006-tregex,0,0.0175027,"the in English have different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. To circumvent this limitation, a simple enhancement step enriches the original UD representation before binarization takes place (Section 3.1). This step adds to the dependency tree missing syntactic information and long-distance dependencies, thereby creating a graph. Whereas D EP L AMBDA is not able to handle graph-structured input, UD EP - 2 We use Tregex (Levy and Andrew, 2006) for substitution mappings and Cornell SPF (Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With l"
D17-1009,N15-1114,0,0.00825081,"P L AMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies co"
D17-1009,Q14-1030,1,0.830946,"ikipedia, or SimpleQuestions (Bordes et al., 2015) are shown in parentheses. On GraphQuestions, we achieve a new state-of-the-art result with a gain of 4.8 F1 points over the previously reported best result. On WebQuestions we are 2.1 points below the best model using comparable resources, and 3.8 points below the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score o"
D17-1009,J93-2004,0,0.0647121,"ng-distance dependencies in relative clauses and control constructions. We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control). Figure 2(a) shows the long-distance dependency in the sentence Anna wants to marry Kristoff. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al., 1993) used by SD, the UD part-of-speech tags do not distinguish question words. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET: WH, ADV: WH and PRON : WH, respectively. Specifically, we use a list of 12 (English), 14 (Spanish) and 35 (German) words, respectively. This is the only part of UD EP L AMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as"
D17-1009,D14-1045,0,0.0131176,"the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1 score to their use of the more fine-grained PTB tag set and Stanford Dependencies. CCGG RAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English. UD EP L AMBDA SRL This is similar to UD EP L AMBDA except that instead of assuming nsubj, dobj and nsubjpass correspond to arg1 , arg2 and arg2 , we employ semantic role labeling to identify the correct interpretation. We used the systems of Roth and Woodsend (2014) for English and German and Bjrkelund et al. (2009) for Spanish trained on the CoNLL-2009 dataset (Haji et al., 2009).8 4.5 GraphQ. WebQ. Results Table 3 shows the performance of G RAPH PARSER with these different representations. Here and in what follows, we use average F1 -score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UD EP L AMBDA consistently outperforms the S INGLE E VENT and D EP T REE representations in all languages. For English, performance is on par with CCGG RAPH, which suggests that UD EP L AMBDA does not sacrifice too much specifi"
D17-1009,P16-2079,0,0.0223299,"versal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UD EP L AMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit applications such as summarization (Liu et al., 2015) and machine reading (Sachan and Xing, 2016). Acknowledgements This work greatly benefited from discussions with Michael Collins, Dipanjan Das, Federico Fancellu, Julia Hockenmaier, Tom Kwiatkowski, Adam Lopez, Valeria de Paiva, Martha Palmer, Fernando Pereira, Emily Pitler, Vijay Saraswat, Nathan Schneider, Bonnie Webber, Luke Zettlemoyer, and the members of ILCC Edinburgh University, the Microsoft Research Redmond NLP group, the Stanford NLP group, and the UW NLP and Linguistics group. We thank Reviewer 2 for useful feedback. The authors would also like to thank the Universal Dependencies community for the treebanks and documentation."
D17-1009,L16-1376,0,0.089339,"(Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx. ∃y. f (x) ∧ g(y) ∧ amodi (ye , xa ). 3 In what follows, all references to UD are to UD v1.3. 91 L AMBDA is designed to work with dependency graphs as well (Section 3.2). Finally, several constructions differ in structure between UD and SD, which requires different handling in the semantic interface (Section 3.3). 3.1 xcomp nsubj Anna wants to dobj marry Kristoff nsubj (a) With long-distance dependency. Enhancement Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications. However, such enhancements are currently only available for a subset of languages in UD. Instead, we rely on a small number of enhancements for our main application—semantic parsing for questionanswering—with the hope that this step can be replaced by an enhanced UD representation in the future. Specifically, we define three kinds of enhancements: (1) long-distance dependencies; (2) types of coordination; and (3) refined question word tags. These correspond to line 2 in Algorithm 1. Fi"
D17-1009,J05-1004,0,0.0829545,"uantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. We discuss how to handle quantifiers in this framework in the supplementary material. Although in the current setup UD EP L AMBDA rules are hand-coded, the number of rules are only proportional to the number of UD labels, making rule-writing manageable.5 Moreover, we view UD EP L AMBDA as a first step towards learning rules for converting UD to richer semantic representations such as PropBank, AMR, or the Parallel Meaning Bank (Palmer et al., 2005; Banarescu et al., 2013; Abzianidze et al., 2017).. 4 Semantic Parsing as Graph Matching Cross-lingual Semantic Parsing To study the multilingual nature of UD EP L AMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed. 4.2 Datasets We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English"
D17-1009,P15-1142,0,0.0101447,"in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multi"
D17-1009,D14-1162,0,0.116047,"tic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input to G RAPH PARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1 -scores for each language and dataset. Features We"
D17-1009,P16-2067,0,0.0225596,"ir translations. k en 1 10 Implementation Details 89.6 95.7 WebQuestions de es 82.8 91.2 86.7 94.0 GraphQuestions en de es 47.2 56.9 39.9 48.4 39.5 51.6 Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model. Table 2: Structured perceptron k-best entity linking accuracies on the development sets. Dependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus. For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).6 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2. input t"
D17-1009,R11-1065,0,0.0249782,"ntic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Instead, UD EP L AMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base"
D17-1009,D16-1054,0,0.127651,"uctions such as control. The different treatments of various linguistic constructions in UD compared to SD also require different handling in UD EP L AMBDA (Section 3.3). Our experiments focus on Freebase semantic parsing as a testbed for evaluating the framework’s multilingual appeal. We convert natural language to logical forms which in turn are converted to machine interpretable formal meaning representations for retrieving answers to questions from Freebase. To facilitate multilingual evaluation, we provide translations of the English WebQuestions (Berant et al., 2013) and GraphQuestions (Su et al., 2016) datasets to German and Spanish. We demonstrate that UD EP L AMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UD EP L AMBDA outperforms strong baselines across Universal Dependencies (UD) offer a uniform cross-lingual syntactic representation, with the aim of advancing multilingual applications. Recent work shows that semantic parsing can be accomplished by transforming syntactic dependencies to logical forms. Howe"
D17-1009,N15-3006,0,0.0206547,"g semantic representations from dependencies have mainly focused on languagespecific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016; Beltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; B´edaride and Gardent, 2011). In contrast, UD EP L AMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016). We evaluate UD EP L AMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated task-independent representation that is then mapped to a grounded representation (Kwiatkowski et a"
D17-1009,D16-1177,0,0.0268016,"Missing"
D17-1009,P16-1220,1,0.172318,"Missing"
D17-1009,N15-3014,0,0.0204028,"Missing"
D17-1009,P14-1090,0,0.131516,"Missing"
D17-1009,D16-1015,0,0.0546486,"Missing"
D17-1009,P15-1128,0,0.0788276,"Missing"
D17-1062,W15-3014,0,0.0122633,"of edits required to change an original complex sentence to simpler output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert complex to simple sentences. As shown in Table 4, Hybrid obtains the highest TER, followed by our models (D RESS and Hybrid on Fluency, Simplicity, and overall. The fact that neural models (EncDecA, D RESS and D RESS -L S) fare well on Fluency, is perhaps not surprising given the recent success of LSTMs in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015). Neural models obtain worse ratings on Adequacy but are closest to the human references on this dimension. D RESS -L S (and D RESS) are significantly better (p < 0.01) on Simplicity than EncDecA, PBMT-R, and Hybrid which indicates that our reinforcement learning based model is effective at creating simpler output. Combined ratings (All) for D RESS -L S are significantly different compared to the other models but not to D RESS and the Reference. Nevertheless, integration of the lexical simplification model boosts performance as ratings increase almost across the board (Simplicity is slightly w"
D17-1062,P02-1028,0,0.0611299,"re words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014). Earlier work focused on individual aspects of the simplification problem. For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woodsend and Lapata (20"
D17-1062,P13-1151,0,0.532018,"end and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010). It contains automatically aligned complex and simple sentences from the ordinary and simple English Wikipedias. The test set consists of 100 complex-simple sentence pairs. The training set contains 89,042 sentence pairs (after removing duplicates and test sentences). We randomly sampled 205 pairs for development and used the remaining sentences for training. We also constructed WikiLarge, a larger Wikipedia corpus by combining previously created simplification corpora. Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu’s (2010) WikiSmall dataset described above. We used the development and test sets created in Xu et al. (2016). These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers. The dataset Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification (Specia et al., 2012). The model presented so far learns lexical substitution and other rewrite operations jointly. In some cases, words are predi"
D17-1062,E99-1042,0,0.842314,"le, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1 1 The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014). Earlier work focused on individual aspects of the simplification problem. For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For"
D17-1062,C96-2183,0,0.898812,"ve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1 1 The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014). Earlier work focused on individual aspects of the simplification problem. For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010)"
D17-1062,N16-1012,0,0.0396671,"Missing"
D17-1062,D16-1127,0,0.0110387,"imposed by the simplification task itself, i.e., the predicted output must be simpler, preserve the meaning of the input, and grammatical. To incorporate this knowledge, the model is trained in a reinforcement learning framework (Williams, 1992): it explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints. Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016). We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b). We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets. 2 P (Y |X) = |Y | Y P (yt |y1:t−1 , X) (1) t=1 P (yt+1 |y1:t , X) = softmax(g(hTt , ct )) (2) where g(·) is a one-hidden-l"
D17-1062,P07-2009,0,0.0129085,"ort-Term Memory Network (LSTM; Hochreiter and Schmidhuber 1997), while the decoder uses another LSTM to generate one word yt+1 at a time in the simplified target Y . Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct , which encodes the source sentence: to their dis-similarity to the (complex) input sentence. The hybrid model developed in Narayan and Gardent (2014) also operates in two phases. Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007). The resulting sentences are further simplified by a model similar to Wubben et al. (2012). Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output. In this paper we propose a simplification model which draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014). Central to this approach is an encoderdecoder architecture implemented by recurrent neural networks. The encoder reads the source sequ"
D17-1062,P16-1057,0,0.0299411,"Missing"
D17-1062,D15-1166,0,0.0383733,"publicly available at https:// github.com/XingxingZhang/dress. 584 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics framework. Given a (complex) source sentence X = (x1 , x2 , . . . , x|X |), our model learns to predict its simplified target Y = (y1 , y2 , . . . , y|Y |). Inferring the target Y given the source X is a typical sequence to sequence learning problem, which can be modeled with attention-based encoderdecoder models (Bahdanau et al., 2015; Luong et al., 2015). Sentence simplification is slightly different from related sequence transduction tasks (e.g., compression) in that it can involve splitting operations. For example, a long source sentence (In 1883, Faur married Marie Fremiet, with whom he had two sons.) can be simplified as two sentences (In 1883, Faur married Marie Fremiet. They had two sons.). Nevertheless, we still view the target as a sequence, i.e., two or more sequences concatenated with full stops. The encoder-decoder model has two parts (see left hand side in Figure 1). The encoder transforms the source sentence X into a sequence of"
D17-1062,W14-1215,0,0.249458,"Missing"
D17-1062,P14-5010,0,0.00502379,"Missing"
D17-1062,N13-1092,0,0.0238186,"Missing"
D17-1062,D16-1261,0,0.0102324,"as its backbone, it must also meet constraints imposed by the simplification task itself, i.e., the predicted output must be simpler, preserve the meaning of the input, and grammatical. To incorporate this knowledge, the model is trained in a reinforcement learning framework (Williams, 1992): it explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints. Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016). We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b). We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets. 2 P (Y |X) = |Y | Y P (yt |y1:t−1 , X) (1) t=1 P (yt+1 |y1:t , X) = softmax(g(hTt ,"
D17-1062,P14-1041,0,0.667577,"tops. The encoder-decoder model has two parts (see left hand side in Figure 1). The encoder transforms the source sentence X into a sequence of hidden states (hS1 , hS2 , . . . , hS|X |) with a Long Short-Term Memory Network (LSTM; Hochreiter and Schmidhuber 1997), while the decoder uses another LSTM to generate one word yt+1 at a time in the simplified target Y . Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct , which encodes the source sentence: to their dis-similarity to the (complex) input sentence. The hybrid model developed in Narayan and Gardent (2014) also operates in two phases. Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007). The resulting sentences are further simplified by a model similar to Wubben et al. (2012). Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output. In this paper we propose a simplification model which draws on insights from neural machine"
D17-1062,W03-1602,0,0.0482656,"ude substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014). Earlier work focused on individual aspects of the simplification problem. For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woo"
D17-1062,P02-1040,0,0.116531,"|). Then, we use h|X |to initialize the hidden state of the decoder LSTM and recover/generate X one word at a time. ∇L(θ) ≈ P|Yˆ | yt |ˆ y1:t−1 , X)[r(ˆ y1:|Yˆ |) − bt ] t=1 ∇ log PRL (ˆ To reduce the variance of gradients, we also introduce a baseline linear regression model bt to estimate the expected future reward at time t (Ranzato et al., 2016). bt takes the concatenation of hTt and ct as input and outputs a real value as the expected reward. The parameters of the regressor are Fluency Xu et al. (2016) observe that SARI correlates less with fluency compared to other metrics such as BLEU (Papineni et al., 2002). The fluency reward rF models the well-formedness of the generated sentences explicitly. It is the normalized sentence probability assigned by an LSTM 587 and the alignment scores is: trained by minimizing mean squared error. We do not back-propagate this error to hTt or ct during training (Ranzato et al., 2016). 3.3 (11) where Wl ∈ R|V |×d and st represents the source: Learning Presented in its original form, the REINFORCE algorithm starts learning with a random policy. This assumption can make model training challenging for generation tasks like ours with large vocabularies (i.e., action sp"
D17-1062,D14-1162,0,0.112964,"Missing"
D17-1062,D11-1038,1,0.961701,"003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs. During inference, the K-best outputs of the PBMT model are reranked according Introduction The main goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning. The simplifica"
D17-1062,D12-1024,0,0.00921088,"h our model uses the encoder-decoder architecture as its backbone, it must also meet constraints imposed by the simplification task itself, i.e., the predicted output must be simpler, preserve the meaning of the input, and grammatical. To incorporate this knowledge, the model is trained in a reinforcement learning framework (Williams, 1992): it explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints. Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016). We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b). We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets. 2 P (Y |X) = |Y | Y P (yt |y1:t−1"
D17-1062,P12-1107,0,0.734687,"Missing"
D17-1062,Q15-1021,0,0.553967,"f possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints. Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016). We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b). We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets. 2 P (Y |X) = |Y | Y P (yt |y1:t−1 , X) (1) t=1 P (yt+1 |y1:t , X) = softmax(g(hTt , ct )) (2) where g(·) is a one-hidden-layer neural network with the following parametrization: g(hTt , ct ) = Wo tanh(Uh hTt + Wh ct ) (3) where Wo ∈ R|V |×d , Uh ∈ Rd×d , and Wh ∈ Rd×d ; |V |is the output vocabulary size and d the hidden unit size. hTt is the hidden state of the decoder LSTM which summariz"
D17-1062,Q16-1029,0,0.317766,"one word yt+1 at a time in the simplified target Y . Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct , which encodes the source sentence: to their dis-similarity to the (complex) input sentence. The hybrid model developed in Narayan and Gardent (2014) also operates in two phases. Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007). The resulting sentences are further simplified by a model similar to Wubben et al. (2012). Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output. In this paper we propose a simplification model which draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014). Central to this approach is an encoderdecoder architecture implemented by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. Al"
D17-1062,W06-3104,0,0.0292428,"onolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs. During inference, the K-best outputs of the PBMT model are reranked according Introduction The main goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning. The simplification task has been the subject of several modeling efforts in recent years due to its relevance"
D17-1062,P01-1067,0,0.0236172,"implification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs. During inference, the K-best outputs of the PBMT model are reranked according Introduction The main goal of sentence simplificat"
D17-1062,2006.amta-papers.25,0,0.120881,"em output for two sentences (Newsela development set). Substitutions are shown in bold. scores in Table 1) which yield simpler output (see the Simplicity column in Table 2). Based on our judgment elicitation study, neural models trained with reinforcement learning perform best, with D RESS -L S having a slight advantage. We further analyzed model performance by computing various statistics on the simplified output. We measured average sentence length and the degree to which D RESS and comparison systems perform rewriting operations. We approximated the latter with Translation Error Rate (TER; Snover et al. 2006), a measure commonly used to automatically evaluate the quality of machine translation output. We used TER to compute the (average) number of edits required to change an original complex sentence to simpler output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert complex to simple sentences. As shown in Table 4, Hybrid obtains the highest TER, followed by our models (D RESS and Hybrid on Fluency, Simplicity, and overall. The fact that neural models (EncDecA, D RESS and D RESS -L S) fare well on Fluen"
D17-1062,S12-1046,0,0.0476186,"Wikipedia corpus by combining previously created simplification corpora. Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu’s (2010) WikiSmall dataset described above. We used the development and test sets created in Xu et al. (2016). These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers. The dataset Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification (Specia et al., 2012). The model presented so far learns lexical substitution and other rewrite operations jointly. In some cases, words are predicted because they seem natural in the their context, but are poor substitutes for the content of the complex sentence. To countenance this, we learn lexical simplifications explicitly and integrate them with our reinforcement learning-based model. We use an pre-trained encoder-decoder model (which is trained on a parallel corpus of complex and simple sentences) to obtain probabilistic word alignments, aka attention scores (see αt in Equation (6)). Let X = (x1 , x2 , . ."
D17-1062,D14-1074,1,0.345649,"Missing"
D17-1062,C10-1152,0,0.903605,"ekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs. During inference, the K-b"
D17-1062,P08-1040,0,0.590128,". Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1 1 The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014). Earlier work focused on individual aspects of the simplification problem. For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002). Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syn"
D17-1091,D13-1160,0,0.104457,"ialized from a uniform distribution U (−0.08, 0.08). The learning rate and decay rate of RMSProp were 0.01 and 0.95, respectively. The batch size was set to 150. To alleviate the exploding gradient problem (Pascanu et al., 2013), the gradient norm was clipped to 5. Early stopping was used to determine the number of epochs. 3.2 3.3 Our model was trained on three datasets, representative of different types of QA tasks. The first two datasets focus on question answering over a structured knowledge base, whereas the third one is specific to answer sentence selection. W EB Q UESTIONS This dataset (Berant et al., 2013) contains 3, 778 training instances and 2, 032 test instances. Questions were collected by querying the Google Suggest API. A breadth-first search beginning with wh- was conducted and the answers were crowd-sourced using Freebase as the backend knowledge base. G RAPH Q UESTIONS The dataset (Su et al., 2016) contains 5, 166 question-answer pairs (evenly split into a training and a test set). It was created by asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the variou"
D17-1091,P14-1133,0,0.107546,"Missing"
D17-1091,D14-1067,0,0.0459875,".edu, mlap@inf.ed.ac.uk Abstract the use of paraphrases in relation to question answering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. Berant and Liang (2014) use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models (Bordes et al., 2014a,b; Dong et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Da"
D17-1091,D15-1181,0,0.0144941,"tead, we translate from multiple pivot sentences (Mallinson et al., 2016). As shown in Figure 2, question q is translated to K-best German pivots, Gq = {g1 , . . . , gK }. The probability of generating paraphrase q 0 = y1 . . . y|q0 |is decomposed as: 0 0  p q |Gq = |q | Y p (yt |y<t , Gq ) t=1 (2) 0 = |q |K Y X 2.2 p (gk |q) p (yt |y<t , gk ) Recall from Equation (1) that pθ (q 0 |q) scores the generated paraphrases q 0 ∈ Hq ∪ {q}. We estimate pθ (q 0 |q) using neural networks given their successful application to paraphrase identification tasks (Socher et al., 2011; Yin and Sch¨utze, 2015; He et al., 2015). Specifically, the input question and its paraphrases are encoded as vectors. Then, we employ a neural network to obtain the score s (q 0 |q) which after normalization becomes the probability pθ (q 0 |q). t=1 k=1 where y<t = y1 , . . . , yt−1 , and |q 0 |is the length of q 0 . Probabilities p (gk |q) and p (yt |y<t , gk ) are computed by the E N -D E and D E -E N models, respectively. We use beam search to decode tokens by conditioning on multiple pivoting sentences. The results with the best decoding scores are considered candidate paraphrases. Examples of NMT paraphrases are shown in Table"
D17-1091,D08-1021,0,0.0997306,"Missing"
D17-1091,P82-1020,0,0.849456,"Missing"
D17-1091,P16-1073,0,0.0916487,"Missing"
D17-1091,N03-1017,0,0.0842091,"ch as the ability to learn continuous representations and to consider wider context while paraphrasing. In our work, we select German as our pivot following Mallinson et al. (2016) who show that it outperforms other languages in a wide range of paraphrasing experiments, and pretrain two NMT systems, English-to-German (E N -D E) and PPDB-based Generation Bilingual pivoting (Bannard and Callison-Burch, 2005) is one of the most well-known approaches to paraphrasing; it uses bilingual parallel corpora to learn paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. 2003). The intuition is that two English strings that translate to the same foreign string can be assumed to have the same meaning. The method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Drawing inspiration from syntax-based SMT, Callison-Burch (2008) and Ganitkevitch et al. (2011) extended this idea to syntactic paraphrases, 877 Source the average size of be locate on which continent language speak in what be the money in WikiAnswers1 users. This corpus is a large resource (the average cluster size is 25), but is relati"
D17-1091,P15-1026,1,0.292885,"Abstract the use of paraphrases in relation to question answering. There have been three main strands of research. The first one applies paraphrasing to match natural language and logical forms in the context of semantic parsing. Berant and Liang (2014) use a template-based method to heuristically generate canonical text descriptions for candidate logical forms, and then compute paraphrase scores between the generated texts and input questions in order to rank the logical forms. Another strand of work uses paraphrases in the context of neural question answering models (Bordes et al., 2014a,b; Dong et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al.,"
D17-1091,N06-2009,0,0.0954207,"such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned Question answering (QA) systems are sensitive to the many different ways natural language expresses the same information need. In this paper we turn to paraphrases as a means of capturing this knowledge and present a general framework which learns felicitous paraphrases for various QA tasks. Our method is trained end-toend using question-answer pairs as a supervision signal. A question and its par"
D17-1091,P13-1158,0,0.123542,"ti-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the supervision signal from the training data, and as such cannot be properly tuned Question answering (QA) systems are"
D17-1091,D16-1147,0,0.00856936,"Missing"
D17-1091,D11-1108,0,0.0149075,"Missing"
D17-1091,W16-6625,1,0.865193,"g et al., 2015). These models are typically trained on question-answer pairs, and employ question paraphrases in a multi-task learning framework in an attempt to encourage the neural networks to output similar vector representations for the paraphrases. The third strand of research uses paraphrases more directly. The idea is to paraphrase the question and then submit the rewritten version to a QA module. Various resources have been used to produce question paraphrases, such as rule-based machine translation (Duboue and ChuCarroll, 2006), lexical and phrasal rules from the Paraphrase Database (Narayan et al., 2016), as well as rules mined from Wiktionary (Chen et al., 2016) and large-scale paraphrase corpora (Fader et al., 2013). A common problem with the generated paraphrases is that they often contain inappropriate candidates. Hence, treating all paraphrases as equally felicitous and using them to answer the question could degrade performance. To remedy this, a scoring model is often employed, however independently of the QA system used to find the answer (Duboue and Chu-Carroll, 2006; Narayan et al., 2016). Problematically, the separate paraphrase models used in previous work do not fully utilize the"
D17-1091,P02-1040,0,0.106423,"a test set). It was created by asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the various systems across datasets (training set). As can be seen, the average paraphrase length is similar to the average length of the original questions. The NMT method generates more paraphrases and has wider coverage, while the average number and coverage of the other two methods varies per dataset. As a way of quantifying the extent to which rewriting takes place, we report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores between the original questions and their paraphrases. The NMT Paraphrase Generation Candidate paraphrases were stemmed (Minnen et al., 2001) and lowercased. We discarded duplicate or trivial paraphrases which only rewrite stop words or punctuation. For the NMT model, we followed the implementation2 and settings described in Mallinson et al. (2016), and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared translation task (4.2 million sentence pairs). We also had access to back-translated monolingual"
D17-1091,2006.amta-papers.25,0,0.0466375,"asking crowd workers to paraphrase 500 Freebase graph queries in natural language. Implementation Details Table 3 presents descriptive statistics on the paraphrases generated by the various systems across datasets (training set). As can be seen, the average paraphrase length is similar to the average length of the original questions. The NMT method generates more paraphrases and has wider coverage, while the average number and coverage of the other two methods varies per dataset. As a way of quantifying the extent to which rewriting takes place, we report BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores between the original questions and their paraphrases. The NMT Paraphrase Generation Candidate paraphrases were stemmed (Minnen et al., 2001) and lowercased. We discarded duplicate or trivial paraphrases which only rewrite stop words or punctuation. For the NMT model, we followed the implementation2 and settings described in Mallinson et al. (2016), and used English↔German as the language pair. The system was trained on data released as part of the WMT15 shared translation task (4.2 million sentence pairs). We also had access to back-translated monolingual training data (Sennrich et al."
D17-1091,D16-1244,0,0.0259126,"Missing"
D17-1091,D16-1054,0,0.0434992,"Missing"
D17-1091,P15-2070,0,0.0669942,"Missing"
D17-1091,D14-1162,0,0.0941715,"del which we call PARA 4QA (as shorthand for learning to paraphrase for question answering) against multiple previous systems on three datasets. In the following we introduce these datasets, provide implementation details for our model, describe the systems used for comparison, and present our results. 3.1 Datasets W IKI QA This dataset (Yang et al., 2015) has 3, 047 questions sampled from Bing query logs. The questions are associated with 29, 258 candidate answer sentences, 1, 473 of which contain the correct answers to the questions. Training For the paraphrase scoring model, we used GloVe (Pennington et al., 2014) vectors3 pretrained on Wikipedia 2014 and Gigaword 5 to initialize the word embedding matrix. We kept this matrix fixed across datasets. Out-of-vocabulary words were replaced with a special unknown symbol. We also augmented questions with start-ofand end-of-sequence symbols. Word vectors for these special symbols were updated during training. Model hyperparameters were validated on the development set. The dimensions of hidden vectors and word embeddings were selected from {50, 100, 200} and {100, 200}, respectively. The dropout rate was selected from {0.2, 0.3, 0.4}. The B I LSTM for the ans"
D17-1091,N16-1170,0,0.0417696,"Missing"
D17-1091,Q16-1010,1,0.849541,"Missing"
D17-1091,P16-1220,1,0.501367,"Missing"
D17-1091,D17-1009,1,0.05169,"Missing"
D17-1091,D15-1237,0,0.0652633,"Missing"
D17-1091,P16-1009,0,0.0474696,"Query graphs for the questions typically contain more than one predicate. For example, to answer the question “who is the ceo of microsoft in 2008”, we need to use one relation to query “ceo of microsoft” and another relation for the constraint “in 2008”. For this task, we employ the S IMPLE G RAPH model described in Reddy et al. (2016, 2017), and follow their training protocol and feature design. In brief, their method uses rules to a ˆ = arg max p a0 |q a0 ∈Cq 879  (8) where Cq is the set of candidate answers, and p (a0 |q) is computed as shown in Equation (1). 3 split into subword units (Sennrich et al., 2016b) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate paraphrases. We used the S size package of PPDB 2.0 (Pavlick et al., 2015) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers (Fader et al., 2014) as described in Section 2.1.3. The extracted rules were ranked using the pointwise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used. Experiments We compared our model which we call PARA 4QA (as shorthand for learning to paraphra"
D17-1091,N15-3014,0,0.0292453,"Missing"
D17-1091,P16-1162,0,0.0159355,"Query graphs for the questions typically contain more than one predicate. For example, to answer the question “who is the ceo of microsoft in 2008”, we need to use one relation to query “ceo of microsoft” and another relation for the constraint “in 2008”. For this task, we employ the S IMPLE G RAPH model described in Reddy et al. (2016, 2017), and follow their training protocol and feature design. In brief, their method uses rules to a ˆ = arg max p a0 |q a0 ∈Cq 879  (8) where Cq is the set of candidate answers, and p (a0 |q) is computed as shown in Equation (1). 3 split into subword units (Sennrich et al., 2016b) to handle out-of-vocabulary words in questions. We used the top 15 decoding results as candidate paraphrases. We used the S size package of PPDB 2.0 (Pavlick et al., 2015) for high precision. At most 10 candidate paraphrases were considered. We mined paraphrase rules from WikiAnswers (Fader et al., 2014) as described in Section 2.1.3. The extracted rules were ranked using the pointwise mutual information between template pairs in the WikiAnswers corpus. The top 10 candidate paraphrases were used. Experiments We compared our model which we call PARA 4QA (as shorthand for learning to paraphra"
D17-1091,D16-1015,0,0.0326675,"Missing"
D17-1091,P15-1128,0,0.0349307,"Missing"
D17-1091,N15-1091,0,0.0236111,"Missing"
D17-1091,P14-1090,0,\N,Missing
D17-1091,Q15-1039,0,\N,Missing
D17-1091,E17-1083,1,\N,Missing
D17-1091,J13-3001,0,\N,Missing
D17-1133,W05-0613,0,0.0309287,"2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM pre"
D17-1133,W04-2504,0,0.122516,"nce a big Kidder insider-trading scandal two years ago.]e3 Figure 1: Example text (bottom) composed of two sentences (three EDUs) and its RST discourse tree representation (top). Introduction The computational treatment of discourse phenomena has recently attracted much attention, due to their increasing importance for potential applications. Knowing how text units can be composed into a coherent document and how they relate to each other e.g., whether they express contrast, cause, or elaboration, can usefully aid downstream tasks such summarization (Yoshida et al., 2014), question answering (Chai and Jin, 2004), and sentiment analysis (Somasundaran, 2010). Rhetorical Structure Theory (RST, Mann and Thompson 1988), one of the most influential frameworks in discourse processing, represents texts by trees whose leaves correspond to Elementary Discourse Units (EDUs) and whose nodes specify how these and larger units (e.g., multisentence segments) are linked to each other by rhetorical relations. Discourse units are further characterized in terms of their importance in text: nuclei denote central segments, whereas satellites denote peripheral ones. Figure 1 shows an example of a discourse tree representi"
D17-1133,P12-1007,0,0.117309,"lgorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed (Feng and Hirst, 2014; Ji and Eisenstein, 2014) which make local decisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a small window of discourse constituents) without considering document-level information, which has been previously found useful in discourse analysis (Feng and Hirst, 2012). In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we first parse each sentence in a document into a tree whose leaves correspond to EDUs, and then parse the document into a tree whose leaves correspond to already preprocessed sentences. The feature learning process for both stages is based on neural network models. At the sentence level, Long-Short Term Memory Networks (LSTMs; Hochreiter and Schmidhuber 1997) learn r"
D17-1133,P14-1048,0,0.486016,"here n denotes the number of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed (Feng and Hirst, 2014; Ji and Eisenstein, 2014) which make local decisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a small window of discourse constituents) without considering document-level information, which has been previously found useful in discourse analysis (Feng and Hirst, 2012). In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we firs"
D17-1133,P14-1002,0,0.547825,"ber of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time discourse parsers have been proposed (Feng and Hirst, 2014; Ji and Eisenstein, 2014) which make local decisions and model the structure of the discourse and its relations separately. In this case, features are extracted from a local context (i.e., a small window of discourse constituents) without considering document-level information, which has been previously found useful in discourse analysis (Feng and Hirst, 2012). In this paper, we propose a simple and efficient linear-time discourse parser with a novel way of learning contextual representations for discourse constituents. To guarantee linear-time complexity, we use a two-stage approach: we first parse each sentence in a"
D17-1133,P13-1048,0,0.631934,"bba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM predicts their discourse relation. Subsequent work (Feng and Hirst, 2014; Joty et al., 2013) has shown that two-stage systems are not only efficient but can also achieve competitive performance. CKY-based parsers which guarantee globally optimal results have also been developed (Joty et al., 2013; Li et al., 2014). Ji and Eisenstein (2014) were the first to apply neural network models to RST discourse parsing; their shift-reduce parser uses a feedforward neural network to learn the representations of the 1290 transition stack and queue. Li et al. (2014) proposed a CKY-based parser which uses recursive neural networks to learn representations for EDUs and their composition during the"
D17-1133,Q16-1023,0,0.0304662,"t,R Figure 3: Intra-sentential relation CRF with pairwise modeling. alleviates the need for elaborate feature engineering and selection. Our approach is based on LSTMs (Hochreiter and Schmidhuber, 1997) which have recently emerged as a popular architecture for modeling sequences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM"
D17-1133,D16-1001,0,0.0388215,"quences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM over the sentence and obtain the output vector sequence [hh0 , · · · , h i , · · · , ht ], where h i = [~h i ,h~i ] is the output vector for the ith word, and ~h i and h~i are the output vectors from the forward and backward directions, respectively. We represent"
D17-1133,P15-1033,0,0.0158928,"relation CRF with pairwise modeling. alleviates the need for elaborate feature engineering and selection. Our approach is based on LSTMs (Hochreiter and Schmidhuber, 1997) which have recently emerged as a popular architecture for modeling sequences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM over the sentence"
D17-1133,D14-1220,0,0.0842061,"tences with three EDUs (e1 , e2 , and e3 ). EDUs e1 and e2 are connected with a mononuclear relation (i.e., Consequence), where e1 is the nucleus and e2 the satellite (indicated by the left pointing arrow in the figure). Span e1:2 is related to e3 via List, a multi-nuclear relation, expressing the fact that both spans are equally important and therefore both nucleus. Given such tree-based representations of discourse structure, it is not surprising that RST-style document analysis is often viewed as a parsing task. State-of-the-art performance on RST parsing is achieved by cubic-time parsers (Li, Li, and Hovy, 2014; Li, Li, and Chang, 2016), with O(n3 ) time complexity (where n denotes the number of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more ef"
D17-1133,1993.eamt-1.1,0,0.573193,"Their CYK-based parser adopts a recursive deep model for composing EDUs hierarchically together with several additional features to boost performance. CID ER performs slightly worse on span and nuclearity compared to Li et al. (2016), but is better at identifying relations. Their system uses an attention-based hierarchical neural network for modeling text spans and a tensor-based transformation for combining two spans. A CKY-like algorithm is used to generate the discourse tree structure. In comparison, CID ER is conceptually simpler, and more efficient. We used paired bootstrap re-sampling (Efron and Tibshirani, 1993) to assess whether differences in performance are statistically significant. CID ER is significantly better than Feng and Hirst’s 2014 system on the relation metric (p < 0.05); it is also significantly better (p < 0.05) than Heilman and Sagae (2015) on all three metrics and better than Ji and Eisenstein (2014) on the span metric. Compared to Li et al. (2014), CID ER is significantly better on the span and relation metrics (p < 0.05). Unfortunately, we cannot perform significance tests against Li et al. (2016) as we do not have access to the output of their system. We also evaluated the speed o"
D17-1133,D16-1035,0,0.148105,"e1 , e2 , and e3 ). EDUs e1 and e2 are connected with a mononuclear relation (i.e., Consequence), where e1 is the nucleus and e2 the satellite (indicated by the left pointing arrow in the figure). Span e1:2 is related to e3 via List, a multi-nuclear relation, expressing the fact that both spans are equally important and therefore both nucleus. Given such tree-based representations of discourse structure, it is not surprising that RST-style document analysis is often viewed as a parsing task. State-of-the-art performance on RST parsing is achieved by cubic-time parsers (Li, Li, and Hovy, 2014; Li, Li, and Chang, 2016), with O(n3 ) time complexity (where n denotes the number of sen1289 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1289–1298 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tences in the document). These systems model the relations between all possible adjacent discourse segments and use a CKY-style algorithm to generate a global optimal tree. The high order complexity renders such parsers inefficient in practice, especially when processing large documents. As a result, more efficient linear-time disco"
D17-1133,P14-5010,0,0.00392229,"e-trained word embeddings in the training set. Matrix W can be subsequently used to to estimate fine-tuned embeddings for words in the test set. Tokenization, POS tagging and sentence splitting were performed using the Stanford 1 For calculating the binary potential scores, 41 relations will lead to a large number of parameters; to avoid this, we only use 18 relations without nuclearity. 2 https://radimrehurek.com/gensim/ S 79.5 82.7 83.6 83.6 N 68.1 69.3 70.1 71.1 R 56.6 55.6 55.4 57.3 Table 1: CID ER performance using different constituent representations (RST-DT test set). CoreNLP toolkit (Manning et al., 2014). All neural network parameters were initialized randomly with Xavier’s initialization (Glorot and Bengio, 2010). The hyper-parameters are tuned by crossvalidation on the training set. Additional Features Most existing state-of-theart systems rely heavily on handcrafted features (Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2013) some of which have been also proved helpful in neural network models (Li et al., 2014, 2016). In our experiments, we use the following basic features which have been widely adopted in various discourse parsing models: (1) the first three words and the las"
D17-1133,P16-1218,0,0.0358024,"ecture for modeling sequences and have been successfully applied to a variety of tasks ranging from machine translation (Sutskever et al., 2014), to speech recognition (Graves et al., 2013), and image description generation (Vinyals et al., 2015b). LSTMs have also been incorporated into syntactic parsing in a variety of ways (Vinyals et al. 2015a; Kiperwasser and Goldberg 2016; Dyer et al. 2015, inter alia). Of particular relevance to this work is LSTM-minus, a method for learning embeddings of text spans, which has achieved competitive performance in both dependency and constituency parsing (Wang and Chang, 2016; Cross and Huang, 2016). We describe below how we extend this method which is based on subtraction between LSTM hidden vectors to discourse parsing. We represent each sentence as a sequence of wsos , w 1 , · · · , w i , · · · , w n , w eos ] word embeddings [w and insert a special embedding wE to indicate the boundaries of EDUs. We run a bidirectional LSTM over the sentence and obtain the output vector sequence [hh0 , · · · , h i , · · · , ht ], where h i = [~h i ,h~i ] is the output vector for the ith word, and ~h i and h~i are the output vectors from the forward and backward directions, res"
D17-1133,D14-1196,0,0.0208935,"ent General Electric Co. had been frayed since a big Kidder insider-trading scandal two years ago.]e3 Figure 1: Example text (bottom) composed of two sentences (three EDUs) and its RST discourse tree representation (top). Introduction The computational treatment of discourse phenomena has recently attracted much attention, due to their increasing importance for potential applications. Knowing how text units can be composed into a coherent document and how they relate to each other e.g., whether they express contrast, cause, or elaboration, can usefully aid downstream tasks such summarization (Yoshida et al., 2014), question answering (Chai and Jin, 2004), and sentiment analysis (Somasundaran, 2010). Rhetorical Structure Theory (RST, Mann and Thompson 1988), one of the most influential frameworks in discourse processing, represents texts by trees whose leaves correspond to Elementary Discourse Units (EDUs) and whose nodes specify how these and larger units (e.g., multisentence segments) are linked to each other by rhetorical relations. Discourse units are further characterized in terms of their importance in text: nuclei denote central segments, whereas satellites denote peripheral ones. Figure 1 shows"
D17-1133,J00-3005,0,0.776825,"in Section 5. Section 7 concludes the paper. 2 Related Work Recent advances in discourse modeling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT; Carlson et al. 2003) and the Penn Discourse Treebank (PDTB, Prasad et al. 2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most"
D17-1133,prasad-etal-2008-penn,0,0.242149,"eing more efficient. The rest of this paper is organized as follows. We overview related work in the following section. We describe the general flow of our parser in Section 3 and provide details on our parsing algorithm and feature learning method in Section 4. Experimental results are reported in Section 5. Section 7 concludes the paper. 2 Related Work Recent advances in discourse modeling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT; Carlson et al. 2003) and the Penn Discourse Treebank (PDTB, Prasad et al. 2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldri"
D17-1133,W09-3813,0,0.258191,"T-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine classifiers working iteratively in a pipeline. At each iteration, a binary SVM predicts which adjacent units should be merged and then a multi-class SVM predicts their disco"
D17-1133,D12-1110,0,0.403935,"for entire sentences. Treating a sentence as a sequence of EDUs and a document as a sequence of sentences allows to incorporate important contextual information on both levels capturing long-distance dependencies. Recurrent neural networks excel at modeling sequences, but cannot capture hierarchical structure which is important when analyzing multisentential discourse. We therefore adopt a more structure-aware representation at the document level which we argue is complementary to the flat representations obtained from the LSTM. We represent documents as trees using recursive neural networks (Socher et al., 2012). Experimental evaluation on the RST Treebank shows that our parser yields comparable performance to previous lineartime systems, without requiring extensive manual feature engineering and improves upon related neural models (Li et al., 2014, 2016) on discourse relation classification, while being more efficient. The rest of this paper is organized as follows. We overview related work in the following section. We describe the general flow of our parser in Section 3 and provide details on our parsing algorithm and feature learning method in Section 4. Experimental results are reported in Sectio"
D17-1133,N03-1030,0,0.129936,"ith discourse-level information such as the RST Discourse Treebank (RST-DT; Carlson et al. 2003) and the Penn Discourse Treebank (PDTB, Prasad et al. 2008). In this work, we focus on RST-style discourse parsing, where a tree representation is derived for an entire document. In PDTB, discourse relations are annotated mostly between adjacent sentences and no global tree structure is provided. Early approaches to discourse parsing (Marcu, 2000; LeThanh et al., 2004) have primarily focused on overt discourse markers (or cue words) and used a series of rules to derive the discourse tree structure. Soricut and Marcu (2003) employed a standard bottom-up chart parsing algorithm with syntactic and lexical features to conduct sentencelevel parsing. Baldridge and Lascarides (2005) and Sagae (2009) used probabilistic head-driven parsing techniques. Subba and Di Eugenio (2009) were the first to incorporate rich compositional semantics into sentence- and document-level discourse parsing. HILDA (Hernault et al., 2010) has been one of the most influential document-level discourse parsers paving the way for many machine learning-based models. HILDA parses a document pre-segmented into EDUs with two support vector machine"
D17-1133,N09-1064,0,0.0569087,"Missing"
D17-1303,S14-2010,0,0.036251,"S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual se"
D17-1303,S15-2045,0,0.0326146,"d standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Ki"
D17-1303,S12-1051,0,0.0318735,"3 84.6 84.5 91.5 S1 S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et a"
D17-1303,J75-4040,0,0.427596,"Missing"
D17-1303,P11-1020,0,0.0528035,"izing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on"
D17-1303,W16-3210,0,0.233939,"Missing"
D17-1303,D16-1026,0,0.275329,"sis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a 2839 Proceedings of the 2017 Conference on Empirical Metho"
D17-1303,D15-1070,0,0.209515,"We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to han"
D17-1303,P16-1227,0,0.0697283,"evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015"
D17-1303,P16-1168,0,0.2997,"Missing"
D17-1303,N16-1021,0,0.155414,"which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation"
D17-1303,W10-0721,0,0.0735935,"man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on word overlap and is high for both the 2014 and the 2015 dataset, indicating that there is substantial"
D17-1303,Q14-1017,0,0.0524692,"the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat"
D17-1303,W16-2346,0,0.0614214,"larity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015"
D17-1303,D17-1026,0,0.134783,"odal embeddings, irrespective of the language. Our results also show that the asymmetric scoring function can help learn better embeddings. In Table 3 we present a few examples where P IVOT-A SYM and PARALLEL -A SYM models performed better on both the languages compared to baseline order embedding model even using descriptions of very different lengths as queries. 4.2 Semantic Textual Similarity Results In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen2842 Model Shared Task Baseline STS Best System GRAN (Wieting et al., 2017) MLMME (Calixto et al., 2017) VSE (Kiros et al., 2015) OE (Vendrov et al., 2016) P IVOT-S YM PARALLEL -S YM P IVOT-A SYM PARALLEL -A SYM VF − − − VGG19 VGG19 VGG19 VGG19 VGG19 VGG19 VGG19 2012 2014 2015 29.9 51.3 60.4 87.3 83.4 86.4 83.7 84.5 85.0 − 72.7 79.7 80.6 82.7 89.6 82.2 84.1 90.8 80.5 81.8 89.2 82.0 81.4 90.4 83.1 83.8 90.3 84.6 84.5 91.5 S1 S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding pape"
D17-1303,P07-1108,0,0.0352625,"2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a 2839 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2839–2845 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Two men playing soccer on a field Joint space translations of each other, i.e., they are not parallel, although they describe the same image. 3 zwei männer kämpfen um einen fussball Figure 1: Our multilingual multimodal model with image as pi"
D17-1303,P17-2066,0,0.114081,"mage-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a"
D17-1303,Q14-1006,0,0.831264,"create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its"
D17-1303,W10-0707,0,\N,Missing
D18-1088,N18-1150,0,0.059276,"017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. Extractive summari"
D18-1088,P16-1046,1,0.940453,"Zhou† † Microsoft Research Asia, Beijing, China ‡ Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh, UK {xizhang,fuwei,mingzhou}@microsoft.com,mlap@inf.ed.ac.uk Abstract (Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010). The successful application of neural network models to a variety of NLP tasks and the availability of large scale summarization datasets (Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 201"
D18-1088,W04-1017,0,0.0245229,"001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document. A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Filatova and Hatzivassiloglou, 2004) coupled with binary classifiers (Kupiec et al., 1995), hidden Markov models (Conroy and O’leary, 2001), graph based methods 779 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779–784 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sum sent1 Latent Variables 1 0 sent1 sent2 sum sent2 1 0 sent3 sent4 ment are given (methods for obtaining these labels are discussed in Section 3). As shown in the lower part of Figure 1, our extractive model has three parts: a sentence encoder to convert each sentence"
D18-1088,P15-1162,0,0.072593,"Missing"
D18-1088,E17-2068,0,0.0115852,"3.30 17.28 15.75 15.82 16.20 17.52 18.20 18.45 18.77 15.43 R-L 36.57 35.50 32.65 36.38 39.08 36.90 35.30 36.39 36.60 37.14 37.54 34.33 Table 1: Results of different models on the CNN/Dailymail test set using full-length F1 ROUGE -1 (R-1), ROUGE -2 (R-2), and ROUGE -L (R-L). regularized all LSTMs with a dropout rate of 0.3 (Srivastava et al., 2014; Zaremba et al., 2014). We also applied word dropout (Iyyer et al., 2015) at rate 0.2. We set the hidden unit size d = 300 for both word-level and sentence-level LSTMs and all LSTMs had one layer. We used 300 dimensional pre-trained FastText vectors (Joulin et al., 2017) to initialize our word embeddings. The latent model was initialized from the extractive model (thus both models have the same size) and we set the weight in Equation (7) to α = 0.5. The latent model was trained with SGD, with learning rate 0.01 for 5 epochs. During inference, for both extractive and latent models, we rank sentences with p(yi = True|y1:i−1 , D) and select the top three as summary (see also Equation (3)). Experiments Dataset and Evaluation We conducted experiments on the CNN/Dailymail dataset (Hermann et al., 2015; See et al., 2017). We followed the same pre-processing steps as"
D18-1088,D15-1044,0,0.0753765,"to learn sentence representations, while they use convolutional neural network coupled with max pooling (Kim et al., 2016). 2.2 Sentence Compression We train a sentence compression model to map a sentence selected by the extractive model to a sentence in the summary. The model can be used to evaluate the quality of a selected sentence with respect to the summary (i.e., the degree to which it is similar) or rewrite an extracted sentence according to the style of the summary. For our compression model we adopt a standard attention-based sequence-to-sequence architecture (Bahdanau et al., 2015; Rush et al., 2015). The training set for this model is generated from the same summarization dataset used to train the exractive model. Let D = (S1 , S2 , . . . , S|D |) denote a document and H = (H1 , H2 , . . . , H|H |) its summary. We view each sentence Hi in the summary as a target sentence and assume that its corresponding source is a sentence in D most similar to it. We measure the similarity between source sentences and candidate targets using ROUGE, i.e., Sj = argmaxSj ROUGE(Sj , Hi ) and hSj , Hi i is a training instance for the compresˆi besion model. The probability of a sentence H ˆ ˆ ˆ ing the comp"
D18-1088,W04-1013,0,0.496949,"ver et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to i"
D18-1088,P17-1099,0,0.866109,"esentations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-b"
D18-1088,N10-1134,0,0.0358833,"nd H = (H1 , H2 , . . . , H|H |) its human summary (Hk is a sentence in H). We assume that there is a latent variable zi ∈ {0, 1} for each sentence Si indicating whether Si should be selected, and zi = 1 entails it should. We use the extractive model from Section 2.1 to produce probability distributions for latent variables (see Equation (3)) and obtain them by sampling zi ∼ p(zi |z1:i−1 , hD i−1 ) (see R(C, H) = α Rp (C, H) + (1 − α) Rr (C, H) (7) Our use of the terms “precision” and “recall” is reminiscent of relevance and coverage in other summarization work (Carbonell and Goldstein, 1998; Lin and Bilmes, 2010; See et al., 2017). We train the model by minimizing the negative expected R(C, H): L(θ) = −E(z1 ,...,z|D |)∼p(·|D) [R(C, H)] (8) where p(·|D) is the distribution produced by the neural extractive model (see Equation (3)). Unfortunately, computing the expectation term is prohibitive, since the possible latent variable combinations are exponential. In practice, we approximate this expectation with a single sample from 1 We also experimented with unnormalized probabilities (i.e., excluding the exp in Equation (4)), however we obtained inferior results. 781 Model L EAD 3 L EAD 3 (Nallapati et al"
D18-1088,W01-0100,0,0.709066,"are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes directly from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models. 1 Introduction Document summarization aims to automatically rewrite a document into a shorter version while retaining its most important content. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document. A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Fil"
D18-1088,P05-3013,0,0.0543716,"Missing"
D18-1088,P10-1058,1,0.908096,"Missing"
D18-1088,K16-1028,0,0.373973,"of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually"
D18-1088,N18-1158,1,0.825962,"(Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which"
D18-1184,D15-1075,0,0.484435,"in the first sentence to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent"
D18-1184,P16-1139,0,0.0994531,"these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying"
D18-1184,P17-1152,0,0.213268,"the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans"
D18-1184,D16-1053,1,0.88839,"Missing"
D18-1184,D16-1001,0,0.015428,"ght child on a non-terminal node and the second term is the score for spanij being the left child. In Figure 2b, we illustrate the outside process with the target span spanij being the right 1556 child of a non-terminal node. This process is calculated recursively from root to bottom. The normalized marginal probability ρij for each span spanij , where 1 ≤ i < n, i < j ≤ n can be calculated by: ρij = αij βij /α0n (12) To compute the representations of all possible spans, we use Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber 1997) with max-pooling and minus features (Cross and Huang, 2016; Liu and Lapata, 2017). We represent each sentence as a sequence of word embeddings [wsos , w1 , · · · , wt , · · · , wn , weos ] and run a bidirectional LSTM to obtain the output vectors. ht = [~ht , h~t ] is the output vector for the tth word, and ~ht and h~t are the output vectors from the forward and backward directions, respectively. We represent a constituent from position i to j with a span vector spij : spmaxpool = max(hi , · · · , hj ) ij spminus = [~hj − ~hi−1 , h~i − h~j+1 ] ij spij = [spmaxpool , spminus ] ij ij (13) spans across the two sentences, and the attended vectors can be"
D18-1184,P08-1109,0,0.220956,"Missing"
D18-1184,Q15-1035,0,0.0238931,"all structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with structured attention over spans in the other sentence. These 1554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics span representations, weighted b"
D18-1184,N16-1108,0,0.0460947,"Missing"
D18-1184,D16-1073,0,0.0145631,"recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Cohen and Smith, 2009). The most recent work in this direction was the Neural E-DMV model of Jiang et al. (2016). While our goal is not to induce a grammar, we do produce a probabilistic grammar as a byproduct of our model. Our results suggest that training on more complex objectives may be a good way to pursue grammar induction in the future; forcing the model to construct consistent, comparable subtrees between the two sentences is a strong signal for grammar induction. Very recently, a few models attempt to infer latent dependency tree structures with neural models in sentence modeling tasks (Yogatama et al., 2017; Choi et al., 2018). 6 Conclusions In this paper we have considered the problem of comp"
D18-1184,P17-1147,0,0.0264384,"d find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of eac"
D18-1184,N09-1009,0,0.019384,"6), the use of inference algorithms as intermediate layers in end-to-end neural networks is a recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Cohen and Smith, 2009). The most recent work in this direction was the Neural E-DMV model of Jiang et al. (2016). While our goal is not to induce a grammar, we do produce a probabilistic grammar as a byproduct of our model. Our results suggest that training on more complex objectives may be a good way to pursue grammar induction in the future; forcing the model to construct consistent, comparable subtrees between the two sentences is a strong signal for grammar induction. Very recently, a few models attempt to infer latent dependency tree structures with neural models in sentence modeling tasks (Yogatama et al., 20"
D18-1184,P04-1061,0,0.182318,"del considers all possible span comparisons, weighted by the spans’ marginal likelihood. likely each span is to appear as a constituent in a parse of the sentence. We use the non-terminal nodes of a binary constituency parse to represent spans. Because of this choice of representation, we can use the nodes in a CKY parsing chart to efficiently marginalize span likelihood over all possible parses for each sentence, and compare nodes in each sentence’s chart. 3.1 Learning Latent Constituency Trees A constituency parser can be partially formalized as a graphical model with the following cliques (Klein and Manning, 2004): the latent variables cikj ∈ 0, 1 for all i < j, indicating whether the span from the i-th token to the j-th token (spanij ) is a constituency node built from the merging of sub-node spanik and span(k+1)j . Given a sentence x = [xi , · · · , xn ], the probability of a tree z is, Q cikj ∈z p(cikj = 1) Q p(z|x) = P (8) z 0 ∈Z cikj ∈z 0 p(cikj = 1) where Z represents all possible constituency trees for x. The parameters for the graph-based CRF constituency parser are δikj reflecting the scores of spanij forming a binary constituency node with k as the splitting point. It is possible to calculate"
D18-1184,W17-5301,0,0.0849449,"to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sent"
D18-1184,D16-1244,0,0.0890286,"Missing"
D18-1184,D14-1162,0,0.0837504,"t (Separated Parameters) QA-LSTM (Tan et al., 2016b) Attentive Pooling Network (Santos et al., 2016) Pairwise Word Interaction (He and Lin, 2016) Lexical Decomposition and Composition (Wang et al., 2016) Noise-Contrastive Estimation (Rao et al., 2016) BiMPM (Wang et al., 2017b) MAP 0.764 0.772 0.780 0.780 0.786 0.730 0.753 0.777 0.771 0.801 0.802 MRR 0.842 0.851 0.846 0.860 0.860 0.824 0.851 0.836 0.845 0.877 0.875 Table 1: Results of our models (top) and previously proposed systems (bottom) on the TREC-QA test set. For both tasks, we initialize our model with 300D 840B GloVe word embeddings (Pennington et al., 2014). The hidden size for the BiLSTM is 150. The feed-forward networks F1 and F2 are two-layer perceptrons with ReLU as the hidden activation function and the size of the hidden and output layers is set to 300. All hyperparameters are selected based on the model’s performance on the development set. 4.1 Answer Sentence Selection We first study the effectiveness of our model for answer sentence selection tasks. Given a question, answer sentence selection aims to rank a list of candidate answer sentences based on their relatedness to the question. We experiment on the TRECQA dataset (Wang et al., 20"
D18-1184,N16-1030,0,0.0192327,"l., 2017; Zhao et al., 2016). However, all of these models are pipelined; they obtain the sentence structure in a non-differentiable preprocessing step, losing the benefits of end-to-end training. Ours is the first model to allow comparison between latent tree structures, trained end-to-end on the comparison objective. Structured attention While it has long been known that inference in graphical models is differentiable (Li and Eisner, 2009; Domke, 2011), and using inference in, e.g., a CRF (Lafferty et al., 2001) as the last layer in a neural network is common practice (Liu and Lapata, 2017; Lample et al., 2016), the use of inference algorithms as intermediate layers in end-to-end neural networks is a recent development. Kim et al. (2017) were the first to use inference to compute structured attentions over latent sentence variables, inducing tree structures trained on the end-to-end objective. Liu and Lapata (2018) showed how to do this more efficiently, although their work is still limited to structured attention over a single sentence. Our model is the first to include latent structured alignments between two sentences. Grammar Induction Unsupervised grammar induction is a well-studied problem (Co"
D18-1184,D16-1264,0,0.0321899,"r sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate"
D18-1184,D09-1005,0,0.0804877,"a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between them, representing each span in each sentence with structured attention over spans in the other sentence. These 1554 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1554–1564 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics span repr"
D18-1184,D17-1133,1,0.925393,"inal node and the second term is the score for spanij being the left child. In Figure 2b, we illustrate the outside process with the target span spanij being the right 1556 child of a non-terminal node. This process is calculated recursively from root to bottom. The normalized marginal probability ρij for each span spanij , where 1 ≤ i < n, i < j ≤ n can be calculated by: ρij = αij βij /α0n (12) To compute the representations of all possible spans, we use Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber 1997) with max-pooling and minus features (Cross and Huang, 2016; Liu and Lapata, 2017). We represent each sentence as a sequence of word embeddings [wsos , w1 , · · · , wt , · · · , wn , weos ] and run a bidirectional LSTM to obtain the output vectors. ht = [~ht , h~t ] is the output vector for the tth word, and ~ht and h~t are the output vectors from the forward and backward directions, respectively. We represent a constituent from position i to j with a span vector spij : spmaxpool = max(hi , · · · , hj ) ij spminus = [~hj − ~hi−1 , h~i − h~j+1 ] ij spij = [spmaxpool , spminus ] ij ij (13) spans across the two sentences, and the attended vectors can be calculated as: eij,kl ="
D18-1184,Q18-1005,1,0.934904,"uashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence and aligning spans between the two sentences. Our method constructs a CKY chart for each sentence using the inside-outside algorithm (Manning et al., 1999), which is fully differentiable (Li and Eisner, 2009; Gormley et al., 2015). This chart has a node for each possible span in the sentence, and a score for the likelihood of that span being a constituent in a parse of the sentence, marginalized over all possible parses. We take these two charts and find alignments between t"
D18-1184,W09-3714,0,0.0422386,"sequence. There are some efforts to strengthen the decomposable attention model with a recurrent neural network (Liu and Lapata, 2018) or intrasentence attention (Parikh et al., 2016). However, these models amount to simply changing the input vectors a and b, and still only perform a wordlevel alignment between the two sentences. 3 Structured Alignment Networks Language is inherently tree structured, and the meaning of sentences comes largely from composing the meanings of subtrees (Chomsky, 2002). It is natural, then, to compare the meaning of two sentences by comparing their substructures (MacCartney and Manning, 2009). For example, when determining the relationship between two sentences in Figure 1, the ideal units of comparison are spans determined by subtrees: “is in Seattle” compared to “based in Washington state”. The challenge with comparing spans drawn from subtrees is that the tree structure of the sentence is latent and must be inferred, either during pre-processing or in the model itself. In this section we present a model that operates on the latent tree structure of each sentence, comparing all possible spans in one sentence with all possible spans in the second sentence, weighted by how 1555 A:"
D18-1184,P14-5010,0,0.0148714,"re our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation for each word. The second baseline is a Simple Span Alignment model; we use an MLP layer over the LSTM outputs to calculate the unnormalized scores and replace the inside-outside algorithm with a simple softmax function to obtain the probability distribution over all candidate spans. We also introduce a pipelined baseline where we extract constituents from trees parsed by the CoreNLP (Manning et al., 2014) constituency parser, and use the Simple Span Alignment model to only align these constituents. As shown in Table 1, we use two variants of the Structured Alignment model, since the structure of the question and the answer sentence may be different; the first model shares parameters across the question and the answer for computing the structures, while the second one uses separate parameters. We view the sentence selection task as a binary classification problem and the final ranking is based on the predicted probability of the sentence containing the correct answer (positive label). We apply"
D18-1184,P15-1150,0,0.324285,"Missing"
D18-1184,P16-1044,0,0.162701,"detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level co"
D18-1184,D07-1003,0,0.0413725,"n et al., 2014). The hidden size for the BiLSTM is 150. The feed-forward networks F1 and F2 are two-layer perceptrons with ReLU as the hidden activation function and the size of the hidden and output layers is set to 300. All hyperparameters are selected based on the model’s performance on the development set. 4.1 Answer Sentence Selection We first study the effectiveness of our model for answer sentence selection tasks. Given a question, answer sentence selection aims to rank a list of candidate answer sentences based on their relatedness to the question. We experiment on the TRECQA dataset (Wang et al., 2007), in which all questions with only positive or negative answers are removed. This leaves us with 1,162 training questions, 65 development questions and 68 test questions. Experimental results are listed in Table 1. We measure performance by the mean average precision (MAP) and mean reciprocal rank (MRR) using the standard TREC evaluation script. In the first block of Table 1, we compare our model and variants thereof against several baselines. The first baseline is the Word-level Decomposable Attention model strengthened with a bidirectional LSTM for obtaining a contextualized representation f"
D18-1184,P17-1018,0,0.242835,"ultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena. 1 Introduction There are many tasks in natural language processing that require matching two sentences: natural language inference (Bowman et al., 2015; Nangia et al., 2017) and paraphrase detection (Wang et al., 2017b) are classification tasks over sentence pairs, and question answering often requires an alignment between a question and a passage of text that may contain the answer (Tan et al., 2016a; Rajpurkar et al., 2016; Joshi et al., 2017). Most neural models for these tasks perform comparisons between the two sentences either at ∗ Work done during an internship at Allen Institute for Artificial Intelligence. the word level (Parikh et al., 2016), or at the sentence level (Bowman et al., 2015). Word-level comparisons ignore the inherent structure of the sentences being compared, at best relying on a r"
D18-1184,C16-1127,0,0.0591136,"Missing"
D18-1184,C16-1212,0,0.0745934,"comparisons ignore the inherent structure of the sentences being compared, at best relying on a recurrent neural network such as an LSTM (Hochreiter and Schmidhuber, 1997) to incorporate some amount of context from neighboring words into each word’s representation. Sentence-level comparisons can incorporate the structure of each sentence individually (Bowman et al., 2016; Tai et al., 2015), but cannot easily compare substructures between the sentences, as these are all squashed into a single vector. Some models do incorporate sentence structure by comparing subtrees between the two sentences (Zhao et al., 2016; Chen et al., 2017), but require pipelined approaches where a parser is run in a non-differentiable preprocessing step, losing the benefits of end-to-end training. In this paper we propose a method, which we call structured alignment networks, to perform comparisons between substructures in two sentences, in a more interpretable way, and without relying on an external, non-differentiable parser. We use a structured attention mechanism (Kim et al., 2017; Liu and Lapata, 2018) to compute a structured alignment between the two sentences, jointly learning a latent tree structure for each sentence"
D18-1206,N18-1150,0,0.418261,"osing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor e"
D18-1206,P16-1046,1,0.86461,"tem and state-of-the-art abstractive approaches when evaluated automatically and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing to"
D18-1206,P17-1012,0,0.256314,"h Broadcasting Corporation (BBC) that often include a firstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture longrange dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our convolutional encoder associates each word with a topic vector capturing whether it is representative of the document’s content, while our convolutional decoder conditions each word prediction on a document topic vector. Experimental results show that when evaluated automatically (in terms of ROUGE) our topicaware convolutional model outperforms an oracle extractive system and state-of-the-art RNN"
D18-1206,N18-1065,0,0.359024,", 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which create a summary by identifying (and subsequently concatenating) the most important sentences in a document (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018b). Abstractive approaches, despite being more faithful to the actual summarization task, either lag behind extractive ones or are mostly extractive, exhibiting a small degree of abstraction (See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018). In this paper we introduce extreme summariza1797 Proceedi"
D18-1206,J10-3005,1,0.862457,"Missing"
D18-1206,P16-1188,0,0.0555743,"abstractive approaches when evaluated automatically and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic anno"
D18-1206,W18-2706,0,0.0695067,"Missing"
D18-1206,P18-1082,0,0.0248868,"extremely 1799 [P A D . w on [P A D ] ] En gl an d conditioning each word prediction on the document topic vector. t0i ⊗ tD xi + p i e Convolutions f GLU f ⊗ f ⊗ ⊗ zu Attention ⊕ w w  P P P c` hL h` ⊗ f GLU ⊗ f ⊕ ⊗ f Convolutions . po rt re t ch po r re ] at M D [P A D ] [P A D ] [P A M at ch x0i + p0i tD g Figure 2: Topic-conditioned convolutional model for extreme summarization. high, and pertinent content can be easily missed. Recently, a convolutional alternative to sequence modeling has been proposed showing promise for machine translation (Gehring et al., 2017a,b) and story generation (Fan et al., 2018). We believe that convolutional architectures are attractive for our summarization task for at least two reasons. Firstly, contrary to recurrent networks which view the input as a chain structure, convolutional networks can be stacked to represent large context sizes. Secondly, hierarchical features can be extracted over larger and larger contents, allowing to represent long-range dependencies efficiently through shorter paths. Our model builds on the work of Gehring et al. (2017b) who develop an encoder-decoder architecture for machine translation with an attention mechanism (Sukhbaatar et al"
D18-1206,N03-1020,0,0.627338,"Missing"
D18-1206,P18-1188,1,0.923319,"elating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which"
D18-1206,N18-1158,1,0.9104,"elating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, these datasets often favor extractive models which"
D18-1206,D15-1044,0,0.353547,"ery different from a headline whose aim is to encourage readers to read the story; it draws on information interspersed in various parts of the document (not only the beginning) and displays multiple levels of abstraction including paraphrasing, fusion, synthesis, and inference. We build a dataset for the proposed task by harvesting online articles from the British Broadcasting Corporation (BBC) that often include a firstsentence summary. We further propose a novel deep learning model which we argue is well-suited to the extreme summarization task. Unlike most existing abstractive approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See et al., 2017; Tan and Wan, 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) which rely on an encoder-decoder architecture modeled by recurrent neural networks (RNNs), we present a topic-conditioned neural model which is based entirely on convolutional neural networks (Gehring et al., 2017b). Convolution layers capture longrange dependencies between words in the document more effectively compared to RNNs, allowing to perform document-level inference, abstraction, and paraphrasing. Our convolutional encoder associate"
D18-1206,E17-2007,0,0.157128,"Missing"
D18-1206,P17-1099,0,0.185195,"and by humans.1 1 Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-sca"
D18-1206,D16-1248,0,0.0229063,"Let D denote a document consisting of a sequence of words (w1 , . . . , wm ); we embed D into a distributional space x = (x1 , . . . , xm ) where xi ∈ Rf is a column in embedding matrix M ∈ RV ×f (where V is the vocabulary size). We also embed the absolute word positions in the document p = (p1 , . . . , pm ) where pi ∈ Rf is a column in position matrix 1800 P ∈ RN ×f , and N is the maximum number of positions. Position embeddings have proved useful for convolutional sequence modeling (Gehring et al., 2017b), because, in contrast to RNNs, they do not observe the temporal positions of words 0 (Shi et al., 2016). Let tD ∈ Rf be the topic distribution of document D and t0 = (t01 , . . . , t0m ) the topic distributions of words in the document 0 (where t0i ∈ Rf ). During encoding, we represent document D via e = (e1 , . . . , em ), where ei is: 0 ei = [(xi + pi ); (t0i ⊗ tD )] ∈ Rf +f , and ⊗ denotes point-wise multiplication. The topic distribution t0i of word wi essentially captures how topical the word is in itself (local context), whereas the topic distribution tD represents the overall theme of the document (global context). The encoder essentially enriches the context of the word with its topical"
D18-1206,P17-1108,0,0.49684,"Introduction Automatic summarization is one of the central problems in Natural Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summari"
D18-1206,N18-2102,0,0.377679,"Language Processing (NLP) posing several challenges relating to understanding (i.e., identifying important content) and generation (i.e., aggregating and rewording the identified content into a summary). Of the many summarization paradigms that have been identified over the years (see Mani, 2001 and Nenkova and McKeown, 2011 for a comprehensive overview), single-document summarization has consistently attracted attention (Cheng and Lapata, 2016; Durrett et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017; Narayan et al., 2017; Fan et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018; Narayan et al., 2018a,b). Neural approaches to NLP and their ability to learn continuous features without recourse to 1 Our dataset, code, and demo are available at: https: //github.com/shashiongithub/XSum. Figure 1: An abridged example from our extreme summarization dataset showing the document and its oneline summary. Document content present in the summary is color-coded. pre-processing tools or linguistic annotations have driven the development of large-scale document summarization datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018). However, th"
D18-1267,P11-1049,0,0.0673364,"ual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how"
D18-1267,2012.eamt-1.60,0,0.019139,"ssion model on three languages initialized with varying compression rates3 (see Section 4 for details on how the models were trained and tested). The compression rate (CR) is used to determine length parameter of Equation (8): Ty0 = Tx ·CR 1. Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment). 2. The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues. 3. The EU bookshop corpus (Skadin¸sˇ et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel. 4. The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs). (11) The figure shows how the output length varie"
D18-1267,D14-1082,0,0.0194995,"ource sentence so that it matches the compression ratio of the validation set. 5 Results M OSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006). These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1). We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples. Table 3(a) reports results on English with a model which controls the output length (L ) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10). We experimented with French (fr) or German (de) as pivot languages. All pivot-based models perform compression in a single step (see Section 2.3). Dual-step compres5 We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016). 6 We add a begin-of-sentence marker at the start of the candidate and re"
D18-1267,P16-1185,0,0.0629603,"Missing"
D18-1267,N16-1012,0,0.423667,"iting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015). Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or synta"
D18-1267,P06-1048,1,0.66521,"or, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German. Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set. 5 Results M OSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006). These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1). We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples. Table 3(a) reports results on English with a model which controls the output length (L ) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10). We experimented with French (fr) or German (de) as pivot languages. All pivot-based models perform compression in"
D18-1267,D15-1042,0,0.0906937,"Missing"
D18-1267,D13-1155,0,0.0471947,"ains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts). 2 Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million. 2453 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015). As a result, the training corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spurious and need to be filtered using language and domain specific heuristics). And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions. Our work addresses the paucity of data for sentence compression models. We argue that multilingual corpora are a rich source for lea"
D18-1267,D16-1026,0,0.0360291,"Missing"
D18-1267,N07-1023,0,0.0342067,"tion (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 20"
D18-1267,C12-1067,0,0.0168628,"peakers of the respective language who process a document at a time. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents. There has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing. Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text ge"
D18-1267,P17-2044,0,0.0126444,"e. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents. There has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing. Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is"
D18-1267,P09-1093,0,0.0294704,"mpressed by native speakers of the respective language who process a document at a time. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents. There has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing. Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model sca"
D18-1267,A00-1043,0,0.0392667,"Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeo"
D18-1267,D16-1140,0,0.0802519,"and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015). Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or syntactic information (e.g.,"
D18-1267,E17-1083,1,0.936309,"guage and domain specific heuristics). And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions. Our work addresses the paucity of data for sentence compression models. We argue that multilingual corpora are a rich source for learning a variety of rewrite rules across languages and that existing neural machine translation (NMT) models (Sutskever et al. 2014; Bahdanau et al. 2015) can be easily adapted to the compression task through bilingual pivoting (Mallinson et al., 2017) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements). We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length (Kikuchi et al., 2016). Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.We also demonstrate that models trained on multilingual data perform well out-of-domain. Although our appro"
D18-1267,E06-1038,0,0.0336769,"he most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural"
D18-1267,W11-1610,0,0.054439,"Missing"
D18-1267,2005.mtsummit-papers.11,0,0.0843565,"pressions in succession reduces both length and content conservatively and as a result produces more meaningful text. In Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions. We show the output of a single-step compression model on three languages initialized with varying compression rates3 (see Section 4 for details on how the models were trained and tested). The compression rate (CR) is used to determine length parameter of Equation (8): Ty0 = Tx ·CR 1. Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment). 2. The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues. 3. The EU bookshop corpus (Skadin¸sˇ et al., 2014) contains publications from European institutions covering a variety of topic"
D18-1267,P07-2045,0,0.00441645,"up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions. We experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in Table 2. 4 BLEU scores were calculated using mteval-v13a.pl. Comparison Systems We compared our m"
D18-1267,W04-1013,0,0.0134269,"y short summary (75 bytes) for a document. The evaluation set consists of 500 source documents (from the New York Times and Associated Press Wire services) each paired with four humanwritten (reference) summaries. We follow previous work (Rush et al., 2015; Chopra et al., 2016) in compressing the first sentence of the document and presenting this as the summary. To make the evaluation unbiased to length, the output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. Our results are shown in Table 7. To compare with existing methods, we also report ROUGE (Lin, 2004) unigram and bigram overlap (Lin, 2004) and the longest common subsequence (ROUGE -L).9 We employed a dual step compression model (see Section 2) as preliminary experiments showed that it was superior to singlestage variants. We compared single and multiple pivot models against existing ABS and ABS+ (Rush et al., 2015), two encoder-decoder models trained on the English Gigaword. ABS+ applies minimum error rate (MERT) training as a copy7 Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: 22.35 8 Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70 9 We used ROUG"
D18-1267,W15-1818,0,0.0195803,"sion specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the comp"
D18-1267,N03-1026,0,0.0657919,"uce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, ther"
D18-1267,D15-1044,0,0.129804,"nce compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015). Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features wit"
D18-1267,E17-3017,1,0.884756,"Missing"
D18-1267,P16-1009,1,0.824345,"ssion rate (CR), TER scores, and number of insertions (Ins), deletions (Del), substitutions (Sub), and shifts (Shft). We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions. We experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly co"
D18-1267,P16-1162,1,0.22348,"ssion rate (CR), TER scores, and number of insertions (Ins), deletions (Del), substitutions (Sub), and shifts (Shft). We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions. We experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly co"
D18-1267,skadins-etal-2014-billions,0,0.0607245,"Missing"
D18-1267,D17-1062,1,0.872488,"Missing"
D18-1267,2006.amta-papers.25,0,0.0562624,"ee column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between. We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances. Overall, French speakers seem more conservative when shortening sentences compared to English and German. In general, compression rates are genre dependent, they range from 0.58 (for English Europarl) to 0.84 (for German TED). We also examined the degree to which crowdworkers paraphrase the source sentence using Translation Edit Rate (TER; Snover et al., 2006), a measure commonly used to automatically evaluate the quality of machine translation output. We used TER to compute the (average) number of edits required to change a long sentence to shorter output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences. We observe that crowdworkers perform a fair amount of rewriting across corpora and languages. The most frequent rewrite operations are deletions followed by substitutions, shifts, and insertions. 4 Experimental Setup Neural Machi"
D18-1267,D16-1033,0,0.198601,"to achieve good performance, they require large amounts of training data, in the region of millions of long-short sentence pairs.2 Existing compression datasets are several orders of magnitude smaller. For example, the ZiffDavis corpus (Knight and Marcu, 2002) contains 1,067 sentences and originated from a collection of news articles on computer products. Clarke and Lapata (2008) create two manual corpora sampled from written (1,433 sentences) and spoken sources (1,370 sentences). Cohn and Lapata (2013) elicit manual compressions for 625 sentences taken from newspaper articles. More recently, Toutanova et al. (2016) crowdsource a larger corpus which contains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts). 2 Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million. 2453 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with"
D18-1267,P05-1036,0,0.0660819,"gle sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in"
D18-1267,N07-1061,0,0.0258365,"d language at hand. We determine the target length experimentally based on a small validation set. 2.3 Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016). We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form. We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1 , ..., fK }. The probability of generating compression y = y1 ...yTy is decomposed as: Fx → − ← − P(y|x) = ∑ P(y |f ; θ ) · P( f |x; θ ) (9) f which we approximate as the tokenwise weighted average of the pivots: j=1 2.2 Length Control Ty Fx To be able to p"
D18-1267,W04-1015,0,0.0804665,"es without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://gith"
D18-1267,P10-1058,1,0.778912,"anguage as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lap"
D18-1267,P07-1108,0,0.0475611,"e domain, genre, and language at hand. We determine the target length experimentally based on a small validation set. 2.3 Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016). We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form. We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1 , ..., fK }. The probability of generating compression y = y1 ...yTy is decomposed as: Fx → − ← − P(y|x) = ∑ P(y |f ; θ ) · P( f |x; θ ) (9) f which we approximate as the tokenwise weighted average of the pivots: j=1 2.2 Length C"
D18-1403,Q18-1002,1,0.862845,"inions to the user (see Figure 1 for an illustration of the task). A number of techniques have been proposed for aspect discovery using part of speech tagging (Hu and Liu, 2004), syntactic parsing (Lu et al., 2009), clustering (Mei et al., 2007; Titov and McDonald, 2008b), data mining (Ku et al., 2006), and information extraction (Popescu and Etzioni, 2005). Various lexicon and rule-based methods (Hu and Liu, 2004; Ku et al., 2006; Blair-Goldensohn et al., 2008) have been adopted for sentiment prediction together with a few learning approaches (Lu et al., 2009; Pappas and Popescu-Belis, 2017; Angelidis and Lapata, 2018). As for the summaries, a common format involves a list of aspects and the number of positive and negative opinions for each (Hu and Liu, 2004). While this format gives an overall idea of people’s opinion, reading the actual text might be necessary to gain a better understanding of specific details. Textual summaries are created following mostly extractive methods (but see Ganesan et al. 2010 for an abstractive approach), and various formats ranging from lists of words (Popescu and Etzioni, 2005), to phrases (Lu et al., 2009), and sentences (Mei et al., 2007; BlairGoldensohn et al., 2008; Lerm"
D18-1403,D15-1263,0,0.0278987,"of reviews Re = {ri }i=1e expressing customers’ opinions. Each review ri is accompanied by the author’s overall rating yi and is split into segments (s1 , . . . , sm ), where each segment sj is in turn viewed as a sequence of words (wj1 , . . . , wjn ). A segment can be a sentence, a phrase, or in our case an Elementary Discourse Unit (EDU; Mann and Thompson 1988) obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012). EDUs roughly correspond to clauses and have been shown to facilitate performance in summarization (Li et al., 2016), document-level sentiment analysis (Bhatia et al., 2015), and single-document opinion extraction (Angelidis and Lapata, 2018). A segment may discuss zero or more aspects, i.e., different product attributes. We use AC = {ai }K i=1 to refer to the aspects pertaining to domain dC . For example, picture quality, sound quality, and connectivity are all aspects of televisions. By convention, a general aspect is assigned to segments that do not discuss any specific aspects. Let As ⊆ AC denote the set of aspects Aspect Extraction Our work builds on the aspect discovery model developed by He et al. (2017), which we extend to facilitate the accurate extracti"
D18-1403,N16-1180,0,0.080342,"Missing"
D18-1403,D14-1181,0,0.00413914,"ereas Yin et al. (2016) use dependency-based embeddings as features in a Conditional Random Field (CRF). Wang et al. (2016) combine a recursive neural network with CRFs to jointly model aspect and sentiment terms. He et al. (2017) propose an aspect-based autoencoder to discover fine-grained aspects without supervision, in a process similar to topic modeling. Their model outperforms LDA-style approaches and forms the basis of our aspect extractor. Sentiment Prediction Fully-supervised approaches based on neural networks have achieved impressive results on fine-grained sentiment classification (Kim, 2014; Socher et al., 2013). More recently, Multiple Instance Learning (MIL) models have been proposed that use freely available review ratings to train segment-level predictors. Kotzias et al. (2015) and Pappas and Popescu-Belis (2017) train sentence-level predictors under a MIL objective, while our previous work (Angelidis and Lapata, 2018) introduced M IL N ET, a hierarchical model that is trained end-to-end on document labels and produces polarity-based opinion summaries of single reviews. Here, we use M IL N ET to predict the sentiment polarity of individual opinions. 3676 Multi-document Summa"
D18-1403,P17-2074,0,0.0388593,"ur model (see the supplementary material for additional results). We also performed a large-scale user study. For every product in the O PO S UM test set, participants were asked to compare summaries produced by: a (randomly selected) human annotator, our best performing model (M IL N ET+MATE+MT+RD), Opinosis, and the Lead baseline. The study was conducted on the Crowdflower platform using Best-Worst Scaling (BWS; Louviere and Woodworth 1991; Louviere et al. 2015), a less labourintensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). We arranged every 4-tuple of competing summaries into four triplets. Every triplet was 3682 This work Opinosis LexRank Human Product domain: Televisions Product name: Sony BRAVIA 46-Inch HDTV Plenty of ports and settings. Easy hookups to audio and satellite sources. The sound is good and strong. This TV looks very good. and the price is even better. The on-screen menu/options is quite nice. and the internet apps work as expected. The picture is clear and sharp. which is TOO SLOW to stream HD video... The software and apps built into this TV. are difficult to use and setup. Their service is h"
D18-1403,W04-3250,0,0.0570409,"pinosis, a graph-based abstractive summarizer that is designed for opinion summarization (Ganesan et al., 2010). All extractive methods operate on the EDU level with a 100-word budget. For Opinosis, we tested an aspect-agnostic variant that takes every review segment for a product as input, and a variant that uses MATE’s groupings of segments to produce and concatenate aspect-specific summaries. Table 5 presents ROUGE-1, ROUGE-2 and ROUGE-L F1 scores, averaged across domains. Our model (M IL N ET+MATE+MT) significantly outperforms all comparison systems (p < 0.05; paired bootstrap resampling; Koehn 2004), whilst using a redundancy filter slightly improves performance. Assisting Opinosis with aspect predictions is beneficial, however, it remains significantly inferior to our model (see the supplementary material for additional results). We also performed a large-scale user study. For every product in the O PO S UM test set, participants were asked to compare summaries produced by: a (randomly selected) human annotator, our best performing model (M IL N ET+MATE+MT+RD), Opinosis, and the Lead baseline. The study was conducted on the Crowdflower platform using Best-Worst Scaling (BWS; Louviere an"
D18-1403,P12-1007,0,0.0551073,"t C denote a corpus of reviews on a set of prod|E | ucts EC = {ei }i=1C from a domain dC , e.g., televisions or keyboards. For every product e, the |R | corpus contains a set of reviews Re = {ri }i=1e expressing customers’ opinions. Each review ri is accompanied by the author’s overall rating yi and is split into segments (s1 , . . . , sm ), where each segment sj is in turn viewed as a sequence of words (wj1 , . . . , wjn ). A segment can be a sentence, a phrase, or in our case an Elementary Discourse Unit (EDU; Mann and Thompson 1988) obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012). EDUs roughly correspond to clauses and have been shown to facilitate performance in summarization (Li et al., 2016), document-level sentiment analysis (Bhatia et al., 2015), and single-document opinion extraction (Angelidis and Lapata, 2018). A segment may discuss zero or more aspects, i.e., different product attributes. We use AC = {ai }K i=1 to refer to the aspects pertaining to domain dC . For example, picture quality, sound quality, and connectivity are all aspects of televisions. By convention, a general aspect is assigned to segments that do not discuss any specific aspects. Let As ⊆ A"
D18-1403,C10-1039,0,0.90527,"and Liu, 2004; Ku et al., 2006; Blair-Goldensohn et al., 2008) have been adopted for sentiment prediction together with a few learning approaches (Lu et al., 2009; Pappas and Popescu-Belis, 2017; Angelidis and Lapata, 2018). As for the summaries, a common format involves a list of aspects and the number of positive and negative opinions for each (Hu and Liu, 2004). While this format gives an overall idea of people’s opinion, reading the actual text might be necessary to gain a better understanding of specific details. Textual summaries are created following mostly extractive methods (but see Ganesan et al. 2010 for an abstractive approach), and various formats ranging from lists of words (Popescu and Etzioni, 2005), to phrases (Lu et al., 2009), and sentences (Mei et al., 2007; BlairGoldensohn et al., 2008; Lerman et al., 2009; Wang and Ling, 2016). In this paper, we present a neural framework for opinion extraction from product reviews. We follow the standard architecture for aspect-based summarization, while taking advantage of the success of neural network models in learning continuous features without recourse to preprocessing tools or linguistic annotations. Central to our system is the ability"
D18-1403,P17-1036,0,0.439187,"tational Linguistics Figure 1: Aspect-based opinion summarization. Opinions on image quality, sound quality, connectivity, and price of an LCD television are extracted from a set of reviews. Their polarities are then used to sort them into positive and negative, while neutral or redundant comments are discarded. specific opinions by using different sources of information freely available with product reviews (product domain labels, user ratings) and minimal domain knowledge (essentially a few aspectdenoting keywords). We incorporate these ideas into a recently proposed aspect discovery model (He et al., 2017) which we combine with a weakly supervised sentiment predictor (Angelidis and Lapata, 2018) to identify highly salient opinions. Our system outputs extractive summaries using a greedy algorithm to minimize redundancy. Our approach takes advantage of weak supervision signals only, requires minimal human intervention and no gold-standard salience labels or summaries for training. Our contributions in this work are three-fold: a novel neural framework for the identification and extraction of salient customer opinions that combines aspect and sentiment information and does not require unrealistic"
D18-1403,E09-1059,0,0.289686,"018). As for the summaries, a common format involves a list of aspects and the number of positive and negative opinions for each (Hu and Liu, 2004). While this format gives an overall idea of people’s opinion, reading the actual text might be necessary to gain a better understanding of specific details. Textual summaries are created following mostly extractive methods (but see Ganesan et al. 2010 for an abstractive approach), and various formats ranging from lists of words (Popescu and Etzioni, 2005), to phrases (Lu et al., 2009), and sentences (Mei et al., 2007; BlairGoldensohn et al., 2008; Lerman et al., 2009; Wang and Ling, 2016). In this paper, we present a neural framework for opinion extraction from product reviews. We follow the standard architecture for aspect-based summarization, while taking advantage of the success of neural network models in learning continuous features without recourse to preprocessing tools or linguistic annotations. Central to our system is the ability to accurately identify aspect3675 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3675–3686 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computat"
D18-1403,W16-3617,0,0.0189504,"For every product e, the |R | corpus contains a set of reviews Re = {ri }i=1e expressing customers’ opinions. Each review ri is accompanied by the author’s overall rating yi and is split into segments (s1 , . . . , sm ), where each segment sj is in turn viewed as a sequence of words (wj1 , . . . , wjn ). A segment can be a sentence, a phrase, or in our case an Elementary Discourse Unit (EDU; Mann and Thompson 1988) obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012). EDUs roughly correspond to clauses and have been shown to facilitate performance in summarization (Li et al., 2016), document-level sentiment analysis (Bhatia et al., 2015), and single-document opinion extraction (Angelidis and Lapata, 2018). A segment may discuss zero or more aspects, i.e., different product attributes. We use AC = {ai }K i=1 to refer to the aspects pertaining to domain dC . For example, picture quality, sound quality, and connectivity are all aspects of televisions. By convention, a general aspect is assigned to segments that do not discuss any specific aspects. Let As ⊆ AC denote the set of aspects Aspect Extraction Our work builds on the aspect discovery model developed by He et al. (2"
D18-1403,N03-1020,0,0.580413,"Missing"
D18-1403,P12-1034,0,0.0278681,"bias parameters. The segment’s vector is then reconstructed as the weighted sum of aspect embeddings: rs = AT pasp s . (5) The model is trained by minimizing a reconstruction loss Jr (θ) that uses randomly sampled segments n1 , n2 , . . . , nkn as negative examples:2 Jr (θ) = kn XX max(0, 1 − rs vs + rs vni ) (6) s∈C i=1 ABAE is essentially a neural topic model; it discovers topics which will hopefully map to aspects, without any preconceptions about the aspects themselves, a feature shared with most previous LDA-style aspect extraction approaches (Titov and McDonald, 2008a; He et al., 2017; Mukherjee and Liu, 2012). These models will set the number of topics to be discovered to a much larger number (∼ 15) than the actual aspects found in the data (∼ 5). This requires a many-to-one mapping between discovered topics and genuine aspects which is performed manually. 2 ABAE also uses a uniqueness regularization term that is not shown here and is not used in our Multi-Seed Aspect Extractor model. Figure 2: Multi-Seed Aspect Extractor (MATE). 4.2 Multi-Seed Aspect Extractor Dynamic aspect extraction is advantageous since it assumes nothing more than a set of relevant reviews for a product and may discover unus"
D18-1403,D15-1168,0,0.034753,"n summarization and related tasks. For a comprehensive overview of non-neural methods we refer the interested reader to Kim et al. (2011) and Liu and Zhang (2012). We are not aware of previous studies which propose a neural-based system for end-to-end opinion summarization without direct supervision, although as we discuss below, recent efforts tackle various subtasks independently. Aspect Extraction Several neural network models have been developed for the identification of aspects (e.g., words or phrases) expressed in opinions. This is commonly viewed as a supervised sequence labeling task; Liu et al. (2015) employ recurrent neural networks, whereas Yin et al. (2016) use dependency-based embeddings as features in a Conditional Random Field (CRF). Wang et al. (2016) combine a recursive neural network with CRFs to jointly model aspect and sentiment terms. He et al. (2017) propose an aspect-based autoencoder to discover fine-grained aspects without supervision, in a process similar to topic modeling. Their model outperforms LDA-style approaches and forms the basis of our aspect extractor. Sentiment Prediction Fully-supervised approaches based on neural networks have achieved impressive results on fi"
D18-1403,H05-1043,0,0.238565,"ife, sound quality, ease of use) and identify expressions that discuss them; (2) sentiment prediction which determines the sentiment orientation (positive or negative) on the aspects found in the first step, and (3) summary generation which presents the identified opinions to the user (see Figure 1 for an illustration of the task). A number of techniques have been proposed for aspect discovery using part of speech tagging (Hu and Liu, 2004), syntactic parsing (Lu et al., 2009), clustering (Mei et al., 2007; Titov and McDonald, 2008b), data mining (Ku et al., 2006), and information extraction (Popescu and Etzioni, 2005). Various lexicon and rule-based methods (Hu and Liu, 2004; Ku et al., 2006; Blair-Goldensohn et al., 2008) have been adopted for sentiment prediction together with a few learning approaches (Lu et al., 2009; Pappas and Popescu-Belis, 2017; Angelidis and Lapata, 2018). As for the summaries, a common format involves a list of aspects and the number of positive and negative opinions for each (Hu and Liu, 2004). While this format gives an overall idea of people’s opinion, reading the actual text might be necessary to gain a better understanding of specific details. Textual summaries are created f"
D18-1403,P03-1048,0,0.191879,"Missing"
D18-1403,D13-1170,0,0.00665297,"t al. (2016) use dependency-based embeddings as features in a Conditional Random Field (CRF). Wang et al. (2016) combine a recursive neural network with CRFs to jointly model aspect and sentiment terms. He et al. (2017) propose an aspect-based autoencoder to discover fine-grained aspects without supervision, in a process similar to topic modeling. Their model outperforms LDA-style approaches and forms the basis of our aspect extractor. Sentiment Prediction Fully-supervised approaches based on neural networks have achieved impressive results on fine-grained sentiment classification (Kim, 2014; Socher et al., 2013). More recently, Multiple Instance Learning (MIL) models have been proposed that use freely available review ratings to train segment-level predictors. Kotzias et al. (2015) and Pappas and Popescu-Belis (2017) train sentence-level predictors under a MIL objective, while our previous work (Angelidis and Lapata, 2018) introduced M IL N ET, a hierarchical model that is trained end-to-end on document labels and produces polarity-based opinion summaries of single reviews. Here, we use M IL N ET to predict the sentiment polarity of individual opinions. 3676 Multi-document Summarization A few extract"
D18-1403,P08-1036,0,0.0485097,"ch aims to find specific features pertaining to the entity of interest (e.g., battery life, sound quality, ease of use) and identify expressions that discuss them; (2) sentiment prediction which determines the sentiment orientation (positive or negative) on the aspects found in the first step, and (3) summary generation which presents the identified opinions to the user (see Figure 1 for an illustration of the task). A number of techniques have been proposed for aspect discovery using part of speech tagging (Hu and Liu, 2004), syntactic parsing (Lu et al., 2009), clustering (Mei et al., 2007; Titov and McDonald, 2008b), data mining (Ku et al., 2006), and information extraction (Popescu and Etzioni, 2005). Various lexicon and rule-based methods (Hu and Liu, 2004; Ku et al., 2006; Blair-Goldensohn et al., 2008) have been adopted for sentiment prediction together with a few learning approaches (Lu et al., 2009; Pappas and Popescu-Belis, 2017; Angelidis and Lapata, 2018). As for the summaries, a common format involves a list of aspects and the number of positive and negative opinions for each (Hu and Liu, 2004). While this format gives an overall idea of people’s opinion, reading the actual text might be nece"
D18-1403,N16-1007,0,0.250088,"aries, a common format involves a list of aspects and the number of positive and negative opinions for each (Hu and Liu, 2004). While this format gives an overall idea of people’s opinion, reading the actual text might be necessary to gain a better understanding of specific details. Textual summaries are created following mostly extractive methods (but see Ganesan et al. 2010 for an abstractive approach), and various formats ranging from lists of words (Popescu and Etzioni, 2005), to phrases (Lu et al., 2009), and sentences (Mei et al., 2007; BlairGoldensohn et al., 2008; Lerman et al., 2009; Wang and Ling, 2016). In this paper, we present a neural framework for opinion extraction from product reviews. We follow the standard architecture for aspect-based summarization, while taking advantage of the success of neural network models in learning continuous features without recourse to preprocessing tools or linguistic annotations. Central to our system is the ability to accurately identify aspect3675 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3675–3686 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Figu"
D18-1403,D16-1059,0,0.0291624,"12). We are not aware of previous studies which propose a neural-based system for end-to-end opinion summarization without direct supervision, although as we discuss below, recent efforts tackle various subtasks independently. Aspect Extraction Several neural network models have been developed for the identification of aspects (e.g., words or phrases) expressed in opinions. This is commonly viewed as a supervised sequence labeling task; Liu et al. (2015) employ recurrent neural networks, whereas Yin et al. (2016) use dependency-based embeddings as features in a Conditional Random Field (CRF). Wang et al. (2016) combine a recursive neural network with CRFs to jointly model aspect and sentiment terms. He et al. (2017) propose an aspect-based autoencoder to discover fine-grained aspects without supervision, in a process similar to topic modeling. Their model outperforms LDA-style approaches and forms the basis of our aspect extractor. Sentiment Prediction Fully-supervised approaches based on neural networks have achieved impressive results on fine-grained sentiment classification (Kim, 2014; Socher et al., 2013). More recently, Multiple Instance Learning (MIL) models have been proposed that use freely"
D18-1403,K17-1045,0,0.131607,"M IL N ET, a hierarchical model that is trained end-to-end on document labels and produces polarity-based opinion summaries of single reviews. Here, we use M IL N ET to predict the sentiment polarity of individual opinions. 3676 Multi-document Summarization A few extractive neural models have been recently applied to generic multi-document summarization. Cao et al. (2015) train a recursive neural network using a ranking objective to identify salient sentences, while follow-up work (Cao et al., 2017) employs a multi-task objective to improve sentence extraction, an idea we adapted to our task. Yasunaga et al. (2017) propose a graph convolution network to represent sentence relations and estimate sentence salience. Our summarization method is tailored to the opinion extraction task, it identifies aspect-specific and salient units, while minimizing the redundancy of the final summary with a greedy selection algorithm (Cao et al., 2015; Yasunaga et al., 2017). Redundancy is also addressed in Ganesan et al. (2010) who propose a graph-based framework for abstractive summarization. Wang and Ling (2016) introduce an encoder-decoder neural method for extractive opinion summarization. Their approach requires dire"
D19-1094,W11-2136,0,0.0144444,"other languages, including Chinese, German, and Spanish. 1 Introduction Semantic role labeling — the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents — has enjoyed renewed interest in the last few years thanks to the popularity of neural network models and their ability to learn continuous representations which forego the need for extensive feature engineering. Recent modeling developments aside, semantic role labeling (SRL) has been generally recognized as a core task in NLP with relevance for applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018), to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). State-of-the art semantic role labelers (He et al., 2018b; Cai et al., 2018) rely on high-quality annotations (of semantic predicates and their arguments) for use in training. These annotations are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains) motivating the need for effective semi-supervised methods that leverage unlabeled examples. CrossView Training (CVT; Clark et al. 2018) is a recently proposed semi-supervised lea"
D19-1094,C10-3009,0,0.0853893,"Missing"
D19-1094,Q17-1010,0,0.0360554,"Missing"
D19-1094,C18-1233,0,0.40745,"Missing"
D19-1094,Q16-1026,0,0.0656444,"Missing"
D19-1094,D18-1217,0,0.388633,"lications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018), to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). State-of-the art semantic role labelers (He et al., 2018b; Cai et al., 2018) rely on high-quality annotations (of semantic predicates and their arguments) for use in training. These annotations are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains) motivating the need for effective semi-supervised methods that leverage unlabeled examples. CrossView Training (CVT; Clark et al. 2018) is a recently proposed semi-supervised learning algorithm that improves representation learning using a mix of labeled and unlabeled data. The main idea behind CVT is to train a model to produce consistent predictions across different restricted views of the input with the aid of auxiliary prediction tasks. Clark et al. (2018) demonstrate performance gains when applying CVT to sequence tagging tasks, machine translation, and dependency parsing. Unfortunately, application of CVT to semantic role labeling is fraught with difficulty. This is partly due to the nature of the task which relies on v"
D19-1094,P10-1025,0,0.0346921,"ans to improve a main task by jointly learning one or more related auxiliary tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Swayamdipta et al., 2017; Peng et al., 2017; Strubell et al., 2018). The idea of resorting to semi-supervised learning as a means of reducing the annotation effort involved in creating labeled data for SRL is by no means new. F¨urstenau and Lapata (2012) propose to augment a labeled dataset with unlabeled examples whose roles are inferred automatically via annotation projection. Other work uses a language model to learn word similarities from unlabeled texts (Croce et al., 2010; Deschacht and Moens, 2009) or constructs an informed prior from labeled data in order to learn a generative model from unlabeled data (Titov and Klementiev, 2012). More recently, Mehta et al. (2018) have proposed a semi-supervised method for constituencybased SRL. Their work builds upon a state-of-theart neural model (He et al., 2018b; Peters et al., 2018) whose training objective they augment with a syntactic inconsistency loss component. Their hypothesis is that by leveraging syntactic structure during training, the SRL model may become more robust in low resource scenarios. This method is"
D19-1094,D09-1003,0,0.105308,"Missing"
D19-1094,P15-1033,0,0.0506759,"Missing"
D19-1094,D15-1112,0,0.124114,"Missing"
D19-1094,J12-1005,1,0.882439,"Missing"
D19-1094,P18-2058,0,0.295942,"ntential constituents — has enjoyed renewed interest in the last few years thanks to the popularity of neural network models and their ability to learn continuous representations which forego the need for extensive feature engineering. Recent modeling developments aside, semantic role labeling (SRL) has been generally recognized as a core task in NLP with relevance for applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018), to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). State-of-the art semantic role labelers (He et al., 2018b; Cai et al., 2018) rely on high-quality annotations (of semantic predicates and their arguments) for use in training. These annotations are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains) motivating the need for effective semi-supervised methods that leverage unlabeled examples. CrossView Training (CVT; Clark et al. 2018) is a recently proposed semi-supervised learning algorithm that improves representation learning using a mix of labeled and unlabeled data. The main idea behind CVT is to train a model to produce consistent predictions acr"
D19-1094,P17-1044,0,0.0471609,"m unlabeled data (Titov and Klementiev, 2012). More recently, Mehta et al. (2018) have proposed a semi-supervised method for constituencybased SRL. Their work builds upon a state-of-theart neural model (He et al., 2018b; Peters et al., 2018) whose training objective they augment with a syntactic inconsistency loss component. Their hypothesis is that by leveraging syntactic structure during training, the SRL model may become more robust in low resource scenarios. This method is very much geared towards improving constituentbased SRL, where syntactic constraints are widely used during decoding (He et al., 2017, 2018a). And requires a robust syntactic parser to analyze (out-of-domain) unlabeled sentences for consistency. Our model does not rely on external tools, and is generally applicable across semantic role representations based on dependencies or constituents (i.e., phrases or spans). However, we leave experiments on the latter for future work. Although the focus of this work has been on semi-supervised learning, we have developed a competitive SRL system which could be used on its own, after being trained on labeled data. Following previous work (Strubell et al., 2018; Cai et al., 2018) we pro"
D19-1094,P18-1192,0,0.361671,"ntential constituents — has enjoyed renewed interest in the last few years thanks to the popularity of neural network models and their ability to learn continuous representations which forego the need for extensive feature engineering. Recent modeling developments aside, semantic role labeling (SRL) has been generally recognized as a core task in NLP with relevance for applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018), to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). State-of-the art semantic role labelers (He et al., 2018b; Cai et al., 2018) rely on high-quality annotations (of semantic predicates and their arguments) for use in training. These annotations are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains) motivating the need for effective semi-supervised methods that leverage unlabeled examples. CrossView Training (CVT; Clark et al. 2018) is a recently proposed semi-supervised learning algorithm that improves representation learning using a mix of labeled and unlabeled data. The main idea behind CVT is to train a model to produce consistent predictions acr"
D19-1094,N15-1121,0,0.159508,"Missing"
D19-1094,D18-1262,0,0.0746492,"Missing"
D19-1094,N15-1142,0,0.0387245,"Missing"
D19-1094,P16-1101,0,0.0587637,"Missing"
D19-1094,N18-2078,0,0.0771282,"cluding Chinese, German, and Spanish. 1 Introduction Semantic role labeling — the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents — has enjoyed renewed interest in the last few years thanks to the popularity of neural network models and their ability to learn continuous representations which forego the need for extensive feature engineering. Recent modeling developments aside, semantic role labeling (SRL) has been generally recognized as a core task in NLP with relevance for applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018), to information extraction (Christensen et al., 2011), and summarization (Khan et al., 2015). State-of-the art semantic role labelers (He et al., 2018b; Cai et al., 2018) rely on high-quality annotations (of semantic predicates and their arguments) for use in training. These annotations are costly to obtain, and mostly unavailable in low resource scenarios (e.g., rare languages or domains) motivating the need for effective semi-supervised methods that leverage unlabeled examples. CrossView Training (CVT; Clark et al. 2018) is a recently proposed semi-supervised learning algorithm that improve"
D19-1094,K17-1041,0,0.104426,"tation learning using a mix of labeled and unlabeled data. The main idea behind CVT is to train a model to produce consistent predictions across different restricted views of the input with the aid of auxiliary prediction tasks. Clark et al. (2018) demonstrate performance gains when applying CVT to sequence tagging tasks, machine translation, and dependency parsing. Unfortunately, application of CVT to semantic role labeling is fraught with difficulty. This is partly due to the nature of the task which relies on various syntactic features, even when conceptualized as a sequence labeling task (Marcheggiani et al., 2017; He et al., 2018b; Cai et al., 2018). The reliance on syntactic features is problematic for semi-supervised training, since these would have to be extracted from large amounts of unlabeled data. In addition, any semantic role labeler would need to identify (and disambiguate) predicates prior to labeling their augments which might be given during training (e.g., as in the CoNLL 2009 dataset), but would still have to be detected on unlabeled data. Resorting to various pre-processing tools for semi-supervised training almost defeats the purpose of using unlabeled data which would have to be anno"
D19-1094,D18-1538,0,0.0161671,"018). The idea of resorting to semi-supervised learning as a means of reducing the annotation effort involved in creating labeled data for SRL is by no means new. F¨urstenau and Lapata (2012) propose to augment a labeled dataset with unlabeled examples whose roles are inferred automatically via annotation projection. Other work uses a language model to learn word similarities from unlabeled texts (Croce et al., 2010; Deschacht and Moens, 2009) or constructs an informed prior from labeled data in order to learn a generative model from unlabeled data (Titov and Klementiev, 2012). More recently, Mehta et al. (2018) have proposed a semi-supervised method for constituencybased SRL. Their work builds upon a state-of-theart neural model (He et al., 2018b; Peters et al., 2018) whose training objective they augment with a syntactic inconsistency loss component. Their hypothesis is that by leveraging syntactic structure during training, the SRL model may become more robust in low resource scenarios. This method is very much geared towards improving constituentbased SRL, where syntactic constraints are widely used during decoding (He et al., 2017, 2018a). And requires a robust syntactic parser to analyze (out-o"
D19-1094,P17-1186,0,0.045026,"Missing"
D19-1094,N18-1202,0,0.699143,"m all tasks subsidiary to semantic role labeling (i.e., POS tagging, dependency parsing, and predicate detection) on labeled and unlabeled data. The sentence learner is jointly trained with a semantic role labeler, and its outputs are fed to the semantic role labeler during supervised and semisupervised learning. Aside from building a selfsufficient semantic role labeler which can be directly applied on plain text, an added benefit of the proposed approach is that the sentence learner naturally provides multiple hidden layers (for the various subtasks) from which “multi-task hidden features” (Peters et al., 2018) can be extracted. to outperform the state of the art in English, and to improve SRL performance in other languages, including Chinese, German, and Spanish. 2 Model Description Figure 1 provides a schematic overview of our model which has two main components, namely a sentence learner and a semantic role labeler. The sentence learner consists of: In addition to overcoming the difficulty of utilizing plain text for semi-supervised SRL, we show that application of CVT to SRL requires special attention over and above the sequence tagging and dependency parsing tasks discussed in Clark et al. (201"
D19-1094,P16-1113,1,0.855516,"d examples as input and calculates the CVT loss given in Equation (8). The semi-supervised objective is the sum of the CVT loss for all auxiliary modules across all tasks. 3 Experiments We implemented our model1 in PyTorch and evaluated it on the English CoNLL-2009 benchmark following the standard training, testing, and development set splits. To evaluate whether our model generalizes to other languages, we also report experiments on Chinese, German, and Spanish, again using standard CoNLL-2009 splits. This subset of languages has been commonly used in previous work (Bj¨orkelund et al., 2010; Roth and Lapata, 2016; Lei et al., 2015) and allows us to compare our model against a wide range of alternative approaches. The benchmarks contain goldstandard dependency annotations, and also gold lemmas, part-of-speech tags, and morphological features. With regard to unlabeled datasets, we used the 1 Billion Word Language Model Benchmark (Chelba et al., 2013) for English, the Sougou News Data2 for Chinese, the NEGRA corpus 3 for 1 Our code is publicly available at https://github. com/RuiCaiNLP/SemiSRL. 2 www.sogou.com/labs/resource/ca.php 3 www.coli.uni-sb.de/sfb378/ negra-corpus/ 1022 Single Models Bj¨orkelund"
D19-1094,P16-2038,0,0.0354364,"less frequent verbs. The strategy of randomly selecting a word from the sentence performs better, precisely because it pays attention to a wider range of predicates. 4 Related Work Our model resonates the recent trend of developing neural network models for semantic role labeling using relatively simple architectures based on bidirectional LSTMs (Marcheggiani et al., 2017; Cai et al., 2018; Strubell et al., 2018). It also agrees with previous work in adopting multi-task learning as a means to improve a main task by jointly learning one or more related auxiliary tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Swayamdipta et al., 2017; Peng et al., 2017; Strubell et al., 2018). The idea of resorting to semi-supervised learning as a means of reducing the annotation effort involved in creating labeled data for SRL is by no means new. F¨urstenau and Lapata (2012) propose to augment a labeled dataset with unlabeled examples whose roles are inferred automatically via annotation projection. Other work uses a language model to learn word similarities from unlabeled texts (Croce et al., 2010; Deschacht and Moens, 2009) or constructs an informed prior from labeled data in order to learn a generative model"
D19-1094,D18-1548,0,0.0987665,"Missing"
D19-1094,C12-1161,0,0.0308731,"2017; Peng et al., 2017; Strubell et al., 2018). The idea of resorting to semi-supervised learning as a means of reducing the annotation effort involved in creating labeled data for SRL is by no means new. F¨urstenau and Lapata (2012) propose to augment a labeled dataset with unlabeled examples whose roles are inferred automatically via annotation projection. Other work uses a language model to learn word similarities from unlabeled texts (Croce et al., 2010; Deschacht and Moens, 2009) or constructs an informed prior from labeled data in order to learn a generative model from unlabeled data (Titov and Klementiev, 2012). More recently, Mehta et al. (2018) have proposed a semi-supervised method for constituencybased SRL. Their work builds upon a state-of-theart neural model (He et al., 2018b; Peters et al., 2018) whose training objective they augment with a syntactic inconsistency loss component. Their hypothesis is that by leveraging syntactic structure during training, the SRL model may become more robust in low resource scenarios. This method is very much geared towards improving constituentbased SRL, where syntactic constraints are widely used during decoding (He et al., 2017, 2018a). And requires a robus"
D19-1094,D17-1159,0,0.0812066,"rovides a schematic overview of our model which has two main components, namely a sentence learner and a semantic role labeler. The sentence learner consists of: In addition to overcoming the difficulty of utilizing plain text for semi-supervised SRL, we show that application of CVT to SRL requires special attention over and above the sequence tagging and dependency parsing tasks discussed in Clark et al. (2018). We investigate how to best formulate different views for CVT focusing on semantic predicates which have been proven to be very useful in recent SRL models (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017; Cai et al., 2018). Experimental results on the CONLL-2009 benchmark datasets show that our model is able 1019 • look-ups of word embeddings and character embeddings; • a bidirectional LSTM (BiLSTM) encoder which takes as input the representation of each word in a sentence and produces context-dependent representations; • a multi-task prediction module to perform POS tagging, dependency parsing, and predicate identification. While the semantic role labeler consists of: • an input layer which combines multi-source representations of the input sentence; • a primary bidirectional LSTM (BiLSTM) e"
D19-1180,W14-0907,0,0.126992,"an overview of the movie’s genre, mood, and artistic style based on screenplay analysis. Gorinski and Lapata (2015) summarize full length screenplays by extracting an optimal chain of scenes via a graph-based approach centered around the characters of the movie. A similar approach has also been adopted by Vicol et al. (2018), who introduce the MovieGraphs dataset consisting of 51 movies and describe video clips with character-centered graphs. Other work creates animated story-boards using the action descriptions of screenplays (Ye and Baldwin, 2008), extracts social networks from screenplays (Agarwal et al., 2014a), or creates xkcd movie narrative charts (Agarwal et al., 2014b). Our work also aims to analyze the narrative structure of movies, but we adopt a high-level approach. We advocate TP identification as a precursor to more fine-grained analysis that unveils character attributes and their relationships. Our approach identifies key narrative events and segments the screenplay accordingly; we argue that this type of preprocessing is useful for applications which might perform question answering and summarization over screenplays. Although our experiments focus solely on the textual modality, turni"
D19-1180,N15-1084,0,0.018318,"s which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epita"
D19-1180,P13-1035,0,0.17446,"re present. Based on the pilot, annotation instructions were devised and an annotation tool was created which allows to label synopses with TPs sentence-bysentence. After piloting the annotation scheme on 30 movies, two new annotators were trained using our instructions and in a second study, they doubly annotated five movies. The remaining movies 1709 § tion. Finally, Koˇcisk`y et al. (2018) recently introduced a dataset consisting of question-answer pairs over 1,572 movie screenplays and books. Previous approaches have focused on finegrained story analysis, such as inducing character types (Bamman et al., 2013, 2014) or understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017). Various approaches have also attempted to analyze the goal and structure of narratives. Black and Wilensky (1979) evaluate the functionality of story grammars in story understanding, Elson and McKeown (2009) develop a platform for representing and reasoning over narratives, and Chambers and Jurafsky (2009) learn fine-grained chains of events. In the context of movie summarization, Gorinski and Lapata (2018) automatically generate an overview of the movie’s genre, mood, and artistic style ba"
D19-1180,P14-1035,0,0.23957,"g to evaluate various theories of storytelling (e.g., by examining a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how"
D19-1180,D18-2029,0,0.0234737,"and scene sentences. Again, generic and entity-specific representations are combined via concatenation. 4.3 End-to-end TP Identification Our ultimate goal is to identify TPs in screenplays without assuming any goldstandard information about their position in the synopsis. We address this with an end-to-end model which first predicts the sentences that act as TPs in the synopsis (e.g., TAM in Section 4.1) and then feeds these predictions to a model which identifies the corresponding TP scenes (e.g., TAM in Section 4.2). 5 Experimental Setup Training We used the Universal Sentence Encoder (USE; Cer et al. 2018) as a pre-trained sentence encoder for all models and tasks; its performance was superior to BERT (Devlin et al., 2018) and other related pre-trained encoders (for more details, see the Appendix). Since the binary labels in both prediction tasks are imbalanced, we apply class weights to the loss function of our models. We weight each class by its inverse frequency in the training set (for more implementation details, see the Appendix). Inference During inference in our first task (i.e., identification of TPs in synopses), we select one sentence per TP. Specifically, we want to track TA Random"
D19-1180,P09-1068,0,0.0636781,"y introduced a dataset consisting of question-answer pairs over 1,572 movie screenplays and books. Previous approaches have focused on finegrained story analysis, such as inducing character types (Bamman et al., 2013, 2014) or understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017). Various approaches have also attempted to analyze the goal and structure of narratives. Black and Wilensky (1979) evaluate the functionality of story grammars in story understanding, Elson and McKeown (2009) develop a platform for representing and reasoning over narratives, and Chambers and Jurafsky (2009) learn fine-grained chains of events. In the context of movie summarization, Gorinski and Lapata (2018) automatically generate an overview of the movie’s genre, mood, and artistic style based on screenplay analysis. Gorinski and Lapata (2015) summarize full length screenplays by extracting an optimal chain of scenes via a graph-based approach centered around the characters of the movie. A similar approach has also been adopted by Vicol et al. (2018), who introduce the MovieGraphs dataset consisting of 51 movies and describe video clips with character-centered graphs. Other work creates animate"
D19-1180,P17-1171,0,0.0228375,"Missing"
D19-1180,P12-1094,0,0.0238116,"a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-sh"
D19-1180,N19-1246,0,0.0288818,"Missing"
D19-1180,D18-1134,0,0.0320345,"Missing"
D19-1180,E12-1065,0,0.0835436,"y analysis works at the intersection of natural language processing and literary studies, aiming to evaluate various theories of storytelling (e.g., by examining a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenpla"
D19-1180,Q18-1001,1,0.929083,"omputational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe) of a story (Pavis, 1998). The German novelist and playwright Gustav Freytag modifie"
D19-1180,N15-1113,1,0.905027,"ationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017). Various approaches have also attempted to analyze the goal and structure of narratives. Black and Wilensky (1979) evaluate the functionality of story grammars in story understanding, Elson and McKeown (2009) develop a platform for representing and reasoning over narratives, and Chambers and Jurafsky (2009) learn fine-grained chains of events. In the context of movie summarization, Gorinski and Lapata (2018) automatically generate an overview of the movie’s genre, mood, and artistic style based on screenplay analysis. Gorinski and Lapata (2015) summarize full length screenplays by extracting an optimal chain of scenes via a graph-based approach centered around the characters of the movie. A similar approach has also been adopted by Vicol et al. (2018), who introduce the MovieGraphs dataset consisting of 51 movies and describe video clips with character-centered graphs. Other work creates animated story-boards using the action descriptions of screenplays (Ye and Baldwin, 2008), extracts social networks from screenplays (Agarwal et al., 2014a), or creates xkcd movie narrative charts (Agarwal et al., 2014b). Our work also aims to analy"
D19-1180,N18-1160,1,0.828518,"d works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe) of a story (Pavis, 1998). The German novelist and playwright Gustav Freytag modified Aristotle’s structure by transforming the triangle"
D19-1180,J97-1003,0,0.921975,"rs → − ← − of the forward hi and backward hi LSTM, respec→ − ← − tively: cpi = hi = [ hi ; hi ] (for a more detailed description, see the Appendix). Representation cpi is the input feature vector for our binary classifier. The model is illustrated in Figure 2a. Topic-Aware Model (TAM) TPs by definition act as boundaries between different thematic units in a movie. Furthermore, long documents are usually comprised of topically coherent text segments, each of which contains a number of text passages such as sentences or paragraphs (Salton et al., 1996). Inspired by text segmentation approaches (Hearst, 1997) which measure the semantic similarity between sequential context windows in order to determine topic boundaries, we enhance our representations with a context interaction layer. The objective of this layer is to measure the similarity of the current sentence with its preceding and following context, thereby encoding whether it functions as a boundary between thematic sections. The enriched model with the context interaction layer is illustrated in Figure 2a. After calculating contextualized sentence representations cpi , we compute the representation of the left lci and right rci contexts of"
D19-1180,N16-1180,0,0.40422,"Missing"
D19-1180,P17-1147,0,0.0161104,"enplays, outperforming strong baselines based on state-of-the-art sentence representations and the expected position of TPs. 2 Related Work Recent years have seen increased interest in the automatic analysis of long and complex narratives. Specifically, Machine Reading Comprehension (MRC) and Question Answering (QA) tasks are transitioning from investigating single short and clean articles or queries (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016) to large scale datasets that consist of complex stories (Tapaswi et al., 2016; Frermann et al., 2018; Koˇcisk`y et al., 2018; Joshi et al., 2017) or require reasoning across multiple documents (Welbl et al., 2018; Wang et al., 2018; Dua et al., 2019; Yang et al., 2018). Tapaswi et al. (2016) introduce a multi-modal dataset consisting of questions over 140 movies, while Frermann et al. (2018) attempt to answer a single question, namely who is the perpetrator in 39 episodes of the well-known crime series CSI, again based on multi-modal informa1708 1 https://github.com/ppapalampidi/TRIPOD 3 The TRIPOD Dataset The TRIPOD dataset contains 99 screenplays, accompanied with cast information (according to IMDb), and Wikipedia plot synopses anno"
D19-1180,Q18-1023,0,0.024202,"Missing"
D19-1180,D18-1055,0,0.0219139,"Missing"
D19-1180,P14-5010,0,0.00426099,"Missing"
D19-1180,P13-2085,0,0.0213684,"language processing and literary studies, aiming to evaluate various theories of storytelling (e.g., by examining a collection of works within a single genre, by an author, or topic) and to develop tools which aid in searching, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or"
D19-1180,D16-1264,0,0.015539,"tences and 13,403 screenplay scenes) annotated with TPs; and (c) we present an end-toend neural network model that identifies turning points in plot synopses and projects them onto scenes in screenplays, outperforming strong baselines based on state-of-the-art sentence representations and the expected position of TPs. 2 Related Work Recent years have seen increased interest in the automatic analysis of long and complex narratives. Specifically, Machine Reading Comprehension (MRC) and Question Answering (QA) tasks are transitioning from investigating single short and clean articles or queries (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016) to large scale datasets that consist of complex stories (Tapaswi et al., 2016; Frermann et al., 2018; Koˇcisk`y et al., 2018; Joshi et al., 2017) or require reasoning across multiple documents (Welbl et al., 2018; Wang et al., 2018; Dua et al., 2019; Yang et al., 2018). Tapaswi et al. (2016) introduce a multi-modal dataset consisting of questions over 140 movies, while Frermann et al. (2018) attempt to answer a single question, namely who is the perpetrator in 39 episodes of the well-known crime series CSI, again based on multi-modal informa1708 1"
D19-1180,D15-1234,0,0.029536,"ng, visualizing, or summarizing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe"
D19-1180,D17-1247,0,0.0434,"izing literary content. Within natural language processing, computational literary analysis has mostly targeted works of fiction such as novels, plays, and screenplays. Examples include analyzing characters, their relationships, and emotional trajectories (Chaturvedi et al., 2017; Iyyer et al., 2016; Elsner, 2012), identifying enemies and allies (Nalisnick and Baird, 2013), villains or heroes (Bamman et al., 2014, 2013), measuring the memorability of quotes (Danescu-Niculescu-Mizil et al., 2012), characterizing gender representation in dialogue (Agarwal et al., 2015; Ramakrishna et al., 2015; Sap et al., 2017), identifying perpetrators in crime series (Frermann et al., 2018), summarizing screenplays (Gorinski and Lapata, 2018), and answering questions about long and complex narratives (Koˇcisk`y et al., 2018). In this paper we are interested in the automatic analysis of narrative structure in screenplays. Narrative structure, also referred to as a storyline or plotline, describes the framework of how one tells a story and has its origins to Aristotle who defined the basic triangle-shaped plot structure representing the beginning (protasis), middle (epitasis), and end (catastrophe) of a story (Pavis"
D19-1180,P18-1178,0,0.0219843,"Missing"
D19-1180,Q18-1021,0,0.0248921,"ntence representations and the expected position of TPs. 2 Related Work Recent years have seen increased interest in the automatic analysis of long and complex narratives. Specifically, Machine Reading Comprehension (MRC) and Question Answering (QA) tasks are transitioning from investigating single short and clean articles or queries (Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2016) to large scale datasets that consist of complex stories (Tapaswi et al., 2016; Frermann et al., 2018; Koˇcisk`y et al., 2018; Joshi et al., 2017) or require reasoning across multiple documents (Welbl et al., 2018; Wang et al., 2018; Dua et al., 2019; Yang et al., 2018). Tapaswi et al. (2016) introduce a multi-modal dataset consisting of questions over 140 movies, while Frermann et al. (2018) attempt to answer a single question, namely who is the perpetrator in 39 episodes of the well-known crime series CSI, again based on multi-modal informa1708 1 https://github.com/ppapalampidi/TRIPOD 3 The TRIPOD Dataset The TRIPOD dataset contains 99 screenplays, accompanied with cast information (according to IMDb), and Wikipedia plot synopses annotated with turning points. The movies were selected from the Script"
D19-1180,D18-1259,0,0.0495527,"ie “Panic Room”. (Thompson, 1999), and by definition they occur at the junctions of acts. Aside from changing narrative direction, TPs define the movie’s structure, tighten the pace, and prevent the narrative from drifting. The five TPs and their definitions are given in Table 1. We propose the task of turning point identification in movies as a means of analyzing their narrative structure. TP identification provides a sequence of key events in the story and segments the screenplay into thematic units. Common approaches to summarization and QA of long or multiple documents (Chen et al., 2017; Yang et al., 2018; Kratzwald and Feuerriegel, 2018; Elgohary et al., 2018) include a retrieval system as the first step, which selects a subset of relevant passages for further processing. However, Koˇcisk`y et al. (2018) demonstrate that these approaches do not perform equally well for extended narratives, since individual passages are very similar and the same entities are referred to throughout the story. We argue that this challenge can be addressed by TP identification, which finds the most important events and segments the narrative into thematic units. Downstream processing for summarization or question"
D19-1212,W19-4301,0,0.0550614,"Missing"
D19-1212,P17-1175,0,0.0691405,"Missing"
D19-1212,N12-1048,0,0.0227948,"an be seen that the multi-view model outperforms all other models. Incremental Inference Incremental sequence labeling refers to making predictions on an incoming sequence when “streamed” in an online fashion. For example, if the sequence is a sentence, we are not allowed to encode the whole sentence first, but instead have to output a relevant label for each word as it arrives in the sequence. Incrementality underlies fundamental human cognition and is essential for scaling systems to large datasets and real-time inference, necessary, for example, in simultaneous translation (interpretation; Bangalore et al., 2012; Yarmohammadi et al., 2013; Cho and Esipova, 2016). 2061 N ONE B I -D IR U NI -D IR MODEL G RISSOM Grissom doesn’t look worried. He takes his You ever been to the gloves off and puts theater, Peter? pr 42.8 39.4 41.3 40.0 43.6 49.6 re 51.2 60.4 63.4 62.7 58.1 49.4 f1 46.6 47.7 50.0 48.8 49.8 49.5 Table 1: Precision (pr), recall (re) and F1 scores for detecting the minority class (perpetrator mentioned) on the held-out dataset. EF stands for early-fusion, while MV for multi-view. The first section of the table reports scores for unidirectional (incremental) models and the second for bidirectio"
D19-1212,Q18-1001,1,0.852946,"available modalities. We demonstrate the effectiveness of our model in an incremental inference setup (Figure 1), where it makes predictions on the fly without encoding the sequence in full, a more realistic scenario of interacting with data. This is a critical feature for online applications such as simultaneous translation (interpretation) and also a desirable behavior for movie processing models that mimic a human viewer watching a movie for the first time. We evaluate our architecture on three tasks pertaining to movie understanding. Specifically, we use the recently introduced dataset of Frermann et al. (2018), which consists of episodes of the Crime Series Investigation (CSI) television series, segmented and aligned for three different modali2057 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2057–2067, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics y1 y2 yt h1 h2 ht ... h1 x11 x21 x31 h2 x12 x22 x32 ht−1 ht x1t x2t x3t Figure 1: General unfolded overview of our model for a multimodal example x with three modalities: at each time step t ∈"
D19-1212,P17-1164,0,0.0160079,"”. Information learned by the attention mechanism may have a distinct conceptual importance (e.g. alignments in machine translation) or simply indicate which elements of the input contribute more to the final output representation. Attention mechanisms can be learned along with the rest of the network in an end-to-end fashion, or can be explicitly supervised by providing the model with pre-calculated attention scores. Supervised attention has been shown to boost the performance of models for machine translation (Mi et al., 2016), constituency parsing (Kamigaito et al., 2017), event detection (Liu et al., 2017b) and aspect-based sentiment analysis (Cheng et al., 2017). Furthermore, image attention mechanisms guided by weak or direct supervision have been proposed for the tasks of image (Liu et al., 2017a) and video captioning (Yu et al., 2017). Multi-head attention mechanisms (Vaswani et al., 2017) employ more than one, independent, attention mechanisms, boasting multiple areas of focus on the input sequence. The main idea behind them is that a single attention head may not prove adequate to capture all the different types and positions of information that are important to the end task. Our attenti"
D19-1212,I17-2002,0,0.01426,"ments of the sequence to a specific “query”. Information learned by the attention mechanism may have a distinct conceptual importance (e.g. alignments in machine translation) or simply indicate which elements of the input contribute more to the final output representation. Attention mechanisms can be learned along with the rest of the network in an end-to-end fashion, or can be explicitly supervised by providing the model with pre-calculated attention scores. Supervised attention has been shown to boost the performance of models for machine translation (Mi et al., 2016), constituency parsing (Kamigaito et al., 2017), event detection (Liu et al., 2017b) and aspect-based sentiment analysis (Cheng et al., 2017). Furthermore, image attention mechanisms guided by weak or direct supervision have been proposed for the tasks of image (Liu et al., 2017a) and video captioning (Yu et al., 2017). Multi-head attention mechanisms (Vaswani et al., 2017) employ more than one, independent, attention mechanisms, boasting multiple areas of focus on the input sequence. The main idea behind them is that a single attention head may not prove adequate to capture all the different types and positions of information that are imp"
D19-1212,D16-1249,0,0.0642349,"Missing"
D19-1212,L18-1602,0,0.0176944,"work and strong baselines. Moreover, we introduce two tasks, crime case and speaker type tagging, that contribute to movie understanding and demonstrate the effectiveness of our model on them.1 1 Introduction While many natural language processing (NLP) problems concern exclusively textual or speech data, the integration of multimodal information (such as images, video or audio) is beneficial for a variety of problems. For example, visual information has been used in affect analysis (Kahou et al., 2016), sentiment analysis (Morency et al., 2011) and machine translation (Calixto et al., 2017; Lala and Specia, 2018). This is also the case for problems which are sequential in nature, such as video summarization (Smith and Kanade, 1998), contin1 Our code is available at https://github.com/ papagandalf/multiview_csi. uous prediction of affect (Nicolaou et al., 2011) or engagement level prediction (Rehg et al., 2013). Most existing multi-view representation learning approaches are tested in an unsupervised setup where the multi-view representations are learned separately from the task, and are designed to accommodate the learning of representation for monolithic (albeit multi-view) data points, not sequences"
D19-1212,N16-1030,0,0.0549442,"example of the labels on which the model operates can be found in Figure 3. We modify the architecture of our model, so that the output of the sequence model cell is fed to different output layers, one for each task. Training proceeds by summing the loss terms for both tasks. In the case of our multiview model, the loss consists of two cross-entropy terms and one correlation term. Moreover, we experiment with adding a Conditional Random Field (CRF) on top of the sequence models, based on recent work that achieves state-of-the-art performance in tagging tasks, such as Named Entity Recognition (Lample et al., 2016). The results for speaker type and case tagging can be found in Table 4, where our model (MV) is compared with an LSTM early fusion model. We use a variant of the evaluation script used for the CoNLL shared tasks5 and report average scores. Our multi-view model consistently outperforms early fusion models. Interestingly, the multi-task MV+CRF model trained exhibits the best performance, suggesting that jointly solving the two tasks improves the capabilities of the model. 4 http://www.conll.org/previous-tasks https://github.com/spyysalo/ conlleval.py 5 5 Related Work Inference on multimodal seq"
D19-1212,D14-1162,0,0.0820859,"abels (binary) and the name of the speaker that uttered them (“None” for scene descriptions). Each speaker belongs to one of the types detective, perpetrator, suspect, extra (none for scene descriptions). An annotated example ex3 Speech has been stripped from the audio track, leaving it only with audio effects and music (so that the text modality will not be deemed redundant and the dataset does not contain overlapping information). Experimental Setup For all experiments, we adopted an experimental setup similar to that of Frermann et al. (2018). For text, we use 50-dimensional GloVe vectors (Pennington et al., 2014) and a convolutional text encoder with maxpooling (filters of sizes 3, 4 and 5, each returning a 75-dimension output). Image features are generated by the final hidden layer of the inception-v4 Szegedy et al., 2017 model (dimensionality of 1,546). Audio features are constructed by concatenating five 13-dimensional Mel-Frequency Cepstral Coefficient (MFCC) feature vectors for each interval. For perpetrator mention identification, we use the case level splits, whereas the speaker and case tasks are performed on the episode level. All LSTM and GRU variants have one layer of length 128 and a dropo"
D19-1212,D18-1548,0,0.0257923,"hich reconstructs the original representations of each view, while our model does not include autoencoding. Casting correlation maximization between three or more variables as the maximization of the sum of the correlation between all pairs of available variables has been previously used in extensions of CCA for more than two views (Benton et al., 2019), or other multi-view learning works (Kumar et al., 2011). Yang et al. (2017) mention it in their paper, although they do not experiment with it. The multi-head attention component of our model bears similarities in spirit to the recent work of Strubell et al. (2018), where an attention head is replaced by a model trained to predict syntac2064 tic dependencies (Dozat and Manning, 2017). In contrast, our model uses explicit supervision for all self-attention heads and is trained to predict the correct attention scores in a multi-task fashion. 6 Conclusions We describe a neural multi-view sequential architecture, paired with a novel objective that takes advantage of supervision, while at the same time, maximizes the correlation between views. We test our approach on the task of perpetrator mention identification of the CSI dataset, on which we show that it"
D19-1212,N16-1174,0,0.0317202,"ld create if only the cross-entropy loss was used. Multi-head Attention Each of the videos of the dataset is assumed to be divided to snippets, with the sequence model operating and making predictions on the snippet level. For each snippet, the text modality may contain a different number of tokens and a fixed-length text representation is generated by a text encoder. We use an RNN as text encoder and the encoded text representation for each snippet is weighted by attention scores calculated over its tokens. Both “query” and “token” representations come from the same sequence (self-attention; Yang et al. 2016). In cases where the dataset contains, apart from snippetlevel, also token-level annotations, the attention module can be directly supervised: we add a term 2060 yt cerpt is shown in Figure 3. 4.1 ht ... ... ht−1 ht x3t x2t a b c ... wt,1 wt,2 wt,k Figure 2: Hierarchical multi-view recurrent model with multi-head attention. Each sentence is encoded with an RNN and three attention heads (a, b and c) calculate attention scores for each of the tokens wti of the t-th sentence of the script. to the model’s loss that minimizes the error of the attention scores, with respect to token-level annotation"
D19-1278,W17-6901,0,0.0240454,"ces to predicting rewrites only. We define the probability of generating graph G conditioned of input sentence w as follows: p(G|w) = p(a|w) = |a| Y i=1 p(ai |a<i , w) (1) r5 r7 L →anchor L →dock r6 r2 L →need p r4 r3 L →anchor L →big Figure 5: A derivation tree corresponding to Figure 4. Solid edges rewrite Tn nonterminals, while dotted rewrite L nonterminals. Input Encoder We represent the ith word wi of input sentence w = w1 . . . w|w |using both learned and pre-trained word embeddings (wi and wip respectively), lemma embedding (li ), part-of-speech embedding (pi ), universal semantic tag (Abzianidze and Bos, 2017) embedding (ui ), and dependency label embedding (di ).4 An input xi is computed as the weighted concatenation of these features followed by a non-linear projection (with vectors and matrices in bold): xi = tanh(W(1) [wi ; wip ; li ; pi ; ui ; di ]) (2) Input xi is then encoded with a bidirectional LSTM, yielding contextual representation hei . 4 Universal semantic tags are language neutral tags intended to characterize lexical semantics. 2772 Graph decoder Since we know in advance whether the next action is GEN-FRAG or GENLABEL, we use different models for them. GEN-FRAG. If step t rewrites a"
D19-1278,P98-1013,0,0.060201,"ship(x1 ) anchor(x3 ) PART O F(x1 , x2 ) big(s1 ) T OPIC(s1 , x3 ) x1 b2 Figure 1: The discourse representation structure for “Every ship in the dock needs a big anchor”. For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray. Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus (Montague, 1973), dependency-based compositional semantics (Liang et al., 2011), frame semantics (Baker et al., 1998), abstract meaning representations (AMR; Banarescu et al. 2013), minimal recursion semantics (MRS; Copestake et al. 2005), and discourse representation theory (DRT; Kamp 1981). Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence “Every ship in the dock needs a big anchor”. Its meaning representation, expressed as a Discourse Representation Structure (DRS, Kamp 1981), is shown in Figure 1.1 A DRS is drawn as a box with 1 For simplicity, our examples do not show time representations, though these are cons"
D19-1278,W13-2322,0,0.0510974,", x3 ) x1 b2 Figure 1: The discourse representation structure for “Every ship in the dock needs a big anchor”. For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray. Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus (Montague, 1973), dependency-based compositional semantics (Liang et al., 2011), frame semantics (Baker et al., 1998), abstract meaning representations (AMR; Banarescu et al. 2013), minimal recursion semantics (MRS; Copestake et al. 2005), and discourse representation theory (DRT; Kamp 1981). Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence “Every ship in the dock needs a big anchor”. Its meaning representation, expressed as a Discourse Representation Structure (DRS, Kamp 1981), is shown in Figure 1.1 A DRS is drawn as a box with 1 For simplicity, our examples do not show time representations, though these are consistently present in our data. two parts: the top part lists var"
D19-1278,P17-1112,0,0.0193591,"amentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Important"
D19-1278,P13-2131,0,0.0414169,"ained UDPipe models available at http://ufal.mff.cuni.cz/udpipe# download; gold-standard universal semantic tags were extracted from the PMB release. 2774 the copy mechanism. Most of the features mentioned above are cross-linguistic and therefore fit both mono and cross-lingual settings, with the exception of lemma and word embeddings, where we exclude the former and replaced the latter with multilingual word embeddings.9 4.5 Evaluation Metric We evaluated our system by scoring the similarity between predicted and gold graphs. We used Counter (van Noord et al., 2018), an adaptation of Smatch (Cai and Knight, 2013) to Discourse Representation Structures where graphs are first transformed into a set of ‘source node – edge label – target node’ triples and the best mapping between the variables is found through an iterative hill-climbing strategy. Furthermore, Counter checks whether DRSs are well-formed in that all boxes should be connected, acyclic, with fully instantiated variables, and correctly assigned sense tags. It is worth mentioning that there can be cases where our parser generates ill-formed graphs according to Counter; this is however not due to the model itself but to the way the graph is conv"
D19-1278,P18-1071,0,0.0208564,"e predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Importantly, any well-formed"
D19-1278,E17-1051,0,0.0482812,"ifficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of sequence prediction with the fidelity of graph prediction? We show that this is possible by developing a new model that predicts sequences through a simple string rewriting process, in which each rewrite corresponds to a well-defined graph fragment. Importantly, any well-formed string produced by our mode"
D19-1278,P18-1170,0,0.0182646,"gs that don’t correspond to graphs—for example, strings with illformed bracketings or unbound variable names. While it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—wh"
D19-1278,P16-1154,0,0.0182967,"target of multiple edges, then the leftmost one is written as a subtree, and the remainder are written as references. Hence, every node is written as a subtree exactly once. The advantage of predicting linearized graphs is twofold. The first advantage is that graphbank datasets usually already contain linearizations, which can be used without additional work. These linearizations are provided by annotators or algorithms and are thus likely to be very consistent in ways that are beneficial to a learning algorithm. The second advantage is that we can use simple, well-understood sequence models (Gu et al., 2016; Jia and Liang, 2016; van Noord et al., 2018) to model them. But this simplicity comes with a cost: sequence models can predict strings that don’t correspond to graphs—for example, strings with illformed bracketings or unbound variable names. While it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible lineari"
D19-1278,Q18-1043,0,0.231588,"Missing"
D19-1278,P16-1002,0,0.014923,"e edges, then the leftmost one is written as a subtree, and the remainder are written as references. Hence, every node is written as a subtree exactly once. The advantage of predicting linearized graphs is twofold. The first advantage is that graphbank datasets usually already contain linearizations, which can be used without additional work. These linearizations are provided by annotators or algorithms and are thus likely to be very consistent in ways that are beneficial to a learning algorithm. The second advantage is that we can use simple, well-understood sequence models (Gu et al., 2016; Jia and Liang, 2016; van Noord et al., 2018) to model them. But this simplicity comes with a cost: sequence models can predict strings that don’t correspond to graphs—for example, strings with illformed bracketings or unbound variable names. While it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a pr"
D19-1278,P11-1060,0,0.0505102,") P IVOT(e1 , x1 ) ⇒ T HEME(e1 ,x3 ) ship(x1 ) anchor(x3 ) PART O F(x1 , x2 ) big(s1 ) T OPIC(s1 , x3 ) x1 b2 Figure 1: The discourse representation structure for “Every ship in the dock needs a big anchor”. For ease of reference in later figures, each box includes a variable corresponding to the box itself, at top right in gray. Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations, which in turn can be expressed in many different formalisms, including lambda calculus (Montague, 1973), dependency-based compositional semantics (Liang et al., 2011), frame semantics (Baker et al., 1998), abstract meaning representations (AMR; Banarescu et al. 2013), minimal recursion semantics (MRS; Copestake et al. 2005), and discourse representation theory (DRT; Kamp 1981). Explicitly or implicitly, a representation in any of these formalisms can be expressed as a directed acyclic graph (DAG). Consider the sentence “Every ship in the dock needs a big anchor”. Its meaning representation, expressed as a Discourse Representation Structure (DRS, Kamp 1981), is shown in Figure 1.1 A DRS is drawn as a box with 1 For simplicity, our examples do not show time"
D19-1278,P18-1040,1,0.873022,"nd e is weighted by matrices W(3) and W(4) , respectively. We then update the stackLSTM representation using the embedding of the non-terminal fragment yt (denoted as yte ), as follows: hdt+1 = LSTM(yte , hdt ) (4) GEN-LABEL. Labels L can be rewritten to either semantic constants (e.g., ‘speaker’, ‘now’, ‘hearer’) or unary predicates that often corresponds to the lemmas of the input words (e.g., ‘love’) or. We predict the former using a model identical to the one for GEN-FRAG. For the latter, we use a selection mechanism to choose an input lemma to copy to output. We model selection following Liu et al. (2018), assigning each input lemma a score oji that we then pass through a softmax layer to obtain a distibution: (5) e oji = hdT j W hi pcopy = S OFT M AX(oji ) i (5) where hi is the encoder hidden state for word wi . We allow the model to learn whether to use softattention or the selection mechanism through a binary classifier, conditioned on the decoder hidden state at time t, hdt . Similar to Equation (4), we update the stackLSTM with the embedding of terminal predicted.5 5 In the PMB, each terminal is annotated for sense (e.g. ‘n.01’, ‘s.01’) and presupposition (e.g. for ‘dockp ’ in Figure 3) a"
D19-1278,D15-1166,0,0.0127877,"Missing"
D19-1278,P18-1037,0,0.0522701,"le it is often possible to fix these strings with pre- or post-processing, we would prefer to model the problem in a way that does not require this. Models that predict graphs are complex and far less well-understood than models that predict sequences. Fundamentally, this is because predicting graphs is difficult: every graph has many possible linearizations, so from a probabilistic perspective, the linearization is a latent variable that must be marginalized out (Li et al., 2018). Groschwitz et al. (2018) model graphs as trees, interpreted as the (latent) derivation trees of a graph grammar; Lyu and Titov (2018) model graphs with a conditional variant of the classic Erd¨os and R´enyi (1959) model, first predicting an alignment for each node of the output graph, and then predicting, for each pair of nodes, whether there is an edge between them. Buys and Blunsom (2017), Chen et al. (2018), and Damonte et al. (2017) all model graph generation as a sequence of actions, each aligned to a word in the conditioning sentence. Each of these models has a latent variable— a derivation tree or alignment—which must be accounted for via preprocessing or complex inference techniques. Can we combine the simplicity of"
D19-1387,D18-1443,0,0.730429,"cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3730–3740, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics coder (Vaswani et al., 2017). We design a new training schedule which separates the optimizers of the encoder and the decoder in order to accommodate the fact that the former is pretrained while the latter must be trained from scratch. Finally, motivated by previous work showing that the combination of extractive and abstractive objectives can help generate better summaries (Gehrmann et al., 2018), we present a two-stage approach where the encoder is fine-tuned twice, first with an extractive objective and subsequently on the abstractive summarization task. We evaluate the proposed approach on three single-document news summarization datasets representative of different writing conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results u"
D19-1387,P16-1154,0,0.0327232,"riting conventions (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to fur"
D19-1387,N18-1150,0,0.442654,"aphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals are tested. 2 2.1 Background Pretrained Language Models Pretrained language models (Peters et al., 2018; Ra"
D19-1387,J10-3005,1,0.914197,"erence summaries, but in XSum, this gap is much smaller. We also observe that on CNN/DailyMail, B ERT E XTA BS produces less novel n-ngrams than B ERTA BS, which is not surprising. B ERT E XTA BS is more biased towards selecting sentences from the source document since it is initially trained as an extractive model. The supplementary material includes examples of system output and additional ablation studies. 5.3 Human Evaluation In addition to automatic evaluation, we also evaluated system output by eliciting human judgments. We report experiments following a questionanswering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018b) which quantifies the degree to which summarization models retain key information from the document. Under this paradigm, a set of questions is created based on the gold summary under the assumption that it highlights the most important document content. Participants are then asked to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. Moreover, we also assessed the overall quality of the summaries produced by abstractive systems which due"
D19-1387,N19-1423,0,0.483496,"ning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves stateof-the-art results across the board in both extractive and abstractive settings.1 1 In most cases, pretrained language models have been employed as encoders for sentence- and paragraph-level natural language understanding problems (Devlin et al., 2019) involving various classification tasks (e.g., predicting whether any two sentences are in an entailment relationship; or determining the completion of a sentence among four alternative sentences). In this paper, we examine the influence of language model pretraining on text summarization. Different from previous tasks, summarization requires wide-coverage natural language understanding going beyond the meaning of individual words and sentences. The aim is to condense a document into a shorter version while preserving most of its meaning. Furthermore, under abstractive modeling formulations, t"
D19-1387,D18-1409,0,0.156843,"Missing"
D19-1387,P16-1188,0,0.527234,"material. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. We first split sentences with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre-processed the dataset following See et al. (2017). Input documents were truncated to 512 tokens. NYT contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834/9,706 training/test examples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward). We used 4,000 examples from the training as validation set. We also followed their filtering procedure, documents with summaries less than 50 words were removed from the dataset. The filtered test set (NYT50) includes 3,452 examples. Sentences were split with the Stanford CoreNLP toolkit (Manning et al., 2014) and preprocessed following Durrett et al. (2016). Input documents were truncated to 800 tokens. XSum contains 226,711 n"
D19-1387,N19-1409,0,0.0270286,"yers: ˜ l = LN(hl−1 + MHAtt(hl−1 )) h ˜ l + FFN(h ˜ l )) hl = LN(h (1) (2) where h0 = x are the input vectors; LN is the layer normalization operation (Ba et al., 2016); MHAtt is the multi-head attention operation (Vaswani et al., 2017); superscript l indicates the depth of the stacked layer. On the top layer, B ERT will generate an output vector ti for each token with rich contextual information. Pretrained language models are usually used to enhance performance in language understanding tasks. Very recently, there have been attempts to apply pretrained models to various generation problems (Edunov et al., 2019; Rothe et al., 2019). When fine-tuning for a specific task, unlike ELMo whose parameters are usually fixed, parameters in B ERT are jointly fine-tuned with additional taskspecific parameters. 2.2 Extractive Summarization Extractive summarization systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. Neural models consider extractive sum3731 Or iginal BERT I n pu t Docu m en t Tok en Em beddi n gs Segm en t Em beddi n gs Posi t i on Em beddi n gs BERT for Summar ization [ CLS] sent one [ SEP] 2nd sent [ SEP] sent again [ SEP] [ CLS]"
D19-1387,P17-2074,0,0.0214302,"s paradigm, a set of questions is created based on the gold summary under the assumption that it highlights the most important document content. Participants are then asked to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. Moreover, we also assessed the overall quality of the summaries produced by abstractive systems which due to their ability to rewrite content may produce disfluent or ungrammatical output. Specifically, we followed the Best-Worst Scaling (Kiritchenko and Mohammad, 2017) method where participants were presented with the output of two systems (and the original document) and 3737 Proportion of novel n-grams B ERT S UM E XT A BS B ERT S UM A BS Reference Abstractive 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 L EAD PTG EN B OTTOM U P TC ONV S2S G OLD B ERT S UM 1-grams 2-grams NYT QA Rank 36.2† — 30.5† -0.27† — — — — — 0.33† 41.8 -0.07 XSum QA Rank 9.20† — 23.7† -0.36† — — 52.1 -0.20† — 0.38† 57.5 0.19 Table 7: QA-based and ranking-based evaluation. Models with † are significantly different from B ERTS UM (using a paired student t-test; p &lt; 0.05). Table cells are filled"
D19-1387,P17-4012,0,0.0508356,"Input documents were truncated to 512 tokens. Aside from various statistics on the three datasets, Table 1 also reports the proportion of novel bi-grams in gold summaries as a measure of their abstractiveness. We would expect models with extractive biases to perform better on datasets with (mostly) extractive summaries, and abstractive models to perform more rewrite operations on datasets with abstractive summaries. CNN/DailyMail and NYT are somewhat extractive, while XSum is highly abstractive. 4.2 Implementation Details For both extractive and abstractive settings, we used PyTorch, OpenNMT (Klein et al., 2017) and the ‘bert-base-uncased’2 version of B ERT to implement B ERT S UM. Both source and target texts 3734 2 https://git.io/fhbJQ were tokenized with B ERT’s subwords tokenizer. Extractive Summarization All extractive models were trained for 50,000 steps on 3 GPUs (GTX 1080 Ti) with gradient accumulation every two steps. Model checkpoints were saved and evaluated on the validation set every 1,000 steps. We selected the top-3 checkpoints based on the evaluation loss on the validation set, and report the averaged results on the test set. We used a greedy algorithm similar to Nallapati et al. (201"
D19-1387,D18-1205,0,0.0576713,".16 54.70 83.31 Table 1: Comparison of summarization datasets: size of training, validation, and test sets and average document and summary length (in terms of words and sentences). The proportion of novel bi-grams that do not appear in source documents but do appear in the gold summaries quantifies corpus bias towards extractive methods. In addition, we propose a two-stage fine-tuning approach, where we first fine-tune the encoder on the extractive summarization task (Section 3.2) and then fine-tune it on the abstractive summarization task (Section 3.3). Previous work (Gehrmann et al., 2018; Li et al., 2018) suggests that using extractive objectives can boost the performance of abstractive summarization. Also notice that this two-stage approach is conceptually very simple, the model can take advantage of information shared between these two tasks, without fundamentally changing its architecture. We name the default abstractive model B ERT S UM A BS and the two-stage fine-tuned model B ERT S UM E XTA BS. 4 Experimental Setup In this section, we describe the summarization datasets used in our experiments and discuss various implementation details. 4.1 Summarization Datasets We evaluated our model o"
D19-1387,W04-1013,0,0.498496,"verlap; RL is the longest common subsequence). Results for comparison systems are taken from the authors’ respective papers or obtained on our data by running publicly released software. we focus on building a minimum-requirements model and these mechanisms may introduce additional hyper-parameters to tune. Thanks to the subwords tokenizer, we also rarely observe issues with out-of-vocabulary words in the output; moreover, trigram-blocking produces diverse summaries managing to reduce repetitions. 5 5.1 Results Automatic Evaluation We evaluated summarization quality automatically using ROUGE (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. Table 2 summarizes our results on the CNN/DailyMail dataset. The first block in the table includes the results of an extractive O RACLE system as an upper bound. We also present the L EAD -3 baseline (which simply selects the first three sentences in a document). The second block in the table includes various extractive models trained on the CNN/DailyMail dataset (see Section 2.2 for an overview). For 3735 Model O RAC"
D19-1387,N19-1173,1,0.890271,"llapati et al., 2017) is one of the earliest neural approaches adopting an encoder based on Recurrent Neural Networks. R EFRESH (Narayan et al., 2018b) is a reinforcement learning-based system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. L A TENT (Zhang et al., 2018) frames extractive summarization as a latent variable inference problem; instead of maximizing the likelihood of “gold” standard labels, their latent model directly maximizes the likelihood of human summaries given selected sentences. S UMO (Liu et al., 2019) capitalizes on the notion of structured attention to induce a multi-root dependency tree representation of the document while predicting the output summary. N EU S UM (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations z = [z1 , ..., zn ], and a decoder then gener"
D19-1387,P14-5010,0,0.00510181,"ore cut and paste operations while others are genuinely abstractive). Table 1 presents statistics on these datasets (test set); example (gold-standard) summaries are provided in the supplementary material. CNN/DailyMail contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. We first split sentences with the Stanford CoreNLP toolkit (Manning et al., 2014) and pre-processed the dataset following See et al. (2017). Input documents were truncated to 512 tokens. NYT contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834/9,706 training/test examples, based on the date of publication (the test set contains all articles published from January 1, 2007 onward). We used 4,000 examples from the training as validation set. We also followed their filtering procedure, documents with summaries less than 50 words were removed from the dataset. The filtered test set (NYT50) includes 3,452 examples. Se"
D19-1387,K16-1028,0,0.145126,"Missing"
D19-1387,D18-1206,1,0.068098,"the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals a"
D19-1387,N18-1158,1,0.0556355,"the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals a"
D19-1387,N18-1202,0,0.444811,"task requires language generation capabilities in order to create summaries containing novel words and phrases not featured in the source text, while extractive summarization is often defined as a binary classification task with labels indicating whether a text span (typically a sentence) should be included in the summary. Introduction Language model pretraining has advanced the state of the art in many NLP tasks ranging from sentiment analysis, to question answering, natural language inference, named entity recognition, and textual similarity. State-of-the-art pretrained models include ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and more recently Bidirectional Encoder Representations from Transformers (B ERT; Devlin et al. 2019). B ERT combines both word and sentence representations in a single very large Transformer (Vaswani et al., 2017); it is 1 Our code is available at https://github.com/ nlpyang/PreSumm. We explore the potential of B ERT for text summarization under a general framework encompassing both extractive and abstractive modeling paradigms. We propose a novel documentlevel encoder based on B ERT which is able to encode a document and obtain representations for its sentences."
D19-1387,P17-1099,0,0.2058,"s (e.g., important information is concentrated at the beginning of the document or distributed more evenly throughout) and summary styles (e.g., verbose vs. more telegraphic; extractive vs. abstractive). Across datasets, we experimentally show that the proposed models achieve state-of-the-art results under both extractive and abstractive settings. Our contributions in this work are threefold: a) we highlight the importance of document encoding for the summarization task; a variety of recently proposed techniques aim to enhance summarization performance via copying mechanisms (Gu et al., 2016; See et al., 2017; Nallapati et al., 2017), reinforcement learning (Narayan et al., 2018b; Paulus et al., 2018; Dong et al., 2018), and multiple communicating encoders (Celikyilmaz et al., 2018). We achieve better results with a minimum-requirement model without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summa"
D19-1387,D18-1088,1,0.939802,"ustrated in red and green color) to distinguish multiple sentences. marization as a sentence classification problem: a neural encoder creates sentence representations and a classifier predicts which sentences should be selected as summaries. S UMMA RU NN ER (Nallapati et al., 2017) is one of the earliest neural approaches adopting an encoder based on Recurrent Neural Networks. R EFRESH (Narayan et al., 2018b) is a reinforcement learning-based system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. L A TENT (Zhang et al., 2018) frames extractive summarization as a latent variable inference problem; instead of maximizing the likelihood of “gold” standard labels, their latent model directly maximizes the likelihood of human summaries given selected sentences. S UMO (Liu et al., 2019) capitalizes on the notion of structured attention to induce a multi-root dependency tree representation of the document while predicting the output summary. N EU S UM (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to"
D19-1387,P19-1499,0,0.206479,"without using any of these mechanisms; b) we showcase ways to effectively employ pretrained language models in summarization under both extractive and abstractive settings; we would expect any improvements in model pretraining to translate in better summarization in the future; and c) the proposed models can be used as a stepping stone to further improve summarization performance as well as baselines against which new proposals are tested. 2 2.1 Background Pretrained Language Models Pretrained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019; Zhang et al., 2019) have recently emerged as a key technology for achieving impressive gains in a wide variety of natural language tasks. These models extend the idea of word embeddings by learning contextual representations from large-scale corpora using a language modeling objective. Bidirectional Encoder Representations from Transformers (B ERT; Devlin et al. 2019) is a new language representation model which is trained with a masked language modeling and a “next sentence prediction” task on a corpus of 3,300M words. The general architecture of B ERT is shown in the left part of Figure 1. Input text is first"
D19-1387,P18-1061,0,0.437953,"system trained by globally optimizing the ROUGE metric. More recent work achieves higher performance with more sophisticated model structures. L A TENT (Zhang et al., 2018) frames extractive summarization as a latent variable inference problem; instead of maximizing the likelihood of “gold” standard labels, their latent model directly maximizes the likelihood of human summaries given selected sentences. S UMO (Liu et al., 2019) capitalizes on the notion of structured attention to induce a multi-root dependency tree representation of the document while predicting the output summary. N EU S UM (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations z = [z1 , ..., zn ], and a decoder then generates the target summary y = [y1 , ..., ym ] token-by-token, in an auto-regressive manner, hence modeling the conditional probability: p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) an"
D19-1387,D15-1044,0,0.403321,"M (Zhou et al., 2018) scores and selects sentences jointly and represents the state of the art in extractive summarization. 2.3 Abstractive Summarization Neural approaches to abstractive summarization conceptualize the task as a sequence-to-sequence problem, where an encoder maps a sequence of tokens in the source document x = [x1 , ..., xn ] to a sequence of continuous representations z = [z1 , ..., zn ], and a decoder then generates the target summary y = [y1 , ..., ym ] token-by-token, in an auto-regressive manner, hence modeling the conditional probability: p(y1 , ..., ym |x1 , ..., xn ). Rush et al. (2015) and Nallapati et al. (2016) were among the first to apply the neural encoderdecoder architecture to text summarization. See et al. (2017) enhance this model with a pointergenerator network (PT GEN) which allows it to copy words from the source text, and a coverage mechanism (C OV) which keeps track of words that have been summarized. Celikyilmaz et al. (2018) propose an abstractive system where multiple agents (encoders) represent the document together with a hierarchical attention mechanism (over the agents) for decoding. Their Deep Communicating Agents (DCA) model is trained end-to-end with"
D19-1391,D13-1160,0,0.23937,"slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidentally execute to correct denotations, but do not reflect the semantics of the question. In this paper, we propose a weakly-supervised neural semantic parser that features st"
D19-1391,N19-1273,0,0.535378,"rs were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a sequence-to-sequence model with attention (Dong and Lapata, 2016). Similarly to Krishnamurthy et al. (2017), we constrained the decod5 Please refer to the Appendix for the full list of hyperparameters used in our experiments. Supervised by Denotations Dev. Test Pasupat and Liang (2015) Neelakantan et al. (2017) Haug et al. (2018) Zhang et al. (2017) Liang et al. (2018) Dasigi et al. (2019) Agarwal et al. (2019) 37.0 34.1 — 40.4 42.3 42.1 43.2 37.1 34.2 34.8 43.7 43.1 43.9 44.1 Typed Seq2Seq Abstract Programs f.w. standard attention f.w. structured attention 37.3 38.3 39.4 43.7 41.4 44.5 Table 1: Results on W IKI TABLE Q UESTIONS. f.w. stands for slots filled with. ing process so that only well-formed programs are predicted. This baseline can be viewed as merging the two stages of our model into one stage where generation of abstract programs and their instantiations are performed with a shared decoder. Standard Attention The aligned representation of slot s in Equation (6) is c"
D19-1391,P16-1004,1,0.944039,"on scores were computed based on the dot product between two vectors. Each MLP is a one-hidden-layer perceptron with ReLU as the activation function. Dropout (Srivastava et al., 2014) was applied to prevent overfitting. All models were trained with Adam (Kingma and Ba, 2015). Implementations of abstract and instantiation grammars were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a sequence-to-sequence model with attention (Dong and Lapata, 2016). Similarly to Krishnamurthy et al. (2017), we constrained the decod5 Please refer to the Appendix for the full list of hyperparameters used in our experiments. Supervised by Denotations Dev. Test Pasupat and Liang (2015) Neelakantan et al. (2017) Haug et al. (2018) Zhang et al. (2017) Liang et al. (2018) Dasigi et al. (2019) Agarwal et al. (2019) 37.0 34.1 — 40.4 42.3 42.1 43.2 37.1 34.2 34.8 43.7 43.1 43.9 44.1 Typed Seq2Seq Abstract Programs f.w. standard attention f.w. structured attention 37.3 38.3 39.4 43.7 41.4 44.5 Table 1: Results on W IKI TABLE Q UESTIONS. f.w. stands for slots fille"
D19-1391,P18-1068,1,0.821412,"ically, we decompose semantic parsing into two steps: 1) a natural language utterance is first mapped to an abstract program which is a composition of high-level operations; and 2) the abstract program is then instantiated with low-level operations that usually involve relations and entities specific to the knowledge base at hand. This decomposition is motivated by the observation that only a small number of sensible abstract programs can be instantiated into consistent programs. Similar ideas of using abstract meaning representations have been explored with fully-supervised semantic parsers (Dong and Lapata, 2018; Catherine Finegan-Dollak and Radev, 2018) and in other related tasks (Goldman et al., 2018; Herzig and Berant, 2018; Nye et al., 2019). For a knowledge base in tabular format, we abstract two basic operations of row selection and column selection from programs: these are handled in the second (instantiation) stage. As shown in Figure 1, the question is first mapped to the abstract program “select (#row slot, #column slot)” whose two slots are subsequently instantiated with filter conditions (row slot) and a column name (column slot). During the instantiation of abstract programs, each slot s"
D19-1391,P18-1168,0,0.081132,"Missing"
D19-1391,D10-1119,0,0.0340866,"o models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence tran"
D19-1391,P11-1060,0,0.0663622,"rogram is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidentally execute to correct denotations, but do not reflect the semantics of the question. In this paper, we propose a weakly-supervised neural semantic parser that features structured latent align"
D19-1391,D08-1082,0,0.180738,"rating an abstract program for a question, our parser finds alignments between slots (with prefix #) and question spans. Based on the alignment, it instantiates each slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidental"
D19-1391,D15-1166,0,0.0114607,"imilarly to Xu et al. (2017) and Catherine Finegan-Dollak and Radev (2018), we generate them with a seq2seq model. Although templatebased approaches would be more efficient in practice, a seq2seq model is more general since it could generate unseen abstract programs which fixed templates could not otherwise handle. Our goal here is to generate a sequence of production rules that lead to abstract programs. During decoding, the hidden state gj of the jth timestep is computed based on the previous production rule, which is mapped to an embedding aj−1 . We also incorporate an attention mechanism (Luong et al., 2015) to compute a contextual vector bj . Finally, a score vector sj is computed by feeding the concatenation of the hidden state and context vector to a multilayer perceptron (MLP): gj = LSTM(gj−1 , aj−1 ) bj = Attention(gj , l) sj = MLP1 ([gj ; bj ]) (3) p(aj |x, t, a<j ) = softmaxaj (sj ) 3.4 Instantiating Abstract Programs To instantiate an abstract program, each slot must obtain its specific semantics from the question. We model this process by an alignment model which learns the correspondence between slots and question spans. Formally, we use a binary alignment matrix A with size m × n × n,"
D19-1391,P18-1037,1,0.715409,"nd natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence transduction (Yu et al., 2016) and AMR parsing (Lyu and Titov, 2018). Learning from Denotations To improve the efficiency of searching for consistent programs, Zhang et al. (2017) use a macro grammar induced from cached consistent programs. Unlike Zhang et al. (2017) who abstract entities and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, reinforcement learning based methods addre"
D19-1391,P17-1097,0,0.0468624,"and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, reinforcement learning based methods address the computational challenge by using a memory buffer (Liang et al., 2018) which stores consistent programs and an auxiliary reward function (Agarwal et al., 2019) which provides feedback to deal with spurious programs. Guu et al. (2017) employ various strategies to encourage even distributions over consistent programs in cases where the parser has been misled by spurious programs. Dasigi et al. (2019) use coverage of lexicon-like rules to guide the search of consistent programs. 6 Conclusions In this paper, we proposed a neural semantic parser that learns from denotations using abstract programs and latent structured alignments. Our parser achieves state-of-the-art performance on two benchmarks, W IKI TABLE Q UESTIONS and W IKI SQL. Empirical analysis shows that the inductive bias introduced by the alignment model helps our"
D19-1391,D18-1190,0,0.0255333,"Missing"
D19-1391,N18-2115,0,0.0366449,"Missing"
D19-1391,P16-1002,0,0.112553,"t program for a question, our parser finds alignments between slots (with prefix #) and question spans. Based on the alignment, it instantiates each slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs which accidentally execute to correct"
D19-1391,P15-1142,0,0.552994,"e, we could replace the uniqueness constraint with modeling the number of slots aligned to a span, or favor sparse alignment distributions. Crucially, the two-stage framework makes it easier to inject prior knowledge about datasets and formalisms while maintaining efficiency. 2 Background Given knowledge base t, our task is to map a natural utterance x to program z, which is then executed against a knowledge base to obtain denotation [[z]]t = d. We train our parser only based on d without access to correct programs z ∗ . Our experiments focus on two benchmarks, namely W IK I TABLE Q UESTIONS (Pasupat and Liang, 2015) and W IKI SQL (Zhong et al., 2017) where each question is paired with a Wikipedia table and a denotation. Figure 1 shows a simplified example taken from W IKI TABLE Q UESTIONS. 2.1 Grammars Executable programs z that can query tables are defined according to a language. Specifically, the search space of programs is constrained by grammar rules so that it can be explored efficiently. We adopt the variable-free language of Liang et al. (2018) and define an abstract grammar and an instantiation grammar which decompose the generation of a program in two stages.3 The first stage involves the gener"
D19-1391,P16-1003,0,0.0138843,"n explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence transduction (Yu et al., 2016) and AMR parsing (Lyu and Titov, 2018). Learning from Denotations To improve the efficiency of searching for consistent programs, Zhang et al. (2017) use a macro grammar induced from cached consistent programs. Unlike Zhang et al. (2017) who abstract entities and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, reinforcement learning based methods address the computational challenge by using a memory buffer (Liang et al., 2018) which stores consistent programs and an auxiliary reward function (Agarwal et al., 2019) which provides feedback to deal with spurious programs. Guu et al. (2017) employ various strategies to encourage even distributions over consistent programs in cases where the parser has been misled by spurious programs. D"
D19-1391,D14-1162,0,0.0820315,"sets, tables are extracted from Wikipedia and cover a wide range of domains. Entity extraction is important during parsing since entities are used as values in filter conditions during instantiation. String entities are extracted by string matching utterance spans and table cells. In W IKI TABLE Q UESTIONS, numbers and dates are extracted from the CoreNLP annotations released with the dataset. W IKI SQL does not have entities for dates, and we use string-based normalization to deal with numbers. Implementation We obtained word embeddings by a linear projection of GloVe pre-trained embeddings (Pennington et al., 2014) which were fixed during training. Attention scores were computed based on the dot product between two vectors. Each MLP is a one-hidden-layer perceptron with ReLU as the activation function. Dropout (Srivastava et al., 2014) was applied to prevent overfitting. All models were trained with Adam (Kingma and Ba, 2015). Implementations of abstract and instantiation grammars were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a s"
D19-1391,P17-1105,0,0.0207745,"programs cannot be correctly instantiated. Table 4 shows the proportion of errors attested by the two attention models. We observe that structured attention suffers less from instantiation errors compared against the standard attention baseline, which points to the benefits of the structured alignment model. 3781 5 Related Work Neural Semantic Parsing We follow the line of work that applies sequence-to-sequence models (Sutskever et al., 2014) to semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural langua"
D19-1391,D17-1160,0,0.0686532,"ctly instantiated. Table 4 shows the proportion of errors attested by the two attention models. We observe that structured attention suffers less from instantiation errors compared against the standard attention baseline, which points to the benefits of the structured alignment model. 3781 5 Related Work Neural Semantic Parsing We follow the line of work that applies sequence-to-sequence models (Sutskever et al., 2014) to semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural language. However, these methods ca"
D19-1391,P18-1034,0,0.123129,"Missing"
D19-1391,Q15-1003,0,0.0749439,"Missing"
D19-1391,P07-1121,0,0.367466,"s: Figure 1: After generating an abstract program for a question, our parser finds alignments between slots (with prefix #) and question spans. Based on the alignment, it instantiates each slot and a complete program is executed against a table to obtain a denotation. Introduction Semantic parsing is the task of translating natural language to machine interpretable meaning representations. Typically, it requires mapping a natural language utterance onto a program, which is executed against a knowledge base to obtain an answer or a denotation. Most previous work (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Jia and Liang, 2016) has focused on the supervised setting where a model is learned from question-program pairs. Weakly supervised semantic parsing (Berant et al., 2013; Liang et al., 2011) reduces the burden of annotating programs by learning from questions paired with their answers (or denotations). Two major challenges arise when learning from denotations: 1) training of the semantic parser requires exploring a large search space of possible programs to find those which are consistent, and execute to correct denotations; 2) the parser should be robust to spurious programs"
D19-1391,P17-1041,0,0.0241277,"d and linked, abstract programs cannot be correctly instantiated. Table 4 shows the proportion of errors attested by the two attention models. We observe that structured attention suffers less from instantiation errors compared against the standard attention baseline, which points to the benefits of the structured alignment model. 3781 5 Related Work Neural Semantic Parsing We follow the line of work that applies sequence-to-sequence models (Sutskever et al., 2014) to semantic parsing (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between pr"
D19-1391,D16-1138,0,0.0167158,"correspondences between programs and natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete latent variables has proved effective in other tasks like sequence transduction (Yu et al., 2016) and AMR parsing (Lyu and Titov, 2018). Learning from Denotations To improve the efficiency of searching for consistent programs, Zhang et al. (2017) use a macro grammar induced from cached consistent programs. Unlike Zhang et al. (2017) who abstract entities and relations from logical forms, we take a step further and abstract the computation of row and column selection. Our work also differs from Pasupat and Liang (2016) who resort to manual annotations to alleviate spuriousness. Instead, we equip our parser with an inductive bias to rule out spurious programs during training. Recently, rein"
D19-1391,N18-2093,0,0.0780745,"Missing"
D19-1391,D07-1071,0,0.0807273,"g (Jia and Liang, 2016; Dong and Lapata, 2016). Our work also relates to models which enforce type constraints (Yin and Neubig, 2017; Rabinovich et al., 2017; Krishnamurthy et al., 2017) so as to restrict the vast search space of potential programs. We use both methods as baselines to show that the structured bias introduced by our model can help our parser handle spurious programs in the setting of learning from denotations. Note that our alignment model can also be applied in the supervised case in order to help the parser rule out incorrect programs. Earlier work has used lexicon mappings (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2010) to model correspondences between programs and natural language. However, these methods cannot generalize to unseen tables where new relations and entities appear. To address this issue, Pasupat and Liang (2015) propose a floating parser which allows partial programs to be generated without being anchored to question tokens. In the same spirit, we use a sequence-to-sequence model to generate abstract programs while relying on explicit alignments to instantiate them. Besides semantic parsing, treating alignments as discrete late"
D19-1391,D17-1125,0,0.782666,"ons of abstract and instantiation grammars were based on AllenNLP (Gardner et al., 2017).5 4.2 Baselines Aside from comparing our model against previously published approaches, we also implemented the following baselines: Typed Seq2Seq Programs were generated using a sequence-to-sequence model with attention (Dong and Lapata, 2016). Similarly to Krishnamurthy et al. (2017), we constrained the decod5 Please refer to the Appendix for the full list of hyperparameters used in our experiments. Supervised by Denotations Dev. Test Pasupat and Liang (2015) Neelakantan et al. (2017) Haug et al. (2018) Zhang et al. (2017) Liang et al. (2018) Dasigi et al. (2019) Agarwal et al. (2019) 37.0 34.1 — 40.4 42.3 42.1 43.2 37.1 34.2 34.8 43.7 43.1 43.9 44.1 Typed Seq2Seq Abstract Programs f.w. standard attention f.w. structured attention 37.3 38.3 39.4 43.7 41.4 44.5 Table 1: Results on W IKI TABLE Q UESTIONS. f.w. stands for slots filled with. ing process so that only well-formed programs are predicted. This baseline can be viewed as merging the two stages of our model into one stage where generation of abstract programs and their instantiations are performed with a shared decoder. Standard Attention The aligned repr"
D19-5630,D18-1045,0,0.0611431,"Missing"
D19-5630,P16-1154,0,0.0169858,"ted via: βt,k ∝ exp(d|t Wb ek ) X qt = βt,k ek (1) Conditional Copy The variable ut is first computed as a switch gate, and then is used to obtain the output probability: k datt t = tanh(Wd [dt ; qt ]) pgen (yt |y<t , z, r)=softmaxyt (Wy datt t + by ) (2) P n×n , W where d ∈ k βt,k = 1, Wb ∈ R n×2n n×|V | |V | y y R , Wy ∈ R , by ∈ R are parameters, and |Vy |is the output vocabulary size. They further augment the decoder with a copy mechanism, allowing the ability to copy words directly from the value portions of records in the |z| content plan (i.e., {zk }k=1 ). They experimented with joint (Gu et al., 2016) and conditional copy methods (Gulcehre et al., 2016). Specifically, they introduce a variable ut ∈ {0, 1} for each time step to indicate whether the predicted token yt is copied (ut = 1) or not (ut = 0). The probability of generating yt is computed by: X p(yt |y<t , z, r) = p(yt , ut |y<t , z, r) p(ut |y<t , z, r) = softmax(wu · datt t + bu ) αt,j ∝ exp(dt |Wc rcs j ) p(yt , ut |y<t , z, r) =  P  p(ut |y<t , z, r) yt ←zk βt,k  P P p(ut |y<t , z, r) k βt,k yt ←rj αt,j ,j∈γk    p(ut |y<t , z, r)pgen (yt |y<t , z, r) ut = 1 ut = 2 ut = 0 P where yt ← zk indicates j∈γk αt,j = 1. that yt ca"
D19-5630,P16-1014,0,0.0135318,"(1) Conditional Copy The variable ut is first computed as a switch gate, and then is used to obtain the output probability: k datt t = tanh(Wd [dt ; qt ]) pgen (yt |y<t , z, r)=softmaxyt (Wy datt t + by ) (2) P n×n , W where d ∈ k βt,k = 1, Wb ∈ R n×2n n×|V | |V | y y R , Wy ∈ R , by ∈ R are parameters, and |Vy |is the output vocabulary size. They further augment the decoder with a copy mechanism, allowing the ability to copy words directly from the value portions of records in the |z| content plan (i.e., {zk }k=1 ). They experimented with joint (Gu et al., 2016) and conditional copy methods (Gulcehre et al., 2016). Specifically, they introduce a variable ut ∈ {0, 1} for each time step to indicate whether the predicted token yt is copied (ut = 1) or not (ut = 0). The probability of generating yt is computed by: X p(yt |y<t , z, r) = p(yt , ut |y<t , z, r) p(ut |y<t , z, r) = softmax(wu · datt t + bu ) αt,j ∝ exp(dt |Wc rcs j ) p(yt , ut |y<t , z, r) =  P  p(ut |y<t , z, r) yt ←zk βt,k  P P p(ut |y<t , z, r) k βt,k yt ←rj αt,j ,j∈γk    p(ut |y<t , z, r)pgen (yt |y<t , z, r) ut = 1 ut = 2 ut = 0 P where yt ← zk indicates j∈γk αt,j = 1. that yt can be copied from zk , yt ← rj indicates that yt can b"
D19-5630,P17-4012,0,0.088209,"Missing"
D19-5630,P16-1009,0,0.0909213,"Missing"
E03-1034,W99-0901,0,0.674888,"co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-bas"
E03-1034,J92-4003,0,0.0420674,"Missing"
E03-1034,C00-1028,0,0.698049,"ccurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., w"
E03-1034,J02-2003,0,0.755607,"for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model o"
E03-1034,W97-0802,0,0.0721583,"ens are then analyzed morphologically (compound recognition, assignment of part-of-speech tags), and a chunk parser identifies phrases and clauses by means of finite state grammars. The grammatical relations recognizer operates on the output of the parser while exploiting a large subcategorization lexicon. Although SMES recognizes a variety of grammatical relations, in our experiments we focused solely on relations of the form (v,r,n) where r can be a subject, direct object, or prepositional object (see the examples in Table 2). For the class-based models, the hierarchy available in GermaNet (Hamp and Feldweg, 1997) was used. The experiments reported in this paper make use of the noun taxonomy of GermaNet (version 3.0, 23,053 noun synsets), and the information encoded in it in terms of the hyponymy/hypernymy relation. Certain modifications to the original GermaNet hierarchy were necessary for the implementation of Li and Abe&apos;s method (1998). The GermaNet noun hierarchy is a directed acyclic graph (DAG) whereas their algorithm operates on trees. A solution to this problem is given by Li and Abe, who transform the DAG into a tree by copying each subgraph having multiple parents. An additional modification"
E03-1034,W02-1030,0,0.0184493,"Missing"
E03-1034,E99-1005,0,0.510241,"loyed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use a probability model or not) and evaluated (e.g., whether to use a task-based evaluation or not). Furthermore, previous work has almost exclusively focused on verbal selectional 27 preferences in English with the exception of Lapata et al. (1999, 2001), who look at adjectivenoun combinations, again for English. Verbs tend to impose stricter selectional preferences on their arguments than adjectives or nouns and thus provide a natural test bed for models of selectional preferences. However, research on verbal selectional preferences has been relatively narrow in scope as it has primarily focused on verbs and their direct objects, ignoring the selectional preferences pertaining to subjects and prepositional complements. The induction of selectional preferences typically addresses two related problems: (a) finding an appropriate class t"
E03-1034,P01-1046,0,0.394602,"nother related limitation of the frequency-based account is that it cannot make any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agre"
E03-1034,J98-2002,0,0.725704,"e any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Light and Greiff, 2002), there is little agreement on how selectional preferences must be modeled (e.g., whether to use"
E03-1034,A97-1031,0,0.0130299,"experiments, we compared the performance of the five methods discussed above against human judgments. Before discussing the details of our evaluation we present our general experimental setup (e.g., the corpora and hierarchy used) and the different types of parameters we explored. All our experiments were conducted on data obtained from the German Siiddeutsche Zeitung (SZ) 30 corpus, a 179 million word collection of newspaper texts. The corpus was parsed using the grammatical relation recognition component of SMES, a robust information extraction core system for the processing of German text (Neumann et al., 1997). SMES incorporates a tokenizer that maps the text into a stream of tokens. The tokens are then analyzed morphologically (compound recognition, assignment of part-of-speech tags), and a chunk parser identifies phrases and clauses by means of finite state grammars. The grammatical relations recognizer operates on the output of the parser while exploiting a large subcategorization lexicon. Although SMES recognizes a variety of grammatical relations, in our experiments we focused solely on relations of the form (v,r,n) where r can be a subject, direct object, or prepositional object (see the exam"
E03-1034,P93-1024,0,0.362844,"ts and incongruent with natural objects. Another related limitation of the frequency-based account is that it cannot make any predictions for words that never occurred in the corpus. A zero co-occurrence count might be due to insufficient evidence or might reflect the fact that a given word combination is inherently implausible. For the above reasons, most approaches model the selectional preferences of predicates (e.g., verbs, nouns, adjectives) by combining observed frequencies with knowledge about the semantic classes of their arguments. The classes can be induced directly from the corpus (Pereira et al., 1993; Brown et al., 1992; Lapata et al., 2001) or taken from a manually crafted taxonomy (Resnik, 1993; Li and Abe, 1998; Clark and Weir, 2002; Ciaramita and Johnson, 2000; Abney and Light, 1999). In the latter case the taxonomy is used to provide a mapping from words to conceptual classes, and in most cases WordNet (Miller et al., 1990) is employed for this purpose. Although most approaches agree on how selectional preferences must be represented, i.e., as a mapping cv : (p,r,c) —> a that maps each predicate p and the semantic class c of its argument with respect to role r to a real number a (Lig"
E03-1034,P01-1045,0,\N,Missing
E03-1073,E99-1003,0,0.0289168,"f compound nouns have concentrated on their automatic acquisition from corpora, syntactic disambiguation (i.e., determine the structure of compounds like income tax relief), and semantic interpretation (i.e., determine the semantic relation between income and tax in income tax). The acquisition of compound nouns is usually subsumed under the general discovery of terms from corpora. Terms are typically acquired by either symbolic or statistical means. Under a symbolic approach, candidate terms are extracted from the corpus using surface syntactic analysis (Lauer, 1995; Justeson and Katz, 1995; Bourigault and Jacquemin, 1999) and sometimes are further submitted to experts for manual inspection. The approach typically assumes no prior terminological knowledge, although Jacquemin (1996) proposed the detection of terminological variants in a corpus by making use of lists of existing terms. The main assumption underlying the statistical approach to term acquisition is that lexically associated words tend to appear together more often than expected on the basis of their individual occurrence frequencies. Once candidate terms are detected in the corpus, statistical tests (e.g., mutual information, the log-likelihood rat"
E03-1073,J90-1003,0,0.189546,"than expected on the basis of their individual occurrence frequencies. Once candidate terms are detected in the corpus, statistical tests (e.g., mutual information, the log-likelihood ratio) are used to determine which co-occurrences are valid terms (see Daille, 1996 and Manning and Schiitze, 1999 for overviews). Most of the statistical tests proposed in the literature rely on the fact that candidate terms will occur frequently in the corpus (Justeson and Katz, 1995) or, when hypothesis testing is applied, on the assumption that two words form a term when they co-occur more often than chance (Church and Hanks, 1990). This means that statistical tests cannot be applied reliably for candidate compounds 235 CoocF BNC >4 >1 >1 =1 52,832 160,214 510,673 350,459 Sample Ace 800 800 800 800 93.5 82.0 71.0 57.7 Table 1: Relation of noun co-occurrence frequency with accuracy with co-occurrence frequency of one and cannot be used to distinguish rare but valid noun compounds from rare but nonce noun sequences (compare (2b) and (2a) which are extracted from the British National Corpus; both bracketed terms were found in the corpus once.). (2) a. Although no one will doubt their possibilities for elegance and robustne"
E03-1073,J02-2003,0,0.0193821,"Missing"
E03-1073,P97-1018,1,0.814673,"ncy. Linguistic models of compound noun formation typically involve a hierarchical structure of lexical rules, which capture the regularities of compound noun formation while 237 (ci,c2) (substance, obj ect) (act, social group) (entity, location) (group. relation) (communication, act) (person, artef act) (institution, person) f(c],c2) 604.7 403.0 382.4 267.6 231.1 162.1 38.7 Examples iron table mining family girls school world language speech treatment developer's kit bank spokesman Table 3: Estimated concept pair frequencies also ruling out certain compounds as candidates (Pustejovsky, 1995; Copestake and Lascarides, 1997). Each lexical rule takes a pair of nouns of certain semantic type as input, and the output of the rule is a compound noun whose semantic representation stipulates the relation between a modifier and its head. For example, the compounds metal tube, leather belt and tin cup are the result of a lexical rule that combines a noun denoting a substance and a noun denoting an artefact to yield a compound denoting the artefact made of the substance. The noun frequency and probability features do not capture meaning regularities concerning the compounding process. For example, we would expect the combi"
E03-1073,A94-1009,0,0.0937025,"Missing"
E03-1073,C94-1103,0,0.0225811,"Missing"
E03-1073,P84-1108,0,0.156527,"Compounds are commonly written as a concatenation of words (see (1a,b)), or as single words (see (lc)), sometimes a hyphen is also used (see (le)). income tax AT & T headquarters bathroom public-relations income-tax relief The use of noun compounds is frequent not only in technical writing and newswire text (McDonald, 1982) but also in fictional prose (Leonard, 1984), and spoken language (Liberman and Sproat, 1992). Novel compounds are used as Alex Lascarides School of Informatics The University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, UK alex@inf.ed.ac.uk a text compression device (Marsh, 1984), i.e., to pack meaning into a minimal amount of linguistic structure, as a deictic device, or as a means to classify an entity which has no specific name (Downing, 1977). Computational investigations of compound nouns have concentrated on their automatic acquisition from corpora, syntactic disambiguation (i.e., determine the structure of compounds like income tax relief), and semantic interpretation (i.e., determine the semantic relation between income and tax in income tax). The acquisition of compound nouns is usually subsumed under the general discovery of terms from corpora. Terms are typ"
E09-1013,W02-1006,0,0.275884,"create a smoothed sense distribution. We also place a symmetric Dirichlet β on φ (Griffiths and Steyvers, 2002). The hyperparmeter β can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed. Our model is represented in graphical notation in Figure 1. The model sketched above only takes word information into account. Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002). The first idea that comes to mind, is to use the same model while treating various features as word-like elements. In other words, we could simply assume that the contexts we wish to model are the union of all our features. Although straightforward, this solution is undesirable. It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model. For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser). Laye"
E09-1013,P04-1036,0,0.174396,"Missing"
E09-1013,S07-1037,0,0.291705,"Missing"
E09-1013,S07-1087,0,0.0700657,"Missing"
E09-1013,W04-2406,0,0.583652,"our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4). Section 5 describes the resources and evaluation methodology used in our experiments. We discuss our results in Section 6, and conclude in Section 7. 2 quently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help"
E09-1013,J98-1004,0,0.91079,"Missing"
E09-1013,H05-1097,0,0.0278591,"du Abstract of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce. A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications. In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand. There is little risk that an important sense will be left out, or that irrelevant senses will influence the results. Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993). Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing"
E09-1013,S07-1002,0,0.577087,"inctions. In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection. We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model. For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem. This is in marked contrast with previous LDA-based models which mostly take only word-based information into account. We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4). Section 5 describes the resources and evaluation methodology used in our experiments. We discuss our results in Section 6, and conclude in Section 7. 2 quently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998)"
E09-1013,E06-1018,0,0.45081,"rd are partitioned into classes by considering their co-occurring contexts. Considerable latitude is allowed in selecting and representing the cooccurring contexts. Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003). The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004). In essence, each instance of a target word is represented as a feature vector which subse2 Such a mapping is only performed to enable evaluation and comparison with other approaches (see Section 5). 104 of topics commonly used in document-modeling tasks. Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch¨utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine wi"
E09-1013,S07-1060,0,0.077172,"framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process. In thi"
E09-1013,D07-1109,0,0.221133,"Missing"
E09-1013,briscoe-carroll-2002-robust,0,0.0544579,"Missing"
E09-1013,D07-1108,0,0.0687249,"), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic. Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word. Then, the word sense is selected based on the word, neighbor, and topic. Boyd"
E09-1013,P05-1048,0,0.0121611,"istinctions which is fixed, and may not be entirely suitable for different applications. In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand. There is little risk that an important sense will be left out, or that irrelevant senses will influence the results. Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993). Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as d"
E09-1013,E03-1020,0,0.344755,", and conclude in Section 7. 2 quently serves as input to the chosen clustering method. A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007). Graph-based methods have also been applied to the sense induction task. In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences. Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003). Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature. The inferred document-level topics can help determine coarsegrained sense distinctions. Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system. In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD. Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm. For each target word, a topic is sampled from the document’s topic distribution, and"
E09-1013,P07-1094,0,0.0306158,"basis of latent variables. Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems. Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account. For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text. In the future, we hope to explore more rigorous parameter estimation techniques. Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values. Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance. In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal. This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation"
E09-1013,N06-2015,0,0.09398,"Missing"
E09-1013,S07-1053,0,\N,Missing
E09-1026,S07-1018,0,0.395682,"the eye]Body part . [A falling rock]Cause crushed [my ankle]Body part . [She]Agent slapped [him]Victim [hard]Degree [for his change of mood]Reason . injured [her [Rachel]Agent friend]Victim [by closing the car door on his left hand]Means . The English FrameNet (version 1.3) contains 502 frames covering 5,866 lexical entries. It also comes with a set of manually annotated example sentences, taken mostly from the British National Corpus. These annotations are often used 1 The approaches are too numerous to list; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al., 2007) for an overview of the stateof-the-art. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 220–228, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 220 mantic compatibility. We formalize the annotation projection problem as a generalization of the linear assignment problem and solve it efficiently using the simplex algorithm. We evaluate our algorithm by comparing the performance of a semantic role labeler trained on the annotations produced by our method and on a smaller dataset consisting solely of hand-labeled instances. R"
E09-1026,P06-4020,0,0.00848325,"Missing"
E09-1026,P06-1146,1,0.749428,"Missing"
E09-1026,J05-1004,0,0.141311,", UK mlap@inf.ed.ac.uk (1) a. b. Introduction c. Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in traind. [Lee]Agent punched [John]Victim [in the eye]Body part . [A falling rock]Cause crushed [my ankle]Body part . [She]Agent slapped [him]Victim [hard]Degree [for his change of mood]Reason . injured [her [Rachel]Agent friend]Victim [by closing the car door on his left hand]Means . The English FrameNet (version 1.3) contains 502 frames covering 5,866 lexical entries. It also comes with a set of manually annotated example sentences, taken mostly from the British National Co"
E09-1026,D08-1048,0,0.346352,"Missing"
E09-1026,C04-1134,0,0.167452,"ave focused primarily on non-English languages. Annotation projection is a popular framework for transferring frame semantic annotations from one language to another by exploiting the translational and structural equivalences present in parallel corpora. The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language. Pad´o and Lapata (2006) transfer semantic role annotations from English onto German and Johansson and Nugues (2006) from English onto Swedish. A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates instantiating these concepts are found in a monolingual corpus and their arguments are labeled with FrameNet roles. Other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data. Swier and Stevenson (2004) present an unsupervised method for labeling the arguments of verbs with their"
E09-1026,furstenau-2008-enriching,1,0.909294,"Missing"
E09-1026,D07-1002,1,0.787791,"ntic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (1) a. b. Introduction c. Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in traind. [Lee]Agent punched [John]Victim [in the eye]Body part . [A falling rock]Cause crushed [my ankle]Body part . [She]Agent slapped [him]Victim [hard]Degree [for his change of mood]Reason . injured [her [Rachel]Agent friend]Victim [by closing the car door on his lef"
E09-1026,J02-3001,0,0.266174,"nt problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (1) a. b. Introduction c. Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in traind. [Lee]Agent punched [John]Victim [in the eye]Body part . [A falling rock]Cause"
E09-1026,P03-1002,0,0.0517221,"led and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (1) a. b. Introduction c. Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in traind. [Lee]Agent punched [John]Victim [in the eye]Body part . [A falling rock]Cause crushed [my ankle]Body part . [She]Agent slapped [him]Victim [hard]Degree [for his change of mood]Reason . injur"
E09-1026,P07-1025,0,0.0279853,"se221 ity model on which future assignments are based. Being unsupervised, their approach requires no manual effort other than creating the frame dictionary. Unfortunately, existing resources do not have exhaustive coverage and a large number of verbs may be assigned no semantic role information since they are not in the dictionary in the first place. Pennacchiotti et al. (2008) address precisely this problem by augmenting FrameNet with new lexical units if they are similar to an existing frame (their notion of similarity combines distributional and WordNet-based measures). In a similar vein, Gordon and Swanson (2007) attempt to increase the coverage of PropBank. Their approach leverages existing annotations to handle novel verbs. Rather than annotating new sentences that contain novel verbs, they find syntactically similar verbs and use their annotations as surrogate training data. Our own work aims to reduce but not entirely eliminate the annotation effort involved in creating training data for semantic role labeling. We thus assume that a small number of manual annotations is initially available. Our algorithm augments these with unlabeled examples whose roles are inferred automatically. We apply our me"
E09-1026,P06-2057,0,0.0265563,"efore not surprising that previous efforts to reduce the need for semantic role annotation have focused primarily on non-English languages. Annotation projection is a popular framework for transferring frame semantic annotations from one language to another by exploiting the translational and structural equivalences present in parallel corpora. The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language. Pad´o and Lapata (2006) transfer semantic role annotations from English onto German and Johansson and Nugues (2006) from English onto Swedish. A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates instantiating these concepts are found in a monolingual corpus and their arguments are labeled with FrameNet roles. Other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data. Swier and Stevenson"
E09-1026,H05-1047,0,0.00639757,"nstances alone. 1 Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk (1) a. b. Introduction c. Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in traind. [Lee]Agent punched [John]Victim [in the eye]Body part . [A falling rock]Cause crushed [my ankle]Body part . [She]Agent slapped [him]Victim [hard]Degree [for his change of mood]Reason . injured [her [Rachel]Agent friend]Victim [by closing the car door on his left hand]Means . The English FrameNet (version 1.3) contains 502 frames covering 5,866 lexical entries. It also comes with a se"
E09-1026,C08-1050,0,0.0350831,"itions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability). We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs. Semantic role labeler We evaluated our method on a semantic role labeling task. Specifically, we compared the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method. Our semantic role labeler followed closely the implementation of Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. For the latter we performed multi-class classification following the one-versus-one method3 (Friedman, 1996). For the experiments reported in this paper we used the L IB L INEAR library (Fan et al., 2008). The misclassification penalty C was set to 0.1. To evaluate against the test set, we linearized the resulting dependency graphs in"
E09-1026,W04-3213,0,\N,Missing
E09-1026,P98-1013,0,\N,Missing
E09-1026,C98-1013,0,\N,Missing
E14-1027,S07-1002,0,0.0364794,"More importantly, the variance in BayesCat resembles the variance present in the gold standard much more closely. The clusterings learnt by CW tend to consist of as a single measure of clustering quality. V-Measure is the harmonic mean between homogeneity and collocation (Rosenberg and Hirschberg, 2007). Like purity, V-Measure performs cluster-based comparisons but is an entropy-based method. It measures the conditional entropy of a cluster given a class, and vice versa. Cluster-F1 is an item-based evaluation metric which we propose drawing inspiration from the supervised metric presented in Agirre and Soroa (2007). Cluster-F1 maps each target word type to a gold cluster based on its soft class membership, and is thus appropriate for evaluation of soft clustering output. We first create a K × G soft mapping matrix M from each induced category ki to gold classes g j from P(g j |ki ). We then map each target word type to a gold class by multiplying its probability distribution over soft clusters with the mapping matrix M , and taking the maximum value. Finally, we compute standard precision, recall and F1 between the mapped system categories and the gold classes. 6 Results Our experiments are designed to"
E14-1027,W06-3812,0,0.171948,"ccessfully applied to natural language acquisition tasks such as word segmentation (Borschinger and Johnson, 2011), or sentence processing (Levy et al., 2009). Sanborn et al. (2006) also use particle filters for small-scale categorization experiments with artificial stimuli. To the best of our knowledge, we present the first particle filtering algorithm for large-scale category acquisition from natural text. Our work is closest to Fountain and Lapata (2011) who also develop a model for inducing natural language categories. Specifically, they propose an incremental version of Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. The latter takes as input a graph which is constructed from corpus-based co-occurrence statistics and produces a hard clustering over the nodes in the graph. Contrary to our model, they treat the tasks of inferring a semantic representamation relevant for extracting semantic categories is to a large extent redundantly encoded in linguistic experience (Riordan and Jones, 2011). Besides, there are known difficulties with feature norms such as the small number of words for which these can be obtained, the quality of the attributes, and variability in the"
E14-1027,E09-1013,1,0.838758,"cquisition as well as the processes of generalizing and generating new categories and exemplars (Jern and Kemp, 2013; Kemp et al., 2012). The above models are conceptually similar to ours. However, they were developed with adult categorization in mind, and use rather simplistic categories representing toy-domains. It is therefore not clear whether they generalize to arbitrary stimuli and data sizes. We aim to show that it is possible to acquire natural language categories on a larger scale purely from linguistic context. Our model is loosely related to Bayesian models of word sense induction (Brody and Lapata, 2009; Yao and Durme, 2011). We also assume that local linguistic context can provide important cues for word meaning and by extension category membership. However, the above models focus on performance optimization and learn in an ideal batch mode, while incorporating various kinds of additional features such as part of speech tags or dependencies. In contrast, we develop a cognitively plausible (early) language learning model and show that categories can be acquired purely from context, as well as in an incremental fashion. From a modeling perspective, we learn categories incrementally using a pa"
E14-1027,D11-1122,1,0.840421,"h-based model for category acquisition. Their algorithm incrementally constructs a graph from co-occurrence counts of target words and their contexts (they use a symmetric context window of five words). Target words constitute the nodes of the graph, their co-occurrences are transformed into a vector of positive PMI values, and graph edges correspond to the cosine similarity between the PMI-vectors representing any two nodes. They use Chinese Whispers (Biemann, 2006) to partition a graph into categories. Purity/Collocation are based on member overlap between induced clusters and gold classes (Lang and Lapata, 2011). Purity measures the degree to which each cluster contains instances that share the same gold class, while collocation measures the degree to which instances with the same gold class are assigned to a single cluster. We report the harmonic mean of purity and collocation 2 The dataset is available from www.frermann.de/data. checked for convergence on the development set. 4 While in theory particles should be averaged, we found that eventually they became highly similar — a common problem known as sample impoverishment, which we plan to tackle in the future. Nevertheless, diversity among partic"
E14-1027,W11-1102,0,0.0304646,"Missing"
E14-1027,D07-1043,0,0.0836636,"Missing"
E17-1059,P12-1039,1,0.752484,"our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product infor"
E17-1059,P16-1004,1,0.841718,"model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. The attention model boosts performance for various tasks (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 201"
E17-1059,D15-1166,0,0.0304329,"Missing"
E17-1059,W15-2922,0,0.026596,"ries. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content"
E17-1059,P82-1020,0,0.85057,"Missing"
E17-1059,D13-1176,0,0.0263763,"valuable for sentiment analysis, but not well studied previously. • We propose an attention-enhanced attributeto-sequence model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. T"
E17-1059,N16-1086,0,0.0159003,"nd hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product information implicitly indicates the style of generated reviews, which makes the results extremely diverse. Another line of related work is the encoderdecoder model with neural networks. Specifically, an encoder is employed to encode"
E17-1059,P02-1040,0,0.117645,"employs distributed representations to avoid using sparse indicator features. Then, we evaluate the NN methods that use different attributes to retrieve reviews, which is a strong baseline for the generation task. The results show that our method outperforms the baseline methods. Moreover, the improvements of the attention mechanism are significant with p < 0.05 according to the bootstrap resampling test (Koehn, 2004). We further show some examples to analyze the attention model in Section 4.4. we use the greedy search algorithm to generate reviews. 4.3 MELM 59.00 Evaluation Results The BLEU (Papineni et al., 2002) score is used for automatic evaluation, which has been shown to correlate well with human judgment on many generation tasks. The BLEU score measures the precision of n-gram matching by comparing the generated results with references, and penalizes length using a brevity penalty term. We compute BLEU-1 (unigram) and BLEU-4 (up to 4 grams) in experiments. 4.3.1 Comparison with Baseline Methods We describe the comparison methods as follows: Rand. The predicted results are randomly sampled from all the reviews in the T RAIN set. This baseline method suggests the expected lower bound for this task"
E17-1059,D14-1181,0,0.00361927,"nce against baseline methods. Moreover, we demonstrate that the attention mechanism significantly improves the performance of our model. The contributions of this work are three-fold: 2 Related Work Sentiment analysis and opinion mining aim to identify and extract subjective content in text (Liu, 2015). Most previous work focuses on using rulebased methods or machine learning techniques for sentiment classification, which classifies reviews into different sentiment categories. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However"
E17-1063,D07-1101,0,0.0611767,"Missing"
E17-1063,W02-1001,0,0.18569,"tion School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB {x.zhang,jianpeng.cheng}@ed.ac.uk, mlap@inf.ed.ac.uk Abstract ald et al., 2005a) or transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006b). Graph-based dependency parsers are typically arc-factored, where the score of a tree is defined as the sum of the scores of all its arcs. An arc is scored with a set of local features and a linear model, the parameters of which can be effectively learned with online algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003; Freund and Schapire, 1999; Collins, 2002). In order to efficiently find the best scoring tree during training and decoding, various maximization algorithms have been developed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b). In general, graphbased methods are optimized globally, using features of single arcs in order to make the learning and inference tractable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execut"
E17-1063,P16-1231,0,0.0847652,"Missing"
E17-1063,P16-2006,0,0.0339734,"do the same for a graph-based parser. Lei et al. (2014b) apply tensor decomposition to obtain word embeddings in their syntactic roles, which they subsequently use in a graphbased parser. Dyer et al. (2015) redesign components of a transition-based system where the buffer, stack, and action sequences are modeled separately with stack long short-term memory networks. The hidden states of these LSTMs are concatenated and used as features to a final transition classifier. Kiperwasser and Goldberg (2016) use bidirectional LSTMs to extract features for a transition- and graph-based parser, whereas Cross and Huang (2016) build a greedy arc-standard parser using similar features. In our work, we formalize dependency parsing Related Work Graph-based Parsing Graph-based dependency parsers employ a model for scoring possible dependency graphs for a given sentence. The graphs are typically factored into their component arcs and the score of a tree is defined as the sum of its arcs. This factorization enables tractable search for the highest scoring graph structure which is commonly formulated as the search for the maximum spanning tree (MST). The Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967; McDonal"
E17-1063,de-marneffe-etal-2006-generating,0,0.0414762,"Missing"
E17-1063,D15-1041,0,0.0341151,"d Wt ∈ Rq×|T |are the word and POS tag embedding matrices, where |V |is the vocabulary size, s is the word embedding size, |T |is the POS tag set size, and q the tag embedding size. The hidden states of the forward and backward LSTMs are concatenated to obtain ai , the final representation of wi : ai = [hFi ; hB i ] i ∈ [0, N ] (4) J(θ) = − S∈T i=1 Note that bidirectional LSTMs are one of many possible ways of representing word wi . Alternative representations include embeddings obtained from feed-forward neural networks (Chen and Manning, 2014; Lei et al., 2014a), character-based embeddings (Ballesteros et al., 2015), and more conventional features such as those introduced in McDonald et al. (2005a). 3.2 where T is the training set, h(wi ) is wi ’s gold standard head3 within sentence S, and NS the number of words in S (excluding ROOT). During inference, for each word wi (i ∈ [1, NS ]) in S, we greedily choose the most likely head wj (j ∈ [0, NS ]): wj = arg max Phead (wj |wi , S) wj :j∈[0,NS ] Head Selection (8) Note that the prediction for each word wi is made independently of the other words in the sentence. Given our greedy inference method, there is no guarantee that predicted hhead, dependenti arcs f"
E17-1063,P15-1033,0,0.452932,"a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Lei et al. (2014a) do the same for a graph-based parser. Lei et al. (2014b) apply tensor decomposition to obtain word embeddings in their syntactic roles, which they subsequently use in a graphbased parser. Dyer et al. (2015) redesign components of a transition-based system where the buffer, stack, and action sequences are modeled separately with stack long short-term memory networks. The hidden states of these LSTMs are concatenated and used as features to a final transition classifier. Kiperwasser and Goldberg (2016) use bidirectional LSTMs to extract features for a transition- and graph-based parser, whereas Cross and Huang (2016) build a greedy arc-standard parser using similar features. In our work, we formalize dependency parsing Related Work Graph-based Parsing Graph-based dependency parsers employ a model"
E17-1063,D10-1125,0,0.0166391,"Missing"
E17-1063,P96-1011,0,0.177616,"t ald et al., 2005a) or transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006b). Graph-based dependency parsers are typically arc-factored, where the score of a tree is defined as the sum of the scores of all its arcs. An arc is scored with a set of local features and a linear model, the parameters of which can be effectively learned with online algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003; Freund and Schapire, 1999; Collins, 2002). In order to efficiently find the best scoring tree during training and decoding, various maximization algorithms have been developed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b). In general, graphbased methods are optimized globally, using features of single arcs in order to make the learning and inference tractable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsu"
E17-1063,P14-1130,0,0.0677326,"xecute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsumoto, 2003; Zhang and Nivre, 2011). Regardless of the algorithm used, most well-known dependency parsers, such as the MST-Parser (McDonald et al., 2005b) and the MaltPaser (Nivre et al., 2006a), rely on extensive feature engineering. Feature templates are typically manually designed and aim at capturing head-dependent relationships which are notoriously sparse and difficult to estimate. More recently, a few approaches (Chen and Manning, 2014; Lei et al., 2014a; Kiperwasser and Goldberg, 2016) apply neural networks for learning dense feature representations. The learned features are subsequently used in a conventional graph- or transition-based parser, or better deConventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we formalize dependency parsing as the problem of independently selecting the head of each word in a sentence. Our model which we call D E NS E (as shorthand for Dependency Neural Selection) produces a distribution over possible heads for each word using features obtained fr"
E17-1063,P14-1043,0,0.0605529,"Missing"
E17-1063,C12-1059,0,0.0111294,"kens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Lei et al. (2014a) do the same for a graph-based parser. Lei et al. (2014b) apply tensor decomposition to obtain word embeddings in their sy"
E17-1063,P13-2109,0,0.0740449,"ton et al., 2014). For the CTB experiments, we trained 300 dimensional 4.3 Results For both English and Chinese experiments, we report unlabeled (UAS) and labeled attachment scores (LAS) on the development and test sets; following Chen and Manning (2014) punctuation is excluded from the evaluation. Experimental results on PTB are shown in Table 2. We compared our model with several recent papers following the same evaluation protocol and experimental settings. The first block in the table contains mostly graph-based parsers which do not use neural networks: Bohnet10 (Bohnet, 2010), Martins13 (Martins et al., 2013), and Z&M14 (Zhang and McDonald, 2014). Z&N11 (Zhang and Nivre, 2011) is a transition-based parser with non-local features. Accuracy results for all four parsers are reported in Weiss et al. (2015). The second block in Table 2 presents results obtained from neural network-based parsers. C&M14 (Chen and Manning, 2014) is a transitionbased parser using features learned with a feed 5 http://stp.lingfil.uu.se/˜nivre/research/ Penn2Malt.html 6 We make the number of sentences in the development and test sets comparable. 7 Dev UAS LAS — — — — — — — — 92.00 89.70 93.20 90.90 — — — — — — — — 90.77 88.3"
E17-1063,P07-1050,0,0.0373949,"isner, 1996; Eisner, 2000) in the case of projective trees. During training, weight parameters of the scoring function can be learned with margin-based algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003) or the structured perceptron (Freund and Schapire, 1999; Collins, 2002). Beyond basic first-order models, the literature offers a few examples of 666 as the task of finding for each word in a sentence its most probable head. Both head selection and the features it is based on are learned using neural networks. The idea of modeling child-parent relations independently dates back to Hall (2007) who use an edge-factored model to generate k-best parse trees which are subsequently reranked using a model based on rich global features. Later Smith (2010) show that a head selection variant of their loopy belief propagation parser performs worse than a model which incorporates tree structure constraints. Our parser is conceptually simpler: we rely on head selection to do most of the work and decode the best tree directly without using a reranker. In common with recent neural network-based dependency parsers, we aim to alleviate the need for hand-crafting feature combinations. Beyond featur"
E17-1063,P04-1013,0,0.0133379,"2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Lei et al. (2014a) do the same for a graph-based parser. Lei et al. (2014b) apply tensor decomposition to obtain word embeddings in their syntactic roles, which they subsequently use in a graphbased parser. Dyer et al. (2015) redesign components of a transition-based system where the buffer,"
E17-1063,P05-1012,0,0.0944864,"sition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006b). Graph-based dependency parsers are typically arc-factored, where the score of a tree is defined as the sum of the scores of all its arcs. An arc is scored with a set of local features and a linear model, the parameters of which can be effectively learned with online algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003; Freund and Schapire, 1999; Collins, 2002). In order to efficiently find the best scoring tree during training and decoding, various maximization algorithms have been developed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b). In general, graphbased methods are optimized globally, using features of single arcs in order to make the learning and inference tractable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsumoto, 2003; Zhang and Nivre, 2011). R"
E17-1063,P10-1110,0,0.0370119,"des a stack for storing partially processed tokens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Lei et al. (2014a) do the same for a graph-based parser. Lei et al. (2014b) apply tensor d"
E17-1063,H05-1066,0,0.080132,"sition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006b). Graph-based dependency parsers are typically arc-factored, where the score of a tree is defined as the sum of the scores of all its arcs. An arc is scored with a set of local features and a linear model, the parameters of which can be effectively learned with online algorithms (Crammer and Singer, 2001; Crammer and Singer, 2003; Freund and Schapire, 1999; Collins, 2002). In order to efficiently find the best scoring tree during training and decoding, various maximization algorithms have been developed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b). In general, graphbased methods are optimized globally, using features of single arcs in order to make the learning and inference tractable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsumoto, 2003; Zhang and Nivre, 2011). R"
E17-1063,P07-1080,0,0.0409427,"ency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Lei et al. (2014a) do the same for a graph-based parser. Lei et al. (2014b) apply tensor decomposition to obtain word embeddings in their syntactic roles, which they subsequently use in a graphbased parser. Dyer et al. (2015) redesign components of a transition-based system where the buffer, stack, and action sequences"
E17-1063,nivre-etal-2006-maltparser,0,0.0413182,"ctable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsumoto, 2003; Zhang and Nivre, 2011). Regardless of the algorithm used, most well-known dependency parsers, such as the MST-Parser (McDonald et al., 2005b) and the MaltPaser (Nivre et al., 2006a), rely on extensive feature engineering. Feature templates are typically manually designed and aim at capturing head-dependent relationships which are notoriously sparse and difficult to estimate. More recently, a few approaches (Chen and Manning, 2014; Lei et al., 2014a; Kiperwasser and Goldberg, 2016) apply neural networks for learning dense feature representations. The learned features are subsequently used in a conventional graph- or transition-based parser, or better deConventional graph-based dependency parsers guarantee a tree structure both during training and inference. Instead, we"
E17-1063,N03-1033,0,0.106391,"Missing"
E17-1063,W06-2933,0,0.0970364,"Missing"
E17-1063,I05-3027,0,0.00724733,"ither dataset, we used the last 374/367 sentences in the Czech/German training set as development data.6 Projective statistics of the four datasets are summarized in Table 1. 4.2 Test UAS LAS 92.88 90.71 92.89 90.55 93.22 91.02 93.00 90.95 91.80 89.60 93.10 90.90 93.99 92.05 94.61 92.79 93.10 91.00 93.90 91.90 90.39 88.05 91.00 88.61 94.02 91.84 94.10 91.90 Table 2: Results on English dataset (PTB with Stanford Dependencies). +E: we post-process non-projective output with the Eisner algorithm. GloVe vectors on the Chinese Gigaword corpus which we segmented with the Stanford Chinese Segmenter (Tseng et al., 2005). For Czech and German, we did not use pre-trained word vectors. The POS tag embedding size was set to q = 30 in the English experiments, q = 50 in the Chinese experiments and q = 40 in both Czech and German experiments. Training Details We trained our models on an Nvidia GPU card; training takes one to two hours. Model parameters were uniformly initialized to [−0.1, 0.1]. We used Adam (Kingma and Ba, 2014) to optimize our models with hyper-parameters recommended by the authors (i.e., learning rate 0.001, first momentum coefficient 0.9, and second momentum coefficient 0.999). To alleviate the"
E17-1063,W03-3017,0,0.025237,"(English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with the state of the art. 2 Transition-based Parsing As the term implies, transition-based parsers conceptualize the process of transforming a sentence into a dependency tree as a sequence of transitions. A transition system typically includes a stack for storing partially processed tokens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikk"
E17-1063,W04-0308,0,0.0205524,"As the term implies, transition-based parsers conceptualize the process of transforming a sentence into a dependency tree as a sequence of transitions. A transition system typically includes a stack for storing partially processed tokens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and"
E17-1063,P15-1032,0,0.10754,"opment and test sets; following Chen and Manning (2014) punctuation is excluded from the evaluation. Experimental results on PTB are shown in Table 2. We compared our model with several recent papers following the same evaluation protocol and experimental settings. The first block in the table contains mostly graph-based parsers which do not use neural networks: Bohnet10 (Bohnet, 2010), Martins13 (Martins et al., 2013), and Z&M14 (Zhang and McDonald, 2014). Z&N11 (Zhang and Nivre, 2011) is a transition-based parser with non-local features. Accuracy results for all four parsers are reported in Weiss et al. (2015). The second block in Table 2 presents results obtained from neural network-based parsers. C&M14 (Chen and Manning, 2014) is a transitionbased parser using features learned with a feed 5 http://stp.lingfil.uu.se/˜nivre/research/ Penn2Malt.html 6 We make the number of sentences in the development and test sets comparable. 7 Dev UAS LAS — — — — — — — — 92.00 89.70 93.20 90.90 — — — — — — — — 90.77 88.35 91.39 88.94 94.17 91.82 94.30 91.95 http://nlp.stanford.edu/projects/glove/ 670 Parser Z&N11 Z&M14 C&M14 Dyer15 K&G16 graph K&G16 trans D E NS E-Pei D E NS E-Pei+E D E NS E D E NS E+E Dev UAS LAS"
E17-1063,J08-4003,0,0.0238398,"s conceptualize the process of transforming a sentence into a dependency tree as a sequence of transitions. A transition system typically includes a stack for storing partially processed tokens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural net"
E17-1063,W03-3023,0,0.112967,"ed (Eisner, 1996; Eisner, 2000; McDonald et al., 2005b). In general, graphbased methods are optimized globally, using features of single arcs in order to make the learning and inference tractable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsumoto, 2003; Zhang and Nivre, 2011). Regardless of the algorithm used, most well-known dependency parsers, such as the MST-Parser (McDonald et al., 2005b) and the MaltPaser (Nivre et al., 2006a), rely on extensive feature engineering. Feature templates are typically manually designed and aim at capturing head-dependent relationships which are notoriously sparse and difficult to estimate. More recently, a few approaches (Chen and Manning, 2014; Lei et al., 2014a; Kiperwasser and Goldberg, 2016) apply neural networks for learning dense feature representations. The learned features are subsequently used in"
E17-1063,D14-1162,0,0.081076,"rescaled the gradient when its norm exceeded 5 (Pascanu et al., 2013). Dropout (Srivastava et al., 2014) was applied to our model with the strategy recommended in the literature (Zaremba et al., 2014; Semeniuta et al., 2016). On all datasets, we used two-layer LSTMs and set d = s = 300, where d is the hidden unit size and s is the word embedding size. As in previous neural dependency parsing work (Chen and Manning, 2014; Dyer et al., 2015), we used pre-trained word vectors to initialize our word embedding matrix We . For the PTB experiments, we used 300 dimensional pre-trained GloVe7 vectors (Pennington et al., 2014). For the CTB experiments, we trained 300 dimensional 4.3 Results For both English and Chinese experiments, we report unlabeled (UAS) and labeled attachment scores (LAS) on the development and test sets; following Chen and Manning (2014) punctuation is excluded from the evaluation. Experimental results on PTB are shown in Table 2. We compared our model with several recent papers following the same evaluation protocol and experimental settings. The first block in the table contains mostly graph-based parsers which do not use neural networks: Bohnet10 (Bohnet, 2010), Martins13 (Martins et al., 2"
E17-1063,D08-1059,0,0.0163246,"system typically includes a stack for storing partially processed tokens, a buffer containing the remaining input, and a set of arcs containing all dependencies between tokens that have been added so far (Nivre, 2003; Nivre et al., 2006b). A dependency tree is constructed by manipulating the stack and buffer, and appending arcs with predetermined operations. Most popular parsers employ an arc-standard (Yamada and Matsumoto, 2003; Nivre, 2004) or arc-eager transition system (Nivre, 2008). Extensions of the latter include the use of non-local training methods to avoid greedy error propagation (Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2012). Neural Network-based Features Neural network representations have a long history in syntactic parsing (Mayberry and Miikkulainen, 1999; Henderson, 2004; Titov and Henderson, 2007). Recent work uses neural networks in lieu of the linear classifiers typically employed in conventional transition- or graph-based dependency parsers. For example, Chen and Manning (2014) use a feed forward neural network to learn features for a transition-based parser, whereas Lei et al. (2014a) do the same for a graph-based parser. Lei et al."
E17-1063,C16-1165,0,0.0183376,"tails We trained our models on an Nvidia GPU card; training takes one to two hours. Model parameters were uniformly initialized to [−0.1, 0.1]. We used Adam (Kingma and Ba, 2014) to optimize our models with hyper-parameters recommended by the authors (i.e., learning rate 0.001, first momentum coefficient 0.9, and second momentum coefficient 0.999). To alleviate the gradient exploding problem, we rescaled the gradient when its norm exceeded 5 (Pascanu et al., 2013). Dropout (Srivastava et al., 2014) was applied to our model with the strategy recommended in the literature (Zaremba et al., 2014; Semeniuta et al., 2016). On all datasets, we used two-layer LSTMs and set d = s = 300, where d is the hidden unit size and s is the word embedding size. As in previous neural dependency parsing work (Chen and Manning, 2014; Dyer et al., 2015), we used pre-trained word vectors to initialize our word embedding matrix We . For the PTB experiments, we used 300 dimensional pre-trained GloVe7 vectors (Pennington et al., 2014). For the CTB experiments, we trained 300 dimensional 4.3 Results For both English and Chinese experiments, we report unlabeled (UAS) and labeled attachment scores (LAS) on the development and test se"
E17-1063,D12-1030,0,0.0209847,"Missing"
E17-1063,P14-2107,0,0.0624119,"eriments, we trained 300 dimensional 4.3 Results For both English and Chinese experiments, we report unlabeled (UAS) and labeled attachment scores (LAS) on the development and test sets; following Chen and Manning (2014) punctuation is excluded from the evaluation. Experimental results on PTB are shown in Table 2. We compared our model with several recent papers following the same evaluation protocol and experimental settings. The first block in the table contains mostly graph-based parsers which do not use neural networks: Bohnet10 (Bohnet, 2010), Martins13 (Martins et al., 2013), and Z&M14 (Zhang and McDonald, 2014). Z&N11 (Zhang and Nivre, 2011) is a transition-based parser with non-local features. Accuracy results for all four parsers are reported in Weiss et al. (2015). The second block in Table 2 presents results obtained from neural network-based parsers. C&M14 (Chen and Manning, 2014) is a transitionbased parser using features learned with a feed 5 http://stp.lingfil.uu.se/˜nivre/research/ Penn2Malt.html 6 We make the number of sentences in the development and test sets comparable. 7 Dev UAS LAS — — — — — — — — 92.00 89.70 93.20 90.90 — — — — — — — — 90.77 88.35 91.39 88.94 94.17 91.82 94.30 91.95"
E17-1063,P11-2033,0,0.0969181,"00; McDonald et al., 2005b). In general, graphbased methods are optimized globally, using features of single arcs in order to make the learning and inference tractable. Transition-based algorithms factorize a tree into a set of parsing actions. At each transition state, the parser scores a candidate action conditioned on the state of the transition system and the parsing history, and greedily selects the highest-scoring action to execute. This score is typically obtained with a classifier based on non-local features defined over a rich history of parsing decisions (Yamada and Matsumoto, 2003; Zhang and Nivre, 2011). Regardless of the algorithm used, most well-known dependency parsers, such as the MST-Parser (McDonald et al., 2005b) and the MaltPaser (Nivre et al., 2006a), rely on extensive feature engineering. Feature templates are typically manually designed and aim at capturing head-dependent relationships which are notoriously sparse and difficult to estimate. More recently, a few approaches (Chen and Manning, 2014; Lei et al., 2014a; Kiperwasser and Goldberg, 2016) apply neural networks for learning dense feature representations. The learned features are subsequently used in a conventional graph- or"
E17-1063,N16-1035,1,0.160416,"es (at inference time) trees for the overwhelming majority of sentences, while non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate D E NS E on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of the approach, our parsers are on par with the state of the art.1 1 Introduction Dependency parsing plays an important role in many natural language applications, such as relation extraction (Fundel et al., 2007), machine translation (Carreras and Collins, 2009), language modeling (Chelba et al., 1997; Zhang et al., 2016) and ontology construction (Snow et al., 2005). Dependency parsers represent syntactic information as a set of head-dependent relational arcs, typically constrained to form a tree. Practically all models proposed for dependency parsing in recent years can be described as graph-based (McDon1 Our code is available XingxingZhang/dense_parser. at http://github.com/ 665 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 665–676, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics hi"
E17-1063,C10-1011,0,\N,Missing
E17-1063,D14-1082,0,\N,Missing
E17-1083,P05-1074,0,0.835722,"with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or parallel corpus), the underlying representation (surface form or syntax trees), and the acquisition method itself. For an overview of these issues we refer the interested reader to Madnani and Dorr (2010). We focus on bilingual pivoting methods and aspects of neural machine translation pertaining to our model. We also discuss related work on paraphrastic embeddings. Bilingual Pivoting Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005). Their method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Paraphrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free"
E17-1083,P14-1133,0,0.0609363,"wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content (Madnani and Dorr, 2010). Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion (Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. (2003)). The intuition behind In the remainder of the paper, we introduce our 881 Proceedings of the 15th Conference of the Eur"
E17-1083,D16-1026,0,0.261996,"quence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2014) is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a), we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or parallel corpus), the un"
E17-1083,ganitkevitch-callison-burch-2014-multilingual,0,0.0321136,"f Edinburgh 10 Crichton Street, Edinburgh EH8 9AB J.Mallinson@ed.ac.uk, {rsennric,mlap}@inf.ed.ac.uk Abstract Bannard and Callison-Burch’s (2005) bilingual pivoting method is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. The method then pivots over f to extract he1 , e2 i as a pair of paraphrases. Drawing inspiration from syntaxbased SMT, several subsequent efforts (CallisonBurch, 2008; Ganitkevitch et al., 2011) extended this technique to syntactic paraphrases leading to the creation of PPDB (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014), a largescale paraphrase database containing over a billion of paraphrase pairs in 23 different languages. Recognizing and generating paraphrases is an important component in many natural language processing applications. A wellestablished technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by “pivoting” over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our mode"
E17-1083,N06-1003,0,0.0207028,"Missing"
E17-1083,D11-1108,0,0.0423867,"Missing"
E17-1083,D08-1021,0,0.0310719,"hrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to"
E17-1083,N13-1092,0,0.44047,"Missing"
E17-1083,P15-1166,0,0.00840479,"reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2014) is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a), we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or para"
E17-1083,P11-1105,0,0.0088428,"onal resources or parameter estimation. It also learns phrase and sentence embeddings for free without any model adjustments or recourse to resources like PPDB. 4 4.2 Throughout our experiments we compare PARA N ET against a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT), which we henceforth refer to as PARA S TAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems (Durrani et al., 2011). SMT translation models were implemented using both GIZA++ (Och and Ney, 2003) and MOSES (Koehn et al., 2007) and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model (Heafield, 2011), trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table 1. Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): Experiments We evaluated PARA N ET in several wa"
E17-1083,S12-1086,0,0.0277751,"task: a random baseline, a logistic regression baseline with minimal Model fr de cz de, fr de, cz fr, cz fr, cz, de random WTMF logistic reg ASOBEK MITRE PARA S TAT PARA N ET 0.540 0.569 0.543 0.571 0.547 0.569 0.543 0.569 0.540 0.570 0.546 0.568 0.539 0.568 0.017 0.350 0.511 0.475 0.619 Table 4: Semantic similarity results (Pearson) on the PIT-2015 data set. Boldface indicates the best performing paraphrasing model. n-gram word overlap features; and a model which uses weighted matrix factorization (WTMF) and has access to dictionary definitions provided in WordNet, OntoNotes, and Wiktionary (Guo and Diab, 2012). The last two rows show the highest scoring systems: ASOBEK (Eyecioglu and Keller, 2015) ranked 1st in the identification subtask and MITRE (Zarrella et al., 2015) in the similarity subtask. Whereas ASOBEK uses knowledge-lean features based on word and character n-gram overlap, MITRE is a combination of multiple systems including mixtures of string matching metrics, alignments using tweet-specific word representations, and recurrent neural networks. As can be seen, PARA N ET achieves better similarity and detection score than all baselines and PARA S TAT, for any combinations of lan887 Model"
E17-1083,N13-1073,0,0.0153296,"iple sentences from one language, but also over multiple sentences from multiple languages. Multi-lingual pivoting has been recently shown to improve translation quality (Firat et al., 2016b), especially for low-resource language pairs. Here, we hypothesize that it will also lead to more accurate paraphrases. Multi-lingual pivoting requires a small extension to late-weighted combination. We illustrate with German as a second language. First, 884 Two men sailing in couple sailing in a a tiny vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign (Dyer et al., 2013). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koe"
E17-1083,W11-2123,0,0.0135758,"tistical Machine Translation system (SMT), which we henceforth refer to as PARA S TAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems (Durrani et al., 2011). SMT translation models were implemented using both GIZA++ (Och and Ney, 2003) and MOSES (Koehn et al., 2007) and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model (Heafield, 2011), trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table 1. Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): Experiments We evaluated PARA N ET in several ways: (a) we examined whether the paraphrases learned by our model correlate with human judgments of paraphrase quality; (b) we assessed PARA N ET in paraphrase and similarity detection tasks; and (c) in a sentence-level paraphrase generation task. We first present deta"
E17-1083,S15-2011,0,0.0112022,"cz de, fr de, cz fr, cz fr, cz, de random WTMF logistic reg ASOBEK MITRE PARA S TAT PARA N ET 0.540 0.569 0.543 0.571 0.547 0.569 0.543 0.569 0.540 0.570 0.546 0.568 0.539 0.568 0.017 0.350 0.511 0.475 0.619 Table 4: Semantic similarity results (Pearson) on the PIT-2015 data set. Boldface indicates the best performing paraphrasing model. n-gram word overlap features; and a model which uses weighted matrix factorization (WTMF) and has access to dictionary definitions provided in WordNet, OntoNotes, and Wiktionary (Guo and Diab, 2012). The last two rows show the highest scoring systems: ASOBEK (Eyecioglu and Keller, 2015) ranked 1st in the identification subtask and MITRE (Zarrella et al., 2015) in the similarity subtask. Whereas ASOBEK uses knowledge-lean features based on word and character n-gram overlap, MITRE is a combination of multiple systems including mixtures of string matching metrics, alignments using tweet-specific word representations, and recurrent neural networks. As can be seen, PARA N ET achieves better similarity and detection score than all baselines and PARA S TAT, for any combinations of lan887 Model fr de cz de, fr de, cz fr, cz fr, cz, de Tokencos DLS@CU PARA S TAT PARA N ET 0.657 0.682"
E17-1083,P15-1001,0,0.173292,"ined with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space representations for phrases and sentences (aka embeddings) that can be usefully incorporated in downstream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and gene"
E17-1083,P13-1158,0,0.0466156,"a) the Multiple-Translation Chinese (MTC) corpus (Huang et al., 2002) contains news stories from three sources of journalistic Mandarin Chinese text translated into English by 4 translation agencies; we sampled 1,000 sentences for training and testing, respectively (each source sentence had an average of 4 paraphrases); (b) the Jules Vernes Twenty Thousand Leagues Under the Sea novel (Leagues) corpus (Pang et al., 2003) contains two English translations of the French novel; we sampled 500 sentences for training/testing (each source sentence had one paraphrase); and (c) the Wikianswers corpus (Fader et al., 2013) which contains questions taken from the website3 wiki answers; we sampled 1,000 questions for training/testing (each question has on average 21 paraphrases). Semantic Textual Similarity In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. We present results on the Semeval-2015 English subtask which contains sentences from a wide range of domains, including newswire headlines, image descriptions, and answers from Q&A websites. The training/test sets consist of 11,250 and 3,000 sentence pairs, respectively. Sentence pairs are rated on"
E17-1083,W15-3014,0,0.089871,"ined with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space representations for phrases and sentences (aka embeddings) that can be usefully incorporated in downstream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and gene"
E17-1083,W07-0716,0,0.0285947,"gual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Paraphrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases"
E17-1083,D13-1176,0,0.0421331,"present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, or generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods"
E17-1083,J03-1002,0,0.0384719,"ngs for free without any model adjustments or recourse to resources like PPDB. 4 4.2 Throughout our experiments we compare PARA N ET against a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT), which we henceforth refer to as PARA S TAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems (Durrani et al., 2011). SMT translation models were implemented using both GIZA++ (Och and Ney, 2003) and MOSES (Koehn et al., 2007) and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model (Heafield, 2011), trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table 1. Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): Experiments We evaluated PARA N ET in several ways: (a) we examined whether the paraphrases learned by our model correlate with"
E17-1083,N03-1024,0,0.546875,"Missing"
E17-1083,N03-1017,0,0.0605213,"(Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. (2003)). The intuition behind In the remainder of the paper, we introduce our 881 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 881–893, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics paraphrase model and experimentally compare it to the phrase-based pivoting approach. We evaluate the model’s paraphrasing capability both intrinsically in a paraphrase detection task (i.e., decide the degree of semantic similarity between two sentences) and extrinsically in a generation task."
E17-1083,P15-2070,0,0.00975573,"Missing"
E17-1083,P07-2045,0,0.0111244,"13). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores for each NMT system can be seen in Table 1. small boat sail boat Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: TF F j F,E1  α(E2i , E1 , F )=∑ P(E2 |E1 , F)·∑(αEi,m2 ,F·αm, j ) (8) F m An example shown in Figure 2 where attention has successfully identified the semantically equivalent parts of two sentences. Beyond providing interpretable paraphrasing, attention scores can be used as features in both generation and classification tasks. Furth"
E17-1083,2005.mtsummit-papers.11,0,0.0723828,"N ET probabilities (Equation (7)) assigned to paraphrase pairs and human judgments. Figure 3 shows correlation coefficients for all language pairs using a single foreign pivot and 200 pivots. Across all language combinations multiple pivots2 achieve better correlations, with the German, Czech pair performing best with ρ = 0.53. For comparison, Pavlick et al. (2015) report a correlation of ρ = 0.41 using Equation (9) and PPDB (Ganitkevitch et al., 2013). The latter contains over 100 million paraphrases and was constructed over several English-to-foreign parallel corpora including Europarl v7 (Koehn, 2005) which contains bitexts for the 19 European languages. Following Pavlick et al. (2015), we next developed a supervised scoring model. Specifically, we fit a decision tree regressor on the PPDB 2.0 dataset using the implementation provided in scikit-learn (Pedregosa et al., 2011). To improve accuracy and control over-fitting we built an ensemble of regression trees using the ExtraTrees algorithm (Geurts et al., 2006) which fits a number of randomized decision trees (a.k.a. extratrees) on various sub-samples of the dataset. In our experiments 1,000 trees were trained to minimize mean square erro"
E17-1083,P04-1077,0,0.0339287,"his to the noisy nature of these two datasets which contain a wealth of paraphrases, a few of which are ungrammatical, contain typos or abbreviations leading to low scores among humans. sentence and rewards those close to the target: iBLEU(s, rs , c) = αBLEU(c, rs ) − (1 − α)BLEU(c, s) where s, is the source sentence, rs , is the target and c is the candidate paraphrase. (1 − α)BLEU(c, s), measures the originality of the candidate paraphrase, BLEU(c, rs ) measures semantic adequacy, and α is a tuning parameter which balances the two. Sentence level BLEU is calculated using plus one smoothing (Lin and Och, 2004). PARA N ET relies on a relatively simple architecture which is trained end-to-end with the objective of maximizing the likelihood of the training data. Since evaluation metrics cannot be straightforwardly integrated into this training procedure, we reranked the k-best paraphrases obtained from PARA N ET using a simple classifier which favors sentences which are dissimilar to the source. Specifically, we trained a decision tree regression model with iBLEU as the target variable using the same features described in Section 4.4. Examples of paraphrases generated by PARA N ET are shown in the App"
E17-1083,P07-1059,0,0.0328543,"generate target paraphrases for a given source input. Due to the architecture of NMT, generation takes advantage of wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content (Madnani and Dorr, 2010). Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion (Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. (2003))."
E17-1083,P16-1009,1,0.169343,"ailing in a a tiny vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign (Dyer et al., 2013). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores for each NMT system can be seen in Table 1. small boat sail boat Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: TF F j F,E1  α(E2i , E1 , F )=∑ P(E2 |E1 , F)·∑(αEi,m2 ,F·αm, j ) (8) F m An example shown in Figure 2 where attention has successfully identified the semantically equivalent parts of two sentences."
E17-1083,D15-1166,0,0.0652313,"ents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, or generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space rep"
E17-1083,P16-1162,1,0.0395329,"ailing in a a tiny vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign (Dyer et al., 2013). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores for each NMT system can be seen in Table 1. small boat sail boat Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: TF F j F,E1  α(E2i , E1 , F )=∑ P(E2 |E1 , F)·∑(αEi,m2 ,F·αm, j ) (8) F m An example shown in Figure 2 where attention has successfully identified the semantically equivalent parts of two sentences."
E17-1083,J10-3003,0,0.158386,"tream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and generate target paraphrases for a given source input. Due to the architecture of NMT, generation takes advantage of wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content (Madnani and Dorr, 2010). Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion (Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the mos"
E17-1083,D13-1170,0,0.00125829,"and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to Socher et al. (2013). In follow-up work (Wieting et al., 2016), they learn sentence embeddings based on supervision provided by PPDB. In our approach, embeddings are learned as part of the model and are available for any-length segments making use of no additional machinery beyond NMT itself. 3 Neural Paraphrasing In this section we present PARA N ET, our Paraphrasing model based on Neural Machine Translation. PARA N ET uses neural machine translation to first translate from English to a foreign pivot, which is then back-translated to English, producing a paraphrase. In the following, we briefly overview the basi"
E17-1083,S14-2039,0,0.0144247,"Missing"
E17-1083,N16-1004,0,0.0165551,"networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2014) is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a), we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g.,"
E17-1083,P08-1089,0,0.0599002,"araphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to Socher et al. (2013). In follow-up work (Wi"
E17-1083,P12-2008,0,0.0669534,"three languages as pivots, and compared PARA N ET and PARA S TAT directly. Our results are summarized in Table 5. The third block in the table presents a In order to select the best paraphrase candidate for a given input sentence, PARA S TAT was optimized on the training set using Minimum Error Training (MERT, Och and Ney (2003)). MERT integrates automatic evaluation metrics such as BLEU into the training process to achieve optimal end-to-end performance. Naively optimizing for BLEU, however, will result in a trivial paraphrasing system heavily biased towards producing identity “paraphrases”. Sun and Zhou (2012) introduce iBLEU which we also adopt. iBLEU penalizes paraphrases which are similar to the source 3 http://wiki.answers.com/ 888 Model fr de cz Gold PARA S TAT PARA N ET 0.280 0.299 0.282 0.295 0.280 0.291 0.599 Model Wikianswers Leagues MTC All PARA S TAT 2.09 2.38 2.23 2.26 PARA N ET 1.86 1.94 1.70 1.83 Humans 2.17 1.81 2.0 2.0 Table 7: Mean Rankings given to paraphrases by human participants (a lower score is better). Table 6: Mean iBLEU across three datasets. datasets. For the sake of brevity, we only show results with one pivot language since combinations performed slightly worse for both"
E17-1083,N07-1061,0,0.0145047,"ces from the same language, F1 and F2 . Each translation path individ(4) Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is not a translation path from the source language to the target. Instead, pivoting takes advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in traditional phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) 883 s0 s1 s2 s3 s4 Le il temps fait est grande superbe EOL EOL s0 s1 s2 s3 s5 s6 s7 s8 s9 The weather is great EOL s4 s5 s6 s7 s8 Figure 1: Late-weighted combination: two pivot sentences are simultaneously translated to one target sentence. Blue circles indicate the encoders, which individually encode the two source sentences. After the EOL token is seen, decoding starts (red circles). At each time step the two decoders produce a probability distribution over all words, which are then combined (in the yellow square) using Equation (6). From this combined distribution a word is chosen, which i"
E17-1083,Q15-1025,0,0.0133408,"kevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to Socher et al. (2013). In follow-up work (Wieting et al., 2016), they learn sentence embeddings based on supervision provided by PPDB. In our approach, embeddings are learned as part of the model and are available for any-length segments making use of no additional machinery beyond NMT itself. 3 Neural Paraphrasing In this section we present PARA N ET, our Paraphrasing model based on Neural Machine Translation. PARA N ET uses neural machine translation to first transla"
E17-1083,P07-1108,0,0.0205991,"of two pivot sentences from the same language, F1 and F2 . Each translation path individ(4) Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is not a translation path from the source language to the target. Instead, pivoting takes advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in traditional phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) 883 s0 s1 s2 s3 s4 Le il temps fait est grande superbe EOL EOL s0 s1 s2 s3 s5 s6 s7 s8 s9 The weather is great EOL s4 s5 s6 s7 s8 Figure 1: Late-weighted combination: two pivot sentences are simultaneously translated to one target sentence. Blue circles indicate the encoders, which individually encode the two source sentences. After the EOL token is seen, decoding starts (red circles). At each time step the two decoders produce a probability distribution over all words, which are then combined (in the yellow square) using Equation (6). From this combined distributi"
E17-1083,Q14-1034,0,0.0181394,"e 3: Paraphrase detection results (F1) on the PIT-2015 data set. Boldface indicates the best performing paraphrasing model. Paraphrase Identification and Similarity The SemEval-2015 shared task on Paraphrase and Semantic Similarity In Twitter (PIT) uses a training and development set of 17,790 sentence pairs and a test set of 972 sentence pairs. By design, the dataset contains colloquial sentences representing informal language usage and sentence pairs which are lexically similar but semantically dissimilar. Sentence pairs were crawled from Twitter’s trending topics and associated tweets (see Xu et al. (2014) for details). The shared task consists of a (binary) paraphrase identification subtask (i.e., determine whether two sentences are paraphrases) and an optional semantic similarity task (i.e., determine the similarity between two sentences on a scale of 1–5, where 5 means completely equivalent and 1 not equivalent). We trained a decision tree regressor on the PIT-2015 similarity dataset using the features described above. Once trained, the decision tree regressor can be readily applied to the semantic similarity subtask. For the paraphrase detection subtask, we use the same model and apply a th"
E17-1083,S15-2002,0,0.031926,"Missing"
E17-1083,N16-1101,0,\N,Missing
H05-1033,W04-2504,0,0.0517817,"Missing"
H05-1033,A00-2018,0,0.0349467,"Missing"
H05-1033,W97-1311,0,0.0605045,"Missing"
H05-1033,J02-1002,0,0.0174422,"l humans agree on discourse chunk segmentation and labelling in order to establish an upper bound for the task. We measured both unlabelled and labelled agreement on the 52 doubly annotated RST-DT texts. The former measures whether humans agree in placing chunk boundaries, whereas the latter additionally measures whether humans agree in assigning chunk labels. To facilitate comparison with our models we report inter-annotator agreement in terms of accuracy and F-score.3 For the unlabelled case we also report Window Difference (WDiff), a commonly used evaluation measure for segmentation tasks (Pevzner and Hearst, 2002). It returns values between 0 (identical segmentations) and 1 (maximally different segmentations) and differs from accuracy in that predicted boundaries which are only slightly off are penalised less than those which are completely wrong. Human agreement is relatively high4 on both segmentation and span labelling (see Table 1), which can be explained by the fact that (i) the RST-DT annotators were given very detailed and precise instructions and (ii) assigning boundaries and labels is an easier task than creating full-scale discourse trees. 4.2 One-Step Chunking For the one-step chunking metho"
H05-1033,W95-0107,0,0.0105372,"t plunged. and determine that the first of these functions as a nucleus at the lowest level of the tree whereas the latter two function as satellites. We do not try to determine that the first two edus are merged at a higher level and then function as the overall nucleus of the sentence. The discourse chunking task assumes a nonhierarchical representation. We converted each sentence-level discourse tree into a flat chunk representation by assigning each token (i.e., word or punctuation mark) a tag encoding its nuclearity status at the edu level. We adopted the chunk representation proposed by Ramshaw and Marcus (1995) and used four different tags: B - NUC and B - SAT for nucleus and satellite-initial tokens, and I - NUC and I - SAT for non-initial tokens, i.e., tokens inside a nucleus and satellite span. As all tokens belong either to a nucleus or a satellite span, we do not need a special tag (typically denoted by O in syntactic chunking) to indicate elements outside a chunk. The chunk representation for the sentence in Figure 1 is thus: “/B - NUC I/I - NUC am/I - NUC optimistic/I - NUC ”/I - NUC said/B - SAT Mr./I - SAT Smith/I - SAT as/B - SAT the/I - SAT market/I - SAT plunged/I SAT ./ I - SAT Discours"
H05-1033,N03-1026,0,0.0133272,"nlabelled Labelled 95 F-score 90 85 80 75 70 65 60 0 472 949 1,428 1,8872,350 2,823 3,290 3,8524,258 4,734 Number of sentences in training data Figure 2: Learning curve for discourse segmentation (unlabelled) and span labelling (labelled) 4.4 Sentence Compression Sentence compression can be likened to summarisation at the sentence level. The task has an immediate impact on several applications ranging from summarisation to audio scanning devices for the blind and caption generation (see Knight and Marcu, 2002 and the references therein). Previous datadriven approaches (Knight and Marcu, 2003; Riezler et al., 2003) relied on parallel corpora to determine what is important in a sentence. The models learned correspondences between long sentences and their shorter counterparts, typically employing a rich feature space induced from parse trees. The task is challenging since the compressed sentences should retain essential information and convey it grammatically. Here, we propose a complementary approach which utilises discourse chunking. A compressed sentence can be obtained from the output of the chunker simply by removing satellites. We thus capitalise on RST’s (Mann and Thompson, 1987) notion of nucleari"
H05-1033,N03-1030,0,0.865213,"segments) are linked to each other by rhetorical relations (e.g., Contrast, Elaboration). Discourse units are further characterised in terms of their text importance: nuclei denote central segments, whereas satellites denote peripheral ones. Recent advances in discourse modelling have greatly benefited from the availability of resources annotated with discourse-level information such as the RST Discourse Treebank (RST-DT, Carlson et al., 2002). Even though discourse parsing at the document-level still poses a significant challenge to data-driven methods, sentence-level discourse models (e.g., Soricut and Marcu, 2003) trained on the RST-DT have attained accuracies comparable to human performance. The availability of discourse annotations is partly responsible for the success of these models. Another important reason is the development of robust syntactic parsers (e.g., Charniak, 2000) that can be used to provide critical structural and lexical information to the discourse parser. Unfortunately, discourse annotated corpora are largely absent for languages other than English. Furthermore, reliance on syntactic parsing renders discourse parsing practically impossible for languages for which state-of-the-art p"
H05-1033,J02-4002,0,0.0684054,"Missing"
H05-1033,W00-0733,0,0.0172664,"n be achieved by incorporating additional features into the labeller, such as the number of chunks in the sentence or the length of the current chunk. A two-step approach also avoids the creation of illegal chunk sequences, such as “B - SAT I - NUC”. However, a potential drawback is that the number of training examples for the labeller is reduced as the instances to be classified are chunks rather than tokens. We explore the performance of the one-step and the two-step methods in Sections 4.2 and 4.3, respectively. 1 A similar approach has been proposed for syntactic chunking, e.g., Tjong Kim Sang (2000). 259 A variety of learning schemes can be employed for the discourse chunking task. We have experimented with Boosting (Schapire and Singer, 2000), Conditional Random Fields (Lafferty et al., 2001), and Support Vector Machines (Vapnik, 1998). Discussion of our results focuses exclusively on boosting, since it had a slight advantage over the other methods. Boosting combines many simple, moderately accurate categorisation rules into a single, highly accurate rule. We used BoosTexter’s (Schapire and Singer, 2000) implementation, which combines boosting with simple decision rules. The system perm"
H05-1033,A00-2024,0,\N,Missing
H05-1042,W03-1016,0,0.0543385,"of content selection components developed for various domains (Kukich, 1983; McKeown, 1985; Sripada et al., 2001; Reiter and Dale, 2000). A common theme across different approaches is the emphasis on coherence: related information is selected “to produce a text that hangs together” (McKeown, 1985). Similarly, our method is also guided by coherence constraints. In our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing a large number of texts from a domain-relevant corpus and careful hand-crafting of content selection rules. Duboue and McKeown (2003) were the first to propose a method for learning content selection rules automatically, thus going beyond mere corpus analysis. They treat content selection as a classification task. Given a collection of texts associated with a domain-specific database, their model learns whether a database entry should be selected for presentation or not. Their modeling approach uses an expressive feature space while considering database entries in isolation. Similarly to Duboue and McKeown (2003), we view content selection as a classification task and learn selection rules from a database and its correspond"
H05-1042,P83-1022,0,0.443446,"s of collective content selection on this complex domain. Furthermore, our results empirically confirm the contribution of discourse constraints for content selection. In the following section, we provide an overview of existing work on content selection. Then, we define the learning task and introduce our approach for collective content selection. Next, we present our experimental framework and data. We conclude the paper by presenting and discussing our results. 2 Related Work The generation literature provides multiple examples of content selection components developed for various domains (Kukich, 1983; McKeown, 1985; Sripada et al., 2001; Reiter and Dale, 2000). A common theme across different approaches is the emphasis on coherence: related information is selected “to produce a text that hangs together” (McKeown, 1985). Similarly, our method is also guided by coherence constraints. In our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing a large number of texts from a domain-relevant corpus and careful hand-crafting of content selection rules. Duboue and McKeown (2003) were the first to propose a method for learning c"
H05-1042,P04-1035,0,0.00367601,"this expression capture the penalty for assigning entities to classes against their individual preferences. For instance, the penalty for selecting an entry x ∈ C+ will equal ind− (x), i.e., x’s individual preference of being ommitted. The third term captures a linking penalty for all pairs of entities (xi , x j ) that are connected by a link of type L, and are assigned to different classes. This formulation is similar to the energy minimization framework, which is commonly used in image analysis (Besag, 1986; Boykov et al., 1999) and has been recently applied in natural language processing (Pang and Lee, 2004). The principal advantages of this formulation lie in its computational properties. Despite seeming intractable — the number of possible subsets to consider for selection is exponential in the number of database entities — the inference problem has an exact solution. Provided that the scores ind+ (x), ind− (x), and linkL (x, y) are 334 positive, we can find a globally optimal label assignment in polynomial time by computing a minimal cut partition in an appropriately constructed graph (Greig et al., 1989). In the following we first discuss how individual preference scores are estimated. Next,"
H05-1042,P98-2209,0,0.507487,"Missing"
H05-1042,W01-0802,0,\N,Missing
H05-1042,C98-2204,0,\N,Missing
H05-1108,boas-2002-bilingual,0,0.0592407,"he house. abandon.v, desert.v, depart.v, departure.n, emerge.v, emigrate.v, emigration.n, escape.v, escape.n, leave.v, quit.v, retreat.v, retreat.n, split.v, withdraw.v, withdrawal.n Table 1: Example of FrameNet frame Introduction Shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications. For instance, information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. The FrameNet project (Fillmore et al., 2003) has played a central role in this endeavour by providing a large lexical resource based on semantic roles. In FrameNet, meaning is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the possible syntactic realisations of their semantic roles, and provides annotated examples"
H05-1108,P97-1003,0,0.0949043,"Missing"
H05-1108,W03-1005,0,0.0155244,"eflect genuine structural differences between translated sentences. Consider the following (simplified) example for the S TATEMENT frame (introduced by say) and its semantic role S TATEMENT (introduced by we): (10) We claim X and we say Y Wir behaupten X und — sagen Y The word alignment correctly aligns the German pronoun wir with the first English we and leaves the second occurrence unaligned. Since there is no corresponding German word for the second we, projection of the S PEAKER role fails. In future work, this problem could be handled with explicit identification of empty categories (see Dienes and Dubey, 2003). 6 Conclusions In this paper, we argue that parallel corpora show promise in relieving the lexical acquisition bottleneck for low density languages. We proposed semantic projection as a means of obtaining FrameNet annotations automatically without additional human effort. We examined semantic parallelism, a prerequisite for accurate projection, and showed that semantic roles can be successfully projected for predicate pairs with matching frame assignments. Similarly to previous work (Hwa et al., 2002), we find that some mileage can be gained by assuming direct correspondence between two langu"
H05-1108,P03-1068,1,0.411785,"d, which arguably cannot have a role set identical to finish. Relying solely on the English FrameNet database for sampling would yield many sentence pairs which are either inappropriate for the present study (because they do not evoke the same frames) or simply problematic for annotation since they are outside the 2 See html. http://www.keenage.com/zhiwang/e_zhiwang. present coverage of the database. For the above reasons, our sample selection procedure was informed by two existing resources, the English FrameNet and SALSA, a FrameNetcompatible database for German currently under development (Erk et al., 2003). We first used the publicly available GIZA++ (Och and Ney, 2003) software to induce English-German word alignments. Next, we gathered all German-English sentences in the corpus that had at least one pair of aligned words (we , wg ), which were listed in FrameNet and SALSA, respectively, and had at least one frame in common. These sentences exemplify 83 frame types, 696 lemma pairs, and 265 unique English and 178 unique German lemmas. Sentence pairs were grouped into three bands according to their frame frequency (High, Medium, Low). We randomly selected 380 pairs from each band. The total sam"
H05-1108,C04-1134,0,0.383294,"English and Chinese. Their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to train a dependency parser. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. Previous work has primarily focused on the projection of morphological and grammatico-syntactic information. Inducing semantic resources from low density languages still poses a significant challenge to data-driven methods. The challenge is recognised by Fung and Chen (2004) who construct a Chinese FrameNet by mapping English FrameNet entries to 860 concepts listed in HowNet2 , an on-line ontology for Chinese, however without exploiting parallel texts. The present work extends previous approaches on annotation projection by inducing FrameNet semantic roles from parallel corpora. Analogously to Hwa et al. (2002), we investigate whether there are indeed semantic correspondences between two languages, since there is little hope for projecting meaningful annotations in nonparallel semantic structures. Similarly to Fung and Chen (2004) we automatically induce semantic"
H05-1108,J02-3001,0,0.0124551,"ing is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the possible syntactic realisations of their semantic roles, and provides annotated examples from the British National Corpus (Burnard, 1995). The availability of rich annotations for the surface realisation of semantic roles has triggered interest in semantic parsing and enabled the development of data-driven models (e.g., Gildea and Jurafsky, 2002). Table 1 illustrates an example from the FrameNet database, the D EPARTING frame. It has two roles, a T HEME which is the moving object and a S OURCE expressing the initial position of the T HEME. The frame elements are realised by different syntactic expressions. For instance, the T HEME is typically an NP, whereas the S OURCE is often expressed by a prepositional phrase (see the expressions in boldface in Table 1). The D EPARTING frame can be evoked by abandon, desert, depart, and several other verbs as well as nouns (see the list of FEEs in Table 1). Although recent advances in semantic pa"
H05-1108,C04-1100,0,0.0318286,"w York. He retreated from his opponent. The woman left the house. abandon.v, desert.v, depart.v, departure.n, emerge.v, emigrate.v, emigration.n, escape.v, escape.n, leave.v, quit.v, retreat.v, retreat.n, split.v, withdraw.v, withdrawal.n Table 1: Example of FrameNet frame Introduction Shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications. For instance, information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. The FrameNet project (Fillmore et al., 2003) has played a central role in this endeavour by providing a large lexical resource based on semantic roles. In FrameNet, meaning is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the possible syntactic realisations of their semantic r"
H05-1108,J03-1002,0,0.00595843,"lying solely on the English FrameNet database for sampling would yield many sentence pairs which are either inappropriate for the present study (because they do not evoke the same frames) or simply problematic for annotation since they are outside the 2 See html. http://www.keenage.com/zhiwang/e_zhiwang. present coverage of the database. For the above reasons, our sample selection procedure was informed by two existing resources, the English FrameNet and SALSA, a FrameNetcompatible database for German currently under development (Erk et al., 2003). We first used the publicly available GIZA++ (Och and Ney, 2003) software to induce English-German word alignments. Next, we gathered all German-English sentences in the corpus that had at least one pair of aligned words (we , wg ), which were listed in FrameNet and SALSA, respectively, and had at least one frame in common. These sentences exemplify 83 frame types, 696 lemma pairs, and 265 unique English and 178 unique German lemmas. Sentence pairs were grouped into three bands according to their frame frequency (High, Medium, Low). We randomly selected 380 pairs from each band. The total sample consisted of ,140 sentence pairs. This procedure produces a r"
H05-1108,W04-3207,0,0.0100441,"r resource-rich languages like English are projected onto another language through aligned parallel texts. Yarowsky et al. (2001) propose several projection algorithms for deriving monolingual tools (ranging from part-ofspeech taggers, to chunkers and morphological analysers) without additional annotation cost. Hwa et al. (2002) assess the degree of syntactic parallelism in dependency relations between English and Chinese. Their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to train a dependency parser. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. Previous work has primarily focused on the projection of morphological and grammatico-syntactic information. Inducing semantic resources from low density languages still poses a significant challenge to data-driven methods. The challenge is recognised by Fung and Chen (2004) who construct a Chinese FrameNet by mapping English FrameNet entries to 860 concepts listed in HowNet2 , an on-line ontology for Chinese, however without exploiting parallel texts. The present wor"
H05-1108,P03-1002,0,0.0210233,"ure was delayed. S OURCE We departed from New York. He retreated from his opponent. The woman left the house. abandon.v, desert.v, depart.v, departure.n, emerge.v, emigrate.v, emigration.n, escape.v, escape.n, leave.v, quit.v, retreat.v, retreat.n, split.v, withdraw.v, withdrawal.n Table 1: Example of FrameNet frame Introduction Shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications. For instance, information extraction (Surdeanu et al., 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. The FrameNet project (Fillmore et al., 2003) has played a central role in this endeavour by providing a large lexical resource based on semantic roles. In FrameNet, meaning is represented by frames, schematic representations of situations. Semantic roles are frame-specific, and are called frame elements. The database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or FEEs), lists the"
H05-1108,H01-1035,0,0.747038,"parallelism across languages. Then we propose two broad classes of projection models that utilise lexical and syntactic information (Section 4), and show experimentally that roles can be projected from English onto German with high accuracy (Section 5). We conclude the paper by discussing the implications of our results and future work (Section 6). 2 Related work A number of recent studies exploit parallel corpora for cross-linguistic knowledge induction. In this paradigm, annotations for resource-rich languages like English are projected onto another language through aligned parallel texts. Yarowsky et al. (2001) propose several projection algorithms for deriving monolingual tools (ranging from part-ofspeech taggers, to chunkers and morphological analysers) without additional annotation cost. Hwa et al. (2002) assess the degree of syntactic parallelism in dependency relations between English and Chinese. Their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to train a dependency parser. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a wor"
H05-1108,P02-1050,0,\N,Missing
I13-1058,N06-1052,0,0.363184,"methods relied on using a background language model, which is typically estimated based on the whole document collection (Ponte and Croft, 1998; Zhai and Lafferty, 2001b; Miller et al., 1999). In contrast to the simple strategy which smoothes all documents with the same background, recently corpus structures have been exploited for more accurate smoothing. The basic idea is to smooth a document language model with the documents similar to the document under consideration through clustering (Liu and Croft, 2004; Xu and Croft, 1999; Mei et al., 2008), document expansion (Kurland and Lee, 2004; Tao et al., 2006), or relevance propagation (Kurland and Lee, 2010; Kurland and Lee, 2006; Qin et al., 2005). All these methods are based on documentlevel semantics similarity to offer “customized” smoothing for each individual document. Besides semantics, positional heuristics for retrieval have been examined in (Keen, 1992; Tao and Zhai, 2007; Liu and Croft, 2002; B¨uttcher et al., 2006). Positional language models are proposed to examine the positional proximity in (Lv and Zhai, 2009; Zhao and Yun, 2009). In their work, the key idea is to define a language model for each position within a document, and scor"
J03-3005,W99-0901,0,0.245912,"Missing"
J03-3005,W00-1702,0,0.0574941,"provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis http://www.lexisnexis.com/ might have databases that are even larger than the Web. Lexis Nexis provides"
J03-3005,A97-1052,0,0.0452224,"Missing"
J03-3005,W98-1505,0,0.0090278,"Missing"
J03-3005,N01-1013,0,0.0131765,"counts and frequencies re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing"
J03-3005,J02-2003,0,0.457246,"Missing"
J03-3005,P02-1030,0,0.0139535,"Missing"
J03-3005,C94-2119,0,0.0378288,"hing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance o"
J03-3005,J93-1005,0,0.473936,"Missing"
J03-3005,C92-3145,0,0.0511375,"Missing"
J03-3005,W02-1030,1,0.760093,"Missing"
J03-3005,N01-1009,0,0.0323944,"Missing"
J03-3005,J02-3004,0,0.00866337,"Missing"
J03-3005,P01-1046,1,0.796807,"Missing"
J03-3005,E99-1005,1,0.745437,"Missing"
J03-3005,P99-1004,0,0.509734,"heuristic methods; these include models for disambiguating the attachment site of prepositional phrases (Hindle and Rooth 1993; Ratnaparkhi 1998), models for interpreting compound nouns (Lauer 1995; Lapata 2002) and polysemous adjectives (Lapata 2001), models for the induction of selectional preferences (Abney and Light 1999), methods for automatically clustering words according to their distribution in particular syntactic contexts (Pereira, Tishby, and Lee 1993), automatic thesaurus extraction (Grefenstette 1994; Curran 2002), and similarity-based models of word co-occurrence probabilities (Lee 1999; Dagan, Lee, and Pereira 1999). In this article we investigate alternative ways for obtaining bigram frequencies that are potentially useful for such models despite the fact that some of these bigrams are identified in a heuristic manner and may be noisy. 2.2 Sampling Bigrams from the NANTC We also obtained corpus counts from a second corpus, the North American News Text Corpus (NANTC). This corpus differs in several important respects from the BNC. It is substantially larger, as it contains 350 million words of text. Also, it is not a balanced corpus, as it contains material from only one ge"
J03-3005,C94-1103,0,0.0133857,"Missing"
J03-3005,J98-2002,0,0.15736,"sing all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next secti"
J03-3005,C94-1079,0,0.0838201,"Missing"
J03-3005,H01-1046,0,0.0119651,"Missing"
J03-3005,W03-2606,0,0.0550276,"Missing"
J03-3005,A00-2034,0,0.00346172,"es re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing). The smoothing method discussed above is simply one type of class-based smoothing. Other, more sophisticated class-based methods do away with the simplifying assumption that the argument co-occurring with a given predicate (adjective, noun, verb) is distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create counts of unseen word combinations by exploiting only corpus-internal evidence, without relying on taxonomic information. Our goal was to demonstrate that frequencies retrieved from the Web are a viable alternative to conventional smoothing methods when data are sparse; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. Howev"
J03-3005,P99-1020,0,0.00472861,"retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information providers such as Lexis Nexis http://www.lexisnexis.com/ might have databases that are even larger tha"
J03-3005,P93-1024,0,0.767769,"Missing"
J03-3005,C00-2094,0,0.0143606,"Missing"
J03-3005,P98-2177,0,0.0176536,"Missing"
J03-3005,P99-1068,0,0.258611,"ble for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax. There is a small body of existing research that tries to harness the potential of the Web for NLP. Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts. Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, ∗ School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: keller@inf.ed.ac.uk † Department of Computer Science, 211 Portobello Street, Sheffield S1 4DP, UK. E-mail: mlap@dcs.shef.ac.uk 1 A reviewer points out that information p"
J03-3005,P99-1014,0,0.189169,"se; we do not claim that our Web-based method is necessarily superior to smoothing or that it should be generally preferred over smoothing methods. However, the next section will present a small-scale study that compares the performance of several smoothing techniques with the performance of Web counts on a standard task from the literature. 3.4 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999; Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al. 1999). 476 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the pseudodisambiguation task is to decide whether a given algorithm re-creates frequencies that make it possible to distinguish between seen and unseen bigrams in a given corpus. A set of pseudobigrams is constructed according to a set of criteria (detailed below) that ensure that they are unattested in the training corpus. Then the seen bigrams are removed from the training data, and the smoothing method is used to re-create the frequencies of both the seen bigrams and the pseudobigrams. The smoothing method is then evalua"
J03-3005,1999.tc-1.8,0,\N,Missing
J03-3005,H01-1052,0,\N,Missing
J03-3005,P01-1005,0,\N,Missing
J03-3005,C98-2172,0,\N,Missing
J04-1003,A97-1052,0,0.0536178,"ointed out by Kipper, Dang, and Palmer (2000), Levin classes exhibit inconsistencies, and verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames. This means that some ambiguities may also arise as a result of accidental errors or inconsistencies. The classification was created not with computational uses in mind, but for human readers, so it has not been necessary to remedy all the errors and omissions that might cause trouble for machines. Similar issues arise in almost all efforts to make use of preexisting lexical resources for computational purposes (Briscoe and Carroll 1997), so none of the above comments should be taken as criticisms of Levin’s achievement. The objective of this article is to show how to train and use a probabilistic version of Levin’s classification in verb sense disambiguation. We treat errors and inconsistencies in the classification as noise. Although all our tests have used Levin’s classes and the British National Corpus, the method itself depends neither on the details of Levin’s classification nor on parochial facts about the English language. Our future work will include tests on other languages, other classifications, and other corpora."
J04-1003,W98-1505,0,0.0283921,"Missing"
J04-1003,C00-1023,0,0.544541,"s has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more than one class. The precise meaning of this ambiguity is left open to interpretation in Levin, as it may indicate that the verb has more than one sense or that one sense (i.e., class) is primary and the alternations for this class should take precedence over the alternations"
J04-1003,C00-1028,0,0.426494,"rnations and verb semantic classes has recently influenced work in dictionary creation (Dorr 1997; Dang et al. 1998; Dorr and Jones 1996) and notably lexicon acquisition on the basis of the assumption that verbal meaning can be gleaned from corpora using cues pertaining to syntactic structure (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Previous work in word sense disambiguation has not tackled explicitly the ambiguity problems arising from Levin’s classification, although methods for deriving informative priors in an unsupervised manner have been proposed by Ciaramita and Johnson (2000) and Chao and Dyer (2000) within the context of noun and adjective sense disambiguation, respectively. In this section we review related work on classification and lexicon acquisition and compare it to our own work. 65 Computational Linguistics Volume 30, Number 1 Dang et al. (1998) observe that verbs in Levin’s (1993) database are listed in more than one class. The precise meaning of this ambiguity is left open to interpretation in Levin, as it may indicate that the verb has more than one sense or that one sense (i.e., class) is primary and the alternations for this class should take preceden"
J04-1003,W02-1005,0,0.0501738,"Missing"
J04-1003,C96-1055,0,0.0785818,"evin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic"
J04-1003,P92-1032,0,0.12757,"Missing"
J04-1003,J93-1005,0,0.0704721,"(e.g., Thameslink presently carries 20,000 passengers daily) is larger than the Carry class, it will be given a higher probability (.45 versus .4). Our estimation scheme is clearly a simplification, but it is an empirical question how much it matters. Tables 5 and 6 show the ten most frequent classes as estimated using (15) and (16). We explore the contribution of the two estimation schemes for P(c) in Experiments 1 and 2. The probabilities P(f |c) and P(f |v) will be unreliable when the frequencies F(f , v) and F(f , c) are small and will be undefined when the frequencies are zero. Following Hindle and Rooth (1993), we smooth the observed frequencies as shown in Table 7. F(f ,V) When F(f , v) is zero, the estimate used is proportional to the average F(V) across 54 Lapata and Brew Verb Class Disambiguation Using Informative Priors Table 7 Smoothed estimates. (a) (c) P(f |v) ≈ P(f |c) ≈ F(f , v) + F(f ,V) F(V) F(v) + 1 F(f , c) + Ff ,C) F(C) F(c) + 1 (b) F(f , V) =  F( f , vi ) i (d) F(C) =  F( f , ci ) i all verbs. Similarly, when F(f , c) is zero, our estimate is proportional to the average F(f ,C) F(C) across all classes. We do not claim that this scheme is perfect, but any deficiencies it may have a"
J04-1003,J98-1001,0,0.0339313,"Missing"
J04-1003,P98-1112,0,0.0727917,"lausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic constraints. More precisely, Levin (1993) provides an index of 3,024 verbs for which she lists the sem"
J04-1003,P99-1051,0,0.188935,"iteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for desc"
J04-1003,A00-2034,0,0.160933,"opt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal me"
J04-1003,J01-3003,0,0.595211,"aightforward syntactic and syntactico-semantic criteria. To adopt this approach is to accept some limitations on the reach of our analyses, since not all semantically interesting differences will have the appropriate reflexes in syntax. Nevertheless, the emphasis on concretely available observables makes Levin’s methodology a good candidate for automation (Palmer 2000). Therefore, Levin’s (1993) classification has formed the basis for many efforts that aim to acquire lexical semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification p"
J04-1003,W98-0703,0,0.0514272,"Missing"
J04-1003,W96-0208,0,0.0918429,"Missing"
J04-1003,W97-0323,0,0.0540735,"Missing"
J04-1003,A00-2009,0,0.050523,"Missing"
J04-1003,N01-1011,0,0.0131953,"ion experiments as it was represented solely by the verb kick (50 instances). In this study we compare a naive Bayesian classifier that relies on a uniform prior (see (20)) against two classifiers that make use of nonuniform prior models: The classifier in (22) effectively uses as prior the baseline model P(c) from Section 2, whereas the classifier in (23) relies on the more informative model P(c, f , v). As a baseline for the disambiguation task, we simply assign the most common class in the training data to every instance in the test data, ignoring context and any form of prior information (Pedersen 2001; Gale, Church, and Yarowsky 1992a). We also report an upper bound on disambiguation performance by measuring how well human judges agree with one another (percentage agreement) on the class assignment task. Recall from Section 4.1 that our corpus was annotated by two judges with Levin-compatible verb classes. 6.2 Results The results of our class disambiguation experiments are summarized in Figures 2–5. In order to investigate differences among different frames, we show how the naive Bayesian classifiers perform for each frame individually. Figures 2–5 (x-axis) also reveal the influence of col"
J04-1003,C00-2108,0,0.403214,"Missing"
J04-1003,J98-3003,0,0.0261116,"semantic information from corpora. These exploit syntactic cues, or at least cues that are plausibly related to syntax (Merlo and Stevenson 2001; Schulte im Walde 2000; Lapata 1999; McCarthy 2000). Other work has used Levin’s classification (in conjunction with other lexical resources) to create dictionaries that express the systematic correspondence between syntax and meaning (Dorr 1997; Dang, Rosenzweig, and Palmer 1997; Dorr and Jones 1996). Levin’s inventory of verbs and classes has been also useful for applications such as machine translation (Dorr 1997; Palmer and Wu 1995), generation (Stede 1998), information retrieval (Levow, Dorr, and Lin 2000), and document classification (Klavans and Kan 1998). Although the classification provides a general framework for describing verbal meaning, it says only which verb meanings are possible, staying silent on the relative likelihoods of the different meanings. The inventory captures systematic regularities in the meaning of words and phrases but falls short of providing a probabilistic model of these regularities. Such a model would be useful in applications that need to resolve ambiguity in the presence of multiple and conflicting probabilistic"
J04-1003,P94-1013,0,0.163515,"Missing"
J04-1003,P95-1026,0,0.104611,"Missing"
J04-1003,P00-1065,0,\N,Missing
J04-1003,J06-2001,0,\N,Missing
J04-1003,J99-4002,0,\N,Missing
J04-1003,P98-1046,0,\N,Missing
J04-1003,C98-1046,0,\N,Missing
J04-1003,C98-1108,0,\N,Missing
J04-1003,J98-1004,0,\N,Missing
J04-1003,1997.mtsummit-workshop.2,0,\N,Missing
J06-4002,P05-1018,1,0.713877,"1, N = 64). Figure 3 plots the relationship between judgments and τ values. To get a better understanding of how this automatic evaluation method compares with human judgments, we examined how well our raters agreed in their assessment. To calculate intersubject agreement we used leave-one-out resampling. The technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik and Diab 2000; Resnik 1999), adjective plausibility (Lapata and Lascarides 2003), and text coherence (Barzilay and Lapata 2005). The set of m subjects’ responses was divided into two sets: a set of size m − 1 (i.e., the response data of all but one subject) and a set of size one (i.e., the response data of a single subject). We then correlated the mean ratings of the former set with the ratings of the latter. This was repeated m times. Since we had 179 subjects, we performed Table 3 Average subject ratings for binned τ values and descriptive statistics. Bins Mean Min Max SD 1 2 3 4 5 6 7 8 5.348 4.916 4.927 4.470 4.382 4.208 4.028 3.966 3.000 1.000 2.000 1.000 1.000 1.000 1.000 1.000 7.000 7.000 7.000 7.000 7.000 7.00"
J06-4002,N04-1015,0,0.469741,"tomatic methods have concentrated on evaluation aspects concerning lexical choice (e.g., words or phrases shared between reference and system translations), content selection (e.g., document units shared between reference and system summaries), and grammaticality (e.g., how many insertions, substitutions, or deletions are required to transform a generated sentence to a reference string). Another promising, but, less studied, avenue for automatic evaluation is information ordering. The task concerns finding an acceptable ordering for a set of preselected information-bearing items (Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text generation, multidocument summarization, and other text synthesis problems. Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves. More concretely, Lapata (2003) proposed the use of Kendall’s τ, a measure"
J06-4002,E06-1032,0,0.0142888,"Missing"
J06-4002,2003.mtsummit-papers.9,0,0.0105753,"rated sentence against a reference corpus string. Despite differences in application and form, automatic evaluation methods usually involve the following desiderata. First, they measure numeric similarity or closeness of system output to one or several gold standards. Second, they are inexpensive, robust, and ideally language independent. Third, correlation with human judgments is an important part of creating and testing an automated metric. For instance, several studies have shown that B LEU correlates with human ratings on machine translation quality (Papineni et al. 2002; Doddington 2002; Coughlin 2003). Bangalore, Rambow, and Whittaker (2000) demonstrate that tree-based evaluation metrics for ∗ School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: mlap@inf.ed.ac.uk Submission received: 28 December 2005; accepted for publication: 6 May 2006. © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 surface generation correlate significantly with human judgments on sentence quality and understandability. Given their simplicity, automatic evaluation methods cannot be considered as a direct replacement for human ev"
J06-4002,W03-2304,0,0.0196744,"., how many insertions, substitutions, or deletions are required to transform a generated sentence to a reference string). Another promising, but, less studied, avenue for automatic evaluation is information ordering. The task concerns finding an acceptable ordering for a set of preselected information-bearing items (Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text generation, multidocument summarization, and other text synthesis problems. Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves. More concretely, Lapata (2003) proposed the use of Kendall’s τ, a measure of rank correlation, as a means of estimating the distance between a system-generated and a human-generated gold-standard order. Rank correlation is an appealing way of evaluating information ordering: It is a well-understood and widely used measure of the strength of association betwe"
J06-4002,W02-2112,0,0.0273798,"ranks. Assuming that a system can exhaustively generate all possible orders for a set of items (with a certain probability), they report the rank given to the reference order when all possible orders are sorted by their probability. The best possible rank is 0 and the worst rank is N! − 1. A system that gives a high rank to the reference order is considered worse than a system that gives it a low rank. However, not all systems are designed to exhaustively enumerate all possible permutations for a given document or have indeed a scoring mechanism that can rank alternative document renderings. Duboue and McKeown (2002) employ an alignment algorithm that allows them to compare the output of their algorithm with a gold-standard order. The alignment algorithm works by considering the similarity between system-generated and gold-standard facts. The similarity function is domain dependent (Duboue and McKeown [2002] generate postcardiac surgery medical briefings) and would presumably have to be redefined for a different set of facts in another domain. Kendall’s τ can be easily used to evaluate the output of automatic systems, irrespectively of the domain or application at hand. It requires no additional tuning an"
J06-4002,P01-1023,0,0.0281458,"lts concludes the article. 2. Kendall’s Measure In common with other automatic evaluation methods, we assume that we have access to a reference output that in most cases will be created by one or several humans. Our task is to compare a system-produced ordering of items against a reference order. For ease of exposition, let us assume that our information-ordering component is part of a generation application whose ultimate goal is to generate coherent and understandable text. It is not crucially important how the items to be ordered are represented. They can be facts in a database (Duboue and McKeown 2001), propositions (Karamanis 2003), discourse trees (Mellish et al. 1998), or sentences (Lapata 2003; Barzilay and Lee 2004). Now, we can think of the items as objects for which a ranking must be produced. Table 1 gives an example of a reference text containing 10 items (A–J) and the orders (i.e., rankings) produced by two hypothetical systems. We can then calculate how much the system orders differ from the reference order, the underlying assumption being that acceptable orders should be fairly similar to the reference. A number of metrics can be employed for this purpose, such as Spearman’s cor"
J06-4002,N03-1020,0,0.0615245,"(Teufel and van Halteren 2004; Nenkova 2005; Mani 2001; White and O’Connell 1994). The relatively high cost of producing human judgments, especially when evaluations must be performed quickly and frequently, has encouraged many researchers to seek ways of evaluating system output automatically. Papineni et al. (2002) proposed B LEU, a method for evaluating candidate translations by comparing them against reference translations (using n-gram co-occurrence overlap). Along the same lines, the content of a system summary can be assessed by measuring its similarity to one or more manual summaries (Hovy and Lin 2003). Bangalore, Rambow, and Whittaker (2000) introduce a variety of quantitative measures for evaluating the accuracy of an automatically generated sentence against a reference corpus string. Despite differences in application and form, automatic evaluation methods usually involve the following desiderata. First, they measure numeric similarity or closeness of system output to one or several gold standards. Second, they are inexpensive, robust, and ideally language independent. Third, correlation with human judgments is an important part of creating and testing an automated metric. For instance,"
J06-4002,W05-1621,0,0.404471,"lation, as a means of estimating the distance between a system-generated and a human-generated gold-standard order. Rank correlation is an appealing way of evaluating information ordering: It is a well-understood and widely used measure of the strength of association between two variables; it is computed straightforwardly and can operate over distinct linguistic units (e.g., sentences, trees, or propositions). Indeed, several studies have adopted Kendall’s τ as a performance measure for evaluating the output of information-ordering components both in the context of concept-to-text generation (Karamanis and Mellish 2005; Karamanis 2003) and summarization (Lapata 2003; Barzilay and Lee 2004; Okazaki, Matsuo, and Ishizuka 2004). Despite its growing popularity, no study to date has investigated whether Kendall’s τ correlates with human judgments on the information-ordering task. This is in marked contrast with other automatic evaluation methods that have been shown to correlate with human assessments. In this article, we aim to rectify this and undertake two studies that examine whether there is indeed a relationship between τ and behavioral data. We first briefly introduce Kendall’s τ and explain how it can be"
J06-4002,J03-2004,0,0.0178324,"rson correlation coefficient of r = 0.45 (p &lt; 0.01, N = 64). Figure 3 plots the relationship between judgments and τ values. To get a better understanding of how this automatic evaluation method compares with human judgments, we examined how well our raters agreed in their assessment. To calculate intersubject agreement we used leave-one-out resampling. The technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree on judging semantic similarity (Resnik and Diab 2000; Resnik 1999), adjective plausibility (Lapata and Lascarides 2003), and text coherence (Barzilay and Lapata 2005). The set of m subjects’ responses was divided into two sets: a set of size m − 1 (i.e., the response data of all but one subject) and a set of size one (i.e., the response data of a single subject). We then correlated the mean ratings of the former set with the ratings of the latter. This was repeated m times. Since we had 179 subjects, we performed Table 3 Average subject ratings for binned τ values and descriptive statistics. Bins Mean Min Max SD 1 2 3 4 5 6 7 8 5.348 4.916 4.927 4.470 4.382 4.208 4.028 3.966 3.000 1.000 2.000 1.000 1.000 1.000"
J06-4002,P03-1069,1,0.801254,"ker 2000). Automatic methods have concentrated on evaluation aspects concerning lexical choice (e.g., words or phrases shared between reference and system translations), content selection (e.g., document units shared between reference and system summaries), and grammaticality (e.g., how many insertions, substitutions, or deletions are required to transform a generated sentence to a reference string). Another promising, but, less studied, avenue for automatic evaluation is information ordering. The task concerns finding an acceptable ordering for a set of preselected information-bearing items (Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text generation, multidocument summarization, and other text synthesis problems. Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves. More concretely, Lapata (2003) proposed the use o"
J06-4002,W98-1411,0,0.421382,"are required to transform a generated sentence to a reference string). Another promising, but, less studied, avenue for automatic evaluation is information ordering. The task concerns finding an acceptable ordering for a set of preselected information-bearing items (Lapata 2003; Barzilay and Lee 2004). It is an essential step in concept-to-text generation, multidocument summarization, and other text synthesis problems. Depending on the application and domain at hand, the items to be ordered may vary greatly from propositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). It is therefore not surprising that evaluation methods have concentrated primarily on the generated orders, thus abstracting away from the items themselves. More concretely, Lapata (2003) proposed the use of Kendall’s τ, a measure of rank correlation, as a means of estimating the distance between a system-generated and a human-generated gold-standard order. Rank correlation is an appealing way of evaluating information ordering: It is a well-understood and widely used measure of the strength of association between two variables; it is compute"
J06-4002,P03-1021,0,0.0126313,"Missing"
J06-4002,C04-1108,0,0.0814561,"Missing"
J06-4002,P02-1040,0,0.0957464,"ement is called for in evaluating systems that generate textual output. Examples include text generation, summarization, and, notably, machine translation. Human evaluations consider many aspects of automatically generated texts ranging from grammaticality to content selection, fluency, and readability (Teufel and van Halteren 2004; Nenkova 2005; Mani 2001; White and O’Connell 1994). The relatively high cost of producing human judgments, especially when evaluations must be performed quickly and frequently, has encouraged many researchers to seek ways of evaluating system output automatically. Papineni et al. (2002) proposed B LEU, a method for evaluating candidate translations by comparing them against reference translations (using n-gram co-occurrence overlap). Along the same lines, the content of a system summary can be assessed by measuring its similarity to one or more manual summaries (Hovy and Lin 2003). Bangalore, Rambow, and Whittaker (2000) introduce a variety of quantitative measures for evaluating the accuracy of an automatically generated sentence against a reference corpus string. Despite differences in application and form, automatic evaluation methods usually involve the following desider"
J06-4002,J98-3005,0,0.0514861,"may be biased toward very good or very bad orders. This means that our hypothetical study would only examine a restricted and potentially skewed range of τ values. Furthermore, in conceptto-text generation applications, information ordering typically operates over symbolic representations that will be unfamiliar to naive informants and could potentially distort their judgments. A related issue arises in text-to-text generation applications where the produced documents are not necessarily grammatical, for example, when a summary is the output of an information fusion component (Barzilay 2003; Radev and McKeown 1998). Again, it is difficult to control whether informants judge the ordering or the grammaticality of the texts. To make the judgment task easier, we concentrated on a document representation familiar to our participants, namely, sentences. We simulated the output of an information-ordering component by randomly generating different sentence orders for a reference text. We elicited judgments for eight texts of the same length (eight sentences). The texts were randomly sampled from a corpus collected by Barzilay and Lee (2004) (sampling took place over eight-sentence-length documents only). The co"
J06-4002,W04-3254,0,0.012733,"Missing"
J06-4002,1994.amta-1.25,0,0.125776,"Missing"
J06-4002,W01-0100,0,\N,Missing
J06-4002,W02-2111,0,\N,Missing
J06-4002,W00-1401,0,\N,Missing
J07-2002,C96-1005,0,0.145569,"Missing"
J07-2002,briscoe-carroll-2002-robust,0,0.0259658,"pendencies like the ones in Figure 4 that will form the context over which the semantic space will be constructed. We base our discussion Figure 4 A dependency analysis of the sentence A lorry might carry sweet apples as parse tree (left) and set of head-relation-modifier triples (right). 167 Computational Linguistics Volume 33, Number 2 and experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin 1998a, 2001). However, there is nothing inherent in our formalization that restricts us to this particular parser. Any other parser with broadly similar dependency output (e.g., Briscoe and Carroll 2002) could serve our purposes. In the remainder of this section, we first give a non-technical description of our algorithm for the construction of semantic spaces. Then, we proceed to discuss each construction step (context selection, basis mapping, and quantification of co-occurrences) in more detail. Finally, we show how our framework subsumes existing models. Table 1 lists the notation we use in the rest of the article. 3.1 The Construction Algorithm Our algorithm for creating semantic space models is summarized in Figure 5. Central to the construction process is the notion of paths, namely se"
J07-2002,W01-0514,0,0.133553,"Missing"
J07-2002,P02-1030,0,0.248675,"research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in terms of syntactic relations. We introduce an algorithm for constructing semantic s"
J07-2002,J93-1003,0,0.077181,"we (2001) notes, raw counts are likely to give misleading results. This is due to the non-uniform distribution of words in corpora which will introduce a frequency bias so that words with similar frequency will be judged more similar than they actually are. It is therefore advisable to use a lexical association function A to factor out chance co-occurrences explicitly. Our definition allows an arbitrary choice of lexical association function (see Manning and Schütze [1999] for an overview). In our experiments, we follow Lowe and McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993). We can visualize the computation using a two-by-two contingency table whose four cells correspond to four events (Kilgarriff 2001): b ¬b t k m ¬t l n 173 Computational Linguistics Volume 33, Number 2 The top left cell records the frequency k with which t and b co-occur (i.e., k corresponds to raw frequency counts). The top right cell l records how many times b is attested with any word other than t, the bottom left cell m represents the frequency of any word other than b with t, and the bottom right cell n records the frequency of pairs involving neither b nor t. The function G2 : R4 → R is"
J07-2002,C02-1091,0,0.0386089,"picture. On the one hand, Grefenstette compared the performance of the two classes of models on the task of automatic thesaurus extraction and found that a syntactically enhanced model gave significantly better results over a simple word co-occurrence model. A replication of Grefenstette’s study with a more sophisticated parser (Curran and Moens 2002) revealed that additional syntactic information yields further improvements. On the other hand, attempts to generate more meaningful indexing terms for information retrieval (IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson et al. 2002) have been largely unsuccessful. Experimental results show minimal differences in retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999] for details). Impact on cognitive modeling. Despite their widespread use in NLP, syntax-based semantic spaces have attracted little attention in cognitive science and computational psycholinguistics. Wiemer-Hastings and Zipitria (2001) construct a semantic space similar to LSA, but enhanced with part-of-speech tags with the aim of modeling human raters in an intelligent tutoring context. Their results, however, show that the t"
J07-2002,O97-1002,0,0.157944,"Missing"
J07-2002,A97-1025,0,0.0323534,"e of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998). The popularity of vector-based models in both fields lies in their ability to represen"
J07-2002,W03-0208,0,0.013113,"Missing"
J07-2002,H05-1053,0,0.0758814,"Missing"
J07-2002,P99-1004,0,0.895987,"ore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in terms of syntactic relations. We introduce an algorithm for"
J07-2002,P98-2127,0,0.151471,"semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998). The popularity of vector-based models in both fields lies in their ability to represent word meaning simply by using distributional statistics. The"
J07-2002,P99-1041,0,0.0443637,"elations. Even in cases where many relations are used (Lin 1998a; Lin and Pantel 2001), only direct relations are taken into account, ignoring potentially important co-occurrence patterns between, for instance, the subject and the object of a verb, or between a verb and its non-local argument (e.g., in control structures). Comparison between model classes. Syntax-based vector space models have been used in NLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefenstette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocation discovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and Carroll 2003). Comparisons between word-based and syntax-based models on the same task are rare, and the effect of syntactic knowledge has not been rigorously investigated or quantified. The few studies on this topic reveal an inconclusive picture. On the one hand, Grefenstette compared the performance of the two classes of models on the task of automatic thesaurus extraction and found that a syntactically enhanced model gave significantly better results over a simple word co-occurrence model. A replication of Grefenstette’s study"
J07-2002,H01-1046,0,0.0154629,"Missing"
J07-2002,W03-1810,0,0.0876768,"Missing"
J07-2002,P04-1036,0,0.156238,"f models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz,"
J07-2002,P04-1003,0,0.105127,"uch as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer and Dumais 1997; Foltz, Kintsch, and Landauer 1998). The popularity of vector-based models in both fields lies in their ability to represent word meaning simply by using distributional statistics. The central assumption here is that the context surrounding a given word provides important information about its meaning (Harris 1968). The semantic properties of words are captured in a multi-dimensional space by vectors that are constructed from large bodies of text by observing the distributional patterns of co-occurrence with their neighboring words. Co-occurre"
J07-2002,J98-1004,0,0.830277,"malization for this class of models, which allows linguistic knowledge to guide the construction process. We evaluate our framework on a range of tasks relevant for cognitive science and natural language processing: semantic priming, synonymy detection, and word sense disambiguation. In all cases, our framework obtains results that are comparable or superior to the state of the art. 1. Introduction Vector space models of word co-occurrence have proved a useful framework for representing lexical meaning in a variety of natural language processing (NLP) tasks, such as word sense discrimination (Schütze 1998) and ranking (McCarthy et al. 2004), text segmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correction (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin 1998a), and notably, information retrieval (Salton, Wang, and Yang 1975). These models have also been popular in cognitive science and figure prominently in several studies simulating human behavior. Examples include similarity judgments (McDonald 2000), semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe and McDonald 2000; McDonald and Brew 2004), and text comprehension"
J07-2002,N03-1036,0,0.198133,"itive behavior in sentence priming tasks (Morris 1994). Furthermore, much research in lexical semantics hypothesizes that the behavior of words, particularly with respect to the expression and interpretation of their arguments, is to a large extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983; Talmy 1985; Gropen et al. 1989; Pinker 1989; Levin 1993; Goldberg 1995). It is therefore not surprising that there have been efforts to enrich vector-based models with morpho-syntactic information. Extensions range from part of speech tagging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analysis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin 1998a). In these semantic space models, contexts are defined over words bearing a syntactic relationship to the target words of interest. This makes semantic spaces more flexible; different types of contexts can be selected; words do not have to co-occur within a small, fixed word window; and word order or argument structure differences can be naturally mirrored in the semantic space. This article proposes a general framework for semantic space models which conceptualizes context in"
J07-2002,N04-3012,0,\N,Missing
J07-2002,W03-1809,0,\N,Missing
J07-2002,C98-2122,0,\N,Missing
J07-2002,J09-3004,0,\N,Missing
J08-1001,W03-1004,1,0.354824,"w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi [(w · xi ) + b] > 0 Finding the optimal hyperplane is an optimization problem which can be solved efﬁciently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classiﬁcation (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simpliﬁed articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difﬁcult.11 Examples of these two categories are given in Table 9. 11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of a corpus c"
J08-1001,P05-1018,1,0.142174,"Parameter Settings. In order to investigate the contribution of linguistic knowledge on model performance we experimented with a variety of grid representations resulting in different parameterizations of the feature space from which our model is learned. We focused on three sources of linguistic knowledge—syntax, coreference resolution, and salience—which play a prominent role in entity-based analyses of dis4 The collections are available from http://people.csail.mit.edu/regina/coherence/. 5 Short texts may have less than 20 permutations. The corpus described in the original ACL publication (Barzilay and Lapata 2005) contained a number of duplicate permutations. These were removed from the current version of the corpus. 12 Barzilay and Lapata Modeling Local Coherence course coherence (see Section 3.3 for details). An additional motivation for our study was to explore the trade-off between robustness and richness of linguistic annotations. NLP tools are typically trained on human-authored texts, and may deteriorate in performance when applied to automatically generated texts with coherence violations. We thus compared a linguistically rich model against models that use more impoverished representations. Mo"
J08-1001,N04-1015,1,0.127659,"nothing prevents the use of our feature vector representation for conventional classiﬁcation tasks. We offer an illustration in Experiment 3, where features extracted from entity grids are used to enhance the performance of a readability assessment system. Here, the learner takes as input a set of documents labeled with discrete classes (e.g., denoting whether a text is difﬁcult or easy to read) and learns to make predictions for unseen instances (see Section 6 for details on the machine learning paradigm we employ). 4. Experiment 1: Sentence Ordering Text structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al. 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information10 Barzilay and Lapata Modeling Local Coherence bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. The information bearing items can be database entries (Karamanis et al. 2004), propositions (Mellish et al. 1998) or sentences (Lapata 2003; Barzilay and Lee 2004). In sentence ordering, a document is viewed as a bag of sentenc"
J08-1001,P87-1022,0,0.275966,"Missing"
J08-1001,briscoe-carroll-2002-robust,0,0.0750339,"actic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account."
J08-1001,A00-2018,0,0.0142276,"labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguistic forms, for e"
J08-1001,P97-1003,0,0.0427463,"g constituent labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreference resolution into account. Even though the same entity appears in different linguis"
J08-1001,J95-2003,0,0.872683,"syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classiﬁcation tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are oft"
J08-1001,P86-1031,0,0.244676,"Missing"
J08-1001,J04-4001,0,0.0506367,"hat our entity-based representation is well-suited for ranking-based generation and text classiﬁcation tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment. 1. Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article f"
J08-1001,P95-1034,0,0.0200585,"oth the underlying discourse representation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in"
J08-1001,N01-1025,0,0.00483851,"ts that crossed each other and ran the entire width and length of the town. Valletta was one of the ﬁrst towns to be laid out in this way. vector w and a threshold b, so that all positive training examples are on one side of the hyperplane, while all negative ones lie on the other side. This is equivalent to requiring yi [(w · xi ) + b] > 0 Finding the optimal hyperplane is an optimization problem which can be solved efﬁciently using the procedure described in Vapnik (1998). SVMs have been widely used for many NLP tasks ranging from text classiﬁcation (Joachims 1998b), to syntactic chunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al. 2005). 6.2 Method Data. For our experiments we used a corpus collected by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and Britannica Elementary. The latter is a new version targeted at children. The corpus contains 107 articles from the full version of the encyclopedia and their corresponding simpliﬁed articles from Britannica Elementary (214 articles in total). Although these texts are not explicitly annotated with grade levels, they still represent two broad readability categories, namely, easy and difﬁcult.11 Examples of these"
J08-1001,P98-1116,0,0.00903236,"sentation and the inference procedure. Thus, our work is complementary to computational models developed on manually annotated data (Miltsakaki and Kukich 2000; Hasler 2004; Poesio et al. 2004). Automatic, albeit noisy, feature extraction allows us to perform a large scale evaluation of differently instantiated coherence models across genres and applications. 2.2 Ranking Approaches in Natural Language Generation Ranking approaches have enjoyed an increasing popularity at all stages in the generation pipeline, ranging from text planning to surface realization (Knight and Hatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al. 1998; Walker, Rambow, and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an underlying system produces a potentially large set of candidate outputs, with respect to various text generation rules encoded as hard constraints. Not all of the resulting alternatives will correspond to well-formed texts, and of those which may be judged acceptable, some will be preferable to others. The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function. The top"
J08-1001,P03-1069,1,0.865056,"ied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides 2003) which are hard to implement in a robust application. This article focuses on local coherence, which captures text relatedness at the level of sentence-to-sentence transitions. Local coherence is undoubtedly necessary for global coherence and has received considerable attention in computational linguistics (Foltz, Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller ∗ Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, 32 Vassar Street, 32-G468 Cambridge, MA 02139. E-mail: regina@csail.mit.edu. ∗∗ School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 2004; Karamanis et al. 2004). It is also supported by much psycholinguistic"
J08-1001,N03-1020,0,0.0639576,"Missing"
J08-1001,H01-1046,0,0.0122859,"their syntactic role. Such information can be expressed in many ways (e.g., using constituent labels or thematic role information). Because grammatical relations ﬁgure prominently in entity-based theories of local coherence (see Section 2), they serve as a logical point of departure. Each grid cell thus corresponds to a string from a set of categories reﬂecting whether the entity in question is a subject (S), object (O), or neither (X). Entities absent from a sentence are signaled by gaps (–). Grammatical role information can be extracted from the output of a broad-coverage dependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statistical parser (Collins 1997; Charniak 2000). We discuss how this information was computed for our experiments in Section 3.3. Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2. Because the text contains six sentences, the grid columns are of length six. Consider for instance the grid column for the entity trial, [ O – – – – X ]. It records that trial is present in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of the sentences. Also note that the grid in Table 1 takes coreferen"
J08-1001,W98-1411,0,0.0678039,"Missing"
J08-1001,P00-1052,0,0.251152,"tive speciﬁcations proposed in the literature, and demonstrate that the predictive power of the theory is highly sensitive to its parameter deﬁnitions. A common methodology for translating entity-based theories into computational models is to evaluate alternative speciﬁcations on manually annotated corpora. Some studies aim to ﬁnd an instantiation of parameters that is most consistent with observable data (Strube and Hahn 1999; Karamanis et al. 2004; Poesio et al. 2004). Other studies adopt a speciﬁc instantiation with the goal of improving the performance of a metric on a task. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with entity transition information, and show that the distribution of transitions correlates with human grades. Analogously, Hasler (2004) investigates whether Centering Theory can be used in evaluating the readability of automatic summaries by annotating human and machine generated extracts with entity transition information. The present work differs from these approaches in goal and methodology. Although our work builds upon existing linguistic theories, we do not aim to directly implement or reﬁne any of them in particular. We provide our model with sour"
J08-1001,J91-1002,0,0.274875,"reader to Barzilay (2003). Salience. Centering and other discourse theories conjecture that the way an entity is introduced and mentioned depends on its global role in a given discourse. We evaluate the impact of salience information by considering two types of models: The ﬁrst model treats all entities uniformly, whereas the second one discriminates between transitions of salient entities and the rest. We identify salient entities based on their frequency,3 following the widely accepted view that frequency of occurrence correlates with discourse prominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991). To implement a salience-based model, we modify our feature generation procedure by computing transition probabilities for each salience group separately, and then 2 When evaluating the output of coreference algorithms, performance is typically measured using a model-theoretic scoring scheme proposed in Vilain et al. (1995). The scoring algorithm computes the recall error by taking each equivalence class S in the gold standard and determining the number of coreference links m that would have to be added to the system’s output to place all entities in S into the same equivalence class produced"
J08-1001,P02-1014,0,0.0212418,"nd the inﬂuence of salience. All these knowledge sources ﬁgure prominently in theories of discourse (see Section 2) and are considered important in determining coherence. Our results empirically validate the importance of salience and syntactic information (expressed by S, O, X, and –) for coherence-based models. The combination of both knowledge sources (Syntax+Salience) yields models with consistently good performance for all our tasks. The beneﬁts of full coreference resolution are less uniform. This is partly due to mismatches between training and testing conditions. The system we employ (Ng and Cardie 2002) was trained on human-authored newspaper texts. The corpora we used in our sentence ordering and readability assessment experiments are somewhat similar (i.e., human-authored narratives), whereas our summary coherence rating experiment employed machine generated texts. It is therefore not surprising that coreference resolution delivers performance gains on the ﬁrst two tasks but not on the latter (see Table 5 in Section 4 and Table 10 in Section 6.3). Our results further show that in lieu of an automatic coreference resolution system, entity classes can be approximated simply by string matchin"
J08-1001,P02-1040,0,0.0958007,"Missing"
J08-1001,J04-3003,0,0.0959281,"Missing"
J08-1001,P05-1065,0,0.62284,"automatically generated summaries. In both experiments, our method yields improvements over state-of-the-art models. We also show the beneﬁts of the entitybased representation in a readability assessment task, where the goal is to predict the comprehension difﬁculty of a given text. In contrast to existing systems which focus on intra-sentential features, we explore the contribution of discourse-level features to this task. By incorporating coherence features stemming from the proposed entity-based representation, we improve the performance of a state-of-the-art readability assessment system (Schwarm and Ostendorf 2005). In the following section, we provide an overview of entity-based theories of local coherence and outline previous work on its computational treatment. Then, we introduce our entity-based representation, and deﬁne its linguistic properties. In the subsequent sections, we present our three evaluation tasks, and report the results of our experiments. Discussion of the results concludes the article. 2. Related Work Our approach is inspired by entity-based theories of local coherence, and is well-suited for developing a coherence metric in the context of a ranking-based text generation system. We"
J08-1001,J01-4004,0,0.256041,"Missing"
J08-1001,J99-3001,0,0.0220453,"; Prince 1981; Grosz, Joshi, and Weinstein 1995). A unifying assumption underlying different approaches is that discourse coherence is achieved in view of the way discourse entities are introduced and discussed. This observation is commonly formalized by devising constraints on the linguistic realization and distribution of discourse entities in coherent texts. At any point in the discourse, some entities are considered more salient than others, and consequently are expected to exhibit different properties. In Centering Theory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube and Hahn 1999; Poesio et al. 2004), salience concerns how entities are realized in an utterance (e.g., whether they are they pronominalized or not). In other theories, salience is deﬁned in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993). More reﬁned accounts expand the notion of salience from a binary distinction to a scalar one; examples include Prince’s (1981) familiarity scale, and Givon’s (1987) and Ariel’s (1988) givenness-continuum. The salience status of an entity is often reﬂected"
J08-1001,W04-3222,0,0.0206683,"Missing"
J08-1001,M95-1005,0,0.437278,"Missing"
J08-1001,J94-2003,0,0.0177928,"Missing"
J08-1001,N01-1003,0,0.0778537,"Missing"
J08-1001,P04-1050,0,\N,Missing
J08-1001,C98-1112,0,\N,Missing
J08-1001,P04-1051,0,\N,Missing
J08-1001,C69-7001,0,\N,Missing
J08-1001,C69-6902,0,\N,Missing
J08-4005,P06-1002,0,0.0102134,"the alignment or have an unaligned word on a boundary, respectively, indicated by a cross. where Ap and B p are the predicted and reference phrase pairs, respectively, and p the atom subscript denotes the subset of atomic phrase pairs, Aatom ⊆ Ap . As shown in Equation (3) we measure precision and recall between atomic phrase pairs and the full space of atomic and composite phrase pairs. This ensures that we do not multiply reward composite phrase pair combinations,11 while also not unduly penalizing non-matching phrase pairs which are composed of atomic phrase pairs in 11 This contrasts with Ayan and Dorr (2006), who use all phrase pairs up to a given size, and therefore might multiply count phrase pairs. 604 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems Table 2 Phrase pairs are speciﬁed by the word alignments from Figure 2, using the possible alignments. The entire set of atomic phrase pairs for either annotator (labeled A or B) and a selection of the remaining 57 composite phrase pairs are shown. The italics denote lexically identical phrase pairs. ∗ This phrase pair is atomic in A but composite in B. Atomic phrase pairs they they discussed the aspects in detail in de"
J08-4005,P05-1074,1,0.755451,"marization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (ﬁghting, battle) or structural (last week’s ﬁghting, the battle last week), and are represented as words or ∗ School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ∗∗ Center for Speech and Language Processing, Johns Hopkin"
J08-4005,W03-1004,0,0.109887,"). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBurch 2007) as well as comparable corpora such as collections of articles produced by two different newswire agencies about the same events (Barzilay and Elhadad 2003). Although paraphrase induction algorithms differ in many respects—for example, the acquired paraphrases often vary in granularity as they can be lexical (ﬁghting, battle) or structural (last week’s ﬁghting, the battle last week), and are represented as words or ∗ School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: tcohn@inf.ed.ac.uk. ∗∗ Center for Speech and Language Processing, Johns Hopkins University, Baltimore, MD, 21218. E-mail: ccb@cs.jhu.edu. † School of Informatics, University of Edinburgh, EH8 9AB, Edinburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission recei"
J08-4005,N03-1003,0,0.458229,"inburgh, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2007; revised submission received: 8 February 2008; accepted for publication: 26 March 2008. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 syntax trees—they all rely on some form of alignment for extracting paraphrase pairs. In its simplest form, the alignment can range over individual words, as is often done in machine translation (Quirk, Brockett, and Dolan 2004). In other cases, the alignments range over entire trees (Pang, Knight, and Marcu 2003) or sentence clusters (Barzilay and Lee 2003). The obtained paraphrases are typically evaluated via human judgments. Paraphrase pairs are presented to judges who are asked to decide whether they are semantically equivalent, that is, whether they can be generally substituted for one another in the same context without great information loss (Barzilay and Lee 2003; Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003; Bannard and Callison-Burch 2005). In some cases the automatically acquired paraphrases are compared against manually generated ones (Lin and Pantel 2001) or evaluated indirectly by demonstrating performance increase for a"
J08-4005,J93-2003,0,0.00937007,"Missing"
J08-4005,N06-1003,1,0.386521,"Missing"
J08-4005,D07-1008,1,0.626972,"Missing"
J08-4005,W06-1628,0,0.0184492,"Missing"
J08-4005,W04-3216,0,0.019874,"Missing"
J08-4005,C04-1051,0,0.725955,"Missing"
J08-4005,N06-2009,0,0.0282029,"Missing"
J08-4005,J07-3002,0,0.072039,"reference alignment B can be then computed using standard recall, precision, and F1 measures (Och and Ney 2003): Precision = |AS ∩ BP | |AS | Recall = |AP ∩ BS | |BS | F1 = 2 · Precision · Recall Precision + Recall (2) where the subscripts S and P denote sure and possible word alignments, respectively. Note that both precision and recall are asymmetric in that they compare sets of possible and sure alignments. This is designed to be maximally generous: sure predictions which are present in the reference as possibles are not penalized in precision, and the converse applies for recall. We adopt Fraser and Marcu (2007)’s deﬁnition of F1, an F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufﬁciently penalize unbalanced precision and recall.9 As our corpus is monolingual, in order to avoid artiﬁcial score inﬂation, we limit the precision and recall calculations to consider only pairs of non-identical words (and phrases, as discussed subsequently). To give an example, consider the sentence pairs in Figure 2, whose alignments have been produced by the two annotators A (left) and B (rig"
J08-4005,N03-1017,0,0.0310332,"Missing"
J08-4005,W05-0809,0,0.020932,"Missing"
J08-4005,W03-0301,0,0.0235874,"or instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In the following section we expl"
J08-4005,C00-2163,0,0.194676,"ons are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In"
J08-4005,P00-1056,0,0.595666,"ons are possible, for instance we could have asked our annotators to identify all constituents that are more or less meaning preserving in our parallel corpus. We chose to work with alignments for two reasons. First, the notion of alignment appears to be central in paraphrasing—most existing paraphrase induction algorithms rely on alignments either implicitly or explicitly for identifying paraphrase units. Secondly, research in machine translation, where several gold-standard alignment corpora have been created, shows that word alignments can be identiﬁed reliably by annotators (Melamed 1998; Och and Ney 2000b; Mihalcea and Pedersen 2003; Martin, Mihalcea, and Pedersen 2005). We therefore create word alignments similar to those observed in machine translation, namely, featuring one-to-one, one-to-many, many-to-one, and many-to-many links between words. Alignment blocks larger than one-to-one are used to specify phrase correspondences. 1 Our deﬁnition of the term phrase follows the SMT literature. It refers to any contiguous sequence of words, whether it is a syntactic constituent or not. See Section 2 for details. 598 Cohn, Callison-Burch, and Lapata Constructing Corpora for Paraphrase Systems In"
J08-4005,J03-1002,0,0.0916633,"were split into two distinct sets, each consisting of 300 sentences (100 per corpus), and were annotated by a single coder. Each coder annotated the same amount of data. In addition, we obtained a trial set of 50 sentences from the MTC corpus which was used for familiarizing our annotators with the paraphrase alignment task (this set does not form part of the corpus). In sum, we obtained paraphrase annotations for 900 sentence pairs, 300 of which are doubly annotated. To speed up the annotation process, the data sources were ﬁrst aligned automatically and then hand-corrected. We used Giza++ (Och and Ney 2003), a publicly available 2 The corpus is made available by the LDC, Catalog Number LDC2002T01, ISBN 1-58563-217-1. 3 The corpus can be downloaded from http://www.isi.edu/∼knight/. 4 The corpus is available at http://research.microsoft.com/research/downloads/Details/607D14D920CD-47E3-85BC-A2F65CD28042/Details.aspx. 599 Computational Linguistics Volume 34, Number 4 implementation of the IBM word alignment models (Brown et al. 1993). Giza++ was trained on the full 993-sentence MTC part1 corpus5 using all 11 translators and all pair−1) training ings of English translations as training instances. Thi"
J08-4005,W99-0604,0,0.180416,"Missing"
J08-4005,N03-1024,0,0.192731,"Missing"
J08-4005,W04-3219,0,0.44813,"Missing"
J08-4005,N06-1057,0,0.0232382,"rd alignments and show that it yields high inter-annotator agreement. As Kappa is suited to nominal data, we employ an alternative agreement statistic which is appropriate for structured alignment tasks. We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically (e.g., by measuring precision, recall, and F1) and also in developing linguistically rich paraphrase models based on syntactic structure. 1. Introduction The ability to paraphrase text automatically carries much practical import for many NLP applications ranging from summarization (Barzilay 2003; Zhou et al. 2006) to question answering (Lin and Pantel 2001; Duboue and Chu-Carroll 2006) and machine translation (Callison-Burch, Koehn, and Osborne 2006). It is therefore not surprising that recent years have witnessed increasing interest in the acquisition of paraphrases from real world corpora. These are most often monolingual corpora containing parallel translations of the same source text (Barzilay and McKeown 2001; Pang, Knight, and Marcu 2003). Truly bilingual corpora consisting of documents and their translations have also been used to acquire paraphrases (Bannard and Callison-Burch 2005; CallisonBur"
J08-4005,P01-1008,0,\N,Missing
J08-4005,J08-4004,0,\N,Missing
J10-3005,W97-0703,0,0.144219,"in formulation and training requirements (some approaches require a parallel corpus, whereas others do not), existing models are similar in that they compress sentences in isolation without taking their surrounding context into account. This is in marked contrast with common practice in summarization. Professional abstractors often rely on contextual cues while creating summaries (EndresNiggemeyer 1998). This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surrounding sentences (Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002). Determining which information is important in a sentence is not merely a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is less likely). A variety of contextual factors can play a role, such as the discourse topic, whether the sentence introduces new entities or events that have not been mentioned before, or the reader’s background knowledge. A sentence-centric view of compression is also at odds with most relevant applications which aim to create a shorter document rather than a single sentence. The resulting docu"
J10-3005,N06-1046,1,0.795931,"s Volume 36, Number 3 optimal solution by searching over the entire compression space2 without employing heuristics or approximations during decoding (see Turner and Charniak [2005] and McDonald [2006] for examples). Besides sentence compression, the ILP modeling framework has been applied to a wide range of natural language processing tasks demonstrating improvements over more traditional methods. Examples include reluctant paraphrasing (Dras 1997), relation extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and coreference resolution (Denis and Baldridge 2007). In the following we describe Clarke and Lapata’s (2008) model in more detail. Sections 4–5 present our extensions and modiﬁcations. 3.1 Language Model Let x = x0 , x1 , x2 , . . . , xn denote a source sentence for which we wish to generate a target compression. We use x0 to denote the “start” token. We introduce a decision variable for each word in the source and constrain it to be binary; a value of 0 represents a word being dropped, whereas a value of 1 include"
J10-3005,J08-1001,1,0.826049,"on a sentence-by-sentence basis without presupposing an explicit discourse structure. Inspired by Centering Theory (Grosz, Weinstein, and Joshi 1995)—a theory of local discourse structure that models the interaction of referential continuity and salience of discourse entities—Or˘asan (2003) proposes a summarization algorithm that extracts sentences with at least one entity in common. The idea here is that summaries containing sentences referring to the same entity will be more coherent. Other work has relied on centering not so much to create summaries but to assess whether they are readable (Barzilay and Lapata 2008). Our approach differs from previous sentence compression approaches in three key respects. First, we present a compression model that is contextually aware; decisions on whether to remove or retain a word (or phrase) are informed by its discourse properties (e.g., whether it introduces a new topic, or whether it is semantically related to the previous sentence). Unlike Jing (2000) we explicitly identify topically important words and assume speciﬁc representations of discourse structure. Secondly, in contrast to Daum´e III and Marcu (2002) and other summarization work, we adopt a less global a"
J10-3005,W97-0702,0,0.0674635,"hesion (Halliday and Hasan 1976) and the way it is expressed in discourse. The term broadly describes a variety of linguistic devices responsible for making the elements of a text appear uniﬁed or connected. Examples include word repetition, anaphora, ellipsis, and the use of synonyms or superordinates. The underlying assumption is that sentences connected to many other sentences are likely to carry salient information and should therefore be included in the summary (Sjorochod’ko 1972). In exploiting cohesion for summarization, it is necessary to somehow represent cohesive ties. For instance, Boguraev and Kennedy (1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad (1997) operationalize cohesion via lexical chains—sequences of related words spanning a topical unit (Morris and Hirst 1991). Besides repetition, they also examine semantic relations based on synonymy, antonymy, hypernymy, and holonymy (we discuss their approach in more detail in Section 4.1). Other approaches characterize the document in terms of discourse structure and rhetorical relations. Documents are commonly represented as trees (Mann and Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Car"
J10-3005,briscoe-carroll-2002-robust,0,0.109315,"e, the most important information is conveyed by clauses S3 (he will resign) and S4 (if he is not reselected), which are embedded. Accordingly, we should give more weight to words found in these clauses than in the main clause (S1 in Figure 1). A simple way to enforce this is to give clauses weight proportional to the level of embedding (see the second term in Equation (9)). Therefore in Figure 1, the term Nl is 1.0 (4/4) for clause S4 , 0.75 (3/4) for clause S3 , and so on. Individual words inherit their weight from their clauses. We obtain syntactic information in our experiments from RASP (Briscoe and Carroll 2002), a domain-independent, robust parsing system for English. However, any other parser with broadly similar output (e.g., Lin 2001) could also serve our purposes. Note that the signiﬁcance score in Equation (9) does not weight differentially the contribution of tf ∗ idf versus level of embedding. Although we found in our experiments that the latter term was as important as tf ∗ idf in producing meaningful compressions, there may be applications or data sets where the contribution of the two terms varies. This could be easily remedied by introducing a weighting factor. 3.3 Sentential Constraints"
J10-3005,P07-1036,0,0.0164413,"Missing"
J10-3005,A00-2018,0,0.0413608,"Missing"
J10-3005,P06-1048,1,0.390745,"he application. 6.3 Evaluation Previous studies evaluate the well-formedness of automatically generated compressions out of context. The target sentences are typically rated by naive subjects on two dimensions, grammaticality and importance (Knight and Marcu 2002). Automatic evaluation measures have also been proposed. Riezler et al. (2003) compare the grammatical relations found in the system output against those found in a gold standard using F1. Although F1 conﬂates grammaticality and importance into a single score, it nevertheless has been shown to correlate reliably with human judgments (Clarke and Lapata 2006). The aims of our evaluation study were twofold. Firstly, we wanted to examine whether our discourse constraints improve the compressions for individual sentences. There is no hope for generating shorter documents if the compressed sentences are either too wordy or too ungrammatical. Secondly and more importantly, our goal was to evaluate the compressed documents as a whole by examining whether they are readable and the degree to which they retain key information when compared to the originals. We evaluated sentence-based compressions automatically using F1 and the grammatical relations annota"
J10-3005,P02-1057,0,0.0145701,"Missing"
J10-3005,N07-1030,0,0.0282421,"Missing"
J10-3005,J95-2003,0,0.0555413,"Missing"
J10-3005,A00-1043,0,0.0237279,"learn either which constituents to delete or which words to place adjacently in the compression output. Relatively few approaches dispense with the parallel corpus and generate compressions in an unsupervised manner using either a scoring function (Hori and Furui 2004; Clarke and Lapata 2008) or compression rules that are approximated from a non-parallel corpus such as the Penn Treebank (Turner and Charniak 2005). The majority of sentence compression approaches only look at sentences in isolation without taking into account any discourse information. However, there are two notable exceptions. Jing (2000) uses information from the local context as evidence for and against the removal of phrases during sentence compression. The idea here is that words or phrases which have more links to the surrounding context are more indicative of its topic, and thus should not be dropped. The topic is not explicitly identiﬁed; instead the importance of each phrase is determined by the number of lexical links within the local context. A link is created between two words if they are repetitions, 413 Computational Linguistics Volume 36, Number 3 morphologically related, or associated in WordNet (Fellbaum 1998)"
J10-3005,J04-4001,0,0.00909295,"ntioned anywhere else in the document and is therefore deemed unimportant. Or it could decide to retain it for the sake of topic continuity. In this article we are interested in creating a compression model that is appropriate for both documents and sentences. Luckily, a variety of discourse theories have been developed over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi 1995; Halliday and Hasan 1976) and have found application in summarization (Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation applications (Scott and de Souza 1990; Kibble and Power 2004). In creating a contextsensitive compression model we are faced with three important questions: (1) Which type of discourse information is useful for compression? (2) Is it amenable to automatic processing (there is little hope for interfacing our compression model with applications if discourse-level cues cannot be identiﬁed robustly)? and (3) How are sentence- and document-based information best integrated in a uniﬁed modeling framework? 412 Clarke and Lapata Discourse Constraints for Document Compression In building our compression model we borrow insights from two popular models of discour"
J10-3005,W03-1101,0,0.0175449,"rammaticality. The popularity of sentence compression is largely due to its relevance for applications. Summarization is a case in point here. Most summarizers to date aim to produce informative summaries at a given compression rate. If we can have a compression component that reduces sentences to a minimal length and still retains the most important content, then we should be able to pack more information content into a ﬁxed size summary. In other words, sentence compression would allow summarizers to increase the overall amount of information extracted without increasing the summary length (Lin 2003; Zajic et al. 2007). It could also be used as a post-processing step in order to render summaries more coherent and less repetitive (Mani, Gates, and Bloedorn 1999). Beyond summarization, a sentence compression module could be used to display text on small screen devices such as PDAs (Corston-Oliver 2001) or as a reading aid for the blind (Grefenstette 1998). Sentence compression could also beneﬁt information retrieval by eliminating extraneous information from the documents indexed by the ∗ Department of Computer Science, University of Illinois at Urbana-Champaign, 201 N Goodwin Ave, Urbana,"
J10-3005,H01-1046,0,0.0142927,"e should give more weight to words found in these clauses than in the main clause (S1 in Figure 1). A simple way to enforce this is to give clauses weight proportional to the level of embedding (see the second term in Equation (9)). Therefore in Figure 1, the term Nl is 1.0 (4/4) for clause S4 , 0.75 (3/4) for clause S3 , and so on. Individual words inherit their weight from their clauses. We obtain syntactic information in our experiments from RASP (Briscoe and Carroll 2002), a domain-independent, robust parsing system for English. However, any other parser with broadly similar output (e.g., Lin 2001) could also serve our purposes. Note that the signiﬁcance score in Equation (9) does not weight differentially the contribution of tf ∗ idf versus level of embedding. Although we found in our experiments that the latter term was as important as tf ∗ idf in producing meaningful compressions, there may be applications or data sets where the contribution of the two terms varies. This could be easily remedied by introducing a weighting factor. 3.3 Sentential Constraints In its original formulation, the model also contains a small number of sentence-level constraints. Their aim is to preserve the m"
J10-3005,W01-0100,0,0.876155,"Missing"
J10-3005,P99-1072,0,0.0858398,"Missing"
J10-3005,W05-0618,0,0.00771086,"15 Computational Linguistics Volume 36, Number 3 optimal solution by searching over the entire compression space2 without employing heuristics or approximations during decoding (see Turner and Charniak [2005] and McDonald [2006] for examples). Besides sentence compression, the ILP modeling framework has been applied to a wide range of natural language processing tasks demonstrating improvements over more traditional methods. Examples include reluctant paraphrasing (Dras 1997), relation extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and coreference resolution (Denis and Baldridge 2007). In the following we describe Clarke and Lapata’s (2008) model in more detail. Sections 4–5 present our extensions and modiﬁcations. 3.1 Language Model Let x = x0 , x1 , x2 , . . . , xn denote a source sentence for which we wish to generate a target compression. We use x0 to denote the “start” token. We introduce a decision variable for each word in the source and constrain it to be binary; a value of 0 represents a word being dropped, w"
J10-3005,W09-1801,0,0.0846849,"Missing"
J10-3005,P09-1039,0,0.0236406,"Missing"
J10-3005,E06-1038,0,0.10027,"arniak 2005; Galley and McKeown 2007; Cohn and Lapata 2009). For example, Knight and Marcu (2002) learn a synchronous context-free grammar (Aho and Ullman 1969) from such a corpus. The grammar rules have weights (essentially probabilities estimated using maximum likelihood) and are used to ﬁnd the best compression from the set of all possible compressions for a given sentence. Other approaches exploit syntactic information without making explicit use of a parallel grammar—for example, by learning which words or constituents to delete from a parse tree (Riezler et al. 2003; Nguyen et al. 2004; McDonald 2006; Clarke and Lapata 2008). Despite differences in formulation and training requirements (some approaches require a parallel corpus, whereas others do not), existing models are similar in that they compress sentences in isolation without taking their surrounding context into account. This is in marked contrast with common practice in summarization. Professional abstractors often rely on contextual cues while creating summaries (EndresNiggemeyer 1998). This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surroundin"
J10-3005,P00-1052,0,0.0393598,"A great deal of research has been devoted to ﬂeshing these out and many different instantiations have been developed in the literature (see Poesio et al. [2004] for details). In our case, the instantiation will have a bearing on the reliability of the algorithm to detect centers. If the parameters are too speciﬁc then it may not be possible to accurately determine the center for a given utterance. Because our aim is to identify centers in discourse automatically, our parameter choice is driven by two considerations: robustness and ease of computation. We therefore follow previous work (e.g., Miltsakaki and Kukich 2000) in assuming that the unit of an utterance is the sentence (i.e., a main clause with accompanying subordinate and adjunct clauses). This is a simplistic view of an utterance; however it is in line with our compression task, which also operates over sentences. We determine which entities are invoked by a sentence using two methods. First, we perform named entity identiﬁcation and coreference resolution on each document using LingPipe,6 a 6 LingPipe can be downloaded from http://alias-i.com/lingpipe/. 425 Computational Linguistics Volume 36, Number 3 publicly available system. Named entities are"
J10-3005,J91-1002,0,0.12873,"mportant questions: (1) Which type of discourse information is useful for compression? (2) Is it amenable to automatic processing (there is little hope for interfacing our compression model with applications if discourse-level cues cannot be identiﬁed robustly)? and (3) How are sentence- and document-based information best integrated in a uniﬁed modeling framework? 412 Clarke and Lapata Discourse Constraints for Document Compression In building our compression model we borrow insights from two popular models of discourse, Centering Theory (Grosz, Weinstein, and Joshi 1995) and lexical chains (Morris and Hirst 1991). Both approaches capture local coherence—the way adjacent sentences bind together to form a larger discourse. They also both share the view that discourse coherence revolves around discourse entities and the way they are introduced and discussed. We ﬁrst automatically augment our documents with annotations pertaining to centering and lexical chains, which we subsequently use to inform our compression model. The latter is an extension of the integer linear programming formulation proposed by Clarke and Lapata (2008). In a nutshell, sentence compression is modeled as an optimization problem. Gi"
J10-3005,P99-1045,0,0.047124,"niversity of Illinois at Urbana-Champaign, 201 N Goodwin Ave, Urbana, IL 61801, USA. E-mail: clarkeje@illinois.edu. ∗∗ School of Informatics, University of Edinburgh, 10 Crichton Street, Edinburgh, EH8 9AB, UK. E-mail: mlap@inf.ed.ac.uk. Submission received: 10 September 2008; revised submission received: 27 October 2009; accepted for publication: 6 March 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 3 retrieval engine. This way it would be possible to store less information in the index without dramatically affecting retrieval performance (Olivers and Dolan 1999). In theory, sentence compression may involve several rewrite operations such as deletion, substitution, insertion, and word reordering. In practice, however, the task is commonly deﬁned as a word deletion problem: Given an input sentence of words x = x1 , x2 , . . . , xn , the aim is to produce a compression by removing any subset of these words (Knight and Marcu 2002). Many sentence compression models aim to learn deletion rules from a parsed parallel corpus of source sentences and their target compressions (Knight and Marcu 2002; Turner and Charniak 2005; Galley and McKeown 2007; Cohn and L"
J10-3005,C94-1056,0,0.0461503,"Missing"
J10-3005,W03-1205,0,0.0551709,"Missing"
J10-3005,J04-3003,0,0.0143021,"Missing"
J10-3005,C04-1197,0,0.0129998,") and Vanderbei (2001) for comprehensive overviews. 415 Computational Linguistics Volume 36, Number 3 optimal solution by searching over the entire compression space2 without employing heuristics or approximations during decoding (see Turner and Charniak [2005] and McDonald [2006] for examples). Besides sentence compression, the ILP modeling framework has been applied to a wide range of natural language processing tasks demonstrating improvements over more traditional methods. Examples include reluctant paraphrasing (Dras 1997), relation extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and coreference resolution (Denis and Baldridge 2007). In the following we describe Clarke and Lapata’s (2008) model in more detail. Sections 4–5 present our extensions and modiﬁcations. 3.1 Language Model Let x = x0 , x1 , x2 , . . . , xn denote a source sentence for which we wish to generate a target compression. We use x0 to denote the “start” token. We introduce a decision variable for each word in the source and constrain it to be"
J10-3005,W06-1616,1,0.313318,"hing over the entire compression space2 without employing heuristics or approximations during decoding (see Turner and Charniak [2005] and McDonald [2006] for examples). Besides sentence compression, the ILP modeling framework has been applied to a wide range of natural language processing tasks demonstrating improvements over more traditional methods. Examples include reluctant paraphrasing (Dras 1997), relation extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and coreference resolution (Denis and Baldridge 2007). In the following we describe Clarke and Lapata’s (2008) model in more detail. Sections 4–5 present our extensions and modiﬁcations. 3.1 Language Model Let x = x0 , x1 , x2 , . . . , xn denote a source sentence for which we wish to generate a target compression. We use x0 to denote the “start” token. We introduce a decision variable for each word in the source and constrain it to be binary; a value of 0 represents a word being dropped, whereas a value of 1 includes the word in the target compression. Let:"
J10-3005,N03-1026,0,0.0154575,"Missing"
J10-3005,J01-2004,0,0.0155301,"Missing"
J10-3005,W04-2401,0,0.0408946,"d reader to Winston and Venkataramanan (2003) and Vanderbei (2001) for comprehensive overviews. 415 Computational Linguistics Volume 36, Number 3 optimal solution by searching over the entire compression space2 without employing heuristics or approximations during decoding (see Turner and Charniak [2005] and McDonald [2006] for examples). Besides sentence compression, the ILP modeling framework has been applied to a wide range of natural language processing tasks demonstrating improvements over more traditional methods. Examples include reluctant paraphrasing (Dras 1997), relation extraction (Roth and Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009), and coreference resolution (Denis and Baldridge 2007). In the following we describe Clarke and Lapata’s (2008) model in more detail. Sections 4–5 present our extensions and modiﬁcations. 3.1 Language Model Let x = x0 , x1 , x2 , . . . , xn denote a source sentence for which we wish to generate a target compression. We use x0 to denote the “start” token. We introduce a decision variable fo"
J10-3005,J01-4003,0,0.0303289,"med entities and all remaining nouns7 to the Cf list. Entity matching between sentences is required to determine the Cb of a sentence. This is done using the named entity’s unique identiﬁer (as provided by LingPipe) or by the entity’s surface form in the case of nouns not classiﬁed as named entities. We follow Grosz, Weinstein, and Joshi (1995) in ranking entities according to their grammatical roles; subjects are ranked more highly than objects, which are in turn ranked higher than other grammatical roles; ties are broken using left-to-right ordering of the grammatical roles in the sentence (Tetreault 2001). We identify grammatical roles using RASP (Briscoe and Carroll 2002). Formally, our centering algorithm is as follows (where Uj corresponds to sentence j): 1. Extract entities from Uj . 2. Create Cf (Uj ) by ranking the entities in Uj according to their grammatical role (subjects > objects > others, ties broken using left-to-right word order of Uj ). 3. Find the highest ranked entity in Cf (Uj−1 ) which occurs in Cf (Uj ); set the entity to be Cb (Uj ). This procedure involves several automatic steps (named entity recognition, coreference resolution, and identiﬁcation of grammatical roles) an"
J10-3005,J02-4002,0,0.0595975,"ts (some approaches require a parallel corpus, whereas others do not), existing models are similar in that they compress sentences in isolation without taking their surrounding context into account. This is in marked contrast with common practice in summarization. Professional abstractors often rely on contextual cues while creating summaries (EndresNiggemeyer 1998). This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surrounding sentences (Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002). Determining which information is important in a sentence is not merely a function of its syntactic position (e.g., deleting the verb or the subject of a sentence is less likely). A variety of contextual factors can play a role, such as the discourse topic, whether the sentence introduces new entities or events that have not been mentioned before, or the reader’s background knowledge. A sentence-centric view of compression is also at odds with most relevant applications which aim to create a shorter document rather than a single sentence. The resulting document must not only be grammatical bu"
J10-3005,W04-1004,0,0.0248532,"er approaches characterize the document in terms of discourse structure and rhetorical relations. Documents are commonly represented as trees (Mann and Thompson 1988; Corston-Oliver 1998; Ono, Sumita, and Miike 1994; Carlson et al. 2001) and the position of a sentence in a tree is indicative of its importance. To give an example, Marcu (2000) proposes a summarization algorithm based on RST. Assuming that nuclei are more salient than satellites, the importance of sentential or clausal units can be determined based on tree depth. Alternatively, discourse structure can be represented as a graph (Wolf and Gibson 2004) and sentence importance is determined in 414 Clarke and Lapata Discourse Constraints for Document Compression graph-theoretic terms, by using graph connectivity measures such as in-degree or PageRank (Brin and Page 1998). Although a great deal of research in summarization has focused on global properties of discourse structure, there is evidence that local coherence may also be useful without the added complexity of computing discourse representations. (Unfortunately, discourse parsers have yet to achieve levels of performance comparable to syntactic parsers.) Teufel and Moens (2002) identify"
J10-3005,N07-1023,0,\N,Missing
J10-3005,E99-1011,0,\N,Missing
J10-3005,C04-1107,0,\N,Missing
J10-3005,P04-1049,0,\N,Missing
J10-3005,P05-1036,0,\N,Missing
J10-3005,I11-1126,0,\N,Missing
J12-1005,P09-1004,0,0.120481,"Missing"
J12-1005,andersen-etal-2008-bnc,0,0.0249901,"entences with manual annotations in FrameNet. The BNC is considerably larger compared with FrameNet, approximately by a factor of 100. Dependency graphs were produced with RASP (Briscoe, Carroll, and Watson 2006). Frame semantic annotations for labeled sentences were merged with their dependency-based representations as described in Section 3.1. Sentences for which this was not possible (mismatch score greater than 0) were excluded from the seed set, but retained in the test sets to allow for unbiased evaluation. For unlabeled BNC sentences, we used an existing RASP-parsed version of the BNC (Andersen et al. 2008). 4.2 Supervised SRL System A natural way of evaluating the proposed semi-supervised method is by comparing two instantiations of a supervised SRL system, one that is trained solely on FrameNet annotations and one that also uses the additional training instances produced by our algorithm. We will henceforth use the term unexpanded to refer to the corpus (and system trained on it) that contains only human-annotated instances, and accordingly, the term expanded to describe the corpus (and system) resulting from the application of our method or any other semi-supervised approach that obtains trai"
J12-1005,S07-1018,0,0.111546,"Missing"
J12-1005,N06-1046,1,0.715322,"tations. We do this by ﬁnding an optimal alignment, that is, an alignment with the highest score as deﬁned in Equation (1). To solve this optimization problem efﬁciently, we recast it as an integer linear program (ILP). The ILP modeling framework has been recently applied to a wide range of natural language processing tasks, demonstrating improvements over more traditional optimization methods. Examples include reluctant paraphrasing (Dras 1999), relation extraction (Roth and tau Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006), sentence compression (Clarke and Lapata 2008), and coreference resolution (Denis and Baldridge 2007). Importantly, the ILP approach3 delivers a globally optimal solution by searching over the entire alignment space without employing heuristics or approximations (see de Marneffe et al. [2007] and Haghighi, Ng, and Manning [2005]). Furthermore, an ILP-based formulation seems well-suited to our problem because the domain of the optimization, namely, the set of partial injective functions from M to N, is discrete. We deﬁne arbitrary linear orders on t"
J12-1005,N03-1003,0,0.121499,"Missing"
J12-1005,P06-4020,0,0.0143123,"Missing"
J12-1005,burchardt-etal-2006-salsa,0,0.126958,"Missing"
J12-1005,N10-1138,0,0.0225788,"unseen word belongs to the frame or not. Pennacchiotti et al. (2008) create “distributional proﬁles” for frames. The meaning of each frame is represented by a vector, which is the (weighted) centroid of the vectors representing the predicates that can evoke it. Unknown predicates are then assigned to the most similar frame. They also propose a WordNet-based model that computes the similarity between the synsets representing an unknown predicate and those activated by the 138 ¨ Furstenau and Lapata Semi-Supervised SRL via Structural Alignment predicates of a frame (see Section 6 for details). Das et al. (2010) represent a departure from the WordNet-based approaches in their use of a latent variable model to allow for the disambiguation of unknown predicates. Unsupervised approaches to SRL have been few and far between. Abend, Reichart, and Rappoport (2009) propose an algorithm that identiﬁes the arguments of predicates by relying only on part-of-speech annotations, without, however, assigning their semantic roles. In contrast, Grenager and Manning (2006) focus on role induction which they formalize as probabilistic inference in a Bayesian network. Their model deﬁnes a joint probability distribution"
J12-1005,P09-1053,0,0.0128085,"al setting, and thus do not project annotations between languages but within the same language. Importantly, we acquire new training instances for both known and unknown predicates. Previous proposals extend FrameNet with novel predicates without inducing annotations that exemplify their usage. We represent labeled and unlabeled instances as graphs, and seek to ﬁnd a globally optimal alignment between their nodes, subject to semantic and structural constraints. Finding similar labeled and unlabeled sentences is reminiscent of paraphrase identiﬁcation (Qiu, Kan, and Chua 2006; Wan et al. 2006; Das and Smith 2009; Chang et al. 2010), the task of determining whether one sentence is a paraphrase of another. The sentences we identify are not strictly speaking paraphrases (even if the two predicates are similar their arguments often are not); however, the idea of modeling the correspondence structure (or alignment) between parts of the two sentences is also present in the paraphrase identiﬁcation work (Das and Smith 2009; Chang et al. 2010). Besides machine translation (Matusov, Zens, and Ney 2004; Taskar, Lacoste-Julien, and Klein 2005), methods based on graph alignments have been previously employed for"
J12-1005,N07-1030,0,0.0329164,"Missing"
J12-1005,D09-1003,0,0.0708208,"ally labeled sentences to humans to inspect and correct. The experiments presented here are limited to verbal categories and focus solely on English. In the future, we would like to examine whether our approach generalizes to other syntactic categories such as nouns, adjectives, and prepositions. An obvious extension also involves experiments with other languages. Experiments on the SALSA corpus (Burchardt et al. 2006) show that similar improvements can be obtained for German ¨ (Furstenau 2011). Finally, the general formulation of our expansion framework allows its application to other tasks. Deschacht and Moens (2009) adapt our approach to augment subsets of the PropBank corpus and observe improvements over a supervised system for a small seed corpus. They also show that deﬁning the lexical similarity measure in terms of Jensen–Shannon divergence instead of cosine similarity can additionally improve performance. Another possibility would be to employ our framework for the acquisition of paraphrases, for example, by extending the multiple-sequence alignment approach of Barzilay and Lee (2003) with our notion of graph alignments. Finally, it would be interesting to investigate how to reduce the dependency on"
J12-1005,C04-1134,0,0.165608,"Missing"
J12-1005,furstenau-2008-enriching,1,0.852366,"emantic roles may in some cases yield no match. There are two reasons for this—parser errors and role annotations violating syntactic structure. We address this problem heuristically: If no perfect match is found, the closest match is determined based on the number of mismatching characters in the string. We thus compute a mismatch score for the FEE and each role. To make allowances for parser errors, we compute these scores for the n-best parses produced by the dependency parser and retain the dependency graph with the lowest mismatch. ¨ This mapping procedure is more thoroughly discussed in Furstenau (2008). Each sentence in the seed corpus contains annotations for a predicate and its semantic roles. A complex sentence (with many subordinate clauses) will be represented by a large dependency graph, with only a small subgraph corresponding to these annotations. Our method for computing alignments between graphs only considers subgraphs with nodes belonging to the predicate-argument structure in question. This allows us to compare graphs in a computationally efﬁcient manner as many irrelevant alignments are discarded, although admittedly the entire graph may provide useful contextual clues to the"
J12-1005,J02-3001,0,0.787506,"We formalize the detection of similar sentences and the projection of role annotations as a graph alignment problem, which we solve exactly using integer linear programming. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1. Introduction Recent years have seen growing interest in the shallow semantic analysis of natural language text. The term is most commonly used to refer to the automatic identiﬁcation and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky 2002). Semantic roles themselves have a long-standing tradition in linguistic theory, dating back to the seminal work of Fillmore (1968). They describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic conﬁgurations. Consider the following example sentences: (1) a. The burglar broke the window with a hammer. b. A hammer broke the window. c. The window broke. ∗ Center for Computational Learning Systems, Columbia University, 475 Riverside Drive, Suite 850, New York, NY 10115, USA. E-mail: hagen@ccls.columbia.edu. (The work reported in this paper was car"
J12-1005,P07-1025,0,0.0178552,"tic role labeling within the same language either by increasing the coverage of existing resources or by inducing role annotations from unlabeled data. Swier and Stevenson (2004) propose a method for bootstrapping a semantic role labeler. Given a verb instance, they ﬁrst select a frame from VerbNet, a semantic role resource akin to FrameNet and PropBank, and label each argument slot with sets of possible roles. Their algorithm then proceeds iteratively by ﬁrst making initial unambiguous role assignments, and then successively updating a probability model on which future assignments are based. Gordon and Swanson (2007) attempt to increase the coverage of PropBank. Their approach leverages existing annotations to handle novel verbs. Rather than annotating new sentences that contain novel verbs, they ﬁnd syntactically similar verbs and use their annotations as surrogate training data. Much recent work has focused on increasing the coverage of FrameNet, either by generalizing semantic roles across different frames or by determining the frame membership of unknown predicates. Matsubayashi, Okazaki, and Tsujii (2009) propose to exploit the relations between semantic roles in an attempt to overcome the scarcity o"
J12-1005,W06-1601,0,0.0195396,"ose activated by the 138 ¨ Furstenau and Lapata Semi-Supervised SRL via Structural Alignment predicates of a frame (see Section 6 for details). Das et al. (2010) represent a departure from the WordNet-based approaches in their use of a latent variable model to allow for the disambiguation of unknown predicates. Unsupervised approaches to SRL have been few and far between. Abend, Reichart, and Rappoport (2009) propose an algorithm that identiﬁes the arguments of predicates by relying only on part-of-speech annotations, without, however, assigning their semantic roles. In contrast, Grenager and Manning (2006) focus on role induction which they formalize as probabilistic inference in a Bayesian network. Their model deﬁnes a joint probability distribution over a verb, its semantic roles, and possible syntactic realizations. More recently, Lang and Lapata (2010) formulate the role induction problem as one of detecting alternations and ﬁnding a canonical syntactic form for them. Their model extends the logistic classiﬁer with hidden variables and is trained on parsed output which is used as a noisy target for learning. Our own work aims to reduce but not entirely eliminate the annotation effort involv"
J12-1005,H05-1049,0,0.0774029,"Missing"
J12-1005,P06-2057,0,0.161351,"surprising that previous efforts to reduce the need for semantic role annotation have focused primarily on languages other than English. Annotation projection is a popular framework for transferring semantic role annotations from one language to another while exploiting the translational and structural equivalences present in parallel corpora. The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language. Pado´ and Lapata (2009) transfer semantic role annotations from English onto German and Johansson and Nugues (2006) from English onto Swedish. A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates instantiating these concepts are found in a monolingual corpus and their arguments are labeled with FrameNet roles. Other work attempts to alleviate the data requirements for semantic role labeling within the same language either by increasing the coverage of existing resources or by inducing role annotations from unlabeled data"
J12-1005,W05-0618,0,0.0199108,"of the frame semantic annotations. We do this by ﬁnding an optimal alignment, that is, an alignment with the highest score as deﬁned in Equation (1). To solve this optimization problem efﬁciently, we recast it as an integer linear program (ILP). The ILP modeling framework has been recently applied to a wide range of natural language processing tasks, demonstrating improvements over more traditional optimization methods. Examples include reluctant paraphrasing (Dras 1999), relation extraction (Roth and tau Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006), sentence compression (Clarke and Lapata 2008), and coreference resolution (Denis and Baldridge 2007). Importantly, the ILP approach3 delivers a globally optimal solution by searching over the entire alignment space without employing heuristics or approximations (see de Marneffe et al. [2007] and Haghighi, Ng, and Manning [2005]). Furthermore, an ILP-based formulation seems well-suited to our problem because the domain of the optimization, namely, the set of partial injective functions from M to N, is discrete. We deﬁne a"
J12-1005,P09-1003,0,0.0824544,"Missing"
J12-1005,C04-1032,0,0.075057,"Missing"
J12-1005,J05-1004,0,0.164758,"Missing"
J12-1005,D08-1048,0,0.0823032,"Missing"
J12-1005,J08-2006,0,0.180781,"Missing"
J12-1005,C04-1197,0,0.0489308,"raphs are substantially similar to warrant projection of the frame semantic annotations. We do this by ﬁnding an optimal alignment, that is, an alignment with the highest score as deﬁned in Equation (1). To solve this optimization problem efﬁciently, we recast it as an integer linear program (ILP). The ILP modeling framework has been recently applied to a wide range of natural language processing tasks, demonstrating improvements over more traditional optimization methods. Examples include reluctant paraphrasing (Dras 1999), relation extraction (Roth and tau Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006), sentence compression (Clarke and Lapata 2008), and coreference resolution (Denis and Baldridge 2007). Importantly, the ILP approach3 delivers a globally optimal solution by searching over the entire alignment space without employing heuristics or approximations (see de Marneffe et al. [2007] and Haghighi, Ng, and Manning [2005]). Furthermore, an ILP-based formulation seems well-suited to our problem because the domain of the optimization, namely, the set of partial i"
J12-1005,W06-1616,0,0.0153909,"ent, that is, an alignment with the highest score as deﬁned in Equation (1). To solve this optimization problem efﬁciently, we recast it as an integer linear program (ILP). The ILP modeling framework has been recently applied to a wide range of natural language processing tasks, demonstrating improvements over more traditional optimization methods. Examples include reluctant paraphrasing (Dras 1999), relation extraction (Roth and tau Yih 2004), semantic role labeling (Punyakanok et al. 2004), concept-to-text generation (Marciniak and Strube 2005; Barzilay and Lapata 2006), dependency parsing (Riedel and Clarke 2006), sentence compression (Clarke and Lapata 2008), and coreference resolution (Denis and Baldridge 2007). Importantly, the ILP approach3 delivers a globally optimal solution by searching over the entire alignment space without employing heuristics or approximations (see de Marneffe et al. [2007] and Haghighi, Ng, and Manning [2005]). Furthermore, an ILP-based formulation seems well-suited to our problem because the domain of the optimization, namely, the set of partial injective functions from M to N, is discrete. We deﬁne arbitrary linear orders on the sets M and N, writing M = {n1 , . . . , nm"
J12-1005,W04-2401,0,0.060975,"Missing"
J12-1005,D07-1002,1,0.878829,"G RINDING frame, and slap the I MPACT frame. The creation of resources that document the realization of semantic roles in example sentences such as FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005) has greatly facilitated the development of learning algorithms capable of automatically analyzing the role semantic structure of input sentences. Moreover, the shallow semantic analysis produced by existing systems has been shown to beneﬁt a wide spectrum of applications ranging from information extraction (Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al. 2005). Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training. Supervised methods deliver reasonably good performance2 (F1 measures in the low 80s on standard test collections for English); however, the reliance on labeled training data, which is both difﬁcult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres. An"
J12-1005,P03-1002,0,0.0302255,"r instance, the verb crush may also evoke the G RINDING frame, and slap the I MPACT frame. The creation of resources that document the realization of semantic roles in example sentences such as FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005) has greatly facilitated the development of learning algorithms capable of automatically analyzing the role semantic structure of input sentences. Moreover, the shallow semantic analysis produced by existing systems has been shown to beneﬁt a wide spectrum of applications ranging from information extraction (Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al. 2005). Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training. Supervised methods deliver reasonably good performance2 (F1 measures in the low 80s on standard test collections for English); however, the reliance on labeled training data, which is both difﬁcult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling"
J12-1005,H05-1010,0,0.0425647,"Missing"
J12-1005,U06-1019,0,0.0306957,"od in a monolingual setting, and thus do not project annotations between languages but within the same language. Importantly, we acquire new training instances for both known and unknown predicates. Previous proposals extend FrameNet with novel predicates without inducing annotations that exemplify their usage. We represent labeled and unlabeled instances as graphs, and seek to ﬁnd a globally optimal alignment between their nodes, subject to semantic and structural constraints. Finding similar labeled and unlabeled sentences is reminiscent of paraphrase identiﬁcation (Qiu, Kan, and Chua 2006; Wan et al. 2006; Das and Smith 2009; Chang et al. 2010), the task of determining whether one sentence is a paraphrase of another. The sentences we identify are not strictly speaking paraphrases (even if the two predicates are similar their arguments often are not); however, the idea of modeling the correspondence structure (or alignment) between parts of the two sentences is also present in the paraphrase identiﬁcation work (Das and Smith 2009; Chang et al. 2010). Besides machine translation (Matusov, Zens, and Ney 2004; Taskar, Lacoste-Julien, and Klein 2005), methods based on graph alignments have been pre"
J12-1005,N09-2004,0,0.0244113,"e creation of resources that document the realization of semantic roles in example sentences such as FrameNet (Fillmore, Johnson, and Petruck 2003) and PropBank (Palmer, Gildea, and Kingsbury 2005) has greatly facilitated the development of learning algorithms capable of automatically analyzing the role semantic structure of input sentences. Moreover, the shallow semantic analysis produced by existing systems has been shown to beneﬁt a wide spectrum of applications ranging from information extraction (Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al. 2005). Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training. Supervised methods deliver reasonably good performance2 (F1 measures in the low 80s on standard test collections for English); however, the reliance on labeled training data, which is both difﬁcult and highly expensive to produce, presents a major obstacle to the widespread application of semantic role labeling across different languages and text genres. And although nowadays corpora with semantic r"
J12-1005,W04-3213,0,\N,Missing
J12-1005,N10-1066,0,\N,Missing
J12-1005,N10-1137,1,\N,Missing
J12-1005,N07-1070,0,\N,Missing
J12-1005,W06-1603,0,\N,Missing
J12-1005,J14-1002,0,\N,Missing
J14-3006,P10-1024,0,0.117573,"algorithm based on the Chinese Restaurant Process. In addition, they present a method that shares linking preferences across verbs using a distance-dependent Chinese Restaurant Process prior which encourages similar verbs to have similar linking preferences. Titov and Klementiev (2012b) further introduce the use of multilingual data for improving role induction. There has also been work on unsupervised methods for argument identification. Abend, Reichart, and Rappoport (2009) devise a method for recognizing the arguments of predicates that relies solely on part of speech annotations, whereas Abend and Rappoport (2010a) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. More generally, shallow semantic representations 637 Computational Linguistics Volume 40, Number 3 induced from syntactic information are commonly used in lexicon acquisition and information extraction tasks. For example, Lin and Pantel (2001) cluster syntactic relations between pairs of words as expressed by parse tree paths into semantic relations by exploiting lexical distributional similarity. Although not compatible with PropBank or semantic roles as such, Poon and Domingos (2009) and Ti"
J14-3006,P09-1004,0,0.279777,"Missing"
J14-3006,N10-1083,0,0.0704768,"Missing"
J14-3006,W06-3812,0,0.0717458,"y finding for each instance in one cluster the instance in the other cluster that is maximally similar or dissimilar and averaging over the scores of these alignments (avgmax) and (b) by using cosine similarity (see Section 4.1). We also report results for the single-layer algorithm proposed in Lang and Lapata (2011b).5 Given a verbal predicate, they construct a single-layer graph whose edge weights express instance-wise similarities directly. The graph is partitioned into vertex clusters representing semantic roles using a variant of Chinese Whispers, a graph clustering algorithm proposed by Biemann (2006). The algorithm iteratively assigns cluster labels to graph vertices by greedily choosing the most common label among the neighbors of the vertex being updated. Both agglomerative partitioning and multi-layered label propagation algorithms systematically achieve higher F1 scores than the baseline—that is, induce non-trivial clusterings and more adequate semantic roles (by attaining higher purity). For example, on the auto/auto data set, the agglomerative algorithm using cosine similarity 5 The results in Table 5 differ slightly from those published in Lang and Lapata (2011b). This is due to a"
J14-3006,J92-4003,0,0.207525,"Missing"
J14-3006,burchardt-etal-2006-salsa,0,0.0560732,"are marked with a grammatical case6 that directly indicates their grammatical function. Although in main declarative clauses the inflected part of the verb has to occur in second position, German is commonly 6 German has (partially ambiguous) markers for Nominative, Accusative, Dative, and Genitive. 660 Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning considered a verb-final language. This is because the verb often takes the final position in subordinate clauses, as do infinitive verbs (Brigitta 1996). 6.1 Data We conducted our experiments on the SALSA corpus (Burchardt et al. 2006), a lexical resource for German, which, like FrameNet for English, associates predicates with frames. SALSA is built as an extra annotation layer over the TIGER corpus (Brants et al. 2002), a treebank for German consisting of approximately 40,000 sentences (700,000 tokens) of newspaper text taken from the Frankfurter Rundschau, although to date not all predicate-argument structures have been annotated. The frame and role inventory of SALSA was taken from FrameNet, but has been extended and adapted where necessary due to lack of coverage and cross-lingual divergences. The syntactic structure of"
J14-3006,corston-oliver-gamon-2004-normalizing,0,0.0122191,"nguistic difference between the German and English data sets is the sparsity of the argument head lemmas, which is significantly higher for German than for English: In the CoNLL 2008 data set, the average number of distinct head lemmas per verb is only 3.69, whereas in the SALSA data set it is 20.12. This is partly due to the fact that the Wall Street Journal text underlying the English data is topically more focused than the Rundschau newspaper text, which covers a broader range of news beyond economics and politics. Moreover, noun compounding is more commonly used in German than in English (Corston-Oliver and Gamon 2004), which leads to higher lexical sparsity. Data sparsity affects our method, which crucially relies on lexical similarity for determining the role-equivalence of clusters. Therefore, we reduced the number of syntactic cues used for cluster initialization in order to avoid creating too many small clusters for which similarities cannot be reliably computed. Specifically, only the syntactic position and function word served as cues to initialize our clusters. Note that, as in English, the relatively small number of syntactic cues that determine the syntactic position within a linking is a conseque"
J14-3006,D09-1002,1,0.919291,"Missing"
J14-3006,J05-1005,0,0.0862585,"Missing"
J14-3006,P12-2029,0,0.275568,"rring the state of the latent variables representing the semantic roles of arguments. Following up on this work, Lang and Lapata (2010) reformulate role induction as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, because each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Garg and Henderson (2012) extend the latent-variable approach by modeling the sequential order of roles. The second approach is similarity-driven and based on clustering. Lang and Lapata (2011a) propose an algorithm that first splits the set of all argument instances of a verb according to their syntactic position within a particular linking and then iteratively merges clusters. A different clusstering algorithm is adopted in Lang and Lapata (2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex in the graph corresponds to an argument instance and edges represent a heuristically defined"
J14-3006,J02-3001,0,0.0800299,"aspect of argument-instance similarity, and we develop extensions of standard clustering algorithms for partitioning such multi-layer graphs. Experiments for English and German demonstrate that our approach is able to induce semantic role clusters that are consistently better than a strong baseline and are competitive with the state of the art. 1. Introduction Recent years have seen increased interest in the shallow semantic analysis of natural language text. The term is often used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky 2002). Semantic roles describe the relations that hold between a predicate and its arguments (e.g., “who” did “what” to “whom”, “when”, “where”, and “how”) abstracting over surface syntactic configurations. This type of semantic information ∗ Department of Computer Science, University of Geneva, 7 route de Drize, 1227 Carouge, Switzerland, E-mail: Joel.Lang@unige.ch. ∗∗ Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh, 10 Crichton Street, EH8 9AB, E-mail: mlap@inf.ed.ac.uk. Submission received: 26 December 2012; revised version received: 19 September"
J14-3006,P07-1025,0,0.0202357,"n supervised methods (M`arquez et al. 2008), although a few semi-supervised and unsupervised approaches 636 Lang and Lapata Similarity-Driven Semantic Role Induction via Graph Partitioning have been proposed. The majority of semi-supervised models have been developed within a framework known as annotation projection. The idea is to combine labeled and unlabeled data by projecting annotations from a labeled source sentence onto an unlabeled target sentence within the same language (Furstenau ¨ and Lapata 2009) or across different languages (Pado´ and Lapata 2009). Beyond annotation projection, Gordon and Swanson (2007) propose to increase the coverage of PropBank to unseen verbs by finding syntactically similar (labeled) verbs and using their annotations as surrogate training data. Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system. Their algorithm induces role labels following a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method starts with a data set containing no role annotations at all, but crucially relies on VerbNet (Kipper, Dang, and Palmer 2000) for"
J14-3006,W06-1601,0,0.826556,"Missing"
J14-3006,W09-1201,0,0.115981,"Missing"
J14-3006,W05-0625,0,0.0163727,"ng rule is applied. We will exemplify how the argument identification component works for the predicate expect in the sentence The company said it expects its sales to remain steady whose parse tree is shown in Figure 2. Initially, all words except the predicate itself are treated as argument candidates. Then, the rules from Table 1 are applied as follows. Firstly, the words the and to are discarded based on their part of speech (Rule 1); then, remain is discarded because the path ends with the relation IM and said is discarded as the 4 A few supervised systems implement a similar definition (Koomen et al. 2005), although in most cases the argument identification component makes a final positive or negative decision regarding the status of an argument candidate. 649 Computational Linguistics Volume 40, Number 3 path ends with an upward-leading OBJ relation (Rule 2). Rule 3 matches to it, which is therefore added as a candidate. Next, steady is discarded because there is a downwardleading OPRD relation along the path and the words company and its are also discarded because of the OBJ relations along the path (Rule 4). Rule 5 does not apply but the word sales is kept as a likely argument (Rule 6). Fina"
J14-3006,N10-1137,1,0.913067,"ications. In this article we will not assume the availability of any role-semantic resources, although we do assume that sentences are syntactically analyzed. There have been two main approaches to role induction from parsed data. Under the first approach, semantic roles are modeled as latent variables in a (directed) graphical model that relates a verb, its semantic roles, and their possible syntactic realizations (Grenager and Manning 2006). Role induction here corresponds to inferring the state of the latent variables representing the semantic roles of arguments. Following up on this work, Lang and Lapata (2010) reformulate role induction as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, because each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Garg and Henderson (2012) extend the latent-variable approach by modeling the sequential order of roles. The second approach is similar"
J14-3006,P11-1112,1,0.816713,"rocess of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, because each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Garg and Henderson (2012) extend the latent-variable approach by modeling the sequential order of roles. The second approach is similarity-driven and based on clustering. Lang and Lapata (2011a) propose an algorithm that first splits the set of all argument instances of a verb according to their syntactic position within a particular linking and then iteratively merges clusters. A different clusstering algorithm is adopted in Lang and Lapata (2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex in the graph corresponds to an argument instance and edges represent a heuristically defined measure of their lexical and syntactic similarity. The similarity-driven approach has been recently adopted by Titov and Klementiev (2012a), who propose a Bayesian clu"
J14-3006,D11-1122,1,0.780106,"rocess of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, because each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that takes advantage of the close relationship between syntactic functions and semantic roles. More recently, Garg and Henderson (2012) extend the latent-variable approach by modeling the sequential order of roles. The second approach is similarity-driven and based on clustering. Lang and Lapata (2011a) propose an algorithm that first splits the set of all argument instances of a verb according to their syntactic position within a particular linking and then iteratively merges clusters. A different clusstering algorithm is adopted in Lang and Lapata (2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex in the graph corresponds to an argument instance and edges represent a heuristically defined measure of their lexical and syntactic similarity. The similarity-driven approach has been recently adopted by Titov and Klementiev (2012a), who propose a Bayesian clu"
J14-3006,J93-2004,0,0.058578,"Missing"
J14-3006,J08-2001,0,0.601588,"Missing"
J14-3006,J01-3003,0,0.0225481,"mbine a part-of-speech tagger and an unsupervised parser in order to identify constituents. Likely arguments can be in turn identified based on a set of rules and the degree of collocation with the predicate. Perhaps unsurprisingly, this method does not match the quality of a rule-based component operating over trees produced by a supervised parser. 5.3 Baseline Method for Semantic Role Induction The linking between semantic roles and syntactic positions is not arbitrary; specific semantic roles tend to map onto specific syntactic positions such as subject or object (Levin and Rappaport 2005; Merlo and Stevenson 2001). We further illustrate this observation in Table 2, which shows how often individual semantic roles map onto certain syntactic positions. The latter are simply defined as the relations governing the argument. The frequencies in the table were obtained from the CoNLL 2008 data set and are aggregates across predicates. As can be seen, semantic roles often approximately correspond to a single syntactic position. For example, A0 is commonly mapped onto subject (SBJ), whereas A1 is often realized as object (OBJ). This motivates a baseline that directly assigns instances to clusters according to th"
J14-3006,J05-1004,0,0.470736,"Missing"
J14-3006,D09-1001,0,0.0535959,"Missing"
J14-3006,J08-2006,0,0.55863,"Missing"
J14-3006,D07-1002,1,0.611982,"26 December 2012; revised version received: 19 September 2013; accepted for publication: 20 November 2013. doi:10.1162/COLI a 00195 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 is shallow but relatively straightforward to infer automatically and useful for the development of broad-coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al. 2005). In the example sentences below, window occupies different syntactic positions—it is the object of broke in sentences (1a,b), and the subject in (1c). In all instances, it bears the same semantic role, that is, the patient or physical object affected by the breaking event. Analogously, ball is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. [Jim]A0 broke the [window]A1 with a [ball]A2 . b. The [ball]A2 broke the [window]A1 . c. The [window]A1 broke [la"
J14-3006,W08-2121,0,0.0916965,"Missing"
J14-3006,P11-1145,0,0.0728952,"10a) distinguish between core and adjunct roles, using an unsupervised parser and part-of-speech tagger. More generally, shallow semantic representations 637 Computational Linguistics Volume 40, Number 3 induced from syntactic information are commonly used in lexicon acquisition and information extraction tasks. For example, Lin and Pantel (2001) cluster syntactic relations between pairs of words as expressed by parse tree paths into semantic relations by exploiting lexical distributional similarity. Although not compatible with PropBank or semantic roles as such, Poon and Domingos (2009) and Titov and Klementiev (2011) also induce semantic information from dependency parses and apply it to a question answering task for the biomedical domain. Another example is the work by Gamallo, Agustini, and Lopes (2005), who cluster similar syntactic positions in order to develop models of selectional preferences to be used for word sense induction and the resolution of attachment ambiguities. The work described here unifies the two clustering methods presented in Lang and Lapata (2011a and 2011b) by reformulating them as graph partitioning algorithms. It also extends them by utilizing multi-layer graphs which separate"
J14-3006,E12-1003,0,0.558936,"ty-driven and based on clustering. Lang and Lapata (2011a) propose an algorithm that first splits the set of all argument instances of a verb according to their syntactic position within a particular linking and then iteratively merges clusters. A different clusstering algorithm is adopted in Lang and Lapata (2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex in the graph corresponds to an argument instance and edges represent a heuristically defined measure of their lexical and syntactic similarity. The similarity-driven approach has been recently adopted by Titov and Klementiev (2012a), who propose a Bayesian clustering algorithm based on the Chinese Restaurant Process. In addition, they present a method that shares linking preferences across verbs using a distance-dependent Chinese Restaurant Process prior which encourages similar verbs to have similar linking preferences. Titov and Klementiev (2012b) further introduce the use of multilingual data for improving role induction. There has also been work on unsupervised methods for argument identification. Abend, Reichart, and Rappoport (2009) devise a method for recognizing the arguments of predicates that relies solely on"
J14-3006,P12-1068,0,0.667232,"ty-driven and based on clustering. Lang and Lapata (2011a) propose an algorithm that first splits the set of all argument instances of a verb according to their syntactic position within a particular linking and then iteratively merges clusters. A different clusstering algorithm is adopted in Lang and Lapata (2011b). Specifically, they induce semantic roles via graph partitioning: Each vertex in the graph corresponds to an argument instance and edges represent a heuristically defined measure of their lexical and syntactic similarity. The similarity-driven approach has been recently adopted by Titov and Klementiev (2012a), who propose a Bayesian clustering algorithm based on the Chinese Restaurant Process. In addition, they present a method that shares linking preferences across verbs using a distance-dependent Chinese Restaurant Process prior which encourages similar verbs to have similar linking preferences. Titov and Klementiev (2012b) further introduce the use of multilingual data for improving role induction. There has also been work on unsupervised methods for argument identification. Abend, Reichart, and Rappoport (2009) devise a method for recognizing the arguments of predicates that relies solely on"
J14-3006,N09-2004,0,0.0106398,"September 2013; accepted for publication: 20 November 2013. doi:10.1162/COLI a 00195 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 is shallow but relatively straightforward to infer automatically and useful for the development of broad-coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al. 2003) and question answering (Shen and Lapata 2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al. 2005). In the example sentences below, window occupies different syntactic positions—it is the object of broke in sentences (1a,b), and the subject in (1c). In all instances, it bears the same semantic role, that is, the patient or physical object affected by the breaking event. Analogously, ball is the instrument of break both when realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. [Jim]A0 broke the [window]A1 with a [ball]A2 . b. The [ball]A2 broke the [window]A1 . c. The [window]A1 broke [last night]TMP . Also notice that all three i"
J14-3006,W04-3213,0,\N,Missing
J14-3006,N07-1070,0,\N,Missing
J14-3006,P03-1002,0,\N,Missing
J19-1002,D11-1039,0,0.0605649,"Missing"
J19-1002,Q13-1005,0,0.152077,"ck Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures"
J19-1002,P98-1013,0,0.385819,"Missing"
J19-1002,W13-2322,0,0.0304597,"nd NashWebber 1972). The literature is rife with semantic formalisms that can be used to define logical forms. Examples include lambda calculus (Montague 1973), which has been used by many semantic parsers (Zettlemoyer and Collins 2005; Kwiatkowksi et al. 2010; Reddy, Lapata, and Steedman 2014) because of its expressiveness and flexibility to construct logical forms of great complexity; Combinatory Categorial Grammar (Steedman 2000); dependency-based compositional semantics (Liang, Jordan, and Klein 2011); frame semantics (Baker, Fillmore, and Lowe 1998); and abstract meaning representations (Banarescu et al. 2013). In this work, we adopt a database querying language as the semantic formalism, namely, the functional query language (FunQL; Zelle 1995). FunQL maps first-order logical forms into function-argument structures, resulting in recursive, tree-structured, 61 Computational Linguistics Volume 45, Number 1 program representations. Although it lacks expressive power, FunQL has a modeling advantage for downstream tasks, because it is more natural to describe the manipulation of a simple world as procedural programs. This modeling advantage has been revealed in recent advances of neural programmings: R"
J19-1002,D13-1160,0,0.148103,"vel features because the dynamic programming algorithm requires features defined over substructures. In comparison, our linear-time parser allows us to generate parse structures incrementally conditioned on the entire sentence. We perform several experiments in downstream question-answering tasks and demonstrate the effectiveness of our approach across different training scenarios. These include full supervision with questions paired with annotated logical forms using the G EO Q UERY (Zettlemoyer and Collins 2005) data set, weak supervision with questionanswer pairs using the W EB Q UESTIONS (Berant et al. 2013a) and G RAPH Q UESTIONS (Su et al. 2016) data sets, and distant supervision without question-answer pairs, using the SPADES (Bisk et al. 2016) data set. Experimental results show that our neural semantic parser is able to generate high-quality logical forms and answer real-world questions on a wide range of domains. The remainder of this article is structured as follows. Section 2 provides an overview of related work. Section 3 introduces our neural semantic parsing framework and discusses the various training scenarios to which it can be applied. Our experiments are described in Section 4, t"
J19-1002,P14-1133,0,0.0824315,"Missing"
J19-1002,Q15-1039,0,0.0140481,"ed with an attention mechanism (Dong and Lapata 2016). On W EB Q UESTIONS, the best performing TNSP system generates logical forms based on top–down pre-order while using soft attention. The same top–down system with structured attention performs closely. Again we observe that bottom–up preorder lags behind. In general, our semantic parser obtains performance on par with the best symbolic systems (see the first block in Table 7a). It is important to note that Bast and Haussmann (2015) develop a question-answering system, which, contrary to ours, cannot produce meaning representations, whereas Berant and Liang (2015) propose a sophisticated agenda-based parser that is trained borrowing ideas from imitation learning. Reddy et al. (2016) learn a semantic parser via intermediate representations that they generate based on the output of a dependency parser. TNSP performs competitively despite not having access to linguistically informed syntactic structure. Regarding neural systems (see the second block in Table 7a), our model outperforms the sequenceto-sequence baseline and other related neural architectures using similar resources. Xu et al. (2016) represent the state of the art on WEBQUESTIONS. Their syste"
J19-1002,D16-1214,1,0.899725,"Missing"
J19-1002,D14-1067,0,0.0290169,"Missing"
J19-1002,P13-1042,0,0.188191,"t al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata 2016;"
J19-1002,D16-1053,1,0.844363,"Missing"
J19-1002,P17-2019,1,0.92724,"rules in a grammar to obtain well-typed logical forms. Rabinovich, Stern, and Klein (2017) propose abstract syntax networks with a modular decoder, whose multiple submodels (one per grammar construct) are composed to generate abstract syntax trees in a top– down manner. Our work shares similar motivation: We generate tree-structured, syntactically valid logical forms, but following a transition-based generation approach (Dyer et al. 2015, 62 Cheng, Reddy, Saraswat, and Lapata Learning an Executable Neural Semantic Parser 2016). Our semantic parser is a generalization of the model presented in Cheng et al. (2017). Whereas they focus solely on top–down generation using hard attention, the parser presented in this work generates logical forms following either a top–down or bottom–up generation order and introduces additional attention mechanisms (i.e., soft and structured attention) for handling mismatches between natural language and logical form tokens. We empirically compare generation orders and attention variants, elaborate on model details, and formalize how the neural parser can be effectively trained under different types of supervision. Training Regimes. Various types of supervision have been e"
J19-1002,P17-1005,1,0.730352,"rules in a grammar to obtain well-typed logical forms. Rabinovich, Stern, and Klein (2017) propose abstract syntax networks with a modular decoder, whose multiple submodels (one per grammar construct) are composed to generate abstract syntax trees in a top– down manner. Our work shares similar motivation: We generate tree-structured, syntactically valid logical forms, but following a transition-based generation approach (Dyer et al. 2015, 62 Cheng, Reddy, Saraswat, and Lapata Learning an Executable Neural Semantic Parser 2016). Our semantic parser is a generalization of the model presented in Cheng et al. (2017). Whereas they focus solely on top–down generation using hard attention, the parser presented in this work generates logical forms following either a top–down or bottom–up generation order and introduces additional attention mechanisms (i.e., soft and structured attention) for handling mismatches between natural language and logical form tokens. We empirically compare generation orders and attention variants, elaborate on model details, and formalize how the neural parser can be effectively trained under different types of supervision. Training Regimes. Various types of supervision have been e"
J19-1002,W10-2903,0,0.162083,"12), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence tra"
J19-1002,P16-1004,1,0.856912,"2; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata 2016; Jia and Liang 2016; Koˇcisky´ et al. 2016). Neural semantic parsers generate a sentence in linear time, while reducing the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost because it is no longer possible to interpret how meaning composition is performed, given that logical forms are structured objects like trees or graphs. Such knowledge plays a critical role in understanding modeling limitations so as to build better semantic parsers. Moreover, without any task-specific knowledge, the lea"
J19-1002,D17-1091,1,0.851895,"neous candidate answers extracted from Freebase. Our model would also benefit from a similar post-processing. With respect to G RAPH Q UESTIONS, we report F1 for various TNSP models (third block in Table 7), and conventional statistical semantic parsers (first block in Table 7b). The first three systems are presented in Su et al. (2016). Again, we observe that a top– down variant of TNSP with soft attention performs best. It is superior to the sequence-tosequence baseline and obtains performance comparable to Reddy et al. (2017) without making use of an external syntactic parser. The model of Dong et al. (2017) is state of the art on G RAPH Q UESTIONS. Their method is trained end-to-end using question-answer pairs as a supervision signal together with question paraphrases as a means of capturing different ways of expressing the same content. Importantly, their system is optimized with question answering in mind, and does not produce logical forms. When learning from denotations, a challenge concerns the handling of an exponentially large set of logical forms. In our approach, we rely on the neural semantic parser to generate a list of candidate logical forms by beam search. Ideally, we hope the beam"
J19-1002,P15-1026,0,0.0415523,"Missing"
J19-1002,P15-1033,0,0.111119,"bstract syntax trees for source code with a grammar-constrained neural decoder. Krishnamurthy, Dasigi, and Gardner (2017) also introduce a neural semantic parser that decodes rules in a grammar to obtain well-typed logical forms. Rabinovich, Stern, and Klein (2017) propose abstract syntax networks with a modular decoder, whose multiple submodels (one per grammar construct) are composed to generate abstract syntax trees in a top– down manner. Our work shares similar motivation: We generate tree-structured, syntactically valid logical forms, but following a transition-based generation approach (Dyer et al. 2015, 62 Cheng, Reddy, Saraswat, and Lapata Learning an Executable Neural Semantic Parser 2016). Our semantic parser is a generalization of the model presented in Cheng et al. (2017). Whereas they focus solely on top–down generation using hard attention, the parser presented in this work generates logical forms following either a top–down or bottom–up generation order and introduces additional attention mechanisms (i.e., soft and structured attention) for handling mismatches between natural language and logical form tokens. We empirically compare generation orders and attention variants, elaborate"
J19-1002,N16-1024,0,0.0623422,"Missing"
J19-1002,W17-2607,0,0.0164948,"rks have been shown to be extremely useful in context modeling and sequence generation (Bahdanau, Cho, and Bengio 2015). Following this direction, Dong and Lapata (2016) and Jia and Liang (2016) have developed neural semantic parsers that treat semantic parsing as a sequence to a sequence learning problem. Jia and Liang (2016) also introduce a data augmentation approach that bootstraps a synchronous grammar from existing data and generates artificial examples as extra training data. Other related work extends the vanilla sequence-to-sequence model in various ways, such as multi-task learning (Fan et al. 2017), parsing cross-domain queries (Herzig and Berant 2017) and context-dependent queries (Suhr, Iyer, and Artzi 2018), and applying the model to other formalisms such as AMR (Konstas et al. 2017) and SQL (Zhong, Xiong, and Socher 2017). The fact that logical forms have a syntactic structure has motivated some of the recent work on exploring structured neural decoders to generate tree or graph structures and grammar-constrained decoders to ensure the outputs are meaningful and executable. Related work includes Yin and Neubig (2017), who generate abstract syntax trees for source code with a grammar"
J19-1002,P17-2098,0,0.0239303,"ntext modeling and sequence generation (Bahdanau, Cho, and Bengio 2015). Following this direction, Dong and Lapata (2016) and Jia and Liang (2016) have developed neural semantic parsers that treat semantic parsing as a sequence to a sequence learning problem. Jia and Liang (2016) also introduce a data augmentation approach that bootstraps a synchronous grammar from existing data and generates artificial examples as extra training data. Other related work extends the vanilla sequence-to-sequence model in various ways, such as multi-task learning (Fan et al. 2017), parsing cross-domain queries (Herzig and Berant 2017) and context-dependent queries (Suhr, Iyer, and Artzi 2018), and applying the model to other formalisms such as AMR (Konstas et al. 2017) and SQL (Zhong, Xiong, and Socher 2017). The fact that logical forms have a syntactic structure has motivated some of the recent work on exploring structured neural decoders to generate tree or graph structures and grammar-constrained decoders to ensure the outputs are meaningful and executable. Related work includes Yin and Neubig (2017), who generate abstract syntax trees for source code with a grammar-constrained neural decoder. Krishnamurthy, Dasigi, and"
J19-1002,P17-1089,0,0.15664,"s. Although it lacks expressive power, FunQL has a modeling advantage for downstream tasks, because it is more natural to describe the manipulation of a simple world as procedural programs. This modeling advantage has been revealed in recent advances of neural programmings: Recurrent neural networks have demonstrated great capability in inducing compositional programs (Neelakantan, Le, and Sutskever 2016; Reed and De Freitas 2016; Cai, Shin, and Song 2017). For example, they learn to perform grade-school additions, bubble sort, and table comprehension in procedures. Finally, some recent work (Iyer et al. 2017; Yin and Neubig 2017; Zhong, Xiong, and Socher 2017) uses other programming languages, such as the SQL, as the semantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, th"
J19-1002,P16-1002,0,0.512797,". Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata 2016; Jia and Liang 2016; Koˇcisky´ et al. 2016). Neural semantic parsers generate a sentence in linear time, while reducing the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost because it is no longer possible to interpret how meaning composition is performed, given that logical forms are structured objects like trees or graphs. Such knowledge plays a critical role in understanding modeling limitations so as to build better semantic parsers. Moreover, without any task-specific knowledge, the learning problem is fai"
J19-1002,P06-1115,0,0.609097,"and Socher 2017) uses other programming languages, such as the SQL, as the semantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support vector machine (Kate and Mooney 2006), a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008; Reddy, Lapata, and Steedman 2014; Reddy et al. 2016), or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a)—scores the set of candidate derivations generated from the grammar. During inference, a chart-based parsing algorithm is commonly used to predict the most likely semantic parse for a sentence. With recent advances in neural networks and deep learning, there is a trend of reformulating semantic parsing as a machine translation problem. The idea is not novel, because semantic parsing has been previ"
J19-1002,P17-1014,0,0.0295639,"(2016) have developed neural semantic parsers that treat semantic parsing as a sequence to a sequence learning problem. Jia and Liang (2016) also introduce a data augmentation approach that bootstraps a synchronous grammar from existing data and generates artificial examples as extra training data. Other related work extends the vanilla sequence-to-sequence model in various ways, such as multi-task learning (Fan et al. 2017), parsing cross-domain queries (Herzig and Berant 2017) and context-dependent queries (Suhr, Iyer, and Artzi 2018), and applying the model to other formalisms such as AMR (Konstas et al. 2017) and SQL (Zhong, Xiong, and Socher 2017). The fact that logical forms have a syntactic structure has motivated some of the recent work on exploring structured neural decoders to generate tree or graph structures and grammar-constrained decoders to ensure the outputs are meaningful and executable. Related work includes Yin and Neubig (2017), who generate abstract syntax trees for source code with a grammar-constrained neural decoder. Krishnamurthy, Dasigi, and Gardner (2017) also introduce a neural semantic parser that decodes rules in a grammar to obtain well-typed logical forms. Rabinovich, S"
J19-1002,D16-1116,0,0.0892223,"Missing"
J19-1002,D17-1160,0,0.181076,"Missing"
J19-1002,D12-1069,0,0.173391,"sented by the logical form longest(and(type.river, location(Ohio))), which when executed against a database of US geography returns the answer Ohio River. In the second example, the logical form count(daughterOf(Barack Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Cla"
J19-1002,P14-1112,0,0.0333867,"Missing"
J19-1002,E17-1117,0,0.0253808,"ively pops the stack-LSTM states as well as corresponding tree tokens on the output stack. The popping stops when a non-terminal state is reached and popped, after which the stack-LSTM reaches an intermediate state st−1:t .1 The representation of the completed subtree u is then computed as u = Wu · [pu : cu ] (15) where pu denotes the parent (non-terminal) embedding of the subtree, cu denotes the average of the children (terminal or completed subtree) embeddings, and Wu denotes the weight matrix. Note that cu can also be computed with more advanced methods, such as a recurrent neural network (Kuncoro et al. 2017). Finally, the subtree embedding u serves as the input to the LSTM and updates st−1:t to st as st = LSTM(u, st−1:t ) (16) Figure 1 provides a graphical view on how the three operations change the configuration of a stack-LSTM. In comparison, the bottom–up transition system uses the same TER operation to update the stack-LSTM representation st when a terminal yt is newly generated: st = LSTM(yt , st−1 ) (17) Differently, the effects of NT and RED are merged into a NT-RED(X) operation. When NT-RED(X) is invoked, a non-terminal yt is first predicted and then the stack-LSTM starts popping its stat"
J19-1002,D10-1119,0,0.605108,"swer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is common"
J19-1002,D13-1161,0,0.343444,"ialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart parsing algorithm is commonly used to parse a sentence in polynomial time. The successful application of recurrent neural networks (Sutskever, Vinyals, and Le 2014; Bahdanau, Cho, and Bengio 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where a"
J19-1002,D11-1140,0,0.0314212,"forms, and denotations. The query What is the longest river in Ohio? is represented by the logical form longest(and(type.river, location(Ohio))), which when executed against a database of US geography returns the answer Ohio River. In the second example, the logical form count(daughterOf(Barack Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to al"
J19-1002,P17-1003,0,0.0597375,"eak supervision from utterance-denotation pairs (Clarke et al. 2010; Liang, Jordan, and Klein 2011; Berant et al. 2013a; Kwiatkowski et al. 2013; Pasupat and Liang 2015). The approach enables more efficient data collection, since denotations (such as answers to a question, responses to a system) are much easier to obtain via crowdsourcing. For this reason, semantic parsing can be scaled to handle large, complex, and open domain problems. Examples include learning semantic parsers from question-answer pairs on Freebase (Liang, Jordan, and Klein 2011; Berant et al. 2013a; Berant and Liang 2014; Liang et al. 2017; Cheng et al. 2017), from system feedbacks (Clarke et al. 2010; Chen and Mooney 2011; Artzi and Zettlemoyer 2013), from abstract examples (Goldman et al. 2018), and from human feedbacks (Iyer et al. 2017) or statements (Artzi and Zettlemoyer 2011). Some work seeks more clever ways of gathering data and trains semantic parsers with even weaker supervision. In a class of distant supervision methods, the input is solely a knowledge base and a corpus of unlabeled sentences. Artificial training data are generated from the given resources. For example, Cai and Yates (2013) generate utterances paire"
J19-1002,J13-2005,0,0.018296,"rance is labeled with annotated logical forms, a weakly supervised setting where utterance-denotation pairs are available, and a distant-supervision setting where only a collection of unlabeled sentences and a knowledge base is given. 3.1 FunQL Semantic Representation As mentioned earlier, we adopt FunQL as our semantic formalism. FunQL is a variablefree recursive meaning representation language that maps simple first-order logical forms to function-argument structures that abstract away from variables and quantifiers (Kate and Mooney 2006). The language is also closely related to lambda DCS (Liang 2013), which makes existential quantifiers implicit. Lambda DCS is more compact in the sense that it can use variables in rare cases to handle anaphora and build composite binary predicates. The FunQL logical forms we define contain the following primitive functional operators. They overlap with simple lambda DCS (Berant et al. 2013a) but differ slightly in syntax to ease recursive generation of logical forms. Let l denote a logical form, JlK represent its denotation, and K refer to a knowledge base. • Unary base case: An entity e (e.g., Barack Obama) is a unary logical form whose denotation is a s"
J19-1002,P11-1060,0,0.303228,"Missing"
J19-1002,Q18-1005,1,0.836676,"attention for predicting InfluentialTeensByYear. Darker shading indicates higher values. which outputs the parameters of the multinomial distribution over logical form tokens (either predicates or entities). When dealing with extremely large knowledge bases, the output space can be pruned and restricted with an entity linking procedure. This method requires us to identity potential entity candidates in the sentence, and then generate only entities belonging to this subset and the relations linking them. Structured Soft Attention. We also explored a structured attention layer (Kim et al. 2017; Liu and Lapata 2018) to encourage the model to attend to contiguous natural language phrases when generating a logical token, while still being differentiable. The structured attention layer we adopt is a linear-chain conditional random field (Lafferty, Mccallum, and Pereira 2001). Assume that at time step t each token in the buffer (e.g., the ith token) is assigned an attention label Ait ∈ {0, 1}. The conditional random field defines p(At ), the probability of the sequence of attention labels at time step t as p(At ) = P where P · ψ(Ati−1 , Ait , bi , st ) P i −1 i i Wf · ψ (At , At , bi , st ) A1t ,··· ,Ant exp"
J19-1002,D08-1082,0,0.188897,"mantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support vector machine (Kate and Mooney 2006), a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008; Reddy, Lapata, and Steedman 2014; Reddy et al. 2016), or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a)—scores the set of candidate derivations generated from the grammar. During inference, a chart-based parsing algorithm is commonly used to predict the most likely semantic parse for a sentence. With recent advances in neural networks and deep learning, there is a trend of reformulating semantic parsing as a machine translation problem. The idea is not novel, because semantic parsing has been previously studied with statistical machine translation approaches in both W"
J19-1002,P15-1142,0,0.0771755,"Missing"
J19-1002,D14-1162,0,0.0823538,"Missing"
J19-1002,C04-1021,0,0.0678008,"base of US geography returns the answer Ohio River. In the second example, the logical form count(daughterOf(Barack Obama)) corresponds to the query How many daughters does Obama have? and is executed against the Freebase knowledge base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishn"
J19-1002,P17-1105,0,0.361335,"Missing"
J19-1002,Q14-1030,1,0.87593,"Missing"
J19-1002,Q16-1010,1,0.595148,"lem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support vector machine (Kate and Mooney 2006), a structured perceptron (Zettlemoyer and Collins 2007; Lu et al. 2008; Reddy, Lapata, and Steedman 2014; Reddy et al. 2016), or a log-linear model (Zettlemoyer and Collins 2005; Berant et al. 2013a)—scores the set of candidate derivations generated from the grammar. During inference, a chart-based parsing algorithm is commonly used to predict the most likely semantic parse for a sentence. With recent advances in neural networks and deep learning, there is a trend of reformulating semantic parsing as a machine translation problem. The idea is not novel, because semantic parsing has been previously studied with statistical machine translation approaches in both Wong and Mooney (2006) and Andreas, Vlachos, and Clark"
J19-1002,D17-1009,1,0.889912,"Missing"
J19-1002,D16-1054,0,0.122317,"algorithm requires features defined over substructures. In comparison, our linear-time parser allows us to generate parse structures incrementally conditioned on the entire sentence. We perform several experiments in downstream question-answering tasks and demonstrate the effectiveness of our approach across different training scenarios. These include full supervision with questions paired with annotated logical forms using the G EO Q UERY (Zettlemoyer and Collins 2005) data set, weak supervision with questionanswer pairs using the W EB Q UESTIONS (Berant et al. 2013a) and G RAPH Q UESTIONS (Su et al. 2016) data sets, and distant supervision without question-answer pairs, using the SPADES (Bisk et al. 2016) data set. Experimental results show that our neural semantic parser is able to generate high-quality logical forms and answer real-world questions on a wide range of domains. The remainder of this article is structured as follows. Section 2 provides an overview of related work. Section 3 introduces our neural semantic parsing framework and discusses the various training scenarios to which it can be applied. Our experiments are described in Section 4, together with detailed analysis of system"
J19-1002,D15-1199,0,0.0626104,"Missing"
J19-1002,H89-1033,0,0.522124,"usses the various training scenarios to which it can be applied. Our experiments are described in Section 4, together with detailed analysis of system output. Discussion of future work concludes in Section 5. 2. Related Work The proposed framework has connections to several lines of research, including various formalisms for representing natural language meaning, semantic parsing models, and the training regimes they adopt. We review related work in these areas here. Semantic Formalism. Logical forms have played an important role in semantic parsing systems since their inception in the 1970s (Winograd 1972; Woods, Kaplan, and NashWebber 1972). The literature is rife with semantic formalisms that can be used to define logical forms. Examples include lambda calculus (Montague 1973), which has been used by many semantic parsers (Zettlemoyer and Collins 2005; Kwiatkowksi et al. 2010; Reddy, Lapata, and Steedman 2014) because of its expressiveness and flexibility to construct logical forms of great complexity; Combinatory Categorial Grammar (Steedman 2000); dependency-based compositional semantics (Liang, Jordan, and Klein 2011); frame semantics (Baker, Fillmore, and Lowe 1998); and abstract meaning"
J19-1002,N06-1056,0,0.686446,"base to return the answer 2. In recent years, semantic parsing has attracted a great deal of attention because of its utility in a wide range of applications, such as question answering (Kwiatkowski et al. 2011; Liang, Jordan, and Klein 2011), relation extraction (Krishnamurthy and Mitchell 2012), goal-oriented dialog (Wen et al. 2015), natural language interfaces (Popescu et al. 2004), robot control (Matuszek et al. 2012), and interpreting instructions (Chen and Mooney 2011; Artzi and Zettlemoyer 2013). Early statistical semantic parsers (Zelle and Mooney 1996; Zettlemoyer and Collins 2005; Wong and Mooney 2006; Kwiatkowksi et al. 2010) mostly require training data in the form of utterances paired with annotated logical forms. More recently, alternative forms of supervision have been proposed to alleviate the annotation burden— for example, training on utterance-denotation pairs (Clarke et al. 2010; Kwiatkowski et al. 2013; Liang 2016), or using distant supervision (Krishnamurthy and Mitchell 2012; Cai and Yates 2013). Despite different supervision signals, training and inference procedures in conventional semantic parsers rely largely on domain-specific grammars and engineering. A CKY-style chart p"
J19-1002,P16-1220,1,0.906697,"tterance (excluding stop-words) and the logical form, the token overlap count between the two, and also similar features between the lemmatized utterance and the logical form. In addition, we include as features the embedding cosine similarity 81 Computational Linguistics Volume 45, Number 1 between the question words and the logical form, the similarity between the question words (e.g., what, who, where, whose, date, which, how many, count ) and relations in the logical form, and the similarity between the question words and answer type as indicated by the last word in the Freebase relation (Xu et al. 2016). Finally, we add as a feature the length of the denotation given by the logical form (Berant et al. 2013a). 4.3 Results In this section, we present the experimental results of our Transition-based Neural Semantic Parser (TNSP). We present various instantiations of our own model as well as comparisons against semantic parsers proposed in the literature. Experimental results on G EO Q UERY are shown in Table 6. The first block contains conventional statistical semantic parsers, previously proposed neural models are presented in the second block, and variants of TNSP are shown in the third block"
J19-1002,P14-1090,0,0.038961,"Missing"
J19-1002,P15-1128,0,0.058584,"Missing"
J19-1002,P17-1041,0,0.226018,"ks expressive power, FunQL has a modeling advantage for downstream tasks, because it is more natural to describe the manipulation of a simple world as procedural programs. This modeling advantage has been revealed in recent advances of neural programmings: Recurrent neural networks have demonstrated great capability in inducing compositional programs (Neelakantan, Le, and Sutskever 2016; Reed and De Freitas 2016; Cai, Shin, and Song 2017). For example, they learn to perform grade-school additions, bubble sort, and table comprehension in procedures. Finally, some recent work (Iyer et al. 2017; Yin and Neubig 2017; Zhong, Xiong, and Socher 2017) uses other programming languages, such as the SQL, as the semantic formalism. Semantic Parsing Model. The problem of learning to map utterances to meaning representations has been studied extensively in the NLP community. Most data-driven semantic parsers consist of three key components: a grammar, a trainable model, and a parsing algorithm. The grammar defines the space of derivations from sentences to logical forms, and the model together with the parsing algorithm find the most likely derivation. The model—which can take, for example, the form of a support v"
J19-1002,D07-1071,0,0.283917,"Missing"
J19-1002,N15-1162,0,0.096033,"Missing"
K18-1035,P17-1005,1,0.210802,"scheduled training scheme which balances the contribution of the two components and objectives. To further boost performance, we propose to neurally encode a lexicon, as a means of injecting prior domain knowledge to the neural parameters. We evaluate our system on three Freebase datasets which consist of utterance denotation pairs: W EB Q UESTIONS (Berant et al., 2013a), G RAPH Q UESTIONS (Su et al., 2016), and S PADES (Bisk et al., 2016). Experimental results across datasets show that our weakly-supervised semantic parser achieves state-of-the-art performance. 2 2.1 Parser Our work follows Cheng et al. (2017b, 2018) in using LISP-style functional queries as the logical formulation. Advantageously, functional queries are recursive, tree-structured and can naturally encode logical form derivations (i.e., functions and their application order). For example, the utterance “who is obama’s eldest daughter” is simply represented with the function-argument structure argmax(daughterOf(Obama), ageOf). Table 1 displays the functions we use in this work; a more detailed specifications can be found in the appendix. To generate logical forms, our system adopts a variant of the neural sequence-to-tree model pro"
K18-1035,D13-1160,0,0.14778,"ly-supervised neural semantic parsing system. 2006; Kwiatkowksi et al., 2010). However, the labeling of logical forms is labor-intensive and challenging to elicit at a large scale. As a result, alternative forms of supervision have been proposed to alleviate the annotation bottleneck faced by semantic parsing systems. One direction is to train a semantic parser in a weakly-supervised setting based on utterance-denotation pairs (Clarke et al., 2010; Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), since such data are relatively easy to obtain via crowdsourcing (Berant et al., 2013a). However, the unavailability of logical forms in the weakly-supervised setting, renders model training more difficult. A fundamental challenge in learning semantic parsers from denotations is finding consistent logical forms, i.e., those which execute to the correct denotation. This search space can be very large, growing exponentially as compositionality increases. Moreover, consistent logical forms unavoidably introduce a certain degree of spuriousness — some of them will accidentally execute to the correct denotation without reflecting the meaning of the utterance. These spurious logical"
K18-1035,P15-1127,0,0.0139515,"logical forms (red color) receive higher scores than semantically-correct ones (blue color). The scores of these spurious logical forms decrease when they are explicitly handled. (Cheng et al., 2017a). have used annotated training data consisting of sentences and their corresponding logical forms (Kate and Mooney, 2006; Kate et al., 2005; Lu et al., 2008; Kwiatkowksi et al., 2010). In order to scale semantic parsing to open-domain problems, weakly-supervised semantic parsers are trained on utterance-denotation pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning"
K18-1035,W10-2903,0,0.0729694,"s on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range. 1 Figure 1: Overview of the weakly-supervised neural semantic parsing system. 2006; Kwiatkowksi et al., 2010). However, the labeling of logical forms is labor-intensive and challenging to elicit at a large scale. As a result, alternative forms of supervision have been proposed to alleviate the annotation bottleneck faced by semantic parsing systems. One direction is to train a semantic parser in a weakly-supervised setting based on utterance-denotation pairs (Clarke et al., 2010; Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), since such data are relatively easy to obtain via crowdsourcing (Berant et al., 2013a). However, the unavailability of logical forms in the weakly-supervised setting, renders model training more difficult. A fundamental challenge in learning semantic parsers from denotations is finding consistent logical forms, i.e., those which execute to the correct denotation. This search space can be very large, growing exponentially as compositionality increases. Moreover, consistent logical forms unavoidably introduce a c"
K18-1035,P14-1133,0,0.0962412,"three variants of our model. We primarily consider the neural parser-ranker system (denoted by NPR) described in Section 2 which is trained to maximize the likelihood of consistent logical forms. We then compare it to a system augmented with a generative ranker (denoted by GRANKER), introducing the second objective of maximizing the reconstruction likelihood. Finally, we examine the impact of neural lexicon encoding when it is used for the generative ranker, and also when it is used for the entire system. 2 http://developers.google.com/ freebase/ 362 Models (Berant et al., 2013a) PARASEMPRE (Berant and Liang, 2014) JACANA (Yao and Van Durme, 2014) S CANNE R (Cheng et al., 2017b) UDEPLAMBDA (Reddy et al., 2017) SEMPRE NPR + GRANKER + lexicon encoding on GRANKER + lexicon encoding on parser and GRANKER F1 10.80 12.79 5.08 17.02 17.70 17.30 17.33 17.67 18.22 with the method of Bisk et al. (2016) which parses an utterance into a syntactic representation which is subsequently grounded to Freebase; and also with Das et al. (2017) who employ memory networks and external text resources. Results on both datasets follow similar trends as in W EB Q UES TIONS . The best performing NPR variant achieves state-of-the-"
K18-1035,P17-2057,0,0.0122953,"en it is used for the generative ranker, and also when it is used for the entire system. 2 http://developers.google.com/ freebase/ 362 Models (Berant et al., 2013a) PARASEMPRE (Berant and Liang, 2014) JACANA (Yao and Van Durme, 2014) S CANNE R (Cheng et al., 2017b) UDEPLAMBDA (Reddy et al., 2017) SEMPRE NPR + GRANKER + lexicon encoding on GRANKER + lexicon encoding on parser and GRANKER F1 10.80 12.79 5.08 17.02 17.70 17.30 17.33 17.67 18.22 with the method of Bisk et al. (2016) which parses an utterance into a syntactic representation which is subsequently grounded to Freebase; and also with Das et al. (2017) who employ memory networks and external text resources. Results on both datasets follow similar trends as in W EB Q UES TIONS . The best performing NPR variant achieves state-of-the-art results on G RAPH Q UESTIONS and it comes close to the best model on S PADES without using any external resources. One of the claims put forward in this paper is that the extended NPR model reduces the impact of spurious logical forms during training. Table 5 highlights examples of spurious logical forms which are not semantically correct but are nevertheless assigned higher scores in the vanilla NPR (red colo"
K18-1035,Q15-1039,0,0.0288109,"Missing"
K18-1035,D16-1214,0,0.347618,"cal form executing to the correct denotation, and an inverse neural parser measures the degree to which the logical form represents the meaning of the utterance. We present a scheduled training scheme which balances the contribution of the two components and objectives. To further boost performance, we propose to neurally encode a lexicon, as a means of injecting prior domain knowledge to the neural parameters. We evaluate our system on three Freebase datasets which consist of utterance denotation pairs: W EB Q UESTIONS (Berant et al., 2013a), G RAPH Q UESTIONS (Su et al., 2016), and S PADES (Bisk et al., 2016). Experimental results across datasets show that our weakly-supervised semantic parser achieves state-of-the-art performance. 2 2.1 Parser Our work follows Cheng et al. (2017b, 2018) in using LISP-style functional queries as the logical formulation. Advantageously, functional queries are recursive, tree-structured and can naturally encode logical form derivations (i.e., functions and their application order). For example, the utterance “who is obama’s eldest daughter” is simply represented with the function-argument structure argmax(daughterOf(Obama), ageOf). Table 1 displays the functions we"
K18-1035,P16-1004,1,0.887817,"n utterance-denotation pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational infer"
K18-1035,D14-1067,0,0.0571272,"Missing"
K18-1035,P15-1026,0,0.0824027,"idea is that relations or entities can be viewed as a single-node tree-structured 6.2 Training Across training regimes, the dimensions of word vector, logical form token vector, and LSTM hidden states (for the semantic parser and the inverse parser) are 50, 50, and 150, respectively. Word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. We used one 361 Models Berant et al. (2013a) Berant and Liang (2014) Berant and Liang (2015) Reddy et al. (2016) Yao and Van Durme (2014) Bast and Haussmann (2015) Bordes et al. (2014) Dong et al. (2015) Yih et al. (2015) Xu et al. (2016) Cheng et al. (2017b) LSTM layer in the forward and backward directions. Dropout was used before the softmax activation (Equations (7), (8), and (18)). The dropout rate was set to 0.5. Momentum SGD (Sutskever et al., 2013) was used as the optimization method to update the parameters of the model. As mentioned earlier, we use entity linking to reduce the beam search space. Entity mentions in S PADES are automatically annotated with Freebase entities (Gabrilovich et al., 2013). For W E B Q UESTIONS and G RAPH Q UESTIONS we perform entity linking following the p"
K18-1035,P13-1042,0,0.026696,", achieving results within the state-of-the-art range. 1 Figure 1: Overview of the weakly-supervised neural semantic parsing system. 2006; Kwiatkowksi et al., 2010). However, the labeling of logical forms is labor-intensive and challenging to elicit at a large scale. As a result, alternative forms of supervision have been proposed to alleviate the annotation bottleneck faced by semantic parsing systems. One direction is to train a semantic parser in a weakly-supervised setting based on utterance-denotation pairs (Clarke et al., 2010; Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), since such data are relatively easy to obtain via crowdsourcing (Berant et al., 2013a). However, the unavailability of logical forms in the weakly-supervised setting, renders model training more difficult. A fundamental challenge in learning semantic parsers from denotations is finding consistent logical forms, i.e., those which execute to the correct denotation. This search space can be very large, growing exponentially as compositionality increases. Moreover, consistent logical forms unavoidably introduce a certain degree of spuriousness — some of them will accidentally execute to the corr"
K18-1035,P15-1033,0,0.0216448,"f(Obama), ageOf). Table 1 displays the functions we use in this work; a more detailed specifications can be found in the appendix. To generate logical forms, our system adopts a variant of the neural sequence-to-tree model proposed in Cheng et al. (2017b). During generation, the prediction space is restricted by the grammar of the logical language (e.g., the type and the number of arguments required by a function) in order to ensure that output logical forms are well-formed and executable. The parser consists of a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder and a stack-LSTM (Dyer et al., 2015) decoder, introduced as follows. The Neural Parser-Ranker Conventional weakly-supervised semantic parsers (Liang, 2016) consist of two major components: a parser, which is chart-based and nonparameterized, recursively builds derivations for each utterance span using dynamic programming. A learner, which is a log-linear model, defines features useful for scoring and ranking the set of candidate derivations, based on the correctness of execution results. As mentioned in Liang (2016), the chart-based parser brings a disadvantage since it does not support incremental contextual interpretation. The"
K18-1035,P17-2019,1,0.36976,"scheduled training scheme which balances the contribution of the two components and objectives. To further boost performance, we propose to neurally encode a lexicon, as a means of injecting prior domain knowledge to the neural parameters. We evaluate our system on three Freebase datasets which consist of utterance denotation pairs: W EB Q UESTIONS (Berant et al., 2013a), G RAPH Q UESTIONS (Su et al., 2016), and S PADES (Bisk et al., 2016). Experimental results across datasets show that our weakly-supervised semantic parser achieves state-of-the-art performance. 2 2.1 Parser Our work follows Cheng et al. (2017b, 2018) in using LISP-style functional queries as the logical formulation. Advantageously, functional queries are recursive, tree-structured and can naturally encode logical form derivations (i.e., functions and their application order). For example, the utterance “who is obama’s eldest daughter” is simply represented with the function-argument structure argmax(daughterOf(Obama), ageOf). Table 1 displays the functions we use in this work; a more detailed specifications can be found in the appendix. To generate logical forms, our system adopts a variant of the neural sequence-to-tree model pro"
K18-1035,D12-1069,0,0.0610248,"Missing"
K18-1035,P18-1168,0,0.0793613,"arser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational inference has also been used in other NLP tasks including relation discovery (Marcheggiani and Titov, 2016), sentence compression (Miao and Blunsom, 2016), and parsing 8 Conclusions In this work we proposed a weakly-supervised neural semantic parsing system trained on utterance-denota"
K18-1035,Q15-1019,0,0.0171955,"color) receive higher scores than semantically-correct ones (blue color). The scores of these spurious logical forms decrease when they are explicitly handled. (Cheng et al., 2017a). have used annotated training data consisting of sentences and their corresponding logical forms (Kate and Mooney, 2006; Kate et al., 2005; Lu et al., 2008; Kwiatkowksi et al., 2010). In order to scale semantic parsing to open-domain problems, weakly-supervised semantic parsers are trained on utterance-denotation pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers f"
K18-1035,D10-1119,0,0.22498,"ations. These candidates are then ranked based on two criterion: their likelihood of executing to the correct denotation, and their agreement with the utterance semantics. We present a scheduled training procedure to balance the contribution of the two objectives. Furthermore, we propose to use a neurally encoded lexicon to inject prior domain knowledge to the model. Experiments on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range. 1 Figure 1: Overview of the weakly-supervised neural semantic parsing system. 2006; Kwiatkowksi et al., 2010). However, the labeling of logical forms is labor-intensive and challenging to elicit at a large scale. As a result, alternative forms of supervision have been proposed to alleviate the annotation bottleneck faced by semantic parsing systems. One direction is to train a semantic parser in a weakly-supervised setting based on utterance-denotation pairs (Clarke et al., 2010; Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), since such data are relatively easy to obtain via crowdsourcing (Berant et al., 2013a). However, the unavailability of logical forms in the we"
K18-1035,D16-1258,0,0.0697302,"Missing"
K18-1035,D13-1161,0,0.0225905,"atasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range. 1 Figure 1: Overview of the weakly-supervised neural semantic parsing system. 2006; Kwiatkowksi et al., 2010). However, the labeling of logical forms is labor-intensive and challenging to elicit at a large scale. As a result, alternative forms of supervision have been proposed to alleviate the annotation bottleneck faced by semantic parsing systems. One direction is to train a semantic parser in a weakly-supervised setting based on utterance-denotation pairs (Clarke et al., 2010; Kwiatkowski et al., 2013; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), since such data are relatively easy to obtain via crowdsourcing (Berant et al., 2013a). However, the unavailability of logical forms in the weakly-supervised setting, renders model training more difficult. A fundamental challenge in learning semantic parsers from denotations is finding consistent logical forms, i.e., those which execute to the correct denotation. This search space can be very large, growing exponentially as compositionality increases. Moreover, consistent logical forms unavoidably introduce a certain degree of spuriousn"
K18-1035,P17-1089,0,0.0160899,"al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational inference has also been used in other NLP tasks including relation discovery (Marcheggiani and Titov, 2016), sentence compression (Miao and Blunsom, 2016), and parsing 8 Conclusions In this work we proposed a weakly-supervised n"
K18-1035,D11-1140,0,0.0481658,"e, growing exponentially as compositionality increases. Moreover, consistent logical forms unavoidably introduce a certain degree of spuriousness — some of them will accidentally execute to the correct denotation without reflecting the meaning of the utterance. These spurious logical forms are misleading supervision sigIntroduction Semantic parsing is the task of converting natural language utterances into machine-understandable meaning representations or logical forms. The task has attracted much attention in the literature due to a wide range of applications ranging from question answering (Kwiatkowski et al., 2011; Liang et al., 2011) to relation extraction (Krishnamurthy and Mitchell, 2012), goal-oriented dialog (Wen et al., 2015), and instruction understanding (Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). In a typical semantic parsing scenario, a logical form is executed against a knowledge base to produce an outcome (e.g., an answer) known as denotation. Conventional semantic parsers are trained on collections of utterances paired with annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 356 Proceedings of the 22nd Conference"
K18-1035,P16-1002,0,0.0431182,"pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational inference has also been us"
K18-1035,P06-1115,0,0.061989,"ident aircraft relationship.flight destination:aviation.aviation incident aircraft relationship.aircraft model(ent.m.0qn2v) aviation.comparable aircraft relationship(ent.m.018rl2) Table 5: Comparison between logical forms preferred by NPR before and after the addition of the inverse parser. Spurious logical forms (red color) receive higher scores than semantically-correct ones (blue color). The scores of these spurious logical forms decrease when they are explicitly handled. (Cheng et al., 2017a). have used annotated training data consisting of sentences and their corresponding logical forms (Kate and Mooney, 2006; Kate et al., 2005; Lu et al., 2008; Kwiatkowksi et al., 2010). In order to scale semantic parsing to open-domain problems, weakly-supervised semantic parsers are trained on utterance-denotation pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted"
K18-1035,P11-1060,0,0.184223,"s compositionality increases. Moreover, consistent logical forms unavoidably introduce a certain degree of spuriousness — some of them will accidentally execute to the correct denotation without reflecting the meaning of the utterance. These spurious logical forms are misleading supervision sigIntroduction Semantic parsing is the task of converting natural language utterances into machine-understandable meaning representations or logical forms. The task has attracted much attention in the literature due to a wide range of applications ranging from question answering (Kwiatkowski et al., 2011; Liang et al., 2011) to relation extraction (Krishnamurthy and Mitchell, 2012), goal-oriented dialog (Wen et al., 2015), and instruction understanding (Chen and Mooney, 2011; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). In a typical semantic parsing scenario, a logical form is executed against a knowledge base to produce an outcome (e.g., an answer) known as denotation. Conventional semantic parsers are trained on collections of utterances paired with annotated logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 356 Proceedings of the 22nd Conference on Computational Nat"
K18-1035,D08-1082,0,0.0266941,"tion:aviation.aviation incident aircraft relationship.aircraft model(ent.m.0qn2v) aviation.comparable aircraft relationship(ent.m.018rl2) Table 5: Comparison between logical forms preferred by NPR before and after the addition of the inverse parser. Spurious logical forms (red color) receive higher scores than semantically-correct ones (blue color). The scores of these spurious logical forms decrease when they are explicitly handled. (Cheng et al., 2017a). have used annotated training data consisting of sentences and their corresponding logical forms (Kate and Mooney, 2006; Kate et al., 2005; Lu et al., 2008; Kwiatkowksi et al., 2010). In order to scale semantic parsing to open-domain problems, weakly-supervised semantic parsers are trained on utterance-denotation pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous"
K18-1035,D16-1116,0,0.0344801,"Missing"
K18-1035,Q16-1017,0,0.0244654,"se of reinforcement learning to train neural semantic parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational inference has also been used in other NLP tasks including relation discovery (Marcheggiani and Titov, 2016), sentence compression (Miao and Blunsom, 2016), and parsing 8 Conclusions In this work we proposed a weakly-supervised neural semantic parsing system trained on utterance-denotation pairs. The system employs a neural sequence-to-tree parser to generate logical forms for a natural language utterance. The logical forms are subsequently ranked with two components and objectives: a log-linear model which scores the likelihood of correct execution, and a generative neural inverse parser which measures whether logical forms are meaning preserving. We proposed a scheduled training procedure to balan"
K18-1035,N16-1074,0,0.0143433,"re updated as normal. Both training objectives are enabled, the system maximizes the likelihood of consistent logical forms and the reconstruction likelihood. 5 Experiments Neural Lexicon Encoding In this section we further discuss how the semantic parser presented so far can be enhanced with a lexicon. A lexicon is essentially a coarse mapping between natural language phrases and knowledge base relations and entities, and has been widely used in conventional chart-based parsers (Berant et al., 2013a; Reddy et al., 2014). Here, we show how a lexicon (either hard-coded or statisticallylearned (Krishnamurthy, 2016)) can be used to benefit a neural semantic parser. The central idea is that relations or entities can be viewed as a single-node tree-structured 6.2 Training Across training regimes, the dimensions of word vector, logical form token vector, and LSTM hidden states (for the semantic parser and the inverse parser) are 50, 50, and 150, respectively. Word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. We used one 361 Models Berant et al. (2013a) Berant and Liang (2014) Berant and Liang (2015) Reddy et al. (2016) Yao and V"
K18-1035,N06-1056,0,0.130548,"Missing"
K18-1035,D16-1031,0,0.0212625,"parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational inference has also been used in other NLP tasks including relation discovery (Marcheggiani and Titov, 2016), sentence compression (Miao and Blunsom, 2016), and parsing 8 Conclusions In this work we proposed a weakly-supervised neural semantic parsing system trained on utterance-denotation pairs. The system employs a neural sequence-to-tree parser to generate logical forms for a natural language utterance. The logical forms are subsequently ranked with two components and objectives: a log-linear model which scores the likelihood of correct execution, and a generative neural inverse parser which measures whether logical forms are meaning preserving. We proposed a scheduled training procedure to balance the two objectives, and a neural lexicon enc"
K18-1035,P16-1220,0,0.128062,"n be viewed as a single-node tree-structured 6.2 Training Across training regimes, the dimensions of word vector, logical form token vector, and LSTM hidden states (for the semantic parser and the inverse parser) are 50, 50, and 150, respectively. Word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. We used one 361 Models Berant et al. (2013a) Berant and Liang (2014) Berant and Liang (2015) Reddy et al. (2016) Yao and Van Durme (2014) Bast and Haussmann (2015) Bordes et al. (2014) Dong et al. (2015) Yih et al. (2015) Xu et al. (2016) Cheng et al. (2017b) LSTM layer in the forward and backward directions. Dropout was used before the softmax activation (Equations (7), (8), and (18)). The dropout rate was set to 0.5. Momentum SGD (Sutskever et al., 2013) was used as the optimization method to update the parameters of the model. As mentioned earlier, we use entity linking to reduce the beam search space. Entity mentions in S PADES are automatically annotated with Freebase entities (Gabrilovich et al., 2013). For W E B Q UESTIONS and G RAPH Q UESTIONS we perform entity linking following the procedure described in Reddy et al."
K18-1035,P16-1003,0,0.0158608,"semantically-correct ones (blue color). The scores of these spurious logical forms decrease when they are explicitly handled. (Cheng et al., 2017a). have used annotated training data consisting of sentences and their corresponding logical forms (Kate and Mooney, 2006; Kate et al., 2005; Lu et al., 2008; Kwiatkowksi et al., 2010). In order to scale semantic parsing to open-domain problems, weakly-supervised semantic parsers are trained on utterance-denotation pairs (Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013b; Choi et al., 2015; Krishnamurthy and Mitchell, 2015; Pasupat and Liang, 2016; Gardner and Krishnamurthy, 2017; Reddy et al., 2017). Most previous work employs a chart-based parser to produce logical forms from a grammar which combines domain-general aspects with lexicons. Recently, neural semantic parsing has attracted a great deal of attention. Previous work has mostly adopted fully-supervised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers from questionanswer pairs"
K18-1035,P14-1090,0,0.0329532,"Missing"
K18-1035,D14-1162,0,0.082199,"nd has been widely used in conventional chart-based parsers (Berant et al., 2013a; Reddy et al., 2014). Here, we show how a lexicon (either hard-coded or statisticallylearned (Krishnamurthy, 2016)) can be used to benefit a neural semantic parser. The central idea is that relations or entities can be viewed as a single-node tree-structured 6.2 Training Across training regimes, the dimensions of word vector, logical form token vector, and LSTM hidden states (for the semantic parser and the inverse parser) are 50, 50, and 150, respectively. Word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. We used one 361 Models Berant et al. (2013a) Berant and Liang (2014) Berant and Liang (2015) Reddy et al. (2016) Yao and Van Durme (2014) Bast and Haussmann (2015) Bordes et al. (2014) Dong et al. (2015) Yih et al. (2015) Xu et al. (2016) Cheng et al. (2017b) LSTM layer in the forward and backward directions. Dropout was used before the softmax activation (Equations (7), (8), and (18)). The dropout rate was set to 0.5. Momentum SGD (Sutskever et al., 2013) was used as the optimization method to update the parameters of the model. As mentioned e"
K18-1035,P15-1128,0,0.0528784,"Missing"
K18-1035,Q14-1030,1,0.845674,"we allow the reconstruction loss to back-propagate to the parser, and all three components are updated as normal. Both training objectives are enabled, the system maximizes the likelihood of consistent logical forms and the reconstruction likelihood. 5 Experiments Neural Lexicon Encoding In this section we further discuss how the semantic parser presented so far can be enhanced with a lexicon. A lexicon is essentially a coarse mapping between natural language phrases and knowledge base relations and entities, and has been widely used in conventional chart-based parsers (Berant et al., 2013a; Reddy et al., 2014). Here, we show how a lexicon (either hard-coded or statisticallylearned (Krishnamurthy, 2016)) can be used to benefit a neural semantic parser. The central idea is that relations or entities can be viewed as a single-node tree-structured 6.2 Training Across training regimes, the dimensions of word vector, logical form token vector, and LSTM hidden states (for the semantic parser and the inverse parser) are 50, 50, and 150, respectively. Word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. We used one 361 Models Beran"
K18-1035,P18-1070,0,0.0418018,"vised, sequence-to-sequence models to generate logical form strings from natural language utterances (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Other work explores the use of reinforcement learning to train neural semantic parsers from questionanswer pairs (Liang et al., 2016) or from user feedback (Iyer et al., 2017). More closely related to our work, Goldman et al. (2018) adopt a neural semantic parser and a discriminative ranker to solve a visual reasoning challenge. They attempt to alleviate the search space and spuriousness challenges with abstractive examples. Yin et al. (2018) adopt a tree-based variational autoencoder for semi-supervised semantic parsing. Neural variational inference has also been used in other NLP tasks including relation discovery (Marcheggiani and Titov, 2016), sentence compression (Miao and Blunsom, 2016), and parsing 8 Conclusions In this work we proposed a weakly-supervised neural semantic parsing system trained on utterance-denotation pairs. The system employs a neural sequence-to-tree parser to generate logical forms for a natural language utterance. The logical forms are subsequently ranked with two components and objectives: a log-linear"
K18-1035,Q16-1010,1,0.859354,"Missing"
K18-1035,D17-1009,1,0.875196,"Missing"
K18-1035,D16-1054,0,0.0273629,"Missing"
K18-1035,D15-1199,0,0.0682308,"Missing"
K18-1035,Q13-1005,0,\N,Missing
N04-1016,P03-1059,0,0.0119155,"rigram model ( f (n1 , p, n2 )); it significantly outperformed the best BNC model. The comparison with the literature in Table 11 showed that the best Altavista model significantly outperformed both the baseline and the best model in the literature (Lauer’s word-based model). The BNC model, on the other hand, Model f (n) f (det, n) f (det, n)/ f (n) Backoff Altavista Count Uncount 87.01 90.13 88.38#6 ∗ 91.22#6 ∗ 83.19 85.38 87.01 89.80 BNC Count Uncount 87.32# 90.39# 51.01 50.23 50.95 50.23 – – Table 12: Performance of Altavista counts and BNC counts for noun countability detection (data from Baldwin and Bond 2003) achieved a performance that is not significantly different from the baseline, and significantly worse than Lauer’s best model. 8 Noun Countability Detection The next analysis task that we consider is the problem of determining the countability of nouns. Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifiers. In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ). Baldwin and Bond (2003) propose a method for automa"
N04-1016,H01-1052,0,0.0181579,"f different parts of speech (Keller and Lapata 2003 only tested bigrams involving nouns, verbs, and adjectives). Another important question is whether web-based methods, which are by definition unsupervised, can be competitive alternatives to supervised approaches used for most tasks in the literature. This paper aims to address these questions. We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a,b). Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection. Table 1 gives an overview of these tasks and their properties. In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts. We compare this model both against a baseline (same model, but parameters estimated on the BNC) and agai"
N04-1016,P01-1005,0,0.0146891,"f different parts of speech (Keller and Lapata 2003 only tested bigrams involving nouns, verbs, and adjectives). Another important question is whether web-based methods, which are by definition unsupervised, can be competitive alternatives to supervised approaches used for most tasks in the literature. This paper aims to address these questions. We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a,b). Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) compound noun bracketing, (e) compound noun interpretation, and (f) noun countability detection. Table 1 gives an overview of these tasks and their properties. In all cases, we propose a simple, unsupervised n-gram based model whose parameters are estimated using web counts. We compare this model both against a baseline (same model, but parameters estimated on the BNC) and agai"
N04-1016,W02-1005,0,0.0126876,"ich word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or partof-speech tags to the left or right of the target word. The results obtained by a variety of classification methods are given in Table 6. All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (19"
N04-1016,J94-4004,0,0.0259447,"Missing"
N04-1016,W95-0104,0,0.0537882,"alternative surface realizations of a word. This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended. The task is to infer which word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are us"
N04-1016,P96-1010,0,0.0150028,"rface realizations of a word. This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended. The task is to infer which word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small"
N04-1016,C94-1042,0,0.0117228,"tion The next analysis task that we consider is the problem of determining the countability of nouns. Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifiers. In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ). Baldwin and Bond (2003) propose a method for automatically learning the countability of English nouns from the BNC. They obtain information about noun countability by merging lexical entries from COMLEX (Grishman et al., 1994) and the ALTJ/E Japanese-to-English semantic transfer dictionary (Ikehara et al., 1991). Words are classified into four classes: countable, uncountable, bipartite (e.g., trousers ), and plural only (e.g., goods ). A memory-based classifier is used to learn the four-way distinction on the basis of several linguistically motivated features such as: number of the head noun, number of the modifier, subject-verb agreement, plural determiners. We devised unsupervised models for the countability learning task and evaluated their performance on Baldwin and Bond’s (2003) test data. We concentrated sole"
N04-1016,1991.mtsummit-papers.16,0,0.00821492,"ity of nouns. Countability is the semantic property that determines whether a noun can occur in singular and plural forms, and affects the range of permissible modifiers. In English, nouns are typically either countable (e.g., one dog, two dogs ) or uncountable (e.g., some peace, *one peace, *two peaces ). Baldwin and Bond (2003) propose a method for automatically learning the countability of English nouns from the BNC. They obtain information about noun countability by merging lexical entries from COMLEX (Grishman et al., 1994) and the ALTJ/E Japanese-to-English semantic transfer dictionary (Ikehara et al., 1991). Words are classified into four classes: countable, uncountable, bipartite (e.g., trousers ), and plural only (e.g., goods ). A memory-based classifier is used to learn the four-way distinction on the basis of several linguistically motivated features such as: number of the head noun, number of the modifier, subject-verb agreement, plural determiners. We devised unsupervised models for the countability learning task and evaluated their performance on Baldwin and Bond’s (2003) test data. We concentrated solely on countable and uncountable nouns, as they account for the vast majority of the dat"
N04-1016,A97-1025,0,0.0168859,"on that each word in the set could be mistakenly typed when another word in the set was intended. The task is to infer which word in a confusion set is the correct one in a given context. This choice can be either syntactic (as for {then, than}) or semantic (as for {principal, principle}). A number of machine learning methods have been proposed for context-sensitive spelling correction. These include a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002). Despite their differences, most approaches use two types of features: context words and collocations. Context word features record the presence of a word within a fixed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or partof-speech tags to the left or right of the target word. The results obtained by a variety of classification methods"
N04-1016,J03-3005,1,0.612398,"s. The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger range of n-grams. For the majority of tasks, we find that simple, unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus. However, in most cases, web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora. We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models. 1 Introduction Keller and Lapata (2003) investigated the validity of web counts for a range of predicate-argument bigrams (verbobject, adjective-noun, and noun-noun bigrams). They presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts (a) correlate with frequencies obtained from a carefully edited, balanced corpus such as the 100M words British National Corpus (BNC), (b) correlate with frequencies recreated using smoothing methods in the case of unseen bigrams, (c) reliably predict human plausibility judgments, and (d) yield state-of-the-art performance on ps"
N04-1016,P95-1007,0,0.740742,"right branching analysis if [n2 n3 ] is more likely than [n1 n2 ]. The dependency model compares [n1 n2 ] against [n1 n3 ] and adopts a right branching analysis if [n1 n3 ] is more likely than [n1 n2 ]. The simplest model of compound noun disambiguation compares the frequencies of the two competing analyses and opts for the most frequent one (Pustejovsky et al., Alta 63.93 77.86 78.68#∗ 68.85 70.49 80.32 68.03 71.31 61.47 65.57 75.40 BNC 63.93 66.39 65.57 65.57 63.11 66.39 63.11 67.21 62.29 57.37 68.03# Table 8: Performance of Altavista counts and BNC counts for compound bracketing (data from Lauer 1995) Model Baseline Best BNC Lauer (1995): adjacency Lauer (1995): dependency Best Altavista Lauer (1995): tuned Upper bound Accuracy 63.93 68.036 †‡ 68.90 77.50 78.68†6 ‡ 80.70 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993). Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. He uses a probability ratio to compare the probability of the leftbranching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model). ∑ P(t1 → t2"
N04-1016,P00-1012,0,0.015274,"hat the relative order of premodifiers is fixed, and independent of context and the noun being modified. The simplest strategy is what Shaw and Hatzivassiloglou (1999) call direct evidence. Given an adjective pair {a, b}, they count how many times ha, bi and hb, ai appear in the corpus and choose the pair with the highest frequency. Unfortunately the direct evidence method performs poorly when a given order is unseen in the training data. To compensate for this, Shaw and Hatzivassiloglou (1999) propose to compute the transitive closure of the ordering relation: if a ≺ c and c ≺ b, then a ≺ b. Malouf (2000) further proposes a back-off bigram model of adjective pairs for choosing among alternative orders (P(ha, bi|{a, b}) vs. P(hb, ai|{a, b})). He also proposes positional probabilities as a means of estimating how likely it is for a given adjective a to appear first in a sequence by looking at each pair in the training data that contains the adjective a and recording its position. Finally, he uses memory-based learning as a means to encode morphological and semantic similarities among different adjective orders. Each adjective pair ab is encoded as a vector of 16 features (the last eight characte"
N04-1016,J93-2004,0,0.0350948,"Missing"
N04-1016,C00-2094,0,0.0105397,"Missing"
N04-1016,J93-2005,0,0.0204803,"supervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. He uses a probability ratio to compare the probability of the leftbranching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model). ∑ P(t1 → t2 )P(t2 → t3 ) (4) Rdep = ti ∈cats(wi ) ∑ P(t1 → t3 )P(t2 → t3 ) ∑ P(t1 → t2 ) ∑ P(t2 → t3 ) ti ∈cats(wi ) Bracketing of Compound Nouns The first analysis task we consider is the syntactic disambiguation of compound nouns, which has received a fair amount of attention in the NLP literature (Pustejovsky et al., 1993; Resnik, 1993; Lauer, 1995). The task can be summarized as follows: given a three word compound n1 n3 n3 , determine the correct binary bracketing of the word sequence (see (3) for an example). (3) Model Baseline f (n1 , n2 ) : f (n2 , n3 ) f (n1 , n2 ) : f (n1 , n3 ) f (n1 , n2 )/ f (n1 ) : f (n2 , n3 )/ f (n2 ) f (n1 , n2 )/ f (n2 ) : f (n2 , n3 )/ f (n3 ) f (n1 , n2 )/ f (n2 ) : f (n1 , n3 )/ f (n3 ) f (n1 , n2 ) : f (n2 , n3 ) (NEAR) f (n1 , n2 ) : f (n1 , n3 ) (NEAR) f (n1 , n2 )/ f (n1 ) : f (n2 , n3 )/ f (n2 ) (NEAR) f (n1 , n2 )/ f (n2 ) : f (n2 , n3 )/ f (n3 ) (NEAR) f (n1 , n2 )/ f"
N04-1016,P99-1018,0,0.0148518,"ovement on confusion sets whose words belong to different parts of speech). An advantage of our method is that it can be used for a large number of confusion sets without relying on the availability of training data. 5 Ordering of Prenominal Adjectives The ordering of prenominal modifiers is important for natural language generation systems where the text must be both fluent and grammatical. For example, the sequence big fat Greek wedding is perfectly acceptable, whereas fat Greek big wedding sounds odd. The ordering of prenominal adjectives has sparked a great deal of theoretical debate (see Shaw and Hatzivassiloglou 1999 for an overview) and efforts have concentrated on defining rules based on semantic criteria that account for different orders (e.g., age ≺ color, value ≺ dimension). Data intensive approaches to the ordering problem rely on corpora for gathering evidence for the likelihood of different orders. They rest on the hypothesis that the relative order of premodifiers is fixed, and independent of context and the noun being modified. The simplest strategy is what Shaw and Hatzivassiloglou (1999) call direct evidence. Given an adjective pair {a, b}, they count how many times ha, bi and hb, ai appear in"
N04-1016,1999.tc-1.8,0,\N,Missing
N04-1020,A00-2018,0,0.102453,"Missing"
N04-1020,W01-1315,0,0.0272671,"llel, Result) have received much attention in linguistics (Kamp and Reyle, 1993; Webber, 1991; Asher and Lascarides, 2003), the automatic interpretation of events and their temporal relations is beyond the capabilities of current open-domain NLP systems. While corpus-based methods have accelerated progress in other areas of NLP, they have yet to make a substantial impact on the processing of temporal information. This is partly due to the absence of readily available corpora annotated with temporal information, although efforts are underway to develop treebanks marked with temporal relations (Katz and Arosio, 2001) and devise annotation schemes that are suitable for coding temporal relations (Ferro et al., 2000; Setzer and Gaizauskas, 2001). Absolute temporal information has received some attention (Wilson et al., 2001; Schilder and Habel, 2001; Wiebe et al., 1998) and systems have been developed for identifying and assigning referents to time expressions. Although the treatment of time expressions is an important first step towards the automatic handling of temporal phenomena, much temporal information is not absolute but relative and not overtly expressed but implicit. Consider the examples in (1) tak"
N04-1020,W99-0632,0,0.0140335,"ed form. Levin (1993) focuses on the relation between verbs and their arguments and hypothesizes that verbs which behave similarly with respect to the expression and interpretation of their arguments share certain meaning components and can therefore be organised into semantically coherent classes (200 in total). Asher and Lascarides (2003) argue that these classes provide important information for identifying semantic relationships between clauses. Verbs in our data were mapped into their corresponding Levin classes (feature VL ); polysemous verbs were disambiguated by the method proposed in Lapata and Brew (1999). Again, for verbs not included in Levin, the lemmatised verb form is used. Noun Identity (N) It is not only verbs, but also nouns that can provide important information about the semantic relation between two clauses (see Asher and Lascarides 2003 for detailed motivation). In our domain for example, the noun share is found in main clauses typically preceding the noun market which is often found in subordinate clauses. Table 3 shows the most frequently attested nouns (excluding proper names) in main (NounM ) and subordinate (NounS ) clauses for each temporal marker. Notice that time denoting n"
N04-1020,N03-2019,0,0.277617,"Missing"
N04-1020,P02-1047,0,0.260788,"j and P a S i t j by simply counting the occurrence of the features a M i and a S i with marker t. For features with zero counts, we use add-k smoothing (Johnson, 1932), where k is a small number less than one. In the testing phase, all occurrences of the relevant temporal markers are removed for the interpretation task and the model must decide which mem    ber of the confusion set to choose. For the sentence fusion task, it is the temporal order of the two clauses that is unknown and must be inferred. A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). They train a set of naive Bayes classifiers on a large corpus (in the order of 40 M sentences) representative of four rhetorical relations using word bigrams as features. The discourse relations are read off from explicit discourse markers thus avoiding time consuming hand coding. Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. First we explore the contribution of linguistic information to the inference task using considerably smaller data sets and secondly apply the proposed model to a generation task, namely i"
N04-1020,W01-1309,0,0.00995065,"en-domain NLP systems. While corpus-based methods have accelerated progress in other areas of NLP, they have yet to make a substantial impact on the processing of temporal information. This is partly due to the absence of readily available corpora annotated with temporal information, although efforts are underway to develop treebanks marked with temporal relations (Katz and Arosio, 2001) and devise annotation schemes that are suitable for coding temporal relations (Ferro et al., 2000; Setzer and Gaizauskas, 2001). Absolute temporal information has received some attention (Wilson et al., 2001; Schilder and Habel, 2001; Wiebe et al., 1998) and systems have been developed for identifying and assigning referents to time expressions. Although the treatment of time expressions is an important first step towards the automatic handling of temporal phenomena, much temporal information is not absolute but relative and not overtly expressed but implicit. Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that John first met and then kissed the girl and that he first left the party and then walked home, even though there are no overt markers signalling the temporal order of the"
N04-1020,W01-1311,0,0.0384578,", the automatic interpretation of events and their temporal relations is beyond the capabilities of current open-domain NLP systems. While corpus-based methods have accelerated progress in other areas of NLP, they have yet to make a substantial impact on the processing of temporal information. This is partly due to the absence of readily available corpora annotated with temporal information, although efforts are underway to develop treebanks marked with temporal relations (Katz and Arosio, 2001) and devise annotation schemes that are suitable for coding temporal relations (Ferro et al., 2000; Setzer and Gaizauskas, 2001). Absolute temporal information has received some attention (Wilson et al., 2001; Schilder and Habel, 2001; Wiebe et al., 1998) and systems have been developed for identifying and assigning referents to time expressions. Although the treatment of time expressions is an important first step towards the automatic handling of temporal phenomena, much temporal information is not absolute but relative and not overtly expressed but implicit. Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that John first met and then kissed the girl and that he first left th"
N04-1020,N03-1030,0,0.023231,"Missing"
N04-1020,W01-1312,0,0.0315501,"ilities of current open-domain NLP systems. While corpus-based methods have accelerated progress in other areas of NLP, they have yet to make a substantial impact on the processing of temporal information. This is partly due to the absence of readily available corpora annotated with temporal information, although efforts are underway to develop treebanks marked with temporal relations (Katz and Arosio, 2001) and devise annotation schemes that are suitable for coding temporal relations (Ferro et al., 2000; Setzer and Gaizauskas, 2001). Absolute temporal information has received some attention (Wilson et al., 2001; Schilder and Habel, 2001; Wiebe et al., 1998) and systems have been developed for identifying and assigning referents to time expressions. Although the treatment of time expressions is an important first step towards the automatic handling of temporal phenomena, much temporal information is not absolute but relative and not overtly expressed but implicit. Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that John first met and then kissed the girl and that he first left the party and then walked home, even though there are no overt markers signalling"
N04-1020,W97-0320,0,\N,Missing
N06-1046,W00-1425,0,0.663667,"San Francisco defenders. d. Holocomb threw to Davis for a leaping catch. After two incompletions in the first quarter, Holcomb found Davis among four San Francisco defenders for a leaping catch. Table 1: Aggregation example (in boldface) from a corpus of football summaries 1 Introduction Aggregation is an essential component of many natural language generation systems (Reiter and Dale, 2000). The task captures a mechanism for merging together two or more linguistic structures into a single sentence. Aggregated texts tend to be more concise, coherent, and more readable overall (Dalianis, 1999; Cheng and Mellish, 2000). Compare, for example, sentence (2) in Table 1 and its nonaggregated counterpart in sentences (1a)–(1d). The difference between the fluent aggregated sentence and its abrupt and redundant alternative is striking. The benefits of aggregation go beyond making texts less stilted and repetitive. Researchers in psycholinguistics have shown that by eliminating redundancy, aggregation facilitates text comprehension and recall (see Yeung (1999) and the references therein). Furthermore, Di Eugenio et al. (2005) demonstrate that aggregation can improve learning in the context of an intelligent tutoring"
N06-1046,P05-1007,0,0.0838581,"Missing"
N06-1046,W05-0618,0,0.301069,"del finds a globally optimal assignment that satisfies partitioninglevel constraints. The computational challenge lies in the complexity of such a model: we need to find an optimal partition in an exponentially large search space. Our approach is based on an Integer Linear Programming (ILP) formulation which can be effectively solved using standard optimization tools. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al., 2004) and the generation of route directions (Marciniak and Strube, 2005). In the following section, we introduce our local pairwise model and afterward we present our global model for partitioning. 4.1 Learning Pairwise Similarity Our goal is to determine whether two database entries should be aggregated given the similarity of their shared attributes. We generate the training data by considering all pairs hei , ej i ∈ E × E, where E is the set of all entries attested in a given document. An entry pair forms a positive instance if its members belong to the same partition in the training data. For example, we will generate 8×7 2 unordered entry pairs for the eight"
N06-1046,C04-1197,0,0.0140205,"nce. Given the pairwise predictions of a local classifier, our model finds a globally optimal assignment that satisfies partitioninglevel constraints. The computational challenge lies in the complexity of such a model: we need to find an optimal partition in an exponentially large search space. Our approach is based on an Integer Linear Programming (ILP) formulation which can be effectively solved using standard optimization tools. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al., 2004) and the generation of route directions (Marciniak and Strube, 2005). In the following section, we introduce our local pairwise model and afterward we present our global model for partitioning. 4.1 Learning Pairwise Similarity Our goal is to determine whether two database entries should be aggregated given the similarity of their shared attributes. We generate the training data by considering all pairs hei , ej i ∈ E × E, where E is the set of all entries attested in a given document. An entry pair forms a positive instance if its members belong to the same partition in the training data. For"
N06-1046,W04-2401,0,0.124076,"rements, our approach relies on global inference. Given the pairwise predictions of a local classifier, our model finds a globally optimal assignment that satisfies partitioninglevel constraints. The computational challenge lies in the complexity of such a model: we need to find an optimal partition in an exponentially large search space. Our approach is based on an Integer Linear Programming (ILP) formulation which can be effectively solved using standard optimization tools. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al., 2004) and the generation of route directions (Marciniak and Strube, 2005). In the following section, we introduce our local pairwise model and afterward we present our global model for partitioning. 4.1 Learning Pairwise Similarity Our goal is to determine whether two database entries should be aggregated given the similarity of their shared attributes. We generate the training data by considering all pairs hei , ej i ∈ E × E, where E is the set of all entries attested in a given document. An entry pair forms a positive instance if its members belon"
N06-1046,W98-1415,0,0.694534,"igent tutoring application. In existing generation systems, aggregation typically comprises two processes: semantic grouping and sentence structuring (Wilkinson, 1995). The first process involves partitioning semantic content (usually the output of a content selection component) into disjoint sets, each corresponding to a single sentence. The second process is concerned with syntactic or lexical decisions that affect the realization of an aggregated sentence. To date, this task has involved human analysis of a domain-relevant corpus and manual development of aggregation rules (Dalianis, 1999; Shaw, 1998). The corpus analysis and knowledge engineering work in such an approach is substantial, prohibitively so in 359 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 359–366, c New York, June 2006. 2006 Association for Computational Linguistics large domains. But since corpus data is already used in building aggregation components, an appealing alternative is to try and learn the rules of semantic grouping directly from the data. Clearly, this would greatly reduce the human effort involved and ease porting generation systems to new domains. In"
N06-1046,N01-1003,0,0.0805436,"and sentence structuring are interleaved in one step, thus enabling the aggregation component to operate over a rich feature space. The common assumption is that other parts of the generation system are already in place during aggregation, and thus the aggregation component has access to discourse, syntactic, and lexical constraints. The interplay of different constraints is usually captured by a set of hand-crafted rules that guide the aggregation process (Scott and de Souza, 1990; Hovy, 1990; Dalianis, 1999; Shaw, 1998). Alternatively, these rules can be learned from a corpus. For instance, Walker et al. (2001) propose an overgenerate-and-rank approach to aggregation within the context of a spoken dialog application. Their system relies on a preference function for selecting an appropriate aggregation among multiple alternatives and assumes access to a large feature space expressing syntactic and pragmatic features of the input representations. The preference function is learned from a corpus of candidate aggregations marked with human ratings. Another approach is put forward by Cheng and Mellish (2000) who use a genetic algorithm in combination with a hand-crafted preference function to opportunist"
N06-1046,H05-1042,1,\N,Missing
N07-1044,briscoe-carroll-2002-robust,0,0.0386572,"Missing"
N07-1044,P06-1013,1,0.84589,"ghly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the “annotate automatically, correct manually” methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default to the dominant sens"
N07-1044,W04-0827,0,0.164611,"Missing"
N07-1044,P04-1036,0,0.0645709,"4 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottl"
N07-1044,W04-0837,0,0.0710926,"4 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Furthermore, current supervised approaches rarely outperform the simple heuristic of choosing the most common or dominant sense in the training data (henceforth “the first sense heuristic”), despite taking local context into account. One reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottl"
N07-1044,E06-1016,0,0.268159,"reason for this is the highly skewed distribution of word senses (McCarthy et al., 2004a). A large number of frequent content words is often associated with only one dominant sense. Obtaining the first sense via annotation is obviously costly and time consuming. Sense annotated corpora are not readily available for different languages or indeed sense inventories. Moreover, a word’s dominant sense will vary across domains and text genres (the word court in legal documents will most likely mean tribunal rather than yard). It is therefore not surprising that recent work (McCarthy et al., 2004a; Mohammad and Hirst, 2006; Brody et al., 2006) attempts to alleviate the annotation bottleneck by inferring the first sense automatically from raw text. Automatically acquired first senses will undoubtedly be noisy when compared to human annotations. Nevertheless, they can be usefully employed in two important tasks: (a) to create preliminary annotations, thus supporting the “annotate automatically, correct manually” methodology used to provide high volume annotation in the Penn Treebank project; and (b) in combination with supervised WSD methods that take context into account; for instance, such methods could default"
N07-1044,S01-1005,0,0.0353219,"ssociations between words and sense descriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest. The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We 348 Proceedings of NAACL HLT 2007, pages 348–355, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclud"
N07-1044,W04-0811,0,0.0145937,"escriptions automatically by querying an IR engine whose index terms have been compiled from the corpus of interest. The approach is inexpensive, languageindependent, requires minimal supervision, and uses no additional knowledge other than the word senses proper and morphological query expansions. We 348 Proceedings of NAACL HLT 2007, pages 348–355, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics evaluate our method on two tasks. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) data sets. Second, we simulate native speakers’ intuitions about the salience of word meanings and examine whether the estimated sense frequencies correlate with sense production data. In all cases our approach outperforms a naive baseline and yields performances comparable to state of the art. In the following section, we provide an overview of existing work on sense ranking. In Section 3, we introduce our IR-based method, and describe several sense ranking models. In Section 4, we present our results. Discussion of our results and future work conclude the paper (Section 5). 2 Related Work M"
N07-1044,H05-1051,0,0.0211367,"etween a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 1 Introduction Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and th"
N07-1044,H05-1097,0,0.0166808,"al engine to estimate the degree of association between a word and its sense descriptions. Experiments on the Senseval test materials yield state-ofthe-art performance. We also show that the estimated sense frequencies correlate reliably with native speakers’ intuitions. 1 Introduction Word sense disambiguation (WSD), the ability to identify the intended meanings (senses) of words in context, is crucial for accomplishing many NLP tasks that require semantic processing. Examples include paraphrase acquisition, discourse parsing, or metonymy resolution. Applications such as machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005) have also been shown to benefit from WSD. Given the importance of WSD for basic NLP tasks and multilingual applications, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data (see Yarowsky and Florian 2002; Mihalcea and Edmonds 2004 and the references therein). Although supervised methods typically achieve better performance than unsupervised alternatives, their applicability is limited to those words fo"
N10-1011,P08-1032,1,0.555257,"|w1 ). We are not aware of any previous work that empirically assesses which measure is best at capturing semantic similarity. We undertake such an empirical comparison as it is not a priory obvious how similarity is best modeled under a multimodal representation. 4 Experimental Setup In this section we discuss our experimental design for assessing the performance of the model presented above. We give details on our training procedure and parameter estimation and present the baseline method used for comparison with our model. Data We trained the multimodal topic model on the corpus created in Feng and Lapata (2008). It contains 3,361 documents that have been downloaded from the BBC News website.2 Each document comes with an image that depicts some of its content. The images are usually 203 pixels wide 2 http://news.bbc.co.uk/ and 152 pixels high. The average document length is 133.85 words. The corpus has 542,414 words in total. Our experiments used a vocabulary of 6,253 textual words. These were words that occurred at least five times in the whole corpus, excluding stopwords. The accompanying images were preprocessed as follows. We first extracted SIFT features from each image (150 on average) which we"
N10-1011,D09-1081,0,0.0306922,"Missing"
N10-1011,D09-1066,0,0.0309453,"Missing"
N10-1125,P08-1032,1,0.542857,"as annotations for the image. These annotations are undoubt831 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 831–839, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics edly noisy, but plenty and cost-free. Moreover, the collateral text is often longer and more informative in comparison to the few keywords reserved for each image in Corel. In this paper we propose a probabilistic image annotation model that learns to automatically label images under such noisy conditions. We use the database created in Feng and Lapata (2008) which consists of news articles, images, and their captions. Our model exploits the redundancy inherent in this multimodal dataset by assuming that images and their surrounding text are generated by a shared set of latent variables or topics. Specifically, we describe documents and images by a common multimodal vocabulary consisting of textual words and visual terms (visiterms). Due to polysemy and synonymy many words in this vocabulary will refer to the same underlying concept. Using Latent Dirichlet Allocation (LDA, Blei and Jordan 2003), a probabilistic model of text generation, we represe"
N10-1137,P09-1004,0,0.497965,"far between. Early work on lexicon acquisition focuses on identifying verbal alternations rather than their linkings. This is often done in conjunction with hand-crafted resources such as a taxonomy of possible alternations (McCarthy and Korhonen, 1998) or WordNet (McCarthy, 2002). Lapata (1999) proposes a corpus-based method that is less reliant on taxonomic resources, however focuses only on two specific verb alternations. Other work attempts to cluster verbs into semantic classes (e.g., Levin 1993) on the basis of their alternation behavior (Schulte im Walde and Brew, 2002). More recently, Abend et al. (2009) propose an unsupervised algorithm for argument identification that relies only on part-of-speech annotations, whereas Grenager and Manning (2006) focus on role induction which they formalize as probabilistic inference in a Bayesian network. Their model defines a joint probability distribution over the particular linking used together with a verb instance and for each verbal argument, its lemma, syntactic function as well as semantic role. Parameters in this model are estimated using the EM algorithm as the training instances include latent variables, namely the semantic roles and linkings. To"
N10-1137,J96-1002,0,0.0182756,", the features used to determine the canonical function must be restricted so that they give no cues about possible alternations. If they would, the model could learn to predict alternations, and therefore produce output closer to the observed syntactic rather than canonical function of an argument. To avoid this pitfall we only use features at or below the node representing the argument head in the parse tree apart from the predicate lemma (see Section 5 for details). Given these local argument features, a simple solution would be to use a standard classifier such as the logistic classifier (Berger et al., 1996) to learn the canonical function of arguments. However, this is problematic, because in our setting the training and application of the classifier happen on the same dataset. The model will over-adapt to the observed targets (i.e., the syntactic functions) and fail to learn appropriate canonical functions. Lexical sparsity is a contributing factor: the parameters associated with sparse lexical features will be unavoidably adjusted so that they are highly indicative of the syntactic function they occur with. One way to improve generalization is to incorporate a layer of latent variables into th"
N10-1137,D09-1002,1,0.427374,"Missing"
N10-1137,J02-3001,0,0.74589,"acle to the widespread application of these systems across different languages and text genres. In this paper we describe a method for inducing the semantic roles of verbal arguments directly from unannotated text. We formulate the role induction problem as one of detecting alternations and finding a canonical syntactic form for them. Both steps are implemented in a novel probabilistic model, a latent-variable variant of the logistic classifier. Our method increases the purity of the induced role clusters by a wide margin over a strong baseline. 1 (1) Introduction Semantic role labeling (SRL, Gildea and Jurafsky 2002) is the task of automatically classifying the arguments of a predicate with roles such as Agent, Patient or Location. These labels capture aspects of the semantics of the relationship between the predicate and the argument while abstracting over surface syntactic configurations. SRL has received much attention in recent years (Surdeanu et al., 2008; M`arquez et al., 2008), partly because of its potential to improve applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), summarizat"
N10-1137,W06-1601,0,0.670115,"n conjunction with hand-crafted resources such as a taxonomy of possible alternations (McCarthy and Korhonen, 1998) or WordNet (McCarthy, 2002). Lapata (1999) proposes a corpus-based method that is less reliant on taxonomic resources, however focuses only on two specific verb alternations. Other work attempts to cluster verbs into semantic classes (e.g., Levin 1993) on the basis of their alternation behavior (Schulte im Walde and Brew, 2002). More recently, Abend et al. (2009) propose an unsupervised algorithm for argument identification that relies only on part-of-speech annotations, whereas Grenager and Manning (2006) focus on role induction which they formalize as probabilistic inference in a Bayesian network. Their model defines a joint probability distribution over the particular linking used together with a verb instance and for each verbal argument, its lemma, syntactic function as well as semantic role. Parameters in this model are estimated using the EM algorithm as the training instances include latent variables, namely the semantic roles and linkings. To make inference tractable they limit the set of linkings to a small number and do not distinguish between different types of adjuncts. Our own wor"
N10-1137,P99-1051,0,0.0191701,"ntic annotations from an annotated corpus in one language onto an unannotated corpus in another language. And F¨urstenau and Lapata (2009) propose a method in which annotations are projected from a source corpus onto a target corpus, however within the same language. Unsupervised approaches to SRL have been few and far between. Early work on lexicon acquisition focuses on identifying verbal alternations rather than their linkings. This is often done in conjunction with hand-crafted resources such as a taxonomy of possible alternations (McCarthy and Korhonen, 1998) or WordNet (McCarthy, 2002). Lapata (1999) proposes a corpus-based method that is less reliant on taxonomic resources, however focuses only on two specific verb alternations. Other work attempts to cluster verbs into semantic classes (e.g., Levin 1993) on the basis of their alternation behavior (Schulte im Walde and Brew, 2002). More recently, Abend et al. (2009) propose an unsupervised algorithm for argument identification that relies only on part-of-speech annotations, whereas Grenager and Manning (2006) focus on role induction which they formalize as probabilistic inference in a Bayesian network. Their model defines a joint probabi"
N10-1137,J93-2004,0,0.0342597,"Missing"
N10-1137,J08-2001,0,0.297278,"Missing"
N10-1137,P98-2247,0,0.0505051,"beled instances. Pad´o and Lapata (2009) project role-semantic annotations from an annotated corpus in one language onto an unannotated corpus in another language. And F¨urstenau and Lapata (2009) propose a method in which annotations are projected from a source corpus onto a target corpus, however within the same language. Unsupervised approaches to SRL have been few and far between. Early work on lexicon acquisition focuses on identifying verbal alternations rather than their linkings. This is often done in conjunction with hand-crafted resources such as a taxonomy of possible alternations (McCarthy and Korhonen, 1998) or WordNet (McCarthy, 2002). Lapata (1999) proposes a corpus-based method that is less reliant on taxonomic resources, however focuses only on two specific verb alternations. Other work attempts to cluster verbs into semantic classes (e.g., Levin 1993) on the basis of their alternation behavior (Schulte im Walde and Brew, 2002). More recently, Abend et al. (2009) propose an unsupervised algorithm for argument identification that relies only on part-of-speech annotations, whereas Grenager and Manning (2006) focus on role induction which they formalize as probabilistic inference in a Bayesian n"
N10-1137,J05-1004,0,0.909412,"ts bear the same semantic role, even though they appear in different syntactic positions: sandwich is the object of eat in sentence (1-a) and its subject in (1-b) but it is in both instances assigned the role Patient. The example illustrates the passive alternation. The latter is merely one type of alternation, many others exist (Levin, 1993), and their computational treatment is one of the main challenges faced by semantic role labelers. Most SRL systems to date conceptualize semantic role labeling as a supervised learning problem and rely on role-annotated data for model training. PropBank (Palmer et al., 2005) has been widely used for the development of semantic role labelers as well as FrameNet (Fillmore et al., 2003). Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., Location or Time) whose interpretation is common across predicates. In addition to large amounts of role-annotated data, SRL systems often make use of a parser to obtain syntactic analyses which subsequently serve as input to a pi"
N10-1137,J08-2006,0,0.763719,"Missing"
N10-1137,P02-1029,0,0.0327139,"Missing"
N10-1137,D07-1002,1,0.890895,"ng (SRL, Gildea and Jurafsky 2002) is the task of automatically classifying the arguments of a predicate with roles such as Agent, Patient or Location. These labels capture aspects of the semantics of the relationship between the predicate and the argument while abstracting over surface syntactic configurations. SRL has received much attention in recent years (Surdeanu et al., 2008; M`arquez et al., 2008), partly because of its potential to improve applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), summarization (Melli et al., 2005), and machine translation (Wu and Fung, 2009). Given sentences (1-a) and (1-b) as input, an SRL system would have to identify the verb predicate a. b. [Michael]Agent eats [a sandwich]Patient . [A sandwich]Patient is eaten [by Michael]Agent . Here, sentence (1-b) is an alternation of (1-a). The verbal arguments bear the same semantic role, even though they appear in different syntactic positions: sandwich is the object of eat in sentence (1-a) and its subject in (1-b) but it is in both instances assigned the role Patient. The example illustrates the passive a"
N10-1137,P03-1002,0,0.110676,"ine. 1 (1) Introduction Semantic role labeling (SRL, Gildea and Jurafsky 2002) is the task of automatically classifying the arguments of a predicate with roles such as Agent, Patient or Location. These labels capture aspects of the semantics of the relationship between the predicate and the argument while abstracting over surface syntactic configurations. SRL has received much attention in recent years (Surdeanu et al., 2008; M`arquez et al., 2008), partly because of its potential to improve applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), summarization (Melli et al., 2005), and machine translation (Wu and Fung, 2009). Given sentences (1-a) and (1-b) as input, an SRL system would have to identify the verb predicate a. b. [Michael]Agent eats [a sandwich]Patient . [A sandwich]Patient is eaten [by Michael]Agent . Here, sentence (1-b) is an alternation of (1-a). The verbal arguments bear the same semantic role, even though they appear in different syntactic positions: sandwich is the object of eat in sentence (1-a) and its subject in (1-b) but it is in both instances assigned the role Pa"
N10-1137,N09-2004,0,0.0658079,"ents of a predicate with roles such as Agent, Patient or Location. These labels capture aspects of the semantics of the relationship between the predicate and the argument while abstracting over surface syntactic configurations. SRL has received much attention in recent years (Surdeanu et al., 2008; M`arquez et al., 2008), partly because of its potential to improve applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), summarization (Melli et al., 2005), and machine translation (Wu and Fung, 2009). Given sentences (1-a) and (1-b) as input, an SRL system would have to identify the verb predicate a. b. [Michael]Agent eats [a sandwich]Patient . [A sandwich]Patient is eaten [by Michael]Agent . Here, sentence (1-b) is an alternation of (1-a). The verbal arguments bear the same semantic role, even though they appear in different syntactic positions: sandwich is the object of eat in sentence (1-a) and its subject in (1-b) but it is in both instances assigned the role Patient. The example illustrates the passive alternation. The latter is merely one type of alternation, many others exist (Levi"
N10-1137,W04-3213,0,\N,Missing
N10-1137,A00-2034,0,\N,Missing
N10-1137,W08-2121,0,\N,Missing
N10-1137,N07-1070,0,\N,Missing
N10-1137,C98-2242,0,\N,Missing
N12-1051,S07-1002,0,0.0175039,"e to the nature of the task which is inherently subjective and application specific (e.g., a dolphin can be a Mammal to a biologist, but a Fish to a fisherman or someone visiting an aquarium). Nevertheless, we assessed the taxonomies produced by the HRG against the WordNet-like taxonomy described above using two measures, one that simply evaluates the grouping of the nouns into classes without taking account of their position in the taxonomy and one which evaluates the taxonomy directly. To evaluate a flat clustering into classes we use the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. Although informative, evaluation based solely on F-score puts the HRG model at a comparative disadvantage as the task of taxonomy induction is significantly more difficult than simple clustering. To overcome this disadvantage we propose an automatic method of evaluating taxonomies directly by first computing the walk distance between pairs of terms that share a gold-standard catego"
N12-1051,P99-1008,0,0.331634,"fit of an automatically induced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group w"
N12-1051,W06-3812,0,0.0169636,"ing the pairwise correlation between distances in each tree (Lapointe, 1995). This captures the intuition that a ‘good’ hierarchy is one in which items appearing near one another in the gold taxonomy also appear near one another in the induced one. It is also conceptually similar to the task-based IS - A evaluation (Snow et al., 2006) which has been traditionally used to evaluate taxonomies. Formally, let G = {g0,1 , g0,2 . . . gn,n−1 }, where ga,b indicates the walk distance between terms a and b Baselines We compared the HRG output against three baselines. The first is Chinese Whispers (CW; Biemann (2006)), a randomized graph-clustering algorithm which like the HRG also takes as input a graph with weighted edges. It produces a hard (flat) clustering over the nodes in the graph, where the number of clusters is determined automatically. Our second baseline is Brown et al.’s (1992) agglomerative clustering algorithm that induces a mapping from word types to classes. It starts with K classes for the K most frequent word types and then proceeds by alternately adding the next most frequent word to the class set and merging the two classes which result in the least decrease in the mutual information"
N12-1051,J92-4003,0,0.332032,"Missing"
N12-1051,P99-1016,0,0.450836,"ially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorithms aim at inducing flat clusters rather than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combination of clustering"
N12-1051,P05-1014,0,0.0341133,"h a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms. 1 Introduction The semantic knowledge encoded in lexical resources such as WordNet (Fellbaum, 1998) has been proven beneficial for several applications including question answering (Harabgiu et al., 2003), document classification (Hung et al., 2004), and textual entailment (Geffet and Dagan, 2005). As the effort involved in creating such resources manually is prohibitive (cost, consistency and coverage are often cited problems) and has to be repeated for new languages or domains, recent years have seen increased interest in automatic taxonomy induction. The task has assumed several guises, such as term extraction — finding the concepts of the taxonomy (Kozareva et al., 2008; Navigli et al., 2011), term relation discovery — learning whether any two terms stand in an semantic relation such as In this paper we propose an unsupervised approach to taxonomy induction. Given a corpus and a se"
N12-1051,N03-1011,0,0.0411675,"ced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if the"
N12-1051,C92-2082,0,0.144683,"nd introduce new ways of evaluating the fit of an automatically induced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in s"
N12-1051,D10-1073,0,0.0577429,"should also point out that our formulation of the inference problem utilizes very little corpus external knowledge other than the set of input terms, and could thus be easily applied to domains or languages where lexical resources are scarce. The Hierarchical Random Graph model (Clauset et al., 2008) has been applied to construct hierarchical decompositions from three sets of network data: a bacterial metabolic network; a food-web among grassland species; and the network of associations among terrorist cells. The only language-related application we are aware of concerns word sense induction. Klapaftis and Manandhar (2010) create a graph of contexts for a polysemous target word and use the HRG to organize them hierarchically, under the assumption that different tree heights correspond to different levels of sense granularity. 0.11 F D 0.50 C 1.00 E F E 1.00 C 1.00 D C A B D A B A A (a) Input graph B E F E (b) Binary tree B C F (c) Hierarchy D (d) Clusters Figure 1: Flow of information through the Hierarchical Random Graph algorithm. From a semantic network (1a), the model constructs a binary tree (1b). Edges in the semantic network are then used to compute the θ parameters for internal nodes in the tree; the ma"
N12-1051,P10-1150,0,0.0250406,"Missing"
N12-1051,P08-1119,0,0.46465,"Missing"
N12-1051,P98-2127,0,0.530406,"wing essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorithms aim at inducing flat clusters rather than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combinati"
N12-1051,J07-2002,1,0.419161,"Missing"
N12-1051,N04-1041,0,0.125396,"logical paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus clustering algorithms group words together if they share contextual features. Most of these algorithms aim at inducing flat clusters rather than taxonomies, with the exception of Brown et al. (1992) whose method induces binary trees. Contrary to the plethora of algorithms developed for relation discovery, methods dedicated to taxonomy learning have been few and far between. Caraballo (1999) was the first to induce a taxonomy from a corpus using a combination of clustering and pattern-based methods. Speci"
N12-1051,P98-2182,0,0.0250918,"ew ways of evaluating the fit of an automatically induced taxonomy against a gold-standard. In the following section we provide an overview of related work. Next, we describe our HRG model in more detail (Section 3) and present the resources and evaluation methodology used in our experiments (Section 4). We conclude the paper by presenting and discussing our results (Sections 4.1–4.4). 2 Related Work The bulk of previous work has focused on term relation discovery following essentially two methodological paradigms, pattern-based bootstrapping and clustering. The former approach (Hearst, 1992; Roark and Charniak, 1998; Berland and Charniak, 1999; Girju et al., 2003; Etzioni et al., 2005; Kozareva et al., 2008) utilizes a few hand-crafted seed patterns representative of taxonomic relations (e.g., IS - A, PART- OF, SIBLING) to extract instances from corpora. These instances are then used to extract new patterns which are in turn used to find new instances and so on. Clustering-based approaches have been mostly employed to discover IS - A and SIBLING relations (Lin, 1998; Caraballo, 1999; Pantel and Ravichandran, 2004). A common assumption is that words are related if they occur in similar contexts and thus c"
N12-1051,P06-1101,0,0.725838,"onomy Induction Using Hierarchical Random Graphs Trevor Fountain and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB t.fountain@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract IS - A , or PART- OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). This paper presents a novel approach for inducing lexical taxonomies automatically from text. We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use"
N12-1051,P09-1031,0,0.370993,"ng Hierarchical Random Graphs Trevor Fountain and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB t.fountain@sms.ed.ac.uk, mlap@inf.ed.ac.uk Abstract IS - A , or PART- OF (Hearst, 1992; Berland and Charniak, 1999), and taxonomy construction —- creating the taxonomy proper by organizing its terms hierarchically (Kozareva and Hovy, 2010; Navigli et al., 2011). Previous work has also focused on the complementary task of augmenting an existing taxonomy with missing information (Snow et al., 2006; Yang and Callan, 2009). This paper presents a novel approach for inducing lexical taxonomies automatically from text. We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a ta"
N12-1051,C98-2177,0,\N,Missing
N12-1051,C98-2122,0,\N,Missing
N12-1093,D10-1049,0,0.546232,"balized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (a) (b)"
N12-1093,H05-1042,1,0.888973,"text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2"
N12-1093,J07-2003,0,0.381389,"eneration in the air travel domain, (b) weather forecast generation, and (c) sportscasting. language. This grammar represents a set of trees which we encode compactly using a weighted hypergraph (or packed forest), a data structure that defines a probability (or weight) for each tree. Generation then boils down to finding the best derivation tree in the hypergraph which can be done efficiently using the Viterbi algorithm. In order to ensure that our generation output is fluent, we intersect our grammar with a language model and perform decoding using a dynamic programming algorithm (Huang and Chiang, 2007). Our model is conceptually simpler than previous approaches and encodes information about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader cl"
N12-1093,H94-1010,0,0.562578,"ary in this domain (henceforth W EATHER G OV) is comparable to ROBO C UP (345 words), however, the texts are longer (|w |= 29.3) and more varied. On average, each forecast has 4 sentences and the content selection problem is more challenging; only 5.8 out of the 36 records per scenario are mentioned in the text which roughly corresponds to 1.4 records per sentence. We used 25,000 scenarios from W EATHER G OV for training, 1,000 scenarios for development and 3,528 scenarios for testing. This is the same partition used in Angeli et al. (2010). For the air travel domain we used the ATIS dataset (Dahl et al., 1994), consisting of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical on1-B EST A NGELI k-B EST H UMAN W EATHER G OV Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. South wind. As high as 23 mph. Chance of precipitation is 20. Breezy, with a chance of showers. Mostly cloudy, with a high near 57. South wind between 3 and 9 mph. ATIS What what what what flights from Denver Phoenix ROBOCUP Pink9 to to Pink7 kicks Show me the flights from Denver to Phoenix Pink9 passes back to Pink7 A chance o"
N12-1093,W02-2112,0,0.136869,"nd structure of the target text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately ("
N12-1093,W05-1506,0,0.010118,"over1 .t) FS2,2 (temp1 ,max) with FS0,1 (temp1 ,start) ··· FS1,2 (skyCover1 ,start) ··· Figure 2: Partial hypergraph representation for the sentence “Sunny with a low around 30 .” For the sake of readability, we show a partial span on the first two words without weights on the hyperarcs. words) as well as the optimal segmentation of the text, provided we have a trained set of weights. The inside-outside algorithm is commonly used for estimating the weights of a PCFG. However, we first transform the CYK parser and our grammar into a hypergraph and then compute the weights using inside-outside. Huang and Chiang (2005) define a weighted directed hypergraph as follows: Definition 1 An ordered hypergraph H is a tuple hN, E,t, Ri, where N is a finite set of nodes, E is a finite set of hyperarcs and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ N is its head node, T (e) ∈ N ∗ is a set of tail nodes and f (e) is a monotonic weight function R|T (e) |to R and t ∈ N is a target node. Definition 2 We impose the arity of a hyperarc to be |e |= |T (e) |= 2, in other words, each head node is connected with at most two tail nodes. Given a context-free grammar G = hN, T,"
N12-1093,P07-1019,0,0.0841923,"a) query generation in the air travel domain, (b) weather forecast generation, and (c) sportscasting. language. This grammar represents a set of trees which we encode compactly using a weighted hypergraph (or packed forest), a data structure that defines a probability (or weight) for each tree. Generation then boils down to finding the best derivation tree in the hypergraph which can be done efficiently using the Viterbi algorithm. In order to ensure that our generation output is fluent, we intersect our grammar with a language model and perform decoding using a dynamic programming algorithm (Huang and Chiang, 2007). Our model is conceptually simpler than previous approaches and encodes information about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader cl"
N12-1093,P08-1067,0,0.0245233,"Missing"
N12-1093,C10-2062,0,0.510524,", 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (a) (b) Flight Search Day Temp"
N12-1093,W01-1812,0,0.0101706,"graph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph representation allows us to integrate an n-gram language model and perform decoding efficiently using k-best Viterbi search, optimizing what to say and how to say at the same time. 3.1 Grammar Definition Our model captures the inherent structure of the database with a number of CFG rewrite rules, in a similar way to how Liang et al. (2009) define Markov chains in the different levels of their hierarchical mod"
N12-1093,D09-1005,0,0.0332863,"destination, day, time). Our goal then is to reduce the tasks of content selection and surface realization into a common probabilistic pars754 ing problem. We do this by abstracting the structure of the database (and accompanying texts) into a PCFG whose probabilities are learned from training data.1 Specifically, we convert the database into rewrite rules and represent them as a weighted directed hypergraph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph r"
N12-1093,P09-1011,0,0.299928,"l content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and"
N12-1093,D11-1149,0,0.104573,"roduced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are automatically extracted and smoothed with domainspecific constraints in order to guarantee fluent output. Other related work (Wong and Mooney, 2007; Lu and Ng, 2011). has focused on generating natural language sentences from logical form (i.e., lambdaexpressions) using mostly synchronous context-free grammars (SCFGs). Similar to Angeli et al. (2010), we also present an end-to-end system that performs content selection and surface realization. However, rather than breaking up the generation task into a sequence of local decisions, we optimize what to say and how to say simultaneously. We do not learn mappings from a logical form, but rather focus on input which is less constrained, possibly more noisy and with a looser structure. Our key insight is to conv"
N12-1093,P02-1040,0,0.0868619,"two configurations of our system. A baseline that uses the top scoring derivation in each subgeneration (1- BEST) and another version which makes better use of our decoding algorithm and considers the best k derivations (i.e., 15 for W EATHER G OV, 40 for ATIS, and 25 for ROBO C UP). We compared our output to Angeli et al. (2010) whose approach is closest to ours and state-of-the-art on the W EATHER G OV domain. For ROBO C UP, we also compare against the bestpublished results (Kim and Mooney, 2010). Evaluation We evaluated system output automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?) and semantic correctness (does the meaning conveyed by the text correspond to the database input?). The subjects used a five point rating scale where a high number indicates better performance. We randomly selected 12 docSystem 1-B EST k-B EST A NGELI K IM -M OONEY ROBO C UP W EATHER"
N12-1093,N07-1022,0,0.307467,"ocal label assignments and their pairwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content selection and surface realization. Kim and Mooney (2010) adopt a two-stage approach: using a generative model similar to Liang et al. (2009), they first decide what to say and then verbalize the selected input with WASP−1 , an existing generation system (Wong and Mooney, 2007). In contrast, Angeli et al. (2010) propose a unified content selection and surface realization model which also operates over the alignment output produced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are"
N12-1093,D07-1071,0,0.0360778,"Missing"
N13-1105,J10-4006,0,0.0503316,"as well as the phases assigned to each cluster greatly influenced the semantic space. On both tasks, the best performing model had the relation partition described in Section 3.1. Section 5 reports our results on the test set using this model. Comparison Models We compared our quantum space against three classical distributional models. These include a simple semantic space, where a word’s meaning is a vector of co-occurrences with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional triples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010) and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008) For all three models we used parameters that have been reported in the literature as optimal. Specifically, for the simple co-occurrence-based space we follow the settings of Mitchell and Lapata (2010): a context window of five words on either side of the target word and 2,000 vector dimensions (i.e., the 2000 most common context words in the BNC). Vector components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall"
N13-1105,D12-1050,1,0.166698,"the BNC). Vector components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. For the neural language model, we adopted the best 854 Models SDS DM NLM QM WordSim353 Nelson Norms 0.433 0.318 0.196 0.535 0.151 0.123 0.091 0.185 Table 1: Performance of distributional models on WordSim353 dataset and Nelson et al.’s (1998) norms (test set). Correlation coefficients are all statistically significant (p < 0.01). performing parameters from our earlier comparison of different vector sources for distributional semantics (Blacoe and Lapata, 2012) where we also used the BNC for training. There we obtained best results with 50 dimensions, a context window of size 4, and an embedding learning rate of 10−9 . Our third comparison model uses Baroni and Lenci’s (2010) third-order tensor2 which they obtained from a very large dependency-parsed corpus containing approximately 2.3 billion words. Their tensor assigns a mutual information score to instances of word pairs w, v − and a linking word l. We obtained vectors → w from the tensor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l, v) contexts as dimensions. 5 Res"
N13-1105,W08-1301,0,0.0938502,"Missing"
N13-1105,W11-0114,0,0.0109669,"05). They respectively demonstrate that Latent Semantic Analysis (Landauer and Dumais, 1997) and the Hyperspace Analog to Language model (Lund and Burgess, 1996) are essentially Hilbert space formalisms, without, however, providing concrete ways of building these models beyond a few hand-picked examples. Interestingly, Bruza and Cole (2005) show how lexical operators may be contrived from corpus co-occurrence counts, albeit admitting to the fact that their operators do not provide sensical eigenkets, most likely because of the simplified method of populating the matrix from corpus statistics. Grefenstette et al. (2011) present a model for capturing semantic composition in a quantum theoretical context, although it appears to be reducible to the classical probabilistic paradigm. It does not make use of the unique aspects of quantum theory (e.g., entanglement, interference, or quantum collapse). Our own work follows Aerts and Czachor (2004) and Bruza and Cole (2005) in formulating a model that exhibits important aspects of quantum theory. Contrary to them, we present a fully-fledged semantic space rather than a proof-of-concept. We obtain quantum states (i.e., lexical representations) for each word by taking"
N13-1105,P12-1092,0,0.0051703,"sor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l, v) contexts as dimensions. 5 Results Our results are summarized in Table 1. As can be seen, the quantum model (QM) obtains performance superior to other better-known models such as Mitchell and Lapata’s (2010) simple semantic space (SDS), Baroni and Lenci’s (2010) distributional memory tensor (DM), and Collobert and Weston’s (2008) neural language model (NLM). Our results on the association norms are comparable to the state of the art (Silberer and Lapata, 2012; Griffiths et al., 2007). With regard to WordSim353, Huang et al. (2012) report correlations in the range of 0.713–0.769, however they use Wikipedia as a training corpus and a more sophisticated version of the NLM presented here, that takes into account global context and performs word sense discrimination. In the future, we also plan to evaluate our model on larger Wikipedia-scale corpora. We would also like to model semantic composition as our approach can do this easily by taking advantage of the notion of quantum measurement. Specifically, we 2 Available at http://clic.cimec.unitn.it/dm/. Models bar SDS pub, snack, restaurant, grill, coctail DM counter, rack,"
N13-1105,P98-2127,0,0.0580667,"model achieves results competitive with a variety of classical models. 1 Introduction The fields of cognitive science and natural language processing have recently produced an ensemble of semantic models which have an impressive track record of replicating human behavior and enabling real-world applications. Examples include simulations of word association (Denhi`ere and Lemaire, 2004; Griffiths et al., 2007), semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997; Griffiths et al., 2007), categorization (Laham, 2000), numerous studies of lexicon acquisition (Grefenstette, 1994; Lin, 1998), word sense discrimination (Sch¨utze, 1998), and paraphrase recognition (Socher et al., 2011). The term “semantic” derives from the intuition that words seen in the context of a given word contribute to its meaning (Firth, 1957). Although the specific details of the individual models differ, they all process a corpus of text as input and represent words (or concepts) in a (reduced) highdimensional space. In this paper, we explore the potential of quantum theory as a formal framework for capturing lexical meaning and modeling semantic processes such as word similarity and association (see Sect"
N13-1105,J98-1004,0,0.235866,"Missing"
N13-1105,D12-1130,1,0.807224,"of word pairs w, v − and a linking word l. We obtained vectors → w from the tensor following the methodology proposed in Blacoe and Lapata (2012) using 100 (l, v) contexts as dimensions. 5 Results Our results are summarized in Table 1. As can be seen, the quantum model (QM) obtains performance superior to other better-known models such as Mitchell and Lapata’s (2010) simple semantic space (SDS), Baroni and Lenci’s (2010) distributional memory tensor (DM), and Collobert and Weston’s (2008) neural language model (NLM). Our results on the association norms are comparable to the state of the art (Silberer and Lapata, 2012; Griffiths et al., 2007). With regard to WordSim353, Huang et al. (2012) report correlations in the range of 0.713–0.769, however they use Wikipedia as a training corpus and a more sophisticated version of the NLM presented here, that takes into account global context and performs word sense discrimination. In the future, we also plan to evaluate our model on larger Wikipedia-scale corpora. We would also like to model semantic composition as our approach can do this easily by taking advantage of the notion of quantum measurement. Specifically, we 2 Available at http://clic.cimec.unitn.it/dm/."
N13-1105,C98-2122,0,\N,Missing
N15-1113,P13-1035,0,0.292158,"Missing"
N15-1113,P14-1035,0,0.0763191,"Missing"
N15-1113,W09-1206,0,0.0203978,"Missing"
N15-1113,J10-3005,1,0.903795,"Missing"
N15-1113,E12-1065,0,0.0638555,"lability of large collections of digitized books and works of fiction has enabled researchers to observe cultural trends, address questions about language use and its evolution, study how individuals rise to and fall from fame, perform gender studies, and so on (Michel et al., 2010). Most existing work focuses on low-level analysis of word patterns, with a few notable exceptions. Elson et al. (2010) analyze 19th century British novels by constructing a conversational network with vertices corresponding to characters and weighted edges corresponding to the amount of conversational interaction. Elsner (2012) analyzes characters and their emotional trajectories, whereas Nalisnick and Baird (2013) identify a character’s enemies and allies in plays based on the sentiment of their utterances. Other work (Bamman et al., 2013, 2014) automatically infers latent character types (e.g., villains or heroes) in novels and movie plot summaries. Although we are not aware of any previous approaches to summarize screenplays, the field of computer vision is rife with attempts to summarize video (see Reed 2004 for an overview). Most techniques are based on visual information and rely on low-level cues such as moti"
N15-1113,P10-1015,0,0.0210182,"several competitive baselines. 2 Related Work Computer-assisted analysis of literary text has a long history, with the first studies dating back to the 1067 1960s (Mosteller and Wallace, 1964). More recently, the availability of large collections of digitized books and works of fiction has enabled researchers to observe cultural trends, address questions about language use and its evolution, study how individuals rise to and fall from fame, perform gender studies, and so on (Michel et al., 2010). Most existing work focuses on low-level analysis of word patterns, with a few notable exceptions. Elson et al. (2010) analyze 19th century British novels by constructing a conversational network with vertices corresponding to characters and weighted edges corresponding to the amount of conversational interaction. Elsner (2012) analyzes characters and their emotional trajectories, whereas Nalisnick and Baird (2013) identify a character’s enemies and allies in plays based on the sentiment of their utterances. Other work (Bamman et al., 2013, 2014) automatically infers latent character types (e.g., villains or heroes) in novels and movie plot summaries. Although we are not aware of any previous approaches to su"
N15-1113,W11-1902,0,0.0261719,"name or a pronoun). In cases where we cannot find a suitable listener, we assume the current speaker is the listener. We obtain character relations from the output of a semantic role labeler. Relations are denoted by verbs whose ARG0 and ARG1 roles are character names. We extract relations from the dialogue but also from scene descriptions. For example, in Figure 1 the description Suddenly, [...] he 6 https://www.gnu.org/software/glpk/ clubs her over the head contains the relation clubs(MAN,CATHERINE). Pronouns are resolved to their antecedent using the Stanford coreference resolution system (Lee et al., 2011). Sentiment We labeled lexical items in screenplays with sentiment values using the AFINN-96 lexicon (Nielsen, 2011), which is essentially a list of words scored with sentiment strength within the range [−5, +5]. The list also contains obscene words (which are often used in movies) and some Internet slang. By summing over the sentiment scores of individual words, we can work out the sentiment of an interaction between two characters, the sentiment of a scene (see Equation (17)), and even the sentiment between characters (e.g., who likes or dislikes whom in the movie in general). Main Character"
N15-1113,P14-5010,0,0.00253247,"matched against Wikipedia2 and IMDB3 and paired with corresponding user-written summaries, plot sections, loglines and taglines (taglines are short snippets used by marketing departments to promote a movie). We also collected metainformation regarding the movie’s genre, its actors, the production year, etc. ScriptBase contains movies comprising 23 genres; each movie is on average accompanied by 3 user summaries, 3 loglines, and 3 taglines. The corpus spans years 1909–2013. Some corpus statistics are shown in Figure 2. The scripts were further post-processed with the Stanford CoreNLP pipeline (Manning et al., 2014) to perform tagging, parsing, named entity recognition and coreference resolution. They were also annotated with semantic roles (e.g., ARG0, ARG1), using the MATE tools (Bj¨orkelund et al., 2009). Our summarization experiments focused on comedies and thrillers. We randomly selected 30 movies 2 http://en.wikipedia.org s1 s2 s3 s4 s5 s6 s7 ... s1 s2 s3 s4 s5 s6 s7 ... // Figure 3: Example of consecutive chain (top). Squares represent scenes in a screenplay. The bottom chain would not be allowed, since the connection between s3 and s5 makes it non-consecutive. for training/development and 65 movi"
N15-1113,P13-2085,0,0.0379912,"abled researchers to observe cultural trends, address questions about language use and its evolution, study how individuals rise to and fall from fame, perform gender studies, and so on (Michel et al., 2010). Most existing work focuses on low-level analysis of word patterns, with a few notable exceptions. Elson et al. (2010) analyze 19th century British novels by constructing a conversational network with vertices corresponding to characters and weighted edges corresponding to the amount of conversational interaction. Elsner (2012) analyzes characters and their emotional trajectories, whereas Nalisnick and Baird (2013) identify a character’s enemies and allies in plays based on the sentiment of their utterances. Other work (Bamman et al., 2013, 2014) automatically infers latent character types (e.g., villains or heroes) in novels and movie plot summaries. Although we are not aware of any previous approaches to summarize screenplays, the field of computer vision is rife with attempts to summarize video (see Reed 2004 for an overview). Most techniques are based on visual information and rely on low-level cues such as motion, color, or audio (e.g., Rasheed et al. 2005). Movie summarization is a special type of"
N15-1113,E06-1021,0,0.01252,"ted in Section 4 necessitates access to a gold standard of key scene chains representing the movie’s most important content. Our experiments concentrated on a sample of 95 movies (comedies and thrillers) from the ScriptBase corpus (Section 3). Performing the scene selection task for such a big corpus manually would be both time consuming and costly. Instead, we used distant supervision based on Wikipedia to automatically generate a gold standard. Specifically, we assume that Wikipedia plots are representative of the most important content in a movie. Using the alignment algorithm presented in Nelken and Shieber (2006), we align script sentences to Wikipedia plot sentences and assume that scenes with at least one alignment are part of the gold chain of scenes. We obtain many-to-many alignments using features such as lemma overlap and word stem similarity. When evaluated on four movies8 (from the training set) whose content was manually aligned to Wikipedia plots, the aligner achieved a precision of .53 at a recall rate of .82 at deciding whether a scene should be aligned. Scenes are ranked according to the number of alignments they contain. When creating gold chains at different compression rates, we start"
N15-1174,D13-1128,0,0.559557,"2015 Annual Conference of the North American Chapter of the ACL, pages 1505–1515, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics lation, where the task is to transform a source sentence E into its target translation F. We argue that generating descriptions for scenes is quite similar, but with a twist: the translation process is very loose and selective; there will always be objects in a scene not worth mentioning, and words in a description that will have no visual counterpart. Our key insight is to represent scenes via visual dependency relations (Elliott and Keller, 2013) corresponding to sentential descriptions. This allows us to create a large parallel corpus for training a statistical machine translation system, which we interface with a content selection component guiding the translation toward interesting or important scene content. Advantageously, our model can be used in the reverse direction, i.e., to generate scenes, without additional engineering effort. Our approach outperforms a number of competitive alternatives, when evaluated both automatically and by humans. 2 Related Work The task of image description generation has recently gained popularity"
N15-1174,P14-2074,0,0.0370006,"ollowing the feature learning procedure outlined in Kiros et al. (2014). 6 2nd 65.8 24.8 9.50 3rd 53.0 31.2 15.8 4th 44.7 31.3 15.8 5th 44.0 37.5 18.5 6th 37.5 58.0 4.50 Table 6: Proportion of SMT descriptions deemed accurate and relevant. System output evaluated for rank placements 1. . . 6. Table 4: Model comparison on scene description task using automatic measures. System Keyword Template SMT Human 1st 75.5 18.0 6.5 Results We evaluated system output automatically using (smoothed) BLUE and METEOR as calculated by NIST’s MultEval software5 using the human-written descriptions as reference. Elliott and Keller (2014) find that both metrics correlate well with human judgments. For a fair comparison, we force our model to output one description, i.e., the most relevant one. Our results are summarized in Table 4. As can be seen, our model (SMT) performs best both in terms of BLEU and METEOR. The templatebased generator (Template) obtains competitive performance which is not surprising, it incorporates some of the ingredients of the SMT system such as 4 We used the implementation at http://www.cs. toronto.edu/˜rkiros/multimodal.html. 5 ftp://jaguar.ncsl.nist.gov/mt/resources/ mteval-v13a-20091001.tar.gz 1512"
N15-1174,P03-1054,0,0.00690454,"a parallel corpus representing the arrangement of objects in a scene and their linguistic realization and then we move on to present our generation model. 4.1 Parallel Corpus Creation As mentioned earlier, each scene in the dataset has six descriptions (on average). For each linguistic description we create its corresponding visual encoding. We initially ground words and phrases by aligning them to pieces of clipart. We parse the descriptions using a dependency parser, and identify expressions that function as arguments (e.g., subject, object). In our experiments we used the Stanford parser (Klein and Manning, 2003) but any parser with similar output could have been used instead. Next, we compute the mutual information (MI) between arguments and clip-art objects defined as: MI(X,Y ) = p(x, y) ∑ ∑ p(x, y) log p(x)p(y) x∈X y∈Y (1) where X is the set of clip-art objects and Y the set of arguments found in the dataset. We assume that the visual rendering of an argument is the clip-art object with which its MI value is highest. Figure 3 shows argument-clipart pairs with high MI values. Having identified which objects in the scene are talked about, we move on to encode their spatial relations. We adopt the rel"
N15-1174,N03-1017,0,0.0160129,"combination of several models where: P(e|f) = exp ∑Kk=1 λk hk (f, e) ∑e0 exp ∑Kk=1 λk hk (f, e0 ) (13) and the decision decision rule is given by: K Finally, to instill some coherence in the descriptions, while avoiding overly repetitive discourse, we disallow objects to be selected more than four times: ∀s , sums = Surface realization The ILP selects all description-worthy pairs of clip art objects for a scene. Using the rules presented in Table 1 we create visual encodings for them (see Table 2, source side), and finally translate them into natural language using a Phrase-based SMT engine (Koehn et al., 2003). Specifically, given a source visual expression f, our aim is to find an equivalent target natural language description eˆ that maximizes the posterior probability: eˆ = argmax ∑ λk hk (f, e) e k=1 (14) where hk (f, e) is a scoring function representing important features for the translation of f into e. Examples include the language model of the target language, a reordering model, or several translation models. K is the number of models (or features) and λk are the weights of the log-linear combination. Typically, the weights Λ = [λ1 , . . . , λK ]T are optimized on a development set, by me"
N15-1174,P07-2045,0,0.00382015,"allow to learn translations for entire phrases instead of individual words. The basic idea behind PB translation is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally reorder the translated target phrases in order to compose the target sentence. For this purpose, phrase-tables are produced, in which a source phrase is listed together with several target phrases and the probability of translating the former into the latter. Throughout our experiments, we obtained translation models using the PB SMT framework implemented in MOSES (Koehn et al., 2007). Mike is kicking the ball nsubj,aux,verb,det,dobj The plane is flying in the sky det,nsubj,aux,verb,prep, det,pobj Table 3: Sample scenes with human-written descriptions and corresponding templates. 5 Model Comparison We evaluated the model described above through comparison to four alternatives, representing different modeling paradigms in the literature. Our first comparison model is based on templates, which are commonly used to produce descriptions for images (Elliott and Keller, 2013; Kulkarni et al., 2011). Rather than manually creating template rules we induce them from dependency-pars"
N15-1174,P12-1038,0,0.0585921,"os et al., 2014; Vinyals et al., 2014). Most methods assume no structural information on the image side either (images are represented as unstructured bags of regions or as feature vectors). A notable exception are Elliott and Keller (2013) who introduce visual dependency relations between objects and argue that such structured representations are beneficial for image description. A large body of work has focused on the complementary problem of matching sentences (Ordonez et al., 2011; Farhadi et al., 2010; Hodosh et al., 2013; 1506 Feng and Lapata, 2013; Mason and Charniak, 2014) or phrases (Kuznetsova et al., 2012; Kuznetsova et al., 2014) to an image from existing human authored descriptions. Sentence-based approaches embed images and descriptions into the same multidimensional space, and retrieve descriptions from images most similar to a query image. Phrase-based approaches are more involved in that phrases need to be composed into a description and extraneous information optionally removed. A common modeling choice is the use of Integer Linear Programming (ILP) which naturally allows to encode various wellformedness constraints (e.g., grammaticality). We are not aware of any previous work generatin"
N15-1174,Q14-1028,0,0.0140995,"et al., 2014). Most methods assume no structural information on the image side either (images are represented as unstructured bags of regions or as feature vectors). A notable exception are Elliott and Keller (2013) who introduce visual dependency relations between objects and argue that such structured representations are beneficial for image description. A large body of work has focused on the complementary problem of matching sentences (Ordonez et al., 2011; Farhadi et al., 2010; Hodosh et al., 2013; 1506 Feng and Lapata, 2013; Mason and Charniak, 2014) or phrases (Kuznetsova et al., 2012; Kuznetsova et al., 2014) to an image from existing human authored descriptions. Sentence-based approaches embed images and descriptions into the same multidimensional space, and retrieve descriptions from images most similar to a query image. Phrase-based approaches are more involved in that phrases need to be composed into a description and extraneous information optionally removed. A common modeling choice is the use of Integer Linear Programming (ILP) which naturally allows to encode various wellformedness constraints (e.g., grammaticality). We are not aware of any previous work generating descriptions for abstrac"
N15-1174,P14-2097,0,0.0122711,"e of templates or syntactic trees (Kiros et al., 2014; Vinyals et al., 2014). Most methods assume no structural information on the image side either (images are represented as unstructured bags of regions or as feature vectors). A notable exception are Elliott and Keller (2013) who introduce visual dependency relations between objects and argue that such structured representations are beneficial for image description. A large body of work has focused on the complementary problem of matching sentences (Ordonez et al., 2011; Farhadi et al., 2010; Hodosh et al., 2013; 1506 Feng and Lapata, 2013; Mason and Charniak, 2014) or phrases (Kuznetsova et al., 2012; Kuznetsova et al., 2014) to an image from existing human authored descriptions. Sentence-based approaches embed images and descriptions into the same multidimensional space, and retrieve descriptions from images most similar to a query image. Phrase-based approaches are more involved in that phrases need to be composed into a description and extraneous information optionally removed. A common modeling choice is the use of Integer Linear Programming (ILP) which naturally allows to encode various wellformedness constraints (e.g., grammaticality). We are not"
N15-1174,E12-1076,0,0.06336,"ves, when evaluated both automatically and by humans. 2 Related Work The task of image description generation has recently gained popularity in the natural language processing and computer vision communities. Several methods leverage recent advances in computer vision and generate novel sentences relying on object detectors, attribute predictors, action detectors, and pose estimators. Generation is performed using templates or syntactic rules which piece the description together while leveraging word-co-occurrence statistics (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013; Mitchell et al., 2012). Recent advances in neural language models have led to approaches which generate captions by conditioning on feature vectors from the output of a deep convolutional neural network without the use of templates or syntactic trees (Kiros et al., 2014; Vinyals et al., 2014). Most methods assume no structural information on the image side either (images are represented as unstructured bags of regions or as feature vectors). A notable exception are Elliott and Keller (2013) who introduce visual dependency relations between objects and argue that such structured representations are beneficial for im"
N15-1174,P02-1038,0,0.169988,"Missing"
N15-1174,P03-1021,0,0.00765148,"ession f, our aim is to find an equivalent target natural language description eˆ that maximizes the posterior probability: eˆ = argmax ∑ λk hk (f, e) e k=1 (14) where hk (f, e) is a scoring function representing important features for the translation of f into e. Examples include the language model of the target language, a reordering model, or several translation models. K is the number of models (or features) and λk are the weights of the log-linear combination. Typically, the weights Λ = [λ1 , . . . , λK ]T are optimized on a development set, by means of Minimum Error Rate Training (MERT; Och (2003)). One of the most popular instantiations of loglinear models in SMT are phrase-based (PB) models (Zens et al., 2002; Koehn et al., 2003). PB models allow to learn translations for entire phrases instead of individual words. The basic idea behind PB translation is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally reorder the translated target phrases in order to compose the target sentence. For this purpose, phrase-tables are produced, in which a source phrase is listed together with several target phrases and the probability of"
N15-1174,D11-1041,0,0.0851907,"outperforms a number of competitive alternatives, when evaluated both automatically and by humans. 2 Related Work The task of image description generation has recently gained popularity in the natural language processing and computer vision communities. Several methods leverage recent advances in computer vision and generate novel sentences relying on object detectors, attribute predictors, action detectors, and pose estimators. Generation is performed using templates or syntactic rules which piece the description together while leveraging word-co-occurrence statistics (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013; Mitchell et al., 2012). Recent advances in neural language models have led to approaches which generate captions by conditioning on feature vectors from the output of a deep convolutional neural network without the use of templates or syntactic trees (Kiros et al., 2014; Vinyals et al., 2014). Most methods assume no structural information on the image side either (images are represented as unstructured bags of regions or as feature vectors). A notable exception are Elliott and Keller (2013) who introduce visual dependency relations between objects and argue that suc"
N15-1174,2002.tmi-tutorials.2,0,0.044333,"r probability: eˆ = argmax ∑ λk hk (f, e) e k=1 (14) where hk (f, e) is a scoring function representing important features for the translation of f into e. Examples include the language model of the target language, a reordering model, or several translation models. K is the number of models (or features) and λk are the weights of the log-linear combination. Typically, the weights Λ = [λ1 , . . . , λK ]T are optimized on a development set, by means of Minimum Error Rate Training (MERT; Och (2003)). One of the most popular instantiations of loglinear models in SMT are phrase-based (PB) models (Zens et al., 2002; Koehn et al., 2003). PB models allow to learn translations for entire phrases instead of individual words. The basic idea behind PB translation is to segment the source sentence into phrases, then to translate each source phrase into a target phrase, and finally reorder the translated target phrases in order to compose the target sentence. For this purpose, phrase-tables are produced, in which a source phrase is listed together with several target phrases and the probability of translating the former into the latter. Throughout our experiments, we obtained translation models using the PB SMT"
N15-1181,E14-1027,1,0.846368,"on results show that our joint model learns accurate categories and feature types achieving results competitive with highly engineered approaches focusing exclusively on feature learning. 2 Related Work The problems of category formation and feature learning have been considered largely independently in the literature. Bayesian categorization models were pioneered by Anderson (1991) and recently reformalized by Sanborn et al. (2006). These models are aimed at replicating human behavior in small scale category acquisition studies, where a fixed set of simple (e.g., binary) features is assumed. Frermann and Lapata (2014) propose a model similar in spirit, which they apply to large scale corpora, while investigating incremental learning in the con1577 text of child category acquisition (see also Fountain and Lapata (2011) for a non-Bayesian approach). Their model associates sets of features with categories as a by-product of the learning process, however these feature sets are independent across categories and are not optimized during learning. Previous approaches on feature learning have primarily focused on emulating or complementing norming studies by automatically extracting normlike properties from textua"
N15-1181,D11-1122,1,0.842966,"ihood ratios provided by Strudel; following Baroni et al. (2010), we then cluster the vectors using K-means and the Cluto toolkit.2 As for the other models, we set the number of categories to K = 40. Metrics To assess the quality of the clusters produced by the models, we measure purity (pur; the extent to which each learnt cluster corresponds to a single gold class) as well as its inverse, collocation (col; the extent to which all items of a particular gold class are represented in a single learnt cluster). Both measures are based on set-overlap, and we also report their harmonic mean ( f 1; Lang and Lapata 2011). In addition, we report the V-measure (v1; Rosenberg and Hirschberg 2007) and its factors measuring the homogeneity of clusters (hom) and their completeness (com). The two factors intuitively correspond to purity and collocation, but are based on information-theoretic measures. Results Our results are summarized in Table 1. They show that BCF and Strudel perform almost identically, and both outperform BayesCat. BCF learns the categories from data, whereas for Strudel 1 http://homepages.inf.ed.ac.uk/s0897549/data/. 2 http://glaros.dtc.umn.edu/gkhome/cluto/cluto/ overview hom com v1 pur col f1"
N15-1181,D07-1043,0,0.00795976,"we then cluster the vectors using K-means and the Cluto toolkit.2 As for the other models, we set the number of categories to K = 40. Metrics To assess the quality of the clusters produced by the models, we measure purity (pur; the extent to which each learnt cluster corresponds to a single gold class) as well as its inverse, collocation (col; the extent to which all items of a particular gold class are represented in a single learnt cluster). Both measures are based on set-overlap, and we also report their harmonic mean ( f 1; Lang and Lapata 2011). In addition, we report the V-measure (v1; Rosenberg and Hirschberg 2007) and its factors measuring the homogeneity of clusters (hom) and their completeness (com). The two factors intuitively correspond to purity and collocation, but are based on information-theoretic measures. Results Our results are summarized in Table 1. They show that BCF and Strudel perform almost identically, and both outperform BayesCat. BCF learns the categories from data, whereas for Strudel 1 http://homepages.inf.ed.ac.uk/s0897549/data/. 2 http://glaros.dtc.umn.edu/gkhome/cluto/cluto/ overview hom com v1 pur col f1 BCF 0.68 0.64 0.66 0.59 0.52 0.55 BayesCat 0.65 0.59 0.62 0.57 0.45 0.50 S"
N16-1022,W03-0601,0,0.338471,"2016 Association for Computational Linguistics Dataset Verbs Acts Images Sen Des PPMI (Yao and Fei-Fei, 2010) 2 24 4800 N N Stanford 40 Actions (Yao et al., 2011) 33 40 9532 N N PASCAL 2012 (Everingham et al., 2015) 9 11 4588 N N 89 Actions (Le et al., 2013) 36 89 2038 N N TUHOI (Le et al., 2014) – 2974 10805 N N COCO-a (Ronchi and Perona, 2015) 140 162 10000 N Y HICO (Chao et al., 2015) 111 600 47774 Y N VerSe (our dataset) 90 163 3518 Y Y Figure 2: Google Image Search trying to disambiguate sit. All clusters pertain to the sit down sense, other senses (baby sit, convene) are not included. (Barnard et al., 2003; Loeff et al., 2006; Saenko and Darrell, 2008; Chen et al., 2015). VSD for nouns is helped by resources such as ImageNet (Deng et al., 2009), a large image database containing 1.4 million images for 21,841 noun synsets and organized according to the WordNet hierarchy. However, we are not aware of any previous work on VSD for verbs, and no ImageNet for verbs exists. Not only image retrieval would benefit from VSD for verbs, but also other multimodal tasks that have recently received a lot of interest, such as automatic image description and visual question answering (Karpathy and Li, 2015; Fan"
N16-1022,C08-1009,1,0.771948,"ction recognition datasets. Acts (actions) are verb-object pairs; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image descriptions are included. 2 Related Work There is an extensive literature on word sense disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with B"
N16-1022,N06-2015,0,0.0281592,"f the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dictionary to learn a latent LDA space overs senses, which they then used to construct sensespecific classifiers by exploiting the text surrounding an image. 2.1 Related Datasets Most of the datasets relevant for verb sense disambiguation were created by the computer vision community for the task of human action recognition (see Table 1 for an overview). These datasets are annotated with a limited number of actions, where an action is conceptualized as ver"
N16-1022,N15-1070,0,0.00980158,"distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dictionary to learn a latent LDA space overs senses, which they then used to construct sensespecific classifiers by exploiting the text surrounding an image. 2.1 Related Datasets Most of the datasets relevant for verb sense disambiguation were creat"
N16-1022,P97-1009,0,0.318417,"omparison of VerSe with existing action recognition datasets. Acts (actions) are verb-object pairs; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image descriptions are included. 2 Related Work There is an extensive literature on word sense disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sens"
N16-1022,P06-2071,0,0.0595377,"Missing"
N16-1022,P04-1036,0,0.0604996,"f VerSe with existing action recognition datasets. Acts (actions) are verb-object pairs; Sen indicates whether sense ambiguity is explicitly handled; Des indicates whether image descriptions are included. 2 Related Work There is an extensive literature on word sense disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has us"
N16-1022,D14-1162,0,0.111689,"Missing"
N16-1022,P15-1173,0,0.0198511,"approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dictionary to learn a latent LDA space overs senses, which they then used to construct sensespecific classifiers by exploiting the text surrounding an image. 2.1 Related Datasets Most of the datasets relevant for verb sense dis"
N16-1022,P10-4014,0,0.0324809,"disambiguation for nouns, verbs, adjectives and adverbs. Most of these approaches rely on lexical databases or sense inventories such as WordNet (Miller et al., 1990) or OntoNotes (Hovy et al., 2006). Unsupervised WSD approaches often rely on distributional representations, computed over the target word and its context (Lin, 1997; McCarthy et al., 2004; Brody and Lapata, 2008). Most supervised approaches use sense annotated corpora to extract linguistic features of the target word (context words, POS tags, collocation features), which are then fed into a classifier to disambiguate test data (Zhong and Ng, 2010). Recently, features based on sense-specific semantic vectors learned using large corpora and a sense inventory such as WordNet have been shown to achieve state-of-the-art results for supervised WSD (Rothe and Schutze, 2015; Jauhar et al., 2015). As mentioned in the introduction, all existing work on visual sense disambiguation has used nouns, starting with Barnard et al. (2003). Sense discrimination for web images was introduced by Loeff et al. (2006), who used spectral clustering over multimodal features from the images and web text. Saenko and Darrell (2008) used sense definitions in a dict"
N16-1035,P01-1017,0,0.0359183,"ions can be then used in classification tasks such as sentiment analysis (Socher et al., 2011b) and paraphrase detection (Socher et al., 2011a). Tai et al. (2015) learn distributed representations over syntactic trees by generalizing the LSTM architecture to tree-structured network topologies. The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability. Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured langu"
N16-1035,D14-1082,0,0.110483,"48.3 50.0 45.0 49.3 53.5 50.7 54.7 55.00 57.02 55.96 48.46 49.90 48.65 — — 55.4 58.9 31.6M 32.5M 43.1M 44.7M 55.29 57.79 56.73 60.67 Parser MSTParser-2nd T REE LSTM T REE LSTM* L D T REE LSTM NN parser* S-LSTM* Development UAS LAS 92.20 88.78 92.51 89.07 92.64 89.09 92.66 89.14 92.00 89.70 93.20 90.90 Test UAS LAS 91.63 88.44 91.79 88.53 91.97 88.69 91.99 88.69 91.80 89.60 93.10 90.90 Table 2: Performance of T REE LSTM and L D T REE LSTM on reranking the top dependency trees produced by the 2nd order MSTParser (McDonald and Pereira, 2006). Results for the NN and S-LSTM parsers are reported in Chen and Manning (2014) and Dyer et al. (2015), respectively. * indicates that the model is initialized with pre-trained word vectors. Table 1: Model accuracy on the MSR sentence completion task. The results of KN5, RNNME and RNNMEs are reported in Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDepNgram and LDepNgram in Gubbins and Vlachos (2013), depRNN+3gram and depRNN+4gram in Mirowski and Vlachos (2015), LBL in Mnih and Teh (2012), Skip-gram and Skipgram+RNNMEs in Mikolov et al. (2013a), and IV LBL in Mnih and Kavukcuoglu (2013); d is the hidden size and |θ |the number of parameters in a model. Table 1 pre"
N16-1035,de-marneffe-etal-2006-generating,0,0.0842167,"Missing"
N16-1035,P15-1033,0,0.0940012,"7 54.7 55.00 57.02 55.96 48.46 49.90 48.65 — — 55.4 58.9 31.6M 32.5M 43.1M 44.7M 55.29 57.79 56.73 60.67 Parser MSTParser-2nd T REE LSTM T REE LSTM* L D T REE LSTM NN parser* S-LSTM* Development UAS LAS 92.20 88.78 92.51 89.07 92.64 89.09 92.66 89.14 92.00 89.70 93.20 90.90 Test UAS LAS 91.63 88.44 91.79 88.53 91.97 88.69 91.99 88.69 91.80 89.60 93.10 90.90 Table 2: Performance of T REE LSTM and L D T REE LSTM on reranking the top dependency trees produced by the 2nd order MSTParser (McDonald and Pereira, 2006). Results for the NN and S-LSTM parsers are reported in Chen and Manning (2014) and Dyer et al. (2015), respectively. * indicates that the model is initialized with pre-trained word vectors. Table 1: Model accuracy on the MSR sentence completion task. The results of KN5, RNNME and RNNMEs are reported in Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDepNgram and LDepNgram in Gubbins and Vlachos (2013), depRNN+3gram and depRNN+4gram in Mirowski and Vlachos (2015), LBL in Mnih and Teh (2012), Skip-gram and Skipgram+RNNMEs in Mikolov et al. (2013a), and IV LBL in Mnih and Kavukcuoglu (2013); d is the hidden size and |θ |the number of parameters in a model. Table 1 presents a summary of our"
N16-1035,C96-1058,0,0.198659,"Missing"
N16-1035,D13-1143,0,0.0583684,"it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability. Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003). Mirowski and Vlachos (2015) re-implement the model of Gubbins and Vlachos (2013) with RNNs. They view sentences as sequences of words over a tree. While they ignore the tree structures themselves, we model them explicitly. Our model shares with other structured-based language models the"
N16-1035,P14-5010,0,0.00446218,"size. Training Details We trained our model with back propagation through time (Rumelhart et al., 1988) on an Nvidia 315 Microsoft Sentence Completion Challenge The task in the MSR Sentence Completion Challenge (Zweig and Burges, 2012) is to select the correct missing word for 1,040 SAT-style test sentences when presented with five candidate completions. The training set contains 522 novels from the Project Gutenberg which we preprocessed as follows. After removing headers and footers from the files, we tokenized and parsed the dataset into dependency trees with the Stanford Core NLP toolkit (Manning et al., 2014). The resulting training set contained 49M words. We converted all words to lower case and replaced those occurring five times or less with UNK. The resulting vocabulary size was 65,346 words. We randomly sampled 4,000 sentences from the training set as our validation set. The literature describes two main approaches to the sentence completion task based on word vectors and language models. In vector-based approaches, all words in the sentence and the five candidate words are represented by a vector; the candidate which has the highest average similarity with the sentence words is selected as"
N16-1035,E06-1011,0,0.0222085,"55.5 — — — 48.1M 1120M 1014M 1029M 48.0M 29.9M 40.2M 45.3M 33.2M 50.1M 67.3M 40.0 48.3 50.0 45.0 49.3 53.5 50.7 54.7 55.00 57.02 55.96 48.46 49.90 48.65 — — 55.4 58.9 31.6M 32.5M 43.1M 44.7M 55.29 57.79 56.73 60.67 Parser MSTParser-2nd T REE LSTM T REE LSTM* L D T REE LSTM NN parser* S-LSTM* Development UAS LAS 92.20 88.78 92.51 89.07 92.64 89.09 92.66 89.14 92.00 89.70 93.20 90.90 Test UAS LAS 91.63 88.44 91.79 88.53 91.97 88.69 91.99 88.69 91.80 89.60 93.10 90.90 Table 2: Performance of T REE LSTM and L D T REE LSTM on reranking the top dependency trees produced by the 2nd order MSTParser (McDonald and Pereira, 2006). Results for the NN and S-LSTM parsers are reported in Chen and Manning (2014) and Dyer et al. (2015), respectively. * indicates that the model is initialized with pre-trained word vectors. Table 1: Model accuracy on the MSR sentence completion task. The results of KN5, RNNME and RNNMEs are reported in Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDepNgram and LDepNgram in Gubbins and Vlachos (2013), depRNN+3gram and depRNN+4gram in Mirowski and Vlachos (2015), LBL in Mnih and Teh (2012), Skip-gram and Skipgram+RNNMEs in Mikolov et al. (2013a), and IV LBL in Mnih and Kavukcuoglu (2013)"
N16-1035,P15-2084,0,0.0855375,"re used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003). Mirowski and Vlachos (2015) re-implement the model of Gubbins and Vlachos (2013) with RNNs. They view sentences as sequences of words over a tree. While they ignore the tree structures themselves, we model them explicitly. Our model shares with other structured-based language models the ability to take dependency information into account. It differs in the following respects: (a) it does not artificially restrict the depth of the dependencies it considers and can thus be viewed as an infinite order dependency language model; (b) it not only estimates the probability of a string but is also capable of generating dependen"
N16-1035,D14-1162,0,0.0893681,", K = 2), and L D T REE LSTM (d = 200, 2 layers, K = 4). We also include the performance of two neural network-based dependency parsers; Chen and Manning (2014) use a neural network classifier to predict the correct transition (NN parser); Dyer et al. (2015) also implement a transition-based dependency parser using LSTMs to represent the contents of the stack and buffer in a continuous space. As can be seen, both T REE LSTM and L D T REE LSTM outperform the baseline MSTParser, with L D T REE LSTM performing best. We also initialized the word embedding matrix We with pre-trained GLOVE vectors (Pennington et al., 2014). We obtained a slight improvement over T REE LSTM (T REE LSTM* in Table 2; d = 200, 2 layer, K = 4) but no improvement over L D T REE LSTM. Finally, notice that L D T REE LSTM is slightly better than the NN parser in terms of UAS but worse than the S-LSTM parser. In the future, we would like to extend our model so that it takes labeled dependency information into account. 3.4 Tree Generation This section demonstrates how to use a trained L D T REE LSTM to generate tree samples. The generation starts at the ROOT node. At each time step t, for each node wt , we add a new edge and node to 4 http"
N16-1035,J01-2004,0,0.0870691,"arned representations can be then used in classification tasks such as sentiment analysis (Socher et al., 2011b) and paraphrase detection (Socher et al., 2011a). Tai et al. (2015) learn distributed representations over syntactic trees by generalizing the LSTM architecture to tree-structured network topologies. The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability. Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters o"
N16-1035,Q15-1013,0,0.0132129,"e-structured network topologies. The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability. Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003). Mirowski and Vlachos (2015) re-implement the model of Gubbins and Vlachos (2013) with RNNs. They view sentences as sequences of words over a tree. While they ignore the tree structures themselve"
N16-1035,P08-1066,0,0.0261582,"ing the LSTM architecture to tree-structured network topologies. The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability. Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based language models for specific applications such as machine translation (Shen et al., 2008; Zhang, 2009; Sennrich, 2015), speech recognition (Chelba et al., 1997) or sentence completion (Gubbins and Vlachos, 2013). All instances of these models apply Markov assumptions on the dependency tree, and adopt standard n-gram smoothing methods for reliable parameter estimation. Emami et al. (2003) and Sennrich (2015) estimate the parameters of a structured language model using feed-forward neural networks (Bengio et al., 2003). Mirowski and Vlachos (2015) re-implement the model of Gubbins and Vlachos (2013) with RNNs. They view sentences as sequences of words over a tree. While they ignore"
N16-1035,D11-1014,0,0.00767948,"Missing"
N16-1035,P15-1150,0,0.0512172,"ego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Our approach is related to but ultimately different from recursive neural networks (Pollack, 1990) a class of models which operate on structured inputs. Given a (binary) parse tree, they recursively generate parent representations in a bottom-up fashion, by combining tokens to produce representations for phrases, and eventually the whole sentence. The learned representations can be then used in classification tasks such as sentiment analysis (Socher et al., 2011b) and paraphrase detection (Socher et al., 2011a). Tai et al. (2015) learn distributed representations over syntactic trees by generalizing the LSTM architecture to tree-structured network topologies. The key feature of our model is not so much that it can learn semantic representations of phrases or sentences, but its ability to predict tree structure and estimate its probability. Syntactic language models have a long history in NLP dating back to Chelba and Jelinek (2000) (see also Roark (2001) and Charniak (2001)). These models differ in how grammar structures in a parsing tree are used when predicting the next word. Other work develops dependency-based lan"
N16-1035,N03-1033,0,0.0854385,"erank the top K dependency trees produced by the second order MSTParser (McDon3 LSTMs and BiLSTMs were also trained with NCE (s = d/2; hyperparameters were tuned on the development set). ald and Pereira, 2006).4 We follow closely the experimental setup of Chen and Manning (2014) and Dyer et al. (2015). Specifically, we trained T REE LSTM and L D T REE LSTM on Penn Treebank sections 2–21. We used section 22 for development and section 23 for testing. We adopted the Stanford basic dependency representations (De Marneffe et al., 2006); part-of-speech tags were predicted with the Stanford Tagger (Toutanova et al., 2003). We trained T REE LSTM and L D T REE LSTM as language models (singletons were replaced with UNK) and did not use any POS tags, dependency labels or composition features, whereas these features are used in Chen and Manning (2014) and Dyer et al. (2015). We tuned d, the number of layers, and K on the development set. Table 2 reports unlabeled attachment scores (UAS) and labeled attachment scores (LAS) for the MSTParser, T REE LSTM (d = 300, 1 layer, K = 2), and L D T REE LSTM (d = 200, 2 layers, K = 4). We also include the performance of two neural network-based dependency parsers; Chen and Man"
N16-1035,D13-1140,0,0.0156452,"lity of a word w being from our model ˆ D (wt )) P(w| Pd (w, D (wt )) is P(w| ˆ D (wt ))+kPn (w) . We apply NCE to large vocabulary models with the following training objective: L NCE (θ) = − 1 |T | ∑ |S |T∑ ∈S t=1  log Pd (wt , D (wt )) 3.2  k + ∑ log[1 − Pd (w˜ t, j , D (wt ))] j=1 where w˜ t, j is a word sampled from the noise distribution Pn (w). We use smoothed unigram frequencies (exponentiating by 0.75) as the noise distribution Pn (w) (Mikolov et al., 2013b). We initialize ln Zˆ = 9 as suggested in Chen et al. (2015), but instead of keeping it fixed we also learn Zˆ during training (Vaswani et al., 2013). We set k = 20. 3 Experiments We assess the performance of our model on two tasks: the Microsoft Research (MSR) sentence completion challenge (Zweig and Burges, 2012), and dependency parsing reranking. We also demonstrate the tree generation capability of our models. In the following, we first present details on model training and then present our results. We implemented our models using the Torch library (Collobert et al., 2011) and our code is available at https:// github.com/XingxingZhang/td-treelstm. 3.1 GPU Card with a mini-batch size of 64. The objective (NLL or NCE) was minimized by st"
N16-1035,W12-2704,0,0.0524272,"g objective: L NCE (θ) = − 1 |T | ∑ |S |T∑ ∈S t=1  log Pd (wt , D (wt )) 3.2  k + ∑ log[1 − Pd (w˜ t, j , D (wt ))] j=1 where w˜ t, j is a word sampled from the noise distribution Pn (w). We use smoothed unigram frequencies (exponentiating by 0.75) as the noise distribution Pn (w) (Mikolov et al., 2013b). We initialize ln Zˆ = 9 as suggested in Chen et al. (2015), but instead of keeping it fixed we also learn Zˆ during training (Vaswani et al., 2013). We set k = 20. 3 Experiments We assess the performance of our model on two tasks: the Microsoft Research (MSR) sentence completion challenge (Zweig and Burges, 2012), and dependency parsing reranking. We also demonstrate the tree generation capability of our models. In the following, we first present details on model training and then present our results. We implemented our models using the Torch library (Collobert et al., 2011) and our code is available at https:// github.com/XingxingZhang/td-treelstm. 3.1 GPU Card with a mini-batch size of 64. The objective (NLL or NCE) was minimized by stochastic gradient descent. Model parameters were uniformly initialized in [−0.1, 0.1]. We used the NCE objective on the MSR sentence completion task (due to the large"
N16-1035,P12-1063,0,0.0158724,"2.00 89.70 93.20 90.90 Test UAS LAS 91.63 88.44 91.79 88.53 91.97 88.69 91.99 88.69 91.80 89.60 93.10 90.90 Table 2: Performance of T REE LSTM and L D T REE LSTM on reranking the top dependency trees produced by the 2nd order MSTParser (McDonald and Pereira, 2006). Results for the NN and S-LSTM parsers are reported in Chen and Manning (2014) and Dyer et al. (2015), respectively. * indicates that the model is initialized with pre-trained word vectors. Table 1: Model accuracy on the MSR sentence completion task. The results of KN5, RNNME and RNNMEs are reported in Mikolov (2012), LSA and RNN in Zweig et al. (2012), UDepNgram and LDepNgram in Gubbins and Vlachos (2013), depRNN+3gram and depRNN+4gram in Mirowski and Vlachos (2015), LBL in Mnih and Teh (2012), Skip-gram and Skipgram+RNNMEs in Mikolov et al. (2013a), and IV LBL in Mnih and Kavukcuoglu (2013); d is the hidden size and |θ |the number of parameters in a model. Table 1 presents a summary of our results together with previoulsy published results. The best performing word vector model is IV LBL (Mnih and Kavukcuoglu, 2013) with an accuracy of 55.5, while the best performing single language model is LBL (Mnih and Teh, 2012) with an accuracy of 54"
N16-1035,D15-1042,0,\N,Missing
N18-1137,H05-1042,1,0.56684,"l-purpose content selection mechanism.1 We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention. 1 Introduction A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts (Barzilay and Lapata, 2005; Kim and Mooney, 2010; Liang et al., 2009). These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection). Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks (Bahdanau et al., 2015; Ranzato et al., 2016) have led to the use of large scale datasets containing loosely related data and text pairs. A prime example are online data sources like DBPedia (Auer et al., 2007) and Wikipedia and"
N18-1137,W10-4217,0,0.0419439,"work in Section 2 and describe the MIL-based content selection approach in Section 3. We explain how the generator is trained in Section 4 and present evaluation experiments in Section 5. Section 7 concludes the paper. 2 Related Work Previous attempts to exploit loosely aligned data and text corpora have mostly focused on extracting verbalisation spans for data units. Most approaches work in two stages: initially, data units are aligned with sentences from related corpora using some heuristics and subsequently extra content is discarded in order to retain only text spans verbalising the data. Belz and Kow (2010) obtain verbalisation spans using a measure of strength of association between data units and words, Walter et al. (2013) extract textual patterns from paths in dependency trees while Mrabet et al. (2016) rely on crowd-sourcing. Perez-Beltrachini and Gardent (2016) learn shared representations for data units and sentences reduced to subjectpredicate-object triples with the aim of extracting verbalisations for knowledge base properties. Our work takes a step further, we not only induce datato-text alignments but also learn generators that produce short texts verbalising a set of facts. Our work"
N18-1137,C16-1333,0,0.0255169,"Missing"
N18-1137,D15-1085,0,0.0293451,"Missing"
N18-1137,E17-1060,0,0.0419985,"n dependency trees while Mrabet et al. (2016) rely on crowd-sourcing. Perez-Beltrachini and Gardent (2016) learn shared representations for data units and sentences reduced to subjectpredicate-object triples with the aim of extracting verbalisations for knowledge base properties. Our work takes a step further, we not only induce datato-text alignments but also learn generators that produce short texts verbalising a set of facts. Our work is closest to recent neural network models which learn generators from independently edited data and text resources. Most previous work (Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2017; Liu et al., 2017) targets the generation of single sentence biographies from Wikipedia infoboxes, while Wiseman et al. (2017) generate game summary documents from a database of basketball games where the input is always the same set of table fields. In contrast, in our scenario, the input data varies from one entity (e.g., athlete) to another (e.g., scientist) and properties might be present or not due to data incompleteness. Moreover, our generator is enhanced with a content selection mechanism based on multi-instance learning. MIL-based techniques have been previously app"
N18-1137,J08-4005,1,0.713928,"ic gradient descent and a fixed learning rate of 0.001. Block sizes were set to 40 (base), 60 (MTL) and 50 (RL). Weights for the MTL objective were also tuned experimentally; we set λ = 0.1 for the first four epochs (training focuses on alignment prediction) and switched to λ = 0.9 for the remaining epochs. Content Alignment We optimized content alignment on the development set against manual alignments. Specifically, two annotators aligned 132 sentences to their infoboxes. We used the Yawat annotation tool (Germann, 2008) and followed the alignment guidelines (and evaluation metrics) used in Cohn et al. (2008). The inter-annotator agreement using macro-averaged f-score was 0.72 (we treated one annotator as the reference and the other one as hypothetical system output). Alignment sets were extracted from the model’s output (cf. Section 3) by optimizing the threshold avg(sim) + a ∗ std(sim) where sim denotes the similarity between the set of property values and words, and a is empirically set to 0.75; avg and std are the mean and standard deviation of sim scores across the development set. Each word was aligned to a property-value if their similarity exceeded a threshold of 0.22. Our best content ali"
N18-1137,P08-4006,0,0.0132666,"with the base encoder-decoder model and trained for 35 additional epochs with stochastic gradient descent and a fixed learning rate of 0.001. Block sizes were set to 40 (base), 60 (MTL) and 50 (RL). Weights for the MTL objective were also tuned experimentally; we set λ = 0.1 for the first four epochs (training focuses on alignment prediction) and switched to λ = 0.9 for the remaining epochs. Content Alignment We optimized content alignment on the development set against manual alignments. Specifically, two annotators aligned 132 sentences to their infoboxes. We used the Yawat annotation tool (Germann, 2008) and followed the alignment guidelines (and evaluation metrics) used in Cohn et al. (2008). The inter-annotator agreement using macro-averaged f-score was 0.72 (we treated one annotator as the reference and the other one as hypothetical system output). Alignment sets were extracted from the model’s output (cf. Section 3) by optimizing the threshold avg(sim) + a ∗ std(sim) where sim denotes the similarity between the set of property values and words, and a is empirically set to 0.75; avg and std are the mean and standard deviation of sim scores across the development set. Each word was aligned"
N18-1137,I17-1004,0,0.0211851,"er et al., 2014) using an encoder-decoder architecture as its backbone. Lebret et al. (2016) introduce the task of generating biographies from Wikipedia data, however they focus on single sentence generation. We generalize the task to multi-sentence text, and highlight the limitations of the standard attention mechanism which is often used as a proxy for content selection. When exposed to sub-sequences that do not correspond to any facts in the input, the soft attention mechanism will still try to justify the sequence and somehow distribute the attention weights over the input representation (Ghader and Monz, 2017). The decoder will still memorise high frequency sub-sequences in spite of these not being supported by any facts in the input. We propose to alleviate these shortcom1516 Proceedings of NAACL-HLT 2018, pages 1516–1527 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics (a) (b) Robert Joseph Flaherty, (February 16, 1884 July 23, 1951) was an American film-maker who directed and produced the first commercially successful feature-length documentary film, Nanook of the North (1922). The film made his reputation and nothing in his later life fully equalled its"
N18-1137,P11-1055,0,0.0565494,"athlete) to another (e.g., scientist) and properties might be present or not due to data incompleteness. Moreover, our generator is enhanced with a content selection mechanism based on multi-instance learning. MIL-based techniques have been previously applied to a variety of problems including image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Carbonetto et al., 2008; Cour et al., 2011), text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015; Karpathy and Fei-Fei, 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011). The application of MIL to content selection is novel to our knowledge. We show how to incorporate content selection into encoder-decoder architectures following training regimes based on multi-task learning and reinforcement learning. Multi-task learning aims to improve a main task by incorporating joint learning of one or more related auxiliary tasks. It has been applied with success to a variety of sequence-prediction tasks focus1517 ing mostly on morphosyntax. Examples include chunking, tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name er"
N18-1137,C10-2062,0,0.0262729,"mechanism.1 We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention. 1 Introduction A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts (Barzilay and Lapata, 2005; Kim and Mooney, 2010; Liang et al., 2009). These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection). Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks (Bahdanau et al., 2015; Ranzato et al., 2016) have led to the use of large scale datasets containing loosely related data and text pairs. A prime example are online data sources like DBPedia (Auer et al., 2007) and Wikipedia and their associated text"
N18-1137,D16-1128,0,0.353597,"Missing"
N18-1137,P09-1011,0,0.363791,"lti-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention. 1 Introduction A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts (Barzilay and Lapata, 2005; Kim and Mooney, 2010; Liang et al., 2009). These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection). Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks (Bahdanau et al., 2015; Ranzato et al., 2016) have led to the use of large scale datasets containing loosely related data and text pairs. A prime example are online data sources like DBPedia (Auer et al., 2007) and Wikipedia and their associated texts which 1 Our code an"
N18-1137,D15-1166,0,0.025228,"oring property-value: |s| SP s = ∑ maxi∈{1,...,|P |} pi • wt (3) t=1 Equation (4) defines our objective which encourages related properties P and sentences s to have higher similarity than other P ′ 6= P and s′ 6= s: LCA = max(0, SP s − SP s ′ + 1) +max(0, SP s − SP ′ s + 1) each word. The second approach relies on reinforcement learning for adjusting the probability distribution of word sequences learnt by a standard word prediction training algorithm. 4.1 Encoder-Decoder Base Generator We follow a standard attention based encoderdecoder architecture for our generator (Bahdanau et al., 2015; Luong et al., 2015). Given a set of properties X as input, the model learns to predict an output word sequence Y which is a verbalisation of (part of) the input. More precisely, the generation of sequence Y is conditioned on input X : |Y | P(Y |X ) = ∏ P(yt |y1:t−1 , X ) (5) t=1 The encoder module constitutes an intermediate representation of the input. For this, we use the property-set encoder described in Section 3 which outputs vector representations {p1 , · · · , p|X |} for a set of property-value pairs. The decoder uses an LSTM and a soft attention mechanism (Luong et al., 2015) to generate one word yt at a"
N18-1137,P14-5010,0,0.00708115,"Missing"
N18-1137,N16-1086,0,0.164516,"where the data corresponds to DBPedia facts and texts are Wikipedia abstracts about people. Figure 1 shows an example for the film-maker Robert Flaherty, the Wikipedia infobox, and the corresponding abstract. We wish to bootstrap a data-to-text generator that learns to verbalise properties about an entity from a loosely related example text. Given the set of properties in Figure (1a) and the related text in Figure (1b), we want to learn verbalisations for those properties that are mentioned in the text and produce a short description like the one in Figure (1c). In common with previous work (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017) our model draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014) using an encoder-decoder architecture as its backbone. Lebret et al. (2016) introduce the task of generating biographies from Wikipedia data, however they focus on single sentence generation. We generalize the task to multi-sentence text, and highlight the limitations of the standard attention mechanism which is often used as a proxy for content selection. When exposed to sub-sequences that do not correspond to any facts in the input, the soft"
N18-1137,P02-1040,0,0.101247,"de f ender of the entity zanetti are “[NAME] played as [POSITION].” and “ Zanetti played as defender.” respectively. Automatic Evaluation Table 3 shows the results of automatic evaluation using BLEU-4 1522 Model Templ ED EDMTL EDRL Abstract 5.47 13.46 13.57 12.97 RevAbs 6.43 35.89 37.18 35.74 Table 3: BLEU-4 results using the original Wikipedia abstract (Abstract) as reference and crowd-sourced revised abstracts (RevAbs) for template baseline (Templ), standard encoder-decoder model (ED), and our contentbased models trained with multi-task learning (EDMTL) and reinforcement learning (EDRL ). (Papineni et al., 2002) against the noisy Wikipedia abstracts. Considering these as a gold standard is, however, not entirely satisfactory for two reasons. Firstly, our models generate considerably shorter text and will be penalized for not generating text they were not supposed to generate in the first place. Secondly, the model might try to reproduce what is in the imperfect reference but not supported by the input properties and as a result will be rewarded when it should not. To alleviate this, we crowd-sourced using AMT a revised version of 200 randomly selected abstracts from the test set. Crowdworkers were sh"
N18-1137,D14-1162,0,0.0815333,"nces 3.51±1.99 3.46±1.94 3.22±1.72 tokens 74.13±43.72 72.85±42.54 66.81±38.16 properties 14.97±8.82 14.96±8.85 21.6±9.97 sent.len 21.06±8.87 21.03±8.85 20.77±8.74 Table 2: Dataset statistics. (sent.len). For the content aligner (cf. Section 3), each sentence constitutes a training instance, and as a result the sizes of the train and development sets are 796,446 and 153,096, respectively. Training Configuration We adjusted all models’ hyperparameters according to their performance on the development set. The encoders for both content selection and generation models were initialised with GloVe (Pennington et al., 2014) pre-trained vectors. The input and hidden unit dimension was set to 200 for content selection and 100 for generation. In all models, we used encoder biLSTMs and decoder LSTM (regularised with a dropout rate of 0.3 (Zaremba et al., 2014)) with one layer. Content selection and generation models (base encoder-decoder and MTL) were trained for 20 epochs with the ADAM optimiser (Kingma and Ba, 2014) using a learning rate of 0.001. The reinforcement learning model was initialised with the base encoder-decoder model and trained for 35 additional epochs with stochastic gradient descent and a fixed le"
N18-1137,S16-2027,1,0.822118,"pts to exploit loosely aligned data and text corpora have mostly focused on extracting verbalisation spans for data units. Most approaches work in two stages: initially, data units are aligned with sentences from related corpora using some heuristics and subsequently extra content is discarded in order to retain only text spans verbalising the data. Belz and Kow (2010) obtain verbalisation spans using a measure of strength of association between data units and words, Walter et al. (2013) extract textual patterns from paths in dependency trees while Mrabet et al. (2016) rely on crowd-sourcing. Perez-Beltrachini and Gardent (2016) learn shared representations for data units and sentences reduced to subjectpredicate-object triples with the aim of extracting verbalisations for knowledge base properties. Our work takes a step further, we not only induce datato-text alignments but also learn generators that produce short texts verbalising a set of facts. Our work is closest to recent neural network models which learn generators from independently edited data and text resources. Most previous work (Lebret et al., 2016; Chisholm et al., 2017; Sha et al., 2017; Liu et al., 2017) targets the generation of single sentence biogr"
N18-1137,C16-1059,0,0.0215917,"offmann et al., 2011). The application of MIL to content selection is novel to our knowledge. We show how to incorporate content selection into encoder-decoder architectures following training regimes based on multi-task learning and reinforcement learning. Multi-task learning aims to improve a main task by incorporating joint learning of one or more related auxiliary tasks. It has been applied with success to a variety of sequence-prediction tasks focus1517 ing mostly on morphosyntax. Examples include chunking, tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015), and machine translation (Luong et al., 2016). Reinforcement learning (Williams, 1992) has also seen popularity as a means of training neural networks to directly optimize a taskspecific metric (Ranzato et al., 2016) or to inject task-specific knowledge (Zhang and Lapata, 2017). We are not aware of any work that compares the two training methods directly. Furthermore, our reinforcement learning-based algorithm differs from previous text generation approaches (Ranzato et al., 2016; Zhang and Lapata, 2017) in that it is applied to documents rather than"
N18-1137,P16-2038,0,0.022178,"(Xu et al., 2014), and information extraction (Hoffmann et al., 2011). The application of MIL to content selection is novel to our knowledge. We show how to incorporate content selection into encoder-decoder architectures following training regimes based on multi-task learning and reinforcement learning. Multi-task learning aims to improve a main task by incorporating joint learning of one or more related auxiliary tasks. It has been applied with success to a variety of sequence-prediction tasks focus1517 ing mostly on morphosyntax. Examples include chunking, tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015), and machine translation (Luong et al., 2016). Reinforcement learning (Williams, 1992) has also seen popularity as a means of training neural networks to directly optimize a taskspecific metric (Ranzato et al., 2016) or to inject task-specific knowledge (Zhang and Lapata, 2017). We are not aware of any work that compares the two training methods directly. Furthermore, our reinforcement learning-based algorithm differs from previous text generation approaches (Ranzato et al., 2016; Zhang and Lapata, 2017) in that it"
N18-1137,Q14-1034,0,0.0209097,"the input data varies from one entity (e.g., athlete) to another (e.g., scientist) and properties might be present or not due to data incompleteness. Moreover, our generator is enhanced with a content selection mechanism based on multi-instance learning. MIL-based techniques have been previously applied to a variety of problems including image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Carbonetto et al., 2008; Cour et al., 2011), text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015; Karpathy and Fei-Fei, 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011). The application of MIL to content selection is novel to our knowledge. We show how to incorporate content selection into encoder-decoder architectures following training regimes based on multi-task learning and reinforcement learning. Multi-task learning aims to improve a main task by incorporating joint learning of one or more related auxiliary tasks. It has been applied with success to a variety of sequence-prediction tasks focus1517 ing mostly on morphosyntax. Examples include chunking, tagging (Collobert et al., 2011; Søgaard and Goldbe"
N18-1137,D14-1074,1,0.888914,"Missing"
N18-1137,D17-1062,1,0.934589,"corporating joint learning of one or more related auxiliary tasks. It has been applied with success to a variety of sequence-prediction tasks focus1517 ing mostly on morphosyntax. Examples include chunking, tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015), and machine translation (Luong et al., 2016). Reinforcement learning (Williams, 1992) has also seen popularity as a means of training neural networks to directly optimize a taskspecific metric (Ranzato et al., 2016) or to inject task-specific knowledge (Zhang and Lapata, 2017). We are not aware of any work that compares the two training methods directly. Furthermore, our reinforcement learning-based algorithm differs from previous text generation approaches (Ranzato et al., 2016; Zhang and Lapata, 2017) in that it is applied to documents rather than individual sentences. 3 Bidirectional Content Selection We consider loosely coupled data and text pairs where the data component is a set P of propertyvalues {p1 : v1 , · · · , p|P |: v|P |} and the related text T is a sequence of sentences (s1 , · · · , s|T |). We define a mention span τ as a (possibly discontinuous) s"
N18-1137,N03-1031,0,\N,Missing
N18-1137,D17-1239,0,\N,Missing
N18-1158,P13-1020,0,0.0258573,"of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Re"
N18-1158,W97-0703,0,0.66836,"e that L EAD is indeed more informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨"
N18-1158,P11-1049,0,0.0913272,"nkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highl"
N18-1158,P16-1046,1,0.107683,"and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentence"
N18-1158,J10-3005,1,0.920121,"Missing"
N18-1158,W04-1017,0,0.0911027,"EAD is considered better than See et al. (2017) in the QA evaluation, whereas we find the opposite when participants are asked to rank systems. We hypothesize that L EAD is indeed more informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 20"
N18-1158,D15-1042,0,0.163937,"Missing"
N18-1158,P14-1062,0,0.054469,"lecting m sentences with top p(1|si , D, θ) scores. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over th"
N18-1158,D14-1181,0,0.00830447,"ary S by selecting m sentences with top p(1|si , D, θ) scores. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply ma"
N18-1158,W14-1504,0,0.061479,"Missing"
N18-1158,D15-1180,0,0.0182507,"res. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017). The main components include a sentence encoder, a document encoder, and a sentence extractor (see the left block of Figure 1) which we describe in more detail below. Sentence Encoder A core component of our model is a convolutional sentence encoder which encodes sentences into continuous representations. In recent years, CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lei et al., 2015; Kim et al., 2016; Cheng and Lapata, 2016) because of their effectiveness in identifying salient patterns in the input (Xu et al., 2015). In the case of summarization, CNNs can identify named-entities and events that correlate with the gold summary. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum v"
N18-1158,P15-1107,0,0.019581,"plied three times each. Max-pooling over time yields two feature lists f K2 and f K4 ∈ R3 . The final sentence embeddings have six dimensions. Document Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with Long Short-Term Memory (LSTM) cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1 , s2 , . . . , sn ), we follow common practice and feed sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015; Narayan et al., 2017). This way we make sure that the network also considers the top sentences of the document which are particularly important for summarization (Rush et al., 2015; Nallapati et al., 2016). Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 (relevant for the summary) or 0 (otherwise). 1748 Sentence extractor y5 y4 y3 y2 y1 Candidate summary Gold summary Sentence encoder REWARD s4 Police are still hunting for the driver s3 s5 s2 s1 s4 s3 s2 s1 Document encoder REINFORCE [convolution] [max pooling] Update ag"
N18-1158,D16-1127,0,0.240915,"-entropy loss using ground truth labels and then follows a curriculum learning strategy (Bengio et al., 2015) to gradually teach the model to produce stable predictions on its own. In our experiments MIXER performed worse than the model of Nallapati et al. (2017) just trained on collective labels. We conjecture that this is due to the unbounded nature of our ranking problem. Recall that our model assigns relevance scores to sentences rather than words. The space of sentential representations is vast and fairly unconstrained compared to other prediction tasks operating with fixed vocabularies (Li et al., 2016; Paulus et al., 2017; Zhang and Lapata, 2017). Moreover, our approximation of the gradient allows the model to 1751 In this section we present our experimental setup for assessing the performance of our model which we call R EFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015)"
N18-1158,N03-1020,0,0.870659,"ce should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based objective. Another discrepancy comes from the mismatch between the learning objective and the evaluation criterion, namely ROUGE (Lin and Hovy, 2003), which takes the entire summary into account. In this paper we argue that cross-entropy training is not optimal for extractive summarization. Models trained this way are prone to generating verbose summaries with unnecessarily long sentences and redundant information. We propose to overcome these difficulties by globally optimizing the ROUGE evaluation metric and learning to rank sentences for summary generation through a reinforcement learning objective. Similar to previous work (Cheng and Lapata, 2016; Narayan et al., 2017; Nallapati et al., 2017), our neural summarization model consists of"
N18-1158,U17-1012,0,0.040784,"Missing"
N18-1158,D17-1061,0,0.0259843,"zation, in our case states are documents (not summaries) and actions are relevance scores which lead to sentence ranking (not sentence-to-sentence transitions). Rather than employing reinforcement learning for sentence selection, our algorithm performs sentence ranking using ROUGE as the reward function. The REINFORCE algorithm (Williams, 1992) has been shown to improve encoder-decoder textrewriting systems by allowing to directly optimize a non-differentiable objective (Ranzato et al., 2015; Li et al., 2016; Paulus et al., 2017) or to inject task-specific constraints (Zhang and Lapata, 2017; Nogueira and Cho, 2017). However, we are not aware of any attempts to use reinforcement learning for training a sentence ranker in the context of extractive summarization. 8 Conclusions In this work we developed an extractive summarization model which is globally trained by optimizing the ROUGE evaluation metric. Our training algorithm explores the space of candidate summaries while learning to optimize a reward function which is relevant for the task at hand. Experimental results show that reinforcement learning offers a great means to steer our model towards generating informative, fluent, and concise summaries ou"
N18-1158,D15-1226,0,0.0592465,"core each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Reinforcement learning ha"
N18-1158,radev-etal-2004-mead,0,0.386489,"Missing"
N18-1158,D14-1075,0,0.0947591,"Missing"
N18-1158,D15-1044,0,0.594398,"f summarization tasks that have been identified over the years (see Nenkova and McKeown, 2011 for a comprehensive overview). Modern approaches to single document summarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations. Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem (Sutskever et al., 2014). Central in most approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See 1 Our code and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most"
N18-1158,D12-1024,0,0.0672832,"istic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in highly scoring summaries. Reinforcement learning has been previously used in the context of traditional multi-document summarization as a means of selecting a sentence or a subset of sentences from a document cluster. Ryang and Abekawa (2012) cast the sentence selection task as a search problem. Their agent observes a state (e.g., a candidate summary), executes an action (a transition operation that produces a new state selecting a not-yet-selected sentence), and then receives a delayed reward based on tf ∗ idf. Follow-on work (Rioux et al., 2014) extends this approach by employing ROUGE as part of the reward function, while Henß et al. (2015) further experiment with Q-learning. Moll´aAliod (2017) has adapt this approach to queryfocused summarization. Our model differs from these approaches both in application and formulation. We"
N18-1158,P08-2052,0,0.0921083,"f L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any"
N18-1158,E17-2007,0,0.0672798,".2 We compared R EFRESH against a baseline which simply selects the first m leading sentences from each document (L EAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss. Cheng and Lapata (2016) train on individual labels, while Nallapati et al. (2017) use collective labels. We also compared our model against the abstractive systems of Chen et al. (2016), Nallapati et al. (2016), See et al. (2017), and Tan and Wan (2017).3 In addition to ROUGE which can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017), we also evaluated system output by eliciting human judgments in two ways. In our first experiment, participants were presented with a news article and summaries generated by three systems: the L EAD baseline, abstracts from See et al. (2017), and extracts from R EFRESH. We also included the human-authored highlights.4 Participants read the articles and were asked to rank the summaries from best (1) to worst (4) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?). We did not allow any ties. W"
N18-1158,P17-1099,0,0.458809,"nFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents for CNN and 196,961/12,148/10,397 for DailyMail). We did not anonymize entities or lower case tokens. We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the “story highlights” associated with each article are gold-standard abstractive summaries. During training we use these to generate high scoring extracts and to estimate rewards for them, but during testing, they are used as reference summaries to evaluate our models. Implementation Details We generated extracts by selecting three sentences (m = 3) for CNN articles and four sentences (m = 4) for DailyMail articles. These decisions were informed by the fact that gold highlights in the CNN/DailyMail validation sets are 2.6/4.2 sentences long. For both data"
N18-1158,P16-1159,0,0.0290088,"mbeddings with the skip-gram model (Mikolov et al., 2013) using context window size 6, negative sampling size 10, and hierarchical softmax 1. Known words were initialized with pre-trained embeddings of size 200. Embeddings for unknown words were initialized to zero, but estimated during training. L EAD R EFRESH Experimental Setup G OLD 5 See et al. converge much faster to an optimal policy. Advantageously, we do not require an online reward estiˆ which leads to a signifimator, we pre-compute Y, cant speedup during training compared to MIXER (Ranzato et al., 2015) and related training schemes (Shen et al., 2016). Q1 Q2 Q3 • A SkyWest Airlines flight made an emergency landing in Buffalo, New York, on Wednesday after a passenger lost consciousness, officials said. • The passenger received medical attention before being released, according to Marissa Snow, spokeswoman for SkyWest. • She said the airliner expects to accommodate the 75 passengers on another aircraft to their original destination – Hartford, Connecticut – later Wednesday afternoon. • Skywest Airlines flight made an emergency landing in Buffalo, New York, on Wednesday after a passenger lost consciousness. • She said the airliner expects to"
N18-1158,D07-1047,0,0.129055,"Missing"
N18-1158,P17-1108,0,0.457427,"mmarization are data-driven, taking advantage of the success of neural network architectures and their ability to learn continuous features without recourse to preprocessing tools or linguistic annotations. Abstractive summarization involves various text rewriting operations (e.g., substitution, deletion, reordering) and has been recently framed as a sequence-to-sequence problem (Sutskever et al., 2014). Central in most approaches (Rush et al., 2015; Chen et al., 2016; Nallapati et al., 2016; See 1 Our code and data are available here: https://github. com/shashiongithub/Refresh. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptuali"
N18-1158,W97-0710,0,0.602162,"informative than See et al. (2017) but humans prefer shorter summaries. The average length of L EAD summaries is 105.7 words compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin an"
N18-1158,C10-1128,0,0.0490031,"ds compared to 61.6 for See et al. (2017). 7 Related Work Traditional summarization methods manually define features to rank sentences for their salience in order to identify the most important sentences in a document or set of documents (Kupiec et al., 1995; Mani, 2001; Radev et al., 2004; Filatova and Hatzivassiloglou, 2004; Nenkova et al., 2006; Sp¨arck Jones, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic"
N18-1158,P10-1058,1,0.933128,"n the task definition and the training objective. While MLE in Equation (1) aims to maximize the likelihood of the ground-truth labels, the model is (a) expected to rank sentences to generate a summary and (b) evaluated using ROUGE at test time. The second discrepancy comes from the reliance on ground-truth labels. Document collections for training summarization systems do not naturally contain labels indicating which sentences should be extracted. Instead, they are typically accompanied by abstractive summaries from which sentence-level labels are extrapolated. Cheng and Lapata (2016) follow Woodsend and Lapata (2010) in adopting a rule-based method which assigns labels to each sentence in the document individually based on their semantic correspondence with the gold summary (see the fourth column in Table 1). An alternative method (Svore et al., 2007; Cao et al., 2016; Nallapati et al., 2017) identifies the set of sentences which collectively gives the highest ROUGE with respect to the gold summary. Sentences in this set are labeled with 1 and 0 otherwise (see the column 5 in Table 1). Labeling sentences individually often generates too many positive labels causing the model to 1749 Collective Oracle ROUG"
N18-1158,D12-1022,1,0.736525,"es, 2007). A vast majority of these methods learn to score each sentence independently (Barzilay and Elhadad, 1997; Teufel and Moens, 1997; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Shen et al., 2007; Schilder and Kondadadi, 2008; Wan, 2010) and a summary is generated by selecting top-scored sentences in a way that is not incorporated into the learning process. Summary quality can be improved heuristically, (Yih et al., 2007), via max-margin methods (Carbonell and Goldstein, 1998; Li et al., 2009), or integer-linear programming (Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013; Parveen et al., 2015). Recent deep learning methods (K˚ageb¨ack et al., 2014; Yin and Pei, 2015; Cheng and Lapata, 2016; Nallapati et al., 2017) learn continuous features without any linguistic preprocessing (e.g., named entities). Like traditional methods, these approaches also suffer from the mismatch between the learning objective and the evaluation criterion (e.g., ROUGE) used at the test time. In comparison, our neural model globally optimizes the ROUGE evaluation metric through a reinforcement learning objective: sentences are highly ranked if they occur in h"
N18-1158,K17-1045,0,0.055014,"h. et al., 2017; Tan and Wan, 2017; Paulus et al., 2017) is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Extractive systems create a summary by identifying (and subsequently concatenating) the most important sentences in a document. A few recent approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2017; Yasunaga et al., 2017) conceptualize extractive summarization as a sequence labeling task in which each label specifies whether each document sentence should be included in the summary. Existing models rely on recurrent neural networks to derive a meaning representation of the document which is then used to label each sentence, taking the previously labeled sentences into account. These models are typically trained using cross-entropy loss in order to maximize the likelihood of the ground-truth labels and do not necessarily learn to rank sentences based on their importance due to the absence of a ranking-based obje"
N18-1158,D17-1062,1,0.913736,"s and then follows a curriculum learning strategy (Bengio et al., 2015) to gradually teach the model to produce stable predictions on its own. In our experiments MIXER performed worse than the model of Nallapati et al. (2017) just trained on collective labels. We conjecture that this is due to the unbounded nature of our ranking problem. Recall that our model assigns relevance scores to sentences rather than words. The space of sentential representations is vast and fairly unconstrained compared to other prediction tasks operating with fixed vocabularies (Li et al., 2016; Paulus et al., 2017; Zhang and Lapata, 2017). Moreover, our approximation of the gradient allows the model to 1751 In this section we present our experimental setup for assessing the performance of our model which we call R EFRESH as a shorthand for REinFoRcement Learning-based Extractive Summarization. We describe our datasets, discuss implementation details, our evaluation protocol, and the systems used for comparison. Summarization Datasets We evaluated our models on the CNN and DailyMail news highlights datasets (Hermann et al., 2015). We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266"
N18-1158,W04-3252,0,\N,Missing
N18-1158,W01-0100,0,\N,Missing
N18-1160,W14-0907,0,0.164219,"lines and generates movie overviews which are well-received by human judges. To the best of our knowledge, this is the first work to automatically analyze and summarize the content of screenplays. 2 Related Work Recent years have seen increased interest in the computational analysis of movie screenplays. Ye and Baldwin (2008) create animated storyboards using the action descriptions of movie scripts. Danescu-Niculescu-Mizil and Lee (2011) use screenplays to study the coordination of linguistic styles in dialog. Bamman et al. (2013) induce personas of film characters from movie plot summaries. Agarwal et al. (2014a; 2014b; 2015) extract social networks from scripts, create xkcd movie narrative charts, and automate the Bechdel test which is designed to assess the presence of women in movies. Gorinski and Lapata (2015) summarize screenplays by selecting important scenes. Our work joins this line of research in an attempt to automatically induce information pertaining to a movie’s content such as its genre and plot elements. There has been a surge of interest recently in repurposing sequence transduction neural network architectures for various generation tasks such as machine translation (Sutskever et al"
N18-1160,N15-1084,0,0.417221,"Missing"
N18-1160,P13-1035,0,0.258413,"Missing"
N18-1160,W14-4012,0,0.214456,"Missing"
N18-1160,N16-1012,0,0.0876113,"Missing"
N18-1160,W11-0609,0,0.0151964,"el is trained end-to-end using screenplays and movie overviews as the supervision signal. In both automatic and human-based evaluations our neural network architecture outperforms competitive baselines and generates movie overviews which are well-received by human judges. To the best of our knowledge, this is the first work to automatically analyze and summarize the content of screenplays. 2 Related Work Recent years have seen increased interest in the computational analysis of movie screenplays. Ye and Baldwin (2008) create animated storyboards using the action descriptions of movie scripts. Danescu-Niculescu-Mizil and Lee (2011) use screenplays to study the coordination of linguistic styles in dialog. Bamman et al. (2013) induce personas of film characters from movie plot summaries. Agarwal et al. (2014a; 2014b; 2015) extract social networks from scripts, create xkcd movie narrative charts, and automate the Bechdel test which is designed to assess the presence of women in movies. Gorinski and Lapata (2015) summarize screenplays by selecting important scenes. Our work joins this line of research in an attempt to automatically induce information pertaining to a movie’s content such as its genre and plot elements. There"
N18-1160,N15-1113,1,0.900795,"Missing"
N18-1160,N16-1063,0,0.237792,"takes a step toward automatic script summarization by jointly modeling the tasks of movie attribute identification and overview generation. Specifically, we propose a novel neural network architecture which draws insights from encoder-decoder models recently proposed for machine translation (Bahdanau et al., 2015) and related sentence generation tasks (Wen et al., 2015; Mei et al., 2016; Lebret et al., 2016). Our model takes the screenplay as input and generates an overview for it. Rather than representing the script as a sequence, we employ feed-forward neural networks (Zhang and Zhou, 2006; Kurata et al., 2016) to encode the screenplay into various attributes (e.g., Plot, Genre) and their labels (e.g., thriller, romance), viewing movie content analysis as a multi-label classification problem. Our decoder generates movie overviews using a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber, 1997), a type of recurrent neural network with a more complex computational unit which is semantically conditioned (Wen et al., 2015, 2016) on this attribute specific representation. Our model is trained end-to-end using screenplays and movie overviews as the supervision signal. In both automatic and"
N18-1160,D16-1128,0,0.109041,"rif. 1770 Proceedings of NAACL-HLT 2018, pages 1770–1781 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics screenplay (e.g., Genre, Plot, Flag, Mood, Place). This work takes a step toward automatic script summarization by jointly modeling the tasks of movie attribute identification and overview generation. Specifically, we propose a novel neural network architecture which draws insights from encoder-decoder models recently proposed for machine translation (Bahdanau et al., 2015) and related sentence generation tasks (Wen et al., 2015; Mei et al., 2016; Lebret et al., 2016). Our model takes the screenplay as input and generates an overview for it. Rather than representing the script as a sequence, we employ feed-forward neural networks (Zhang and Zhou, 2006; Kurata et al., 2016) to encode the screenplay into various attributes (e.g., Plot, Genre) and their labels (e.g., thriller, romance), viewing movie content analysis as a multi-label classification problem. Our decoder generates movie overviews using a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber, 1997), a type of recurrent neural network with a more complex computational unit which is sem"
N18-1160,N16-1086,0,0.0602265,"Missing"
N18-1160,P02-1040,0,0.100878,"aining data, and rehashes its overview as output; (3) an attentionbased LSTM (Bahdanau et al., 2015) trained on script sentence pairs (31,000 in total); and (4) six attention-based LSTMs, one per attribute type, trained on script sentence pairs (on average 5,200 per LSTM). The attention LSTMs were trained on the same screenplay features as M ORGAN, with the attention mechanism at each timestep t focusing on parts of the input. Example overviews generated by M ORGAN, the attention LSTMs, and the nearest neighbor system are shown in Table 6. We evaluated system output with multireference BLEU4 (Papineni et al., 2002), using sentences from the extended gold-standard as references. Table 5 (first column) summarizes our results. As can be seen, M ORGAN outperforms the attention based models, the nearest neighbor system, and the random baseline. The attention-based models cannot succinctly capture the movie’s content in order to render it into meaningful sentences. Although the generated sentences are more or less grammatical on their own (see Table 6), the generated overview lacks coherence, and is fairly repetitive. The model does not reliably learn what type of information to focus on for the generation ta"
N18-1160,D15-1167,0,0.0422037,"53 9 ol ... Jinni 29 406 31 8 173 9 wt p0 oL1 oL2 ... Attribute Mood Plot Genre Attitude Place Flag oL ht-1 CS cell rt pt-1 wt it pt Ct ht ot ht-1 ft LSTM cell wt ht-1 wt ht-1 wt ht-1 Figure 2: Neural network architecture: given feature vector x representing a screenplay, we employ feedforward multi-label classification networks to encode the movie into a content vector pθ representing attribute labels; this encoding is fed into an LSTM with a content selection cell. plays as a sequence of sentences is problematic both computationally and conceptually. Even if we used a hierarchical encoder (Tang et al., 2015; Yang et al., 2016) by first building representations of sentences and then aggregating those into a representation of a screenplay, it is doubtful whether a fixed length vector could encode the content of the movie in its entirety or whether the attention mechanism would effectively isolate the parts of the input relevant for generation. We therefore propose an architecture that consists of two stacked neural network models for the tasks of movie attribute identification and overview generation. Figure 2 illustrates our model. We use simple feed-forward neural networks to impose some structu"
N18-1160,D15-1199,0,0.0585167,"Missing"
N18-1160,N16-1015,0,0.0350196,"Missing"
N18-1160,N16-1174,0,0.0492998,"29 406 31 8 173 9 wt p0 oL1 oL2 ... Attribute Mood Plot Genre Attitude Place Flag oL ht-1 CS cell rt pt-1 wt it pt Ct ht ot ht-1 ft LSTM cell wt ht-1 wt ht-1 wt ht-1 Figure 2: Neural network architecture: given feature vector x representing a screenplay, we employ feedforward multi-label classification networks to encode the movie into a content vector pθ representing attribute labels; this encoding is fed into an LSTM with a content selection cell. plays as a sequence of sentences is problematic both computationally and conceptually. Even if we used a hierarchical encoder (Tang et al., 2015; Yang et al., 2016) by first building representations of sentences and then aggregating those into a representation of a screenplay, it is doubtful whether a fixed length vector could encode the content of the movie in its entirety or whether the attention mechanism would effectively isolate the parts of the input relevant for generation. We therefore propose an architecture that consists of two stacked neural network models for the tasks of movie attribute identification and overview generation. Figure 2 illustrates our model. We use simple feed-forward neural networks to impose some structure on the input by i"
N18-1160,D17-1062,1,0.842787,"rrative charts, and automate the Bechdel test which is designed to assess the presence of women in movies. Gorinski and Lapata (2015) summarize screenplays by selecting important scenes. Our work joins this line of research in an attempt to automatically induce information pertaining to a movie’s content such as its genre and plot elements. There has been a surge of interest recently in repurposing sequence transduction neural network architectures for various generation tasks such as machine translation (Sutskever et al., 2014), sentence compression (Chopra et al., 2016), and simplification (Zhang and Lapata, 2017). Central to these approaches is an encoder-decoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. Previously proposed architectures are not directly applicable to our task for at least two reasons: (a) the correspondence between screenplays and overviews is very loose, and (b) the screenplay is not strictly speaking a sequence (a screenplay is more like a book consisting of thousands of sentences), and cannot be easily compressed into a vectorbased re"
N19-1173,D07-1101,0,0.0263053,"i = j ¯ ij = Lij + exp(ˆ L Lij otherwise −1 ¯ eij = (1 − δ1,j )Aij [L ]jj ¯ −1 ]ji − (1 − δi,1 )Aij [L ¯ −1 ]i1 ri = exp(ˆ ri )[L return ri , eij access to labels for the roots (aka summary sentences), while tree edges are latent and learned without an explicit training signal. And as previous work (Liu and Lapata, 2017) has shown, a single application of TMT leads to shallow tree structures. Secondly, the calculation of r˜i and e˜ij would be based on first-order features alone, however, higher-order information pertaining to siblings and grandchildren has proved useful in discourse parsing (Carreras, 2007). We address these issues with an inference algorithm which iteratively infers latent trees. In contrast to multi-layer neural network architectures like the Transformer or Recursive Neural Networks (Tai et al., 2015) where word representations are updated at every layer based on the output of previous layers, we refine only the tree structure during each iteration, word representations are not passed across multiple layers. Empirically, at early iterations, the model learns shallow and 1748 simple trees, and information propagates mostly between neighboring nodes; as the structure gets more r"
N19-1173,P14-1048,0,0.10089,"Missing"
N19-1173,D14-1168,0,0.0867573,"e-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–1755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. One wily coyote traveled a bit too far from home, and its resulting adventure through Harlem had alarmed residents doing a double take and scampering to get out of its way Wednesday morning. Police say frightened New Yorkers reported the coyote sighting around 9:30 a.m., and an emergency service unit was dispatched to find the animal. The little troublemaker was caught and tranquil"
N19-1173,N18-1150,0,0.0583725,"Missing"
N19-1173,D13-1158,0,0.0555327,"Missing"
N19-1173,P16-1046,1,0.938134,"ually sentences) in a document. Recent 1 Our code is publicly available at https://github. com/nlpyang/SUMO. approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural network architectures (Bahdanau et al., 2015). The idea is to predict a label for each sentence specifying whether it should be included in the summary. Existing systems mostly rely on recurrent neural networks (Hochreiter and Schmidhuber, 1997) to model the document and obtain a vector representation for each sentence (Nallapati et al., 2017; Cheng and Lapata, 2016). Intersentential relations are captured in a sequential manner, without taking the structure of the document into account, although the latter has been shown to correlate with what readers perceive as important in a text (Marcu, 1999). Another problem in neural-based extractive models is the lack of interpretability. While capable of identifying summary sentences, these models are not able to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate). The summarization literature offers examples of mod"
N19-1173,D14-1179,0,0.0207797,"Missing"
N19-1173,J10-3005,1,0.827546,"doc-att) outperform L EAD -3 across metrics. S UMO (3-layer) is competitive or better than stateof-the-art approaches. Examples of system output are shown in Table 4. Finally, we should point out that S UMO is superior to Marcu (1999) even though the latter employs linguistically informed document representations. 3.4 Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments. Our first evaluation quantified the degree to which summarization models retain key information from the document following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption that it highlights the most important document content. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the article. The more questions a system can answer, the better it is at summarizing the document as a whole. We randomly selected 20 documents from the CNN/DailyMail and NYT datasets, respectively and wrote multiple question-answer pairs for each gold summary. We created 71 questions in total varying from two to s"
N19-1173,D18-1409,0,0.0886611,"Missing"
N19-1173,P16-1188,0,0.493253,"luated S UMO on two benchmark datasets, namely the CNN/DailyMail news highlights dataset (Hermann et al., 2015) and the New York Times Annotated Corpus (NYT; Sandhaus 2008). The CNN/DailyMail dataset contains news articles and associated highlights, i.e., a few bullet points giving a brief overview of the article. We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26"
N19-1173,P17-2074,0,0.0457368,"Missing"
N19-1173,D07-1015,0,0.723746,"Missing"
N19-1173,D18-1149,0,0.0136327,"tability in the summarization process by helping explain how document content contributes to the model’s decisions. We design a new iterative structure refinement algorithm, which learns to induce document-level structures through repeatedly refining the trees predicted by previous iterations and allows the model to infer complex trees which go beyond simple parent-child relations (Liu and Lapata, 2018; Kim et al., 2017). The idea of structure refinement is conceptually related to recently proposed models for solving iterative inference problems (Marino et al., 2018; Putzky and Welling, 2017; Lee et al., 2018). It is also related to structured prediction energy networks (Belanger et al., 2017) which approach structured prediction as iterative miminization of an energy function. However, we are not aware of any previous work considering structure refinement for tree induction problems. Our contributions in this work are three-fold: a novel conceptualization of extractive summarization as a tree induction problem; a model which capitalizes on the notion of structured attention to learn document representations based on iterative structure refinement; and large-scale evaluation studies (both automatic"
N19-1173,W04-1013,0,0.0496642,"was set to 30K. We used 300D word embeddings which were initialized randomly from N (0, 0.01). The sentence-level Transformer has 6 layers and the hidden size of FFN was set to 512. The number of heads in MHAtt was set to 4. Adam was used for training (β1 = 0.9, β2 = 0.999). We adopted the learning rate schedule from Vaswani et al. (2017) with warming-up on the first 8,000 steps. S UMO and related Transformer models produced 3-sentence summaries for each document at test time (for both CNN/DailyMail and NYT datasets). 3.3 Automatic Evaluation We evaluated summarization quality using ROUGE F1 (Lin, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. Table 1 summarizes our results. We evaluated two variants of S UMO, with one and three structured-attention layers. We compared against a baseline which simply selects the first three sentences in each document (L EAD-3) and several incarnations of the basic Transformer model introduced in Section 2.1. These include a Transformer without document-level self-attention and two variants with document-level self attention"
N19-1173,Q18-1005,1,0.934844,"tractive summary is generated. Despite the intuitive appeal of discourse structure for the summarization task, the reliance on a parser which is both expensive to obtain (since it must be trained on labeled data) and error prone, presents a major obstacle to its widespread use. Recognizing the merits of structure-aware representations for various NLP tasks, recent efforts have focused on learning latent structures (e.g., parse trees) while optimizing a neural network model for a down-stream task. Various methods impose structural constraints on the basic attention mechanism (Kim et al., 2017; Liu and Lapata, 2018), formulate structure learning as a reinforcement learning problem (Yogatama et al., 2017; Williams et al., 2018), or sparsify the set of possible structures (Niculae et al., 2018). Although latent structures are mostly induced for individual sentences, Liu and Lapata (2018) induce dependency-like structures for entire documents. Drawing inspiration from this work and existing discourse-informed summarization models (Marcu, 1999; Hirao et al., 2013), we frame extractive summarization as a tree induction problem. Our model represents documents as multiroot dependency trees where each root node"
N19-1173,W01-0100,0,0.421629,"summarizer1 performs competitively against state-of-the-art methods. 1 Introduction Single-document summarization is the task of automatically generating a shorter version of a document while retaining its most important information. The task has received much attention in the natural language processing community due to its potential for various information access applications. Examples include tools which digest textual content (e.g., news, social media, reviews), answer questions, or provide recommendations. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention. In abstractive summarization, various text rewriting operations generate summaries using words or phrases that were not in the original text, while extractive approaches form summaries by copying and concatenating the most important spans (usually sentences) in a document. Recent 1 Our code is publicly available at https://github. com/nlpyang/SUMO. approaches to (single-document) extractive summarization frame the task as a sequence labeling problem taking advantage of the success of neural n"
N19-1173,D17-1159,1,0.817496,"i and cli represent parent and child vectors, respectively, while vector zil is updated with contextual information at hop l. At the final iteration (lines 9 and 10), the top sentence embeddings v K−1 are used to calculate the final root probabilities rK . We define the model’s loss function as the summation of the losses of all iterations: L= K X [y log(rk ) + (1 − y) log(1 − rk )] (12) k=1 S UMO uses the root probabilities of the top layer as the scores for summary sentences. The k-Hop-Propagation function resembles the computation used in Graph Convolution Networks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017). GCNs have been been recently applied to latent trees (Corro and Titov, 2019), however not in combination with iterative refinement. 3 Experiments In this section we present our experimental setup, describe the summarization datasets we used, discuss implementation details, our evaluation protocol, and analyze our results. Algorithm 2: Structured Summarization Model Input: Document d Output: Root probabilities r K after K iterations 1 Calculate sentence vectors s using sentence-level Transformer TS 0 2 v ←s 3 for k ← 1 to K − 1 do 4 Calculate unnormalized root scores: r˜ik = Wrk vik−1 5 Calcu"
N19-1173,J93-2004,0,0.0707446,"Missing"
N19-1173,N18-1158,1,0.785996,"of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26.0 11.7 26.9 6.10 19.5 — — — — — — 11.1 25.6 11.4 26.0 11.8 26.3 11.6 26.2 12.0 26.5 R-1 40.7 41.0 31.9 — — — 40.5 41.5 41.7 41.6 42.0 DM R-2 R-L 18.3 37.2 18.8 37.7 12.4 23.5 — — — — — — 18.1 36.8 18.7 38.0 18.8 38.0 18.8 37.6 19.1 38.0 CNN+DM R-1 R-2 R-L 39.6 17.7 36.2 40.0 18.2 36.6 26.5 9.80 20.4 — — — 39.5 17.3 36.4 41.7 19.5 37.9 39.7"
N19-1173,D18-1108,0,0.0129512,"must be trained on labeled data) and error prone, presents a major obstacle to its widespread use. Recognizing the merits of structure-aware representations for various NLP tasks, recent efforts have focused on learning latent structures (e.g., parse trees) while optimizing a neural network model for a down-stream task. Various methods impose structural constraints on the basic attention mechanism (Kim et al., 2017; Liu and Lapata, 2018), formulate structure learning as a reinforcement learning problem (Yogatama et al., 2017; Williams et al., 2018), or sparsify the set of possible structures (Niculae et al., 2018). Although latent structures are mostly induced for individual sentences, Liu and Lapata (2018) induce dependency-like structures for entire documents. Drawing inspiration from this work and existing discourse-informed summarization models (Marcu, 1999; Hirao et al., 2013), we frame extractive summarization as a tree induction problem. Our model represents documents as multiroot dependency trees where each root node is a summary sentence, and the subtrees attached to it are sentences whose content is related to and covered by the summary sentence. An example of a document and its corresponding"
N19-1173,prasad-etal-2008-penn,0,0.0815454,"le of identifying summary sentences, these models are not able to rationalize their predictions (e.g., a sentence is in the summary because it describes important content upon which other related sentences elaborate). The summarization literature offers examples of models which exploit the structure of the underlying document, inspired by existing theories of discourse such as Rhetorical Structure Theory (RST; Mann and Thompson 1988). Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–175"
N19-1173,P17-1099,0,0.725316,"66/1,220/1,093 CNN documents and 196,961/12,148/10,397 DailyMail documents). We did not anonymize entities. The NYT dataset contains 110,540 articles with abstractive summaries. Following Durrett et al. (2016), we split these into 100,834 training and 9,706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). We also followed their filtering procedure, documents with summaries that are shorter than 50 words were removed from the raw dataset. The 1749 Model R-1 L EAD-3 29.2 Narayan et al. (2018) 30.4 25.6 Marcu (1999) Durrett et al. (2016) — See et al. (2017) — Celikyilmaz et al. (2018) — Transformer (no doc-att) 29.2 Transformer (1-layer doc-att) 29.5 Transformer (3-layer doc-att) 29.6 S UMO (1-layer) 29.5 S UMO (3-layer) 29.7 CNN R-2 R-L 11.2 26.0 11.7 26.9 6.10 19.5 — — — — — — 11.1 25.6 11.4 26.0 11.8 26.3 11.6 26.2 12.0 26.5 R-1 40.7 41.0 31.9 — — — 40.5 41.5 41.7 41.6 42.0 DM R-2 R-L 18.3 37.2 18.8 37.7 12.4 23.5 — — — — — — 18.1 36.8 18.7 38.0 18.8 38.0 18.8 37.6 19.1 38.0 CNN+DM R-1 R-2 R-L 39.6 17.7 36.2 40.0 18.2 36.6 26.5 9.80 20.4 — — — 39.5 17.3 36.4 41.7 19.5 37.9 39.7 17.0 35.9 40.6 18.1 36.7 40.6 18.1 36.9 40.5 18.0 36.8 41.0 18.4"
N19-1173,P15-1150,0,0.168463,"Missing"
N19-1173,Q18-1019,0,0.0619806,"Missing"
N19-1173,D14-1196,0,0.0219041,"ries of discourse such as Rhetorical Structure Theory (RST; Mann and Thompson 1988). Most approaches produce summaries based on tree-like document representations obtained by a parser trained on discourse annotated corpora (Carlson et al., 2003; Prasad et al., 2008). For instance, Marcu (1999) argues that a good summary can be generated by traversing the RST discourse tree structure top-down, following nucleus nodes (discourse units in RST are characterized regarding their text importance; nuclei denote central units, whereas satellites denote peripheral ones). Other work (Hirao et al., 2013; Yoshida et al., 2014) extends this idea by transforming RST trees into dependency trees and generating summaries by tree trimming. Gerani et al. (2014) summarize product reviews; their system aggregates RST trees rep1745 Proceedings of NAACL-HLT 2019, pages 1745–1755 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 1. One wily coyote traveled a bit too far from home, and its resulting adventure through Harlem had alarmed residents doing a double take and scampering to get out of its way Wednesday morning. Police say frightened New Yorkers reported the coyote sighting"
N19-1173,D18-1088,1,0.863752,"f-the-art methods while being able to rationalize model predictions. 2 Model Description Let d denote a document containing several sentences [sent1 , sent2 , · · · , sentm ], where senti is the i-th sentence in the document. Extractive summarization can be defined as the task of assigning a label yi ∈ {0, 1} to each senti , indicating whether the sentence should be included in the summary. It is assumed that summary sentences represent the most important content of the document. 1746 2.1 Baseline Model Most extractive models frame summarization as a classification problem. Recent approaches (Zhang et al., 2018; Dong et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016) incorporate a neural network-based encoder to build representations for sentences and apply a binary classifier over these representations to predict whether the sentences should be included in the summary. Given predicted scores r and gold labels y, the loss function can be defined as: L=− m X For our extractive summarization task, the baseline system is composed of a sentence-level Transformer (TS ) and a document-level Transformer (TD ), which have the same structure. For each sentence si = [wi1 , wi2 , · · · , win ] in th"
N19-1173,D17-1225,0,0.0143628,"attention instantiated with one and three layers. Several stateof-the-art models are also included in Table 1, both extractive and abstractive. R EFRESH (Narayan et al., 2018) is an extractive summarization system trained by globally optimizing the ROUGE metric with reinforcement learning. The system of Marcu (1999) is another extractive summarizer based on RST parsing. It uses discourse structures and RST’s notion of nuclearity to score document sentences in terms of their importance and selects the most important ones as the summary. Our re-implementation of Marcu (1999) used the parser of Zhao and Huang (2017) to obtain RST trees. Durrett et al. (2016) develop a summarization system which integrates a compression model that enforces grammaticality and coherence. See et al. (2017) present an abstractive summarization system based on 1750 an encoder-decoder architecture. Celikyilmaz et al.’s (2018) system is state-of-the-art in abstractive summarization using multiple agents to represent the document as well a hierarchical attention mechanism over the agents for decoding. As far as S UMO is concerned, we observe that it outperforms a simple Transformer model without any document attention as well as"
N19-1238,N18-3011,0,0.18965,"ins, and often provide rich annotations of relationships that extend beyond the scope of 2284 Proceedings of NAACL-HLT 2019, pages 2284–2293 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics a single sentence. But due to their automatic nature, they also introduce challenges for generation such as erroneous annotations, structural variety, and significant abstraction of surface textual features (such as grammatical relations or predicateargument structure). To effect our study, we use a collection of abstracts from a corpus of scientific articles (Ammar et al., 2018). We extract entity, coreference, and relation annotations for each abstract with a stateof-the-art information extraction system (Luan et al., 2018), and represent the annotations as a knowledge graph which collapses co-referential entities. An example of a text and graph are shown in Figure 1. We use these graph/text pairs to train a novel attention-based encoder-decoder model for knowledge-graph-to-text generation. Our model, GraphWriter, extends the successful Transformer for text encoding (Vaswani et al., 2017) to graphstructured inputs, building on the recent Graph Attention Network arch"
N19-1238,H05-1042,1,0.689082,"nowledgegraphs paired with scientific texts for further study. Through detailed automatic and human evaluations, we demonstrate that automatically extracted knowledge can be used for multi-sentence text generation. We further show that structuring and encoding this knowledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task. 2 Related Work Our work falls under the larger scope of conceptto-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but r"
N19-1238,P18-1026,0,0.280878,"Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been 2285 Vocab Tokens Entities Avg Length Avg #Vertices Avg #Edges Title 29K 413K 9.9 - Abstract 77K 5.8M 141.2 - KG 54K 1.2M 518K 12.42 4.43 Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance. shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At each timestep, a vertex is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veliˇckovi´c et al. (2018), which impose fewer constraints on their input. Finally, our work is related to Wang et al. (2018) who offer a method for generating scientific abstracts from titles. Their model uses a gated rewriter network to write"
N19-1238,W14-3348,0,0.0162431,"e provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria. Because our dataset is scientific in nature, evaluations must be done by experts and we can only collect a limited number of these high quality datapoints.2 The study was conducted by 15 experts (i.e. computer science students) who were familiar with the abstract writing task and the content of the abstracts they judged. To supplement this, we also provide automatic metrics. We use BLEU (Papineni et al., 2002), an n-gram overlap measure popular in text generation tasks, and METEOR (Denkowski and Lavie, 2014), a machine translation with paraphrase and language-specific considerations. Comparisons We compare our GraphWriter against several strong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of (Veliˇckovi´c et al., 2018). This encoder consists of PReLU activations stacked between 6 self-attention layers. To determine the usefulness of including graph relations, we compare to a model which uses only entities and title (EntityWriter). Finally, we compare with the gated rewriter model of Wang et al. (2018) (Rewriter). This model uses only the document titl"
N19-1238,C10-2062,0,0.019601,"ledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task. 2 Related Work Our work falls under the larger scope of conceptto-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more challenging domains such as ours. Lebret et al. (2016) tackles the task of generating the first sentence of a Wikipedia entry"
N19-1238,N16-1095,0,0.014676,"1−p probability is given to αvocab , which is calculated by scaling [ht kct ] to the vocabulary size and taking a softmax. 5 Experiments Evaluation Metrics We evaluate using a combination of human and automatic evaluations. For human evaluation, participants were asked to compare abstracts generated by various models and those written by the authors of the scientific articles. We used Best-Worst Scaling (BWS; (Louviere and Woodworth, 1991; Louviere et al., 2015)), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2016). Participants were presented with two or three abstracts and asked to decide which one was better and which one was worse in order of grammar and fluency (is the abstract written in well-formed English?), coherence (does the abstract have an introduction, state the problem or task, describe a solution, and discuss evaluations or results?), and informativeness (does the abstract relate to the provided title and make use of appropriate scientific terms?). We provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria. Because our dataset is scie"
N19-1238,P17-1014,0,0.0733805,"al models to the data-to-text task. They introduce a large dataset where a text summary of a basketball game is paired with two tables of relevant statistics and show that neural models struggle to compete with template based methods over this data. We propose generating from graphs rather than tables, and show that graphs can be effectively encoded to capture both local and global structure in the input. We show that modeling knowledge as a graph improves generation results, connecting our work to other graph-to-text tasks such as generating from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural model for this task, and show that pretraining on a large dataset of noisy automatic parses can improve results. However, they do not directly model the graph structure, relying on linearization and sequence encoding instead. Current works improve this through more sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which of"
N19-1238,D13-1157,1,0.957938,"h strings of natural language text. However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of 1 Data and code available at https://github.com/ rikdz/GraphWriter Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes. The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotatio"
N19-1238,D16-1128,0,0.438369,"age text. However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of 1 Data and code available at https://github.com/ rikdz/GraphWriter Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes. The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotations of relationships t"
N19-1238,P18-1150,0,0.0951155,"e sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been 2285 Vocab Tokens Entities Avg Length Avg #Vertices Avg #Edges Title 29K 413K 9.9 - Abstract 77K 5.8M 141.2 - KG 54K 1.2M 518K 12.42 4.43 Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance. shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At each timestep, a vertex is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veliˇckovi´c et al. (2"
N19-1238,P09-1011,0,0.0929414,"racted knowledge can be used for multi-sentence text generation. We further show that structuring and encoding this knowledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task. 2 Related Work Our work falls under the larger scope of conceptto-text generation. Barzilay and Lapata (2005) introduced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more cha"
N19-1238,P16-1008,0,0.0321171,"Missing"
N19-1238,D18-1360,1,0.917823,"olis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics a single sentence. But due to their automatic nature, they also introduce challenges for generation such as erroneous annotations, structural variety, and significant abstraction of surface textual features (such as grammatical relations or predicateargument structure). To effect our study, we use a collection of abstracts from a corpus of scientific articles (Ammar et al., 2018). We extract entity, coreference, and relation annotations for each abstract with a stateof-the-art information extraction system (Luan et al., 2018), and represent the annotations as a knowledge graph which collapses co-referential entities. An example of a text and graph are shown in Figure 1. We use these graph/text pairs to train a novel attention-based encoder-decoder model for knowledge-graph-to-text generation. Our model, GraphWriter, extends the successful Transformer for text encoding (Vaswani et al., 2017) to graphstructured inputs, building on the recent Graph Attention Network architecture (Veliˇckovi´c et al., 2018). The result is a powerful, general model for graph encoding which can incorporate global structural information"
N19-1238,D15-1166,0,0.0334909,"r copying input from the knowledge graph and title. At each decoding timestep t we use decoder hidden state ht to compute context vectors cg and cs for the graph and 2288 title sequence respectively. cg is computed using multi-headed attention contextualized by ht : c g = ht + N n X n L αjn WG v j (6) n=1 j∈V αj = a(ht , vL j ) (7) for a as described in Equation (1) by attending over the graph contextualized encodings VL . cs is computed similarly, attending over the title encoding T. We then construct the final context vector by concatenation, ct = [cg kcs ]. We use an input-feeding decoder (Luong et al., 2015) where both ht and ct are passed as input to the next RNN timestep. We compute a probability p of copying from the input using ht and ct in a fashion similar to See et al. (2017), that is: p = σ(Wcopy [ht kct ] + bcopy ) (8) The final next-token probability distribution is: p ∗ αcopy + (1 − p) ∗ αvocab , (9) Where the probability distribution αcopy over entities and input tokens is computed as αjcopy = a([ht kct ], xj ) for xj ∈ VkT. The remaining 1−p probability is given to αvocab , which is calculated by scaling [ht kct ] to the vocabulary size and taking a softmax. 5 Experiments Evaluation"
N19-1238,W18-6501,0,0.171392,"capture both local and global structure in the input. We show that modeling knowledge as a graph improves generation results, connecting our work to other graph-to-text tasks such as generating from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural model for this task, and show that pretraining on a large dataset of noisy automatic parses can improve results. However, they do not directly model the graph structure, relying on linearization and sequence encoding instead. Current works improve this through more sophisticated graph encoding techniques. Marcheggiani and Perez-Beltrachini (2018) encode input graphs directly using a graph convolution encoder (Kipf and Welling, 2017). Our model extends the graph attention networks of Veliˇckovi´c et al. (2018), a direct descendant of the convolutional approach which offers more modeling power and has been 2285 Vocab Tokens Entities Avg Length Avg #Vertices Avg #Edges Title 29K 413K 9.9 - Abstract 77K 5.8M 141.2 - KG 54K 1.2M 518K 12.42 4.43 Table 1: Data statistics of our AGENDA dataset. Averages are computed per instance. shown to improve performance. Song et al. (2018) uses a graph LSTM model to effect information propagation. At eac"
N19-1238,P18-2042,0,0.248995,"x is represented by a gated combination of the vertices to which it is connected and the labeled edges connecting them. Beck et al. (2018) use a similar gated graph neural network. Both of these gated models make heavy use of label information, which is much sparser in our knowledge graphs than in AMR. Generally, AMR graphs are denser, rooted, and connected, whereas the knowledge our model works with lacks these characteristics. For this reason, we focus on attention-based models such as Veliˇckovi´c et al. (2018), which impose fewer constraints on their input. Finally, our work is related to Wang et al. (2018) who offer a method for generating scientific abstracts from titles. Their model uses a gated rewriter network to write and revise several draft outputs in several sequence-to-sequence steps. While we operate in the same general domain as this work, our task setup is ultimately different due to the use of extracted information as input. We argue that our setup improves the task defined in Wang et al. (2018), and our more general model can be applied across tasks and domains. 3 The AGENDA Dataset We consider the problem of generating a text from automatically extracted information (knowledge)."
N19-1238,N16-1086,0,0.0278393,"ced a collective content selection model for generating summaries of football games from tables of game statistics. Liang et al. (2009) jointly learn to segment and align text with records, reducing the supervision needed for learning. Kim and Mooney (2010) improve this technique by learning a semantic parse to logical forms. Konstas and Lapata (2013) focus on the generation objective, jointly learning planning and generating using a rhetorical (RST) grammar induction approach. These earlier works often focused on smaller record generation datasets such as WeatherGov and RoboCup, but recently Mei et al. (2016) showed how neural models can achieve strong results on these standards, prompting researchers to investigate more challenging domains such as ours. Lebret et al. (2016) tackles the task of generating the first sentence of a Wikipedia entry from the associated infobox. They provide a large dataset of such entries and a language model conditioned on tables. Our work focuses on a multi-sentence task where relations can extend beyond sentence boundaries. Wiseman et al. (2017) study the difficulty of applying neural models to the data-to-text task. They introduce a large dataset where a text summa"
N19-1238,D17-1239,0,0.221143,"nerating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing. Here the variety of possible topics is great (e.g. topics as diverse as driving, writing poetry, and picking stocks are all referenced in one subfield of 1 Data and code available at https://github.com/ rikdz/GraphWriter Many researchers have sought to address these issues by working with structured inputs. Data-totext generation models (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Puduppully et al., 2019) condition text generation on table-structured inputs. Tabular input representations provide more guidance for producing longer texts, but are only available for limited domains as they are assembled at great expense by manual annotation processes. The current work explores the possibility of using information extraction (IE) systems to automatically provide context for generating longer texts (Figure 1). Robust IE systems are available and have support over a large variety of textual domains, and often provide rich annotations of relationships that extend beyond the"
N19-1238,P02-1040,0,0.104479,"does the abstract relate to the provided title and make use of appropriate scientific terms?). We provided examples of good and bad abstracts and explain how they succeed or fail to meet the defined criteria. Because our dataset is scientific in nature, evaluations must be done by experts and we can only collect a limited number of these high quality datapoints.2 The study was conducted by 15 experts (i.e. computer science students) who were familiar with the abstract writing task and the content of the abstracts they judged. To supplement this, we also provide automatic metrics. We use BLEU (Papineni et al., 2002), an n-gram overlap measure popular in text generation tasks, and METEOR (Denkowski and Lavie, 2014), a machine translation with paraphrase and language-specific considerations. Comparisons We compare our GraphWriter against several strong baselines. In GAT, we replace our Graph Transformer encoder with a Graph Attention Network of (Veliˇckovi´c et al., 2018). This encoder consists of PReLU activations stacked between 6 self-attention layers. To determine the usefulness of including graph relations, we compare to a model which uses only entities and title (EntityWriter). Finally, we compare wi"
N19-1238,P17-1099,0,0.0956651,"uence respectively. cg is computed using multi-headed attention contextualized by ht : c g = ht + N n X n L αjn WG v j (6) n=1 j∈V αj = a(ht , vL j ) (7) for a as described in Equation (1) by attending over the graph contextualized encodings VL . cs is computed similarly, attending over the title encoding T. We then construct the final context vector by concatenation, ct = [cg kcs ]. We use an input-feeding decoder (Luong et al., 2015) where both ht and ct are passed as input to the next RNN timestep. We compute a probability p of copying from the input using ht and ct in a fashion similar to See et al. (2017), that is: p = σ(Wcopy [ht kct ] + bcopy ) (8) The final next-token probability distribution is: p ∗ αcopy + (1 − p) ∗ αvocab , (9) Where the probability distribution αcopy over entities and input tokens is computed as αjcopy = a([ht kct ], xj ) for xj ∈ VkT. The remaining 1−p probability is given to αvocab , which is calculated by scaling [ht kct ] to the vocabulary size and taking a softmax. 5 Experiments Evaluation Metrics We evaluate using a combination of human and automatic evaluations. For human evaluation, participants were asked to compare abstracts generated by various models and tho"
P03-1017,W01-0514,0,0.0275868,"ord co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the c"
P03-1017,J93-1003,0,0.013767,"ts associated with the nodes and edges. Cosine distance ∑i x√ i yi ∑i x2i ∑i y2i xi = ∑i xi log αxi +(1−α)y i cos(~x,~y) = √ Skew divergence sα (~x,~y) Figure 2: Distance measures The dependency-based semantic space was constructed with the word-based path equivalence function from Section 2.3. As basis elements for our semantic space the 1000 most frequent words in the BNC were used. Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993). We experimented with a variety of distance measures such as cosine, Euclidean distance, L 1 norm, Jaccard’s coefficient, Kullback-Leibler divergence and the Skew divergence (see Lee 1999 for an overview). We obtained the best results for cosine (Experiment 1) and Skew divergence (Experiment 2). The two measures are shown in Figure 2. The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by Lee (1999) as a linguistically motivated distance measure. We use a value of α = .99. We explored in detail the influence of different types and sizes of conte"
P03-1017,A97-1025,0,0.0189986,"sent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important inform"
P03-1017,P99-1004,0,0.710852,"ment of Computer Science University of Sheffield Regent Court, 211 Portobello Street Sheffield S1 4DP, UK mlap@dcs.shef.ac.uk a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink ) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms (Lowe and McDonald, 2000; McD"
P03-1017,P98-2127,0,0.910708,"lected in Mirella Lapata Department of Computer Science University of Sheffield Regent Court, 211 Portobello Street Sheffield S1 4DP, UK mlap@dcs.shef.ac.uk a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink ) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface for"
P03-1017,H01-1046,0,0.0283752,"Missing"
P03-1017,J98-1004,0,0.0858584,"aditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies"
P03-1017,C98-2122,0,\N,Missing
P03-1017,kilgarriff-yallop-2000-whats,0,\N,Missing
P03-1069,A00-2018,0,0.0375897,"Missing"
P03-1069,J95-2003,0,0.470205,"Missing"
P03-1069,W00-1411,0,0.15045,"Missing"
P03-1069,J93-2004,0,0.0275901,"Missing"
P03-1069,W98-1411,0,0.0211231,"agraph orders and different levels of coherence. Finding the tree that yields the best possible text is effectively a search problem. One way to address it is by narrowing down the search space either exhaustively or heuristically. Marcu (1997) argues that global coherence can be achieved if constraints on local coherence are satisfied. The latter are operationalized as weights on the ordering and adjacency of facts and are derived from a corpus of naturally occurring texts. A constraint satisfaction algorithm is used to find the tree with maximal weights from the space of all possible trees. Mellish et al. (1998) advocate stochastic search as an alternative to exhaustively examining the search space. Rather than requiring a global optimum to be found, they use a genetic algorithm to select a tree that is coherent enough for people to understand (local optimum). The problem of finding an acceptable ordering does not arise solely in concept-to-text generation but also in the emerging field of text-to-text generation (Barzilay, 2003). Examples of applications that require some form of text structuring, are single- and multidocument summarization as well as question answering. Note that these applications"
P05-1018,N04-1015,1,0.70992,"nging from collaborative filtering (Joachims, 2002a) to parsing (Toutanova et al., 2004). In our ranking experiments, we use Joachims’ (2002a) SVMlight package for training and testing with all parameters set to their default values. 4 Evaluation Set-Up In this section we describe two evaluation tasks that assess the merits of the coherence modeling framework introduced above. We also give details regarding our data collection, and parameter estimation. Finally, we introduce the baseline method used for comparison with our approach. 4.1 Text Ordering Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al., 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information-bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. Since local coherence is a key property of any well-formed text, our model can be used to rank alternative sentence orderings. We do not assume that local coherence is sufficient to uniquely determine the best ordering — other constraints clearly play a role here. However,"
P05-1018,P97-1003,0,0.034898,"continues to show [increased earnings] O despite [the trial] X . Table 2: Summary augmented with syntactic annotations for grid computation. we employ a state-of-the-art noun phrase coreference resolution system (Ng and Cardie, 2002) trained on the MUC (6–7) data sets. The system decides whether two NPs are coreferent by exploiting a wealth of features that fall broadly into four categories: lexical, grammatical, semantic and positional. Once we have identified entity classes, the next step is to fill out grid entries with relevant syntactic information. We employ a robust statistical parser (Collins, 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. Passive verbs are recognized using a small set of patterns, and the underlying deep grammatical role for arguments involved in the passive construction is entered in the grid (see the grid cell o for Microsoft, Sentence 2, Table 2). 143 When a noun is attested more than once with a different grammatical role in the same sentence, we default to the role with the highest grammatical ranking: subjects are ranked higher than objects, which"
P05-1018,J95-2003,0,0.998158,"which is inspired by Centering Theory and can be computed automatically from raw text. We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. 1 Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson, 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza, 1990; Kibble and Power, 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale, 2000). Furthermore, coherence constraints are"
P05-1018,J04-4001,0,0.287963,"g problem and show that the proposed discourse representation supports the effective learning of a ranking function. Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model. 1 Introduction A key requirement for any system that produces text is the coherence of its output. Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson, 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza, 1990; Kibble and Power, 2004). The ability of these systems to generate high quality text, almost indistinguishable from human writing, makes the incorporation of coherence theories in robust large-scale systems particularly appealing. The task is, however, challenging considering that most previous efforts have relied on handcrafted rules, valid only for limited domains, with no guarantee of scalability or portability (Reiter and Dale, 2000). Furthermore, coherence constraints are often embedded in complex representations (e.g., Asher and Lascarides, 2003) which are hard to implement in a robust application. This paper f"
P05-1018,P03-1069,1,0.815505,"rious tasks ranging from collaborative filtering (Joachims, 2002a) to parsing (Toutanova et al., 2004). In our ranking experiments, we use Joachims’ (2002a) SVMlight package for training and testing with all parameters set to their default values. 4 Evaluation Set-Up In this section we describe two evaluation tasks that assess the merits of the coherence modeling framework introduced above. We also give details regarding our data collection, and parameter estimation. Finally, we introduce the baseline method used for comparison with our approach. 4.1 Text Ordering Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al., 2004) are commonly evaluated by their performance at information-ordering. The task concerns determining a sequence in which to present a pre-selected set of information-bearing items; this is an essential step in concept-to-text generation, multi-document summarization, and other text-synthesis problems. Since local coherence is a key property of any well-formed text, our model can be used to rank alternative sentence orderings. We do not assume that local coherence is sufficient to uniquely determine the best ordering — other constraints clearly pl"
P05-1018,N03-1020,0,0.0592586,"Missing"
P05-1018,P00-1052,0,0.020558,"intuition by introducing constraints on the distribution of discourse entities in coherent text. These constraints are formulated in terms of focus, the most salient entity in a discourse segment, and transition of focus between adjacent sentences. The theory also establishes constraints on the linguistic realization of focus, suggesting that it is more likely to appear in prominent syntactic positions (such as subject or object), and to be referred to with anaphoric expressions. A great deal of research has attempted to translate principles of Centering Theory into a robust coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004; Karamanis et al., 2004). Such a translation is challenging in several respects: one has to specify the “free parameters” of the system (Poesio et al., 2004) and to determine ways of combining the effects of various constraints. A common methodology that has emerged in this research is to develop and evaluate coherence metrics on manually annotated corpora. For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essays with transition information, and show that the distribution of transitions correlates with human grades. Karamanis et al. (2004) use a similar met"
P05-1018,J91-1002,0,0.310426,"salient entities and the rest, collecting statistics for each group separately. We identify salient entities based on their S O X – – – –– – X XX XO – XS OX O OO – OS S SO SX SS d1 d2 d3 0 0 0 .03 0 0 0 .02 .07 0 0 .12 .02 .02 .05 .25 0 0 0 .02 0 .07 0 .02 0 0 .06 .04 0 0 0 .36 .02 0 0 .03 0 0 0 .06 0 0 0 .05 .03 .07 .07 .29 Table 3: Example of a feature-vector document representation using all transitions of length two given syntactic categories: S, O, X, and –. frequency,1 following the widely accepted view that the occurrence frequency of an entity correlates with its discourse prominence (Morris and Hirst, 1991; Grosz et al., 1995). Ranking We view coherence assessment as a ranking learning problem. The ranker takes as input a set of alternative renderings of the same document and ranks them based on their degree of local coherence. Examples of such renderings include a set of different sentence orderings of the same text and a set of summaries produced by different systems for the same document. Ranking is more suitable than classification for our purposes since in text generation, a system needs a scoring function to compare among alternative renderings. Furthermore, it is clear that coherence ass"
P05-1018,P02-1014,0,0.0551969,". 3 [The case] S revolves around [evidence] O of [Microsoft] S aggressively pressuring [Netscape] O into merging [browser software] O . 4 [Microsoft]S claims [its tactics] S are commonplace and good economically. 5 [The government] S may file [a civil suit]O ruling that [conspiracy] S to curb [competition] O through [collusion]X is [a violation of the Sherman Act] O . 6 [Microsoft]S continues to show [increased earnings] O despite [the trial] X . Table 2: Summary augmented with syntactic annotations for grid computation. we employ a state-of-the-art noun phrase coreference resolution system (Ng and Cardie, 2002) trained on the MUC (6–7) data sets. The system decides whether two NPs are coreferent by exploiting a wealth of features that fall broadly into four categories: lexical, grammatical, semantic and positional. Once we have identified entity classes, the next step is to fill out grid entries with relevant syntactic information. We employ a robust statistical parser (Collins, 1997) to determine the constituent structure for each sentence, from which subjects (s), objects (o), and relations other than subject or object (x) are identified. Passive verbs are recognized using a small set of patterns,"
P05-1018,P02-1040,0,0.105341,"th naturally occurring coherence violations as perceived by human readers. A representative example of such texts are automatically generated summaries which often contain sentences taken out of context and thus display problems with respect to local coherence (e.g., dangling anaphors, thematically unrelated sentences). A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts. Existing automatic evaluation measures such as BLEU (Papineni et al., 2002) and ROUGE (Lin 2 The collections are available from http://www.csail. mit.edu/regina/coherence/. 145 and Hovy, 2003), are not designed for the coherence assessment task, since they focus on content similarity between system output and reference texts. Data Our evaluation was based on materials from the Document Understanding Conference (DUC, 2003), which include multi-document summaries produced by human writers and by automatic summarization systems. In order to learn a ranking, we require a set of summaries, each of which have been rated in terms of coherence. We therefore elicited judgment"
P05-1018,J04-3003,0,0.047824,"ically from raw text. Second, we learn patterns of entity distribution from a corpus, without attempting to directly implement or refine Centering constraints. 2 Related Work In this section we introduce our entity-based representation of discourse. We describe how it can be computed and how entity transition patterns can be extracted. The latter constitute a rich feature space on which probabilistic inference is performed. Local coherence has been extensively studied within the modeling framework put forward by Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004; Kibble and Power, 2004). One of the main assumptions underlying Centering is that a text segment which foregrounds a single entity is perceived to be more coherent than a segment in which multiple entities are discussed. The theory formalizes this intuition by introducing constraints on the distribution of discourse entities in coherent text. These constraints are formulated in terms of focus, the most salient entity in a discourse segment, and transition of focus between adjacent sentences. The theory also establishes constraints on the linguistic realization of focus, suggesting that it is"
P05-1018,J99-3001,0,0.0139369,"can be computed automatically from raw text. Second, we learn patterns of entity distribution from a corpus, without attempting to directly implement or refine Centering constraints. 2 Related Work In this section we introduce our entity-based representation of discourse. We describe how it can be computed and how entity transition patterns can be extracted. The latter constitute a rich feature space on which probabilistic inference is performed. Local coherence has been extensively studied within the modeling framework put forward by Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004; Kibble and Power, 2004). One of the main assumptions underlying Centering is that a text segment which foregrounds a single entity is perceived to be more coherent than a segment in which multiple entities are discussed. The theory formalizes this intuition by introducing constraints on the distribution of discourse entities in coherent text. These constraints are formulated in terms of focus, the most salient entity in a discourse segment, and transition of focus between adjacent sentences. The theory also establishes constraints on the linguistic realization of focus,"
P05-1018,W04-3222,0,0.0209531,"procedure is to find a parameter vector ~w that yields a “ranking score” function ~w · Φ(xi j ), which minimizes the number of violations of pairwise rankings provided in the training set. Thus, the ideal ~w would satisfy the condition ~w · (Φ(xi j ) − Φ(xik )) &gt; 0 ∀ j, i, k such that j &gt; k. The problem is typically treated as a Support Vector Machine constraint optimization problem, and can be solved using the search technique described in Joachims (2002a). This approach has been shown to be highly effective in various tasks ranging from collaborative filtering (Joachims, 2002a) to parsing (Toutanova et al., 2004). In our ranking experiments, we use Joachims’ (2002a) SVMlight package for training and testing with all parameters set to their default values. 4 Evaluation Set-Up In this section we describe two evaluation tasks that assess the merits of the coherence modeling framework introduced above. We also give details regarding our data collection, and parameter estimation. Finally, we introduce the baseline method used for comparison with our approach. 4.1 Text Ordering Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al., 2004) are commonly evaluated by their performa"
P05-1018,W98-1411,0,\N,Missing
P05-1018,A00-2018,0,\N,Missing
P05-1018,J94-2003,0,\N,Missing
P05-1018,M95-1005,0,\N,Missing
P05-1018,P04-1050,0,\N,Missing
P05-1018,W03-1004,1,\N,Missing
P05-1018,N01-1003,0,\N,Missing
P05-1018,H01-1046,0,\N,Missing
P05-1018,P95-1034,0,\N,Missing
P05-1018,P98-1116,0,\N,Missing
P05-1018,C98-1112,0,\N,Missing
P05-1018,briscoe-carroll-2002-robust,0,\N,Missing
P05-1018,P87-1022,0,\N,Missing
P05-1018,P04-1051,0,\N,Missing
P05-1018,P05-1065,0,\N,Missing
P05-1018,J01-4004,0,\N,Missing
P05-1018,P86-1031,0,\N,Missing
P05-1018,P96-1036,0,\N,Missing
P05-1018,C69-7001,0,\N,Missing
P05-1018,C69-6902,0,\N,Missing
P05-1018,N01-1025,0,\N,Missing
P06-1013,W02-0811,0,0.0432469,"typically achieve better performance than unsupervised alternatives, their applicability is limited to those words for which sense labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available (Yarowsky and Florian, 2002). The work presented here evaluates and compares the performance of well-established unsupervised WSD algorithms. We show that these algorithms yield sufficiently diverse outputs, thus motivating the use of combination methods for improving WSD performance. While combination approaches have been studied previously for supervised WSD (Florian et al., 2002), their use in an unsupervised setting is, to our knowledge, novel. We examine several existing and novel combination methods and demonstrate that our combined systems consistently outperform the 97 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 97–104, c Sydney, July 2006. 2006 Association for Computational Linguistics parametrized and can be adjusted to take the into account gloss length or to ignore function words. Distributional and WordNet Similarity McCarthy et al. (2004) propose a method for automatically ranking t"
P06-1013,P04-1036,0,0.658244,"sing the SemCor and Senseval-3 data sets demonstrate that our ensembles yield significantly better results when compared with state-of-the-art. 1 This paper focuses on unsupervised methods which we argue are useful for broad coverage sense disambiguation. Unsupervised WSD algorithms fall into two general classes: those that perform token-based WSD by exploiting the similarity or relatedness between an ambiguous word and its context (e.g., Lesk 1986); and those that perform type-based WSD, simply by assigning all instances of an ambiguous word its most frequent (i.e., predominant) sense (e.g., McCarthy et al. 2004; Galley and McKeown 2003). The predominant senses are automatically acquired from raw text without recourse to manually annotated data. The motivation for assigning all instances of a word to its most prevalent sense stems from the observation that current supervised approaches rarely outperform the simple heuristic of choosing the most common sense in the training data, despite taking local context into account (Hoste et al., 2002). Furthermore, the approach allows sense inventories to be tailored to specific domains. Introduction Word sense disambiguation (WSD), the task of identifying the"
P06-1013,H05-1052,0,0.091332,"Missing"
P06-1013,H93-1061,0,0.0579067,"nss) with regard to si . The ranking score of sense si is then increased as a function of the WordNet similarity score and the distributional similarity score (dss) between the target word and the neighbor: state-of-the-art (e.g., McCarthy et al. 2004). Importantly, our WSD algorithms and combination methods do not make use of training material in any way, nor do they use the first sense information available in WordNet. In the following section, we briefly describe the unsupervised WSD algorithms considered in this paper. Then, we present a detailed comparison of their performance on SemCor (Miller et al., 1993). Next, we introduce our system combination methods and report on our evaluation experiments. We conclude the paper by discussing our results. 2 The Disambiguation Algorithms In this section we briefly describe the unsupervised WSD algorithms used in our experiments. We selected methods that vary along the following dimensions: (a) the type of WSD performed (i.e., token-based vs. type-based), (b) the representation and size of the context surrounding an ambiguous word (i.e., graph-based vs. word-based, document vs. sentence), and (c) the number and type of semantic relations considered for dis"
P06-1013,E06-1016,0,0.136393,"Missing"
P06-1013,J91-1002,0,0.014805,"thod presented above has four parameters: (a) the semantic space model representing the distributional properties of the target words (it is acquired from a large corpus representative of the domain at hand and can be augmented with syntactic relations such as subject or object), (b) the measure of distributional similarity for discovering neighbors (c) the number of neighbors that the ranking score takes into account, and (d) the measure of sense similarity. Lexical Chains Lexical cohesion is often represented via lexical chains, i.e., sequences of related words spanning a topical text unit (Morris and Hirst, 1991). Algorithms for computing lexical chains often perform WSD before inferring which words are semantically related. Here we describe one such disambiguation algorithm, proposed by Galley and McKeown (2003), while omitting the details of creating the lexical chains themselves. Galley and McKeown’s (2003) method consists of two stages. First, a graph is built representing all possible interpretations of the target words ∑ Overlap(context, Rel(sk )) Rel∈Relations where context is a simple (space separated) concatenation of all words wi for −n ≤ i ≤ n, i 6= 0 in a context window of length ±n around"
P06-1013,W97-0201,0,0.117829,"Missing"
P06-1013,H05-1051,0,0.0342821,"ost common sense in the training data, despite taking local context into account (Hoste et al., 2002). Furthermore, the approach allows sense inventories to be tailored to specific domains. Introduction Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, holds promise for many NLP applications requiring broad-coverage language understanding. Examples include summarization, question answering, and text simplification. Recent studies have also shown that WSD can benefit machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005). Given the potential of WSD for many NLP tasks, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data, i.e., corpus occurrences of ambiguous words marked up with labels indicating the appropriate sense given the context (see Mihalcea and Edmonds 2004 and the references therein). A classifier automatically learns disambiguation cues from these hand-labeled examples. Although supervised methods typically achieve better performance than unsupervis"
P06-1013,J01-2002,0,0.079106,"Missing"
P06-1013,H05-1097,0,0.0120766,"outperform the simple heuristic of choosing the most common sense in the training data, despite taking local context into account (Hoste et al., 2002). Furthermore, the approach allows sense inventories to be tailored to specific domains. Introduction Word sense disambiguation (WSD), the task of identifying the intended meanings (senses) of words in context, holds promise for many NLP applications requiring broad-coverage language understanding. Examples include summarization, question answering, and text simplification. Recent studies have also shown that WSD can benefit machine translation (Vickrey et al., 2005) and information retrieval (Stokoe, 2005). Given the potential of WSD for many NLP tasks, much work has focused on the computational treatment of sense ambiguity, primarily using data-driven methods. Most accurate WSD systems to date are supervised and rely on the availability of training data, i.e., corpus occurrences of ambiguous words marked up with labels indicating the appropriate sense given the context (see Mihalcea and Edmonds 2004 and the references therein). A classifier automatically learns disambiguation cues from these hand-labeled examples. Although supervised methods typically a"
P06-1013,briscoe-carroll-2002-robust,0,0.011318,"n the text. These weights were imported from Galley and McKeown into our implementation without modification. Because the SemCor corpus is relatively small (less than 700,00 words), it is not ideal for constructing a neighbor thesaurus appropriate for McCarthy et al.’s (2004) method. The latter requires each word to participate in a large number of cooccurring contexts in order to obtain reliable distributional information. To overcome this problem, we followed McCarthy et al. and extracted the neighbor thesaurus from the entire BNC. We also recreated their semantic space, using a RASPparsed (Briscoe and Carroll, 2002) version of the BNC and their set of dependencies (i.e., VerbObject, Verb-Subject, Noun-Noun and AdjectiveNoun relations). Similarly to McCarthy et al., we used Lin’s (1998) measure of distributional similarity, and considered only the 50 highest ranked |{w ∈ W f |fs (w) = fm (w)}| |W f | A baseline for this task can be easily defined for each word type by selecting a sense at random from its sense inventory and assuming that this is the predominant sense: Baselinesr = 1 1 |W f |w ∑ |senses(w)| ∈W f We evaluate the algorithms’ disambiguation performance by measuring the ratio of tokens for whi"
P06-1013,W04-0856,0,\N,Missing
P06-1048,A00-2018,0,0.00896152,"Missing"
P06-1048,A00-1043,0,0.659397,"pervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality. 1 Algorithms for sentence compression fall into two broad classes depending on their training requirements. Many algorithms exploit parallel corpora (Jing 2000; Knight and Marcu 2002; Riezler et al. 2003; Nguyen et al. 2004a; Turner and Charniak 2005; McDonald 2006) to learn the correspondences between long and short sentences in a supervised manner, typically using a rich feature space induced from parse trees. The learnt rules effectively describe which constituents should be deleted in a given context. Approaches that do not employ parallel corpora require minimal or no supervision. They operationalise compression in terms of word deletion without learning specific rules and can therefore rely on little linguistic knowledge such as part-of-speech"
P06-1048,E06-1038,0,0.815802,"se models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality. 1 Algorithms for sentence compression fall into two broad classes depending on their training requirements. Many algorithms exploit parallel corpora (Jing 2000; Knight and Marcu 2002; Riezler et al. 2003; Nguyen et al. 2004a; Turner and Charniak 2005; McDonald 2006) to learn the correspondences between long and short sentences in a supervised manner, typically using a rich feature space induced from parse trees. The learnt rules effectively describe which constituents should be deleted in a given context. Approaches that do not employ parallel corpora require minimal or no supervision. They operationalise compression in terms of word deletion without learning specific rules and can therefore rely on little linguistic knowledge such as part-of-speech tags or merely the lexical items alone (Hori and Furui 2004). Alternatively, the rules of compression are"
P06-1048,C04-1107,0,0.321268,"Missing"
P06-1048,N03-1026,0,0.137728,"eakly supervised wordbased compression algorithm and examine how these models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality. 1 Algorithms for sentence compression fall into two broad classes depending on their training requirements. Many algorithms exploit parallel corpora (Jing 2000; Knight and Marcu 2002; Riezler et al. 2003; Nguyen et al. 2004a; Turner and Charniak 2005; McDonald 2006) to learn the correspondences between long and short sentences in a supervised manner, typically using a rich feature space induced from parse trees. The learnt rules effectively describe which constituents should be deleted in a given context. Approaches that do not employ parallel corpora require minimal or no supervision. They operationalise compression in terms of word deletion without learning specific rules and can therefore rely on little linguistic knowledge such as part-of-speech tags or merely the lexical items alone (Hor"
P06-1048,P05-1036,0,0.657,"orithm and examine how these models port to different domains (written vs. spoken text). To achieve this, a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used. Finally, we assess whether automatic evaluation measures can be used to determine compression quality. 1 Algorithms for sentence compression fall into two broad classes depending on their training requirements. Many algorithms exploit parallel corpora (Jing 2000; Knight and Marcu 2002; Riezler et al. 2003; Nguyen et al. 2004a; Turner and Charniak 2005; McDonald 2006) to learn the correspondences between long and short sentences in a supervised manner, typically using a rich feature space induced from parse trees. The learnt rules effectively describe which constituents should be deleted in a given context. Approaches that do not employ parallel corpora require minimal or no supervision. They operationalise compression in terms of word deletion without learning specific rules and can therefore rely on little linguistic knowledge such as part-of-speech tags or merely the lexical items alone (Hori and Furui 2004). Alternatively, the rules of"
P06-1048,W04-1015,0,0.0905938,"Missing"
P06-1048,W00-1401,0,0.0235033,"hine translation), they are typically performed once, at the end of the development cycle. Automatic evaluation measures would allow more extensive parameter tuning and crucially experimentation with larger data sets. Most human studies to date are conducted on a small compression sample, the test portion of the Ziff-Davis corpus (32 sentences). Larger sample sizes would expectedly render human evaluations time consuming and generally more difficult to conduct frequently. Here, we review two automatic evaluation measures that hold promise for the compression task. Simple String Accuracy (SSA, Bangalore et al. 2000) has been proposed as a baseline evaluation metric for natural language generation. It is based on the string edit distance between the generated output and a gold standard. It is a measure of the number of insertion (I), deletion (D) and substitution (S) errors between two strings. It is defined in (4) where R is the length of the gold standard string. Simple String Accuracy = (1 − 5 Experimental Set-up In this section we present our experimental setup for assessing the performance of the two algorithms discussed above. We explain how different model parameters were estimated. We also describ"
P06-1048,briscoe-carroll-2002-robust,0,\N,Missing
P06-1146,P05-1066,0,0.0485505,"Missing"
P06-1146,P97-1003,0,0.0317846,"Missing"
P06-1146,P05-1039,0,0.0382399,"Missing"
P06-1146,C04-1134,0,0.0531419,"filters and graph-based algorithms eliminate alignment noise to a large extent. Analysis of the models’ output revealed that the remaining errors are mostly due to incorrect parses (none of the parsers employed in this work were trained on the Europarl corpus) but also to modelling deficiencies. Recall from Section 3 that our global models cannot currently capture one-to-zero correspondences, i.e., deletions and insertions. 7 Related work Previous work has primarily focused on the projection of grammatical (Yarowsky and Ngai, 2001) and syntactic information (Hwa et al., 2002). An exception is Fung and Chen (2004), who also attempt to induce FrameNet-style annotations in Chinese. Their method maps English FrameNet entries to concepts listed in HowNet7 , an on-line ontology for Chinese, without using parallel texts. The present work extends our earlier projection framework (Padó and Lapata, 2005) by proposing global methods for automatic constituent alignment. Although our models are evaluated on the semantic role projection task, we believe they also show promise in the context of statistical machine translation. Especially for systems that use syntactic information to enhance translation quality. For"
P06-1146,J02-3001,0,0.0126883,"d demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples include question answering, information extraction, and notably machine translation. Our experiments concentrated primarily on the first projection step, i.e., establishing the right level of linguistic analysis for effecting projection. We showed that projection schemes based on constituent alignments significantly outperform schemes that rely exclusively on word alignments. A local optimisation strategy was used to find constituent alignments, while relying on a simple filtering technique to hand"
P06-1146,P03-1011,0,0.0453339,"Missing"
P06-1146,W04-3228,0,0.0394542,"Missing"
P06-1146,H05-1107,0,0.0974989,"its of projection; these are typically words but can also be chunks or syntactic constituents; (b) inducing alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples"
P06-1146,P02-1050,0,0.0615109,"syntactic constituents; (b) inducing alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples include question answering, information extraction, and notably machin"
P06-1146,N03-1017,0,0.00195893,"t and its predicate. This definition covers long-distance dependencies such as control constructions for verbs, or support constructions for nouns and adjectives, and can be extended slightly to accommodate coordination. This argument-based filter reduces target trees to a set of likely arguments. In the example in Figure 3, all tree nodes are removed except Kim and pünktlich zu kommen. entire English-German Europarl bitext as training data (20M words). We used the GIZA++ default settings to induce alignments for both directions (source-target, target-source). Following common practise in MT (Koehn et al., 2003), we considered only their intersection (bidirectional alignments are known to exhibit high precision). We also produced manual word alignments for all sentences in our corpus, using the GIZA++ alignments as a starting point and following the Blinker annotation guidelines (Melamed, 1998). Method and parameter choice The constituent alignment models we present are unsupervised in that they do not require labelled data for inferring correct alignments. Nevertheless, our models have three parameters: (a) the similarity measure for identifying semantically equivalent constituents; (b) the filterin"
P06-1146,2005.mtsummit-papers.11,0,0.109912,"Missing"
P06-1146,J03-1002,0,0.010076,"Missing"
P06-1146,H05-1108,1,0.88534,"alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. Examples include question answering, information extraction, and notably machine translation. Our experiments concentrate"
P06-1146,C04-1073,0,0.00824849,"Missing"
P06-1146,W04-3212,0,0.0110476,"tives, adverbs, verbs, or nouns, from the source and target sen1164 S VP S VP Kim versprach, pünktlich zu kommen. Figure 3: Filtering of unlikely arguments (predicate in boldface, potential arguments in boxes). tences (Padó and Lapata, 2005). We also use a novel filter which removes all words which remain unaligned in the automatic word alignment. Nonterminal nodes whose terminals are removed by these filters, are also pruned. Argument filtering Previous work in shallow semantic parsing has demonstrated that not all nodes in a tree are equally probable as semantic roles for a given predicate (Xue and Palmer, 2004). In fact, assuming a perfect parse, there is a “set of likely arguments”, to which almost all semantic roles roles should be assigned to. This set of likely arguments consists of all constituents which are a child of some ancestor of the predicate, provided that (a) they do not dominate the predicate themselves and (b) there is no sentence boundary between a constituent and its predicate. This definition covers long-distance dependencies such as control constructions for verbs, or support constructions for nouns and adjectives, and can be extended slightly to accommodate coordination. This ar"
P06-1146,H01-1035,0,0.235924,"s: (a) determining the units of projection; these are typically words but can also be chunks or syntactic constituents; (b) inducing alignments between the projection units and projecting annotations along these alignments; (c) reducing the amount of noise in the projected annotations, often due to errors and omissions in the word alignment. The degree to which analyses are parallel across languages is crucial for the success of projection approaches. A number of recent studies rely on this notion of parallelism and demonstrate that annotations can be adequately projected for parts of speech (Yarowsky and Ngai, 2001; Hi and Hwa, 2005), chunks (Yarowsky and Ngai, 2001), and dependencies (Hwa et al., 2002). In previous work (Padó and Lapata, 2005) we considered the annotation projection of semantic roles conveyed by sentential constituents such as AGENT, PATIENT, or I NSTRUMENT. Semantic roles exhibit a high degree of parallelism across languages (Boas, 2005) and thus appear amenable to projection. Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic"
P06-2019,briscoe-carroll-2002-robust,0,0.0645605,"nts that extend the basic language model presented in Equations (1)–(7). Our aim is to bring some syntactic knowledge into the compression model and to preserve the meaning of the original sentence as much as possible. Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). Importantly, we do not require any additional knowledge sources (such as a lexicon) beyond the parse and grammatical relations of the original sentence. This is provided in our experiments by the Robust Accurate Statistical Parsing (RASP) toolkit (Briscoe and Carroll 2002). However, there is nothing inherent in our formulation that restricts us to RASP; any other parser with similar output could serve our purposes. i=0 j=1 yj − ∑ O: He became a power player in Greek Politics in 1974, when he founded the socialist Pasok Party. He became a player in the Pasok. He became a player in the Pasok Party. He became a player in politics. He became a player in politics when he founded the Pasok Party. Finally, AppleShare Printer Server, formerly a separate package, is now bundled with AppleShare File Server. Finally, AppleShare, a separate, AppleShare. Finally, AppleShare"
P06-2019,A00-1043,0,0.908989,"programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 1 Introduction A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1 , w2 , . . . , wn , a compression is formed by removing any subset of these wo"
P06-2019,W05-0618,0,0.0358785,": a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and semantically valid. Our task is to find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and constraints must be linear. A number of decin max z = ∑ yi · P(wi ) i=1 Thus if a word is selected, its corresponding"
P06-2019,E06-1038,0,0.588153,"model searches for the best compression given the source and channel models. However, the compression found is usually sub-optimal as heuristics are used to reduce the search space or is only locally optimal due to the search method employed. The decoding process used in Turner and Charniak’s (2005) model first searches for the best combination of rules to apply. As they traverse their list of compression rules they remove sentences outside the 100 best compressions (according to their channel model). This list is eventually truncated to 25 compressions. In other models (Hori and Furui 2004; McDonald 2006) the compression score is maximised could prevent overly long or overly short compressions or generally avoid compressions that lack a main verb or consist of repetitions of the same word. In the following section we provide an overview of previous approaches to sentence compression. In Section 3 we motivate the treatment of sentence compression as an optimisation problem and formulate our language model and constraints in the IP framework. Section 4 discusses our experimental set-up and Section 5 presents our results. Discussion of future work concludes the paper. 2 Previous Work Jing (2000)"
P06-2019,P99-1045,0,0.0283015,"aints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 1 Introduction A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1 , w2 , . . . , wn , a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approac"
P06-2019,C04-1197,0,0.010432,"desirable it is. Our formulation consists of two basic components: a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and semantically valid. Our task is to find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and constraints must be linear. A number of decin max"
P06-2019,N03-1026,0,0.293998,"ng sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1 , w2 , . . . , wn , a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approaches (Knight and Marcu 2002; Turner and Charniak 2005) are instantiations of the noisy-channel model: given a long sentence l, the aim is to find the corresponding short sentence s which maximises the conditional probability P(s|l). In a discriminative setting (Knight 144 Proceedings of the COLING/ACL 2006 Main Co"
P06-2019,W04-2401,0,0.0926555,"nd select the best one, as determined by how desirable it is. Our formulation consists of two basic components: a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and semantically valid. Our task is to find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and"
P06-2019,P05-1036,0,0.144992,"irella Lapata School of Informatics, University of Edinburgh 2 Bucclecuch Place, Edinburgh EH8 9LW, UK jclarke@ed.ac.uk, mlap@inf.ed.ac.uk and Marcu 2002; Riezler et al. 2003; McDonald 2006), sentences are represented by a rich feature space (typically induced from parse trees) and the goal is to learn rewrite rules indicating which words should be deleted in a given context. Both modelling paradigms assume access to a training corpus consisting of original sentences and their compressions. Unsupervised approaches to the compression problem are few and far between (see Hori and Furui 2004 and Turner and Charniak 2005 for exceptions). This is surprising considering that parallel corpora of original-compressed sentences are not naturally available in the way multilingual corpora are. The scarcity of such data is demonstrated by the fact that most work to date has focused on a single parallel corpus, namely the Ziff-Davis corpus (Knight and Marcu 2002). And some effort into developing appropriate training data would be necessary when porting existing algorithms to new languages or domains. In this paper we present an unsupervised model of sentence compression that does not rely on a parallel corpus – all tha"
P06-2019,W04-1015,0,0.00979482,"essions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 1 Introduction A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1 , w2 , . . . , wn , a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative"
P06-2019,C04-1107,0,\N,Missing
P07-1092,J93-2003,0,0.00908956,"arget phrases. This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 1 Introduction Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the outpu"
P07-1092,W03-0310,0,0.0589821,"g multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1"
P07-1092,N06-1003,0,0.393761,"ferent languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1 0.2 0.5 standard Italian all all + standard 0.005 0.01 proportion of test events in phrase table and evaluated against a test set. Although very few small phrases are unknown, the majority of larger phrases are unseen. The Italian and all results show that triangulation alone can provide similar or improved coverage compared to the standard sourcetarget model; further improvement is achieved by combining the tr"
P07-1092,W05-0828,0,0.0228095,"nto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1 0.2 0.5 standard Italian all all + standard 0.005 0.01 proportion of test events in phrase table and evaluated against a test set. Although very few small phrases are u"
P07-1092,W06-1607,0,0.0894961,"ge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters, etc.). Our method translates each target phrase, t, first to an intermediate language, i, and then into the source language, s. We call this two-stage translation process triangulation (Kay, 1997). We present a probabilistic formulation through which we can estimate the desired phrase translation distribution (phrase-table) by marginaliP sation, p(s|t) = i p(s, i|t). As with conventional smoothing methods (Koehn et al., 2003; Foster et al., 2006), triangulation increases the robustness of phrase translation estimates. In contrast to smoothing, our method alleviates data sparseness by exploring additional multiparallel data rather than adjusting the probabilities of existing data. Importantly, triangulation provides us with separately estimated phrase-tables which could be further smoothed to provide more reliable distributions. Moreover, the triangulated phrase-tables can be easily combined with the standard sourcetarget phrase-table, thereby improving the coverage over unseen source phrases. As an example, consider Figure 1 which sho"
P07-1092,N03-1017,0,0.363586,"e phrase. Central to our approach is triangulation, the process of translating from a source to a target language via an intermediate third language. This allows the use of a much wider range of parallel corpora for training, and can be combined with a standard phrase-table using conventional smoothing methods. Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase-based system. 1 Introduction Statistical machine translation (Brown et al., 1993) has seen many improvements in recent years, most notably the transition from word- to phrase-based models (Koehn et al., 2003). Modern SMT systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world’s lang"
P07-1092,2005.mtsummit-papers.11,0,0.0297866,"Missing"
P07-1092,E06-1005,0,0.0182169,"biguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying candidate phrases in the source lan729 1 0.02 0.05 0.1 0.2 0.5 standard Italian all all + standard 0.005 0.01 proportion of test events in phrase table and evaluated against a test set. A"
P07-1092,2001.mtsummit-papers.46,0,0.289389,"on for Computational Linguistics 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires pa"
P07-1092,W99-0604,0,0.026701,")λj (4) T j where T and S denote a target and source sentence respectively. The parameters, λj , were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to 3 treat each distribution as a feature, and learn the mixFor details see http://www.statmt.org/wpt05/ ing weights automatically. Note that we must indi- mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent w"
P07-1092,P03-1021,0,0.0192285,"ing setting was chosen to simulate translating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: Y T∗ = arg max fj (T, S)λj (4) T j where T and S denote a target and source sentence respectively. The parameters, λj , were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). This has the same form use"
P07-1092,P02-1040,0,0.0805575,"slating to or from a “low density” language, where only a few small independently sourced parallel corpora are available. These bitexts were used for direct translation and triangulation. All experimental results were evaluated on the ACL/WMT 20053 set of 2,000 sentences, and are reported in BLEU percentage-points. Decoding Pharaoh (Koehn, 2003), a beamsearch decoder, was used to maximise: Y T∗ = arg max fj (T, S)λj (4) T j where T and S denote a target and source sentence respectively. The parameters, λj , were trained using minimum error rate training (Och, 2003) to maximise the BLEU score (Papineni et al., 2002) on a 150 sentence development set. We used a standard set of features, comprising a 4-gram language model, distance based distortion model, forward and backward translation probabilities, forward and backward lexical translation scores and the phraseand word-counts. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al., 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al., 2003). This has the same form used for log-linear training of SMT decoders (Och, 200"
P07-1092,J03-3002,0,0.01324,"T systems are capable of producing high quality translations when provided with large quantities of training data. With only a small training sample, the translation output is often inferior to the output from using larger corpora because the translation algorithm must rely on more sparse estimates of phrase frequencies and must also ‘back-off’ to smaller sized phrases. This often leads to poor choices of target phrases and reduces the coherence of the output. Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world’s languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems. 728 In this paper we provide a means for obtaining more reliable translation frequency estimates from small datasets. We make use of multi-parallel corpora (sentence aligned parallel texts over three or more languages). Such corpora are often created by international organisations, the United Nations (UN) being a prime example. They present a challenge for current SMT systems due to their relatively moderate size and domain variability (examples of UN texts include policy documents, proceedings of meetings, letters,"
P07-1092,N07-1061,0,0.756544,"l Linguistics 2 Related Work The idea of using multiple source languages for improving the translation quality of the target language dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available. Systems which have used this notion of triangulation typically create several candidate sentential target translations for source sentences via different languages. A single translation is then selected by finding the candidate that yields the best overall score (Och and Ney, 2001; Utiyama and Isahara, 2007) or by cotraining (Callison-Burch and Osborne, 2003). This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al., 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. Beyond SMT, the use of an intermediate language as a translation aid has also found application in cross-lingual information retrieval (Gollins and Sanderson, 2001). Callison-Burch et al. (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Their method acquires paraphrases by identifying can"
P07-1092,N04-1033,0,0.0628609,"hn et al., 2003). This has the same form used for log-linear training of SMT decoders (Och, 2003), which allows us to 3 treat each distribution as a feature, and learn the mixFor details see http://www.statmt.org/wpt05/ ing weights automatically. Note that we must indi- mt-shared-task. 731 Lexical weights The lexical translation score is used for smoothing the phrase-table translation estimate. This represents the translation probability of a phrase when it is decomposed into a series of independent word-for-word translation steps (Koehn et al., 2003), and has proven a very effective feature (Zens and Ney, 2004; Foster et al., 2006). Pharaoh’s lexical weights require access to word-alignments; calculating these alignments between the source and target words in a phrase would prove difficult for a triangulated model. Therefore we use a modified lexical score, corresponding to the maximum IBM model 1 score for the phrase pair: lex(t|s) = Y 1 max p(tk |sak ) Z a (5) k where the maximisation4 ranges over all one-tomany alignments and Z normalises the score by the number of possible alignments. The lexical probability is obtained by interpolating a relative frequency estimate on the sourcetarget bitext w"
P08-1028,briscoe-carroll-2002-robust,0,0.0979609,"not provide a comprehensive test set. In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects. In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated. Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject. We first compiled a list of intransitive verbs from CELEX2 . All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll, 2002) version of the British National Corpus (BNC). Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors. Each reference subject-verb tuple (e.g., horse ran ) was paired with two landmarks, each a synonym of the verb. The landmarks were chosen so as to represent distinct verb senses, one compatible 2 http://www.ru.nl/celex/ 240 with the reference (e.g., horse galloped ) and one incompatible (e.g., horse dissolved ). Landmarks were taken from WordNet (Fellbaum, 1998). Specifically, they belonged to different synsets and were maxi"
P08-1028,W01-0514,0,0.00464279,"ome increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Schu¨ tze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed at representing words in isolation and m"
P08-1028,O97-1002,0,0.0762497,"pus (BNC). Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors. Each reference subject-verb tuple (e.g., horse ran ) was paired with two landmarks, each a synonym of the verb. The landmarks were chosen so as to represent distinct verb senses, one compatible 2 http://www.ru.nl/celex/ 240 with the reference (e.g., horse galloped ) and one incompatible (e.g., horse dissolved ). Landmarks were taken from WordNet (Fellbaum, 1998). Specifically, they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs, each paired with 10 nouns, and 2 landmarks (400 pairs of sentences in total). These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences. In the pretest, subjects saw a reference sentence containing a subject-verb tuple and its landmarks and were asked to choose which landmark was most similar to the reference or neither. Our items were converted into simple sentences (all in past tense) by adding articl"
P08-1028,P04-1036,0,0.0100206,"ector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Schu¨ tze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespre"
P08-1028,J07-2002,1,0.763572,"Missing"
P08-1028,N04-3012,0,0.249964,"s. Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples. Then the experimental items were presented; each contained two sentences, one with the reference verb and one with its landmark. Examples of our items are given in Table 1. Here, burn is a high similarity landmark (High) for the reference The fire glowed, whereas beam is a low similarity landmark (Low). The opposite is the case for the reference The face 3 We assessed a wide range of semantic similarity measures using the WordNet similarity package (Pedersen et al., 2004). Most of them yielded similar results. We selected Jiang and Conrath’s measure since it has been shown to perform consistently well across several cognitive and NLP tasks (Budanitsky and Hirst, 2001). The The The The The The Noun fire face child discussion sales shoulders Reference glowed glowed strayed strayed slumped slumped High burned beamed roamed digressed declined slouched Low beamed burned digressed roamed slouched declined Table 1: Example Stimuli with High and Low similarity landmarks 7 6 5 4 3 2 1 0 glowed. Sentence pairs were presented serially in random order. Participants were a"
P08-1028,W01-0513,0,0.0172648,"gess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science. The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968). A variety of NLP tasks have made good use of vector-based models. Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Schu¨ tze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975). In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998). Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004). Despite their widespread use, vector-based models are typically directed a"
P08-1028,J98-1004,0,0.594664,"Missing"
P08-1028,J03-2004,0,\N,Missing
P08-1028,W03-1812,0,\N,Missing
P08-1028,N03-1003,0,\N,Missing
P08-1028,P05-1074,0,\N,Missing
P08-1028,W03-1809,0,\N,Missing
P08-1028,P98-2127,0,\N,Missing
P08-1028,C98-2122,0,\N,Missing
P09-1025,P04-1011,0,0.0171746,"d their actions is merely the first step towards creating an automatic story generator. The latter must be able to select which information to include in the story, in what order to present it, how to convert it into English. Recent work in natural language generation has seen the development of learning methods for realizing each of these tasks automatically without much hand coding. For example, Duboue and McKeown (2002) and Barzilay and Lapata (2005) propose to learn a content planner from a parallel corpus. Mellish et al. (1998) advocate stochastic search methods for document structuring. Stent et al. (2004) learn how to combine the syntactic structure of elementary speech acts into one or more sentences from a corpus of good and bad examples. And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. Although successful on their own, these methods have not been yet integrated together into an end-to-end probabilistic system. Our work attempts to do this for the story generation task, while bridging the gap between story generators and NLG systems. Our generator operat"
P09-1025,H05-1042,1,0.820763,"tructure, characters, events, and vocabularies. It is precisely this type of information we wish to extract and quantify. Of course, building a database of characters and their actions is merely the first step towards creating an automatic story generator. The latter must be able to select which information to include in the story, in what order to present it, how to convert it into English. Recent work in natural language generation has seen the development of learning methods for realizing each of these tasks automatically without much hand coding. For example, Duboue and McKeown (2002) and Barzilay and Lapata (2005) propose to learn a content planner from a parallel corpus. Mellish et al. (1998) advocate stochastic search methods for document structuring. Stent et al. (2004) learn how to combine the syntactic structure of elementary speech acts into one or more sentences from a corpus of good and bad examples. And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. Although successful on their own, these methods have not been yet integrated together into an end-to-end proba"
P09-1025,briscoe-carroll-2002-robust,0,0.0747596,"stem about relationships on the sentence level. However, a story created simply by concatenating sentences in isolation will often be incoherent. Investigations into the interpretation of narrative discourse (Asher and Lascarides, 2003) have shown that lexical information plays an important role in determining Content Planning As mentioned earlier our generator has access to a knowledge base recording entities and their interactions. These are essentially predicate argument structures extracted from a corpus. In our experiments this knowledge base was created using the RASP relational parser (Briscoe and Carroll, 2002). We collected all verb-subject, verb-object, verb-adverb, and noun-adjective relations from the parser’s output and scored them with the mutual 219 OBJ:catch 5 6 SUBJ:chase OBJ:chase 2.2 SUBJ:frighten So far we have described how we gather knowledge about entities and their interactions, which must be subsequently combined into a sentence. The backbone of our sentence planner is a grammar with subcategorization information which we collected from the lexicon created by Korhonen and Briscoe (2006) and the COMLEX dictionary (Grishman et al., 1994). The grammar rules act as templates. They each"
P09-1025,P08-1090,0,0.0163551,"stic approach advocated here is feasible at all, before venturing towards more ambitious components. 8 1 1 SUBJ:run 2 SUBJ:fall 5 2 SUBJ:escape SUBJ:jump Figure 3: Graph encoding (partially ordered) chains of events the discourse relations between propositions. Although we don’t have an explicit model of rhetorical relations and their effects on sentence ordering, we capture the lexical inter-dependencies between sentences by focusing on events (verbs) and their precedence relationships in the corpus. For every entity in our training corpus we extract event chains similar to those proposed by Chambers and Jurafsky (2008). Specifically, we identify the events every entity relates to and record their (partial) order. We assume that verbs sharing the same arguments are more likely to be semantically related than verbs with no arguments in common. For example, if we know that someone steals and then runs, we may expect the next action to be that they hide or that they are caught. In order to track entities and their associated events throughout a text, we first resolve entity mentions using OpenNLP2 . The list of events performed by co-referring entities and their grammatical relation (i.e., subject or object) ar"
P09-1025,W02-2112,0,0.01946,"bles) typically have similar structure, characters, events, and vocabularies. It is precisely this type of information we wish to extract and quantify. Of course, building a database of characters and their actions is merely the first step towards creating an automatic story generator. The latter must be able to select which information to include in the story, in what order to present it, how to convert it into English. Recent work in natural language generation has seen the development of learning methods for realizing each of these tasks automatically without much hand coding. For example, Duboue and McKeown (2002) and Barzilay and Lapata (2005) propose to learn a content planner from a parallel corpus. Mellish et al. (1998) advocate stochastic search methods for document structuring. Stent et al. (2004) learn how to combine the syntactic structure of elementary speech acts into one or more sentences from a corpus of good and bad examples. And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. Although successful on their own, these methods have not been yet integrated to"
P09-1025,C94-1042,0,0.0451265,"s created using the RASP relational parser (Briscoe and Carroll, 2002). We collected all verb-subject, verb-object, verb-adverb, and noun-adjective relations from the parser’s output and scored them with the mutual 219 OBJ:catch 5 6 SUBJ:chase OBJ:chase 2.2 SUBJ:frighten So far we have described how we gather knowledge about entities and their interactions, which must be subsequently combined into a sentence. The backbone of our sentence planner is a grammar with subcategorization information which we collected from the lexicon created by Korhonen and Briscoe (2006) and the COMLEX dictionary (Grishman et al., 1994). The grammar rules act as templates. They each take a verb as their head and propose ways of filling its argument slots. This means that when generating a story, the choice of verb will affect the structure of the sentence. The subcategorization templates are weighted by their probability of occurrence in the reference dictionaries. This allows the system to prefer less elaborate grammatical structures. The grammar rules were converted to a format compatible with our surface realizer (see Section 2.3) and include information pertaining to mood, agreement, argument role, etc. Our sentence plan"
P09-1025,J95-2003,0,0.0527351,"Missing"
P09-1025,P95-1034,0,0.0465184,"y, in what order to present it, how to convert it into English. Recent work in natural language generation has seen the development of learning methods for realizing each of these tasks automatically without much hand coding. For example, Duboue and McKeown (2002) and Barzilay and Lapata (2005) propose to learn a content planner from a parallel corpus. Mellish et al. (1998) advocate stochastic search methods for document structuring. Stent et al. (2004) learn how to combine the syntactic structure of elementary speech acts into one or more sentences from a corpus of good and bad examples. And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. Although successful on their own, these methods have not been yet integrated together into an end-to-end probabilistic system. Our work attempts to do this for the story generation task, while bridging the gap between story generators and NLG systems. Our generator operates over predicate-argument and predicate-predicate co-occurrence statistics gathered from corpora. These are used to produce a large set of candidate stories which are subsequen"
P09-1025,korhonen-etal-2006-large,0,0.0170202,"Missing"
P09-1025,A97-1039,0,0.0383751,"their own, these methods have not been yet integrated together into an end-to-end probabilistic system. Our work attempts to do this for the story generation task, while bridging the gap between story generators and NLG systems. Our generator operates over predicate-argument and predicate-predicate co-occurrence statistics gathered from corpora. These are used to produce a large set of candidate stories which are subsequently ranked based on their interestingness and coherence. The top-ranked candidate is selected for presentation and verbalized using a language model interfaced with RealPro (Lavoie and Rambow, 1997), a text generation engine. This generate-and-rank architecture circumvents the complexity of traditional generation Figure 1: Children’s stories from McGuffey’s Eclectic Primer Reader; it contains primary reading matter to be used in the first year of school work. systems, where numerous, often conflicting constraints, have to be encoded during development in order to produce a single high-quality output. As a proof of concept we initially focus on children’s stories (see Figure 1 for an example). These stories exhibit several recurrent patterns and are thus amenable to a data-driven approach"
P09-1025,P98-2127,0,0.0194444,"mes computationally prohibitive. Fortunately, we can use beam search to prune low-scoring sentences and the stories they generate. For example, we may prefer sentences describing actions that are common for their characters. We also apply two additional criteria in selecting good stories, namely whether they are coherent and interesting. At each depth in the tree we maintain the N-best stories. Once we reach the required length, the highest scoring story is presented to the user. In the following we describe the components of our system in more detail. 2.1 information-based metric proposed in Lin (1998):   k w, r, w0 k × k ∗, r, ∗ k MI = ln (1) k w, r, ∗ k × k ∗, r, w0 k where w and w0 are two words with relation type r. ∗ denotes all words in that particular relation and k w, r, w0 k represents the number of times w, r, w0 occurred in the corpus. These MI scores are used to inform the generation system about likely entity relationships at the sentence level. Table 1 shows high scoring relations for the noun dog extracted from the corpus used in our experiments (see Section 4 for details). Note that MI weighs binary relations which in some cases may be likely on their own without making se"
P09-1025,W98-1411,0,0.0835818,"Missing"
P09-1025,J08-1001,1,\N,Missing
P09-1025,C98-2122,0,\N,Missing
P10-1021,N01-1021,0,0.576616,"ted by Staub and Clifton (2006): following the word either, readers predict or and the complement that follows it, and process it faster compared to a control condition without either. Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. Much recent work has focused on developing computational measures of these constraints and expectations. Again, the literature is split into syntactic and semantic models. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficu"
P10-1021,P10-2012,1,0.839713,"to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures. An interesting question is which aspects of semantics our model is able to capture, i.e., why does the combination of LSA or LDA representations with an incremental parser yield a better fit of the behavioral data. In the psycholinguistic literature, various types of semantic information have been investigated: lexical semantics (word senses, selectional restrictions, thematic roles), sentential semantics (scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key"
P10-1021,P10-1120,0,0.0761262,"scope, binding), and discourse semantics (coreference and coherence); see Keller (2010) of a detailed discussion. We conjecture that our model is mainly capturing lexical semantics (through the vector space representation of words) and sentential semantics (through the multiplication or addition of words). However, discourse coreference effects (such as the ones reported by Altmann and Steedman (1988) and much subsequent work) are probably not amenable to a treatment in terms of vector space semantics; an explicit representation of discourse entities and coreference relations is required (see Dubey 2010 for a model of human sentence processing that can handle coreference). A key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly. For example, we could envisage a parser that uses semantic representations to guide its search, e.g., by pruning syntactic analyses that have a low semantic probability. At the same time, the semantic model should have access to syntactic information, i.e., the composition of word representations should take their syntactic relationships into account, rather than just linear order. Refer"
P10-1021,J01-2004,0,0.26386,"ords). (Rayner 1998) demonstrates that eye-movements are related to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore"
P10-1021,P04-1003,0,0.185212,"rsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. Pynte et al. (2008) use Latent Semantic Analysis (LSA, Landauer and Dumais 1997) to assess the degree of contextual constraint exerted on a word by its context. In this framework, word meanings are represented as vectors in a high dimensional space and distance in this space is interpreted as an index of processing difficulty. Other work (McDonald and Brew 2004) models contextual constraint in information theoretic terms. The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. The measures discussed above are typically computed automatically on real-language corpora using data-driven methods and their predictions are verified through analysis of eye-movements that people make while reading. Ample evidence The analysis of reading times can provide insights into the proc"
P10-1021,D09-1034,0,0.451527,"ng times. Mc197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are predictive of first fixation and first pass durations: the higher the transitional probability, the shorter the fixation time. Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk |wk−1 ). Analogously, forward probability is the conditional probability of the current word given the next word, P(wk |wk+1 ). (e.g., left-to-right vs. top-down, PCFGs vs dependency parsing) and different degrees of lexicalization (see Roark et al. 2009 for an overview) . For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1) with parts of speech (Demberg and Keller 2008). Surprisal could be also defined using a vanilla language model that does not take any structural or grammatical information into account (Frank 2009). 2.2 2.3 Syntactic Constraint Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. These models are based on the idea that words with similar meanings will be foun"
P10-1021,P08-1028,1,0.735474,"ation is updated using a Bayesian inference mechanism to reflect the newly arrived information. Like LSA, ICD is based on word co-occurrence vectors, however it does not employ singular value decomposition, and constructs a word-word rather than a word-document co-occurrence matrix. Although this model has been shown to successfully simulate single- and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Their aim is not so much to model processing difficulty, but to construct vector-based meaning representations that go beyond individual words. They introduce a h = f (u, v) (2) where h denotes the composition of u and v. Different composition models arise, depending on how f is chosen. Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi = ui + vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus deriv"
P10-1021,D09-1045,1,0.30952,"lated to the moment-to-moment cognitive activities of readers. They also provide an accurate temporal record of the on-line processing of natural language, and through the analysis of eyemovement measurements (e.g., the amount of time spent looking at a word) can give insight into the processing difficulty involved in reading. In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e.g., phrases, sentences, even documents) rather than isolated words. Some existing models of sentence processing integrate semantic information into a probabilistic parser (Narayanan and Jurafsky 2002; Pad´o et al. 2009); however, the semantic component of these models is limited to semantic role information, rather than attempting to build a full semantic representation for a sentence. Furthermore, the models of Narayanan and Jurafsky (2002) and Pad´o et al. (2009) do"
P10-1021,J07-2002,1,0.660376,"Missing"
P10-1058,P00-1041,0,0.042792,". • King gave his “I have a dream” speech in 1963. • Billboards use image from 9/11 to encourage GOP votes. • 9/11 image wrong for ad, say Florida political parties. • Floridian praises President Bush, says ex-President Clinton failed to stop al Qaeda. Table 1: Two example CNN news articles, showing the title and the first few paragraphs, and below, the original highlights that accompanied each story. guage model. Lastly, our model is more compact, has fewer parameters, and does not require two training procedures. Our approach bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000), although we output several sentences rather than a single one. Headline generation models typically extract individual words from a document to produce a very short summary, whereas we extract phrases and ensure that they are combined into grammatical sentences through our ILP constraints. Svore et al. (2007) were the first to foreground the highlight generation task which we adopt as an evaluation testbed for our model. Their approach is however a purely extractive one. Using an algorithm based on neural networks and third-party resources (e.g., news query logs and Wikipedia entries) they r"
P10-1058,D07-1001,1,0.350626,"aintain the informativeness and grammaticality of a competitive extractive system. The model itself is relatively simple and knowledge-lean, and achieves good performance without reference to any resources outside the corpus collection. Future extensions are many and varied. An obvious next step is to examine how the model generalizes to other domains and text genres. Although coherence is not so much of an issue for highlights, it certainly plays a role when generating standard summaries. The ILP model can be straightforwardly augmented with discourse constraints similar to those proposed in Clarke and Lapata (2007). We would also like to generalize the model to arbitrary rewrite operations, as our results indicate that compression rates are likely to improve with more sophisticated paraphrasing. Table 5: Generated highlights for the stories in Table 1 using the phrase ILP model. ROUGE -1 ROUGE -2 .184 ± .015 .180 ± .076 .191 ± .015 .200 ± .014 Table 7: ROUGE results on DUC-2002 corpus (140 documents). —: only ROUGE -1 and ROUGE -2 results are given in Martins and Smith (2009). • A Florida man is using billboards with an image of the burning World Trade Center to encourage votes for a Republican presiden"
P10-1058,P02-1057,0,0.0137015,"Missing"
P10-1058,W03-0501,0,0.209134,"Missing"
P10-1058,A00-1043,0,0.0211493,"ons, deletions, or reorderings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine gene"
P10-1058,J02-4006,0,0.0278278,"Missing"
P10-1058,P03-1054,0,0.0302828,"ess optimistic.” We can see from this example that encoding the possible outputs as decisions on branches of the phrase structure tree provides a more compact representation of many options than would be possible with an explicit enumeration of all possible compressions. Which output is chosen (if any) Highlight generation We generated highlights for a test set of 600 documents. We created and 570 solved an ILP for each document. Sentences were first tokenized to separate words and punctuation, then parsed to obtain phrases and dependencies as described in Section 4 using the Stanford parser (Klein and Manning, 2003). For each phrase, features were extracted and salience scores calculated from the feature weights determined through SVM training. The distance from the SVM hyperplane represents the salience score. The ILP model (see Equation (1)) was parametrized as follows: the maximum number of highlights NS was 4, the overall limit on length LT was 75 tokens, the length of each highlight was in the range of [8, 28] tokens, and the topic coverage set T contained the top 5 tf.idf words. These parameters were chosen to capture the properties seen in the majority of the training set; they were also relaxed e"
P10-1058,W03-1101,0,0.0725731,"ns, or reorderings. Sentence compression is often regarded as a promising first step towards ameliorating some of the problems associated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compr"
P10-1058,N03-1020,0,0.0179496,"leaf nodes in order. The ILP problems we created had on average 290 binary variables and 380 constraints. The mean solve time was 0.03 seconds. highlights (2c) which we set to 3. There are no sentence length or grammaticality constraints, as there is no sentence compression. max x s.t. (2a) ∑ f i xi i∈S ∑ xi ≥ 1 ∀t ∈ T (2b) i∈St ∑ xi ≤ NS (2c) i∈S xi ∈ {0, 1} ∀i ∈ S . (2d) The SVM was trained with the same features used to obtain phrase-based salience scores, but with sentence-level labels (labels (1) and (2) positive, (3) negative). Evaluation We evaluated summarization quality using ROUGE (Lin and Hovy, 2003). For the highlight generation task, the original CNN highlights were used as the reference. We report unigram overlap (ROUGE -1) as a means of assessing informativeness and the longest common subsequence (ROUGE -L) as a means of assessing fluency. In addition, we evaluated the generated highlights by eliciting human judgments. Participants were presented with a news article and its corresponding highlights and were asked to rate the latter along three dimensions: informativeness (do the highlights represent the article’s main topics?), grammaticality (are they fluent?), and verbosity (are the"
P10-1058,W01-0100,0,0.777879,"ciated with extractive summarization. The task is commonly expressed as a word deletion problem. It involves creating a short grammatical summary of a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard. Another reason is the summarization objective itself. If"
P10-1058,W09-1801,0,0.35585,"a single sentence, by removing elements that are considered extraneous, while retaining the most important information (Knight and Marcu, 2002). Interfacing extractive summarization with a sentence compression module could improve the conciseness of the generated summaries and render them more informative (Jing, 2000; Lin, 2003; Zajic et al., 2007). Despite the bulk of work on sentence compression and summarization (see Clarke and Lapata 2008 and Mani 2001 for overviews) only a handful of approaches attempt to do both in a joint model (Daum´e III and Marcu, 2002; Daum´e III, 2006; Lin, 2003; Martins and Smith, 2009). One reason for this might be the performance of sentence compression systems which falls short of attaining grammaticality levels of human output. For example, Clarke and Lapata (2008) evaluate a range of state-of-the-art compression systems across different domains and show that machine generated compressions are consistently perceived as worse than the human gold standard. Another reason is the summarization objective itself. If our goal is to summarize news articles, then we may be better off selecting the first n sentences of the document. This “lead” baseline may err on the side of verb"
P10-1058,E06-1038,0,0.0140421,"Missing"
P10-1058,C04-1129,0,0.0139177,"ow readability and text quality 565 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565–574, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics move redundant phrases, and use (manual) recombination rules to produce coherent output. Wan and Paris (2008) segment sentences heuristically into clauses before extraction takes place, and show that this improves summarization quality. In the context of multiple-document summarization, heuristics have also been used to remove parenthetical information (Conroy et al., 2004; Siddharthan et al., 2004). Witten et al. (1999) (among others) extract keyphrases to capture the gist of the document, without however attempting to reconstruct sentences or generate summaries. marization that incorporates compression into the task. A key insight in our approach is to formulate summarization as a phrase rather than sentence extraction problem. Compression falls naturally out of this formulation as only phrases deemed important should appear in the summary. Obviously, our output summaries must meet additional requirements such as sentence length, overall length, topic coverage and, importantly, grammat"
P10-1058,D07-1047,0,0.289026,"s through the use of integer linear programming (ILP), a well-studied optimization framework that is able to search the entire solution space efficiently. We apply our model to the task of generating highlights for a single document. Examples of CNN news articles with human-authored highlights are shown in Table 1. Highlights give a brief overview of the article to allow readers to quickly gather information on stories, and usually appear as bullet points. Importantly, they represent the gist of the entire document and thus often differ substantially from the first n sentences in the article (Svore et al., 2007). They are also highly compressed, written in a telegraphic style and thus provide an excellent testbed for models that generate compressed summaries. Experimental results show that our model’s output is comparable to hand-written highlights both in terms of grammaticality and informativeness. 2 A few previous approaches have attempted to interface sentence compression with summarization. A straightforward way to achieve this is by adopting a two-stage architecture (e.g., Lin 2003) where the sentences are first extracted and then compressed or the other way round. Other work implements a joint"
P10-1058,A00-2024,0,\N,Missing
P10-1126,P00-1041,0,0.196994,"or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation as follows. Given an"
P10-1126,W03-0501,0,0.008615,"manual annotation or background ontological information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation a"
P10-1126,P08-1032,1,0.427614,"to the image keywords and consider only the n-best ones. Alternatively, we could consider the single most relevant sentence together with its surrounding context under the assumption that neighboring sentences are about the same or similar topics. 7 Experimental Setup In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models. Data All our experiments were conducted on the corpus created by Feng and Lapata (2008), following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing). Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. The underlying topic model was trained with 1,000 topics using only content words (i.e., nouns, verbs, and adjectives) that appeared no less than five times"
P10-1126,N10-1125,1,0.549071,"levance to the article, provide context for the picture, and ultimately draw the reader into the article. It is also worth noting that journalists often write their own captions rather than simply extract sentences from the document. In doing so they rely on general world knowledge but also expertise in current affairs that goes beyond what is described in the article or shown in the picture. 4 Image Annotation As mentioned earlier, our approach relies on an image annotation model to provide description keywords for the picture. Our experiments made use of the probabilistic model presented in Feng and Lapata (2010). The latter is well-suited to our task as it has been developed with noisy, multimodal data sets in mind. The model is based on the assumption that images and their surrounding text are generated by mixtures of latent topics which are inferred from a concatenated representation of words and visual features. Specifically, images are preprocessed so that they are represented by word-like units. Local image descriptors are computed using the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). The general idea behind the algorithm is to first sample an image with the difference-of-Ga"
P10-1126,C02-1137,0,0.0287464,"gical information we exploit a multimodal database of news articles, images, and their captions. The latter is admittedly noisy, yet can be easily obtained from on-line sources, and contains rich information about the entities and events depicted in the images and their relations. Similar to previous work, we also follow a two-stage approach. Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. The caption generation task bears some resemblance to headline generation (Dorr et al., 2003; Banko et al., 2000; Jin and Hauptmann, 2002) where the aim is to create a very short summary for a document. Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i.e., the caption should also mention some of the objects or individuals depicted in the image). We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. Our approach thus differs from most work in summarization which is solely text-based. 3 Problem Formulation We formulate image caption generation as follows. Given an image I, and a related kno"
P10-1126,P03-1054,0,0.00216815,"similar topics. 7 Experimental Setup In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. We give details on our training procedure, parameter estimation, and present the baseline methods used for comparison with our models. Data All our experiments were conducted on the corpus created by Feng and Lapata (2008), following their original partition of the data (2,881 image-caption-document tuples for training, 240 tuples for development and 240 for testing). Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. The underlying topic model was trained with 1,000 topics using only content words (i.e., nouns, verbs, and adjectives) that appeared no less than five times in the corpus. For all models discussed here (extractive and abstractive) we report results with the 15 best annotation keywords. For the abstractive models, we used a trigram model trained with the SRI toolkit on a newswire corpus co"
P10-1126,J98-3004,0,0.0362029,"eral framework for generating text descriptions of image and video content based on image parsing. Specifically, images are hierarchically decomposed into their constituent visual patterns which are subsequently converted into a semantic representation using WordNet. The image parser is trained on a corpus, manually annotated with graphs representing image structure. A multi-sentence description is generated using a document planner and a surface realizer. Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al., 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e.g., the author’s goal to convey ostensible increase or decrease of a quantity of interest (Elzer et al., 2005). Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. The task of generating captions for news images is novel to our knowledge. Instead of relying on manual annotation or"
P10-1126,2006.amta-papers.25,0,0.0151678,"size was set to 500 (with at least 50 states for the word-based model). For the phrase-based model, we also experimented with reducing the search scope, either by considering only the n most similar sentences to the keywords (range [2, 10]), or simply the single most similar sentence and its neighbors (range [2, 5]). The former method delivered better results with 10 sentences (and the KL divergence similarity function). Evaluation We evaluated the performance of our models automatically, and also by eliciting human judgments. Our automatic evaluation was based on Translation Edit Rate (TER, Snover et al. 2006), a measure commonly used to evaluate the quality of machine translation output. TER is defined as the minimum number of edits a human would have to perform to change the system output so that it exactly matches a reference translation. In our case, the original captions written by the BBC journalists were used as reference: Ins + Del + Sub + Shft (16) Nr where E is the hypothetical system output, Er the reference caption, and Nr the reference length. The number of possible edits include insertions (Ins), deletions (Del), substitutions (Sub) and shifts (Shft). TER is similar to word error rate"
P10-1158,briscoe-carroll-2002-robust,0,0.108053,"ow the narrative schemas are extracted and plots merged, and then discuss our evolutionary search procedure. Entity-based Schema Extraction Before we can generate a plot for a story we must have an idea of the actions associated with the entities in the story, the order in which these actions are performed and also which other entities can participate. This information is stored in a directed graph which we explain below. Our algorithm processes each document at a time, it operates over dependency structures and assumes that entity mentions have been resolved. In our experiments we used Rasp (Briscoe and Carroll, 2002), a broad coverage dependency parser, and the OpenNLP1 coreference resolution engine.2 However, any dependency parser or coreference tool could serve our 1 See http://opennlp.sourceforge.net/. coreference resolution tool we employ is not error-free and on occasion will fail to resolve a pronoun. We map unresolved pronouns to the generic labels person or object. 2 The purpose. We also assume that the actions associated with a given entity are ordered and that linear order corresponds to temporal order. This is a gross simplification as it is well known that temporal relationships between events"
P10-1158,P08-1090,0,0.0170689,"forge.net/. coreference resolution tool we employ is not error-free and on occasion will fail to resolve a pronoun. We map unresolved pronouns to the generic labels person or object. 2 The purpose. We also assume that the actions associated with a given entity are ordered and that linear order corresponds to temporal order. This is a gross simplification as it is well known that temporal relationships between events are not limited to precedence, they may overlap, occur simultaneously, or be temporally unrelated. We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. For each entity e in the corpus we build a directed graph G = (V, E) whose nodes V denote predicate argument relationships, and edges E represent transitions from node Vi to node V j . As an example of our schema construction process, consider a very small corpus consisting of the two documents shown in Figure 1. The schema for princess after processing the first document is given on the left hand side. Each node in this graph corresponds to an action attested with princess (we also record who performs it and where or how). Nodes are themselves dependenc"
P10-1158,W00-1425,0,0.0329215,"reviously employed in natural language generation, especially in the context of document planning. Structuring a set of facts into a coherent text is effectively a search problem that may lead to combinatorial explosion for large domains. Mellish et al. (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts. Rather than requiring a global optimum to be found, the genetic algorithm selects an order (based on coherence) that is good enough for people to understand. Cheng and Mellish (2000) focus on the interaction of aggregation and text planning and use genetic algorithms to search for the best aggregated document that satisfies coherence constraints. The application of genetic algorithms to story generation is novel to our knowledge. Our work also departs from McIntyre and Lapata (2009) in two important ways. Firstly, our generator does not rely on a knowledge base of seemingly unrelated entities and relations. Rather, we employ 1563 a document planner to create and structure a plot for a story. The planner is built automatically from a training corpus and creates plots dynam"
P10-1158,C94-1042,0,0.0657007,"pairs of nodes within the same graph by looking at intrasentential verb-verb co-occurrences in the training corpus. For example, the nodes (prince have problem, prince keep secret) could become the sentence the prince has a problem keeping a secret. We leave it up to the sentence planner to decide how the two actions should be combined.4 The sentence planner will also insert adverbs and adjectives, using co-occurrence likelihoods acquired from the training corpus. It is essentially a phrase structure grammar compiled from the lexical resources made available by Korhonen and Briscoe (2006) and Grishman et al. (1994). The grammar rules act as templates for combining clauses and filling argument slots. 4 We only turn an action into a subclause if its subject entity is same as that of the previous action. 1565 princess love prince prince slay dragon prince rescue princess prince ask king’s permission  prince marry princess in castle temple  goblin dragon   hold princess in prince rescue princess   prince slay dragon princess love prince  prince marry princess in lair cave castle temple prince rule country  prince ask king’s permission Figure 2: Narrative schema for the entity prince. princess have i"
P10-1158,W02-2111,0,0.019168,"ated from the knowledge base. Unlikely stories are pruned using beam search. In addition, stories are reranked using two scoring functions based on coherence and interest. These are learnt from training data, i.e., stories labeled with numeric values for interest and coherence. Evolutionary search techniques have been previously employed in natural language generation, especially in the context of document planning. Structuring a set of facts into a coherent text is effectively a search problem that may lead to combinatorial explosion for large domains. Mellish et al. (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts. Rather than requiring a global optimum to be found, the genetic algorithm selects an order (based on coherence) that is good enough for people to understand. Cheng and Mellish (2000) focus on the interaction of aggregation and text planning and use genetic algorithms to search for the best aggregated document that satisfies coherence constraints. The application of genetic algorithms to story generation is novel to our knowledge. Our work also departs from McIn"
P10-1158,P95-1034,0,0.0422118,"undant as interesting stories emerge naturally through the operations of crossover and mutation. 5 Surface Realization Once the final generation of the population has been reached, the fittest story is selected for surface realization. The realizer takes each sentence in the story and reformulates it into input compatible with the RealPro (Lavoie and Rambow, 1997) text generation engine. Realpro creates several variants of the same story differing in the choice of determiners, number (singular or plural), and prepositions. A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). Ideally, the realizer should also select an appropriate tense for the sentence. However, we make the simplifying assumption that all sentences are in the present tense. 6 Experimental Setup In this section we present our experimental set-up for assessing the performance of our story generator. We give details on our training corpus, system, parameters (such as the population size for the GA search), the baselines used for comparison, and explain how our system output was evaluated. Corpus The generator was trained on the same corpus used in McIntyre and Lapata (2009), 437 stories from the An"
P10-1158,korhonen-etal-2006-large,0,0.016514,"Missing"
P10-1158,A97-1039,0,0.195815,"ntent of a story is determined by consulting a data-driven knowledge base that records the entities (i.e., nouns) appearing in a corpus and the actions they perform. These are encoded as dependency relations (e.g., subj-verb, verb-obj). In order to promote between-sentence coherence the generator also make use of an action graph that contains action-role pairs and the likelihood of transitioning from one to another. The sentence planner aggregates together entities and their actions into a sentence using phrase structure rules. Finally, surface realization is performed by interfacing RealPro (Lavoie and Rambow, 1997) with a language model. The system searches for the best story overall as well as the best sentences that can be generated from the knowledge base. Unlikely stories are pruned using beam search. In addition, stories are reranked using two scoring functions based on coherence and interest. These are learnt from training data, i.e., stories labeled with numeric values for interest and coherence. Evolutionary search techniques have been previously employed in natural language generation, especially in the context of document planning. Structuring a set of facts into a coherent text is effectively"
P10-1158,P98-2127,0,0.0231447,"transitions from node Vi to node V j . As an example of our schema construction process, consider a very small corpus consisting of the two documents shown in Figure 1. The schema for princess after processing the first document is given on the left hand side. Each node in this graph corresponds to an action attested with princess (we also record who performs it and where or how). Nodes are themselves dependency trees (see Figure 4a), but are linearized in the figure for the sake of brevity. Edges in the graph indicate ordering and are weighted using the mutual information metric proposed in Lin (1998) (the weights are omitted from the example).3 The first sentence in the text gives rise to the first node in the graph, the second sentence to the second node, and so on. Note that the third sentence is not present in the graph as it is not about the princess. When processing the second document, we simply expand this graph. Before inserting a new node, we check if it can be merged with an already existing one. Nodes are merged only if they have the same verb and similar arguments, with the focal entity (i.e., princess) appearing in the same argument slot. In our example, the nodes “prince mar"
P10-1158,P09-1025,1,0.0649134,"police, agent, authority, government}. Their approach relies on the intuition that in a coherent text events that are about the same participants are 1562 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1562–1572, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics likely to be part of the same story or narrative. Their model extracts narrative chains, essentially events that share argument slots and merges them into schemas. The latter could be used to construct or enrich the knowledge base of a story generator. In McIntyre and Lapata (2009) we presented a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement. The generator operates over predicateargument and predicate-predicate co-occurrence tuples gathered from training data. These are used to produce a large set of candidate stories which are subsequently ranked based on their interestingness and coherence. The approach is unusual in that it does not involve an explicit story planning component. Stories are created stochastically by selecting entities and the events they are most frequently attested with. In this work we de"
P10-1158,W98-1411,0,0.0849046,"Missing"
P10-1158,P94-1019,0,0.0080632,"we check if it can be merged with an already existing one. Nodes are merged only if they have the same verb and similar arguments, with the focal entity (i.e., princess) appearing in the same argument slot. In our example, the nodes “prince marry princess in castle” and “prince marry princess in temple” can be merged as they contain the same verb and number of similar arguments. The nodes “princess have influence” and “princess have baby” cannot be merged as influence and baby are semantically unrelated. We compute argument similarity using WordNet (Fellbaum, 1998) and the measure proposed by Wu and Palmer (1994) which is based on path length. We merge nodes with related arguments only if their similarity exceeds a threshold (determined empirically). 3 We use mutual information to identify event sequences strongly associated with the graph entity. 1564 The dragon holds the princess in a cave. The prince slays the dragon. The princess loves the prince. The prince asks the king’s permission. The prince marries the princess in the temple. The princess has a baby. The goblin holds the princess in a lair. The prince rescues the princess and marries her in a castle. The ceremony is beautiful. The princess h"
P10-1158,J08-1001,1,\N,Missing
P10-1158,P09-1068,0,\N,Missing
P10-1158,C98-2122,0,\N,Missing
P11-1112,D09-1002,1,0.650269,"Missing"
P11-1112,J02-3001,0,0.793883,"applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize seRecent years have seen increased interest in the shallow semantic analysis of natural language text. The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic configurations. In the example sentences below. window occupies different syntactic positions — it is the object of broke in sentences (1a,b), and the subject in (1c) — while bearing the same semantic role, i.e., the 1 More precisely, A0 and A1 have a common interpretation physical object affected by the breaking event. Anal- across predicates as proto-agent and proto-patient in the sense ogously, rock is the instrument of break both when of Dowty (1991). 1117 Proceedings"
P11-1112,P07-1025,0,0.0418967,"and demonstrate improvements over competitive unsupervised methods by a wide margin. 2 Related Work As mentioned earlier, much previous work has focused on building supervised SRL systems (M`arquez et al., 2008). A few semi-supervised approaches have been developed within a framework known as annotation projection. The idea is to combine labeled and unlabeled data by projecting annotations from a labeled source sentence onto an unlabeled target sentence within the same language (F¨urstenau and Lapata, 2009) or across different languages (Pad´o and Lapata, 2009). Outwith annotation projection, Gordon and Swanson (2007) attempt to increase the coverage of PropBank by leveraging existing labeled data. Rather than annotating new sentences that contain previously unseen verbs, they find syntactically similar verbs and use their annotations as surrogate training data. Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Their method is unsupervised in that it starts with a dataset containing no role annotations at all. However, it requires significant human effort as it make"
P11-1112,W06-1601,0,0.664766,"lying only on part of speech annotations, without, however, assigning semantic roles. In contrast, Lang and Lapata (2010) focus solely on the role induction problem which they formulate as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that makes use of the close relationship between syntactic functions and semantic roles. Grenager and Manning (2006) propose a directed graphical model which relates a verb, its semantic roles, and their possible syntactic realizations. Latent variables represent the semantic roles of arguments and role induction corresponds to inferring the state of these latent variables. Our own work also follows the unsupervised learning paradigm. We formulate the induction of semantic roles as a clustering problem and propose a split-merge algorithm which iteratively manipulates clusters representing semantic roles. The motivation behind our approach was to design a conceptually simple system, that allows for the incor"
P11-1112,N10-1137,1,0.741468,"that it starts with a dataset containing no role annotations at all. However, it requires significant human effort as it makes use of VerbNet (Kipper et al., 2000) in order to identify the arguments of predicates and make initial role assignments. VerbNet is a broad coverage lexicon organized into verb classes each of which is explicitly associated with argument realization and semantic role specifications. Abend et al. (2009) propose an algorithm that identifies the arguments of predicates by relying only on part of speech annotations, without, however, assigning semantic roles. In contrast, Lang and Lapata (2010) focus solely on the role induction problem which they formulate as the process of detecting alternations and finding a canonical syntactic form for them. Verbal arguments are then assigned roles, according to their position in this canonical form, since each position references a specific role. Their model extends the logistic classifier with hidden variables and is trained in a manner that makes use of the close relationship between syntactic functions and semantic roles. Grenager and Manning (2006) propose a directed graphical model which relates a verb, its semantic roles, and their possib"
P11-1112,J08-2001,0,0.177245,"Missing"
P11-1112,J05-1004,0,0.641647,"istic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin. 1 Introduction realized as a prepositional phrase in (1a) and as a subject in (1b). (1) a. b. c. [Joe]A0 broke the [window]A1 with a [rock]A2 . The [rock]A2 broke the [window]A1 . The [window]A1 broke. The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. Under the PropBank annotation framework (which we will assume throughout this paper) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language underst"
P11-1112,J08-2006,0,0.728798,"Missing"
P11-1112,D07-1002,1,0.890233,"ore roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize seRecent years have seen increased interest in the shallow semantic analysis of natural language text. The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the relations that hold between a predicate and its arguments, abstr"
P11-1112,P03-1002,0,0.208212,"r) each predicate is associated with a set of core roles (named A0, A1, A2, and so on) whose interpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize seRecent years have seen increased interest in the shallow semantic analysis of natural language text. The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the relations that ho"
P11-1112,N09-2004,0,0.144701,"terpretations are specific to that predicate1 and a set of adjunct roles (e.g., location or time) whose interpretation is common across predicates. This type of semantic analysis is admittedly shallow but relatively straightforward to automate and useful for the development of broad coverage, domain-independent language understanding systems. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). Since both argument identification and labeling can be readily modeled as classification tasks, most state-of-the-art systems to date conceptualize seRecent years have seen increased interest in the shallow semantic analysis of natural language text. The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Semantic roles describe the relations that hold between a predicate and its arguments, abstracting over surface syntactic configurations"
P11-1112,W04-3213,0,\N,Missing
P11-1112,W08-2121,0,\N,Missing
P11-1112,N07-1070,0,\N,Missing
P11-1112,P09-1004,0,\N,Missing
P12-1039,D10-1049,0,0.187718,"antly, in this framework non-local features are computed at all internal hypergraph nodes, allowing the decoder to take advantage of them continuously at all stages of the generation process. We incorporate features that are local with respect to a span of a sub-derivation in the packed forest; we also (approximately) include features that arbitrarily exceed span boundaries, thus capturing more global knowledge. Experimental results on the ATIS domain (Dahl et al., 1994) demonstrate that our model outperforms a baseline based on the best derivation and a stateof-the-art discriminative system (Angeli et al., 2010) by a wide margin. Our contributions in this paper are threefold: we recast concept-to-text generation in a probabilistic parsing framework that allows to jointly optimize content selection and surface realization; we represent parse derivations compactly using hypergraphs and illustrate the use of an algorithm for generating (rather than parsing) in this framework; finally, the application of discriminative reranking to conceptto-text generation is novel to our knowledge and as our experiments show beneficial. 2 Related Work Early discriminative approaches to text generation were introduced i"
P12-1039,W05-0909,0,0.019038,"cal features, hence the use of k-best derivation lists.5 We compared our model to Angeli et al. (2010) whose approach is closest to ours.6 We evaluated system output automatically, using the BLEU-4 modified precision score (Papineni et 5 Since the addition of these features, essentially incurs reranking, it follows that the systems would exhibit the exact same performance as the baseline system with 1-best lists. 6 We are grateful to Gabor Angeli for providing us with the code of his system. 375 al., 2002) with the human-written text as reference. We also report results with the METEOR score (Banerjee and Lavie, 2005), which takes into account word re-ordering and has been shown to correlate better with human judgments at the sentence level. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization (see Figure 3) and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?) and semantic correctness (does the meaning conveyed by the text correspond to the database input?). The subjects used a five point rating scale where a high number indicates better perf"
P12-1039,P05-1022,0,0.0670049,"a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such as syntactic parsing (Charniak and Johnson, 2005; Huang, 2008), machine translation (Shen et al., 2004; Li and Khudanpur, 2009) and semantic parsing (Ge and Mooney, 2006). Our model is closest to Huang (2008) who also performs forest reranking on a hypergraph, using both local and non-local features, whose weights are tuned with the averaged perceptron algorithm (Collins, 2002). We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al. (2010), our model optimizes content selection and surface realization simultaneously, rather than as a sequenc"
P12-1039,J07-2003,0,0.202914,"F). Note, that in order to estimate the trigram feature at the FS node, we need to carry word information in the derivations of its antecedents, as we go bottom-up.2 Given these two types of features, we can then adapt Huang’s (2008) approximate decoding algoˆ Essentially, we perform bottomˆ h). rithm to find (w, up Viterbi search, visiting the nodes in reverse topological order, and keeping the k-best derivations for each. The score of each derivation is a linear combination of local and non-local features weights. In machine translation, a decoder that implements forest rescoring (Huang and Chiang, 2007) uses the language model as an external criterion of the goodness of sub-translations on account of their grammaticality. Analogously here, non-local features influence the selection of the best combinations, by introducing knowledge that exceeds the confines of the node under consideration and thus depend on the sub-derivations generated so far. (e.g., word trigrams spanning a field node rely on evidence from antecedent nodes that may be arbitrarily deeper than the field’s immediate children). Our treatment of leaf nodes (see rules (8) and (9)) differs from the way these are usually handled i"
P12-1039,W02-1001,0,0.0970964,"ation component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such as syntactic parsing (Charniak and Johnson, 2005; Huang, 2008), machine translation (Shen et al., 2004; Li and Khudanpur, 2009) and semantic parsing (Ge and Mooney, 2006). Our model is closest to Huang (2008) who also performs forest reranking on a hypergraph, using both local and non-local features, whose weights are tuned with the averaged perceptron algorithm (Collins, 2002). We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al. (2010), our model optimizes content selection and surface realization simultaneously, rather than as a sequence. The discriminative aspect of two models is also fundamentally different. We have a single reranking component that applies throughout, whereas they train different discriminative models for each local decision. 1. S → R(start) 2. R(ri .t) → FS(r j , start) R(r j .t) [P(r j .t |ri .t) · λ] 3 3. R(ri .t) → FS(r j , start) [P(r j"
P12-1039,H94-1010,0,0.291299,"encodes exponentially many derivations, we can explore a much larger hypothesis space than would have been possible with an n-best list. Importantly, in this framework non-local features are computed at all internal hypergraph nodes, allowing the decoder to take advantage of them continuously at all stages of the generation process. We incorporate features that are local with respect to a span of a sub-derivation in the packed forest; we also (approximately) include features that arbitrarily exceed span boundaries, thus capturing more global knowledge. Experimental results on the ATIS domain (Dahl et al., 1994) demonstrate that our model outperforms a baseline based on the best derivation and a stateof-the-art discriminative system (Angeli et al., 2010) by a wide margin. Our contributions in this paper are threefold: we recast concept-to-text generation in a probabilistic parsing framework that allows to jointly optimize content selection and surface realization; we represent parse derivations compactly using hypergraphs and illustrate the use of an algorithm for generating (rather than parsing) in this framework; finally, the application of discriminative reranking to conceptto-text generation is n"
P12-1039,P06-2034,0,0.0608386,"s of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such as syntactic parsing (Charniak and Johnson, 2005; Huang, 2008), machine translation (Shen et al., 2004; Li and Khudanpur, 2009) and semantic parsing (Ge and Mooney, 2006). Our model is closest to Huang (2008) who also performs forest reranking on a hypergraph, using both local and non-local features, whose weights are tuned with the averaged perceptron algorithm (Collins, 2002). We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al. (2010), our model optimizes content selection and surface realization simultaneously, rather than as a sequence. The discriminative aspect of two models is also fundamentally different. We have a single reranking component that appl"
P12-1039,W06-1417,0,0.0111444,"ion and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study. 1 Introduction Concept-to-text generation broadly refers to the task of automatically producing textual output from non-linguistic input such as databases of records, logical form, and expert system knowledge bases (Reiter and Dale, 2000). A variety of concept-totext generation systems have been engineered over the years, with considerable success (e.g., Dale et al. (2003), Reiter et al. (2005), Green (2006), Turner et al. (2009)). Unfortunately, it is often difficult to adapt them across different domains as they rely mostly on handcrafted components. Following a generative approach, we could first learn the weights of the PCFG by maximising the joint likelihood of the model and then perform generation by finding the best derivation tree in the hypergraph. The performance of this baseline system could be potentially further improved using discriminative reranking (Collins, 2000). Typically, this method first creates a list of n-best candidates from a generative model, and then reranks them with"
P12-1039,P07-1019,0,0.014614,"l fields (F). Note, that in order to estimate the trigram feature at the FS node, we need to carry word information in the derivations of its antecedents, as we go bottom-up.2 Given these two types of features, we can then adapt Huang’s (2008) approximate decoding algoˆ Essentially, we perform bottomˆ h). rithm to find (w, up Viterbi search, visiting the nodes in reverse topological order, and keeping the k-best derivations for each. The score of each derivation is a linear combination of local and non-local features weights. In machine translation, a decoder that implements forest rescoring (Huang and Chiang, 2007) uses the language model as an external criterion of the goodness of sub-translations on account of their grammaticality. Analogously here, non-local features influence the selection of the best combinations, by introducing knowledge that exceeds the confines of the node under consideration and thus depend on the sub-derivations generated so far. (e.g., word trigrams spanning a field node rely on evidence from antecedent nodes that may be arbitrarily deeper than the field’s immediate children). Our treatment of leaf nodes (see rules (8) and (9)) differs from the way these are usually handled i"
P12-1039,P08-1067,0,0.0873507,"from to denver boston number dep/ar 9 departure month dep/ar august departure arg1 arg2 type arrival time 1600 < type what query flight λx. f light(x) ∧ f rom(x, denver) ∧ to(x, boston) ∧ day number(x, 9) ∧ month(x, august)∧ less than(arrival time(x), 1600) Give me the flights leaving Denver August ninth coming back to Boston before 4pm. Figure 1: Example of non-linguistic input as a structured database and logical form and its corresponding text. We omit record fields that have no value, for the sake of brevity. baseline system. An appealing alternative is to rerank the hypergraph directly (Huang, 2008). As it compactly encodes exponentially many derivations, we can explore a much larger hypothesis space than would have been possible with an n-best list. Importantly, in this framework non-local features are computed at all internal hypergraph nodes, allowing the decoder to take advantage of them continuously at all stages of the generation process. We incorporate features that are local with respect to a span of a sub-derivation in the packed forest; we also (approximately) include features that arbitrarily exceed span boundaries, thus capturing more global knowledge. Experimental results on"
P12-1039,W01-1812,0,0.0400883,"ds whose type is integer. Function g( f .v) generates an integer number given the field value, using either of the following six ways (Liang et al., 2009): identical to the field value, rounding up or rounding down to a multiple of 5, rounding off to the closest multiple of 5 and finally adding or subtracting some unexplained noise.1 The weight is a multinomial over the six generation function modes, given the record field f . The CFG in Table 1 will produce many derivations for a given input (i.e., a set of database records) which we represent compactly using a hypergraph or a packed forest (Klein and Manning, 2001; Huang, 2008). Simplified examples of this representation are shown in Figure 2. 3.2 Hypergraph Reranking For our generation task, we are given a set of database records d, and our goal is to find the best corresponding text w. This corresponds to the best grammar derivation among a set of candidate derivations represented implicitly in the hypergraph structure. As shown in Table 1, the mapping from d to w is unknown. Therefore, all the intermediate multinomial distributions, described in the previous section, define a hidden correspondence structure h, between records, fields, and their valu"
P12-1039,N12-1093,1,0.649676,"rds. Baseline Feature This is the log score of a generative decoder trained on the PCFG from Table 1. We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). Decoding was performed approx4 The resulting dataset and a technical report describing the mapping procedure in detail are available from http://homepages.inf.ed.ac.uk/s0793019/index.php? page=resources 374 imately via cube pruning (Chiang, 2007), by integrating a trigram language model extracted from the training set (see Konstas and Lapata (2012) for details). Intuitively, the feature refers to the overall goodness of a specific derivation, applied locally in every hyperedge. Alignment Features Instances of this feature family refer to the count of each PCFG rule from Table 1. For example, the number of times rule R(search1 .t) → FS( f light1 , start)R( f light1 .t) is included in a derivation (see Figure 2(a)) Lexical Features These features encourage grammatical coherence and inform lexical selection over and above the limited horizon of the language model captured by Rules (6)–(9). They also tackle anomalies in the generated output"
P12-1039,D09-1005,0,0.0249437,"s (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. 4.2 Features Broadly speaking, we defined two types of features, namely lexical and structural ones. In addition, we used a generatively trained PCFG as a baseline feature and an alignment feature based on the cooccurrence of records (or fields) with words. Baseline Feature This is the log score of a generative decoder trained on the PCFG from Table 1. We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). Decoding was performed approx4 The resulting dataset and a technical report describing the mapping procedure in detail are available from http://homepages.inf.ed.ac.uk/s0793019/index.php? page=resources 374 imately via cube pruning (Chiang, 2007), by integrating a trigram language model extracted from the training set (see Konstas and Lapata (2012) for details). Intuitively, the feature refers to the overall goodness of a specific derivation, applied locally in every hyperedge. Alignment Features Instances of this feature family refer to the count of each PCFG rule from Table 1. For example,"
P12-1039,P06-1096,0,0.0626103,"Missing"
P12-1039,P09-1011,0,0.138535,"ion task. Local and non-local information (e.g., word n-grams, long370 range dependencies) was taken into account with the use of features in a maximum entropy probability model. More recently, Wong and Mooney (2007) describe an approach to surface realization based on synchronous context-free grammars. The latter are learned using a log-linear model with minimum error rate training (Och, 2003). Angeli et al. (2010) were the first to propose a unified approach to content selection and surface realization. Their model operates over automatically induced alignments of words to database records (Liang et al., 2009) and decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such a"
P12-1039,P03-1021,0,0.0199151,"ed in spoken dialogue systems, and usually tackled content selection and surface realization separately. Ratnaparkhi (2002) conceptualized surface realization (from a fixed meaning representation) as a classification task. Local and non-local information (e.g., word n-grams, long370 range dependencies) was taken into account with the use of features in a maximum entropy probability model. More recently, Wong and Mooney (2007) describe an approach to surface realization based on synchronous context-free grammars. The latter are learned using a log-linear model with minimum error rate training (Och, 2003). Angeli et al. (2010) were the first to propose a unified approach to content selection and surface realization. Their model operates over automatically induced alignments of words to database records (Liang et al., 2009) and decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization compone"
P12-1039,P02-1040,0,0.0942865,"Missing"
P12-1039,N04-1023,0,0.200359,"Missing"
P12-1039,W09-0607,0,0.0148359,"ting in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study. 1 Introduction Concept-to-text generation broadly refers to the task of automatically producing textual output from non-linguistic input such as databases of records, logical form, and expert system knowledge bases (Reiter and Dale, 2000). A variety of concept-totext generation systems have been engineered over the years, with considerable success (e.g., Dale et al. (2003), Reiter et al. (2005), Green (2006), Turner et al. (2009)). Unfortunately, it is often difficult to adapt them across different domains as they rely mostly on handcrafted components. Following a generative approach, we could first learn the weights of the PCFG by maximising the joint likelihood of the model and then perform generation by finding the best derivation tree in the hypergraph. The performance of this baseline system could be potentially further improved using discriminative reranking (Collins, 2000). Typically, this method first creates a list of n-best candidates from a generative model, and then reranks them with arbitrary features (bo"
P12-1039,N07-1022,0,0.0984533,"minative reranking to conceptto-text generation is novel to our knowledge and as our experiments show beneficial. 2 Related Work Early discriminative approaches to text generation were introduced in spoken dialogue systems, and usually tackled content selection and surface realization separately. Ratnaparkhi (2002) conceptualized surface realization (from a fixed meaning representation) as a classification task. Local and non-local information (e.g., word n-grams, long370 range dependencies) was taken into account with the use of features in a maximum entropy probability model. More recently, Wong and Mooney (2007) describe an approach to surface realization based on synchronous context-free grammars. The latter are learned using a log-linear model with minimum error rate training (Och, 2003). Angeli et al. (2010) were the first to propose a unified approach to content selection and surface realization. Their model operates over automatically induced alignments of words to database records (Liang et al., 2009) and decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which"
P12-1039,D07-1071,0,0.0185589,"and the second argument denotes the value. We also defined special record types, such as condition and search. The latter is introduced for every lambda operator and assigned the categorical field what with the value flight which refers to the record type of variable x. Contrary to datasets used in previous generation studies (e.g., ROBO C UP (Chen and Mooney, 2008) and W EATHER G OV (Liang et al., 2009)), ATIS has a much richer vocabulary (927 words); each scenario corresponds to a single sentence (average length is 11.2 words) with 2.65 out of 19 record types mentioned on average. Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. 4.2 Features Broadly speaking, we defined two types of features, namely lexical and structural ones. In addition, we used a generatively trained PCFG as a baseline feature and an alignment feature based on the cooccurrence of records (or fields) with words. Baseline Feature This is the log score of a generative decoder trained on the PCFG from Table 1. We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Li and E"
P12-1039,N10-1069,0,\N,Missing
P12-1054,C10-1034,0,0.0488041,"ly, ranking strategies have become extremely important for retrieving information quickly. Many websites currently offer a real-time search service which returns ranked lists of Twitter posts or shared links according to user queries. Ranking methods used by these sites employ three criteria, namely recency, popularity and content relevance (Dong et al., 2010). State-of-art tweet retrieval methods include a linear regression model biased towards text quality with a regularization factor inspired by the hypothesis that documents similar in content may have similar quality (Huang et al., 2011). Duan et al. (2010) learn a ranking model using SVMs and features based on tweet content, the relations among users, and tweet specific characteristics (e.g., urls, number of retweets). Tweet Recommendation Previous work has also focused on tweet recommendation systems, assuming no explicit query is provided by the users. Collaborative filtering is perhaps the most obvious method for recommending tweets (Hannon et al., 2010). Chen et al. (2010) investigate how to select interesting URLs linked from Twitter and recommend the top ranked ones to users. Their recommender takes three dimensions into account: the sour"
P12-1054,I11-1042,0,0.0740539,"eets being posted daily, ranking strategies have become extremely important for retrieving information quickly. Many websites currently offer a real-time search service which returns ranked lists of Twitter posts or shared links according to user queries. Ranking methods used by these sites employ three criteria, namely recency, popularity and content relevance (Dong et al., 2010). State-of-art tweet retrieval methods include a linear regression model biased towards text quality with a regularization factor inspired by the hypothesis that documents similar in content may have similar quality (Huang et al., 2011). Duan et al. (2010) learn a ranking model using SVMs and features based on tweet content, the relations among users, and tweet specific characteristics (e.g., urls, number of retweets). Tweet Recommendation Previous work has also focused on tweet recommendation systems, assuming no explicit query is provided by the users. Collaborative filtering is perhaps the most obvious method for recommending tweets (Hannon et al., 2010). Chen et al. (2010) investigate how to select interesting URLs linked from Twitter and recommend the top ranked ones to users. Their recommender takes three dimensions in"
P12-1054,P10-1056,0,0.0119906,"large dataset consisting of 9,449,542 users, 364,287,744 tweets, 596,777,491 links, and 55,526,494 retweets. The crawler monitored the data from 3/25/2011 to 5/30/2011. We used approximately one month of this data for training and the rest for testing. 521 Before building the graphs (i.e., the tweet graph, the author graph, and the tweet-author graph), the dataset was preprocessed as follows. We removed tweets of low linguistic quality and subsequently discarded users without any linkage to the remaining tweets. We measured linguistic quality following the evaluation framework put forward in Pitler et al. (2010). For instance, we measured the out-ofvocabulary word ratio (as a way of gauging spelling errors), entity coherence, fluency, and so on. We further removed stopwords and performed stemming. Parameter Settings We ran LDA with 500 iterations of Gibbs sampling. The number of topics n was set to 100 which upon inspection seemed generally coherent and meaningful. We set the damping factor µ to 0.15 following the standard PageRank paradigm. We opted for more or less generic parameter values as we did not want to tune our framework to the specific dataset at hand. We examined the parameter λ which co"
P12-1054,P10-1094,0,0.00957457,"sumed that all nodes in the matrix M are equi-probable before the walk. In contrast, we use the topic preference vector as a prior on M. Let Diag(r) denote a diagonal matrix whose eigenvalue is vector r. Then m becomes: m = (1 − µ)[Diag(r)M]T m + µr = (1 − µ)[Diag(tDT )M]T m + µtDT (4) Diversity We would also like our output to be diverse without redundant information. Unfortunately, equation (4) will have the opposite effect, as it assigns high scores to closely connected node communities. A greedy algorithm such as Maximum Marginal Relevance (Carbonell and Goldstein, 1998; Wan et al., 2007; Wan et al., 2010) may achieve diversity by iteratively selecting the most prestigious or popular vertex and then penalizing the vertices “covered” by those that have been already selected. Rather than adopting a greedy vertex selection method, we follow DivRank (Mei et al., 2010) a recently proposed algorithm that balances popularity and diversity in ranking, based on a time-variant random walk. In contrast to PageRank, DivRank assumes that the transition probabilities change over time. Moreover, it is assumed that the transition probability from one state to another is reinforced by the number of previous vis"
P12-1054,D11-1124,1,0.764494,"Missing"
P13-1056,D11-1131,0,0.0198335,"Missing"
P13-1056,P08-1032,1,0.0990214,"Missing"
P13-1056,P09-1010,0,0.0142424,"ortant for representing concepts. Finally, multiple participants are required to create a representation for each conIntroduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded 572 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–582, c Sofia, Bulgaria,"
P13-1056,W11-2503,0,0.0790666,"and visual modalities were fused to create a joint representation. ting each entry less than δ to zero. Figure 1 shows the results of the attribute prediction on the test set on the basis of the computed centroids; specifically, we plot recall against precision based on threshold δ.5 Table 4 shows the 10 nearest neighbors for five example concepts from our dataset. Again, we measure the cosine similarity between a concept and all other concepts in the dataset when these are represented by their visual attribute vector pw . Concatenation Model Variants of this model were originally proposed in Bruni et al. (2011) and Johns and Jones (2012). Let T ∈ RN×D denote a term-attribute co-occurrence matrix, where each cell records a weighted co-occurrence score of a word and a textual attribute. Let P ∈ [0, 1]N×F denote a visual matrix, representing a probability distribution over visual attributes for each word. A word’s meaning can be then represented by the concatenation of its normalized textual and visual vectors. 5 Attribute-based Semantic Models We evaluated the effectiveness of our attribute classifiers by integrating their predictions with traditional text-only models of semantic representation. These"
P13-1056,P12-1015,0,0.730695,"age tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded 572 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–582, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics we show that this attribute-based image representation can be usefully integrated with textual data to create distributional models that give a better fit to human word association data over models that rely on huma"
P13-1056,D12-1130,1,0.740995,"isual attributes. We create a new large-scale taxonomy of visual attributes covering more than 500 concepts and their corresponding 688K images. We use this dataset to train attribute classifiers and integrate their predictions with text-based distributional models of word meaning. We show that these bimodal models give a better fit to human word association data compared to amodal models and word representations based on handcrafted norming data. 1 One strand of research uses feature norms as a stand-in for sensorimotor experience (Johns and Jones, 2012; Andrews et al., 2009; Steyvers, 2010; Silberer and Lapata, 2012). Feature norms are obtained by asking native speakers to write down attributes they consider important in describing the meaning of a word. The attributes represent perceived physical and functional properties associated with the referents of words. For example, apples are typically green or red, round, shiny, smooth, crunchy, tasty, and so on; dogs have four legs and bark, whereas chairs are used for sitting. Feature norms are instrumental in revealing which dimensions of meaning are psychologically salient, however, their use as a proxy for people’s perceptual representations can itself be"
P13-1056,I11-1162,0,0.236714,"ely surprising. Visual input represents a major source of data from which humans can learn semantic representations of linguistic and non-linguistic communicative actions (Regier, 1996). Furthermore, since images are ubiquitous, visual data can be gathered far easier than some of the other modalities. Distributional models that integrate the visual modality have been learned from texts and images (Feng and Lapata, 2010; Bruni et al., 2012b) or from ImageNet (Deng et al., 2009), e.g., by exploiting the fact that images in this database are hierarchically organized according to WordNet synsets (Leong and Mihalcea, 2011). Images are typically represented on the basis of low-level features such as SIFT (Lowe, 2004), whereas texts are treated as bags of words. Our work also focuses on images as a way of physically grounding the meaning of words. We, however, represent them by high-level visual attributes instead of low-level image features. Attributes are not concept or category specific (e.g., animals have stripes and so do clothing items; balls are round, and so are oranges and coins), and thus allow us to express similarities and differences across concepts more easily. Furthermore, attributes allow us to ge"
P13-1056,H89-1033,0,0.316444,"Missing"
P13-1056,D08-1082,0,0.029609,"to each concept. It is not entirely clear how people generate attributes and whether all of these are important for representing concepts. Finally, multiple participants are required to create a representation for each conIntroduction Recent years have seen increased interest in grounded language acquisition, where the goal is to extract representations of the meaning of natural language tied to the physical world. The language grounding problem has assumed several guises in the literature such as semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Lu et al., 2008; B¨orschinger et al., 2011), mapping natural language instructions to executable actions (Branavan et al., 2009; Tellex et al., 2011), associating simplified language to perceptual data such as images or video (Siskind, 2001; Roy and Pentland, 2002; Gorniak and Roy, 2004; Yu and Ballard, 2007), and learning the meaning of words based on linguistic and perceptual input (Bruni et al., 2012b; Feng and Lapata, 2010; Johns and Jones, 2012; Andrews et al., 2009; Silberer and Lapata, 2012). In this paper we are concerned with the latter task, namely constructing perceptually grounded 572 Proceedings"
P13-1056,N10-1011,1,\N,Missing
P14-1068,S07-1002,0,0.00725877,"taset into hard categorizations by assigning each noun to its most typical category as extrapolated from human typicality ratings (for details see Fountain and Lapata, 2010). CW can optionally apply a minimum weight threshold which we optimized using the categorization dataset from Baroni et al. (2010). The latter contains a classification of 82 McRae et al. (2005) nouns into 10 categories. These nouns were excluded from the gold standard (Fountain and Lapata, 2010) in our final evaluation. We evaluated the clusters produced by CW using the F-score measure introduced in the SemEval 2007 task (Agirre and Soroa, 2007); it is the harmonic mean of precision and recall defined as the number of correct members of a cluster divided by the number of items in the cluster and the number of items in the gold-standard class, respectively. Table 2: Mean semantic and visual similarity ratings for the McRae et al. (2005) nouns using a scale of 1 (highly dissimilar) to 5 (highly similar). =0.97, StD =0.11) and for visual similarity 0.63 (Min =0.19, Max =0.90, SD =0.14). These results indicate that the participants found the task relatively straightforward and produced similarity ratings with a reasonable level of consis"
P14-1068,W06-3812,0,0.0164781,"te co-occurrencebased textual representations with visual repreCategorization The task of categorization (i.e., grouping objects into meaningful categories) is a classic problem in the field of cognitive science, central to perception, learning, and the use of language. We evaluated model output against a gold standard set of categories created by Fountain and Lapata (2010). The dataset contains a classification, produced by human participants, of McRae et al.’s (2005) nouns into (possibly multiple) semantic categories (40 in total).6 To obtain a clustering of nouns, we used Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm. In the categorization setting, Chinese Whispers (CW) produces a hard clustering over a weighted graph whose nodes cor6 The dataset can be downloaded from //homepages.inf.ed.ac.uk/s0897549/data/. Comparison with Other Models http: 727 Models McRae Attributes SAE SVD kCCA Bruni RNN-640 Semantic T V T+V 0.71 0.49 0.68 0.58 0.61 0.68 0.65 0.60 0.70 — — 0.67 — — 0.57 — — 0.52 0.41 — — T 0.58 0.46 0.52 — — — 0.34 # 1 2 3 4 5 6 7 8 9 10 Visual V T+V 0.52 0.62 0.56 0.58 0.60 0.64 — 0.57 — 0.55 — 0.46 — — Table 3: Correlation of model predictions against simil"
P14-1068,N10-1011,1,0.624196,"ds that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environment and relate to sensorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. 721 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognitio"
P14-1068,P12-1015,0,0.199917,"pecially those applying stacked autoencoders), it differs considerably from them in at least two aspects. Firstly, most of these approaches aim to learn a shared representation between modalities Related Work The presented model has connections to several lines of work in NLP, computer vision research, and more generally multimodal learning. We review related work in these areas below. Grounded Semantic Spaces Grounded semantic spaces are essentially distributional models augmented with perceptual information. A model akin to Latent Semantic Analysis (Landauer and Dumais, 1997) is proposed in Bruni et al. (2012b) who concatenate two independently constructed textual and visual spaces and subsequently project them onto a lower-dimensional space using Singular Value Decomposition. Several other models have been extensions of Latent Dirichlet Allocation (Blei et al., 2003) where topic distributions are learned from words 722 so as to infer some missing modality from others (e.g., to infer text from images and vice versa); in contrast, we aim to learn an optimal representation for each modality and their optimal combination. Secondly, our problem setting is different from the former studies, which usual"
P14-1068,P13-4032,0,0.0240714,"outperforms all other bimodal models on both similarity tasks. It yields a correlation coefficient of ρ = 0.70 on semantic similarity and ρ = 0.64 on visual similarity. Human agreement on the former task is 0.76 and 0.63 on the latter. Table 4 shows examples of word pairs with highest semantic and visual similarity according to the SAE model. We also observe that simply concatenating textual and visual attributes (Attributes, T+V) performs competitively with SVD and better than kCCA. This indicates that the attribute-based representation is a powerful predictor on its own. Interestingly, both Bruni et al. (2013) and Mikolov et al. (2011) which do not make use of attributes are out-performed by all other attribute-based systems (see columns T and T+V in Table 3). Our results on the categorization task are given in Table 5. In this task, simple concatenation of visual and textual attributes does not yield improved performance over the individual modalities (see row Attributes in Table 5). In contrast, all bimodal models (SVD, kCCA, and SAE) are better than their unimodal equivalents and RNN-640. The SAE outperforms both kCCA and SVD by a large margin delivering clustering performance similar to the McR"
P14-1068,N13-1090,0,0.0466821,"Missing"
P14-1068,W06-2501,0,0.0627431,"Missing"
P14-1068,D13-1115,0,0.308247,"Missing"
P14-1068,P13-1056,1,0.873327,"autoencoders (Bengio et al., 2007) to induce semantic representations integrating visual and textual information. The literature describes several successful approaches to multimodal learning using different variants of deep networks (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012) and data sources including text, images, audio, and video. Unlike most previous work, our model is defined at a finer level of granularity — it computes meaning representations for individual words and is unique in its use of attributes as a means of representing the textual and visual modalities. We follow Silberer et al. (2013) in arguing that an attribute-centric representation is expedient for several reasons. Firstly, attributes provide a natural way of expressing salient properties of word meaning as demonstrated in norming studies (e.g., McRae et al., 2005) where humans often employ attributes when asked to describe a concept. Secondly, from In this paper we address the problem of grounding distributional representations of lexical meaning. We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input. The two modalities are encoded as vectors of attribu"
P14-1068,D12-1130,1,0.724439,"sorimotor experience (Regier, 1996; Landau et al., 1998; Barsalou, 2008). To account for this, new types of perceptually grounded distributional models have emerged. 721 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics and other perceptual units. Feng and Lapata (2010) use visual words which they extract from a corpus of multimodal documents (i.e., BBC news articles and their associated images), whereas others (Steyvers, 2010; Andrews et al., 2009; Silberer and Lapata, 2012) use feature norms obtained in longitudinal elicitation studies (see McRae et al. (2005) for an example) as an approximation of the visual environment. More recently, topic models which combine both feature norms and visual words have also been introduced (Roller and Schulte im Walde, 2013). Drawing inspiration from the successful application of attribute classifiers in object recognition, Silberer et al. (2013) show that automatically predicted visual attributes act as substitutes for feature norms without any critical information loss. The visual and textual modalities on which our model is"
P14-1068,P13-1171,0,0.0143375,"text and images, respectively. We evaluate our model on its ability to simulate similarity judgments and concept categorization. On both tasks, our approach outperforms baselines and related models. 1 Introduction Recent years have seen a surge of interest in single word vector spaces (Turney and Pantel, 2010; Collobert et al., 2011; Mikolov et al., 2013) and their successful use in many natural language applications. Examples include information retrieval (Manning et al., 2008), search query expansions (Jones et al., 2006), document classification (Sebastiani, 2002), and question answering (Yih et al., 2013). Vector spaces have been also popular in cognitive science figuring prominently in simulations of human behavior involving semantic priming, deep dyslexia, text comprehension, synonym selection, and similarity judgments (see Griffiths et al., 2007). In general, these models specify mechanisms for constructing semantic representations from text corpora based on the distributional hypothesis (Harris, 1970): words that appear in similar linguistic contexts are likely to have related meanings. Word meaning, however, is also tied to the physical world. Words are grounded in the external environmen"
P14-1068,D11-1014,0,0.0601814,"ut layer and hidden representations of all the autoencoders are stacked and all network parameters are fine-tuned with backpropagation. To further optimize the parameters of the network, a supervised criterion can be imposed on top of the last hidden layer such as the minimization of a prediction error on a supervised task (Bengio, 2009). Another approach is to unfold the stacked autoencoders and fine-tune them with respect to the minimization of the global reconstruction error (Hinton and Salakhutdinov, 2006). Alternatively, a semi-supervised criterion can be used (Ranzato and Szummer, 2008; Socher et al., 2011) through combination of the unsupervised training criterion (global reconstruction) with a supervised criterion (prediction of some target given the latent representation). Background Our model learns higher-level meaning representations for single words from textual and visual input in a joint fashion. We first briefly review autoencoders in Section 3.1 with emphasis on aspects relevant to our model which we then describe in Section 3.2. Autoencoders An autoencoder is an unsupervised neural network which is trained to reconstruct a given input from its latent representation (Bengio, 2009). It"
P14-1068,P13-2154,0,0.0210803,"Missing"
P14-1068,Q14-1017,0,\N,Missing
P16-1004,S13-1045,0,0.0221039,"Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013"
P16-1004,W10-2903,0,0.087492,"based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks s"
P16-1004,D12-1069,0,0.167243,"Missing"
P16-1004,D10-1119,0,0.0853908,"s include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicon"
P16-1004,W05-0602,0,0.0378387,"their translations into knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semant"
P16-1004,D11-1140,0,0.756642,"ng models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates"
P16-1004,D13-1161,0,0.383307,"Missing"
P16-1004,W14-2405,0,0.0217722,"Missing"
P16-1004,J13-2005,0,0.831853,"ls (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic pars"
P16-1004,D08-1082,0,0.847713,"nto knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers withou"
P16-1004,P15-1002,0,0.49244,"adapt the general encoder-decoder paradigm to the semantic parsing task. Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarchical tree decoder which explicitly captures the compositional structure of logical forms. We also introduce an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015b) allowing the model to learn soft alignments between natural language and logical forms and present an argument identification step to handle rare mentions of entities and numbers. Introduction Semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries. There has recently been a surge of interest in developing machine learning methods for semantic parsing (see the references in Section 2), due in part to the existence of corpora containing utterances annotated with formal meaning representations. Figure 1 shows an example o"
P16-1004,P15-1001,0,0.0241346,"eveloped with question-answering in mind. In the typical application setting, natural language questions are mapped into logical forms and executed on a knowledge base to obtain an answer. Due to the nature of the question-answering task, many natural language utterances contain entities or numbers that are often parsed as arguments in the logical form. Some of them are unavoidably rare or do not appear in the training set at all (this is especially true for small-scale datasets). Conventional sequence encoders simply replace rare words with a special unknown word symbol (Luong et al., 2015a; Jean et al., 2015), which would be detrimental for semantic parsing. We have developed a simple procedure for argument identification. Specifically, we identify entities and numbers in input questions and replace them with their type names and unique IDs. For instance, we pre-process the training example “jobs with a salary of 40000” and its logical form “job(ANS), salary greater than(ANS, 40000, year)” as “jobs with a salary of num0 ” G EO This is a standard semantic parsing benchmark which contains 880 queries to a database of U.S. geography. G EO has 880 instances split into a training set of 680 training ex"
P16-1004,D15-1166,0,0.278376,"adapt the general encoder-decoder paradigm to the semantic parsing task. Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarchical tree decoder which explicitly captures the compositional structure of logical forms. We also introduce an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015b) allowing the model to learn soft alignments between natural language and logical forms and present an argument identification step to handle rare mentions of entities and numbers. Introduction Semantic parsing is the task of translating text to a formal meaning representation such as logical forms or structured queries. There has recently been a surge of interest in developing machine learning methods for semantic parsing (see the references in Section 2), due in part to the existence of corpora containing utterances annotated with formal meaning representations. Figure 1 shows an example o"
P16-1004,D13-1176,0,0.0782393,"red features and is easy to adapt across domains and meaning representations. LSTM what microsoft jobs do not require a bscs? Sequence Sequence/Tree Encoder Decoder answer(J,(compa ny(J,&apos;microsoft&apos;),j ob(J),not((req_de g(J,&apos;bscs&apos;))))) Logical Form Figure 1: Input utterances and their logical forms are encoded and decoded with neural networks. An attention layer is used to learn soft alignments. Encoder-decoder architectures based on recurrent neural networks have been successfully applied to a variety of NLP tasks ranging from syntactic parsing (Vinyals et al., 2015a), to machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), and image description generation (Karpathy and FeiFei, 2015; Vinyals et al., 2015b). As shown in Figure 1, we adapt the general encoder-decoder paradigm to the semantic parsing task. Our model learns from natural language descriptions paired with meaning representations; it encodes sentences and decodes logical forms using recurrent neural networks with long short-term memory (LSTM) units. We present two model variants, the first one treats semantic parsing as a vanilla sequence transduction task, whereas our second model is equipped with a hierarch"
P16-1004,P96-1008,0,0.418122,"airs of questions and their translations into knowledge base queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Oth"
P16-1004,P06-1115,0,0.805764,"neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and d"
P16-1004,P13-1092,0,0.0469775,"Missing"
P16-1004,P07-1121,0,0.439228,"ved significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates"
P16-1004,P15-1085,0,0.623324,"training set of 680 training examples and 200 test examples (Zettlemoyer and Collins, 2005). We used the same meaning representation based on lambda-calculus as Kwiatkowski et al. (2011). Values for the variables city, state, country, river, and number are identified. ATIS This dataset has 5, 410 queries to a flight booking system. The standard split has 4, 480 training instances, 480 development instances, and 450 test instances. Sentences are paired with lambda-calculus expressions. Values for the variables date, time, city, aircraft code, airport, airline, and number are identified. I FTTT Quirk et al. (2015) created this dataset by extracting a large number of if-this-then-that 37 Dataset J OBS G EO ATIS I FTTT Length 9.80 22.90 7.60 19.10 11.10 28.10 6.95 21.80 Example what microsoft jobs do not require a bscs? answer(company(J,’microsoft’),job(J),not((req deg(J,’bscs’)))) what is the population of the state with the largest area? (population:i (argmax $0 (state:t $0) (area:i $0))) dallas to san francisco leaving after 4 in the afternoon please (lambda $0 e (and (>(departure time $0) 1600:ti) (from $0 dallas:ci) (to $0 san francisco:ci))) Turn on heater when temperature drops below 58 degree TRI"
P16-1004,Q14-1030,1,0.504553,"s et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems rely on high-quality lexicons, manually-built templates, and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; S"
P16-1004,D15-1044,0,0.0421264,", and features which are either domain- or representationspecific. We instead present a general method that can be easily adapted to different domains and meaning representations. We adopt the general encoder-decoder framework based on neural networks which has been recently repurposed for various NLP tasks such as syntactic parsing (Vinyals et al., 2015a), machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), image description generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015b), question answering (Hermann et al., 2015), and summarization (Rush et al., 2015). Mei et al. (2016) use a sequence-to-sequence model to map navigational instructions to actions. 3 Problem Formulation Our aim is to learn a model which maps natural language input q = x1 · · · x|q |to a logical form representation of its meaning a = y1 · · · y|a |. The conditional probability p (a|q) is decomposed as: p (a|q) = |a| Y p (yt |y<t , q) (1) t=1 where y<t = y1 · · · yt−1 . Our method consists of an encoder which encodes natural language input q into a vector representation and a decoder which learns to generate y1 , · · · , y|a |conditioned on the encoding vector. In the followin"
P16-1004,W00-1317,0,0.0596882,"learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi a"
P16-1004,D07-1071,0,0.935906,"us modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfrom annotations, e.g., from sentences paired with conversational logs (Artzi and Zettlemoyer, 2011), system demonstrations (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), question-answer pairs (Clarke et al., 2010; Liang et al., 2013), and distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Reddy et al., 2014). Our model learns from natural language descriptions paired with meaning representations. Most previous systems re"
P16-1004,N15-1162,0,0.465285,"e queries, whereas the second model generates the queries conditioned on the learned representations. However, they do not report empirical evaluation results. Our work synthesizes two strands of research, namely semantic parsing and the encoder-decoder architecture with neural networks. The problem of learning semantic parsers has received significant attention, dating back to Woods (1973). Many approaches learn from sentences paired with logical forms following various modeling strategies. Examples include the use of parsing models (Miller et al., 1996; Ge and Mooney, 2005; Lu et al., 2008; Zhao and Huang, 2015), inductive logic programming (Zelle and Mooney, 1996; Tang and Mooney, 2000; Thomspon and Mooney, 2003), probabilistic automata (He and Young, 2006), string/tree-to-tree transformation rules (Kate et al., 2005), classifiers based on string kernels (Kate and Mooney, 2006), machine translation (Wong and Mooney, 2006; Wong and Mooney, 2007; Andreas et al., 2013), and combinatory categorial grammar induction techniques (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Other work learns semantic parsers without relying on logicalfro"
P16-1004,P16-1008,0,0.025337,"curated descriptions are often of low quality, and thus align very loosely to their corresponding ASTs. 4.4 Error Analysis Finally, we inspected the output of our model in order to identify the most common causes of errors which we summarize below. Under-Mapping The attention model used in our experiments does not take the alignment history into consideration. So, some question words, expecially in longer questions, may be ignored in the decoding process. This is a common problem for encoder-decoder models and can be addressed by explicitly modelling the decoding coverage of the source words (Tu et al., 2016; Cohn et al., 2016). Keeping track of the attention history would help adjust future attention and guide the decoder towards untranslated source words. Acknowledgments We would like to thank Luke Zettlemoyer and Tom Kwiatkowski for sharing the ATIS dataset. The support of the European Research Council under award number 681760 “Translating Multiple Modalities into Text” is gratefully acknowledged. References Jacob Andreas, Andreas Vlachos, and Stephen Clark. 2013. Semantic parsing as machine translation. In Proceedings of the 51st ACL, pages 47–52, Sofia, Bulgaria. Argument Identification Som"
P16-1004,N06-1056,0,0.694417,"Missing"
P16-1004,D11-1039,0,\N,Missing
P16-1004,P13-2009,0,\N,Missing
P16-1004,P11-1060,0,\N,Missing
P16-1004,Q13-1005,0,\N,Missing
P16-1046,W13-3214,0,0.0154198,"ons, so six feature maps are used under each kernel width. The blue feature maps have width two and the red feature maps have width three. The sentence embeddings obtained under each kernel width are summed to get the final sentence representation (denoted by green). Document Reader The role of the reader is to derive the meaning representation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-overtime pooling operation (Kalchbrenner and Blunsom, 2013; Zhang and Lapata, 2014; Kim et al., 2016). Next, we build representations for documents using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the acquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below. Recurrent Document Encoder At the document level, a recurrent neural network composes a sequence of sentence vectors into a document vector. Note that t"
P16-1046,P00-1041,0,0.420466,"ims to create a summary from D by selecting a subset of j sentences (where j &lt; m). We do this by scoring each sentence within D and predicting a label yL ∈ {0, 1} indicating whether the sentence should be included in the summary. As we apply supervised training, the objective is to maximize the likelihood of all sentence labels yL = (y1L , · · · , ym L ) given the input document D and model parameters θ: Our work touches on several strands of research within summarization and neural sequence modeling. The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words. Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences. A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous represent"
P16-1046,D14-1181,0,0.00798212,"we try to find a semantically equivalent replacement present in the news article. Specifically, we check if a neighbor, represented 3 The script for constructing our datasets is modified from the one released in Hermann et al. (2015). 486 by pre-trained4 embeddings, is in the original document and therefore constitutes a valid substitution. If we cannot find any substitutes, we discard the document-highlight pair. Following this procedure, we obtained a word extraction dataset containing 170K articles, again from the DailyMail. 4 sentence-level classification tasks such as sentiment analysis (Kim, 2014). Let d denote the dimension of word embeddings, and s a document sentence consisting of a sequence of n words (w1 , · · · , wn ) which can be represented by a dense column matrix W ∈ Rn×d . We apply a temporal narrow convolution between W and a kernel K ∈ Rc×d of width c as follows: Neural Summarization Model The key components of our summarization model include a neural network-based hierarchical document reader and an attention-based hierarchical content extractor. The hierarchical nature of our model reflects the intuition that documents are generated compositionally from words, sentences,"
P16-1046,D15-1232,0,0.10222,"k touches on several strands of research within summarization and neural sequence modeling. The idea of creating a summary by extracting words from the source document was pioneered in Banko et al. (2000) who view summarization as a problem analogous to statistical machine translation and generate headlines using statistical models for selecting and ordering the summary words. Our word-based model is similar in spirit, however, it operates over continuous representations, produces multi-sentence output, and jointly selects summary words and organizes them into sentences. A few recent studies (Kobayashi et al., 2015; Yogatama et al., 2015) perform sentence extraction based on pre-trained sentence embeddings following an unsupervised optimization paradigm. Our work also uses continuous representations to express the meaning of sentences and documents, but importantly employs neural networks more directly to perform the actual summarization task. m log p(yL |D; θ) = ∑ log p(yiL |D; θ) i=1 (1) Although extractive methods yield naturally grammatical summaries and require relatively little linguistic analysis, the selected sentences make for long summaries containing much redundant information. For this reaso"
P16-1046,W04-3247,0,0.325679,"Missing"
P16-1046,W04-1017,0,0.0138319,"one or more documents, whilst preserving their information content. Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document. Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length (Radev et al., 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al., 2006), and event features such as action nouns (Filatova and Hatzivassiloglou, 2004). Sentences are We develop a general framework for singledocument summarization which can be used to extract sentences or words. Our model includes a neural network-based hierarchical document reader or encoder and an attention-based content extractor. The role of the reader is to derive the meaning representation of a document based on its sentences and their constituent words. Our models adopt a variant of neural attention to extract sentences or words. Contrary to previous work where attention is an intermediate step used to blend hidden units of an encoder to a vector propagating additiona"
P16-1046,N03-1020,0,0.372135,"m 20 noise samples. At test time we still loop through the words in the input document (and a stop-word list) to decide which word to output next. System Comparisons We compared the output of our models to various summarization methods. These included the standard baseline of simply selecting the “leading” three sentences from each document as the summary. We also built a sentence extraction baseline classifier using logistic regression and human engineered features. The classifier was trained on the same datasets Evaluation We evaluated the quality of the summaries automatically using ROUGE (Lin and Hovy, 2003). We report unigram and bigram over490 DUC 2002 LEAD LREG ILP NN - ABS TGRAPH URANK NN - SE NN - WE DailyMail L EAD LREG NN - ABS NN - SE NN - WE ROUGE -1 ROUGE -2 ROUGE - L 43.6 21.0 40.2 43.8 20.7 40.3 45.4 21.3 42.8 15.8 5.2 13.8 48.1 24.3 — 48.5 21.5 — 47.4 23.0 43.5 27.0 7.9 22.8 Models LEAD ILP NN - SE NN - WE NN - ABS Human 2nd 0.17 0.38 0.28 0.04 0.01 0.23 3rd 0.37 0.13 0.21 0.03 0.05 0.29 4th 0.15 0.13 0.14 0.21 0.16 0.17 5th 0.16 0.11 0.12 0.51 0.23 0.03 6th MeanR 0.05 3.27 0.06 2.77 0.03 2.74 0.20 4.79 0.54 5.24 0.01 2.51 Table 2: Rankings (shown as proportions) and mean ranks given"
P16-1046,P16-1154,0,0.0421824,"he decoder, our model applies attention directly to select sentences or words of the input document as the output summary. Similar neural attention architectures have been previously used for geometry reasoning (Vinyals et al., 2015), under the name Pointer Networks. 1 Resources are available for download at http:// homepages.inf.ed.ac.uk/s1537177/resources.html 484 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 484–494, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics resentations can be challenging to learn. Gu et al. (2016) and Gulcehre et al. (2016) propose a similar “copy” mechanism in sentence compression and other tasks; their model can accommodate both generation and extraction by selecting which sub-sequences in the input sequence to copy in the output. We evaluate our models both automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing handengineered features and sophisticated linguisti"
P16-1046,P05-3013,0,0.0211964,"Missing"
P16-1046,P16-1014,0,0.0140997,"applies attention directly to select sentences or words of the input document as the output summary. Similar neural attention architectures have been previously used for geometry reasoning (Vinyals et al., 2015), under the name Pointer Networks. 1 Resources are available for download at http:// homepages.inf.ed.ac.uk/s1537177/resources.html 484 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 484–494, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics resentations can be challenging to learn. Gu et al. (2016) and Gulcehre et al. (2016) propose a similar “copy” mechanism in sentence compression and other tasks; their model can accommodate both generation and extraction by selecting which sub-sequences in the input sequence to copy in the output. We evaluate our models both automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing handengineered features and sophisticated linguistic constraints. One stumblin"
P16-1046,P03-1021,0,0.0467268,"to enforce grammaticality due to the lexical diversity and sparsity of the document highlights. A possible enhancement would be to pair the extractor with a neural language model, which can be pretrained on a large amount of unlabeled documents and then jointly tuned with the extractor during decoding (Gulcehre et al., 2015). A simpler alternative which we adopt is to use n-gram features collected from the document to rerank candidate summaries obtained via beam decoding. We incorporate the features in a log-linear reranker whose feature weights are optimized with minimum error rate training (Och, 2003). 5 Implementation Details We trained our models with Adam (Kingma and Ba, 2014) with initial learning rate 0.001. The two momentum parameters were set to 0.99 and 0.999 respectively. We performed mini-batch training with a batch size of 20 documents. All input documents were padded to the same length with an additional mask variable storing the real length for each document. The size of word, sentence, and document embeddings were set to 150, 300, and 750, respectively. For the convolutional sentence model, we followed Kim et al. (2016)8 and used a list of kernel sizes {1, 2, 3, 4, 5, 6, 7}."
P16-1046,D15-1226,0,0.276395,"have shown competitive performance on the DUC2002 single document summarization task. The first approach is the phrase-based extraction model of Woodsend and Lapata (2010). Their system learns to produce highlights from parsed input (phrase structure trees and dependency graphs); it selects salient phrases and recombines them subject to length, coverage, and grammar constraints enforced via integer linear programming (ILP). Like ours, this model is trained on document-highlight pairs, and produces telegraphic-style bullet points rather than full-blown summaries. The other two systems, TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art. TGRAPH is a graph-based sentence extraction model, where the graph is constructed from topic models and the optimization is performed by constrained ILP. URANK adopts a unified ranking system for both single- and multidocument summarization. or unknown (e.g., at test time). Rush et al. (2015) address this issue by adding a new set of features and a log-linear model component to their system. As our model enjoys the advantage of generation by extraction, we can force the model to inspect the context surro"
P16-1046,radev-etal-2004-mead,0,0.0585698,"ntroduction The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. Much effort in automatic summarization has been devoted to sentence extraction, where a summary is created by identifying and subsequently concatenating the most salient text units in a document. Most extractive methods to date identify sentences based on human-engineered features. These include surface features such as sentence position and length (Radev et al., 2004), the words in the title, the presence of proper nouns, content features such as word frequency (Nenkova et al., 2006), and event features such as action nouns (Filatova and Hatzivassiloglou, 2004). Sentences are We develop a general framework for singledocument summarization which can be used to extract sentences or words. Our model includes a neural network-based hierarchical document reader or encoder and an attention-based content extractor. The role of the reader is to derive the meaning representation of a document based on its sentences and their constituent words. Our models adopt a va"
P16-1046,D15-1044,0,0.747454,"usands of document-summary pairs1 . Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation. 1 mlap@inf.ed.ac.uk In this work we propose a data-driven approach to summarization based on neural networks and continuous sentence features. There has been a surge of interest recently in repurposing sequence transduction neural network architectures for NLP tasks such as machine translation (Sutskever et al., 2014), question answering (Hermann et al., 2015), and sentence compression (Rush et al., 2015). Central to these approaches is an encoderdecoder architecture modeled by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2015) is often used to locate the region of focus during decoding. Introduction The need to access and digest large amounts of textual data has provided strong impetus to develop automatic summarization systems aiming to create shorter versions of one or more documents, whilst preserving their information content. M"
P16-1046,D07-1047,0,0.00877072,"automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing handengineered features and sophisticated linguistic constraints. One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization (Woodsend and Lapata, 2010; Svore et al., 2007) and reading comprehension (Hermann et al., 2015) we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in the range of a few h"
P16-1046,C10-1128,0,0.0695557,"e on the DUC2002 single document summarization task. The first approach is the phrase-based extraction model of Woodsend and Lapata (2010). Their system learns to produce highlights from parsed input (phrase structure trees and dependency graphs); it selects salient phrases and recombines them subject to length, coverage, and grammar constraints enforced via integer linear programming (ILP). Like ours, this model is trained on document-highlight pairs, and produces telegraphic-style bullet points rather than full-blown summaries. The other two systems, TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010), produce more typical summaries and represent the state of the art. TGRAPH is a graph-based sentence extraction model, where the graph is constructed from topic models and the optimization is performed by constrained ILP. URANK adopts a unified ranking system for both single- and multidocument summarization. or unknown (e.g., at test time). Rush et al. (2015) address this issue by adding a new set of features and a log-linear model component to their system. As our model enjoys the advantage of generation by extraction, we can force the model to inspect the context surrounding an entity and i"
P16-1046,P10-1058,1,0.896966,"We evaluate our models both automatically (in terms of ROUGE) and by humans on two datasets: the benchmark DUC 2002 document summarization corpus and our own DailyMail news highlights corpus. Experimental results show that our summarizers achieve performance comparable to state-of-the-art systems employing handengineered features and sophisticated linguistic constraints. One stumbling block to applying neural network models to extractive summarization is the lack of training data, i.e., documents with sentences (and words) labeled as summary-worthy. Inspired by previous work on summarization (Woodsend and Lapata, 2010; Svore et al., 2007) and reading comprehension (Hermann et al., 2015) we retrieve hundreds of thousands of news articles and corresponding highlights from the DailyMail website. Highlights usually appear as bullet points giving a brief overview of the information contained in the article (see Figure 1 for an example). Using a number of transformation and scoring algorithms, we are able to match highlights to document content and construct two large scale training datasets, one for sentence extraction and the other for word extraction. Previous approaches have used small scale training data in"
P16-1046,D15-1228,0,0.0645649,"Missing"
P16-1046,D14-1074,1,0.0829445,"d under each kernel width. The blue feature maps have width two and the red feature maps have width three. The sentence embeddings obtained under each kernel width are summed to get the final sentence representation (denoted by green). Document Reader The role of the reader is to derive the meaning representation of the document from its constituent sentences, each of which is treated as a sequence of words. We first obtain representation vectors at the sentence level using a single-layer convolutional neural network (CNN) with a max-overtime pooling operation (Kalchbrenner and Blunsom, 2013; Zhang and Lapata, 2014; Kim et al., 2016). Next, we build representations for documents using a standard recurrent neural network (RNN) that recursively composes sentences. The CNN operates at the word level, leading to the acquisition of sentence-level representations that are then used as inputs to the RNN that acquires document-level representations, in a hierarchical fashion. We describe these two sub-components of the text reader below. Recurrent Document Encoder At the document level, a recurrent neural network composes a sequence of sentence vectors into a document vector. Note that this is a somewhat simpli"
P16-1113,W11-2136,0,0.0533615,"goal of semantic role labeling (SRL) is to identify and label the arguments of semantic predicates in a sentence according to a set of predefined relations (e.g., “who” did “what” to “whom”). Semantic roles provide a layer of abstraction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to argu"
P16-1113,W09-1206,0,0.0259266,"Missing"
P16-1113,C10-3009,0,0.0561063,"Missing"
P16-1113,C10-1011,0,0.0217723,"Missing"
P16-1113,W09-1207,0,0.0285503,"Missing"
P16-1113,D09-1003,0,0.0204523,"Missing"
P16-1113,D15-1112,0,0.594303,"Missing"
P16-1113,S15-1033,0,0.194129,"pectively. 6 Related Work Neural Networks for SRL Collobert et al. (2011) pioneered neural networks for the task of P R F1 83.2 82.4 80.4 75.9 75.1 75.2 79.4 78.6 77.7 P R F1 81.8 81.2 82.1 78.5 78.3 75.4 80.1 79.7 78.6 P R F1 83.1 83.2 78.9 78.0 77.4 74.3 80.5 80.2 76.5 Table 7: Results (in percentage) on the CoNLL2009 test sets for Chinese, German and Spanish. semantic role labeling. They developed a feedforward network that uses a convolution function over windows of words to assign SRL labels. Apart from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model. These approaches all make use of binary features derived from syntactic parses either to indicate constituency boundaries or to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no"
P16-1113,J02-3001,0,0.0697656,"traction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), st"
P16-1113,N15-1121,0,0.606604,"Missing"
P16-1113,D15-1169,0,0.0286861,"edicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions triggered by such phenomena. For instance, con1192 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1192–1202, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj¨orkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)). Despite all systems claiming stateof-the-art or competitive performance, none of them is able to correctly identify He as the agent argument of the predicate raise. Given the complex dependency path relation between the predicate and its argument, none of the systems actually identifies He as an argument at all. In this paper, we develop a new neural network model that can be applied to the task of semantic role labeling. The goal of this model is to better handle control predicates and other phenomena that can be observed from the dependency structure of a sentence. In particular, we aim t"
P16-1113,P15-2047,0,0.027825,"model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a result problems that may r"
P16-1113,P15-2029,0,0.0133036,"d on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a result problems that may result from erroneous parse trees are mitigated. 7 Conclusions We introduced a neural network architecture for semantic role labeling that jointly learns embeddings for dependency paths and feature combinations. Our experimental results indicate that our model substantially increases classification performance, leading to new state-of-the-art results. In a qualitive analysis, we found that"
P16-1113,J13-4006,0,0.043061,"to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all. Our approach falls in between the two extremes: we rely on syntactic parse information but rather than solely making using of sparse binary features, we explicitly model dependency paths in a neural network architecture. Other SRL approaches Within the SRL literature, recent alternatives to neural network architectures include sigmoid belief networks (Henderson et al., 2013) as well as low-rank tensor models (Lei et al., 2015). Whereas Lei et al. only make use of dependency paths as binary indicator features, Henderson et al. propose a joint model for syntactic and semantic parsing that learns and ap1199 plies incremental dependency path representations to perform SRL decisions. The latter form of representation is closest to ours, however, we do not build syntactic parses incrementally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only. Apart from more powerful models, most recent progress in SRL can be attributed to novel"
P16-1113,P82-1020,0,0.651084,"Missing"
P16-1113,J05-1004,0,0.206555,"ldea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time. Examples include control and raising verbs, nested conjunctions or other recursive structures, as well as rare nominal predicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions triggered by such phenomena. For instance, con1192 Proceedings of the 54th Annual Meeting of the As"
P16-1113,P10-1099,0,0.0169377,"Whereas Lei et al. only make use of dependency paths as binary indicator features, Henderson et al. propose a joint model for syntactic and semantic parsing that learns and ap1199 plies incremental dependency path representations to perform SRL decisions. The latter form of representation is closest to ours, however, we do not build syntactic parses incrementally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only. Apart from more powerful models, most recent progress in SRL can be attributed to novel features. For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel w"
P16-1113,C08-1050,0,0.100724,"et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time. Examples include control and raising verbs, nested conjunctions or other recursive structures, as well as rare nominal predicates. The diffic"
P16-1113,P14-2095,0,0.0292695,"Missing"
P16-1113,W05-0634,0,0.0866458,"es are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infr"
P16-1113,J08-2005,0,0.789326,"de range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and Nugues, 2008). Most semantic role labeling approaches to date rely heavily on lexical and syntactic indicator features. Through the availability of large annotated resources, such as PropBank (Palmer et al., 2005), statistical models based on such features achieve high accuracy. However, results often fall short when the input to be labeled involves instances of linguistic phenomena that are relevant for the labeling decision but appear infrequently at training time."
P16-1113,Q15-1032,1,0.825178,"tally. Instead, we take syntactically preprocessed text as input and focus on the SRL task only. Apart from more powerful models, most recent progress in SRL can be attributed to novel features. For instance, Deschacht and Moens (2009) and Huang and Yates (2010) use latent variables, learned with a hidden markov model, as features for representing words and word sequences. Zapirain et al. (2013) propose different selection preference models in order to deal with the sparseness of lexical features. Roth and Woodsend (2014) address the same problem with word embeddings and compositions thereof. Roth and Lapata (2015) recently introduced features that model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentime"
P16-1113,D14-1045,1,0.86505,"onjunctions or other recursive structures, as well as rare nominal predicates. The difficulty lies in that simple lexical and syntactic indicator features are not able to model interactions triggered by such phenomena. For instance, con1192 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1192–1202, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sider the sentence He had trouble raising funds and the analyses provided by four publicly available tools in Table 1 (mate-tools, Bj¨orkelund et al. (2010); mateplus, Roth and Woodsend (2014); TensorSRL, Lei et al. (2015); and easySRL, Lewis et al. (2015)). Despite all systems claiming stateof-the-art or competitive performance, none of them is able to correctly identify He as the agent argument of the predicate raise. Given the complex dependency path relation between the predicate and its argument, none of the systems actually identifies He as an argument at all. In this paper, we develop a new neural network model that can be applied to the task of semantic role labeling. The goal of this model is to better handle control predicates and other phenomena that can be observed from"
P16-1113,P15-1109,0,0.203621,"art from constituency boundaries, their system does not make use of any syntactic information. Foland and Martin (2015) extended their model and showcased significant improvements when including binary indicator features for dependency paths. Similar features were used by FitzGerald et al. (2015), who include role labeling predictions by neural networks as factors in a global model. These approaches all make use of binary features derived from syntactic parses either to indicate constituency boundaries or to represent full dependency paths. An extreme alternative has been recently proposed in Zhou and Xu (2015), who model SRL decisions with a multi-layered LSTM network that takes word sequences as input but no syntactic parse information at all. Our approach falls in between the two extremes: we rely on syntactic parse information but rather than solely making using of sparse binary features, we explicitly model dependency paths in a neural network architecture. Other SRL approaches Within the SRL literature, recent alternatives to neural network architectures include sigmoid belief networks (Henderson et al., 2013) as well as low-rank tensor models (Lei et al., 2015). Whereas Lei et al. only make u"
P16-1113,Q13-1019,0,0.0083313,"ument identification, argument classification, and re-ranking. The neural-network components introduced in Section 2 are used in the last three steps. The following sub-sections describe all components in more detail. 3.1 Predicate Identification and Disambiguation Given a syntactically analyzed sentence, the first two steps in an end-to-end SRL system are to identify and disambiguate the semantic predicates in the sentence. Here, we focus on verbal and nominal predicates but note that other syntactic categories have also been construed as predicates in the NLP literature (e.g., prepositions; Srikumar and Roth (2013)). For both identification and disambiguation steps, we apply the same logistic reraise.01 1st best arg 2nd best arg ARG? ARG? best label 2nd best label best A0 A1 A0 raise.01 raise.01 R ERANKER he funds funds A0 A1 score raise.01 funds score best overall scoring structure step n and formalize the state of the hidden layer h and softmax output sc for each class category c as follows: h = max(0, WBh B + Weh en + bh ) sense P REDICATE class s label c h He had trouble raising funds. O UTPUT HeA0 had trouble raising fundsA1 . Figure 4: Pipeline architecture of our SRL system. gression classifiers"
P16-1113,P15-1150,0,0.0109569,"Missing"
P16-1113,J08-2002,0,0.0292913,"CATION ... o1 e1 en Vector of binary indicator features B (2) (3) D ISAMBIGUATION A RGUMENT (4) I DENTIFICATION Figure 3: Neural model for joint learning of path embeddings and higher-order features: The path sequence x1 . . . xn is fed into a LSTM layer, a hidden layer h combines the final embedding en and binary input features B, and an output layer s assigns the highest probable class label c. A RGUMENT C LASSIFICATION sc = 3 hs s Wes c en + Wc h + bc es hs Σi (Wi en + Wi h + bsi ) (6) (7) System Architecture The overall architecture of our SRL system closely follows that of previous work (Toutanova et al., 2008; Bj¨orkelund et al., 2009) and is depicted in Figure 4. We use a pipeline that consists of the following steps: predicate identification and disambiguation, argument identification, argument classification, and re-ranking. The neural-network components introduced in Section 2 are used in the last three steps. The following sub-sections describe all components in more detail. 3.1 Predicate Identification and Disambiguation Given a syntactically analyzed sentence, the first two steps in an end-to-end SRL system are to identify and disambiguate the semantic predicates in the sentence. Here, we f"
P16-1113,P12-1095,0,0.015937,"le labeling (SRL) is to identify and label the arguments of semantic predicates in a sentence according to a set of predefined relations (e.g., “who” did “what” to “whom”). Semantic roles provide a layer of abstraction beyond syntactic dependency relations, such as subject and object, in that the provided labels are insensitive to syntactic alternations and can also be applied to nominal predicates. Previous work has shown that semantic roles are useful for a wide range of natural language processing tasks, with recent applications including statistical machine translation (Aziz et al., 2011; Xiong et al., 2012), plagiarism detection (Osman et al., 2012; Paul and Jamal, 2015), and multi-document abstractive summarization (Khan et al., 2015). The task of semantic role labeling (SRL) was pioneered by Gildea and Jurafsky (2002). In their work, features based on syntactic constituent trees were identified as most valuable for labeling predicate-argument relationships. Later work confirmed the importance of syntactic parse features (Pradhan et al., 2005; Punyakanok et al., 2008) and found that dependency parse trees provide a better form of representation to assign role labels to arguments (Johansson and"
P16-1113,D15-1206,0,0.0113031,"roduced features that model the influence of discourse on role labeling decisions. Rather than coming up with completely new features, in this work we proposed to revisit some well-known features and represent them in a novel way that generalizes better. Our proposed model is inspired both by the necessity to overcome the problems of sparse lexico-syntactic features and by the recent success of SRL models based on neural networks. Dependency-based embeddings The idea of embedding dependency structures has previously been applied to tasks such as relation classification and sentiment analysis. Xu et al. (2015) and Liu et al. (2015) use neural networks to embed dependency paths between entity pairs. To identify the relation that holds between two entities, their approaches make use of pooling layers that detect parts of a path that indicate a specific relation. In contrast, our work aims at modeling an individual path as a complete sequence, in which every item is of relevance. Tai et al. (2015) and Ma et al. (2015) learn embeddings of dependency structures representing full sentences, in a sentiment classification task. In our model, embeddings are learned jointly with other features, and as a resu"
P16-1113,W04-3212,0,0.0228742,"word form, its predicted part-of-speech tag as well as dependency relations to all syntactic children. 3.2 Argument Identification and Classification Given a sentence and a set of sense-disambiguated predicates in it, the next two steps of our SRL system are to identify all arguments of each predicate and to assign suitable role labels to them. For both steps, we train several LSTM-based neural network models as described in Section 2. In particular, we train separate networks for nominal and verbal predicates and for identification and classification. Following the findings of earlier work (Xue and Palmer, 2004), we assume that different feature sets are relevant for the respective tasks and hence different embedding representations should be learned. As binary input features, we use the following sets from the SRL literature (Bj¨orkelund et al., 2010). 1195 Argument labeling step forget gate memory→gates |e| |h| alpha dropout rate Identification (verb) Identification (noun) Classification (verb) Classification (noun) − − + − + + − − 25 16 5 88 90 125 300 500 0.0006 0.0009 0.0155 0.0055 0.42 0.25 0.50 0.46 Table 2: Hyperparameters selected for best models and training procedures Lexico-syntactic feat"
P16-1113,W09-1209,0,0.592956,"Missing"
P16-1113,J13-3006,0,\N,Missing
P16-1113,W09-1201,0,\N,Missing
P17-1005,W15-0128,0,0.0277833,"tural Language Representations for Semantic Parsing Jianpeng Cheng† Siva Reddy† Vijay Saraswat‡ and Mirella Lapata† † School of Informatics, University of Edinburgh ‡ IBM T.J. Watson Research {jianpeng.cheng,siva.reddy}@ed.ac.uk, vsaraswa@us.ibm.com, mlap@inf.ed.ac.uk Abstract representation (Kwiatkowski et al., 2013; Reddy et al., 2016, 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learni"
P17-1005,P15-1033,0,0.0194296,"bility p(U |x) is factorized over time steps as: p(U |x) = p(a, u|x) = T Y t=1 (1) p(at |a&lt;t , x)p(ut |a&lt;t , x)I(at 6=RED) p(utGENERAL |a&lt;t , x) ∝ exp(Wp · et ) To choose a natural language term, we directly compute a probability distribution of all natural language terms (in the buffer) conditioned on the stack representation st and select the most relevant term (Jia and Liang, 2016): where I is an indicator function. To predict the actions of the transition system, we encode the input buffer with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and the output stack with a stack-LSTM (Dyer et al., 2015). At each time step, the model uses the representation of the transition system et to predict an action: p(at |a&lt;t , x) ∝ exp(Wa · et ) (3) p(utNL |a&lt;t , x) ∝ exp(st ) (4) When the predicted action is RED, the completed subtree is composed into a single representation on the stack. For the choice of composition function, we use a single-layer neural network as in Dyer et al. (2015), which takes as input the concatenated representation of the predicate and argument of the subtree. (2) where et is the concatenation of the buffer representation bt and the stack representation st . While the stack"
P17-1005,D13-1160,0,0.827823,"erent from linguistically motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approa"
P17-1005,N16-1024,0,0.176489,"where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. all is a special predicate that returns a collection of entities. exclude is a predicate that returns the difference between two input sets. An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms. This property makes FunQL logical forms convenient to be predicted with recurrent neural networks (Vinyals et al., 2015; Choe and Charniak, 2016; Dyer et al., 2016). However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables. A more compact logical formulation which our method also applies to is λ-DCS (Liang, 2013). In the absence of anaphora and composite binary predicates, conversion algorithms exist between FunQL and λ-DCS. However, we leave this to future work. Preliminaries Problem Formulation Let K denote a knowledge base or more generally a reasoning system, and x an utterance paired with a grounded meaning representation G or its denotation y. Our problem is to learn a semantic parser that maps x to G vi"
P17-1005,P14-1133,0,0.261837,"anner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations. Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Berant et al., 2013; Berant and Liang, 2014), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or th"
P17-1005,P14-1134,0,0.0313606,"ally motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yiel"
P17-1005,Q15-1039,0,0.189964,"er three datasets, we treat surrogate meaning representations which lead to the correct answer as gold standard. The surrogates were selected from a subset of candidate Freebase graphs, which were obtained by entity linking. Entity mentions in S PADES have been automatically annotated with Freebase entities (Gabrilovich et al., 2013). For W EB Q UESTIONS and G RAPH Q UESTIONS, we follow the procedure described in Reddy et al. (2016). We identify po4 http://developers.google.com/ freebase/ 49 Models Berant et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bast and Haussmann (2015) Berant and Liang (2015) Reddy et al. (2016) Bordes et al. (2014) Dong et al. (2015) Yih et al. (2015) Xu et al. (2016) Neural Baseline S CANNE R F1 35.7 33.0 39.9 49.4 49.7 50.3 39.2 40.8 52.5 53.3 48.3 49.4 Models Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Kwiatkowksi et al. (2010) Kwiatkowski et al. (2011) Kwiatkowski et al. (2013) Zhao and Huang (2015) Liang et al. (2011) Dong and Lapata (2016) Jia and Liang (2016) Jia and Liang (2016) with extra data S CANNE R Table 5: G EO Q UERY results. Table 3: W EB Q UESTIONS results. Models SEMPRE (Berant et al., 2013) PARASEMPRE (Berant and Liang, 2014)"
P17-1005,D16-1214,1,0.897283,"ich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or their denotations. We conduct experiments on four datasets, including G EO Q UERY (which has logical forms; Zelle and Mooney 1996), S PADES (Bisk et al., 2016), W EB Q UESTIONS (Berant et al., 2013), and G RAPH Q UESTIONS (Su et al., 2016) (which have denotations). Our semantic parser achieves the state of the art on S PADES and G RAPH Q UESTIONS, while obtaining competitive results on G EO Q UERY and W EB Q UESTIONS. A side-product of our modeling framework is that the induced intermediate representations can contribute to rationalizing neural predictions (Lei et al., 2016). Specifically, they can shed light on the kinds of representations (especially predicates) useful for semantic parsing. Evaluation of the induced predicate-argument relations ag"
P17-1005,D14-1067,0,0.0665306,"Missing"
P17-1005,P15-1143,0,0.0186659,"rsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the prob"
P17-1005,P13-1042,0,0.0795468,"ilable at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations. Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Berant et al., 2013; Berant and Liang, 2014), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features. The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding. The whole network is trained end-to-end on natural language uttera"
P17-1005,D16-1257,0,0.0243005,"e(all), next to(texas))) where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. all is a special predicate that returns a collection of entities. exclude is a predicate that returns the difference between two input sets. An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms. This property makes FunQL logical forms convenient to be predicted with recurrent neural networks (Vinyals et al., 2015; Choe and Charniak, 2016; Dyer et al., 2016). However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables. A more compact logical formulation which our method also applies to is λ-DCS (Liang, 2013). In the absence of anaphora and composite binary predicates, conversion algorithms exist between FunQL and λ-DCS. However, we leave this to future work. Preliminaries Problem Formulation Let K denote a knowledge base or more generally a reasoning system, and x an utterance paired with a grounded meaning representation G or its denotation y. Our problem is to learn a semantic parser"
P17-1005,P16-1004,1,0.825987,"Reddy et al., 2016, 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. Moreover, without any tasks"
P17-1005,P16-1002,0,0.376516,"4; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. Moreover, without any taskspecific prior knowled"
P17-1005,J13-2005,0,0.108582,"stic conventions. 2 Predicate answer Usage denotation wrapper type entity type checking all aggregation logical connectors querying for an entire set of entities one-argument meta predicates for sets two-argument meta predicates for sets Sub-categories — stateid, cityid, riverid, etc. — count, largest, smallest, etc. intersect, union, exclude Table 1: List of domain-general predicates. tion y. Grounded Meaning Representation We represent grounded meaning representations in FunQL (Kate et al., 2005) amongst many other alternatives such as lambda calculus (Zettlemoyer and Collins, 2005), λ-DCS (Liang, 2013) or graph queries (Holzschuher and Peinl, 2013; Harris et al., 2013). FunQL is a variable-free query language, where each predicate is treated as a function symbol that modifies an argument list. For example, the FunQL representation for the utterance which states do not border texas is: answer(exclude(state(all), next to(texas))) where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation. all is a special predicate that returns a collection of entities. exclude is a pre"
P17-1005,P11-1060,0,0.875123,"the handling of unseen words and knowledge transfer across domains (Bender et al., 2015). The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016). Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering. But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed. Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers. Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missin"
P17-1005,P14-5010,0,0.00234263,"Missing"
P17-1005,D16-1116,0,0.132496,"Missing"
P17-1005,D12-1069,0,0.0921173,"Missing"
P17-1005,P15-1142,0,0.139675,"Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This is achieved with a transition-based approach which by design yields recursive semantic str"
P17-1005,Q15-1019,0,0.0691589,"Missing"
P17-1005,D14-1162,0,0.0945929,"set of features include the embedding similarity between the relation and the utterance, as well as the similarity between the relation and the question words. The last set of features includes the answer type as indicated by the last word in the Freebase relation (Xu et al., 2016). We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1. The dimensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are [50, 100, 100, 100]. The word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. In this section, we verify empirically that our semantic parser derives useful meaning representations. We give details on the evaluation datasets and baselines used for comparison. We also describe implementation details and the features used in the discriminative ranker. 4.1 Datasets We evaluated our model on the following datasets which cover different domains, and use different types of training data, i.e., pairs of natural language utterances and grounded meanings or question-answer pairs. G EO Q UERY (Zelle and Mooney, 1996) contains 880"
P17-1005,D10-1119,0,0.0264859,"ul for semantic parsing and how these are different from linguistically motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1005 from data. This i"
P17-1005,Q14-1030,1,0.910705,"ations so as to build better semantic parsers. Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets). In this work, we propose a neural semantic parser that alleviates the aforementioned problems. Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates. However, rather than using an external parser (Reddy et al., 2014, 2016) or manually specified CCG grammars (Kwiatkowski et al., 2013), we induce intermediate representations in the form of predicate-argument structures We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art"
P17-1005,D13-1161,0,0.23211,"ut any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets). In this work, we propose a neural semantic parser that alleviates the aforementioned problems. Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates. However, rather than using an external parser (Reddy et al., 2014, 2016) or manually specified CCG grammars (Kwiatkowski et al., 2013), we induce intermediate representations in the form of predicate-argument structures We introduce a neural semantic parser which is interpretable and scalable. Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains. The semantic parser is trained end-to-end using annotated logical forms or their denotations. We achieve the state of the art on S PADES and G RAPH Q UESTIONS and obtain competitive results on G"
P17-1005,Q16-1010,1,0.75919,"Missing"
P17-1005,D11-1140,0,0.571151,"Missing"
P17-1005,D16-1011,0,0.0082742,"Missing"
P17-1005,1998.amta-tutorials.1,0,0.612057,"Missing"
P17-1005,N15-1162,0,0.184412,"Missing"
P17-1005,N06-1056,0,0.268253,"of representations useful for semantic parsing and how these are different from linguistically motivated ones.1 1 Introduction Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations. Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches. Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015). Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded 1 Our code is available at https://github.com/ cheng6076/scanner. 44 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/"
P17-1005,P16-1220,1,0.67213,"on score as a feature in the discriminative reranker, thus leaving the final disambiguation to the semantic parser. Apart from the entity score, the discriminative ranker uses the following basic features. The first feature is the likelihood score of a grounded representation aggregating all intermediate representations. The second set of features include the embedding similarity between the relation and the utterance, as well as the similarity between the relation and the question words. The last set of features includes the answer type as indicated by the last word in the Freebase relation (Xu et al., 2016). We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1. The dimensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are [50, 100, 100, 100]. The word embeddings were initialized with Glove embeddings (Pennington et al., 2014). All other embeddings were randomly initialized. In this section, we verify empirically that our semantic parser derives useful meaning representations. We give details on the evaluation datasets and baselines used for comparison. We also describe implement"
P17-1005,P14-1090,0,0.0872756,"Missing"
P17-1005,P15-1128,0,0.0562248,"Missing"
P17-1005,P16-2033,0,0.00557879,"intermediate syntactic representation and then grounded to Freebase. Specifically, Bisk et al. (2016) evaluate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required. As can be seen, S CANNE R outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons. For fair comparison, we also built a neural baseline that encodes an utterance with a recurrent neural network and then predicts a grounded meaning representation directly (Ture and Jojic, 2016; Yih et al., 2016). Again, we observe that S CANNE R outperforms this baseline. Results on W EB Q UESTIONS are summarized in Table 3. S CANNE R obtains performance on par with the best symbolic systems (see the first block in the table). It is important to note that Bast and Haussmann (2015) develop a question answering system, which contrary to ours cannot produce meaning representations whereas Berant and Liang (2015) propose a sophisticated agenda-based parser which is trained borrowing ideas from imitation learning. S CANNE R is conceptually similar to Reddy et al. (2016) who also learn a semantic parser vi"
P17-1005,D07-1071,0,0.789834,"Missing"
P17-1005,D14-1107,0,\N,Missing
P17-1005,D16-1054,0,\N,Missing
P17-2019,W09-3839,0,0.0291949,"Elhadad, 2010), which are directly trained to maximize the conditional probability of the parse tree given the sentence, where linear-time decoding algorithms exist (e.g., for transition-based parsers). In this work, we propose a parsing and language modeling framework that marries a generative model with a discriminative recognition algorithm in order to have the best of both worlds. The idea of combining these two types of models is not new. For example, Collins and Koo (2005) propose to use a generative model to generate candidate constituency trees and a discriminative model to rank them. Sangati et al. (2009) follow the opposite direction and employ a generative model to re-rank the dependency trees produced by a discriminative parser. However, previous work combines the two types of models in a goal-oriented, pipeline fashion, which lacks model interpretations and focuses solely on parsing. In comparison, our framework unifies generative and discriminative parsers with a single objective, which connects to expectation maximization and variational inference in grammar induction settings. In a nutshell, we treat parse trees as latent factors generating natural language sentences and parsing as a po"
P17-2019,N03-1014,0,0.0523765,"he framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1 1 Introduction Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing. At test time, these models require a relatively expensive recognition algo1 Our code is available at https://github.com/ cheng6076/virnng.git. 118 Proceedings of the 55th Annual Meeting of the Association"
P17-2019,P12-1046,0,0.0413555,"Missing"
P17-2019,P08-1067,0,0.103524,"Missing"
P17-2019,P13-1045,0,0.0510952,"Missing"
P17-2019,P07-1080,0,0.0303711,"d on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1 1 Introduction Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing. At test time, these models require a relatively expensive recognition algo1 Our code is available at https://github.com/ cheng6076/virnng.git. 118 Proceedings of the 55th Annual Meeting of the Association for Computational Linguisti"
P17-2019,D16-1031,0,0.165612,") The encoder is a discriminative RNNG that computes the conditional probability q(a|x) of the transition action sequence a given an observed sentence x. This conditional probability is factorized over time steps as: q(a|x) = |a| Y t=1 q(at |vt ) log p(x) ≥ Eq(a|x) log p(x, a) = Lx q(a|x) (8) where p(x, a) = p(x|a)p(a) comes from the decoder or the generative model, and q(a|x) comes from the encoder or the recognition model. The objective function6 in Equation (8), denoted by Lx , is unsupervised and suited to a grammar induction task. This objective can be optimized with the methods shown in Miao and Blunsom (2016). Next, consider the case when the parse tree is observed. We can directly maximize the log likelihood of the parse tree for the encoder output log q(a|x) and the decoder output log p(a): (5) where vt is the transitional state embedding of the encoder at time step t. The next action is predicted similarly to Equation (2), but conditioned on vt . Thanks to the discriminative property, vt has access to any contextual features defined over the entire sentence and the stack — q(a|x) acts as a context sensitive posterior approximation. Our features4 are: 1) the stack embedding et obtained with a st"
P17-2019,P13-1043,0,0.0542544,"Missing"
P17-2019,N07-1051,0,0.0698452,"on and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.1 1 Introduction Generative models defining joint distributions over parse trees and sentences are good theoretical models for interpreting natural language data, and appealing tools for tasks such as parsing, grammar induction and language modeling (Collins, 1999; Henderson, 2003; Titov and Henderson, 2007; Petrov and Klein, 2007; Dyer et al., 2016). However, they often impose strong independence assumptions which restrict the use of arbitrary features for effective disambiguation. Moreover, generative parsers are typically trained by maximizing the joint probability of the parse tree and the sentence—an objective that only indirectly relates to the goal of parsing. At test time, these models require a relatively expensive recognition algo1 Our code is available at https://github.com/ cheng6076/virnng.git. 118 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages"
P17-2019,J03-4003,0,\N,Missing
P17-2019,J05-1003,0,\N,Missing
P17-2019,D14-1162,0,\N,Missing
P17-2019,N16-1024,0,\N,Missing
P17-2019,N10-1115,0,\N,Missing
P18-1040,P17-1112,0,0.091708,"Missing"
P18-1040,E17-2039,0,0.171166,"Missing"
P18-1040,P13-2131,0,0.160099,"on task, we cannot expect model output to exactly match the gold standard. For instance, the numbering of the referents may be different, but nevertheless valid, or the order of the children of a tree node (e.g., “DRS(india(x1 ) say(e1 ))” and “DRS(say(e1 ) india(x1 ))” are the same). We thus use F1 instead of exact match accuracy. Specifically, we report D-match4 a metric designed to evaluate scoped meaning representations and released as part of the distribution of the Parallel Meaning Bank corpus (Abzianidze et al., 2017). D-match is based on Smatch5 , a metric used to evaluate AMR graphs (Cai and Knight, 2013); it calculates F1 on discourse representation graphs (DRGs), i.e., triples of nodes, arcs, and their referents, applying multiple restarts to obtain a good referent (node) mapping between graphs. We converted DRSs (predicted and goldstandard) into DRGs following the top-down procedure described in Algorithm 1.6 I S C ONDI TION returns true if the child is a condition (e.g., india(x1 )), where three arcs are created, one is connected to a parent node and the other two are connected to arg1 and arg2, respectively (lines 7–12). I S Q UANTIFIER returns true if the child is a quantifier (e.g., π1"
P18-1040,P17-1005,1,0.843668,"hool of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk Abstract ear ordering has also prompted efforts to develop recurrent neural network architectures tailored to tree or graph-structured decoding (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017) Most previous work focuses on building semantic parsers for question answering tasks, such as querying a database to retrieve an answer (Zelle and Mooney, 1996; Cheng et al., 2017), or conversing with a flight booking system (Dahl et al., 1994). As a result, parsers trained on query-based datasets work on restricted domains (e.g., restaurants, meetings; Wang et al. 2015), with limited vocabularies, exhibiting limited compositionality, and a small range of syntactic and semantic constructions. In this work, we focus on open-domain semantic parsing and develop a general-purpose system which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). DRT is a popular theory of meaning representation designed to accou"
P18-1040,D15-1198,0,0.020494,"formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order logic. 30 sentence length Figure 5: F1 score as a function of sentence length. Figure 5 shows F1 performance for the three parsers on sentences of different length. We observe a similar trend for all models: as sentence length increases, model performance decreases. The baseline and shallow models do not perform well on short sentences which despite containing fewer words, can still represent complex meaning which is challenging to capture sequentially. On the other hand, the performance of the deep model is relatively stable. LSTMs in this case function relati"
P18-1040,J07-4004,0,0.0326455,", (hpvari, hre f i)∗ , (hpvari, hconditioni)∗ k1 : hexpt i, k2 : hexpt i k2 :hexpt i | , coo(k1 , k2 ) sub(k1 , k2 ) (6) hunaryi ::= ¬hexpt i |2hexpt i|3hexpt i|hre f i : hexpt i (1) hbinaryi ::=hexpt i→hexpt i|hexpt i∨hexpt i|hexpt i?hexpt i 3 The Groningen Meaning Bank Corpus Corpus Creation DRSs in GMB were obtained from Boxer (Bos, 2008, 2015), and then refined using expert linguists and crowdsourcing methods. Boxer constructs DRSs based on a pipeline of tools involving POS-tagging, named entity recognition, and parsing. Specifically, it relies on the syntactic analysis of the C&C parser (Clark and Curran, 2007), a general-purpose parser using the framework of Combinatory Categorial Grammar (CCG; Steedman 2001). DRSs are obtained from CCG parses, with semantic composition being guided by the CCG syntactic derivation. Documents in the GMB were collected from a variety of sources including Voice of America (a newspaper published by the US Federal Government), the Open American National Corpus, Aesop’s fables, humorous stories and jokes, and country descriptions from the CIA World Factbook. The dataset consists of 10,000 documents each annotated with a DRS. Various statistics on the GMB are shown in Tab"
P18-1040,H94-1010,0,0.246362,"Edinburgh EH8 9AB Jiangming.Liu@ed.ac.uk, scohen@inf.ed.ac.uk, mlap@inf.ed.ac.uk Abstract ear ordering has also prompted efforts to develop recurrent neural network architectures tailored to tree or graph-structured decoding (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017) Most previous work focuses on building semantic parsers for question answering tasks, such as querying a database to retrieve an answer (Zelle and Mooney, 1996; Cheng et al., 2017), or conversing with a flight booking system (Dahl et al., 1994). As a result, parsers trained on query-based datasets work on restricted domains (e.g., restaurants, meetings; Wang et al. 2015), with limited vocabularies, exhibiting limited compositionality, and a small range of syntactic and semantic constructions. In this work, we focus on open-domain semantic parsing and develop a general-purpose system which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). DRT is a popular theory of meaning representation designed to account for a variety of linguistic phenomena, including the interpre"
P18-1040,W13-2322,0,0.138251,"and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin. 1 Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; Kate et al. 2005) to dependency-based compositional semantics (λ-DCS; Liang et al. 2011), lambda calculus (Zettlemoyer and Collins, 2005), abstract meaning representations (Banarescu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2"
P18-1040,P16-1004,1,0.933513,"escu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016). The fact that meaning representations do not naturally conform to a lin429 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 429–439 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics x1 , e1 , π1 statement(x1 ), say(e1 ), Cause(e1 , x1 ), Topic(e1 ,π1 ) x7 , s2 , x8 , x9 , e3 x3 , s1 , x3 , x5 , e2 π1 : k1 : x2 thing(x) ⇒ Topic(s1 , x3 ), dead(s1 ), x6 man(x3 ), of(x2 , x3 ), k2 : thing(x6 ) magazine(x4 ), on(x5 ,x4 ) vest(x5 ), wear(e2 ), Agent(e2"
P18-1040,basile-etal-2012-developing,0,0.0502792,"node, we have already obtained the structure of the entire tree. deep shallow baseline 80 60 15 20 25 Wide-coverage Semantic Parsing Our model is trained on the GMB (Bos et al., 2017), a richly annotated resource in the style of DRT which provides a unique opportunity for bootstrapping wide-coverage semantic parsers. Boxer (Bos, 2008) was a precursor to the GMB, the first semantic parser of this kind, which deterministically maps CCG derivations onto formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order logic. 30 sentence length Figure 5: F1 score as a function of sentence length. Figure 5 shows F1 performance for"
P18-1040,P82-1020,0,0.828613,"Missing"
P18-1040,P16-1002,0,0.0387247,"minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016). The fact that meaning representations do not naturally conform to a lin429 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 429–439 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics x1 , e1 , π1 statement(x1 ), say(e1 ), Cause(e1 , x1 ), Topic(e1 ,π1 ) x7 , s2 , x8 , x9 , e3 x3 , s1 , x3 , x5 , e2 π1 : k1 : x2 thing(x) ⇒ Topic(s1 , x3 ), dead(s1 ), x6 man(x3 ), of(x2 , x3 ), k2 : thing(x6 ) magazine(x4 ), on(x5 ,x4 ) vest(x5 ), wear(e2 ), Agent(e2 , x2 ), Theme(e2 , x"
P18-1040,W08-2222,0,0.753358,"as temporal order and communicative intentions (see continuation(k1 , k2 ) in Figure 1). More formally, DRSs are expressions of type hexpe i (denoting individuals or discourse referents) and hexpt i (i.e., truth values): hexpe i ::= hre f i, hexpt i ::= hdrsi|hsdrsi, (hpvari, hre f i)∗ , (hpvari, hconditioni)∗ k1 : hexpt i, k2 : hexpt i k2 :hexpt i | , coo(k1 , k2 ) sub(k1 , k2 ) (6) hunaryi ::= ¬hexpt i |2hexpt i|3hexpt i|hre f i : hexpt i (1) hbinaryi ::=hexpt i→hexpt i|hexpt i∨hexpt i|hexpt i?hexpt i 3 The Groningen Meaning Bank Corpus Corpus Creation DRSs in GMB were obtained from Boxer (Bos, 2008, 2015), and then refined using expert linguists and crowdsourcing methods. Boxer constructs DRSs based on a pipeline of tools involving POS-tagging, named entity recognition, and parsing. Specifically, it relies on the syntactic analysis of the C&C parser (Clark and Curran, 2007), a general-purpose parser using the framework of Combinatory Categorial Grammar (CCG; Steedman 2001). DRSs are obtained from CCG parses, with semantic composition being guided by the CCG syntactic derivation. Documents in the GMB were collected from a variety of sources including Voice of America (a newspaper publish"
P18-1040,W15-1841,0,0.230071,"Missing"
P18-1040,J16-3006,0,0.02988,"rivations onto formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order logic. 30 sentence length Figure 5: F1 score as a function of sentence length. Figure 5 shows F1 performance for the three parsers on sentences of different length. We observe a similar trend for all models: as sentence length increases, model performance decreases. The baseline and shallow models do not perform well on short sentences which despite containing fewer words, can still represent complex meaning which is challenging to capture sequentially. On the other hand, the performance of the deep model is relatively stable. LSTMs in t"
P18-1040,N06-1056,0,0.061337,"anguage to machine interpretable meaning representations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; Kate et al. 2005) to dependency-based compositional semantics (λ-DCS; Liang et al. 2011), lambda calculus (Zettlemoyer and Collins, 2005), abstract meaning representations (Banarescu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016). The fact that meaning representations do not naturally conform to a lin429 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 429–439 c Melbo"
P18-1040,D16-1116,0,0.0611133,"Missing"
P18-1040,P16-1127,0,0.0186379,"h is progressively refined). We provide examples of model output in the supplementary material. 7 DRG w/o refs & conds P R F1 52.89 71.80 60.91 83.30 62.91 71.68 93.91 88.51 91.13 Table 4: GMB test set. Table 3: GMB development set. 10 DRG w/o refs F1 P R F1 57.69 47.20 58.93 52.42 65.24 66.05 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB"
P18-1040,D17-1160,0,0.017859,"refined). We provide examples of model output in the supplementary material. 7 DRG w/o refs & conds P R F1 52.89 71.80 60.91 83.30 62.91 71.68 93.91 88.51 91.13 Table 4: GMB test set. Table 3: GMB development set. 10 DRG w/o refs F1 P R F1 57.69 47.20 58.93 52.42 65.24 66.05 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB show that our decoder is able"
P18-1040,P17-1041,0,0.0115588,".91 88.51 91.13 Table 4: GMB test set. Table 3: GMB development set. 10 DRG w/o refs F1 P R F1 57.69 47.20 58.93 52.42 65.24 66.05 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB show that our decoder is able to recover discourse representation structures to a good degree (77.54 F1 ), albeit with some simplifications. In the future, we plan to m"
P18-1040,C12-1094,0,0.609827,"trees sequentially, and then expand non-terminal nodes, ensuring that when we generate the children of a node, we have already obtained the structure of the entire tree. deep shallow baseline 80 60 15 20 25 Wide-coverage Semantic Parsing Our model is trained on the GMB (Bos et al., 2017), a richly annotated resource in the style of DRT which provides a unique opportunity for bootstrapping wide-coverage semantic parsers. Boxer (Bos, 2008) was a precursor to the GMB, the first semantic parser of this kind, which deterministically maps CCG derivations onto formal meaning representations. Le and Zuidema (2012) were the first to train a semantic parser on an early release of the GMB (2,000 documents; Basile et al. 2012), however, they abandon lambda calculus in favor of a graph based representation. The latter is closely related to AMR, a general-purpose meaning representation language for broad-coverage text. In AMR the meaning of a sentence is represented as a rooted, directed, edge-labeled and leaf-labeled graph. AMRs do not resemble classical meaning representations and do not have a model-theoretic interpretation. However, see Bos (2016) and Artzi et al. (2015) for translations to first-order l"
P18-1040,P11-1060,0,0.0240508,"ding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin. 1 Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. A variety of meaning representations have been adopted over the years ranging from functional query language (FunQL; Kate et al. 2005) to dependency-based compositional semantics (λ-DCS; Liang et al. 2011), lambda calculus (Zettlemoyer and Collins, 2005), abstract meaning representations (Banarescu et al., 2013), and minimal recursion semantics (Copestake et al., 2005). Existing semantic parsers are for the most part data-driven using annotated examples consisting of utterances and their meaning representations (Zelle and Mooney, 1996; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction"
P18-1040,P17-1049,0,0.0173502,"5 62.93 64.45 77.54 82.87 79.40 81.10 Related Work 8 Tree-structured Decoding A few recent approaches develop structured decoders which make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) generate trees in a top-down fashion, while in other work (Xiao et al., 2016; Krishnamurthy et al., 2017) the decoder generates from a grammar that guarantees that predicted logical forms are well-typed. In a similar vein, Yin and Neubig (2017) generate abstract syntax trees (ASTs) based on the application of production rules defined by the grammar. Rabinovich et al. (2017) introduce a modular decoder whose various components are dynamically composed according to the generated tree structure. In comparison, our model does not use grammar information explicConclusions We introduced a new end-to-end model for opendomain semantic parsing. Experimental results on the GMB show that our decoder is able to recover discourse representation structures to a good degree (77.54 F1 ), albeit with some simplifications. In the future, we plan to model document-level representations which are more in line with DRT and the GMB annotations. Acknowledgments We thank the anonymous"
P18-1040,W13-0122,0,0.194002,"nd Reyle, 1993). Basic DRSs consist of discourse referents (e.g., x, y) representing entities in the discourse and discourse conditions (e.g., man(x), magazine(y)) representing information about discourse referents. Following conventions in the DRT literature, we visualize DRSs in a box-like format (see Figure 1). GMB adopts a variant of DRT that uses a neoDavidsonian analysis of events (Kipper et al., 2008), i.e., events are first-order entities characterized by one-place predicate symbols (e.g., say(e1 ) in Figure 1). In addition, it follows Projective Discourse Representation Theory (PDRT; Venhuizen et al. 2013) an extension of DRT specifically developed to account for the interpretation of presuppositions and related projection phenomena 1 https://github.com/EdinburghNLP/EncDecDRSparsing 430 sections 00-99 20-99 10-19 00-09 (e.g., conventional implicatures). In PDRT, each basic DRS introduces a label, which can be bound by a pointer indicating the interpretation site of semantic content. To account for the rhetorical structure of texts, GMB adopts Segmented Discourse Representation Theory (SDRT; Asher and Lascarides 2003). In SDRT, discourse segments are linked with rhetorical relations reflecting d"
P18-1040,P15-1129,0,0.0221408,"Missing"
P18-1068,P13-2009,0,0.0588517,"ed in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for mul"
P18-1068,Q13-1005,0,0.123826,"syntax trees (Aho et al., 2007) in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively rep"
P18-1068,D13-1160,0,0.176179,"LP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation"
P18-1068,P17-1089,0,0.212727,"entation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. 732 (lambda $0 e (and $0 e (flight $0 ) (< $0 ) Sketch-Guided Output Deco"
P18-1068,N06-1022,0,0.0590303,"r model for the generation of abstract syntax trees (Aho et al., 2007) in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using pro"
P18-1068,P16-1002,0,0.786326,"sults on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders. 1 Introduction Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Ass"
P18-1068,P17-1005,1,0.870424,"cisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. 732 (lambda $0 e (and $0 e (flight $0 ) (< $0 ) Sketch-Guided Output Decoding <s> (departure _time $0 ) ti0 ) $0 ) ti0 ) </s> ) ( |, ) Sketch Encoding (lambda#2 (and flight@1 (< departure _time@1 ? ) ) ) </s> ? ) ) ) Sketch Decoding <s> (lambda#2 (and flight@1 (< departure _time@1 ( |) Input Encoding Encoder units Decoder units all flights before ti0 Figure 1: We ﬁrst generate the meaning sketch a for natural language input x. Then, a"
P18-1068,D16-1116,0,0.129875,"Missing"
P18-1068,P16-1004,1,0.90972,"s characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders. 1 Introduction Semantic parsing maps natural language utterances onto machine interpretable meaning representations (e.g., executable queries or logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computatio"
P18-1068,D17-1160,0,0.500369,"ion of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dataset Length Example G EO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all ﬂights from dallas before 10am y : (lambda $0 e (and (ﬂight $0) (from $0 dallas:ci) (< (departure time $0) 1000:ti))"
P18-1068,W17-2607,0,0.112404,"wiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers"
P18-1068,D10-1119,0,0.209378,"Missing"
P18-1068,W05-0602,0,0.0862539,"ns of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augme"
P18-1068,D13-1161,0,0.089859,"Missing"
P18-1068,P16-1154,0,0.0584332,"he original tokens with their token types, except delimiters (e.g., “[”, and “:”), operators (e.g., “+”, and “*”), and built-in keywords (e.g., “True”, and “while”). For instance, the expression “if s[:4].lower() == ’http’:” becomes “if NAME [ : NUMBER ] . NAME ( ) == STRING :”, with details about names, values, and strings being omitted. D JANGO is a diverse dataset, spanning various real-world use cases and as a result models are often faced with out-of-vocabulary (OOV) tokens (e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with a copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Jia and Liang, 2016), which allows the ﬁne meaning decoder (Section 3.2) to directly copy tokens from the natural language input. The ﬁrst element between a pair of brackets is an operator or predicate name, and any remaining elements are its arguments. Algorithm 1 shows the pseudocode used to extract sketches from λ-calculus-based meaning representations. We strip off arguments and variable names in logical forms, while keeping predicates, operators, and composition information. We use the symbol “@” to denote the number of missing arguments in a predicate. For exampl"
P18-1068,D11-1140,0,0.832537,"tions has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), shar"
P18-1068,P16-1014,0,0.0317806,"s with their token types, except delimiters (e.g., “[”, and “:”), operators (e.g., “+”, and “*”), and built-in keywords (e.g., “True”, and “while”). For instance, the expression “if s[:4].lower() == ’http’:” becomes “if NAME [ : NUMBER ] . NAME ( ) == STRING :”, with details about names, values, and strings being omitted. D JANGO is a diverse dataset, spanning various real-world use cases and as a result models are often faced with out-of-vocabulary (OOV) tokens (e.g., variable names, and numbers) that are unseen during training. We handle OOV tokens with a copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Jia and Liang, 2016), which allows the ﬁne meaning decoder (Section 3.2) to directly copy tokens from the natural language input. The ﬁrst element between a pair of brackets is an operator or predicate name, and any remaining elements are its arguments. Algorithm 1 shows the pseudocode used to extract sketches from λ-calculus-based meaning representations. We strip off arguments and variable names in logical forms, while keeping predicates, operators, and composition information. We use the symbol “@” to denote the number of missing arguments in a predicate. For example, we extract “from@2”"
P18-1068,J13-2005,0,0.161781,"Missing"
P18-1068,P16-1057,0,0.379224,"Missing"
P18-1068,P17-2098,0,0.14554,"mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning representations. 732 (lambda $0 e"
P18-1068,D08-1082,0,0.105245,"ediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learnin"
P18-1068,W00-1317,0,0.552254,"our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these mode"
P18-1068,D15-1166,0,0.0942872,"ion probabilities are factorized as: p (a|x) = |a|  p (at |a<t , x) (2) p (yt |y<t , x, a) (3) t p (y|x, a) = LSTM − − et = [→ e t, ← e t] t+1 t (6) where [·, ·] denotes vector concatenation, et ∈ Rn , and fLSTM is the LSTM function. Coarse Meaning Decoder The decoder’s hidden vector at the t-th time step is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the embedding of the previously predicted token. The hidden states of the ﬁrst time step in the decoder are initialized by the concatenated encoding − − e |x |, ← e 1 ]. Additionally, we use an vectors d0 = [→ attention mechanism (Luong et al., 2015) to learn soft alignments. We compute the attention score for the current time step t of the decoder, with the k-th hidden state in the encoder as: t=1 |y|  Sketch Generation t=1 where a<t = a1 · · · at−1 , and y<t = y1 · · · yt−1 . In the following, we will explain how p (a|x) and p (y|x, a) are estimated. st,k = exp{dt · ek }/Zt 733 (7) |x| where Zt = j=1 exp{dt · ej } is a normalization term. Then we compute p (at |a<t , x) via: edt = |x|  st,k ek output will indicate whether missing details have been generated (e.g., if the decoder emits a closing quote token for “STRING”). Moreover, ty"
P18-1068,P14-5010,0,0.00422056,"16), where natural language expressions are lowercased and stemmed with NLTK (Bird et al., 2009), and entity mentions are replaced by numbered markers. We combined predicates and left brackets that indicate hierarchical structures to make meaning representations compact. We employed the preprocessed D JANGO data provided by Yin and Neubig (2017), where input expressions are tokenized by NLTK, and quoted strings in the input are replaced with place holders. W IK I SQL was preprocessed by the script provided by Zhong et al. (2017), where inputs were lowercased and tokenized by Stanford CoreNLP (Manning et al., 2014). Conﬁguration Model hyperparameters were cross-validated on the training set for G EO, and were validated on the development split for the other datasets. Dimensions of hidden vectors and word embeddings were selected from {250, 300} and {150, 200, 250, 300}, respectively. The dropout rate was selected from {0.3, 0.5}. Label smoothing (Szegedy et al., 2016) was employed for G EO and ATIS. The smoothing parameter was set to 0.1. For W IKI SQL, the hidden size of σ(·) 737 Method G EO ATIS Method Accuracy ZC07 (Zettlemoyer and Collins, 2007) UBL (Kwiatkowksi et al., 2010) FUBL (Kwiatkowski et al"
P18-1068,P07-1121,0,0.347102,"sing sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016),"
P18-1068,D14-1162,0,0.0850496,"Missing"
P18-1068,P16-1127,0,0.4597,"r logical forms). The successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dataset Length Example G EO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all ﬂights from dallas before 10am y : (lambda $0 e (and (ﬂight $0) ("
P18-1068,P13-1092,0,0.0762511,"Missing"
P18-1068,P17-1105,0,0.671258,"ikhail Snitko record for after 1996? y : SELECT Record Company WHERE (Year of Recording > 1996) AND (Conductor = Mikhail Snitko) a : WHERE > AND = Table 1: Examples of natural language expressions x, their meaning representations y, and meaning sketches a. The average number of tokens is shown in the second column. with previous systems, despite employing relatively simple sequence decoders. 2 use a transition system to generate variable-free queries. Yin and Neubig (2017) design a grammar model for the generation of abstract syntax trees (Aho et al., 2007) in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro gram"
P18-1068,P17-1041,0,0.367847,"he successful application of recurrent neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to treat semantic parsing as a sequence-to-sequence problem (Jia and Liang, 2016; Dong and Lapata, 2016; Ling et al., 2016). The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure. Examples include tree decoders (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017), decoders constrained by a grammar model (Xiao et al., 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017), or modular 731 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 731–742 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Dataset Length Example G EO 7.6 13.7 6.9 x : which state has the most rivers running through it? y : (argmax $0 (state:t $0) (count $1 (and (river:t $1) (loc:t $1 $0)))) a : (argmax#1 state:t@1 (count#1 (and river:t@1 loc:t@2 ) ) ) ATIS 11.1 21.1 9.2 x : all ﬂights from dallas before 10am y : (lambda $0 e (and (ﬂight $0) (from $0 dallas:ci) (<"
P18-1068,P17-2007,0,0.0503599,"ally learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or meaning representations (Susanto and Lu, 2017; Herzig and Berant, 2017), and utilizing user feedback signals (Iyer et al., 2017). There are also efforts to develop structured decoders that make use of the syntax of meaning representations. Dong and Lapata (2016) and Alvarez-Melis and Jaakkola (2017) develop models which generate tree structures in a topdown fashion. Xiao et al. (2016) and Krishnamurthy et al. (2017) employ the grammar to constrain the decoding process. Cheng et al. (2017) 3 Problem Formulation Our goal is to learn semantic parsers from instances of natural language expressions paired with their structured meaning represe"
P18-1068,D07-1071,0,0.804567,"g representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016"
P18-1068,D17-1125,0,0.0300138,"in depth-ﬁrst, left-to-right order. Rabinovich et al. (2017) propose a modular decoder whose submodels are dynamically composed according to the generated tree structure. Our own work also aims to model the structure of meaning representations more faithfully. The ﬂexibility of our approach enables us to easily apply sketches to different types of meaning representations, e.g., trees or other structured objects. Coarse-to-ﬁne methods have been popular in the NLP literature, and are perhaps best known for syntactic parsing (Charniak et al., 2006; Petrov, 2011). Artzi and Zettlemoyer (2013) and Zhang et al. (2017) use coarse lexical entries or macro grammars to reduce the search space of semantic parsers. Compared with coarse-to-ﬁne inference for lexical induction, sketches in our case are abstractions of the ﬁnal meaning representation. The idea of using sketches as intermediate representations has also been explored in the ﬁeld of program synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty"
P18-1068,N15-1162,0,0.512991,"ram synthesis (Solar-Lezama, 2008; Zhang and Sun, 2013; Feng et al., 2017). Yaghmazadeh et al. (2017) use S EMPRE (Berant et al., 2013) to map a sentence into SQL sketches which are completed using program synthesis techniques and iteratively repaired if they are faulty. Related Work Various models have been proposed over the years to learn semantic parsers from natural language expressions paired with their meaning representations (Tang and Mooney, 2000; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). These systems typically learn lexicalized mapping rules and scoring models to construct a meaning representation for a given input. More recently, neural sequence-to-sequence models have been applied to semantic parsing with promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), eschewing the need for extensive feature engineering. Several ideas have been explored to enhance the performance of these models such as data augmentation (Koˇcisk´y et al., 2016; Jia and Liang, 2016), transfer learning (Fan et al., 2017), sharing parameters for multiple languages or mean"
P18-1068,P11-1060,0,\N,Missing
P18-1069,C04-1046,0,0.0758888,"santo and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop"
P18-1069,P17-2098,0,0.026462,"have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; So"
P18-1069,P17-1089,0,0.0291545,"(Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and ques"
P18-1069,P16-1002,0,0.32272,"dence model signiﬁcantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores. 1 Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries). The neural sequenceto-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), neural semantic parsers remain difﬁcult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the ∗ Work carried out during an internship at Microsoft Research. 743 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We compute these conﬁdence metrics for a given prediction and use th"
P18-1069,P16-1004,1,0.939878,"ults show that our conﬁdence model signiﬁcantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores. 1 Introduction Semantic parsing aims to map natural language text to a formal meaning representation (e.g., logical forms or SQL queries). The neural sequenceto-sequence architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has been widely adopted in a variety of natural language processing tasks, and semantic parsing is no exception. However, despite achieving promising results (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016), neural semantic parsers remain difﬁcult to interpret, acting in most cases as a black box, not providing any information about what made them arrive at a particular decision. In this work, we explore ways to estimate and interpret the ∗ Work carried out during an internship at Microsoft Research. 743 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 743–753 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics We compute these conﬁdence metrics for a given"
P18-1069,W17-2631,0,0.0143956,"do indeed produce uncertain outputs, which we would like our framework to identify. A widely-used conﬁdence scoring method is based on posterior probabilities p (y|x) where x is the input and y the model’s prediction. For a linear model, this method makes sense: as more positive evidence is gathered, the score becomes larger. Neural models, in contrast, learn a complicated function that often overﬁts the training data. Posterior probability is effective when making decisions about model output, but is no longer a good indicator of conﬁdence due in part to the nonlinearity of neural networks (Johansen and Socher, 2017). This observation motivates us to develop a conﬁdence modeling framework for sequenceto-sequence models. We categorize the causes of uncertainty into three types, namely model uncertainty, data uncertainty, and input uncertainty and design different metrics to characterize them. In this work we focus on conﬁdence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate conﬁdence scores that indicate whether model predictions"
P18-1069,P18-1068,1,0.721265,"Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical"
P18-1069,D16-1116,0,0.0361875,"Missing"
P18-1069,W17-2607,0,0.0279905,"; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To t"
P18-1069,D17-1160,0,0.0356097,"ores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the c"
P18-1069,D11-1140,0,0.172747,"Missing"
P18-1069,P17-1030,0,0.0311205,"an estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framework which shows that the use of dropout in neural networks can be interpreted as a Bayesian approximation of Gaussian Process. We adapt their framework so as to represent uncertainty in the encoder-decoder architectures, and extend it by adding Gaussian noise to weights. 3 Neural Semantic Parsing Model In the following section we describe the neural semantic parsing model (Dong"
P18-1069,P16-1057,0,0.0299597,"Missing"
P18-1069,P13-2121,0,0.0350755,"Missing"
P18-1069,D08-1082,0,0.0305941,"Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s output vectors; iii) bridge vectors; and iv) de"
P18-1069,D15-1166,0,0.0607755,"o estimate “what we do not know”. To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a). of qt . Once the tokens of the input sequence are encoded into vectors, e|q |is used to initialize the hidden states of the ﬁrst time step in the decoder. Similarly, the hidden vector of the decoder at time step t is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the word vector of the previously predicted token. Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: rt,k ∝ exp{dt · ek } 4.1 The model’s parameters or structures contain uncertainty, which makes the model less conﬁdent about the values of p (a|q). For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below: Dropout Perturbation Our ﬁrst metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to es"
P18-1069,P17-2007,0,0.0191705,"nce-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004"
P18-1069,P15-1002,0,0.0402218,"o estimate “what we do not know”. To this end, we identify three causes of uncertainty, and design various metrics characterizing each one of them. We then feed these metrics into a regression model in order to predict s (q, a). of qt . Once the tokens of the input sequence are encoded into vectors, e|q |is used to initialize the hidden states of the ﬁrst time step in the decoder. Similarly, the hidden vector of the decoder at time step t is computed by dt = fLSTM (dt−1 , at−1 ), where at−1 ∈ Rn is the word vector of the previously predicted token. Additionally, we use an attention mechanism (Luong et al., 2015a) to utilize relevant encoder-side context. For the current time step t of the decoder, we compute its attention score with the k-th hidden state in the encoder as: rt,k ∝ exp{dt · ek } 4.1 The model’s parameters or structures contain uncertainty, which makes the model less conﬁdent about the values of p (a|q). For example, noise in the training data and the stochastic learning algorithm itself can result in model uncertainty. We describe metrics for capturing uncertainty below: Dropout Perturbation Our ﬁrst metric uses dropout (Srivastava et al., 2014) as approximate Bayesian inference to es"
P18-1069,W00-1317,0,0.0545413,"al networks with long short-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s"
P18-1069,H05-1096,0,0.10971,"Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framewo"
P18-1069,P16-1127,0,0.0949186,"atively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Rela"
P18-1069,P15-1085,1,0.857422,"trics for a given prediction and use them as features in a regression model which is trained on held-out data to ﬁt prediction F1 scores. At test time, the regression model’s outputs are used as conﬁdence scores. Our approach does not interfere with the training of the model, and can be thus applied to various architectures, without sacriﬁcing test accuracy. Furthermore, we propose a method based on backpropagation which allows to interpret model behavior by identifying which parts of the input contribute to uncertain predictions. Experimental results on two semantic parsing datasets (I FTTT, Quirk et al. 2015; and D JANGO, Oda et al. 2015) show that our model is superior to a method based on posterior probability. We also demonstrate that thresholding conﬁdence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and s"
P18-1069,P17-1041,0,0.135118,"based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimat"
P18-1069,P17-1105,0,0.0122733,"retable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapata, 2018), network sharing (Susanto and Lu, 2017; Herzig and Berant, 2017), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estima"
P18-1069,D07-1071,0,0.115715,"hort-term memory units (LSTMs; Hochreiter and Schmidhuber 1997) which process tokens sequentially. The probability of generating the whole sequence p (a|q) is factorized as: p (a|q) = |a|  p (at |a&lt;t , q) (1) t=1 where a&lt;t = a1 · · · at−1 . Let et ∈ Rn denote the hidden vector of the encoder at time step t. It is computed via et = fLSTM (et−1 , qt ), where fLSTM refers to the LSTM unit, and qt ∈ Rn is the word embedding Semantic Parsing Various methods have been developed to learn a semantic parser from natural language descriptions paired with meaning representations (Tang and Mooney, 2000; Zettlemoyer and Collins, 2007; Lu et al., 2008; Kwiatkowski 744 ܽଵ … … ܽȁȁ Algorithm 1 Dropout Perturbation iv) ଵ i) Input: q, a: Input and its prediction M: Model parameters 1: for i ← 1, · · · , F do ˆ i ← Apply dropout layers to M  Figure 1 2: M ˆ i) 3: Run forward pass and compute pˆ(a|q; M  ii) ݍଵ ଶ … ȁȁ ݍଶ … ݍȁȁ iii) i) ଵ ଶ … &lt;s&gt; ܽଵ … ȁȁ ܽ  ିଵ ˆ i )}F 4: Compute variance of {ˆ p(a|q; M i=1  Equation (6) Figure 1: We use dropout as approximate Bayesian inference to obtain model uncertainty. The dropout layers are applied to i) token vectors; ii) the encoder’s output vectors; iii) bridge ve"
P18-1069,D17-1298,0,0.0270318,"ground truth; we compute the overlap between tokens identiﬁed as contributing to uncertainty by our method and those found in the gold standard. Overlap is shown for top 2 and 4 tokens. Best results are in bold. 7 Conclusions In this paper we presented a conﬁdence estimation model and an uncertainty interpretation method for neural semantic parsing. Experimental results show that our method achieves better performance than competitive baselines on two datasets. Directions for future work are many and varied. The proposed framework could be applied to a variety of tasks (Bahdanau et al., 2015; Schmaltz et al., 2017) employing sequence-to-sequence architectures. We could also utilize the conﬁdence estimation model within an active learning framework for neural semantic parsing. google calendar−any event starts THEN facebook −create a status message−(status message ({description})) ATT post calendar event to facebook BP post calendar event to facebook feed−new feed item−(feed url( url sports.espn.go.com)) THEN ... ATT espn mlb headline to readability BP espn mlb headline to readability weather−tomorrow’s low drops below−(( temperature(0)) (degrees in(c))) THEN ... ATT warn me when it’s going to be freezing"
P18-1069,P10-1063,0,0.0574285,"7), user feedback (Iyer et al., 2017), and transfer learning (Fan et al., 2017). Current semantic parsers will by default generate some output for a given input even if this is just a random guess. System results can thus be somewhat unexpected inadvertently affecting user experience. Our goal is to mitigate these issues with a conﬁdence scoring model that can estimate how likely the prediction is correct. Related Work Conﬁdence Estimation Conﬁdence estimation has been studied in the context of a few NLP tasks, such as statistical machine translation (Blatz et al., 2004; Uefﬁng and Ney, 2005; Soricut and Echihabi, 2010), and question answering (Gondek et al., 2012). To the best of our knowledge, conﬁdence modeling for semantic parsing remains largely unexplored. A common scheme for modeling uncertainty in neural networks is to place distributions over the network’s weights (Denker and Lecun, 1991; MacKay, 1992; Neal, 1996; Blundell et al., 2015; Gan et al., 2017). But the resulting models often contain more parameters, and the training process has to be accordingly changed, which makes these approaches difﬁcult to work with. Gal and Ghahramani (2016) develop a theoretical framework which shows that the use o"
P18-1069,N15-1162,0,0.0310473,"nterpret model behavior by identifying which parts of the input contribute to uncertain predictions. Experimental results on two semantic parsing datasets (I FTTT, Quirk et al. 2015; and D JANGO, Oda et al. 2015) show that our model is superior to a method based on posterior probability. We also demonstrate that thresholding conﬁdence scores achieves a good trade-off between coverage and accuracy. Moreover, the proposed uncertainty backpropagation method yields results which are qualitatively more interpretable compared to those based on attention scores. 2 et al., 2011; Andreas et al., 2013; Zhao and Huang, 2015). More recently, a few sequence-to-sequence models have been proposed for semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016) and shown to perform competitively whilst eschewing the use of templates or manually designed features. There have been several efforts to improve these models including the use of a tree decoder (Dong and Lapata, 2016), data augmentation (Jia and Liang, 2016; Koˇcisk´y et al., 2016), the use of a grammar model (Xiao et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Krishnamurthy et al., 2017), coarse-toﬁne decoding (Dong and Lapa"
P18-1069,P13-2009,0,\N,Missing
P18-1188,P16-1046,1,0.880852,"re large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence"
P18-1188,D15-1042,0,0.0909022,"Missing"
P18-1188,D14-1181,0,0.010422,"y and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017). The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below. The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues. Sentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a se"
P18-1188,P15-1107,0,0.0268866,"e times each. The max-pooling over time operation yields two feature lists f K2 and f K4 ∈ R3 . The final sentence embeddings have six dimensions. Document encoder Document Encoder The document encoder composes a sequence of sentences to obtain a document representation. We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997). Given a document D consisting of a sequence of sentences (s1 , s2 , . . . , sn ), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015). Sentence Extractor Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues. It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer. Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder). Instead, our extractor attends to a sequence of p pieces of external information E : (e1 , e2 , ...,"
P18-1188,N03-1020,0,0.198902,"e table present different variants of XN ET. We experimented with three types of external information: title (TITLE), image captions (CAPTION) and the first sentence (FS) of the document. The bottom block of the table presents models with more than one type of external information. The best performing model (highlighted in boldface) is used on the test set. ata (2016) report only on the DailyMail dataset. We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5 Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6 Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency. In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously. teresting direction of research but we do not pursue it here. It requires decoding with multiple types of attentions and this is not the focus of this paper. 5 We are unable to compare our results to the"
P18-1188,D15-1106,0,0.0230143,"es in problems such as language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehensio"
P18-1188,D16-1147,0,0.349286,"posed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the do"
P18-1188,N18-1158,1,0.835886,"on (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words. Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document. Our model explicitly biases the extractor with external cues and 2020 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics implicitly bias"
P18-1188,D16-1264,0,0.264849,"each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other. 2 Document Modeling For Sentence Extraction Given a document D consisting of a sequence of n sentences (s1 , s2 , ..., sn ) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise. Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 20"
P18-1188,D15-1044,0,0.105793,"Missing"
P18-1188,P17-1099,0,0.479627,"et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the ar"
P18-1188,P17-1108,0,0.515521,"t al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed"
P18-1188,Q16-1019,0,0.271774,"do not use this information. We also conduct a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete. Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior"
P18-1188,N16-1090,0,0.0300827,"e second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide docum"
P18-1188,P16-1125,0,0.0303368,"long-term dependencies in problems such as language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine r"
P18-1188,P17-1018,0,0.0276416,"well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their cons"
P18-1188,C16-1127,0,0.0908422,"t a human evaluation to judge which type of summary participants prefer. Our results overwhelmingly show that human subjects find our summaries more informative and complete. Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016). Our model with ISF and IDF scores as external features achieves competitive results for answer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016). We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where ea"
P18-1188,K17-1028,0,0.0234859,"RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017). In this paper, we formalize the use of external information to further guide document modeling for end goals. We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.” Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor. Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representat"
P18-1188,D15-1237,0,0.0852545,"Missing"
P18-1188,N16-1174,0,0.0542042,"language modeling. However, document modeling, a key to many natural language ∗ The first three authors made equal contributions to this paper. The work was done when the second author was visiting Edinburgh. 1 Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016). Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity. Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level. It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 20"
P19-1195,J04-4001,0,0.049962,"urage further work in this area; a comprehensive evaluation and comparison study which highlights the merits and shortcomings of various recently proposed datato-text generation models on two datasets. 2 Related Work The sports domain has attracted considerable attention since the early days of generation systems (Robin, 1994; Tanaka-Ishii et al., 1998). Likewise, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. Ji et al. (2017) make use of dynamic entity representations for language modeling. And Clark et al. (2018) extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamic"
P19-1195,D16-1032,0,0.0307624,"en shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. Ji et al. (2017) make use of dynamic entity representations for language modeling. And Clark et al. (2018) extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamically represent and condition on multiple entities in parallel. Kiddon et al. (2016) make use of fixed entity representations to improve the coverage and coherence of the output for recipe generation. Bosselut et al. (2018) model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during generation. Henaff et al. (2017) make use of dynamic entity representations in machine reading. Entity representations are scored against a query vector to directly predict an output class or combined as a weighted sum followed by softmax over the vocabulary. We make use of a similar entity representation model, ext"
P19-1195,P17-4012,0,0.0845709,"Missing"
P19-1195,D16-1128,0,0.22009,"Missing"
P19-1195,J95-2003,0,0.824578,"aramanis et al., 2004). Without knowing anything about baseball or how game summaries are typically written, a glance at the text in Figure 1 reveals that it is about a few entities, namely players who had an important part in the game (e.g., Brad Keller, Hunter Dozier) and their respective teams (e.g., Orioles, Royals). The prominent role of entities in achieving discourse coherence has been long recognized within the linguistic and cognitive science literature (Kuno, 1972; Chafe, 1976; Halliday and Hasan, 1976; Karttunen, 1976; Clark and Haviland, 1977; Prince, 1981), with Centering Theory (Grosz et al., 1995) being most prominent at formalizing how entities are linguistically realized and distributed in texts. In this work we propose an entity-centric neural architecture for data-to-text generation. Instead of treating entities as ordinary tokens, we create entity-specific representations (i.e., for players and teams) which are dynamically updated as text is 2023 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2023–2035 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics TEAM Inn1 Inn2 Inn3 Inn4 . . . R H E . ."
P19-1195,P16-1014,0,0.0364786,"(blocks B and C). Module fθ represents update equations (6)–(8) where θ is the set of trainable parameters. The gate represents the entity memory update (Equation (9)). Block B covers Equations (10) and (11), and block C Equations (12) and (13). where Wy ∈ R|Vy |×n , by ∈ R|Vy |are parameters and |Vy |is the output vocabulary size. We further augment the decoder with a copy mechanism i.e., the ability to copy values from the input; copy implies yt = rj,1 for some t and j (e.g., Royals, Orioles, 9, 2 in the summary in Figure 1 are copied from r). We use the conditional copy method proposed in Gulcehre et al. (2016) where a binary variable is introduced as a switch gate to indicate whether yt is copied or not. 4 To capture the fact that discourse in descriptive texts may shift from one entity to the next, e.g., some entities may be salient in the beginning of the game summary (see Brad Kelly in the text in Figure 1), others only towards the end (see Dozier in Figure 1), and a few throughout (e.g., references to teams), we update entity representations at each time step during decoding. We use gate γ t to indicate whether there should be an update in the entity representation: Entity Memory and Hierarchic"
P19-1195,D15-1166,0,0.189521,"Missing"
P19-1195,D17-1195,0,0.0425517,"of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and neural network models. Ji et al. (2017) make use of dynamic entity representations for language modeling. And Clark et al. (2018) extend this work by adding entity context as input to the decoder. Both approaches condition on a single entity at a time, while we dynamically represent and condition on multiple entities in parallel. Kiddon et al. (2016) make use of fixed entity representations to improve the coverage and coherence of the output for recipe generation. Bosselut et al. (2018) model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during gene"
P19-1195,N16-1086,0,0.408435,"tion is the task of generating textual output from non-linguistic input (Reiter and Dale, 1997; Gatt and Krahmer, 2018). The input may take on several guises including tables of records, simulations of physical systems, spreadsheets, and so on. As an example, Figure 1 shows (in a table format) the scoring summary of a major league baseball (MLB) game, a play-by-play summary with details of the most important events in the game recorded chronologically (i.e., in which play), and a human-written summary. Modern approaches to data-to-text generation have shown great promise (Lebret et al., 2016; Mei et al., 2016; Perez-Beltrachini and Lapata, 2018; Puduppully et al., 2019; Wiseman et al., 1 Our code and dataset can be found at https:// github.com/ratishsp/data2text-entity-py. 2017) thanks to the use of large-scale datasets and neural network models which are trained end-toend based on the very successful encoder-decoder architecture (Bahdanau et al., 2015). In contrast to traditional methods which typically implement pipeline-style architectures (Reiter and Dale, 2000) with modules devoted to individual generation components (e.g., content selection or lexical choice), neural models have no special-p"
P19-1195,P04-1050,0,0.031904,"(e.g., content selection or lexical choice), neural models have no special-purpose mechanisms for ensuring how to best generate a text. They simply rely on representation learning to select content appropriately, structure it coherently, and verbalize it grammatically. In this paper we are interested in the generation of descriptive texts such as the game summary shown in Figure 1. Descriptive texts are often characterized as “entity coherent” which means that their coherence is based on the way entities (also known as domain objects or concepts) are introduced and discussed in the discourse (Karamanis et al., 2004). Without knowing anything about baseball or how game summaries are typically written, a glance at the text in Figure 1 reveals that it is about a few entities, namely players who had an important part in the game (e.g., Brad Keller, Hunter Dozier) and their respective teams (e.g., Orioles, Royals). The prominent role of entities in achieving discourse coherence has been long recognized within the linguistic and cognitive science literature (Kuno, 1972; Chafe, 1976; Halliday and Hasan, 1976; Karttunen, 1976; Clark and Haviland, 1977; Prince, 1981), with Centering Theory (Grosz et al., 1995) be"
P19-1195,P02-1040,0,0.108954,"+Gate 31.84 91.97 RW Results Automatic Evaluation We first discuss the results of automatic evaluation using the metrics defined in Wiseman et al. (2017). Let yˆ be the gold output and y the model output. Relation Generation measures how factual y is compared to input r. Specifically, it measures the precision and number of relations extracted from y which are also found in r. Content Selection measures the precision and recall of relations between yˆ and y. Content Ordering measures the DamerauLevenshtein distance between relations in y and relations in yˆ. In addition, we also report BLEU (Papineni et al., 2002) with the gold summaries as reference. Table 2 (top) summarizes our results on the RO TOW IRE test set (results on the development set are available in the Appendix). We report results for our dynamic entity memory model (ENT), the best system of Wiseman et al. (2017) (WS2017) which is an encoder-decoder model with conditional copy, and NCP+CC (Puduppully et al., 2019). We see that ENT achieves scores comparable to NCP+CC, but performs better on the metrics of RG precision, CS precision, and CO. ENT achieves substantially higher scores in CS precision compared to WS-2017 and NCP+CC, without an"
P19-1195,N18-1137,1,0.909429,"Missing"
P19-1195,P98-2209,0,0.0811669,"ons in this work are three-fold: a novel entity-aware model for data-to-text generation which is linguistically motivated, yet resource lean (no preprocessing is required, e.g., to extract document plans); a new dataset for data-to-text generation which we hope will encourage further work in this area; a comprehensive evaluation and comparison study which highlights the merits and shortcomings of various recently proposed datato-text generation models on two datasets. 2 Related Work The sports domain has attracted considerable attention since the early days of generation systems (Robin, 1994; Tanaka-Ishii et al., 1998). Likewise, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004). Modeling entities and their communicative actions has also been shown to improve system output in interactive storytelling 2024 (Cavazza et al., 2002; Cavazza and Charles, 2005) and dialogue generation (Walker et al., 2011). More recently, the benefits of modeling entities explicitly have been demonstrated in various tasks and ne"
P19-1195,N16-1174,0,0.0567619,"8) model actions and their effects on entities for the same task. However, in contrast to our work, they keep entity representations fixed during generation. Henaff et al. (2017) make use of dynamic entity representations in machine reading. Entity representations are scored against a query vector to directly predict an output class or combined as a weighted sum followed by softmax over the vocabulary. We make use of a similar entity representation model, extend it with hierarchical attention and apply it to data-to text generation. The hierarchical attention mechanism was first introduced in Yang et al. (2016) as a way of learning document-level representations. We apply attention over records and subsequently over entity memories. Several models have been proposed in the last few years for data-to-text generation (Mei et al. 2016; Lebret et al. 2016; Wiseman et al. 2017, inter alia) based on the very successful encoderdecoder architecture (Bahdanau et al., 2015). Various attempts have also been made to improve these models, e.g., by adding content selection (PerezBeltrachini and Lapata, 2018) and content planning (Puduppully et al., 2019) mechanisms. However, we are not aware of any prior work in"
P19-1195,D17-1239,0,\N,Missing
P19-1195,N18-1204,0,\N,Missing
P19-1195,C69-7001,0,\N,Missing
P19-1195,C69-6902,0,\N,Missing
P19-1195,C98-2204,0,\N,Missing
P19-1500,J05-3002,0,0.342095,"te the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-s"
P19-1500,P15-1153,0,0.06386,"ased on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document s"
P19-1500,N18-1150,0,0.265358,"years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Multi-document summarization — the task of producing summaries from clusters of themati1 Our code and data is available at https://github. com/nlpyang/hiersumm. cally related documents — has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are"
P19-1500,N13-1136,0,0.315194,"Missing"
P19-1500,J10-3005,1,0.796189,"ncreasing training time. Table 3 summarizes ablation studies aiming to assess the contribution of individual components. Our experiments confirmed that encoding paragraph position in addition to token position within each paragraph is beneficial (see row w/o PP), as well as multi-head pooling (w/o MP is a model where the number of heads is set to 1), and the global transformer layer (w/o GT is a model with only 5 local transformer layers in the encoder). four questions per gold summary. Examples of questions and their answers are given in Table 5. We adopted the same scoring mechanism used in Clarke and Lapata (2010), i.e., correct answers are marked with 1, partially correct ones with 0.5, and 0 otherwise. A system’s score is the average of all question scores. Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments on 20 randomly selected test instances. Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption"
P19-1500,D08-1019,0,0.21173,"e of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained"
P19-1500,D18-1443,0,0.277678,"ork models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Multi-document summarization — the task of producing summaries from clusters of themati1 Our code and data is available at https://github. com/nlpyang/hiersumm. cally related documents — has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferences (DUC and TAC), but are relatively small (in the range of a few hundr"
P19-1500,N18-1065,0,0.11036,"Missing"
P19-1500,P17-2074,0,0.0415555,"from two to 2 This was not the case with the other Transformer models. Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them taking into account the following criteria: Informativeness (does the summary convey important facts about the topic in question?), Fluency (is the summary fluent and grammatical?), and Succinctness (does the summary avoid repetition?). We used Best-Worst Scaling (Louviere et al., 2015), a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales (Kiritchenko and Mohammad, 2017). Participants were presented with the gold summary and summaries generated from 3 out of 4 systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard, taking into account the criteria mentioned above. The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst. Ratings range from −1 (worst) to 1 (best). Both evaluations were conducted on the Amazon Mechanical Turk platform with 5 responses per hit. Participants evaluated summaries produced by the Lead baseline, th"
P19-1500,D18-2012,0,0.0322323,"aphs, and each paragraph has 70.1 tokens. The average length of the target summary is 139.4 tokens. We split the dataset with 1, 579, 360 instances for training, 38, 144 for validation and 38, 205 for test. ROUGE-L Recall L0 = 5 L0 = 10 L0 = 20 L0 = 40 Similarity 24.86 32.43 40.87 49.49 Ranking 39.38 46.74 53.84 60.42 Methods Table 1: ROUGE-L recall against target summary for L0 -best paragraphs obtained with tf-idf cosine similarity and our ranking model. For both ranking and summarization stages, we encode source paragraphs and target summaries using subword tokenization with SentencePiece (Kudo and Richardson, 2018). Our vocabulary consists of 32, 000 subwords and is shared for both source and target. Paragraph Ranking To train the regression model, we calculated the ROUGE-2 recall (Lin, 2004) of each paragraph against the target summary and used this as the ground-truth score. The hidden size of the two LSTMs was set to 256, and dropout (with dropout probability of 0.2) was used before all linear layers. Adagrad (Duchi et al., 2011) with learning rate 0.15 is used for optimization. We compare our ranking model against the method proposed in Liu et al. (2018) who use the tf-idf cosine similarity between"
P19-1500,D18-1387,0,0.0197233,"ese into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2018). Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summarization as a supervised"
P19-1500,W04-1013,0,0.279494,"for test. ROUGE-L Recall L0 = 5 L0 = 10 L0 = 20 L0 = 40 Similarity 24.86 32.43 40.87 49.49 Ranking 39.38 46.74 53.84 60.42 Methods Table 1: ROUGE-L recall against target summary for L0 -best paragraphs obtained with tf-idf cosine similarity and our ranking model. For both ranking and summarization stages, we encode source paragraphs and target summaries using subword tokenization with SentencePiece (Kudo and Richardson, 2018). Our vocabulary consists of 32, 000 subwords and is shared for both source and target. Paragraph Ranking To train the regression model, we calculated the ROUGE-2 recall (Lin, 2004) of each paragraph against the target summary and used this as the ground-truth score. The hidden size of the two LSTMs was set to 256, and dropout (with dropout probability of 0.2) was used before all linear layers. Adagrad (Duchi et al., 2011) with learning rate 0.15 is used for optimization. We compare our ranking model against the method proposed in Liu et al. (2018) who use the tf-idf cosine similarity between each paragraph and the article title to rank the input paragraphs. We take the first L0 paragraphs from the ordered paragraph set produced by our ranker and the similarity-based met"
P19-1500,Q18-1005,1,0.699319,"ion with weight Wc ∈ Rd∗d : ci = Wc [context1i ; · · · ; contextni head ] (21) where Wo1 ∈ Rdf f ∗d and Wo2 ∈ Rd∗df f are the weights, df f is the hidden size of the feed-forward later. This way, each token within paragraph Ri can collect information from other paragraphs in a hierarchical and efficient manner. 3.2.4 Graph-informed Attention The inter-paragraph attention mechanism can be viewed as learning a latent graph representation (self-attention weights) of the input paragraphs. Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks (Liu and Lapata, 2018; Kim et al., 2017; Williams et al., 2018; Niculae et al., 2018; Fernandes et al., 2019), much work in multi-document summarization has taken advantage of explicit graph representations, each focusing on different facets of the summarization task 5074 (e.g., capturing redundant information or representing passages referring to the same event or entity). One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model, to generate better summaries. We experimented with two well-established graph representations which we discuss briefly below. However,"
P19-1500,C16-1143,0,0.0667403,"Missing"
P19-1500,N18-1158,1,0.84883,"answers are given in Table 5. We adopted the same scoring mechanism used in Clarke and Lapata (2010), i.e., correct answers are marked with 1, partially correct ones with 0.5, and 0 otherwise. A system’s score is the average of all question scores. Human Evaluation In addition to automatic evaluation, we also assessed system performance by eliciting human judgments on 20 randomly selected test instances. Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question-answering (QA) paradigm (Clarke and Lapata, 2010; Narayan et al., 2018). We created a set of questions based on the gold summary under the assumption that it contains the most important information from the input paragraphs. We then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary. The more questions a system can answer, the better it is at summarization. We created 57 questions in total varying from two to 2 This was not the case with the other Transformer models. Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them takin"
P19-1500,D18-1108,0,0.0285352,"tni head ] (21) where Wo1 ∈ Rdf f ∗d and Wo2 ∈ Rd∗df f are the weights, df f is the hidden size of the feed-forward later. This way, each token within paragraph Ri can collect information from other paragraphs in a hierarchical and efficient manner. 3.2.4 Graph-informed Attention The inter-paragraph attention mechanism can be viewed as learning a latent graph representation (self-attention weights) of the input paragraphs. Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks (Liu and Lapata, 2018; Kim et al., 2017; Williams et al., 2018; Niculae et al., 2018; Fernandes et al., 2019), much work in multi-document summarization has taken advantage of explicit graph representations, each focusing on different facets of the summarization task 5074 (e.g., capturing redundant information or representing passages referring to the same event or entity). One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model, to generate better summaries. We experimented with two well-established graph representations which we discuss briefly below. However, there is nothing inherent in our model that restricts us to th"
P19-1500,W14-3703,0,0.149686,"quence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner. We represent cross-document relationships via an attention mechanism which allows to"
P19-1500,W00-1009,0,0.38217,"ges 5070–5081 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 decode the summary. Although the model of Liu et al. (2018) takes an important first step towards abstractive multidocument summarization, it still considers the multiple input documents as a concatenated flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural s"
P19-1500,P17-1099,0,0.786089,"interest in recent years, thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations. The availability of large-scale datasets (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents. Several approaches have shown promising results with sequence-to-sequence models that encode a source document and then decode it into an abstractive summary (See et al., 2017; Celikyilmaz et al., 2018; Paulus et al., 2018; Gehrmann et al., 2018). Multi-document summarization — the task of producing summaries from clusters of themati1 Our code and data is available at https://github. com/nlpyang/hiersumm. cally related documents — has received significantly less attention, partly due to the paucity of suitable data for the application of learning methods. High-quality multi-document summarization datasets (i.e., document clusters paired with multiple reference summaries written by humans) have been produced for the Document Understanding and Text Analysis Conferenc"
P19-1500,D08-1079,0,0.140383,"ted flat sequence, being agnostic of the hierarchical structures and the relations that might exist among documents. For example, different web pages might repeat the same content, include additional content, present contradictory information, or discuss the same fact in a different light (Radev, 2000). The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004; Christensen et al., 2013; Wan, 2008; Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them. In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries. Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner. We represent cross-document relationships via an attentio"
P19-1500,Q18-1019,0,0.0566491,"Missing"
P19-1500,K17-1045,0,0.193695,"hods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links. Abstractive approaches have met with limited success. A few systems generate summaries based on sentence fusion, a technique which identifies fragments conveying common information across documents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have ach"
P19-1500,W18-6545,0,0.027652,"ents and combines these into sentences (Barzilay and McKeown, 2005; Filippova and Strube, 2008; Bing et al., 2015). Although neural abstractive models have achieved promising results on single-document summarization (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018), the extension of sequence-to-sequence architectures to multi-document summarization is less straightforward. Apart from the lack of sufficient training data, neural models also face the computational challenge of processing multiple source documents. Previous solutions include model transfer (Zhang et al., 2018; Lebanoff and Liu, 2018), where a sequence-to-sequence model is pretrained on single-document summarization data and finetuned on DUC (multi-document) benchmarks, or unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2018). Liu et al. (2018) propose a methodology for constructing large-scale summarization datasets and a two-stage model which first extracts salient information from source documents and then uses a decoder-only architecture (that can attend to very long sequences) to generate the summary. We follow their setup in viewing multi-document summa"
P19-1504,N04-1015,0,0.455054,"ation and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For instance, the summary in Table 1 is about a species of damselfly; the second sentence describes the region where the species is found and the fourth the type of habitat the species lives in. We would expect other Animal Wikipedia summaries to exhibit similar content organization. In this work we propose a neural model which is guided by the topic structure of target summaries, i.e., the way content is organized into sentences and the type of"
P19-1504,N18-1150,0,0.0481031,"and human evaluation demonstrate that our summaries have better content coverage. 1 Introduction Abstractive multi-document summarization aims at generating a coherent summary from a cluster of thematically related documents. Recently, Liu et al. (2018) proposed generating the lead section of a Wikipedia article as a variant of multidocument summarization and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For instance, the summary in Table 1 is about a species of damselfly; the second sentence describe"
P19-1504,J10-3005,1,0.836706,"luation with two human-based studies carried out on Amazon Mechanical Turk (AMT) over 45 randomly selected examples from the test set (15 from each domain). We compared the TSS2S, CV-S2S and CV-S2D+T models. The first study focused on assessing the extent to which generated summaries retain salient information from the input set of paragraphs. We folModel Company QA Rank TF-S2S 5 1.87 CV-S2S 5 2.27 CV-S2D+T 7 1.87 Film QA Rank 6 2.27 6.67 1.76 7 1.98 Animal QA Rank 9 1.87 8.33 2.04 9.33 2.09 Table 5: QA-based evaluation and system ranking. lowed a question-answering (QA) scheme as proposed in Clarke and Lapata (2010). Under this scheme, a set of questions are created based on the gold summary; participants are then asked to answer these questions by reading system summaries alone without access to the input. The more questions a system can answer, the better it is at summarizing the input paragraphs as a whole (see Appendix A for example questions). Correct answers are given a score of 1, partially correct answers score 0.5, and zero otherwise. The final score is the average of all question scores. We created between two and four factoid questions for each summary; a total of 40 questions for each domain."
P19-1504,W17-3518,1,0.815765,"isting sequential decoders on three data sets representing different domains. Automatic and human evaluation demonstrate that our summaries have better content coverage. 1 Introduction Abstractive multi-document summarization aims at generating a coherent summary from a cluster of thematically related documents. Recently, Liu et al. (2018) proposed generating the lead section of a Wikipedia article as a variant of multidocument summarization and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For ins"
P19-1504,P15-1107,0,0.0199086,"ly unexplored within neural text generation, it has been been recognized as useful for summarization. Barzilay and Lee (2004) build a model of the content structure of source documents and target summaries and use it to extract salient facts from the source. Sauper and Barzilay (2009) cluster texts by target topic and use a global optimisation algorithm to select the best combination of facts from each cluster. Although these models have shown good results in terms of content selection, they cannot generate target summaries. Our model is also related to the hierarchical decoding approaches of Li et al. (2015) and Tan et al. (2017). However, the former approach is auto-encoding the same inputs (our model carries out content selection for the summarization task), while the latter generates independent sentences. They also both rely on recurrent neural models, while we use convolutional neural networks. To our knowledge this is the first hierarchical decoder proposed for a non-recurrent architecture. To evaluate our model, we introduce W IKI C ATS UM, a dataset1 derived from Liu et al. (2018) 1 Our dataset and code are available at https:// 5107 Proceedings of the 57th Annual Meeting of the Associati"
P19-1504,W04-1013,0,0.142034,"Missing"
P19-1504,D15-1166,0,0.0510967,"st with the previous target embedding gti : dlti = Wdl (olti + st ) + gti altij = P exp(dlti • zj ) • z 0) j l j 0 exp(dti clti = |X | X altij (zj + ej ) (7) (8) (9) j=1 (3) (4) j=1 s where αjt is the attention weight for the document-level decoder attending to input token xj at time step t. 3.2 l−1 {olt1 , · · · , oltn } = conv({o0 t1 , · · · , o0 tn ) (5) where ht is the LSTM hidden state of step t and cst is the context vector computed by attending to the input. The initial hidden state h0 is initialized with the averaged sum of the encoder output states. We use a soft attention mechanism (Luong et al., 2015) to compute the context vector cst : s αtj =P Hierarchical Convolutional Decoder Sentence-level Decoder Each sentence st = (yt1 , . . . , yt|st |) in target summary S is generated by a sentence-level decoder. The convolutional architecture proposed in Gehring et al. (2017) combines word embeddings with positional embeddings. That is, the word representation wti of each target word yti is combined with vector ei indicating where this word is in the sentence, wti = emb(yti ) + ei . We extend this The prediction of word yti is conditioned on the output vectors of the top convolutional layer, as L"
P19-1504,P14-5010,0,0.00587121,"Missing"
P19-1504,W18-6501,1,0.809122,"ge. 1 Introduction Abstractive multi-document summarization aims at generating a coherent summary from a cluster of thematically related documents. Recently, Liu et al. (2018) proposed generating the lead section of a Wikipedia article as a variant of multidocument summarization and released WikiSum, a large-scale summarization dataset which enables the training of neural models. Like most previous work on neural text generation (Gardent et al., 2017; See et al., 2017; Wiseman et al., 2017; Puduppully et al., 2019; Celikyilmaz et al., 2018; Liu et al., 2018; PerezBeltrachini and Lapata, 2018; Marcheggiani and Perez-Beltrachini, 2018), Liu et al. (2018) represent the target summaries as a single long sequence, despite the fact that documents are organized into topically coherent text segments, exhibiting a specific structure in terms of the content they discuss (Barzilay and Lee, 2004). This is especially the case when generating text within a specific domain where certain topics might be discussed in a specific order (Wray, 2002). For instance, the summary in Table 1 is about a species of damselfly; the second sentence describes the region where the species is found and the fourth the type of habitat the species lives in."
P19-1504,D18-1206,1,0.873442,"ls github.com/lauhaide/WikiCatSum. Generation with Content Guidance Our model takes as input a set of ranked paragraphs P = {p1 · · · p|P |} which we concatenate to form a flat input sequence X = (x1 · · · x|X |) where xi is the i-th token. The output of the model is a multi-sentence summary S = (s1 , · · · , s|S |) where st denotes the t-th sentence. We adopt an encoder-decoder architecture which makes use of convolutional neural networks (CNNs; Gehring et al. 2017). CNNs permit parallel training (Gehring et al., 2017) and have shown good performance in abstractive summarization tasks (e.g., Narayan et al. 2018). Figure 1 illustrates the architecture of our model. We use the convolutional encoder of Gehring et al. (2017) to obtain a sequence of states (z1 , · · · , z|X |) given an input sequence of tokens (x1 , · · · , x|X |). A hierarchical convolutional decoder generates the target sentences (based on the encoder outputs). Specifically, a document-level decoder first generates sentence vectors (LSTM Document Decoder in Figure 1), representing the content specification for each sentence that the model plans to decode. A sentence-level decoder (CNN Sentence Decoder in Figure 1) is then applied to gen"
P19-1504,P17-1108,0,0.113283,"Missing"
P19-1504,P09-1024,0,\N,Missing
P19-1504,Q16-1029,0,\N,Missing
P19-1504,P17-1099,0,\N,Missing
P19-1504,D17-1239,0,\N,Missing
P19-1628,D16-1035,0,0.0292162,"essively larger discourse units, ultimately covering the entire document. Discourse units are linked to each other by rhetorical relations (e.g., Contrast, Elaboration) and are further characterized in terms of their text importance: nuclei denote central segments, whereas satellites denote peripheral ones. The notion of nuclearity has been leveraged extensively in document summarization (Marcu, 1997, 1998; Hirao et al., 2013) and in our case provides motivation for taking directionality into account when measuring centrality. We could determine nuclearity with the help of a discourse parser (Li et al. 2016; Feng and Hirst 2014; Joty et al. 2013; Liu and Lapata 2017, inter alia) but problematically such parsers rely on the availability of annotated corpora as well as a wider range of standard NLP tools which might not exist for different domains, languages, or text genres. We instead approximate nuclearity by relative position in the hope that sentences occurring earlier in a document should be more central. Given any two sentences si , sj (i &lt; j) taken from the same document D, we formalize this simple intuition by transforming the undirected edge weighted by the similarity score eij between si"
P19-1628,A97-1042,0,0.693328,"xt importance: nuclei denote central segments, whereas satellites denote peripheral ones. We propose a simple, yet effective approach for measuring directed centrality for single-document summarization, based on the assumption that the contribution of any two nodes’ connection to their respective centrality is influenced by their relative position. Position information has been frequently used in summarization, especially in the news domain, either as a baseline that creates a summary by selecting the first n sentences of the document (Nenkova, 2005) or as a feature in learning-based systems (Lin and Hovy, 1997; Schilder and Kondadadi, 2008; Ouyang et al., 2010). We transform undirected edges between sentences into directed ones by differentially weighting them according to their orientation. Given a pair of sentences in the same document, one is looking forward (to the sentences following it), and the other is looking backward (to the sentences preceding it). For some types of documents (e.g., news articles) one might further expect sentences occurring early on to be more central and therefore backward-looking edges to have larger weights. We evaluate the proposed approach on three single-document"
P19-1628,N03-1020,0,0.63965,"Missing"
P19-1628,D17-1133,1,0.846135,"e entire document. Discourse units are linked to each other by rhetorical relations (e.g., Contrast, Elaboration) and are further characterized in terms of their text importance: nuclei denote central segments, whereas satellites denote peripheral ones. The notion of nuclearity has been leveraged extensively in document summarization (Marcu, 1997, 1998; Hirao et al., 2013) and in our case provides motivation for taking directionality into account when measuring centrality. We could determine nuclearity with the help of a discourse parser (Li et al. 2016; Feng and Hirst 2014; Joty et al. 2013; Liu and Lapata 2017, inter alia) but problematically such parsers rely on the availability of annotated corpora as well as a wider range of standard NLP tools which might not exist for different domains, languages, or text genres. We instead approximate nuclearity by relative position in the hope that sentences occurring earlier in a document should be more central. Given any two sentences si , sj (i &lt; j) taken from the same document D, we formalize this simple intuition by transforming the undirected edge weighted by the similarity score eij between si and sj into two directed ones differentially weighted by λ1"
P19-1628,C16-1143,0,0.0605689,"Missing"
P19-1628,W97-0713,0,0.808827,"of largescale datasets containing hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It therefore comes as no surprise that unsupervised approaches have been the subject of much previous research (Marcu, 1997; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017). A very popular algorithm for extractive single-document summarization is TextRank (Mihalcea and Tarau, 2004); it represents document sentences as nodes in a graph with undirected edges whose weights are computed based on sentence similarity. In order to decide which sentence to include in the summary, a node’s centrality is often measured using graph-based ranking algorithms such as PageRank (Bri"
P19-1628,W98-1124,0,0.114128,"Missing"
P19-1628,P04-3020,0,0.346928,"e used as a standard comparison when assessing the merits of more sophisticated supervised approaches over and above the baseline of extracting the leading sentences (which our model outperforms). Taken together, our results indicate that directed centrality improves the selection of salient content substantially. Interestingly, its significance for unsupervised summarization has gone largely unnoticed in the research community. For example, gensim (Barrios et al., 2016), a widely used open-source implementation of TextRank only supports building undirected graphs, even though follow-on work (Mihalcea, 2004) experiments with position-based directed graphs similar to ours. Moreover, our approach highlights the effectiveness of pretrained embeddings for the summarization task, and their promise for the development of unsupervised methods in the future. We are not aware of any previous neural-based approaches to unsupervised single-document summarization, although some effort has gone into developing unsupervised models for multi-document summarization using reconstruction objectives (Li et al., 2017; Ma et al., 2016; Chu and Liu, 2018). 2 2.1 Centrality-based Summarization Undirected Text Graph A p"
P19-1628,K16-1028,0,0.0539828,"apture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.1 1 Introduction Single-document summarization is the task of generating a shorter version of a document while retaining its most important content (Nenkova et al., 2011). Modern neural network-based approaches (Nallapati et al., 2016; Paulus et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016; See et al., 2017; Narayan et al., 2018b; Gehrmann et al., 2018) have achieved promising results thanks to the availability of largescale datasets containing hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summa"
P19-1628,D18-1206,1,0.677896,"nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.1 1 Introduction Single-document summarization is the task of generating a shorter version of a document while retaining its most important content (Nenkova et al., 2011). Modern neural network-based approaches (Nallapati et al., 2016; Paulus et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016; See et al., 2017; Narayan et al., 2018b; Gehrmann et al., 2018) have achieved promising results thanks to the availability of largescale datasets containing hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It therefore comes as no sur"
P19-1628,N18-1158,1,0.711606,"nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.1 1 Introduction Single-document summarization is the task of generating a shorter version of a document while retaining its most important content (Nenkova et al., 2011). Modern neural network-based approaches (Nallapati et al., 2016; Paulus et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016; See et al., 2017; Narayan et al., 2018b; Gehrmann et al., 2018) have achieved promising results thanks to the availability of largescale datasets containing hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It therefore comes as no sur"
P19-1628,C10-2106,0,0.0283298,"reas satellites denote peripheral ones. We propose a simple, yet effective approach for measuring directed centrality for single-document summarization, based on the assumption that the contribution of any two nodes’ connection to their respective centrality is influenced by their relative position. Position information has been frequently used in summarization, especially in the news domain, either as a baseline that creates a summary by selecting the first n sentences of the document (Nenkova, 2005) or as a feature in learning-based systems (Lin and Hovy, 1997; Schilder and Kondadadi, 2008; Ouyang et al., 2010). We transform undirected edges between sentences into directed ones by differentially weighting them according to their orientation. Given a pair of sentences in the same document, one is looking forward (to the sentences following it), and the other is looking backward (to the sentences preceding it). For some types of documents (e.g., news articles) one might further expect sentences occurring early on to be more central and therefore backward-looking edges to have larger weights. We evaluate the proposed approach on three single-document news summarization datasets representative of differ"
P19-1628,D15-1226,0,0.124155,"less, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It therefore comes as no surprise that unsupervised approaches have been the subject of much previous research (Marcu, 1997; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017). A very popular algorithm for extractive single-document summarization is TextRank (Mihalcea and Tarau, 2004); it represents document sentences as nodes in a graph with undirected edges whose weights are computed based on sentence similarity. In order to decide which sentence to include in the summary, a node’s centrality is often measured using graph-based ranking algorithms such as PageRank (Brin and Page, 1998). In this paper, we argue that the centrality measure can be improved in two important respects. Firstly, to better capture sentential meaning an"
P19-1628,W15-2701,0,0.115169,"ccount, we leave this to future work and only consider the definition of centrality from Equation (6) in this paper. 3 Sentence Similarity Computation The key question now is how to compute the similarity between two sentences. There are many variations of the similarity function of TextRank (Barrios et al., 2016) based on symbolic sentence representations such as tf-idf. We instead employ a state-of-the-art neural representation learning model. We use BERT (Devlin et al., 2018) as our sentence encoder and fine-tune it based on a type of sentence-level distributional hypothesis (Harris, 1954; Polajnar et al., 2015) which we explain below. Fine-tuned BERT representations are subsequently used to compute the similarity between sentences in a document. 3.1 BERT as Sentence Encoder We use BERT (Bidirectional Encoder Representations from Transformers; Devlin et al. 2018) to map sentences into deep continuous representations. BERT adopts a multi-layer bidirectional Transformer encoder (Vaswani et al., 2017) and uses two unsupervised prediction tasks, i.e., masked language modeling and next sentence prediction, to pre-train the encoder. 6238 The language modeling task aims to predict masked tokens by jointly c"
P19-1628,W00-0403,0,0.813576,"e datasets containing hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It therefore comes as no surprise that unsupervised approaches have been the subject of much previous research (Marcu, 1997; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017). A very popular algorithm for extractive single-document summarization is TextRank (Mihalcea and Tarau, 2004); it represents document sentences as nodes in a graph with undirected edges whose weights are computed based on sentence similarity. In order to decide which sentence to include in the summary, a node’s centrality is often measured using graph-based ranking algorithms such as PageRank (Brin and Page, 1998). I"
P19-1628,P08-2052,0,0.0258372,"i denote central segments, whereas satellites denote peripheral ones. We propose a simple, yet effective approach for measuring directed centrality for single-document summarization, based on the assumption that the contribution of any two nodes’ connection to their respective centrality is influenced by their relative position. Position information has been frequently used in summarization, especially in the news domain, either as a baseline that creates a summary by selecting the first n sentences of the document (Nenkova, 2005) or as a feature in learning-based systems (Lin and Hovy, 1997; Schilder and Kondadadi, 2008; Ouyang et al., 2010). We transform undirected edges between sentences into directed ones by differentially weighting them according to their orientation. Given a pair of sentences in the same document, one is looking forward (to the sentences following it), and the other is looking backward (to the sentences preceding it). For some types of documents (e.g., news articles) one might further expect sentences occurring early on to be more central and therefore backward-looking edges to have larger weights. We evaluate the proposed approach on three single-document news summarization datasets re"
P19-1628,P17-1099,0,0.734213,"bution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin.1 1 Introduction Single-document summarization is the task of generating a shorter version of a document while retaining its most important content (Nenkova et al., 2011). Modern neural network-based approaches (Nallapati et al., 2016; Paulus et al., 2018; Nallapati et al., 2017; Cheng and Lapata, 2016; See et al., 2017; Narayan et al., 2018b; Gehrmann et al., 2018) have achieved promising results thanks to the availability of largescale datasets containing hundreds of thousands of document-summary pairs (Sandhaus, 2008; Hermann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It the"
P19-1628,D08-1079,0,0.636985,"rmann et al., 2015b; Grusky et al., 2018). Nevertheless, it is unrealistic to expect that large-scale and high-quality training data will be available or cre1 Our code is available at https://github.com/ mswellhao/PacSum. ated for different summarization styles (e.g., highlights vs. single-sentence summaries), domains (e.g., user- vs. professionally-written articles), and languages. It therefore comes as no surprise that unsupervised approaches have been the subject of much previous research (Marcu, 1997; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015; Yin and Pei, 2015; Li et al., 2017). A very popular algorithm for extractive single-document summarization is TextRank (Mihalcea and Tarau, 2004); it represents document sentences as nodes in a graph with undirected edges whose weights are computed based on sentence similarity. In order to decide which sentence to include in the summary, a node’s centrality is often measured using graph-based ranking algorithms such as PageRank (Brin and Page, 1998). In this paper, we argue that the centrality measure can be improved in two import"
P19-1628,W04-3252,0,\N,Missing
P19-1628,J10-3005,1,\N,Missing
P19-1628,P02-1058,0,\N,Missing
P19-1628,D13-1158,0,\N,Missing
P19-1628,W01-0100,0,\N,Missing
P19-1628,P16-1046,1,\N,Missing
P19-1628,D18-1443,0,\N,Missing
P19-1629,W13-2322,0,0.192662,"ient(e2 , x2 ) male(x1 ) e2 ≤ e1 because(k1 , k2 ) b. SDRS k1 k2 DRS max(x1) fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1) because(k1, k2) DRS POS DRS Max fell . john(x2) push(e2) Patient(e2, x1) male(x1) John might push him . temp_before(e2, e1) Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez"
P19-1629,N18-1118,0,0.0307579,"er than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al., 2014), summarization (Goyal and Eisenstein, 2016), conversational agents (Vinyals and Le, 2015), machine translation (Sim Smith, 2017; Bawden et al., 2018), and question anwsering (Rajpurkar et al., 2018). Our contributions in this work can be summarized as follows: 1) We formally define Discourse Representation Tree structures for sentences and documents; 2) We present a general framework for parsing discourse structures of arbitrary length and granularity; our framework is based on a neural model which decomposes the generation of meaning representations into three stages following a coarse-to-fine approach (Liu et al., 2018; Dong and Lapata, 2018); 3) We further demonstrate that three modeling innovations are key to tree structure prediction:"
P19-1629,P17-1112,0,0.0190714,"hao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like format (top) and as a tree (bottom). Red lines indicate terminals corresponding to words and green lines indicate non-terminals corresponding to sentences.  and POS are modality operators for possibility. DRT is a popular theory of meaning representation (Kamp, 1981; Kamp and Reyle, 1993; Asher, 1993; Asher and Lascarides, 2003) designed to account for a vari"
P19-1629,P17-1005,1,0.927333,"lemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like format (top) and as a tree (bottom). Red lines indicate terminals corresponding to words and green lines indicate non-terminals corresponding to sentences.  and POS are modality operators for possibility. DRT is a popular theory of meaning representation"
P19-1629,H94-1010,0,0.0896294,"a. DRT parsing resembles the task of mapping sentences to Abstract Meaning Representations (AMRs; Banarescu et al. 2013) in that logical forms are broad-coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis o"
P19-1629,E17-1051,1,0.867659,"copying (Gu et al., 2016) and attention (Mikolov 6255 et al., 2013). Our copying mechanism is more specialized and linguistically-motivated: it considers the semantics of the input text for deciding which tokens to copy. While our multi-attention mechanism is fairly general, it extracts features from different encoder representations (word- or sentencelevel) and flexibly integrates supervised and unsupervised attention in a unified framework. A few recent approaches focus on the alignment between semantic representations and input text, either as a preprocessing step (Foland and Martin, 2017; Damonte et al., 2017) or as a latent variable (Lyu and Titov, 2018). Instead, our parser implicitly models word-level alignments with multi-attention and explicitly obtains sentence-level alignments with supervised attention, aiming to jointly train a semantic parser. 8 Conclusions In this work we proposed a novel semantic parsing task to obtain Discourse Representation Tree Structures and introduced a general framework for parsing texts of arbitrary length and granularity. Experimental results on two benchmarks show that our parser is able to obtain reasonably accurate sentence- and document-level discourse repre"
P19-1629,P16-1004,1,0.923501,"age to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like for"
P19-1629,P18-1068,1,0.847888,"nstein, 2016), conversational agents (Vinyals and Le, 2015), machine translation (Sim Smith, 2017; Bawden et al., 2018), and question anwsering (Rajpurkar et al., 2018). Our contributions in this work can be summarized as follows: 1) We formally define Discourse Representation Tree structures for sentences and documents; 2) We present a general framework for parsing discourse structures of arbitrary length and granularity; our framework is based on a neural model which decomposes the generation of meaning representations into three stages following a coarse-to-fine approach (Liu et al., 2018; Dong and Lapata, 2018); 3) We further demonstrate that three modeling innovations are key to tree structure prediction: a supervised hierarchical attention mechanism, a linguistically-motivated copy strategy, and constraint-based inference to ensure wellformed DRTS output; 4) Experimental results on sentence- and document-level benchmarks show that our model outperforms competitive baselines by a wide margin. We release our code and DRTS benchmarks in the hope of driving research in semantic parsing further.2 1 The shared task on Discourse Representation Structure parsing in IWCS 2019. https://sites.google.com/ vie"
P19-1629,S16-1186,0,0.0307466,"ations (AMRs; Banarescu et al. 2013) in that logical forms are broad-coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as informati"
P19-1629,P17-1043,0,0.158148,"et al. 2013) in that logical forms are broad-coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al.,"
P19-1629,W16-5903,0,0.0196135,"Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al., 2014), summarization (Goyal and Eisenstein, 2016), conversational agents (Vinyals and Le, 2015), machine translation (Sim Smith, 2017; Bawden et al., 2018), and question anwsering (Rajpurkar et al., 2018). Our contributions in this work can be summarized as follows: 1) We formally define Discourse Representation Tree structures for sentences and documents; 2) We present a general framework for parsing discourse structures of arbitrary length and granularity; our framework is based on a neural model which decomposes the generation of meaning representations into three stages following a coarse-to-fine approach (Liu et al., 2018; Dong and Lapa"
P19-1629,P16-1154,0,0.0429728,"Recently, Liu et al. (2018) conceptualized DRT parsing as a tree structure prediction problem which they modeled with a series of encoder-decoder architectures. van Noord et al. (2018b) adapt models from neural machine translation (Klein et al., 2017) to DRT parsing, also following a graph-based representation. Previous work has focused exclusively on sentences, whereas we design a general framework for parsing sentences and documents and provide a model which can be used interchangeably for both. Various mechanisms have been proposed to improve sequence-to-sequence models including copying (Gu et al., 2016) and attention (Mikolov 6255 et al., 2013). Our copying mechanism is more specialized and linguistically-motivated: it considers the semantics of the input text for deciding which tokens to copy. While our multi-attention mechanism is fairly general, it extracts features from different encoder representations (word- or sentencelevel) and flexibly integrates supervised and unsupervised attention in a unified framework. A few recent approaches focus on the alignment between semantic representations and input text, either as a preprocessing step (Foland and Martin, 2017; Damonte et al., 2017) or"
P19-1629,P82-1020,0,0.817306,"Missing"
P19-1629,P16-1002,0,0.0242781,"table meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Buys and Blunsom, 2017). In this work we focus on parsing formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). Figure 1: Meaning representation for the discourse “Max fell. John might push him.” in box-like format (top) and as a tr"
P19-1629,D14-1181,0,0.00702113,"embeddings ewij , pre-trained word embeddings e¯wij , and lemma embeddings e`ij (where f (·) is a non-linear function). Embeddings ewij and e`ij are randomly initialized and tuned during training, while e¯wij are fixed. The encoder represents words and sentences in a unified framework compatible with sentenceand document-level DRTS parsing. Our experiments employed recurrent neural networks with long-short term memory units (LSTMs; Hochreiter and Schmidhuber 1997), however, there is nothing inherent in our framework that is LSTM specific. For instance, representations based on convolutional (Kim, 2014) or recursive neural networks (Socher et al., 2012) are also possible. Word Representation We encode the input text with a bidirectional LSTM (biLSTM): ←→ ←−→ [hx00 : hxmn ] = biLSTM(x00 : xmn ), ←→ where hxij denotes the hidden representation of the encoder for xij , which denotes the input representation of token j in sentence i. 6 The left boundary of sentence i is the right boundary of sentence i − 1, the left boundary of the first sentence is hdi, and the right boundary of the last sentence is h/di. 6250 where g v (·) is a linear function with the name v.7 Given encoder representations Hx"
P19-1629,P17-4012,0,0.041168,"the parser cannot learn reliable representations for them. Moreover, as the size of documents increases, ambiguity for the resolution of coreferring expressions increases, suggesting that explicit modeling of anaphoric links might be necessary. 7 Related Work Le and Zuidema (2012) were the first to train a data-driven DRT parser using a graph-based representation. Recently, Liu et al. (2018) conceptualized DRT parsing as a tree structure prediction problem which they modeled with a series of encoder-decoder architectures. van Noord et al. (2018b) adapt models from neural machine translation (Klein et al., 2017) to DRT parsing, also following a graph-based representation. Previous work has focused exclusively on sentences, whereas we design a general framework for parsing sentences and documents and provide a model which can be used interchangeably for both. Various mechanisms have been proposed to improve sequence-to-sequence models including copying (Gu et al., 2016) and attention (Mikolov 6255 et al., 2013). Our copying mechanism is more specialized and linguistically-motivated: it considers the semantics of the input text for deciding which tokens to copy. While our multi-attention mechanism is f"
P19-1629,D16-1116,0,0.0420033,"Missing"
P19-1629,D11-1140,0,0.0342763,"2 ) b. SDRS k1 k2 DRS max(x1) fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1) because(k1, k2) DRS POS DRS Max fell . john(x2) push(e2) Patient(e2, x1) male(x1) John might push him . temp_before(e2, e1) Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata, 2016; Cheng et al., 2017; Yin and Neubig, 2017; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 201"
P19-1629,C12-1094,0,0.397289,"es and beyond. In general, DeepCopy has an advantage over comparison systems due to the more sophisticated alignment information and the fact that it aims to generate global document-level structures. Our results also indicate that modeling longer documents which are relatively few in the training set is challenging mainly because the parser cannot learn reliable representations for them. Moreover, as the size of documents increases, ambiguity for the resolution of coreferring expressions increases, suggesting that explicit modeling of anaphoric links might be necessary. 7 Related Work Le and Zuidema (2012) were the first to train a data-driven DRT parser using a graph-based representation. Recently, Liu et al. (2018) conceptualized DRT parsing as a tree structure prediction problem which they modeled with a series of encoder-decoder architectures. van Noord et al. (2018b) adapt models from neural machine translation (Klein et al., 2017) to DRT parsing, also following a graph-based representation. Previous work has focused exclusively on sentences, whereas we design a general framework for parsing sentences and documents and provide a model which can be used interchangeably for both. Various mec"
P19-1629,P11-1060,0,0.0256204,") now(t1 ) e 1 ≤ t1 k2 : : john(x2 ) push(e2 ) Patient(e2 , x2 ) male(x1 ) e2 ≤ e1 because(k1 , k2 ) b. SDRS k1 k2 DRS max(x1) fall(e1) Agent(e1,x1) now(t1) temp_before(e1, t1) because(k1, k2) DRS POS DRS Max fell . john(x2) push(e2) Patient(e2, x1) male(x1) John might push him . temp_before(e2, e1) Introduction Semantic parsing is the task of mapping natural language to machine interpretable meaning representations. Various models have been proposed over the years to learn semantic parsers from linguistic expressions paired with logical forms, SQL queries, or source code (Kate et al., 2005; Liang et al., 2011; Zettlemoyer and Collins, 2005; Banarescu et al., 2013; Wong and Mooney, 2007; Kwiatkowski et al., 2011; Zhao and Huang, 2015). The successful application of encoder-decoder models (Sutskever et al., 2014; Bahdanau et al., 2015) to a variety of NLP tasks has prompted the reformulation of semantic parsing as a sequenceto-sequence learning problem (Dong and Lapata, 2016; Jia and Liang, 2016; Koˇcisk`y et al., 2016), although most recent efforts focus on architectures which make use of the syntax of meaning representations, e.g., by developing tree or graph-structured decoders (Dong and Lapata,"
P19-1629,P18-1040,1,0.760495,"coverage, they represent compositional utterances with varied vocabu6248 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6248–6262 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics lary and syntax and are ungrounded, i.e., they are not tied to a specific database from which answers to queries might be retrieved (Zelle and Mooney, 1996; Cheng et al., 2017; Dahl et al., 1994). Our work departs from previous generalpurpose semantic parsers (Flanigan et al., 2016; Foland and Martin, 2017; Lyu and Titov, 2018; Liu et al., 2018; van Noord et al., 2018b) in that we focus on building representations for entire documents rather than isolated utterances, and introduce a novel semantic parsing task based on DRT. Specifically, our model operates over Discourse Representation Tree Structures (DRTSs) which are DRSs rendered in a tree-style format (Liu et al. 2018; see Figure 1b). Discourse representation parsing has been gaining more attention lately.1 The semantic analysis of text beyond isolated sentences can enhance various NLP applications such as information retrieval (Zou et al., 2014), summarization (Goyal and Eisens"
Q14-1030,D11-1039,0,0.0484367,"interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). F"
Q14-1030,Q13-1005,0,0.640509,"ls of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a se"
Q14-1030,W13-2322,0,0.0261855,"hout using annotated question-answer pairs. We have shown how to obtain graph representations from the output of a CCG parser and subsequently learn their correspondence to Freebase using a rich feature set and their denotations as a form of weak supervision. Our parser yields state-of-the art performance on three large Freebase domains and is not limited to question answering. We can create semantic parses for any type of NL sentences. Our work brings together several strands of research. Graph-based representations of sentential meaning have recently gained some attention in the literature (Banarescu et al., 2013), and attempts to map sentences to semantic graphs have met with good inter-annotator agreement. Our work is also closely related to Kwiatkowski et al. (2013) and Berant and Liang (2014) who present open-domain se388 mantic parsers based on Freebase and trained on QA pairs. Despite differences in formulation and model structure, both approaches have explicit mechanisms for handling the mismatch between natural language and the KB (e.g., using logical-type equivalent operators or paraphrases). The mismatch is handled implicitly in our case via our graphical representation which allows for the i"
Q14-1030,D13-1160,0,0.919534,"e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert t"
Q14-1030,P14-1133,0,0.753639,"m conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert the output of an open-dom"
Q14-1030,C04-1180,1,0.417277,"tory Categorial Grammar The graph like structure of Freebase inspires us to create a graph like structure for natural language, and learn a mapping between them. To do this we take advantage of the representational power of Combinatory Categorial Grammar (Steedman, 2000). CCG is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomCameron Titanic Cameron λyλx. directed.arg1(e, x) ∧ directed.arg2(e, y) NP Cameron e directed dir e cte d.a r g1 < e dir S directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ect 2 See Bos et al. (2004) for a detailed introduction to semantic representation using CCG. 3 Neo-Davidsonian semantics is a form of first-order logic that uses event identifiers (e) to connect verb predicates and their subcategorized arguments through conjunctions. 380 Cameron directed n 1997 in directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ∧ directed.in(e, 1997) ena. CCG is well known for capturing long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverag"
Q14-1030,P09-1010,0,0.0253326,"m each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually :"
Q14-1030,P12-1014,0,0.0161775,"eir relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually : Se Se : λPλe.lexe .annually(e) ∧ P(e) state : NPx /NPx : λPλx.lexx .sta"
Q14-1030,P13-1042,0,0.850757,"s have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs"
Q14-1030,P04-1014,0,0.0112942,"nation, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverage semantic analysis. Moreover, due to the transparent interface between syntax and semantics, it is relatively straightforward to built a semantic parse for a sentence from its corresponding syntactic derivation tree (Bos et al., 2004). In our case, the choice of syntactic parser is motivated by the scale of our problem; the parser must be broad-coverage and robust enough to handle a web-sized corpus. For these reasons, we rely on the C&C parser (Clark and Curran, 2004), a generalpurpose CCG parser, to obtain syntactic derivations. To our knowledge, we present the first attempt to use a CCG parser trained on treebanks for grounded semantic parsing. Most previous work has induced task-specific CCG grammars (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010). An example CCG derivation is shown in Figure 4. Semantic parses are constructed from syntactic CCG parses, with semantic composition being guided by the CCG syntactic derivation.2 We use a neo-Davidsonian (Parsons, 1990) semantics to represent semantic parses.3 Each word has a semantic categor"
Q14-1030,J07-4004,0,0.0398868,"mple of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching problem. We convert the output of an open-domain combinatory categorial grammar (CCG) parser (Clark and Curran, 2007) into a graphical representation and subsequently map it onto Freebase. The parser’s graphs (also called ungrounded graphs) are mapped to all possible Freebase subgraphs (also called grounded graphs) by replacing edges and nodes with relations and types in Freebase. Each grounded graph corresponds to a unique grounded logical query. During learning, our semantic parser is trained to identify which KB subgraph best corresponds to the NL graph. Problem377 Transactions of the Association for Computational Linguistics, 2 (2014) 377–392. Action Editor: Noah Smith. c Submitted 3/2014; Revised 6/2014"
Q14-1030,P02-1042,1,0.698829,"cted.arg2(e, Titanic) ect 2 See Bos et al. (2004) for a detailed introduction to semantic representation using CCG. 3 Neo-Davidsonian semantics is a form of first-order logic that uses event identifiers (e) to connect verb predicates and their subcategorized arguments through conjunctions. 380 Cameron directed n 1997 in directed.arg1(e, Cameron) ∧ directed.arg2(e, Titanic) ∧ directed.in(e, 1997) ena. CCG is well known for capturing long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as standard local predicate-argument dependencies (Clark et al., 2002), thus supporting wide-coverage semantic analysis. Moreover, due to the transparent interface between syntax and semantics, it is relatively straightforward to built a semantic parse for a sentence from its corresponding syntactic derivation tree (Bos et al., 2004). In our case, the choice of syntactic parser is motivated by the scale of our problem; the parser must be broad-coverage and robust enough to handle a web-sized corpus. For these reasons, we rely on the C&C parser (Clark and Curran, 2004), a generalpurpose CCG parser, to obtain syntactic derivations. To our knowledge, we present the"
Q14-1030,W10-2903,0,0.627118,"gical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large com"
Q14-1030,W02-1001,0,0.0343689,"ven a NL sentence s, we construct from its CCG syntactic derivation all corresponding ungrounded graphs u. Using a beam search procedure (described in Section 4.2), we find the best scoring graphs (g, ˆ u), ˆ maximizing over different graph configurations (g, u) of s: (g, ˆ u) ˆ = arg max Φ(g, u, s, K B ) · θ g,u (1) We define the score of (g, ˆ u) ˆ as the dot product between a high dimensional feature representation Φ = (Φ1 , . . . Φm ) and a weight vector θ (see Section 3.3 for details on the features we employ). We estimate the weights θ using the averaged structured perceptron algorithm (Collins, 2002). As shown in Algorithm 1, the perceptron makes several passes over sentences, and in each iteration it computes the best scoring (g, ˆ u) ˆ among the candidate graphs for a given sentence. In line 6, the algorithm updates θ with the difference (if any) be383 Algorithm 1: Averaged Structured Perceptron 1 2 3 4 Input: Training sentences: {si }Ni=1 θ←0 for t ← 1 . . . T do for i ← 1 . . . N do (gˆi , uˆi ) = arg max Φ(gi , ui , si , K B ) · θ gi ,ui 5 6 7 + if (u+ i , gi ) 6= (uˆi , gˆi ) then + θ ← θ + Φ(g+ i , ui , si , K B )−Φ(gˆi , uˆi , si , K B ) return 1 T T 1 N i ∑t=i N ∑i=1 θ t tween th"
Q14-1030,P13-1158,0,0.286302,"sisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora."
Q14-1030,P11-1149,0,0.0128605,"ed, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge"
Q14-1030,P11-1060,0,0.8618,"d Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowl"
Q14-1030,P11-1055,0,0.0290585,"ails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi"
Q14-1030,Q13-1016,0,0.0259582,") formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013; Liang et al., 2011; Clarke et al., 2010), feedback-based learning has been previously used for interpreting and following NL instructions (Branavan et al., 2009; Chen and Mooney, 2011), playing computer games (Branavan et al., 2012), and grounding language in the physical world (Krishnamurthy and Kollar, 2013; Matuszek et al., 2012). Lemma * * * * * be the * not, n’t no * * POS VB*, IN, TO, POS NN, NNS NNP*, PRP* RB* JJ* * * CD Semantic Class * NEGATION * WDT, WP*, WRB WDT, WP*, WRB COMPLEMENT EVENT TYPE ENTITY EVENTMOD TYPEMOD COPULA UNIQUE COUNT QUESTION CLOSED Semantic Category directed : (Se NPx <1&gt;)/NPy <2&gt; : λQλPλe.∃x∃y. directed.arg1(e, x) ∧ directed.arg2(e, y) ∧ P(x) ∧ Q(y) movie : NP : λx.movie(x) Obama : NP : λx.equal(x, Obama) annually : Se Se : λPλe.lexe .annually(e) ∧ P(e) state : NPx /NPx : λPλx.lexx .state(x) ∧ P(x) be: (Sy NPx )/NPy : λQλPλy.∃x.lexy (x) ∧ P(x) ∧ Q(y) the : NPx /"
Q14-1030,D12-1069,0,0.773171,"aches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or"
Q14-1030,D13-1161,0,0.680647,"te the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) and web-scale corpora. Specifically, we exploit Freebase, a large community-authored knowledge base that spans many sub-domains and stores real world facts in graphical format, and parsed sentences from a large corpus. We formulate semantic parsing as a graph matching"
Q14-1030,D10-1119,1,0.94973,"s requiring communication with machines in a language interpretable by them. Semantic parsing addresses the specific task of learning to map natural language (NL) to machine interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining m"
Q14-1030,Q13-1015,1,0.822387,"s for the incorporation of all manner of powerful features. More generally, our method is based on the assumption that linguistic structure has a correspondence to Freebase structure which does not always hold (e.g., in Who is the grandmother of Prince William?, grandmother is not directly expressed as a relation in Freebase). Additionally, our model fails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, a"
Q14-1030,P09-1113,0,0.0420296,"ionally, our model fails when questions are too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond"
Q14-1030,P13-1092,0,0.16245,"ing data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answer pairs have been gaining momentum as a means of scaling semantic parsers to large, open-domain problems (Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014). Figure 1 shows an example of a question, its annotated logical form, and answer (or denotation). In this paper, we build a semantic parser that does not require example annotations or question-answer pairs but instead learns from a large knowledge base (KB) a"
Q14-1030,N13-1008,0,0.0175128,"too short without any lexical clues (e.g., What did Charles Darwin do? ). Supervision from annotated data or paraphrasing could improve performance in such cases. In the future, we plan to explore cluster-based semantics (Lewis and Steedman, 2013) to increase the robustness on unseen NL predicates. Our work joins others in exploiting the connections between natural language and open-domain knowledge bases. Recent approaches in relation extraction use distant supervision from a knowledge base to predict grounded relations between two target entities (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013). During learning, they aggregate sentences containing the target entities, ignoring richer contextual information. In contrast, we learn from each individual sentence taking into account all entities present, their relations, and how they interact. Krishnamurthy and Mitchell (2012) formalize semantic parsing as a distantly supervised relation extraction problem combined with a manually specified grammar to guide semantic parse composition. Finally, our approach learns a model of semantics guided by denotations as a form of weak supervision. Beyond semantic parsing (Artzi and Zettlemoyer, 2013"
Q14-1030,P07-1121,0,0.589876,"to play a game are tasks requiring communication with machines in a language interpretable by them. Semantic parsing addresses the specific task of learning to map natural language (NL) to machine interpretable formal meaning representations. Traditionally, sentences are converted into logical form grounded in the symbols of some fixed ontology or relational database. Approaches for learning semantic parsers have been for the most part supervised, using annotated training data consisting of sentences and their corresponding logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). More recently, alternative forms of supervision have been proposed to alleviate the annotation burden, e.g., by learning from conversational logs (Artzi and Zettlemoyer, 2011), from sentences paired with system behavior (Chen and Mooney, 2011; Goldwasser and Roth, 2011; Artzi and Zettlemoyer, 2013), via distant supervision (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013), from questions (Goldwasser et al., 2011; Poon, 2013; Fader et al., 2013), and questionanswer pairs (Clarke et al., 2010; Liang et al., 2011). Indeed, methods which learn from question-answe"
Q14-1030,P14-1090,0,0.512733,"Missing"
Q14-1030,D07-1071,0,0.68019,"Missing"
Q15-1006,J08-1001,1,0.669325,"experts or experienced users in the absence 1 The corpus can be downloaded from http: //www.homepages.inf.ed.ac.uk/alouis/ solutionComplexity.html. 75 Frequency and learn to predict an ordering for new sets of solutions. This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner. Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004). More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b). Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data. 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Solution set size Figure 1: Histogram of solution set sizes of any interaction with other users or their devices and"
Q15-1006,P06-1049,0,0.0195836,"em on a web interface and assigned a rank to each solution to create an order. No ties were allowed and a complete ordering was required to keep the annotation simple. The annotators were fluent English speakers and had some knowledge of computer hardware and software. We refrained from including novice users in our study as they are likely to have very different personal preferences resulting in more divergent rankings. Results We measured inter-annotator agreement using Kendall’s τ , a metric of rank correlation which has been reliably used in information ordering evaluations (Lapata, 2006; Bollegala et al., 2006; Madnani et al., 2007). τ ranges between −1 and +1, where +1 indicates equivalent rankings, −1 completely reverse rankings, and 0 independent rankings. Table 2 shows the pairwise inter-annotator agreement as well as the agreement between each annotator and the original FAQ order. The table shows fair agreement between the annotators confirming that this is a reasonable task for humans to do. As can be seen, there are some individual differences, with the inter-annotator agreement varying from 0.421 (for A,B) to 0.625 (for A,D). The last column in Table 2 reports the agreement 76 between our a"
Q15-1006,P09-1010,0,0.0288407,"., whether users express themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approxi"
Q15-1006,P11-1028,0,0.0135884,"gh our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least to most complex) 3 Problem formulation Our aim in this work is to learn models wh"
Q15-1006,N09-1042,0,0.117762,"for new sets of solutions. This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner. Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004). More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b). Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data. 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Solution set size Figure 1: Histogram of solution set sizes of any interaction with other users or their devices and thus constitute a generic list of steps to try out. We assume that in such a situation, the solution providers are likely to suggest simpler solutions before other complex ones, leadin"
Q15-1006,D09-1100,0,0.012473,"s themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least"
Q15-1006,N07-1055,0,0.0182355,"procedure. We opted for a discriminative ranking approach instead which uses the generative models to compute a rich set of features. This choice allows us to simultaneously obtain features tapping on to different aspects learned by the models and to use well-defined objective functions. Below, we briefly describe the features based on our generative models. We also present additional features used to create baselines for system comparison. Likelihood We created a Hidden Markov Model based on the sample from the posterior of our models (for a similar HMM approximation of a Bayesian model see Elsner et al. (2007)). For our model, the HMM has L states, and each state sm corresponds to a complexity level dm . We used the complexity language models φm estimated from the posterior as the emission probability distribution for the corresponding states. The transition probabilities of the HMM were computed based on the complexity level assignments for the training solution sequences in our posterior sample. The probability of transitioning to state sj from state si , p(sj |si ), is the conc(d ,d ) ditional probability p(dj |di ) computed as c(di i )j , where c(di , dj ) is the number of times the complexity"
Q15-1006,D10-1084,0,0.0292318,"a particular repair action. Our notion of complexity is conceptually similar to the cost of an action, however we learn to predict complexity levels rather than calibrate them manually. Also note that our troubleshooting task is not device specific. Our models learn from troubleshootingoriented data without any restrictions on the problems being solved. Previous work on web-based user support has mostly focused on thread analysis. The idea is to model the content structure of forum threads by analyzing the requests for information and suggested solutions in the thread data (Wang et al., 2011; Kim et al., 2010). Examples of such analysis include identifying which earlier post(s) a given post responds to and in what manner (e.g., is it a question, an answer or a confirmation). Other related work (Lui and Baldwin, 2009) identifies user characteristics in such data, i.e., whether users express themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some rela"
Q15-1006,P03-1069,1,0.673814,"ed by computer experts or experienced users in the absence 1 The corpus can be downloaded from http: //www.homepages.inf.ed.ac.uk/alouis/ solutionComplexity.html. 75 Frequency and learn to predict an ordering for new sets of solutions. This setup is related to previous studies on information ordering where the aim is to learn statistical patterns of document structure which can be then used to order new sentences or paragraphs in a coherent manner. Some approaches approximate the structure of a document via topic and entity sequences using local dependencies such as conditional probabilities (Lapata, 2003; Barzilay and Lapata, 2008) or Hidden Markov Models (Barzilay and Lee, 2004). More recently, global approaches which directly model the permutations of topics in the document have been proposed (Chen et al., 2009b). Following this line of work, one of our models uses the Generalized Mallows Model (Fligner and Verducci, 1986) in its generative process which allows to model permutations of complexity levels in the training data. 70 65 60 55 50 45 40 35 30 25 20 15 10 5 0 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Solution set size Figure 1: Histogram of solution set sizes of any interaction with othe"
Q15-1006,W07-2312,0,0.044981,"Missing"
Q15-1006,P14-5010,0,0.00308879,"t of detail from complex ones). We devised three features based on the number of sentences (within a solution), words, and average sentence length. Syntax/Semantics Another related class of features estimates solution complexity based on sentence structure and meaning. We obtained eight syntactic features based on the number of nouns, verbs, adjectives and adverbs, prepositions, pronouns, wh-adverbs, modals, and punctuation. Other features compute the average and maximum depth of constituent parse trees. The part-of-speech tags and parse trees were obtained using the Stanford CoreNLP toolkit (Manning et al., 2014). In addition, we computed 10 semantic features using WordNet (Miller, 1995). They are the average number of senses for each category (noun, verb, adjective/adverb), and the maximum number of senses for the same three classes. We also include the average and maximum lengths of the path to the root of the hypernym tree for nouns and verbs. This class of features roughly approximates the indicators typically used in predicting text readability (Schwarm and Ostendorf, 2005; McNamara et al., 2014). 5.2 Experimental Setup We performed 10-fold cross-validation. We trained the ranking model on 240 pr"
Q15-1006,P05-1065,0,0.0273753,"maximum depth of constituent parse trees. The part-of-speech tags and parse trees were obtained using the Stanford CoreNLP toolkit (Manning et al., 2014). In addition, we computed 10 semantic features using WordNet (Miller, 1995). They are the average number of senses for each category (noun, verb, adjective/adverb), and the maximum number of senses for the same three classes. We also include the average and maximum lengths of the path to the root of the hypernym tree for nouns and verbs. This class of features roughly approximates the indicators typically used in predicting text readability (Schwarm and Ostendorf, 2005; McNamara et al., 2014). 5.2 Experimental Setup We performed 10-fold cross-validation. We trained the ranking model on 240 problem-solution sets; 30 sets were reserved for development and 30 for 82 testing (in each fold). The most frequent 20 words in each training set were filtered as stopwords. The development data was used to tune the parameters and hyperparameters of the models and the number of complexity levels. We experimented with ranges [5– 20] and found that the best number of levels was 10 for the position model and 20 for the permutationbased model, respectively. For the expected"
Q15-1006,P10-1083,0,0.0150187,"technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our work bears some relation to language grounding, the problem of extracting representations of the meaning of natural language tied to the physical world. Mapping instructions to executable actions is an instance of language grounding with applications to automated troubleshooting (Branavan et al., 2009; Eisenstein et al., 2009), navigation (Vogel and Jurafsky, 2010), and game-playing (Branavan et al., 2011). In our work, there is no direct attempt to model the environment or the troubleshooting steps. Rather, we study the language of instructions and how it correlates with the complexity of the implied actions. Our results show that it possible to predict complexity, while being agnostic about the semantics of the domain or the effect of the instructions in the corresponding environment. Our generative models are trained on existing archives of problems with corresponding solutions (approximately ordered from least to most complex) 3 Problem formulation"
Q15-1006,D11-1002,0,0.0220793,"ed by carrying out a particular repair action. Our notion of complexity is conceptually similar to the cost of an action, however we learn to predict complexity levels rather than calibrate them manually. Also note that our troubleshooting task is not device specific. Our models learn from troubleshootingoriented data without any restrictions on the problems being solved. Previous work on web-based user support has mostly focused on thread analysis. The idea is to model the content structure of forum threads by analyzing the requests for information and suggested solutions in the thread data (Wang et al., 2011; Kim et al., 2010). Examples of such analysis include identifying which earlier post(s) a given post responds to and in what manner (e.g., is it a question, an answer or a confirmation). Other related work (Lui and Baldwin, 2009) identifies user characteristics in such data, i.e., whether users express themselves clearly, whether they are technically knowledgeable, and so on. Although our work does not address threaded discourse, we analyze the content of troubleshooting data and show that it is possible to predict the complexity levels for suggested solutions from surface lexical cues. Our w"
Q15-1006,N04-1015,0,\N,Missing
Q15-1006,J06-4002,1,\N,Missing
Q15-1006,U10-1009,0,\N,Missing
Q15-1032,E14-1023,0,0.0771782,"word in context can influence correct role assignment. While concepts such as polysemy, homonymy and metonymy are all relevant here, the scarce training data available for FrameNet-based SRL calls for a light-weight model that can be applied without large amounts of labeled data. We therefore employ distributional word representations which we critically adapt based on document content. We describe our contribution in Section 5.1. Entities that fill semantic roles are sometimes mentioned in discourse. Given a specific mention 3 We note that better results have been reported in Hermann et al. (2014) and T¨ackstr¨om et al. (2015). However, both of these more recent approaches rely on a custom frame identification component as well as proprietary tools and models for tagging and parsing which are not publicly available. Argument identification and classification Lemma form of f POS tag of f Any syntactic dependents of f * Subcat frame of f * Voice of a* Any lemma in a* Number of words in a First word and POS tag in a Second word and POS tag in a Last word and POS tag in a Relation from first word in a to its parent Relation from second word in a to its parent Relation from last word in a t"
Q15-1032,C10-3009,0,0.18338,"Missing"
Q15-1032,N10-1030,0,0.0483915,"a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level."
Q15-1032,C12-1042,0,0.10028,"Missing"
Q15-1032,P10-1025,0,0.0644011,"roperties (typically represented by co-occurrence counts) of linguistic entities such as words and phrases (Sahlgren, 2008). Although the absolute meaning of distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in the context of FrameNet, where different senses are observed across a diverse range of texts including spoken dialogue and debate transcripts as well Country Frame Frame Element Iran Supply Commerce buy R ECIPIENT B UYER China Supply Commerce sell S UPPLIER S ELLER"
Q15-1032,D11-1096,0,0.0162915,"irst to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and t"
Q15-1032,P11-1144,0,0.144737,"s the baseline system described in Section 4. The second system, henceforth Framat+context , is an enhanced version of the baseline that additionally uses all extensions described in Section 5. Finally, we also consider the output of SEMAFOR (Das et al., 2014), a state-of-the-art model for frame-semantic role labeling. Although all systems are provided with entire documents as input, SEMAFOR and Framat process each document sentence-by-sentence whereas Framat+context also uses features over all sentences. For evaluation, we use the same FrameNet training and evaluation texts as established in Das and Smith (2011). We compute precision, recall and F1 -score using the modified SemEval-2007 scorer from the SEMAFOR website.6 6 7 http://www.ark.cs.cmu.edu/SEMAFOR/eval/ Results produced by running SEMAFOR on the exact same 455 Table 5: Full structure prediction results using gold frames, Framat and different sets of context features. All numbers are percentages. Results Table 4 summarizes our results with Framat, Framat+context , and SEMAFOR using gold and predicted frames (see the upper and lower half of the table, respectively). Although differences in system architecture lead to different precision/recal"
Q15-1032,J14-1002,0,0.401776,"ame elements which are instantiated within the same sentence (i.e., a given predicate’s arguments). The adopted SRL system has been developed for PropBank/NomBank-style role labeling and we make several changes to adapt it to FrameNet. Specifically, we change the argument labeling procedure from predicate-specific to frame-specific 2 Version 1.5, released September 2010. 451 roles and implement I/O methods to read and generate FrameNet XML files. For direct comparison with the previous state-of-the-art for FrameNetbased SRL, we further implement additional features used in the SEMAFOR system (Das et al., 2014) and combine the role labeling components of mate-tools with SEMAFOR’s preprocessing toolchain.3 All features used in our system are listed in Table 1. The main differences between our adaptation of mate-tools and SEMAFOR are as follows: whereas the latter implements identification and labeling of role fillers in one step, mate-tools follow the insight that these two steps are conceptually different (Xue and Palmer, 2004) and should be modeled separately. Accordingly, mate-tools contain a global reranking component which takes into account identification and labeling decisions while SEMAFOR on"
Q15-1032,D09-1003,0,0.046518,"often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so"
Q15-1032,D10-1113,1,0.820547,"ticle about a politician’s visit to the country. as travel guides and newspaper articles. Country names, for example, can be observed as fillers for different roles depending on the text genre and its perspective. Whereas some text may talk about a country as an interesting holiday destination (e.g., “Berlitz Intro to Jamaica”), others may discuss what a country is good at or interested in (e.g., “Iran [Nuclear] Introduction”). A list of the most frequent roles assigned to different country names are displayed in Table 2. Previous approaches model word meaning in context (Thater et al., 2010; Dinu and Lapata, 2010, inter alia) using sentence-level information which is already available in traditional SRL systems in the form of explicit features. Here, we go one step further and define a simple model in which word meaning representations are adapted to each document. As a starting point, we use the GloVe toolkit (Pennington et al., 2014) for learning representations4 and apply it to the Wikipedia corpus made available by the Westbury Lab.5 The learned representations can be seen as word vectors whose components encode basic bits of related encyclopaedic knowledge. We adapt these general representations"
Q15-1032,S15-1033,0,0.0297529,"the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments incl"
Q15-1032,J12-4003,0,0.0296284,"proaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments include the distance between mentions and any discourse relations occurring between them (Gerber and Chai, 2012), roles assigned to mentions in the previous context, the discourse prominence of the denoted entity (Silberer and Frank, 2012), and its centering status (Laparra and Rigau, 2013). None of these features have been used in a standard SRL system to date (and trivially, not all of them will be helpful as, for example, the number of sentences between a predicate and an argument is always zero within a sentence). In this paper, we ext"
Q15-1032,J02-3001,0,0.423636,"the usefulness of such additional information. The remainder of this paper is structured as follows. In Section 2, we present related work on semantic role labeling and the various features applied in traditional SRL systems. In Section 3, we provide additional background on the FrameNet resource. Sections 4 and 5 describe our baseline system and contextual extensions, respectively, and Section 6 presents our experimental results. We conclude the paper by discussing in more detail the output of our system and highlighting avenues for future work. 2 Related Work Early work in SRL dates back to Gildea and Jurafsky (2002), who were the first to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (M"
Q15-1032,W13-0111,0,0.0283497,"f distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in the context of FrameNet, where different senses are observed across a diverse range of texts including spoken dialogue and debate transcripts as well Country Frame Frame Element Iran Supply Commerce buy R ECIPIENT B UYER China Supply Commerce sell S UPPLIER S ELLER Iraq Locative relation Arriving G ROUND G OAL Table 2: Most frequent roles assigned to country names appearing FrameNet texts: whereas Iran and China are mostl"
Q15-1032,P14-1136,0,0.379277,"h EH8 9AB {mroth,mlap}@inf.ed.ac.uk Abstract including question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), stock price prediction (Xie et al., 2013), and social network extraction (Agarwal et al., 2014). Whereas some tasks directly utilize information encoded in the FrameNet resource, others make use of FrameNet indirectly through the output of SRL systems that are trained on data annotated with frame-semantic representations. While advances in machine learning have recently given rise to increasingly powerful SRL systems following the FrameNet paradigm (Hermann et al., 2014; T¨ackstr¨om et al., 2015), little effort has been devoted to improve such models from a linguistic perspective. Frame semantic representations have been useful in several applications ranging from text-to-scene generation, to question answering and social network analysis. Predicting such representations from raw text is, however, a challenging task and corresponding models are typically only trained on a small set of sentence-level annotations. In this paper, we present a semantic role labeling system that takes into account sentence and discourse context. We introduce several new features"
Q15-1032,P10-1099,0,0.140601,"the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments,"
Q15-1032,C08-1050,0,0.0833314,"avenues for future work. 2 Related Work Early work in SRL dates back to Gildea and Jurafsky (2002), who were the first to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth"
Q15-1032,W13-0114,0,0.0139823,"es on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments include the distance between mentions and any discourse relations occurring between them (Gerber and Chai, 2012), roles assigned to mentions in the previous context, the discourse prominence of the denoted entity (Silberer and Frank, 2012), and its centering status (Laparra and Rigau, 2013). None of these features have been used in a standard SRL system to date (and trivially, not all of them will be helpful as, for example, the number of sentences between a predicate and an argument is always zero within a sentence). In this paper, we extend the contextual features used for resolving implicit arguments to the SRL task and show how a set of discourse-level enhancements can be added to a traditional sentence-level SRL model. 3 FrameNet The Berkeley FrameNet project (Ruppenhofer et al., 2010) develops a semantic lexicon and an annotated example corpus based on Fillmore’s (1976) th"
Q15-1032,J13-4004,0,0.0120809,"efined frame elements) to just 27 (number of semantic types observed for frame elements in the training data). In practice, we define one binary indicator feature fs for each semantic type s observed at training time. Given a potential filler, we set the feature value of fs to 1 (otherwise 0) if and only if there exists a co-referent entity mention annotated as a frame element filler with semantic type s. Since texts in FrameNet do not contain any manual mark-up of coreference relations, we rely on entity mentions and coreference chains predicted by the Stanford Coreference Resolution system (Lee et al., 2013). 5.3 Discourse Newness Our third contextual feature type is based on the observation that the salience of a discourse entity and its semantic prominence are interrelated. Previous work (Rose, 2011) showed that semantic prominence, as signal-led by semantic roles, can better explain subsequent phenomena related to discourse salience (such as pronominalization) than syntactic indicators. Our question here is whether this insight can be also applied in reverse. Can information on discourse salience be useful as an indicator for semantic roles? For this feature, we make use of the same coreferenc"
Q15-1032,N15-1121,0,0.040238,"Missing"
Q15-1032,N13-1090,0,0.0211214,"computed coreference chains. We describe the motivation and actual implementation of this feature in Section 5.3. 452 Modeling Word Meaning in Context The underlying idea of distributional models of semantics is that meaning can be acquired based on distributional properties (typically represented by co-occurrence counts) of linguistic entities such as words and phrases (Sahlgren, 2008). Although the absolute meaning of distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in th"
Q15-1032,P04-1043,0,0.035425,"), who were the first to model role assignment to verb arguments based on FrameNet. Their model makes use of lexical and syntactic features, including binary indicators for the words involved, syntactic categories, dependency paths as well as position and voice in a given sentence. Most subsequent work in SRL builds on Gildea and Jurafsky’s feature set, often with the addition of features that describe relevant syntactic structures in more detail, e.g., the argument’s leftmost/rightmost dependent (Johansson and Nugues, 2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using"
Q15-1032,C08-1084,0,0.0306773,"Missing"
Q15-1032,D08-1048,1,0.901343,"Missing"
Q15-1032,D14-1162,0,0.100984,"hains. We describe the motivation and actual implementation of this feature in Section 5.3. 452 Modeling Word Meaning in Context The underlying idea of distributional models of semantics is that meaning can be acquired based on distributional properties (typically represented by co-occurrence counts) of linguistic entities such as words and phrases (Sahlgren, 2008). Although the absolute meaning of distributional representations remains unclear, they have proven highly successful for modeling relative aspects of meaning, as required for instance in word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Given their ability to model lexical similarity, it is not surprising that such representations are also successful at representing similar words in semantic tasks related to role labeling (Pennacchiotti et al., 2008; Croce et al., 2010; Zapirain et al., 2013). Although distributional representations can be used directly as features for role labeling (Pad´o et al., 2008; Gorinski et al., 2013; Roth and Woodsend, 2014, inter alia), further gains should be possible when considering document-specific properties such as genre and context. This is particularly true in the context of FrameNet, whe"
Q15-1032,D14-1045,1,0.93992,"2008). More sophisticated features include the use of convolution kernels (Moschitti, 2004; Croce et al., 2011) in order to represent predicate-argument structures and their lexical similarities more accurately. Beyond lexical and syntactic information, a few approaches employ additional semantic features based on annotated word senses (Che et al., 2010) and selectional preferences (Zapirain et al., 2013). Deschacht and Moens (2009) and Huang and Yates (2010) use sentence-internal sequence information, in the form of latent states in a hidden markov model. More recently, a few approaches 450 (Roth and Woodsend, 2014; Lei et al., 2015; Foland and Martin, 2015) explore ways of using low-rank vector and tensor approximations to represent lexical and syntactic features as well as combinations thereof. To the best of our knowledge, there exists no prior work where features based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful fe"
Q15-1032,R11-1046,0,0.0186717,"ents suitable for an entity given co-occurring words, we can also can explicitly consider previous role assignments to the same entity. As shown in Table 2, a country that fills the S UPPLIER role is more likely to also fill the role of a S ELLER than that of a B UYER. Given the high number of different frame elements in FrameNet, only a small fraction of pairs can be found in the training data, which entails that directly utilizing role co-occurrences might not be helpful. In order to benefit from previous role assignments in discourse, we follow related work on resolving implicit arguments (Ruppenhofer et al., 2011; Silberer and Frank, 2012) and consider the semantic types of role assignments (see Section 3) as features instead of the role labels themselves. This tremendously reduces the feature space from more than 8,000 options (number of defined frame elements) to just 27 (number of semantic types observed for frame elements in the training data). In practice, we define one binary indicator feature fs for each semantic type s observed at training time. Given a potential filler, we set the feature value of fs to 1 (otherwise 0) if and only if there exists a co-referent entity mention annotated as a fr"
Q15-1032,D07-1002,1,0.911714,"Missing"
Q15-1032,S12-1001,0,0.134012,"res based on discourse context are used to assign roles on the sentence level. Discourse-like features have been previously applied in models that deal with so-called implicit arguments, i.e., roles which are not locally realized but resolvable within the greater discourse context (Ruppenhofer et al., 2010; Gerber and Chai, 2012). Successful features for resolving implicit arguments include the distance between mentions and any discourse relations occurring between them (Gerber and Chai, 2012), roles assigned to mentions in the previous context, the discourse prominence of the denoted entity (Silberer and Frank, 2012), and its centering status (Laparra and Rigau, 2013). None of these features have been used in a standard SRL system to date (and trivially, not all of them will be helpful as, for example, the number of sentences between a predicate and an argument is always zero within a sentence). In this paper, we extend the contextual features used for resolving implicit arguments to the SRL task and show how a set of discourse-level enhancements can be added to a traditional sentence-level SRL model. 3 FrameNet The Berkeley FrameNet project (Ruppenhofer et al., 2010) develops a semantic lexicon and an an"
Q15-1032,Q15-1003,0,0.179993,"Missing"
Q15-1032,P10-1097,0,0.0614322,"Missing"
Q15-1032,P05-1073,0,0.167102,"Missing"
Q15-1032,P13-1086,0,0.0608974,"Missing"
Q15-1032,W04-3212,0,0.108074,"generate FrameNet XML files. For direct comparison with the previous state-of-the-art for FrameNetbased SRL, we further implement additional features used in the SEMAFOR system (Das et al., 2014) and combine the role labeling components of mate-tools with SEMAFOR’s preprocessing toolchain.3 All features used in our system are listed in Table 1. The main differences between our adaptation of mate-tools and SEMAFOR are as follows: whereas the latter implements identification and labeling of role fillers in one step, mate-tools follow the insight that these two steps are conceptually different (Xue and Palmer, 2004) and should be modeled separately. Accordingly, mate-tools contain a global reranking component which takes into account identification and labeling decisions while SEMAFOR only uses reranking techniques to filter overlapping argument predictions and other constraints (see Das et al., 2014 for details). We discuss the advantage of a global reranker for our setting in Section 5. 5 Extensions based on Context Context can be relevant for semantic role labeling in various different ways. In this section, we motivate and describe four extensions over previous approaches. The first extension is a se"
Q15-1032,C00-2137,0,0.19467,"Missing"
Q15-1032,J13-3006,0,\N,Missing
Q16-1003,W06-3812,0,0.0297294,"ongs given its surrounding context. Bayesian models have been previously developed for various tasks in lexical semantics (Brody and La32 A non-Bayesian approach is put forward in Mitra et al. (2014, 2015) who adopt a graph-based framework for representing word meaning (see Tahmasebi et al. (2011) for a similar earlier proposal). In this model words correspond to nodes in a semantic network and edges are drawn between words sharing contextual features (extracted from a dependency parser). A graph is constructed for each time interval, and nodes are clustered into senses with Chinese Whispers (Biemann, 2006), a randomized graph clustering algorithm. By comparing the induced senses for each time slice and observing intercluster differences, their method can detect whether senses emerge or disappear. independent (all that is needed is a time-stamped corpus and tools for basic pre-processing). Contrary to Mitra et al. (2014, 2015), we do not treat the tasks of inferring a semantic representation for words and their senses as two separate processes. Evaluation of models which detect meaning change is fraught with difficulties. There is no standard set of words which have undergone meaning change or b"
Q16-1003,E09-1013,1,0.34703,"Missing"
Q16-1003,P12-1011,0,0.0472124,"tatively evaluate our model on the SemEval-2015 benchmark datasets released as part of the Diachronic Text Evaluation exercise (Popescu and Strapparava 2015; DTE). In the following we first present the DTE subtasks, and then move on to describe our training data, parameter settings, and systems used for comparison to our model. SemEval DTE Tasks Diachronic text evaluation is an umbrella term used by the SemEval-2015 organizers to represent three subtasks aiming to assess the performance of computational methods used to identify when a piece of text was written. A similar problem is tackled in Chambers (2012) who label documents with time stamps whilst focusing on explicit time expressions and their discriminatory power. The SemEval data consists of news snippets, which range between a few words and multiple sentences. A set of training snippets, as well as gold-annotated development and test datasets are provided. DTE subtasks 1 and 2 involve temporal classification: given a news snippet and a set of non-overlapping time intervals covering the period 1700 through 2010, the system’s task is to select the interval corresponding to the snippet’s year of origin. Temporal intervals are consecutive and"
Q16-1003,C14-1154,0,0.523414,"tracking meaning change. Comparisons across evaluations and against a variety of related systems show that despite not being designed with any particular task in mind, our model performs competitively across the board. ´ S´eaghdha, 2010; Ritter et al., 2010) pata, 2009; O and word meaning change detection is no exception. Using techniques from non-parametric topic modeling, Lau et al. (2012) induce word senses (aka. topics) for a given target word over two time periods. Novel senses are then are detected based on the discrepancy between sense distributions in the two periods. Follow-up work (Cook et al., 2014; Lau et al., 2014) further explores methods for how to best measure this sense discrepancy. Rather than inferring word senses, Wijaya and Yeniterzi (2011) use a Topics-over-Time model and k-means clustering to identify the periods during which selected words move from one topic to another. 2 Our work draws ideas from dynamic topic modeling (Blei and Lafferty, 2006b) where the evolution of topics is modeled via (smooth) changes in their associated distributions over the vocabulary. Although the dynamic component of our model is closely related to previous work in this area (Mimno et al., 2008)"
Q16-1003,cook-stevenson-2010-automatically,0,0.0123938,"We therefore induce a global and consistent set of temporal representations for each word. Our model is knowledgelean (it does not make use of a parser) and language Related Work Most work on diachronic language change has focused on detecting whether and to what extent a word’s meaning changed (e.g., between two epochs) without identifying word senses and how these vary over time. A variety of methods have been applied to the task ranging from the use of statistical tests in order to detect significant changes in the distribution of terms from two time periods (Popescu and Strapparava, 2013; Cook and Stevenson, 2010), to training distributional similarity models on time slices (Gulordava and Baroni, 2011; Sagi et al., 2009), and neural language models (Kim et al., 2014; Kulkarni et al., 2015). Other work (Mihalcea and Nastase, 2012) takes a supervised learning approach and predicts the time period to which a word belongs given its surrounding context. Bayesian models have been previously developed for various tasks in lexical semantics (Brody and La32 A non-Bayesian approach is put forward in Mitra et al. (2014, 2015) who adopt a graph-based framework for representing word meaning (see Tahmasebi et al. (2"
Q16-1003,W11-2508,0,0.362115,"ssociation for Computational Linguistics, vol. 4, pp. 31–45, 2016. Action Editor: Tim Baldwin. Submission batch: 12/2015; Revision batch: 2/2016; Published 2/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. second sense relating to computer device. Moreover, it infers subtle changes within a single sense. For instance, in the 1970s the words {cable, ball, mousepad} were typical for the computer device sense, whereas nowadays the terms {optical, laser, usb} are more typical. Contrary to previous work (Mitra et al., 2014; Mihalcea and Nastase, 2012; Gulordava and Baroni, 2011) where temporal representations are learnt in isolation, our model assumes that adjacent representations are co-dependent, thus capturing the nature of meaning change being fundamentally smooth and gradual (McMahon, 1994). This also serves as a form of smoothing: temporally neighboring representations influence each other if the available data is sparse. Experimental evaluation shows that our model (a) induces temporal representations which reflect word senses and their development over time, (b) is able to detect meaning change between two time periods, and (c) is expressive enough to obtain"
Q16-1003,W14-2517,0,0.146248,"e Related Work Most work on diachronic language change has focused on detecting whether and to what extent a word’s meaning changed (e.g., between two epochs) without identifying word senses and how these vary over time. A variety of methods have been applied to the task ranging from the use of statistical tests in order to detect significant changes in the distribution of terms from two time periods (Popescu and Strapparava, 2013; Cook and Stevenson, 2010), to training distributional similarity models on time slices (Gulordava and Baroni, 2011; Sagi et al., 2009), and neural language models (Kim et al., 2014; Kulkarni et al., 2015). Other work (Mihalcea and Nastase, 2012) takes a supervised learning approach and predicts the time period to which a word belongs given its surrounding context. Bayesian models have been previously developed for various tasks in lexical semantics (Brody and La32 A non-Bayesian approach is put forward in Mitra et al. (2014, 2015) who adopt a graph-based framework for representing word meaning (see Tahmasebi et al. (2011) for a similar earlier proposal). In this model words correspond to nodes in a semantic network and edges are drawn between words sharing contextual fe"
Q16-1003,P14-1025,0,0.0852419,"hange. Comparisons across evaluations and against a variety of related systems show that despite not being designed with any particular task in mind, our model performs competitively across the board. ´ S´eaghdha, 2010; Ritter et al., 2010) pata, 2009; O and word meaning change detection is no exception. Using techniques from non-parametric topic modeling, Lau et al. (2012) induce word senses (aka. topics) for a given target word over two time periods. Novel senses are then are detected based on the discrepancy between sense distributions in the two periods. Follow-up work (Cook et al., 2014; Lau et al., 2014) further explores methods for how to best measure this sense discrepancy. Rather than inferring word senses, Wijaya and Yeniterzi (2011) use a Topics-over-Time model and k-means clustering to identify the periods during which selected words move from one topic to another. 2 Our work draws ideas from dynamic topic modeling (Blei and Lafferty, 2006b) where the evolution of topics is modeled via (smooth) changes in their associated distributions over the vocabulary. Although the dynamic component of our model is closely related to previous work in this area (Mimno et al., 2008), our model is spec"
Q16-1003,E12-1060,0,0.122072,", and (c) is expressive enough to obtain useful features for identifying the time interval in which a piece of text was written. Overall, our results indicate that an explicit model of temporal dynamics is advantageous for tracking meaning change. Comparisons across evaluations and against a variety of related systems show that despite not being designed with any particular task in mind, our model performs competitively across the board. ´ S´eaghdha, 2010; Ritter et al., 2010) pata, 2009; O and word meaning change detection is no exception. Using techniques from non-parametric topic modeling, Lau et al. (2012) induce word senses (aka. topics) for a given target word over two time periods. Novel senses are then are detected based on the discrepancy between sense distributions in the two periods. Follow-up work (Cook et al., 2014; Lau et al., 2014) further explores methods for how to best measure this sense discrepancy. Rather than inferring word senses, Wijaya and Yeniterzi (2011) use a Topics-over-Time model and k-means clustering to identify the periods during which selected words move from one topic to another. 2 Our work draws ideas from dynamic topic modeling (Blei and Lafferty, 2006b) where th"
Q16-1003,P12-2051,0,0.463582,"d a 31 Transactions of the Association for Computational Linguistics, vol. 4, pp. 31–45, 2016. Action Editor: Tim Baldwin. Submission batch: 12/2015; Revision batch: 2/2016; Published 2/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. second sense relating to computer device. Moreover, it infers subtle changes within a single sense. For instance, in the 1970s the words {cable, ball, mousepad} were typical for the computer device sense, whereas nowadays the terms {optical, laser, usb} are more typical. Contrary to previous work (Mitra et al., 2014; Mihalcea and Nastase, 2012; Gulordava and Baroni, 2011) where temporal representations are learnt in isolation, our model assumes that adjacent representations are co-dependent, thus capturing the nature of meaning change being fundamentally smooth and gradual (McMahon, 1994). This also serves as a form of smoothing: temporally neighboring representations influence each other if the available data is sparse. Experimental evaluation shows that our model (a) induces temporal representations which reflect word senses and their development over time, (b) is able to detect meaning change between two time periods, and (c) is"
Q16-1003,P14-1096,0,0.388383,"subsequently acquired a 31 Transactions of the Association for Computational Linguistics, vol. 4, pp. 31–45, 2016. Action Editor: Tim Baldwin. Submission batch: 12/2015; Revision batch: 2/2016; Published 2/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. second sense relating to computer device. Moreover, it infers subtle changes within a single sense. For instance, in the 1970s the words {cable, ball, mousepad} were typical for the computer device sense, whereas nowadays the terms {optical, laser, usb} are more typical. Contrary to previous work (Mitra et al., 2014; Mihalcea and Nastase, 2012; Gulordava and Baroni, 2011) where temporal representations are learnt in isolation, our model assumes that adjacent representations are co-dependent, thus capturing the nature of meaning change being fundamentally smooth and gradual (McMahon, 1994). This also serves as a form of smoothing: temporally neighboring representations influence each other if the available data is sparse. Experimental evaluation shows that our model (a) induces temporal representations which reflect word senses and their development over time, (b) is able to detect meaning change between"
Q16-1003,P10-1045,0,0.0125463,"Missing"
Q16-1003,I13-1040,0,0.0326748,"aging smooth change over time. We therefore induce a global and consistent set of temporal representations for each word. Our model is knowledgelean (it does not make use of a parser) and language Related Work Most work on diachronic language change has focused on detecting whether and to what extent a word’s meaning changed (e.g., between two epochs) without identifying word senses and how these vary over time. A variety of methods have been applied to the task ranging from the use of statistical tests in order to detect significant changes in the distribution of terms from two time periods (Popescu and Strapparava, 2013; Cook and Stevenson, 2010), to training distributional similarity models on time slices (Gulordava and Baroni, 2011; Sagi et al., 2009), and neural language models (Kim et al., 2014; Kulkarni et al., 2015). Other work (Mihalcea and Nastase, 2012) takes a supervised learning approach and predicts the time period to which a word belongs given its surrounding context. Bayesian models have been previously developed for various tasks in lexical semantics (Brody and La32 A non-Bayesian approach is put forward in Mitra et al. (2014, 2015) who adopt a graph-based framework for representing word meani"
Q16-1003,P10-1044,0,0.0102517,"al representations which reflect word senses and their development over time, (b) is able to detect meaning change between two time periods, and (c) is expressive enough to obtain useful features for identifying the time interval in which a piece of text was written. Overall, our results indicate that an explicit model of temporal dynamics is advantageous for tracking meaning change. Comparisons across evaluations and against a variety of related systems show that despite not being designed with any particular task in mind, our model performs competitively across the board. ´ S´eaghdha, 2010; Ritter et al., 2010) pata, 2009; O and word meaning change detection is no exception. Using techniques from non-parametric topic modeling, Lau et al. (2012) induce word senses (aka. topics) for a given target word over two time periods. Novel senses are then are detected based on the discrepancy between sense distributions in the two periods. Follow-up work (Cook et al., 2014; Lau et al., 2014) further explores methods for how to best measure this sense discrepancy. Rather than inferring word senses, Wijaya and Yeniterzi (2011) use a Topics-over-Time model and k-means clustering to identify the periods during whi"
Q16-1003,W09-0214,0,0.119047,"n (it does not make use of a parser) and language Related Work Most work on diachronic language change has focused on detecting whether and to what extent a word’s meaning changed (e.g., between two epochs) without identifying word senses and how these vary over time. A variety of methods have been applied to the task ranging from the use of statistical tests in order to detect significant changes in the distribution of terms from two time periods (Popescu and Strapparava, 2013; Cook and Stevenson, 2010), to training distributional similarity models on time slices (Gulordava and Baroni, 2011; Sagi et al., 2009), and neural language models (Kim et al., 2014; Kulkarni et al., 2015). Other work (Mihalcea and Nastase, 2012) takes a supervised learning approach and predicts the time period to which a word belongs given its surrounding context. Bayesian models have been previously developed for various tasks in lexical semantics (Brody and La32 A non-Bayesian approach is put forward in Mitra et al. (2014, 2015) who adopt a graph-based framework for representing word meaning (see Tahmasebi et al. (2011) for a similar earlier proposal). In this model words correspond to nodes in a semantic network and edges"
Q16-1003,S15-2142,0,0.0779009,"Missing"
Q16-1003,S15-2148,0,0.0690695,"Missing"
Q16-1003,S15-2144,0,0.0368928,"contain implicit ones, e.g., as indicated by lexical choice or spelling. The snippet in example (9) was published in 1891 and the spelling of to-day, which was common up to the early 20th century, is an implicit cue: (9) The local wheat market was not quite so strong to-day as yesterday. time intervals of differing granularity. For this task, which is admittedly harder, levels of temporal granularity are coarser corresponding to 6-year, 12-year and 20-year intervals. Participating SemEval Systems We compared our model against three other systems which participated in the SemEval task.8 AMBRA (Zampieri et al., 2015) adopts a learning-to-rank modeling approach and uses several stylistic, grammatical, and lexical features. IXA (Salaberri et al., 2015) uses a combination of approaches to determine the period of time in which a piece of news was written. This involves searching for specific mentions of time within the text, searching for named entities present in the text and then establishing their reference time by linking these to Wikipedia, using Google n-grams, and linguistic features indicative of language change. Finally, UCD (Szymanski and Lynch, 2015) employs SVMs for classification using a variety"
Q16-1003,S15-2147,0,\N,Missing
Q16-1010,D15-1138,0,0.00679455,"atkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vecto"
Q16-1010,P02-1041,0,0.0927099,"(Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on te"
Q16-1010,P14-1091,0,0.0366925,"Missing"
Q16-1010,P14-1133,0,0.232833,"Missing"
Q16-1010,Q15-1039,0,0.656552,"Missing"
Q16-1010,D13-1160,0,0.418417,"za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-arg"
Q16-1010,D14-1067,0,0.304718,"in and obtains the best result to date. Interestingly, D EP T REE outperforms S IMPLE G RAPH in this case. We attribute this to the small training set and larger lexical variation of Free917. The structural features of the graph-based representations seem highly beneficial in this case. 6.3 Error Analysis We categorized 100 errors made by D EP L AMBDA (+C +E) on the WebQuestions development set. In 43 cases the correct answer is present in the beam, 136 Method Cai and Yates (2013) Berant et al. (2013) Kwiatkowski et al. (2013) Yao and Van Durme (2014) Berant and Liang (2014) Bao et al. (2014) Bordes et al. (2014) Yao (2015) Yih et al. (2015) (FB API) Bast and Haussmann (2015) Berant and Liang (2015) Yih et al. (2015) (Y&C) Free917 Accuracy WebQuestions Average F1 59.0 62.0 68.0 – 68.5 – – – – 76.4 – – – 35.7 – 33.0 39.9 37.5 39.2 44.3 48.4 49.4 49.7 52.5 This Work D EP T REE S IMPLE G RAPH CCGG RAPH (+ C + E) D EP L AMBDA (+ C + E) 53.2 43.7 73.3 78.0 40.4 48.5 48.6 50.3 Table 3: Question-answering results on the WebQuestions and Free917 test sets. but ranked below an incorrect answer (e.g., for where does volga river start, the annotated gold answer is Valdai Hills, which is ranked second, with Russi"
Q16-1010,C04-1180,1,0.158689,"There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment."
Q16-1010,P13-1042,0,0.356088,"Missing"
Q16-1010,P15-1127,1,0.568655,"was sworn into office when john f kennedy was assassinated ), we do not have a special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from"
Q16-1010,W09-3726,0,0.0740413,"it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus, mimicking the stru"
Q16-1010,W02-1001,1,0.101306,"∈ &lt;n denotes the features for the pair of ungrounded and grounded graphs. Note that for a given query there may be multiple ungrounded graphs, primarily due to the optional use of the CON TRACT operation.3 The feature function has access to the ungrounded and grounded graphs, to the question, as well as to the content of the knowledge base and the denotation |g|K (the denotation of a grounded graph is defined as the set of entities or attributes reachable at its TARGET node). See Section 5.3 for the features employed. The model parameters are estimated with the averaged structured perceptron (Collins, 2002; Fre3 Another source of ambiguity may be a lexical item having multiple lambda-calculus entries; in our rules this only arises when analyzing count expressions such as how many. und and Schapire, 1999). Given a training questionanswer pair (q, A), the update is: θt+1 ← θt + Φ(u+ , g + , q, K) − Φ(ˆ u, gˆ, q, K) , where (u+ , g + ) denotes the pair of gold ungrounded and grounded graphs for q. Since we do not have direct access to these gold graphs, we instead rely on the set of oracle graphs, OK,A (q), as a proxy: (u+ , g + ) = arg max θt · Φ(u, g, q, K) , (u,g)∈OK,A (q) where OK,A (q) is def"
Q16-1010,P01-1019,0,0.126066,"nd Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph"
Q16-1010,C04-1026,0,0.10619,"tics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to a first-order logical form, 137 and present results on textual entailment. Our work, in contrast, assumes access only to dependency trees and offers an alternative method based on the lambda calculus,"
Q16-1010,P15-1026,0,0.238247,"et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grounded to Freebase by learning from question-answer pairs. E"
Q16-1010,P14-1134,0,0.0159776,"gh an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting represent"
Q16-1010,E03-1030,0,0.118015,"araphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TAG (Gardent and Kallmeyer, 2003; Joshi et al., 2007) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015). However, few have used dependency structures for this purpose. Debusmann et al. (2004) and Cimiano (2009) describe grammar-based conversions of dependencies to semantic representations, but do not validate them empirically. Stanovsky et al. (2016) use heuristics based on linguistic grounds to convert dependencies to proposition structures. B´edaride and Gardent (2011) propose a graph-rewriting technique to convert a graph built from dependency trees and semantic role structures to"
Q16-1010,P09-1069,0,0.0392655,"entation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to ra"
Q16-1010,P15-1143,0,0.0199683,"ntic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally deriving logical forms. The resulting representation is subsequently grou"
Q16-1010,N10-1145,0,0.0444525,"ource semantic representation and the target application’s representation is an inherent problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion"
Q16-1010,D12-1069,0,0.104733,"cquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances ove"
Q16-1010,Q15-1019,0,0.0603458,"s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparable method that generates ungrounded logical forms using"
Q16-1010,D10-1119,1,0.364959,"tructures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of"
Q16-1010,D13-1161,1,0.949182,"lambda-calculus expression and the relabeled s-expression is beta-reduced to give the logical form in Figure 1(c). Since dependency syntax does not have an associated type theory, we introduce a type system that assigns a single type to all constituents, thus avoiding the need for type checking (Section 2). D EP L AMBDA uses this system to generate robust logical forms, even when the dependency structure does not mirror predicate-argument relationships in constructions such as conjunctions, prepositional phrases, relative clauses, and wh-questions (Section 3). These ungrounded logical forms (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015) are used for question answering against Freebase, by passing them as input to G RAPH PARSER (Reddy et al., 2014), a system that learns to map logical predicates to Freebase, resulting in grounded Freebase queries (Section 4). We show that our approach achieves state-of-the-art performance on the Free917 dataset and competitive performance on the WebQuestions dataset, whereas building the Freebase queries directly from dependency trees gives significantly lower performance. Finally, we show that our approach outperforms a directly comparab"
Q16-1010,D14-1107,1,0.663912,"XPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connected to an edge. After an edge is grounded, the entity type nodes connected to it are grounded in turn, before the next edge is processed. To restrict the search, if two beam items correspond to the same grounded graph, the one with the lower score is discarded. A beam size of 100 was used in all experiments. Features. We use the f"
Q16-1010,P11-1060,0,0.141153,"plication’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provides us with a robust way of compositionally der"
Q16-1010,N15-1114,0,0.0108635,"representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al., 2001; Copestake et al., 2005), TA"
Q16-1010,P14-5010,0,0.00310002,"presentation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use eight handcrafted part-of-speech patterns to identify entity span candidates. We use the Stanford CoreNLP caseless tagger for part-of-speech tagging (Manning et al., 2014). For each candidate mention span, we retrieve the top 10 entities according to the Freebase API.5 We then create a lattice in which the nodes correspond to mention-entity pairs, scored by their Freebase API scores, and the edges encode the fact that no joint assignment of entities to mentions can contain overlapping spans. Finally, we generate ungrounded graphs for the top 10 paths through the lattice and treat the final entity disambiguation as part of the semantic parsing problem. 4 5 http://github.com/sivareddyg/graph-parser http://developers.google.com/freebase/ Representation -C -E -C +E"
Q16-1010,P13-2109,0,0.018844,"robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since our approach uses dependency trees as input, we hypothesize that it will generalize better to domains that are well covered by dependency parsers than methods that induce semantic grammars from scratch. The system that maps a dependency tree to its logical form (hencefo"
Q16-1010,H05-1066,0,0.0191937,"Missing"
Q16-1010,P14-1041,0,0.019399,"special treatment for them in the semantic representation. Differences in syntactic parsing performance and the somewhat limited expressivity of the semantic representation are likely the reasons for CCGG RAPH’s lower performance. 7 Related Work There are two relevant strands of prior work: general purpose ungrounded semantics and grounded semantic parsing. The former have been studied on their own and as a component in tasks such as semantic parsing to knowledge bases (Kwiatkowski et al., 2013; Reddy et al., 2014; Choi et al., 2015; Krishnamurthy and Mitchell, 2015), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015) and relation extraction (Rockt¨aschel et al., 2015). There are two ways of generating these representations: either relying on syntactic structure and producing the semantics post hoc, or generating it directly from text. We adopt the former approach, which was pioneered by Montague (1973) and is becoming increasingly attractive with the advent of accurate syntactic parsers. There have been extensive studies on extracting semantics from syntactic representations such as LFG (Dalrymple et al., 1995), HPSG (Copestake et al.,"
Q16-1010,N15-1077,0,0.0234129,"ith approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given q"
Q16-1010,P15-1146,0,0.0242314,"Missing"
Q16-1010,P13-1092,0,0.00889818,"nt problem with approaches using general-purpose representations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential"
Q16-1010,Q14-1030,1,0.452199,"mbda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of t"
Q16-1010,N15-1118,0,0.0159445,"Missing"
Q16-1010,N15-1040,0,0.0128228,"epresentations. Kwiatkowski et al. (2013) propose lambda-calculus operations to generate multiple type-equivalent expressions to handle this mismatch. In contrast, we use graph-transduction operations which are relatively easier to interpret. There is also growing work on converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and a"
Q16-1010,N06-1056,0,0.0904852,"trongest result to date on Free917 and competitive results on WebQuestions. 1 Disney acquired Pixar nnp vbd nnp (nsubj (dobj acquired Pixar) Disney) (b) The s-expression for the dependency tree. λx. ∃yz. acquired(xe ) ∧ Disney(ya ) ∧ Pixar(za ) ∧ arg1 (xe , ya ) ∧ arg2 (xe , za ) (c) The composed lambda-calculus expression. Figure 1: The dependency tree is binarized into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et"
Q16-1010,P07-1121,0,0.0158316,"converting syntactic structures to the target application’s structure without going through an intermediate semantic representation, e.g., answer-sentence selection (Punyakanok et al., 2004; Heilman and Smith, 2010; Yao et al., 2013) and semantic parsing (Ge and Mooney, 2009; Poon, 2013; Parikh et al., 2015; Xu et al., 2015; Wang et al., 2015; Andreas and Klein, 2015). A different paradigm is to directly parse the text into a grounded semantic representation. Typically, an over-generating grammar is used whose accepted parses are ranked (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Groschwitz et al., 2015). In contrast, Bordes et al. (2014) and Dong et al. (2015) discard the notion of a target representation altogether and instead learn to rank potential answers to a given question by embedding questions and answers into the same vector space. 8 Conclusion We have introduced a method for converting dependency structures to logical forms using the lambda calculus. A key idea of this work is the use of a single semantic type for every constituent of the dependency tree, which provid"
Q16-1010,P15-1049,0,0.0484602,"Missing"
Q16-1010,P14-1090,0,0.408064,"Missing"
Q16-1010,N13-1106,0,0.0699006,"Missing"
Q16-1010,N15-3014,0,0.0687989,"iginal dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the parent is linked to the event with an edge labeled arg0 . If a word is a question word, an additional TARGET predicate is attached to its entity node. S IMPLE G RAPH. This representation has a single event to which all entities in the question are connected by the predicate arg1 . An additional TARGET node is connected to the event by the predicate arg0 . This is similar to the template representation of Yao (2015) and Bast and Haussmann (2015). Note that this cannot represent any compositional structure. CCGG RAPH. Finally, we compare to the CCGbased semantic representation of Reddy et al. (2014), adding the CONTRACT and EXPAND operations to increase its expressivity. 5.3 Implementation Details Below are more details of our entity resolution model, the syntactic parser used, features in the grounding model and the beam search procedure. Entity Resolution. For Free917, we follow prior work and resolve entities by string match against the entity lexicon provided with the dataset. For WebQuestions, we use"
Q16-1010,P15-1128,0,0.133944,"into its s-expression, which is then composed into the lambda expression representing the sentence logical form. Semantic parsers map sentences onto logical forms that can be used to query databases (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006), instruct robots (Chen and Mooney, 2011), extract information (Krishnamurthy and Mitchell, 2012), or describe visual scenes (Matuszek et al., 2012). Current systems accomplish this by learning task-specific grammars (Berant et al., 2013), by using strongly-typed CCG grammars (Reddy et al., 2014), or by eschewing the use of a grammar entirely (Yih et al., 2015). b dobj (a) The dependency tree for Disney acquired Pixar. Introduction a root In recent years, there have been significant advances in developing fast and accurate dependency parsers for many languages (McDonald et al., 2005; Nivre et al., 2007; Martins et al., 2013, inter alia). Motivated by the desire to carry these advances over to semantic parsing tasks, we present a robust method for mapping dependency trees to logical forms that represent underlying predicate-argument structures.1 We empirically validate the utility of these logical forms for question answering from databases. Since ou"
Q16-1010,P14-2107,0,0.0173573,".0 73.4 (c) Accuracy 42.6 48.2 46.5 48.8 42.6 48.2 48.9 50.4 D EP T REE S IMPLE G RAPH CCGG RAPH D EP L AMBDA Table 1: Oracle statistics and accuracies on the Web21.3 40.9 68.3 69.3 21.3 40.9 69.4 71.3 Table 2: Oracle statistics and accuracies on the Free917 Questions development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. development set. +(-)C: with(out) CONTRACT. +(-)E: with(out) EXPAND. Syntactic Parsing. We recase the resolved entity mentions and run a case-sensitive second-order conditional random field part-of-speech tagger (Lafferty et al., 2001). The hypergraph parser of Zhang and McDonald (2014) is used for dependency parsing. The tagger and parser are both trained on the OntoNotes 5.0 corpus (Weischedel et al., 2011), with constituency trees converted to Stanford-style dependencies (De Marneffe and Manning, 2008). To derive the CCG-based representation, we use the output of the EasyCCG parser (Lewis and Steedman, 2014). edge, we can ground the edge to a Freebase relation, contract the edge in either direction, or skip the edge. For an entity type node, we can ground the node to a Freebase type, or skip the node. The order of traversal is based on the number of named entities connect"
Q16-1010,J90-1001,0,\N,Missing
Q16-1010,D15-1198,0,\N,Missing
Q18-1001,D15-1075,0,0.0381959,"inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2016), and notably question answering based on text (Hermann et al., 1 Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understanding is an offline process, models are expected to digest large amounts of data before being able to answer a question, or make inferences. They"
Q18-1001,D13-1128,0,0.0286237,"the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve over time. Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have also been aligned to scre"
Q18-1001,N15-1113,1,0.773651,"aphide and Huang, 2001). A few datasets have been released recently which include movies and textual data. MovieQA (Tapaswi et al., 2016) is a large-scale dataset which contains 408 movies and 14,944 questions, each accompanied with five candidate answers, one of which is correct. For some movies, the dataset also contains subtitles, video clips, scripts, plots, and text from the Described Video Service (DVS), a narration service for the visually impaired. MovieDescription (Rohrbach et al., 2017) is a related dataset which contains sentences aligned to video clips from 200 movies. Scriptbase (Gorinski and Lapata, 2015) is another movie database which consists of movie screenplays (without video) and has been used to generate script summaries. In contrast to the story comprehension tasks envisaged in MovieQA and MovieDescription, we focus on a single cinematic genre (i.e., crime series), and have access to entire episodes (and their corresponding screenplays) as opposed to video-clips or DVSs for some of the data. Rather than answering multiple factoid questions, we aim to solve a single problem, albeit one that is inherently challenging to both humans and machines. Question Answering A variety of question a"
Q18-1001,D14-1005,0,0.0209416,"deling problem (Section 4). We describe our experiments in Section 5. 2 Related Work Our research has connections to several lines of work in natural language processing, computer vision, and more generally multi-modal learning. We review related literature in these areas below. Language Grounding Recent years have seen increased interest in the problem of grounding language in the physical world. Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model"
Q18-1001,N15-1016,0,0.0247117,"3) and formalize the modeling problem (Section 4). We describe our experiments in Section 5. 2 Related Work Our research has connections to several lines of work in natural language processing, computer vision, and more generally multi-modal learning. We review related literature in these areas below. Language Grounding Recent years have seen increased interest in the problem of grounding language in the physical world. Various semantic space models have been proposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in addition"
Q18-1001,N15-1174,1,0.859125,"roposed which learn the meaning of words based on linguistic and visual or acoustic input (Bruni et al., 2014; Silberer et al., 2016; Lazaridou et al., 2015; Kiela and Bottou, 2014). A variety of cross-modal methods which fuse techniques from image and text processing have also been applied to the tasks of generating image descriptions and retrieving images given a natural language query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve over time. Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have"
Q18-1001,D14-1162,0,0.0818224,"Missing"
Q18-1001,D16-1264,0,0.275576,"odal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2016), and notably question answering based on text (Hermann et al., 1 Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understanding is an offline process, models are expected to digest large amounts of data before being"
Q18-1001,D13-1020,0,0.183112,"which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input. 1 Introduction The success of neural networks in a variety of applications (Sutskever et al., 2014; Vinyals et al., 2015) and the creation of large-scale datasets have played a critical role in advancing machine understanding of natural language on its own or together with other modalities. The problem has assumed several guises in the literature such as reading comprehension (Richardson et al., 2013; Rajpurkar et al., 2016), recognizing textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2016), and notably question answering based on text (Hermann et al., 1 Our dataset is available at https://github.com/ EdinburghNLP/csi-corpus. 2015; Weston et al., 2015), images (Antol et al., 2015), or video (Tapaswi et al., 2016). In order to make the problem tractable and amenable to computational modeling, existing approaches study isolated aspects of natural language understanding. For example, it is assumed that understanding is an offline process, models are expected to digest large amo"
Q18-1001,N15-1173,0,0.028616,"uage query (Vinyals et al., 2015; Xu et al., 2015; Karpathy and Fei-Fei, 2015). Another strand of research focuses on how to explicitly encode the underlying semantics of images making use of structural representations (Ortiz et al., 2015; Elliott and Keller, 2013; Yatskar et al., 2016; Johnson et al., 2015). Our work shares the common goal of grounding language in additional modalities. Our model is, however, not static, it learns representations which evolve over time. Video Understanding Work on video understanding has assumed several guises such as generating descriptions for video clips (Venugopalan et al., 2015a; Venugopalan et al., 2015b), retrieving video clips with natural language queries (Lin et al., 2014), learning actions in video (Bojanowski et al., 2013), and tracking characters (Sivic et al., 2009). Movies have also been aligned to screenplays (Cour et al., 2008), plot synopses (Tapaswi et al., 2015), and books (Zhu et al., 2015) with the aim of improving scene prediction and semantic browsing. Other work uses low-level features (e.g., based on face detection) to establish social networks of main characters in order to summarize movies or perform genre Peter Berglund: Grissom doesn't look"
Q18-1001,D15-1237,0,0.0941421,"Missing"
Q18-1002,baccianella-etal-2010-sentiwordnet,0,0.0149031,"g is superior to more conventional neural architectures and other baselines on detecting segment sentiment and extracting informative opinions in reviews.1 2 Background Our work lies at the intersection of multiple research areas, including sentiment classification, opinion mining and multiple instance learning. We review related work in these areas below. Sentiment Classification Sentiment classification is one of the most popular tasks in sentiment analysis. Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 201"
Q18-1002,D15-1263,0,0.16059,"arning from individually labeled segments, our model only requires document-level supervision and learns to introspectively judge the sentiment of constituent segments. Beyond showing how to utilize document collections of rated reviews to train fine-grained sentiment predictors, we also investigate the granularity of the extracted segments. Previous research (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classification (Bhatia et al., 2015), we also represent documents via Rhetorical Structure Theory’s (Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although definitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003). We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes document-level sentiment supervision to judge the polarity of its constituent segments; the creation of S POT, a pu"
Q18-1002,E06-1039,0,0.142926,"Missing"
Q18-1002,P16-1046,1,0.816535,". Action Editor: Ani Nenkova. Submission batch: 7/17; Revision batch: 11/2017; Published 1/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Instead of learning from individually labeled segments, our model only requires document-level supervision and learns to introspectively judge the sentiment of constituent segments. Beyond showing how to utilize document collections of rated reviews to train fine-grained sentiment predictors, we also investigate the granularity of the extracted segments. Previous research (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classification (Bhatia et al., 2015), we also represent documents via Rhetorical Structure Theory’s (Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although definitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003). We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this wo"
Q18-1002,P02-1057,0,0.244571,"Missing"
Q18-1002,W14-4408,0,0.105878,"Missing"
Q18-1002,P12-1007,0,0.379321,"search (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences. Inspired by recent work in summarization (Li et al., 2016) and sentiment classification (Bhatia et al., 2015), we also represent documents via Rhetorical Structure Theory’s (Mann and Thompson, 1988) Elementary Discourse Units (EDUs). Although definitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003). We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them. Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes document-level sentiment supervision to judge the polarity of its constituent segments; the creation of S POT, a publicly available dataset which contains Segment-level POlariTy annotations (for sentences and EDUs) and can be used for the evaluation of MIL-style models like ours; and the empirical finding (through automatic and human-based evaluation) that neural multiple instance learning is superior to more conventional neural architectures and other baselines on d"
Q18-1002,C10-1039,0,0.263866,"Missing"
Q18-1002,D14-1168,0,0.296134,"Missing"
Q18-1002,P11-1055,0,0.0547248,"our work. Applications of MIL are many and varied. MIL was first explored by Keeler and Rumelhart (1992) for recognizing handwritten post codes, where the position and value of individual digits was unknown. MIL techniques have since been applied to drug activity prediction (Dietterich et al., 1997), image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011). When applied to sentiment analysis, MIL takes advantage of supervision signals on the document level in order to train segment-level sentiment predictors. Although their work is not couched in the framework of MIL, T¨ackstr¨om and McDonald (2011) show how sentence sentiment labels can be learned as latent variables from document-level annotations using hidden conditional random fields. Pappas and Popescu-Belis (2014) use a multiple instance regression model to assign sentiment scores to specific aspects of products. The Group-Instance Cost Function (GICF), proposed by Kotzias et al. (2015),"
Q18-1002,N15-1011,0,0.0162734,"the proliferation of user-generated content in the form of online reviews, blogs, internet forums, and social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016), sentences (Kim, 2014), or phrases (Socher et al., 2011; Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast, the acquisition of sentence- or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summar"
Q18-1002,D14-1181,0,0.418604,"d social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016), sentences (Kim, 2014), or phrases (Socher et al., 2011; Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast, the acquisition of sentence- or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summarizing consumer opinions in online product reviews. The usefulness of finer-grained sentim"
Q18-1002,E09-1059,0,0.0697974,"Missing"
Q18-1002,W16-3617,0,0.0631462,"Missing"
Q18-1002,P05-1015,0,0.2478,"(Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granular"
Q18-1002,W02-1011,0,0.0253682,"sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments"
Q18-1002,D14-1052,0,0.177658,"pes is required to assign a bag label. Zhou et al. (2009) used graph kernels to aggregate predictions, exploiting relations between instances in object and text categorization. Xu and Frank (2004) proposed a multiple-instance logistic regression classifier where instance predictions were simply averaged, assuming equal and independent contribution toward bag classification. More recently, Kotzias et al. (2015) used sentence vectors obtained by a pre-trained hierarchical CNN (Denil et al., 2014) as features under an unweighted average MIL objective. Prediction averaging was further extended by Pappas and Popescu-Belis (2014; 2017), who used a weighted summation of predictions, an idea which we also adopt in our work. Applications of MIL are many and varied. MIL was first explored by Keeler and Rumelhart (1992) for recognizing handwritten post codes, where the position and value of individual digits was unknown. MIL techniques have since been applied to drug activity prediction (Dietterich et al., 1997), image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classification (Andrews and Hofmann, 2004), image captioning (W"
Q18-1002,C10-1103,0,0.0298267,"d Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granularity (i.e., words,"
Q18-1002,D11-1014,0,0.124323,"Missing"
Q18-1002,D13-1170,0,0.319441,"d summary of a 2-out-of-5 star review with positive and negative snippets. Introduction Sentiment analysis has become a fundamental area of research in Natural Language Processing thanks to the proliferation of user-generated content in the form of online reviews, blogs, internet forums, and social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016), sentences (Kim, 2014), or phrases (Socher et al., 2011; Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast,"
Q18-1002,J11-2001,0,0.872936,"inion mining and multiple instance learning. We review related work in these areas below. Sentiment Classification Sentiment classification is one of the most popular tasks in sentiment analysis. Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their abi"
Q18-1002,D15-1167,0,0.889209,"form of online reviews, blogs, internet forums, and social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016), sentences (Kim, 2014), or phrases (Socher et al., 2011; Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast, the acquisition of sentence- or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summarizing consumer opinions in online product revi"
Q18-1002,P02-1053,0,0.00966305,"ed evaluation) that neural multiple instance learning is superior to more conventional neural architectures and other baselines on detecting segment sentiment and extracting informative opinions in reviews.1 2 Background Our work lies at the intersection of multiple research areas, including sentiment classification, opinion mining and multiple instance learning. We review related work in these areas below. Sentiment Classification Sentiment classification is one of the most popular tasks in sentiment analysis. Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee,"
Q18-1002,P12-2018,0,0.0438211,"ianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granularity (i.e., words, phrases, and sentences). We describe Kim’s"
Q18-1002,C10-2153,0,0.0278106,"e et al., 2005; Baccianella et al., 2010) based on which the overall po1 Our code and S POT dataset are publicly available at: https://github.com/stangelid/milnet-sent 18 larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words). More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment. Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013). Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering. Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granularity (i.e., words, phrases, and senten"
Q18-1002,N16-1174,0,0.749617,"ews, blogs, internet forums, and social media. A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions. The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification. Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016), sentences (Kim, 2014), or phrases (Socher et al., 2011; Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts. Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews). In contrast, the acquisition of sentence- or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summarizing consumer opinions in online product reviews. The usefulness"
Q18-1002,Q14-1034,0,\N,Missing
Q18-1005,D15-1263,0,0.0584554,"ty of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yang.liu2@ed.ac.uk,mlap@inf.ed.ac.uk Abstract ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model tha"
Q18-1005,D15-1075,0,0.0261047,"amely natural language inference. We then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language. Our 68 Natural Language Inference The ability to reason about the semantic relationship between two sentences is an integral part of text understanding. We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral. For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation. After removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9,824 for testing. Sentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows. Let [xp1 , · · · , xpn ] and [xh1 , · · · , xhm ] be the input vectors for the premise and hypothesis, respectively. Application of structured attention yields new vector reph ]. Then resentation"
Q18-1005,P16-1139,0,0.0515415,"r two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels. The hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the Models Classifier with handcrafted features (Bowman et al., 2015) 300D LSTM encoders (Bowman et al., 2015) 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 200D Matching LSTMs (Wang and Jiang, 2016) 450D LSTMN with deep attention fusion (Cheng et al., 2016) Decomposable Attention over word embeddings (Parikh et al., 2016) Enhanced BiLSTM Inference Model (Chen et al., 2017) 175D No Attention 175D Simple intra-sentence attention 100D Structured intra-sentence attention with Inside-Outside 175D Structured intra-sentence attention with Matrix Inversion Acc 78.2 80.6 83.2 83.5 86.1 86.3 86.8 88.0 85.3 86.2 86.8 86.9 θ — 3.0M 3.7M 252K 1.9M 3.4M 582K 4.3M 600K 1.1M 1.2M 1.1M Table 1: Test accu"
Q18-1005,R13-1016,0,0.173339,"ention (sentence-level) 100D Structured Attention (document-level) 100D Structured Attention (both levels) Yelp IMDB CZ Movies Debates θ 59.8 40.9 78.5 74.0 — 57.7 34.1 — —— 59.7 — — — — 63.7 42.5 — — — 65.1 45.3 — — — — — — 75.7 — 68.2 49.4 80.8 74.0 273K 66.7 47.5 80.5 73.7 330K 67.7 48.2 81.4 75.3 860K 68.0 48.8 81.5 74.6 842K 67.8 48.6 81.1 75.2 842K 68.6 49.2 82.1 76.5 860K Table 4: Test accuracy on four datasets and number of parameters θ (excluding embeddings). Regarding feature-based classification methods, results on Yelp and IMDB are taken from Tang et al. (2015a), on CZ movies from Brychcın and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the recurrent unit (LSTM or GRU). 4.2 Document Classification In this section, we evaluate our document-level model on a variety of classification tasks. We selected four datasets which we describe below. Table 3 summarizes some statistics for each dataset. Yelp reviews were obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we"
Q18-1005,W01-1605,0,0.333608,"Missing"
Q18-1005,P17-1152,0,0.0371546,"ton et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the Models Classifier with handcrafted features (Bowman et al., 2015) 300D LSTM encoders (Bowman et al., 2015) 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 200D Matching LSTMs (Wang and Jiang, 2016) 450D LSTMN with deep attention fusion (Cheng et al., 2016) Decomposable Attention over word embeddings (Parikh et al., 2016) Enhanced BiLSTM Inference Model (Chen et al., 2017) 175D No Attention 175D Simple intra-sentence attention 100D Structured intra-sentence attention with Inside-Outside 175D Structured intra-sentence attention with Matrix Inversion Acc 78.2 80.6 83.2 83.5 86.1 86.3 86.8 88.0 85.3 86.2 86.8 86.9 θ — 3.0M 3.7M 252K 1.9M 3.4M 582K 4.3M 600K 1.1M 1.2M 1.1M Table 1: Test accuracy on the SNLI dataset and number of parameters θ (excluding embeddings). Wherever available we also provide the size of the recurrent unit. Models No Attention Simple Attention Matrix Inversion Inside-Outside Speed Max Avg 0.0050 0.0033 0.0057 0.0042 0.0070 0.0045 0.1200 0.03"
Q18-1005,D16-1053,1,0.381844,"ions of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Proc"
Q18-1005,P12-1007,0,0.0351431,"xt Representations Yang Liu and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yang.liu2@ed.ac.uk,mlap@inf.ed.ac.uk Abstract ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from"
Q18-1005,W16-3616,0,0.185888,"ical model over latent variables. They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the inside-outside algorithm to normalize the scores with the marginal probabilities of a dependency tree. Without recourse to an external parser, their model learns meaningful task-specific dependency structures, achieving competitive results in several sentence-level tasks. However, for document modeling, this approach has two drawbacks. Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006). As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross. Secondly, the inside-outside algorithm involves a dynamic programming process which is difficult to parallelize, making it impractical for modeling long documents.1 In this paper, we propose a new model for representing documents while automatically learning richer structural dependencies. Using a variant of Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984), our model implicitly considers non-projective depen1 In our experiments, adding the inside-outside pass incre"
Q18-1005,D13-1158,0,0.0469277,"ext vectors are concatenated with ei and transformed with weights Wr ∈ Rke ∗3ke to obtain the updated semantic vector ri ∈ Rke with rich structural information (see Figure 3). 3.3 Document Model We build document representations hierarchically: sentences are composed of words and documents are composed of sentences. Composition on the document level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013). As illustrated in Figure 4, given a document with n sentences [s1 , s2 , · · · , sn ], for each sentence si , the input is a sequence of word embeddings [ui1 , ui2 , · · · , uim ], where m is the number of tokens in si . By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri1 , ri2 , · · · , rim ]. Then a pooling operation produces a fixed-length vector vi for each sentence. Analogously, we view the document as a sequence of sentence vectors [v1 , v2 , · · · , vn ] whose embeddings are fed to"
Q18-1005,P82-1020,0,0.852256,"Missing"
Q18-1005,P17-1092,0,0.456212,"n, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projec"
Q18-1005,D07-1015,0,0.721659,"Missing"
Q18-1005,P14-1003,0,0.026708,"and eroot is a special embedding for the root node. The context vectors are concatenated with ei and transformed with weights Wr ∈ Rke ∗3ke to obtain the updated semantic vector ri ∈ Rke with rich structural information (see Figure 3). 3.3 Document Model We build document representations hierarchically: sentences are composed of words and documents are composed of sentences. Composition on the document level also makes use of structured attention in the form of a dependency graph. Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013). As illustrated in Figure 4, given a document with n sentences [s1 , s2 , · · · , sn ], for each sentence si , the input is a sequence of word embeddings [ui1 , ui2 , · · · , uim ], where m is the number of tokens in si . By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri1 , ri2 , · · · , rim ]. Then a pooling operation produces a fixed-length vector vi for each sentence. Analogously, we view the document as a sequence of sente"
Q18-1005,P11-1100,0,0.0185251,"Missing"
Q18-1005,Q16-1037,0,0.0136631,"ucing structural information. We applied this approach to model documents hierarchically, incorporating both sentenceand document-level structure. Experiments on sentence and document modeling tasks show that the representations learned by our model achieve competitive performance against strong comparison systems. Analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser. Directions for future work are many and varied. Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism. We also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or even extract important content. Acknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760). We also thank the anonymous TACL reviewers and the action editor whose feedback helped improve the present paper, members of EdinburghNLP for helpful discussions and suggestions, and Barbora Skarabela for tr"
Q18-1005,D17-1133,1,0.796561,"g Liu and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yang.liu2@ed.ac.uk,mlap@inf.ed.ac.uk Abstract ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015). The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure. For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content. In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empo"
Q18-1005,P14-5010,0,0.00398433,"Missing"
Q18-1005,W07-2216,0,0.0247203,"end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser. The inside-outside model of Kim et al. (2017) and our model both have a O(n3 ) worst case complexity. However, major operations in our approach can be parallelized efficiently on GPU computing hardware. Although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences. Advantageously, it can induce non-projective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007). Our contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efficient training procedure which allows to compute document level representations of arbitrary length; and a large scale evaluation study showing that the proposed model performs competitively against strong baselines while inducing intermediate structures which are both interpretable and meaningful. 2 Background In this section, we describe how previous work uses the attention mechanism for representing individual sentences. The key idea is to c"
Q18-1005,S15-1036,0,0.0308688,"Missing"
Q18-1005,W13-3303,0,0.0285593,"e structural biases. Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical StrucLinguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers). Unfortunately, the relian"
Q18-1005,D16-1147,0,0.0189763,"aper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capturing the element and an “infinite” window around it. Specifically, we run a bidirectional LSTM over sentence T , and take the output vectors [h1 , h2 , · · · , hn ] as the representations of words in T , where ht ∈ Rk is the output vector for word ut based on its context. We then exploit the structure of T which we induce based on an attention mechanism detailed below to obtain more precise representations. Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSTM output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts: [et , dt ] = ht (7) Rkt , where et ∈ the semantic vector, encodes semantic information for specific tasks, and dt ∈ Rks , the structure vector, is used to calculate structured attention. We use a series of operations based on the MatrixTree Theorem (Tutte, 1984) to incorporate the struc66 Structured Attention Mechanism Dependency representations of natural lan"
Q18-1005,D16-1244,0,0.0309547,"Missing"
Q18-1005,D14-1162,0,0.121071,"(rjh ) m X exp(oij ) Pm r¯ip = [rip , ] k=1 exp(oik ) r¯ih = [rih , j=1 m X i=1 p r = n X i=1 g(¯ rip ), exp(oij ) Pm ] k=1 exp(okj ) h r = m X g(¯ rih ) (20) (21) (22) (23) i=1 where M LP () is a two-layer perceptron with a ReLU activation function. The new representations r p , r h are then concatenated and fed into another two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels. The hidden size of the LSTM was set to 150. The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50. We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings. All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the Models Classifier with handcrafted features (Bowman et al., 2015) 300D LSTM encoders (Bowman et al., 2015) 300D Stack-Augmented Parser-Interpreter Neural Net (Bowman et al., 2016) 100D LSTM with inter-attention (Rockt¨aschel et al., 2016) 200D Matching LSTMs (Wang and Jiang, 2016) 450D LSTMN with deep attention fusion (Cheng et al., 2016) Decomposable Attention over word embeddings (Parikh et al., 2016) Enhanced BiLSTM Inference Model (Chen et al., 2017"
Q18-1005,prasad-etal-2008-penn,0,0.0240246,"Missing"
Q18-1005,D15-1167,0,0.690205,"rmance. It is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. The main idea is 63 Transactions of the Association for Computational Linguistics, vol. 6, pp. 63–75, 2018. Action Editor: Bo Pang. Submission batch: 5/2017; Published 1/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts. Their model learns to pay more or less attention to individual sentences when constructing the representation of the document. Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce stru"
Q18-1005,P15-1098,0,0.518433,"rmance. It is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations. The main idea is 63 Transactions of the Association for Computational Linguistics, vol. 6, pp. 63–75, 2018. Action Editor: Bo Pang. Submission batch: 5/2017; Published 1/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b). Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts. Their model learns to pay more or less attention to individual sentences when constructing the representation of the document. Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016). Kim et al. (2017) introduce stru"
Q18-1005,W06-1639,0,0.0129875,"associated with user ratings ranging from 1 to 10. Czech reviews were obtained from Brychcın and Habernal (2013). The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative. We include Czech in our experiments since it has more flexible word order compared to English, with non-projective dependency structures being more frequent. Experiments on this dataset perform 10-fold cross-validation following previous work (Brychcın and Habernal, 2013). 2 http://www.csfd.cz/ 70 Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005. Each debate consists of a series of speech segments, each labeled by the vote (“yea” or “nay”) cast for the proposed bill by the the speaker of each segment. We used the pre-processed corpus from Yogatama and Smith (2014).3 Following previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token. Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on"
Q18-1005,N16-1170,0,0.0432669,"Missing"
Q18-1005,N16-1174,0,0.685993,"iate structures which are both interpretable and meaningful. 1 Introduction Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013). Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016). Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical StrucLinguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers). Unfortunately, the reliance on labeled data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling. Moreover, despite r"
Q18-1005,P14-1074,0,0.0313941,"on (document-level) 100D Structured Attention (both levels) Yelp IMDB CZ Movies Debates θ 59.8 40.9 78.5 74.0 — 57.7 34.1 — —— 59.7 — — — — 63.7 42.5 — — — 65.1 45.3 — — — — — — 75.7 — 68.2 49.4 80.8 74.0 273K 66.7 47.5 80.5 73.7 330K 67.7 48.2 81.4 75.3 860K 68.0 48.8 81.5 74.6 842K 67.8 48.6 81.1 75.2 842K 68.6 49.2 82.1 76.5 860K Table 4: Test accuracy on four datasets and number of parameters θ (excluding embeddings). Regarding feature-based classification methods, results on Yelp and IMDB are taken from Tang et al. (2015a), on CZ movies from Brychcın and Habernal (2013), and Debates from Yogatama and Smith (2014). Wherever available we also provide the size of the recurrent unit (LSTM or GRU). 4.2 Document Classification In this section, we evaluate our document-level model on a variety of classification tasks. We selected four datasets which we describe below. Table 3 summarizes some statistics for each dataset. Yelp reviews were obtained from the 2013 Yelp Dataset Challenge. This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in Tan"
Q18-1005,J08-1001,1,\N,Missing
Q19-1022,E17-2026,0,0.104755,"earned from training data (dependency annotations) without ever utilizing an external parser. Our model falls under the general paradigm of multi-task learning (Caruana, 1993) which aims to improve a main task by jointly learning one or more related auxiliary tasks. Multi-task learning has been successfully applied to various sequence-prediction tasks including chunking, tagging (Collobert et al., 2011b; Bjerva et al., 2016; Plank, 2016; Søgaard and Goldberg, 2016; Hashimoto et al., 2017), name error detection (Cheng et al., 2015), machine translation (Luong et al., 2016), supersense tagging (Bingel and Søgaard, 2017), entailment (Hashimoto et al., 2017), and semantic role labeling (Collobert et al., 2011b; Strubell et al., 2018). Experimental results on the CoNLL-2009 benchmark dataset show that our model is able to outperform the state of the art in English, and to improve SRL performance in other languages, including Chinese, German, and Spanish. 2 Model Description Most supervised semantic role labeling systems adopt an architecture consisting of the following steps: (a) predicate identification and disambiguation (e.g., crowds in Figure 1 is a predicate with sense crowd.02); (b) argument identificatio"
Q19-1022,C16-1333,0,0.0413944,"Missing"
Q19-1022,C10-3009,0,0.0441832,"Missing"
Q19-1022,W05-0620,0,0.813145,"Missing"
Q19-1022,D15-1112,0,0.102414,"Missing"
Q19-1022,S15-1033,0,0.0196652,"are given as a supervision signal to the dependency information extractor. 4 Related Work Our model resonates with the recent trend of developing neural network models for semantic role labeling. It also agrees with previous work in devising ways to better take advantage of syntactic information for the SRL task within a relatively simple modeling framework based on bi-directional LSTMs (Marcheggiani et al., 2017). Previous proposals for incorporating syntactic information include the use of low-rank tensor factorizations (Lei et al., 2015), convolu352 tional and time-domain neural networks (Foland and Martin, 2015), jointly embedded arguments and semantic roles in a shared vector space (FitzGerald et al., 2015), learning representations of shortest dependency paths between a predicate and its potential arguments (Roth and Lapata, 2016), encoding sentences with graph convolutional networks (Marcheggiani and Titov, 2017), constrained decoding (He et al., 2017), and argument pruning (He et al., 2018). In contrast to these approaches, we do not use an external dependency parser but rather incorporate syntactic information as part of the model’s learning objective. Aside from assigning semantic roles, our mo"
Q19-1022,D15-1085,0,0.040443,"Missing"
Q19-1022,D17-1206,0,0.110821,"arc linking it to the predicate. The two auxiliary tasks provide dependency information that is specific to the SRL task and is learned from training data (dependency annotations) without ever utilizing an external parser. Our model falls under the general paradigm of multi-task learning (Caruana, 1993) which aims to improve a main task by jointly learning one or more related auxiliary tasks. Multi-task learning has been successfully applied to various sequence-prediction tasks including chunking, tagging (Collobert et al., 2011b; Bjerva et al., 2016; Plank, 2016; Søgaard and Goldberg, 2016; Hashimoto et al., 2017), name error detection (Cheng et al., 2015), machine translation (Luong et al., 2016), supersense tagging (Bingel and Søgaard, 2017), entailment (Hashimoto et al., 2017), and semantic role labeling (Collobert et al., 2011b; Strubell et al., 2018). Experimental results on the CoNLL-2009 benchmark dataset show that our model is able to outperform the state of the art in English, and to improve SRL performance in other languages, including Chinese, German, and Spanish. 2 Model Description Most supervised semantic role labeling systems adopt an architecture consisting of the following steps: (a) p"
Q19-1022,P17-1044,0,0.382885,"guistics. Distributed under a CC-BY 4.0 license.  Figure 1: Example sentence from the CoNLL-2009 English dataset annotated with syntactic dependencies (bottom) and semantic roles (top). (LSTMs). He et al. (2018) emphasize the role of syntax in argument identification rather than role labeling. Specifically, they develop an argument pruning algorithm that operates over dependency structures and selects argument candidates subject to a parameter determining their distance from the predicate. The predicate and its arguments are then encoded with an LSTM similar to Marcheggiani and Titov (2017). He et al. (2017) incorporate syntax at decoding time, in the form of constraints on the output structure (e.g., consistency with a parse tree is enforced by rejecting or penalizing arguments that are not constituents), whereas Strubell et al. (2018) incorporate syntactic information in a multi-task neural network model that simultaneously performs part-of-speech tagging, dependency parsing, predicate detection, and SRL. In this paper we argue that syntactic information is important for semantic role labeling and syntactic parsing is not. Despite recent advances in dependency parsing (Dozat and Manning, 2016;"
Q19-1022,P18-1192,0,0.386761,"vided strong impetus to develop deep end-to-end models for SRL that forego the need for extensive feature engineering. Recently proposed models (Zhou and Xu, 2015; 343 Transactions of the Association for Computational Linguistics, vol. 7, pp. 343–356, 2019. Action Editor: Alessandro Moschitti. Submission batch: 10/2018; Revision batch: 1/2019; Published 6/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  Figure 1: Example sentence from the CoNLL-2009 English dataset annotated with syntactic dependencies (bottom) and semantic roles (top). (LSTMs). He et al. (2018) emphasize the role of syntax in argument identification rather than role labeling. Specifically, they develop an argument pruning algorithm that operates over dependency structures and selects argument candidates subject to a parameter determining their distance from the predicate. The predicate and its arguments are then encoded with an LSTM similar to Marcheggiani and Titov (2017). He et al. (2017) incorporate syntax at decoding time, in the form of constraints on the output structure (e.g., consistency with a parse tree is enforced by rejecting or penalizing arguments that are not constitu"
Q19-1022,N15-1142,0,0.0292757,"den states) dhidden (hidden layer representation) doutput (output label embeddings) dr (role representation)  dl (output lemma representation) K (BiLSTM depth) J (BiLSTM depth) batch size input layer dropout rate hidden layer dropout rate learning rate auxiliary tasks loss weight α 100 300 300 16 100 300 200 32 128 128 4 2 30 0.3 0.3 0.001 0.5 Table 1: Hyperparameter values. to Hajic et al. [2009] for details on individual languages and their annotations). For experiments on English, we used the embeddings of Dyer et al. (2015), which were learned using the structured skip n-gram approach of Ling et al. (2015). In a few experiments we also used English character embeddings following He et al. (2018). These were pre-trained with a CNN-BiLSTM model (Peters et al., 2018) on the 1 Billion Word Benchmark,2 which is publicly released as part of the AllenNLP toolkit.3 Embeddings4 for Chinese, Spanish, and German were pre-trained on Wikipedia using fastText (Bojanowski et al., 2017). The dropout mechanism was applied to the input layer and the top hidden layer of the BiLSTM encoders. We used the Adam optimizer (Kingma and Ba, 2014) to train our models. We performed hyperparameter tuning and model selection"
Q19-1022,P82-1020,0,0.77054,"Missing"
Q19-1022,C08-1050,0,0.171357,"Labeling without Parsing Rui Cai and Mirella Lapata Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB Rui.Cai@ed.ac.uk mlap@inf.ed.ac.uk He et al., 2017; Marcheggiani et al., 2017) largely rely on bi-directional recurrent neural networks (Hochreiter and Schmidhuber, 1997) and predict semantic roles from textual input. They achieve competitive results while being syntax agnostic, thereby challenging conventional wisdom that parse trees provide a better form of representation for assigning semantic role labels (Johansson and Nugues, 2008). There are, however, good reasons why syntax ought to help semantic role labeling. First and foremost, SRL systems are trained on datasets whose semantic role annotations have been produced on top of treebanked corpora, and as a result are closely tied to syntactic information. An example sentence with roles labeled in the style of PropBank (Palmer et al., 2005) is shown in Figure 1. Here, many arcs in the syntactic dependency graph are mirrored in the semantic dependency graph, suggesting that syntactic dependencies could provide useful information to the SRL task. Secondly, predicates are t"
Q19-1022,N18-2078,0,0.195669,"tperforms the state of the art in English, and consistently improves performance in other languages, including Chinese, German, and Spanish. 1 Introduction Semantic role labeling (SRL) aims to identify the arguments of semantic predicates in a sentence and label them with a set of predefined relations (e.g., ‘‘who’’ did ‘‘what’’ to ‘‘whom,’’ ‘‘when,’’ and ‘‘where’’). Semantic roles capture basic predicate-argument structure while abstracting over surface syntactic configurations and have been shown to benefit a wide spectrum of applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011) and summarization (Khan et al., 2015). The successful application of neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to develop deep end-to-end models for SRL that forego the need for extensive feature engineering. Recently proposed models (Zhou and Xu, 2015; 343 Transactions of the Association for Computational Linguistics, vol. 7, pp. 343–356, 2019. Action Editor: Alessandro Moschitti. Submission batch: 10/2018; Revision batch: 1/2019; Published 6/2019. c 2019 Association"
Q19-1022,K17-1041,0,0.636031,"(a) predicate identification and disambiguation (e.g., crowds in Figure 1 is a predicate with sense crowd.02); (b) argument identification (e.g., the arguments of the predicate crowds in Figure 1 are He and plate); and (c) argument classification (e.g., the semantic roles for He and plate are A0 and A1, respectively). In this paper we focus solely on identifying arguments and labeling them with semantic roles using an offthe-shelf disambiguation model for the first step (Bj¨orkelund et al., 2010; Roth and Lapata, 2016). Our semantic role labeler is built on top of the syntax-agnostic model of Marcheggiani et al. (2017), which achieves good performance on the CoNLL-2009 English dataset without making use of a parser. Figure 2 provides a schematic overview of our model, which has two main components, namely, a dependency information extractor, and a semantic role predictor. The aim of the dependency extractor is to learn syntactic information for each word which subsequently serves as input (combined with word representations) to the semantic role labeler. The dependency extractor consists of: • a word representation component (which boils down to a simple embedding look-up); • a K -layer bidirectional LSTM ("
Q19-1022,Q16-1023,0,0.201377,"incorporate syntax at decoding time, in the form of constraints on the output structure (e.g., consistency with a parse tree is enforced by rejecting or penalizing arguments that are not constituents), whereas Strubell et al. (2018) incorporate syntactic information in a multi-task neural network model that simultaneously performs part-of-speech tagging, dependency parsing, predicate detection, and SRL. In this paper we argue that syntactic information is important for semantic role labeling and syntactic parsing is not. Despite recent advances in dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016), the use of an external parser often leads to pipeline-style architectures where errors propagate to later processing stages, affecting model performance. To mitigate such errors, Marcheggiani and Titov (2017) calculate a scalar gate for each edge in the dependency tree. And perhaps unsurprisingly, the performance of their system decreases when more than one GCN layer is stacked, as the effect of noisy information is amplified. Our key insight is to focus on dependency labels, which provide important information for semantic roles without requiring access to a full-blown syntactic representat"
Q19-1022,D17-1159,0,0.0944724,"Missing"
Q19-1022,N10-1137,1,0.793892,"re trained on datasets whose semantic role annotations have been produced on top of treebanked corpora, and as a result are closely tied to syntactic information. An example sentence with roles labeled in the style of PropBank (Palmer et al., 2005) is shown in Figure 1. Here, many arcs in the syntactic dependency graph are mirrored in the semantic dependency graph, suggesting that syntactic dependencies could provide useful information to the SRL task. Secondly, predicates are typically associated with a standard linking, that is, a deterministic mapping from syntactic roles to semantic ones (Lang and Lapata, 2010; Surdeanu et al., 2008). For example, subject (SBJ) is commonly mapped onto A0, whereas A1 is often realized as object (OBJ). Even in cases where there is no canonical mapping, dependency labels are still closely related to certain semantic roles, like the syntactic function TMP and the semantic role AM-TMP. The question of how to effectively incorporate syntactic information into sequential neural network models has met with different answers in the literature. Marcheggiani and Titov (2017) make use of graph convolutional networks (GCNs; Duvenaud et al., 2015; Kearnes et al., 2016; Kipf and"
Q19-1022,J05-1004,0,0.196636,"mantic roles from textual input. They achieve competitive results while being syntax agnostic, thereby challenging conventional wisdom that parse trees provide a better form of representation for assigning semantic role labels (Johansson and Nugues, 2008). There are, however, good reasons why syntax ought to help semantic role labeling. First and foremost, SRL systems are trained on datasets whose semantic role annotations have been produced on top of treebanked corpora, and as a result are closely tied to syntactic information. An example sentence with roles labeled in the style of PropBank (Palmer et al., 2005) is shown in Figure 1. Here, many arcs in the syntactic dependency graph are mirrored in the semantic dependency graph, suggesting that syntactic dependencies could provide useful information to the SRL task. Secondly, predicates are typically associated with a standard linking, that is, a deterministic mapping from syntactic roles to semantic ones (Lang and Lapata, 2010; Surdeanu et al., 2008). For example, subject (SBJ) is commonly mapped onto A0, whereas A1 is often realized as object (OBJ). Even in cases where there is no canonical mapping, dependency labels are still closely related to ce"
Q19-1022,J93-2004,0,0.0676855,"e label classifier. Analogously, we represent dependency link information elink as:  elink = softmax(M LPLN K )[l] ∗ el l∈{N,C,P } (11) 3 Experiments We implemented our model in PyTorch1 and evaluated it on the English, Chinese, German, and Spanish CoNLL-2009 benchmark datasets following the standard training, testing, and development set splits. The datasets contain goldstandard dependency annotations, and also gold lemmas, part-of-speech tags, and morphological features. Data for the different languages was generated by merging various language specific treebanks such as the Penn Treebank (Parcus et al., 1993) and Brown corpus (Francis and Kucera, 1979) for English, the Prague Dependency Treebank for Czech (Hajiˇcov´a et al., 1999), the Chinese Treebank (Xue et al., 2005), and Proposition Bank (Xue and Palmer, 2009) for Chinese, and so on (we refer the interested reader Hyperparameter value dw (English word embeddings) dw (other languages word embeddings) dc (character embeddings) dpos (POS embeddings) dl (lemma embeddings) dh (LSTM hidden states) dhidden (hidden layer representation) doutput (output label embeddings) dr (role representation)  dl (output lemma representation) K (BiLSTM depth) J (B"
Q19-1022,D18-1548,0,0.135552,"Missing"
Q19-1022,W08-2121,0,0.404142,"Missing"
Q19-1022,P17-1186,0,0.0262283,"nition, and SRL. Søgaard and Goldberg (2016) train a multi-task model for POS-tagging, syntactic chunking, and combinatory categorical grammar supertagging, while Hashimoto et al. (2017) introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Zhang and Weiss (2016) propose stack-propagation using a continuous and differentiable link between POS tagging and dependency parsing, in which POS tags are utilized as a regularizer of learned representations for parsing. MTL has also been applied to semantic dependency parsing (Peng et al., 2017; Swayamdipta et al., 2017) and semantic role labeling. Strubell et al. (2018) present an end-to-end SRL model that is trained to jointly predict parts of speech and predicates, perform parsing, and attend to syntactic parse parents, while assigning semantic role labels. Most recent MTL models (Bingel and Søgaard, 2017; Hashimoto et al., 2017) use different layers for multiple tasks with different datasets, separately optimizing each task at each epoch. In our case, the SRL task and the two auxiliary tasks share the same input, and as a result optimization for all three tasks takes place simul"
Q19-1022,N18-1202,0,0.613078,"atenation operator. The parameters of the pre-trained word embeddings xpe are shared with the word embeddings  xpe used for our dependency information extractor, and are updated during training. In order to obtain more syntax-aware representations, we utilize hidden-layer representations vhidden and dependency embeddings (elabel and elink ). The final representation R, which serves as input to the SRL, is the concatenation of three syntactically informed representations: R = x ◦ vhidden ◦ elabel ◦ elink Hidden-layer Representations In order to compute vhidden , we draw inspiration from ELMo (Peters et al., 2018), a recently proposed model for generating word representations based on bidirectional LSTMs trained with a coupled language model objective. Unlike more traditional word embeddings (Mikolov et al., 2013), ELMo representations are deep, essentially a linear combination of the representations learned at all layers of the LSTM instead of just the final layer. We also utilize the combination of the intermediate layer representations in the dependency information extractor. Given sentence (w1 , . . . , wN ), a BiLSTM encoder with L layers computes for each word a set of 2L representations: ← − S ="
Q19-1022,C16-1059,0,0.0345454,"Missing"
Q19-1022,W13-3516,0,0.0886473,"Missing"
Q19-1022,P16-1147,0,0.0165091,"diction), thereby learning syntactic information specific to the SRL task. Multi-task learning (MTL; Caruana, 1993) has been a popular approach for various NLP tasks, starting with Collobert et al. (2011a) who propose a multi-task model for POS-tagging, chunking, named entity recognition, and SRL. Søgaard and Goldberg (2016) train a multi-task model for POS-tagging, syntactic chunking, and combinatory categorical grammar supertagging, while Hashimoto et al. (2017) introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Zhang and Weiss (2016) propose stack-propagation using a continuous and differentiable link between POS tagging and dependency parsing, in which POS tags are utilized as a regularizer of learned representations for parsing. MTL has also been applied to semantic dependency parsing (Peng et al., 2017; Swayamdipta et al., 2017) and semantic role labeling. Strubell et al. (2018) present an end-to-end SRL model that is trained to jointly predict parts of speech and predicates, perform parsing, and attend to syntactic parse parents, while assigning semantic role labels. Most recent MTL models (Bingel and Søgaard, 2017; H"
Q19-1022,P16-1113,1,0.95718,"upervised semantic role labeling systems adopt an architecture consisting of the following steps: (a) predicate identification and disambiguation (e.g., crowds in Figure 1 is a predicate with sense crowd.02); (b) argument identification (e.g., the arguments of the predicate crowds in Figure 1 are He and plate); and (c) argument classification (e.g., the semantic roles for He and plate are A0 and A1, respectively). In this paper we focus solely on identifying arguments and labeling them with semantic roles using an offthe-shelf disambiguation model for the first step (Bj¨orkelund et al., 2010; Roth and Lapata, 2016). Our semantic role labeler is built on top of the syntax-agnostic model of Marcheggiani et al. (2017), which achieves good performance on the CoNLL-2009 English dataset without making use of a parser. Figure 2 provides a schematic overview of our model, which has two main components, namely, a dependency information extractor, and a semantic role predictor. The aim of the dependency extractor is to learn syntactic information for each word which subsequently serves as input (combined with word representations) to the semantic role labeler. The dependency extractor consists of: • a word repres"
Q19-1022,P15-1109,0,0.0887682,"ure basic predicate-argument structure while abstracting over surface syntactic configurations and have been shown to benefit a wide spectrum of applications ranging from machine translation (Aziz et al., 2011; Marcheggiani et al., 2018) to information extraction (Christensen et al., 2011) and summarization (Khan et al., 2015). The successful application of neural networks to a variety of NLP tasks (Bahdanau et al., 2015; Vinyals et al., 2015) has provided strong impetus to develop deep end-to-end models for SRL that forego the need for extensive feature engineering. Recently proposed models (Zhou and Xu, 2015; 343 Transactions of the Association for Computational Linguistics, vol. 7, pp. 343–356, 2019. Action Editor: Alessandro Moschitti. Submission batch: 10/2018; Revision batch: 1/2019; Published 6/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.  Figure 1: Example sentence from the CoNLL-2009 English dataset annotated with syntactic dependencies (bottom) and semantic roles (top). (LSTMs). He et al. (2018) emphasize the role of syntax in argument identification rather than role labeling. Specifically, they develop an argument pruning algorithm that"
Q19-1022,P16-2038,0,0.0384625,"ent pruning (He et al., 2018). In contrast to these approaches, we do not use an external dependency parser but rather incorporate syntactic information as part of the model’s learning objective. Aside from assigning semantic roles, our model performs two auxiliary tasks (dependency and link type prediction), thereby learning syntactic information specific to the SRL task. Multi-task learning (MTL; Caruana, 1993) has been a popular approach for various NLP tasks, starting with Collobert et al. (2011a) who propose a multi-task model for POS-tagging, chunking, named entity recognition, and SRL. Søgaard and Goldberg (2016) train a multi-task model for POS-tagging, syntactic chunking, and combinatory categorical grammar supertagging, while Hashimoto et al. (2017) introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Zhang and Weiss (2016) propose stack-propagation using a continuous and differentiable link between POS tagging and dependency parsing, in which POS tags are utilized as a regularizer of learned representations for parsing. MTL has also been applied to semantic dependency parsing (Peng et al., 2017; Swayamdipta et al., 2017)"
Q19-1022,Q17-1010,0,\N,Missing
Q19-1037,N18-1111,0,0.112377,"et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Huang, 2015; Chen and Cardie, 2018) assumes that labeled data may exist for multiple domains, but in insufficient amounts to train classifiers for one or more of them. The aim of multi-domain text classification is to leverage all the available resources in order to improve system performance across domains simultaneously. In this paper we investigate the question of how domain-specific data might be obtained in order to enable the development of text classification tools as well as more domain aware applications such as summarization, question answering, and In this paper we introduce domain detection as a new natural language"
Q19-1037,Q18-1002,1,0.932749,"et al., 2003; Zhou et al., 2009). Within NLP, multiple instance learning has been predominantly applied to sentiment analysis. Kotzias et al. (2015) use sentence vectors obtained by a pre-trained hierarchical convolutional neural network (Denil et al., 2014) as features under a MIL objective that simply averages instance contributions towards bag classification (i.e., positive/ negative document sentiment). Pappas and PopescuBelis (2014) adopt a multiple instance regression model to assign sentiment scores to specific product aspects, using a weighted summation of predictions. More recently, Angelidis and Lapata (2018) propose MILNET, a multiple instance learning network model for sentiment analysis. They use an attention mechanism to flexibly weigh predictions and recognize sentiment-heavy text snippets (i.e., sentences or clauses). We depart from previous MIL-based work in devising an encoding module with self-attention and non-recurrent structure, which is particularly suitable for modeling long documents efficiently. Compared with MILNET (Angelidis and Lapata, 2018), our approach generalizes to segments of arbitrary granularity; it introduces an instance scoring function that supports multilabel rather"
Q19-1037,E17-1104,0,0.0199957,"ion and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yumo.xu@ed.ac.uk, mlap@inf.ed.ac.uk Abstract pointing device or positive sentiment when describing a laboratory experiment performed on a rodent. The ability to handle a wide variety of domains1 has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to Twitter, blog posts, medical journals, Reddit comments, and parliamentary debates (Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Huang, 2015; Chen and Car"
Q19-1037,P07-1033,0,0.082177,"ion, representation learning, multiple instance learning, and topic modeling. We review related work below. Domain adaptation A variety of domain adaptation methods (Jiang and Zhai, 2007; Arnold et al., 2007; Pan et al., 2010) have been proposed to deal with the lack of annotated data in novel domains faced by supervised models. Daume and Marcu (2006) propose to learn three separate models, one specific to the source domain, one specific to the target domain, and a third one representing domain general information. A simple yet effective feature augmentation technique is further introduced in Daume (2007) which Finkel and Manning (2009) subsequently recast within a hierarchical Bayesian framework. More recently, Lu et al. (2016) present a general regularization framework for domain adaptation while Camacho-Collados and Navigli (2017) integrate domain information within lexical resources. A popular approach within text classification learns features that are invariant across multiple domains while explicitly modeling the individual characteristics of each domain (Chen and Cardie, 2018; Wu and Huang, 2015; Bousmalis et al., 2016). Similar to domain adaptation, our detection task also identifies"
Q19-1037,W06-1615,0,0.178642,"ability to handle a wide variety of domains1 has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to Twitter, blog posts, medical journals, Reddit comments, and parliamentary debates (Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Huang, 2015; Chen and Cardie, 2018) assumes that labeled data may exist for multiple domains, but in insufficient amounts to train classifiers for one or more of them. The aim of multi-domain text classification is to leverage all the available resources in order to improve syst"
Q19-1037,P08-2065,0,0.0430052,"(Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Huang, 2015; Chen and Cardie, 2018) assumes that labeled data may exist for multiple domains, but in insufficient amounts to train classifiers for one or more of them. The aim of multi-domain text classification is to leverage all the available resources in order to improve system performance across domains simultaneously. In this paper we investigate the question of how domain-specific data might be obtained in order to enable the development of text classification tools as well as more domain aware applications such as summarization, question answering, and In this paper we introduc"
Q19-1037,D16-1095,0,0.0348368,"Missing"
Q19-1037,P15-1162,0,0.0599636,"Missing"
Q19-1037,P07-1034,0,0.460694,"ormed on a rodent. The ability to handle a wide variety of domains1 has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to Twitter, blog posts, medical journals, Reddit comments, and parliamentary debates (Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Huang, 2015; Chen and Cardie, 2018) assumes that labeled data may exist for multiple domains, but in insufficient amounts to train classifiers for one or more of them. The aim of multi-domain text classification is to leverage all the available resources in"
Q19-1037,D14-1181,0,0.0153091,"Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yumo.xu@ed.ac.uk, mlap@inf.ed.ac.uk Abstract pointing device or positive sentiment when describing a laboratory experiment performed on a rodent. The ability to handle a wide variety of domains1 has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to Twitter, blog posts, medical journals, Reddit comments, and parliamentary debates (Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zon"
Q19-1037,W04-3252,0,0.013118,"ually associated with multiple domains, we determine a word’s ranking for a given domain based on the highest score. As shown in Table 5, 9 Domain-Specific Summarization In this section we illustrate how fine-grained domain scores can be used to produce domain 591 Method TEXTRANK DETRANK summaries, following an extractive, unsupervised approach. We assume the user specifies the domains they are interested in a priori (e.g., LAW, HEA) and the system returns summaries targeting the semantics of these domains. Specifically, we introduce DETRANK, an extension of the well-known TEXTRANK algorithm (Mihalcea and Tarau, 2004), which incorporates domain signals acquired by DETNET∗ . For each document, TEXTRANK builds a directed graph G = (V, E ) with nodes V corresponding to sentences, and undirected edges E whose weights are computed based on sentence similarity. Specifically, edge weights are represented with matrix E where each element E i,j corresponds to the transition probability from vertex i to vertex j . Following Barrios et al. (2016), E i,j is computed with the Okapi BM25 algorithm (Robertson et al., 1995), a probabilistic version of TF-IDF, and small weights (&lt; 0.001) are set to zeros. Unreachable nodes"
Q19-1037,D14-1052,0,0.0674676,"Missing"
Q19-1037,D09-1026,0,0.780633,"ated probabilistically using a mixture over K topics that are in turn characterized by a distribution over words. And words in a document are generated by repeatedly sampling a topic according to the topic distribution and selecting a word given the chosen topic. Although most topic models are unsupervised, some variants can also accommodate documentlevel supervision (Mcauliffe and Blei, 2008; Lacoste-Julien et al., 2009). However, these models are not appropriate for analyzing multiply labeled corpora because they limit documents to being associated with a single label. Multimultinomial LDA (Ramage et al. 2009b) relaxes this constraint by modeling each document as a bag of words with a bag of labels, and topics for each observation are drawn from a shared topic distribution. Labeled LDA (L-LDA; Ramage et al., 2009a) goes one step further by directly associating labels with latent topics thereby learning label-word correspondences. L-LDA is a natural extension of both LDA by incorporating supervision and multinomial naive Bayes (McCallum and Nigam, 1998) by incorporating a mixture model (Ramage et al., 2009a). Similar to L-LDA, DETNET is also designed to perform learning and inference in multi-label"
Q19-1037,N16-1174,0,0.0428505,"or Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yumo.xu@ed.ac.uk, mlap@inf.ed.ac.uk Abstract pointing device or positive sentiment when describing a laboratory experiment performed on a rodent. The ability to handle a wide variety of domains1 has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to Twitter, blog posts, medical journals, Reddit comments, and parliamentary debates (Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Hua"
Q19-1037,D12-1042,0,0.014066,"t summaries. This, on the other hand, possibly renders DETRANK’s summaries more verbose (see the Succinctness ratings in Table 6). Figure 4 shows example summaries for the Wikipedia article Arms Industry for domains MIL and BUS.11 Both summaries begin with a sentence that introduces the arms industry to the reader. When MIL is the domain of interest, the summary focuses on military products such as guns and missiles. When the domain changes to BUS, the summary puts more emphasis on trade—for example, market competition and companies doing military business, such as Boeing and Eurofighter. 10 (Surdeanu et al., 2012), and named entity recognition (Tang et al., 2017). More generally, our experiments show that the proposed framework can be applied to textual data using minimal supervision, significantly alleviating the annotation bottleneck for text classification problems. A key feature in achieving performance superior to competitive baselines is the hierarchical nature of our model, where representations are encoded step-by-step, first for words, then for sentences, and finally for documents. The framework flexibly integrates prior information which can be used to enhance the otherwise weak supervision s"
Q19-1037,D17-1280,0,0.0228935,"DETRANK’s summaries more verbose (see the Succinctness ratings in Table 6). Figure 4 shows example summaries for the Wikipedia article Arms Industry for domains MIL and BUS.11 Both summaries begin with a sentence that introduces the arms industry to the reader. When MIL is the domain of interest, the summary focuses on military products such as guns and missiles. When the domain changes to BUS, the summary puts more emphasis on trade—for example, market competition and companies doing military business, such as Boeing and Eurofighter. 10 (Surdeanu et al., 2012), and named entity recognition (Tang et al., 2017). More generally, our experiments show that the proposed framework can be applied to textual data using minimal supervision, significantly alleviating the annotation bottleneck for text classification problems. A key feature in achieving performance superior to competitive baselines is the hierarchical nature of our model, where representations are encoded step-by-step, first for words, then for sentences, and finally for documents. The framework flexibly integrates prior information which can be used to enhance the otherwise weak supervision signal or to render the model more robust across ge"
Q19-1037,D16-1076,0,0.0198401,"hool of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB yumo.xu@ed.ac.uk, mlap@inf.ed.ac.uk Abstract pointing device or positive sentiment when describing a laboratory experiment performed on a rodent. The ability to handle a wide variety of domains1 has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to Twitter, blog posts, medical journals, Reddit comments, and parliamentary debates (Kim, 2014; Yang et al., 2016; Conneau et al., 2017; Zhang et al., 2016). The question of how to best deal with multiple domains when training data are available for one or few of them has met with much interest in the literature. The field of domain adaptation (Jiang and Zhai, 2007; Blitzer et al., 2006; Daume III, 2007; Finkel and Manning, 2009; Lu et al., 2016) aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data are available. Another line of work (Li and Zong, 2008; Wu and Huang, 2015; Chen and Cardie, 2018) assumes th"
Q19-1037,E17-2036,0,\N,Missing
Q19-1037,N09-1068,0,\N,Missing
W02-1706,J93-1002,0,0.029425,"Missing"
W02-1706,A97-1052,0,0.0294218,"Missing"
W02-1706,W01-1808,0,0.0444384,"Missing"
W02-1706,A94-1009,0,0.0258217,"Missing"
W02-1706,grover-etal-2000-lt,1,0.850189,"raven and Kumlien (1999) for discussion of methods for IE from MEDLINE. Our processing paradigm is XML-based. As a mark-up language for NLP tasks, XML is expressive and flexible yet constrainable. Furthermore, there exist a wide range of XML-based tools for NLP applications which lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. In processing MEDLINE abstracts we have built a number of such pipelines using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). We have also successfully integrated non-XML publicdomain tools into our pipelines and incorporated their output into the XML mark-up using the LT XML program xmlperl (McKelvie, 2000). In Section 2 we describe our use of XML-based tokenisation tools and techniques and in Sections 3 and 4 we describe two different approaches to analysing MEDLINE data which are built on top of the tokenisation. The first approach uses a handcoded grammar to give complete syntactic and semantic analyses of sentences. The second approach performs a shallower statistically-based analysis w"
W02-1706,P01-1034,1,0.692272,"Missing"
W02-1706,H94-1020,0,0.145023,"Missing"
W02-1706,J97-3003,0,0.0213577,"ields of an OHSUMED entry and convert them into XML mark-up: each abstract is put inside a RECORD element which contains sub-structure reflecting e.g. author, title, MESH code and the abstract itself. From this point on, all processing is directed at the ABSTRACT elements through the query “.*/ABSTRACT”1 . Steps 3 and 4 make calls to fsgmatch to identify S and W (word) elements as described above and after this point, in step 5, the S mark-up is discarded (using the LT TTT program sgdelmarkup) since it has now served its purpose. Step 6 contains a call to the other main LT TTT program, ltpos (Mikheev, 1997), which performs both sentence identification and POS tagging. The subquery (-qs) option picks out ABSTRACTs as the elements within RECORDs (-q option) that are to be processed; the -qw option indicates that the input has already been segmented into words marked 1 The query language that the LT TTT and LT XML tools use is a specialised XML query language which pinpoints the part of the XML tree-structure that is to be processed at that point. This query language pre-dates XPath and in expressiveness it constitutes a subset of XPath except that it also allows regular expressions over text conte"
W02-1706,W00-1427,0,\N,Missing
W02-1706,C02-1013,0,\N,Missing
W04-3210,P01-1017,0,0.0184608,"Missing"
W04-3210,H92-1019,0,0.227927,"Missing"
W04-3210,W03-1009,0,0.358882,"liable cue for paragraph identification.5 3.2.1 Non-syntactic Features Distance (Ds , Dw ): These features encode the distance of the current sentence from the previous paragraph break. We measured distance in terms of the number of intervening sentences (D s ) as well as in terms of the number of intervening words (D w ). If paragraph breaks were driven purely by aesthetics one would expect this feature to be among the most successful ones.6 Sentence Length (Length): This feature encodes the number of words in the current sentence. Average sentence length is known to vary with text position (Genzel and Charniak, 2003) and it is possible that it also varies with paragraph position. Relative Position (Pos): The relative position of a sentence in the text is calculated by dividing the current sentence number by the number of sentences in the text. The motivation for this feature is that paragraph length may vary with text position. For example, it is possible that paragraphs at the beginning and end of a text are shorter than paragraphs in the middle and hence a paragraph break is more likely at the two former text positions. Quotes (Quote p , Quotec , Quotei ): These features encode whether the previous or c"
W04-3210,J97-1003,0,0.353531,"Missing"
W04-3210,J02-1002,0,0.072683,"Missing"
W04-3210,A97-1004,0,0.100203,"Missing"
W04-3210,A00-1012,0,0.0213962,"y hard to read, which can cause processing difficulties, especially if speech recognition is used to provide deaf students with real-time transcripts of lectures. Furthermore, sometimes the output of a speech recogniser needs to be processed automatically by applications such as information extraction or summarisation. Most of these applications (e.g., Christensen et al., (2004)) port techniques developed for written texts to spoken texts and therefore require input that is punctuated and broken into paragraphs. While there has been some research on finding sentence boundaries in spoken text (Stevenson and Gaizauskas, 2000), there has been little research on determining paragraph boundaries.1 If paragraph boundaries were mainly an aesthetic device for visually breaking up long texts into smaller chunks, as has previously been suggested (see Longacre (1979)), paragraph boundaries could be easily inserted by splitting a text into several equal-size segments. Psycho-linguistic research, however, indicates that paragraph boundaries are not purely aesthetic. For example, Stark (1988) 1 There has been research on using phonetic cues to segment speech into “acoustic paragraphs” (Hauptmann and Smith, 1995). However, the"
W04-3210,P01-1064,0,0.0360184,"anguages and across domains. We also assess human performance on the same task and whether it differs across domains. 2 Related Work Previous work has focused extensively on the task of automatic text segmentation whose primary goal is to divide individual texts into sub-topics. Despite their differences, most methods are unsupervised and typically rely on the distribution of words in a given text to provide cues for topic segmentation.2 Hearst’s (1997) TextTiling algorithm, for example, determines sub-topic boundaries on the basis of term overlap in adjacent text blocks. In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. Beeferman et al. (1999) use supervised learning methods to infer boundaries between texts. They employ language models to detect topic shifts and combine them with cue word features. 2 Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. Our work differs from these previous approaches in that paragraphs do not always corre"
W13-3809,D11-1135,0,\N,Missing
W13-3809,W04-2407,0,\N,Missing
W13-3809,D10-1106,0,\N,Missing
W13-3809,S07-1002,0,\N,Missing
W13-3809,P09-1113,0,\N,Missing
W13-3809,P04-1054,0,\N,Missing
W13-3809,P04-1053,0,\N,Missing
W13-3809,P06-1015,0,\N,Missing
W13-3809,D09-1001,0,\N,Missing
W13-3809,P07-1073,0,\N,Missing
W13-3809,N06-1039,0,\N,Missing
W13-3809,P05-1045,0,\N,Missing
W13-3809,D07-1076,0,\N,Missing
W19-1203,E17-2039,0,0.16373,"Missing"
W19-1203,W19-1201,0,0.137366,"Missing"
W19-1203,P17-4012,0,0.0503272,"sformer transformer True dot 5 Table 1: Choice of hyperparameters for our neural network models. remain them case-sensitive. Following previous work (van Noord et al., 2018), the indices of variables in clauses are relative, as shown in Figure 3(b), which is the same to the character-level preprocessing. 2.2 Neural Models We adopt Recurrent Neural Networks (RNNs) equipped with Long Shot-Term Memory (LSTM; Hochreiter and Schmidhuber 1997) units and the Transformer model (Vaswani et al., 2017) as our neural models. For the model implementation, we use the one provided by the OpenNMT-py toolkit (Klein et al., 2017). The hyperparameters we used are shown in Table 1 which are institutionally set without optimization. Fine-tuning We propose a fine-tuning approach to enable the system to effectively use more training data in various quality, i.e. bronze and silver data. The fine-tuning approach allows the system train to convergence on one dataset (e.g. silver and gold data) and then continues to train to convergence on another dataset (e.g. gold data), where the optimizers are reset. LSTM sg-data sg-data + g-data Transformer sg-data sg-data + g-data P 73.91 86.05 P 69.11 82.32 character R F1 75.00 74.45 84"
W19-1203,P18-1040,1,0.439748,"al. 2017) provides a large collection of English texts annotated with Discourse Representation Structures (DRS), while the Parallel Meaning Bank (PMB; Abzianidze et al. 2017) provides DRSs in English, German, Italian and Dutch. Furthermore, the PMB introduces clause representation, as shown on the top of Figure 1. With the recent introduction of neural network learning to the Natural Language Processing community, several neural DRS parsers have been developed for the problem of DRS parsing, i.e. the problem of taking a document or a sentence as input, and outputting their corresponding DRS. Liu et al. (2018) convert box-style DRSs to tree-style DRSs and propose the three-step tree DRS parser on the GMB, while van Noord et al. (2018) adopt a neural machine translation approach to parse sentences to their clause-style DRSs on PMB. Due to the different standard of annotations between GMB and PMB, and that the IWCS-2019 Shared Task of DRS Parsing mainly focuses on averagely short sentences in PMB annotations, our systems take sentences as input and output a clause-style DRS of PMB represented as a sequence for the IWCS-2018 Shared Task of DRS parsing (Abzianidze et al., 2019). 2 The Parsing System Fi"
W19-1203,D14-1162,0,0.089445,"(2018) to transform back the output of our models to the clause format, and then use COUNTER (van Noord et al., 2018) as our evaluation metric. 3 Experiments In this section, we introduce the training data that we used and the results on the PMB benchmarks. 3.1 Data The training data consists of all of the bronze data (bronze), all of the silver data (silver), and the training section of the gold data (gold). All data is preprocessed. We mix bronze, silver and gold as bsg-data, and mix silver and gold as sg-data, and name the training section of gold data as g-data. Meanwhile, we adopt GloVe (Pennington et al., 2014) pre-trained word embeddings5 to initialize the representation of input tokens. 3.2 Results Table 2 shows the results on test data, where sg-data means that the models are only trained on sg-data, and + g-data means that the models are continually fine-tuned on g-data. With LSTM, the character model performs marginally better than the word model. However, with Transformer, the word model performs significantly better than the character model. With both LSTM and Transformer, fine-tuning on g-data significantly improves the performance. Although the character LSTM is marginally better than the w"
W19-1203,L18-1267,0,0.236563,"Missing"
W19-1203,Q18-1043,0,0.245075,"Missing"
W97-0410,C96-1075,1,0.852035,"ented. Introduction Spoken language understanding systems have been reasonably successful in limited semantic domains I. The limited domains naturally constrain vocabulary and perplexity, making speech recognition tractable. In addition, the relatively small range of meanings that could be conveyed make parsing and understanding tractable. Now, with the increasing success of large vocabulary continuous speech recognition (LVCSR), the challenge is to similarly scale up spoken language understanding. In this paper we describe our plans for extending the JANUS speech-to-speech translation system [1] [2] from the Appointment Scheduling domain to a broader domain, Travel Planning, which has a rich sub-domain structure, covering many topics. In the last three years, the JANUS project has been developing a speech-to-speech translation system for the Appointment Scheduling domain (two people setting up a time to meet with each other). Although the data we have been working with is spontaneous speech, the scheduling scenario naturally limits the vocabulary to about 3000 words in English and about 4000 words in Spanish and German, which have more inflection. Similarly, the types of dialogues ar"
W97-0410,C96-1061,0,0.0193217,"guage input string is first analyzed by a parser, which produces a languageindependent interlingua content representation. The interlingua is then passed to a generation component, which produces an output string in the target language. In an attempt to achieve both robustness and translation accuracy when faced with speech disfluencies and recognition errors, we use two different parsing strategies: a GLFt parser designed to be more accurate, and a Phoenix parser designed to be more robust. Detailed descriptions of the system components appear in our previous publications [1] [2] [3] [4] [5] [6]. 68 s c h e d u l e - m e e t i n g in addition to syntactic categories such as NP and VP. There were several reasons for chosing semantic grammars. First, the domain lends itself well to semantic g r a m m a r s because there are many fixed expressions and c o m m o n expressions that are almost formulaic. Breaking these down syntactically would be an unnecessary complication. Additionally, spontaneous spoken language is often syntactically ill formed, yet semantically coherent. Semantic g r a m m a r s allow our robust parsers to extract the key concepts being conveyed, even when the input"
