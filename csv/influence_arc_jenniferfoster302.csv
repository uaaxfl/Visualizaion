2012.amta-papers.27,W10-1408,1,0.906003,"Missing"
2012.amta-papers.27,W09-0437,0,0.0184242,"syntax-based modelling. The string-to-tree modelling in this work is based on their approach. Zollmann et al. (2008) observed that the gain achieved by hierarchical and syntax-based models could be largely compensated for by increasing the reordering limit in the phrase-based model. They also found that, for language pairs involving substantial reordering like Chinese-English, tree-based models performed better than phrase-based. However, for relatively monotonic pairs like ArabicEnglish, all models produced similar results. Experimenting with French-English, GermanEnglish and English-German, Auli et al. (2009) compared a phrase-based model to a hierarchical phrase-based model by exploring as much of the search space of both types of models as was computationally feasible. Given that the search spaces were very similar, they concluded that the differences between the two types of models can be explained by the way they score hypotheses rather than by the hypotheses they produce. Using the same framework as in this work, Hoang et al. (2009) compared phrase-based, hierarchical phrase-based and string-to-tree models. While the phrase-based and hierarchical phrase-based models achieved similar results,"
2012.amta-papers.27,P07-1051,0,0.0215658,"e it to be. While relative improvements over phrase-based baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture long-distance reordering — a problem for phrase-based models. A combined framework of such varying techniques can exploit the advantages of all of them while compensating for the weaknesses of each individual method. To accomplish this goal, a more detailed insight into the characteristics of each method may be useful. Towards this objective, we look for possible systematic differences between variants"
2012.amta-papers.27,J07-2003,0,0.659981,"systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and conclude. 2 Related Work DeNeefe et al. (2007) compared a string-to-tree model with a phrase-based model. While the syntaxbased model performed better than the phrase-based mo"
2012.amta-papers.27,D07-1079,0,0.278714,"rsity Dublin 9, Ireland Jennifer Foster† ‡ Symantec Research Labs Ballycoolin Business Park Blanchardstown, Dublin 15, Ireland firstname.lastname@dcu.ie firstname lastname@symantec.com Abstract trees on the source side (tree-to-string), target side (string-to-tree), or both (tree-to-tree). Utilisation of such linguistic generalisation, however, has proven to be a more complicated task than one might first imagine it to be. While relative improvements over phrase-based baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture"
2012.amta-papers.27,W11-2107,0,0.028467,"M 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-domain translation is challenging and that English-German translation is more difficult than English-Fr"
2012.amta-papers.27,W07-0732,0,0.0339778,"to investigate such behaviours, we select 100 sentences from each development set, and compare the outputs of two of the systems, namely HP and ST, for each of these sentences. 50 of the selected sentences are the solo-win cases of HP and the other 50 are those of ST (see section 5.3). The reason why these two systems are selected is that HP is the overall best performing system, and ST is the best performing syntax-based systems according to the various comparisons so far. Each data set was evaluated by a linguist using eight error categories. These categories are adapted from those used by Dugast et al. (2007) to evaluate post-editing changes. The evaluators were asked to count the number of errors in each output sentence under each category. While they were given the reference translation, they were not constrained to it and were allowed to compare against the closest correct translation to the output itself. We believe that this can better reflect the real performance of the systems, as it is not limited to a single reference, though we might lose some correlation with automatic metScore ties Real ties PB Any/Solo wins HP Any/Solo wins TS Any/Solo wins ST Any/Solo wins TT Any/Solo wins In-domain"
2012.amta-papers.27,I11-1100,1,0.84065,"Missing"
2012.amta-papers.27,W02-1039,0,0.0401634,"baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture long-distance reordering — a problem for phrase-based models. A combined framework of such varying techniques can exploit the advantages of all of them while compensating for the weaknesses of each individual method. To accomplish this goal, a more detailed insight into the characteristics of each method may be useful. Towards this objective, we look for possible systematic differences between variants of phrase-based and syntax-based systems via various ana"
2012.amta-papers.27,N04-1035,0,0.0395526,"ng syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and conclude. 2 Related Work DeNeefe et al. (2007) compared a string-to-tree model with a phrase-based model. While the syntaxbased model performed better than the phrase-based model on Chinese-to-English translation, it was shown to be worse on Arabic-to-English translation. They found that non-lexical rules form only a small fraction of the translation rule table in syntax-based modelling. The string-to-tree modelling in this w"
2012.amta-papers.27,2009.iwslt-papers.4,0,0.104625,"wever, for relatively monotonic pairs like ArabicEnglish, all models produced similar results. Experimenting with French-English, GermanEnglish and English-German, Auli et al. (2009) compared a phrase-based model to a hierarchical phrase-based model by exploring as much of the search space of both types of models as was computationally feasible. Given that the search spaces were very similar, they concluded that the differences between the two types of models can be explained by the way they score hypotheses rather than by the hypotheses they produce. Using the same framework as in this work, Hoang et al. (2009) compared phrase-based, hierarchical phrase-based and string-to-tree models. While the phrase-based and hierarchical phrase-based models achieved similar results, they both performed slightly better than the syntax-based model. They argue that, in order to improve syntax-based modelling, word alignment should be amended. There have been several efforts to exploit the difference between such models in MT system combination or multi-engine machine translation (MEMT) (Huang and Papineni, 2007). The task, however, has been shown to be difficult. Zwarts and Dras (2008) tried to identify what type o"
2012.amta-papers.27,D07-1029,0,0.020084,"ay they score hypotheses rather than by the hypotheses they produce. Using the same framework as in this work, Hoang et al. (2009) compared phrase-based, hierarchical phrase-based and string-to-tree models. While the phrase-based and hierarchical phrase-based models achieved similar results, they both performed slightly better than the syntax-based model. They argue that, in order to improve syntax-based modelling, word alignment should be amended. There have been several efforts to exploit the difference between such models in MT system combination or multi-engine machine translation (MEMT) (Huang and Papineni, 2007). The task, however, has been shown to be difficult. Zwarts and Dras (2008) tried to identify what type of sentence could be better translated by a syntax-based model compared to a phrase-based model. Using a classification approach, they separately tested three sets of features. Sentence length and system-internal features including decoder output score did not lead to an accurate classifier. They then hypothesised that noisy parse trees may impede the performance of the syntaxbased system and built another classifier based on source sentence length, parser confidence score, and linked fragme"
2012.amta-papers.27,2006.amta-papers.8,0,0.0223031,"y translated into French using an online translation tool and then post-edited by human translators. 4. German forum data: 1,500 sentences taken from the Symantec English online forums, split into development (600) and test (900). These were automatically translated into German using an online translation tool and then postedited by human translators. Baseline Systems We train the following five statistical machine translation systems: 1. PB: a standard phrase-based system (Och and Ney, 2004) 2. HP: a hierarchical phrase-based system (Chiang, 2007) 3. TS: a tree-to-string syntax-based system (Huang et al., 2006). System Comparison 5.1 Multiple Metrics In order to carry out a reliable comparison, we evaluate the baseline systems at the document level using 1 We used the SAMT-2 parse relaxation method. The two parsers achieve Parseval labelled f-scores in the 89-90 range on Section 23 of the Wall Street Journal section of the Penn Treebank. Due to some character encoding issue, our own parser could not be used to parse the German data and this is why the Berkeley parser is used instead. 2 PB HP TS ST TT Oracle 1-best Oracle 500-best BLEU 0.6140 0.6188 0.5919 0.6013 0.5783 0.6658 0.7770 NIST 10.73 10.78"
2012.amta-papers.27,H94-1020,0,0.0570017,"er (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al., 2003) for training the French model and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1994) for training the English model.2 4 5 1. French translation memory: 5,000 held-out sentences from the Symantec English-French translation memory, split into development (2000) and test (3000). 2. German translation memory: 5,000 held-out sentences from the Symantec English-German translation memory, split into development (2000) and test (3000) 3. French forum data: 1,500 sentences taken from the Symantec English online forums, split into development (600) and test (900). These were automatically translated into French using an online translation tool and then post-edited by human translators."
2012.amta-papers.27,P08-1114,0,0.0468741,"Missing"
2012.amta-papers.27,J04-4002,0,0.487368,"n translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and c"
2012.amta-papers.27,P03-1021,0,0.0325614,"luding a maximum phrase length of 7 and a decoder distortion limit of 6 when applied. The HP system was trained using the default settings including a maximum chart span of 20. The same chart span was used for the TS,ST and TT systems. To relax the strict constraint on rule extraction in the TS, ST and TT systems, any pairs of adjacent nodes in the parse tree are combined together to form new nodes (Zollmann et al., 2008)1 . This significantly increases the number of extracted rules and consequently the translation accuracy. All five systems are tuned using minimum error rate training (MERT) (Och, 2003) on the respective developments sets. We use our in-house C++ implementation of a PCFG-LA parser (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al.,"
2012.amta-papers.27,P02-1040,0,0.0858599,".90 7.04 6.71 6.75 6.68 7.40 8.25 En-Fr TER 0.6024 0.5904 0.6118 0.6057 0.6121 0.5408 0.4717 GTM 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-doma"
2012.amta-papers.27,N07-1051,0,0.0158851,"e number of extracted rules and consequently the translation accuracy. All five systems are tuned using minimum error rate training (MERT) (Och, 2003) on the respective developments sets. We use our in-house C++ implementation of a PCFG-LA parser (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al., 2003) for training the French model and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1994) for training the English model.2 4 5 1. French translation memory: 5,000 held-out sentences from the Symantec English-French translation memory, split into development (2000) and test (3000). 2. German translation memory: 5,000 held-out sentences from the Symantec English-German translation memory, split into development (2000) and test (3000) 3. French fo"
2012.amta-papers.27,P06-1055,0,0.0355118,"rm new nodes (Zollmann et al., 2008)1 . This significantly increases the number of extracted rules and consequently the translation accuracy. All five systems are tuned using minimum error rate training (MERT) (Och, 2003) on the respective developments sets. We use our in-house C++ implementation of a PCFG-LA parser (Attia et al., 2010) to provide the parse trees for the English and French sides of the translation training data and the English sides of the evaluation data for source-syntax systems (TS and TT). The German side of the translation training data was parsed by the Berkeley parser (Petrov et al., 2006). Both parsers use the max-rule parsing algorithm (Petrov and Klein, 2007). We use the Tiger treebank (Brants et al., 2002) for training the German parsing model, the French Treebank (Abeill´e et al., 2003) for training the French model and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1994) for training the English model.2 4 5 1. French translation memory: 5,000 held-out sentences from the Symantec English-French translation memory, split into development (2000) and test (3000). 2. German translation memory: 5,000 held-out sentences from the Symantec English-German tran"
2012.amta-papers.27,W06-1608,0,0.065892,"Missing"
2012.amta-papers.27,W12-3117,1,0.827502,"the gain to be achieved by combining outputs. Manual analysis of the outputs and translation process showed that there was no obvious systematic difference between syntax-based and non-syntaxbased modelling, mostly due to the relaxation of syntactic constraints on translation rule extraction. This makes it difficult to find features to be exploited in combining these models, despite the potential gain which was observed in their oracle combination. In the future, we hope to perform successful system combination by exploring the space of features used in our recent work on quality estimation (Rubino et al., 2012). Another avenue for future work is to focus on improving parser accuracy on our datasets by leveragSource Reference HP output If you choose to continue, you will need to set the options manually from the Altiris eXpress Deployment Server Configuration control panel applet. Si vous d´ecidez de continuer, vous devrez configurer les options manuellement a` partir de l’applet du panneau de configuration Altiris eXpress Deployment Server. Si vous d´ecidez de continuer, vous devrez configurer les options manuellement a` partir de l’applet Altiris eXpress Deployment Server Configuration Control Pane"
2012.amta-papers.27,2006.amta-papers.25,0,0.267766,"n-Fr TER 0.6024 0.5904 0.6118 0.6057 0.6121 0.5408 0.4717 GTM 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-domain translation is challenging and"
2012.amta-papers.27,2003.mtsummit-papers.51,0,0.0604137,"18 0.6057 0.6121 0.5408 0.4717 GTM 0.3972 0.4008 0.3924 0.3952 0.3910 0.4265 0.4687 METEOR 0.5335 0.5341 0.5202 0.5248 0.5166 0.5585 0.6117 BLEU 0.1681 0.1662 0.1643 0.1654 0.1633 0.1935 0.2457 NIST 5.500 5.502 5.503 5.471 5.371 5.921 6.730 En-De TER 0.7428 0.7384 0.7197 0.7286 0.7358 0.6700 0.6049 GTM 0.3062 0.3082 0.3128 0.3117 0.3090 0.3376 0.3791 METEOR 0.4057 0.4028 0.3977 0.3976 0.3966 0.4248 0.4750 Table 2: Baseline and oracle system combination scores on out-of-domain development set (forum text) five popular metrics: BLEU (Papineni et al., 2002), NIST, TER (Snover et al., 2006), GTM (Turian et al., 2003) and METEOR (Denkowski and Lavie, 2011)3 . The results with these metrics for in-domain and out-of-domain development sets are presented in Table 1 and Table 2 respectively. The last two rows of each table are described in section 5.5. We report scores on the development sets as our analysis has been performed on these. Performance is considerably higher for the indomain evaluation sets compared to the out-ofdomain ones and for the En-Fr compared to En-De. Neither of these results are surprising since it is well known that out-of-domain translation is challenging and that English-German transl"
2012.amta-papers.27,P98-2230,0,0.0871534,"he syntax-based methods underperform the phrase-based models and the relaxation of syntactic constraints to broaden translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is"
2012.amta-papers.27,P01-1067,0,0.107206,"hods underperform the phrase-based models and the relaxation of syntactic constraints to broaden translation rule coverage means that these models do not necessarily generate output which is more grammatical than the output produced by the phrase-based models. Although the systems generate different output and can potentially be fruitfully combined, the lack of systematic difference between these models makes the combination task more challenging. 1 Johann Roturier‡ Introduction There has been a long tradition of using syntactic knowledge in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the mod"
2012.amta-papers.27,P08-1064,0,0.0504096,"Missing"
2012.amta-papers.27,W06-3119,0,0.0357834,"e in statistical machine translation (Wu and Wong, 1998; Yamada and Knight, 2001). After the emergence of phrase-based statistical machine translation (Och and Ney, 2004), several attempts have been made to further augment these techniques with information about the structure of the language. Hierarchical phrase-based modelling (Chiang, 2007) emphasises the recursive structure of language without concerning itself with the linguistic details. On the other hand, syntax-based modelling uses syntactic categories in addition to recursion, in mapping from source to the target (Galley et al., 2004; Zollmann and Venugopal, 2006). Syntactic information is incorporated into the model from compared along several dimensions. Finally we will discuss our observations and conclude. 2 Related Work DeNeefe et al. (2007) compared a string-to-tree model with a phrase-based model. While the syntaxbased model performed better than the phrase-based model on Chinese-to-English translation, it was shown to be worse on Arabic-to-English translation. They found that non-lexical rules form only a small fraction of the translation rule table in syntax-based modelling. The string-to-tree modelling in this work is based on their approach."
2012.amta-papers.27,C08-1144,0,0.244566,"d Jennifer Foster† ‡ Symantec Research Labs Ballycoolin Business Park Blanchardstown, Dublin 15, Ireland firstname.lastname@dcu.ie firstname lastname@symantec.com Abstract trees on the source side (tree-to-string), target side (string-to-tree), or both (tree-to-tree). Utilisation of such linguistic generalisation, however, has proven to be a more complicated task than one might first imagine it to be. While relative improvements over phrase-based baselines have been reported for some language pairs, those baselines seem to remain the best option for other language pairs (DeNeefe et al., 2007; Zollmann et al., 2008). The performance of syntax-based models is affected by errors introduced by existing imperfect syntactic parsers (Quirk and Corston-Oliver., 2006). Moreover, some non-syntactic phrases (e.g. I’m) identified by the phrase-based models bring useful information to the translation which are missed by syntax-based models trained on trees obtained using supervised parsing (Bod, 2007). Phrasal coherence between the two languages (Fox, 2002) is another factor affecting the performance of syntaxbased models. Nevertheless, these models should in theory be better able to capture long-distance reordering"
2012.amta-papers.27,C98-2225,0,\N,Missing
2012.amta-papers.27,D08-1076,0,\N,Missing
2012.amta-papers.27,P08-1000,0,\N,Missing
2013.mtsummit-posters.12,O10-3002,0,0.0338159,"Missing"
2013.mtsummit-posters.13,P11-1022,0,0.0216977,"ddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). For an overview on various feature sets and machine learning algorithms, we refer the reader to the recent shared-task on the topic (Callison-Burch et al., 2012). Most QE work focuses on estimating a score that indicates overall quality having professional translators as intended user, e.g. post-editing effort. Little work has been done for other applications of MT, such as gisting. One notable exception is the work by Specia et al. (2011), where adequacy scores for Arabic-English translations are predicted. The feature set used include standard features that try to capture general aspects o"
2013.mtsummit-posters.13,C04-1046,0,0.238333,"robust than those used in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 1 Introduction Quality estimation (QE) for machine translation (MT) is an area concerned with predicting a quality indicator for an automatically translated text without referring to human translations (the so-called reference translations typical of most MT evaluation metrics) (Blatz et al., 2004; Specia et al., 2009). The widespread use of MT in the translation industry has strongly motivated work in this area. As a consequence, the majority of existing work focuses on predicting some form of post-editing effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features use"
2013.mtsummit-posters.13,W10-1703,0,0.0187796,"est) to 5 scale (lowest). The three scores per segment pair are weighted and averaged in order to obtain one continuous score (∈ [1; 5]). From this dataset, 1, 832 segments are used to train the QE models while 422 segments are used as a test set. To build the topic models, we use the parallel corpora used by the M OSES system which generated the Spanish translations. This 2 3 http://www.tausdata.org/ http://www.bing.com/translator/ 298 corpus contains the concatenation of Europarl v5 and the News Commentary corpus from WMT10 translation task in English and Spanish (∼ 1.7M translation pairs) (Callison-Burch et al., 2010). 4.2 Additional Features In addition to the TM features described in Section 3, for all datasets, for comparison we consider a baseline set of 17 features that performed well across languages in previous work and were used as the official baseline in the WMT12 QE task (Callison-Burch et al., 2012): – number of tokens in the source & target sentences; – average source token length; – average number of occurrences of the target word within the target sentence; – number of punctuation marks in source and target sentences; – language model (LM) probability of source and target sentences using 3-g"
2013.mtsummit-posters.13,W12-3102,1,0.866279,"Missing"
2013.mtsummit-posters.13,P10-1064,0,0.0295252,"Missing"
2013.mtsummit-posters.13,P07-2045,0,0.00602471,"n of all Arabic-English newswire parallel data provided by LDC and the Arabic-English UN data,1 totalling ∼ 6.4M translation pairs after removing sentences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3"
2013.mtsummit-posters.13,C10-1070,0,0.0138361,"n 2 2 i=1 (ui ) × i=1 (vi ) Instead of measuring the distance between two n-dimensional sets of points, it is also possible to compute the sum of the absolute differences of their coordinates. These metrics, usually categorised as the Minkowski family of distance metrics, are inspired by a grid, city-like, organisation of the space. Usually referred as rectilinear or Manhattan distance, the city-block distance is directly inspired by the Euclidean distance (Krause, 1975) and has shown interesting results when applied to language processing tasks, such as context-based terminology translation (Laroche and Langlais, 2010). The city-block distance between two n-dimensional vectors u and v is given by (3): n X cityblock(u, v) = |ui − vi | (3) i=1 These three metrics allow us to compute the distance between source and target distributions assuming that they are represented in an Euclidean space. To avoid this constraint, we use two other measures in this study, based on probabilistic uncertainty as introduced by Shannon’s work (Shannon, 1948). With the measure of relative entropy, an asymmetric way of comparing two distributions suggested in (Kullback and Leibler, 1951) is given by (4): n X ui KL(u, v) = ui ln (4"
2013.mtsummit-posters.13,W12-3122,0,0.354907,"professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general aspects of quality are different from features that define adequacy. Previous work on adequacy estimation has focused on linguistic features contrasting the source and translation texts (Specia et al., 2011; Mehdad et al., 2012), e.g. the proportion of overlapping typed dependency relations in the source and target sentences with arguments that align to each other (based on wordalignment information). While these can provide interesting indicators, they are often very sparse and noisy. Sparsity happens because many of these features do not apply to most sentences, such as features comparing named entities in the source and target sentences. A significant amount of noise can come from the fact that linguistic processors, such as syntactic parsers and named entity recognisers, need to be applied to potentially low-qual"
2013.mtsummit-posters.13,D09-1092,0,0.10337,"ose used in previous work in that they focus on important (content) words in the source and target texts, as opposed to more abstract linguistic relationships between these words. We believe these are more robust as they do not depend on further analysis, and that they can be made less sparse through the exploitation of models with different dimensionalities and the use of distance metrics between topic distributions as opposed to topic distributions themselves. A challenge we face is how to model topics in a bilingual setting. We exploit two variants of TMs for that: Polylingual Topic Model (Mimno et al., 2009) and Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 295–302. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. a joint Latent Dirichlet Allocation approach (Blei et al., 2003), and a few variants of features based on these models, including the word distribution themselves and distance metrics between source and target distributions (Section 3). We experiment with three families of datasets: two annotated for ad"
2013.mtsummit-posters.13,P03-1021,0,0.00239403,"ntences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3 accessed through the second version of their API. Finally, the last system is the Systran Enterprise Server version 6, customised with the"
2013.mtsummit-posters.13,P02-1040,0,0.107966,"translation pairs after removing sentences longer than 80 words. This was virtually the same parallel corpus used to build the SMT systems. User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: M OSES, B ING 1 http://www.uncorpora.org/ and S YSTRAN. The M OSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT (Och, 2003). A trigram Language Model (LM) was built using Witten-Bell smoothing. This system was trained using in-domain translation memories (up to ∼ 1.6M translation units) plus ∼ 1M translation units from the Computer Software domain (obtained from the TAUS Data Association2 ) for the translation model, as well as additional monolingual forum data (up to 20K French sentences) for the LM. The B ING system is a freely available generic SMT system, Bing Translator,3 accessed through the second version of their API. Finally, the last system is the Systran Enterprise Server version"
2013.mtsummit-posters.13,2011.mtsummit-papers.27,0,0.0889134,"mised with the use of a domain specific 10K+ dictionary entries. Each translation is evaluated by a professional translator using two possible labels: 0 if the translation does not preserve the meaning of the source sentence and 1 if the meaning is preserved. The final dataset contains, for each of the three translations generated by the three MT systems, 694 source segments, 694 translated segments, and one adequacy score. From this dataset, 500 segments are used to train the QE models and 194 segments are held out for evaluation purposes. More information about this dataset can be found in (Roturier and Bensadoun, 2011). To build the topic models for the adequacy features, we use the in-domain Translation Memories used to train the M OSES system. WMT12 QE Data The WMT12 QE dataset (Callison-Burch et al., 2012) is composed of 2, 254 English sentences translated into Spanish by a M OSES phrase-based system and evaluated by three professional translators in terms of postediting effort on a 1 (highest) to 5 scale (lowest). The three scores per segment pair are weighted and averaged in order to obtain one continuous score (∈ [1; 5]). From this dataset, 1, 832 segments are used to train the QE models while 422 seg"
2013.mtsummit-posters.13,W12-3117,1,0.696965,"11), extracted from both source and target sentences, e.g. common dependency relations in source and target sentences. Both previous approaches for adequacy estimation severely suffer from data sparsity while attempting to model contrastive linguistic information between source and target sentences. As a consequence, the reported results are poor, sometimes even below simple baselines such as the majority class on the training data. None of the previous work uses lexicalised features or topic models built based on those features for adequacy estimation. As we will discuss in the next section, Rubino et al. (2012) used topic models as part of a larger feature set to estimate post-editing effort. However, the contribution of this information source was not tested. 3 Topic Models for Quality Estimation The first study on using topic modelling for QE was conducted in (Rubino et al., 2012) as part of the WMT12 QE shared task to estimate the postediting effort of news texts translated from English to Spanish. A joint LDA approach was used. We expand on that work by exploring two types of bilingual topic models and defining a number of variants of features based on source and target (translation) distributio"
2013.mtsummit-posters.13,P10-1063,0,0.176662,"g more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting subsegments that need revision (Bach et al., 2011). For an overview on various feature sets and machine learning algorithms, we refer the reader to the recent shared-task on the topic (Callison-Burch et al., 2012). Most QE work focuses on estimating a score that indicates overall quality having professional translators as intended user, e.g. post-editing effort. Little work has been done fo"
2013.mtsummit-posters.13,2010.jec-1.5,1,0.750247,"elves and distance metrics between source and target distributions (Section 3). We experiment with three families of datasets: two annotated for adequacy, containing newswire and user-generated content (from a product forum), and one news dataset annotated for postediting effort (Section 4). We show that TM features are more effective for both adequacyannotated types of datasets (Section 5). 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system"
2013.mtsummit-posters.13,2009.eamt-1.5,1,0.650408,"ed in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy. 1 Introduction Quality estimation (QE) for machine translation (MT) is an area concerned with predicting a quality indicator for an automatically translated text without referring to human translations (the so-called reference translations typical of most MT evaluation metrics) (Blatz et al., 2004; Specia et al., 2009). The widespread use of MT in the translation industry has strongly motivated work in this area. As a consequence, the majority of existing work focuses on predicting some form of post-editing effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general a"
2013.mtsummit-posters.13,2011.mtsummit-papers.58,1,0.919036,"diting effort to help professional translators (Section 2). However, one equally appealing application is that of estimating the adequacy of translations for gisting purposes. An indicator of such a type is particularly relevant in contexts where the reader does not know the source language. QE is generally addressed as a machine learning task. Intuitively, it is expected that features used to capture general aspects of quality are different from features that define adequacy. Previous work on adequacy estimation has focused on linguistic features contrasting the source and translation texts (Specia et al., 2011; Mehdad et al., 2012), e.g. the proportion of overlapping typed dependency relations in the source and target sentences with arguments that align to each other (based on wordalignment information). While these can provide interesting indicators, they are often very sparse and noisy. Sparsity happens because many of these features do not apply to most sentences, such as features comparing named entities in the source and target sentences. A significant amount of noise can come from the fact that linguistic processors, such as syntactic parsers and named entity recognisers, need to be applied t"
2013.mtsummit-posters.13,2011.eamt-1.12,1,0.598309,"etween source and target distributions (Section 3). We experiment with three families of datasets: two annotated for adequacy, containing newswire and user-generated content (from a product forum), and one news dataset annotated for postediting effort (Section 4). We show that TM features are more effective for both adequacyannotated types of datasets (Section 5). 2 Related Work Most research work on QE for machine translation is focused on feature engineering and feature selection, with some recent work on devising more reliable and less subjective quality labels (Specia and Farzindar, 2010; Specia, 2011). Blatz et al. (2004) present the first comprehensive study on QE for MT: 91 features were proposed and used to train predictors based on an automatic measure (e.g. NIST (Doddington, 2002)) as the quality label. Examples of successful cases of QE include improving post-editing efficiency by filtering out low quality segments which would require more effort or time to correct than translating from scratch (Specia, 2011), selecting high quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translatio"
2020.acl-main.665,C10-1012,0,0.0934949,"dataset and 8 of the Wikitext and CNN datasets. 2.6 Evaluation The evaluation is performed on detokenized sentences9 using the official evaluation script from the 2018 shared task. We focus on BLEU-4 score (Papineni et al., 2002) which was shown in both shared tasks to be highly correlated with human evaluation scores. 3 Results In Table 1, we compare our results on the test set with those reported in Yu et al. (2019b), which include the Yu et al. system (Yu19), the best 2018 shared task result for English (Elder and Hokamp, 2018) (ST18) and Yu et al.’s implementation of two other baselines, Bohnet et al. (2010) (B10) and Puduppully et al. (2016) (P16) . Ignoring for now the result with synthetic data, we can see that our system is competitive with that of Yu et al (72.3 vs 72.7). In Section 2.3, we described three improvements to our baseline system: random linearization, scoping and restricted beam search. An ablation analysis of these improvements on the dev set is shown in Table 2. The biggest improvement comes from the introduction of random lineariza9 Using detokenized inputs for BLEU makes the score very sensitive to detokenization used and in the 2019 shared task evaluation was changed to use"
2020.acl-main.665,D19-1052,0,0.0269763,"Missing"
2020.acl-main.665,W19-2308,1,0.826601,"z et al., 2011; Mille et al., 2018, 2019) appears to be a relatively straightforward problem. Given a tree of lemmas, a system has to restore the original word order of the sentence and inflect its lemmas, see Figure 1. Yet SR systems often struggle, even for a relatively fixed word order language such as English. Improved performance would facilitate investigation of more complex versions of the shallow task, such as the deep task in which function words are pruned from the tree, which may be of more practical use in pipeline natural language generation (NLG) systems (Moryossef et al., 2019; Elder et al., 2019; Castro Ferreira et al., 2019). In this paper we explore the use of synthetic data for the English shallow task. Synthetic data is created by taking an unlabelled sentence, parsing it with an open source universal dependency parser1 and transforming the result into the input representation. Unlike in the 2018 shared task, where a system trained with synthetic data performed roughly the same as a system trained on the original dataset (Elder and Hokamp, 2018; King and White, 2018), we find its use leads to a large improvement in performance. The state-of-the-art on the dataset is 72.7 BLEU-4 s"
2020.acl-main.665,W18-3606,1,0.945058,"s are pruned from the tree, which may be of more practical use in pipeline natural language generation (NLG) systems (Moryossef et al., 2019; Elder et al., 2019; Castro Ferreira et al., 2019). In this paper we explore the use of synthetic data for the English shallow task. Synthetic data is created by taking an unlabelled sentence, parsing it with an open source universal dependency parser1 and transforming the result into the input representation. Unlike in the 2018 shared task, where a system trained with synthetic data performed roughly the same as a system trained on the original dataset (Elder and Hokamp, 2018; King and White, 2018), we find its use leads to a large improvement in performance. The state-of-the-art on the dataset is 72.7 BLEU-4 score (Yu et al., 2019b) – our system achieves a similar result of 72.3, which improves to 80.1 with the use of synthetic data. We analyse the ways in which synthetic data helps to improve performance, finding that longer sentences are particularly improved and more exactly correct linearizations are generated overall. 1 A number of these exist, e.g. https://github. com/stanfordnlp/stanfordnlp and http: //lindat.mff.cuni.cz/services/udpipe/ 7465 Proceedings o"
2020.acl-main.665,N18-1014,0,0.0140897,"earch, the output sequence length is artificially constrained to contain the same number of tokens as the input. 2.3 Improvements to baseline Random linearizations In the baseline system, a single random depth first linearization of the training data is obtained and used repeatedly to train the model. Instead, we obtain multiple linearizations, so that each epoch of training data potentially contains a different linearization of the same dependency tree. This makes the model more robust to different linearizations, which is helpful as neural networks don’t generally deal well with randomness (Juraska et al., 2018). Scoping brackets Similar to Konstas et al. (2017) we apply scoping brackets around child nodes. This provides further indication of the tree structure to the model, despite using a linear sequence as input. Restricted beam search In an attempt to reduce unnecessary errors during decoding, our beam search looks at the input sequence and restricts the available vocabulary to only tokens from the input, and tokens which have not yet appeared in the output sequence. This is similar to the approach used by King and White (2018). 7466 2.4 Synthetic Data To augment the existing training data we cre"
2020.acl-main.665,W18-3605,0,0.240129,"ee, which may be of more practical use in pipeline natural language generation (NLG) systems (Moryossef et al., 2019; Elder et al., 2019; Castro Ferreira et al., 2019). In this paper we explore the use of synthetic data for the English shallow task. Synthetic data is created by taking an unlabelled sentence, parsing it with an open source universal dependency parser1 and transforming the result into the input representation. Unlike in the 2018 shared task, where a system trained with synthetic data performed roughly the same as a system trained on the original dataset (Elder and Hokamp, 2018; King and White, 2018), we find its use leads to a large improvement in performance. The state-of-the-art on the dataset is 72.7 BLEU-4 score (Yu et al., 2019b) – our system achieves a similar result of 72.3, which improves to 80.1 with the use of synthetic data. We analyse the ways in which synthetic data helps to improve performance, finding that longer sentences are particularly improved and more exactly correct linearizations are generated overall. 1 A number of these exist, e.g. https://github. com/stanfordnlp/stanfordnlp and http: //lindat.mff.cuni.cz/services/udpipe/ 7465 Proceedings of the 58th Annual Meeti"
2020.acl-main.665,P17-4012,0,0.0147386,"it has been noted that the use of synthetic data is problematic in NLG tasks (WeatherGov (Liang et al., 2009) being the notable example) our data is created differently. The WeatherGov dataset is constructed by pairing a table with the output of a rule-based NLG system. This means any system trained on WeatherGov only re-learns the rules used to generate the text. Our approach is the reverse; we parse an existing, naturally occurring sentence, and, thus, the model must learn to reverse the parsing algorithm. 2.5 Training The system is trained using a custom fork7 of the OpenNMT-py framework (Klein et al., 2017), the only change made was to the beam search decoding code. Hyperparameter details and replication instructions are provided in our project’s repository8 , in particular in the config directory. Vocabulary size varies based on the datasets in use. It is determined by using any tokens which appears 10 times or more. When using the original shared task dataset, the vocabulary size is 7 https://github.com/Henry-E/OpenNMT-py https://github.com/Henry-E/ surface-realization-shallow-task 8 B10 P16 ST18 Yu19 Ours Ours + Synthetic data BLEU-4 70.8 65.9 69.1 72.7 72.3 80.1 Table 1: Test set results for"
2020.acl-main.665,P17-1014,0,0.227968,"ight not scale to the use of synthetic data, and an inadvertent consequence of such a rule is that it may produce results which could be misleading for future research directions. For instance, the system which was the clear winner of this year’s shared task (Yu et al., 2019a) used tree-structured long short-term memory (LSTM) networks (Tai et al., 2015). In general, tree LSTMs can be slow and difficult to train.2 Song et al. (2018) utilized a variant of the tree LSTM in a similar NLG task, converting abstract meaning representation (AMR) graphs to text. Following the state-of-the-art system (Konstas et al., 2017), which used standard LSTMs, Song et al. augmented their training with synthetic data. Though their system outperformed Konstas et al. at equivalent levels of additional training sentences, it was unable to scale up to the 20 million sentences used by the best Konstas et al. system and ultimately did not outperform them.3 Critics of neural NLG approaches4 emphasise that quality and reliability are at the core of production-ready NLG systems. What we are essentially arguing is that if using synthetic data contributes to producing higher quality outputs, then we ought to ensure we are designing"
2020.acl-main.665,P09-1011,0,0.0448839,"Missing"
2020.acl-main.665,P14-5010,0,0.0060435,"Missing"
2020.acl-main.665,W18-3601,0,0.081903,"ts on the English 2018 dataset, that the use of synthetic data can have a substantial positive effect – an improvement of almost 8 BLEU points for a previously state-of-the-art system. We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data. 1 : AP the from story this This story comes from the AP: Figure 1: Example tree and reference sentence Introduction The shallow task of the recent surface realization (SR) shared tasks (Belz et al., 2011; Mille et al., 2018, 2019) appears to be a relatively straightforward problem. Given a tree of lemmas, a system has to restore the original word order of the sentence and inflect its lemmas, see Figure 1. Yet SR systems often struggle, even for a relatively fixed word order language such as English. Improved performance would facilitate investigation of more complex versions of the shallow task, such as the deep task in which function words are pruned from the tree, which may be of more practical use in pipeline natural language generation (NLG) systems (Moryossef et al., 2019; Elder et al., 2019; Castro Ferreir"
2020.acl-main.665,D19-6301,0,0.27781,"Missing"
2020.acl-main.665,N19-1236,0,0.043689,"Missing"
2020.acl-main.665,N19-4007,0,0.0142869,"Lins SR + Random Lins + Scope SR + Random Lins + Scope + Restricted Beam 40 Table 2: Dev set results for ablation of the baseline system plus improvements, trained only on the original dataset Table 3: Dev set results for the SR shared task data with additional synthetic data: the role of the corpus The role of corpus Table 3 compares the Wikitext corpus as a source of additional training data to the CNN corpus. Both the individual results and the result obtained by combining the two corpora show that there is little difference between the two. Sentence length and BLEU score Using compare-mt (Neubig et al., 2019) we noticed a striking difference between the systems with regards to performance on sentences of different length.10 This is shown in Figure 2. Even though the synthetic data sentences were limited to 50 tokens in length, the synthetic data performed equally well for sentence length buckets 50-60 and 60+, while the baseline data system performed relatively worse. It is possible this is due to the synthetic data system containing a larger vocabulary and being exposed to a wider range of commonly occurring phrases, which make up parts of longer sentences. 10 These are results for the tokenized"
2020.acl-main.665,P02-1040,0,0.108735,"193 tokens, training is done for 33 epochs and takes 40 minutes on two Nvidia 1080 Ti GPUs. All hyperparameters stay the same when training with the synthetic data, except for vocabulary size and training time. For the combined shared task, Wikitext and CNN datasets the vocabulary size is 89,233, training time increases to around 2 days, and uses 60 random linearizations of the shared task dataset and 8 of the Wikitext and CNN datasets. 2.6 Evaluation The evaluation is performed on detokenized sentences9 using the official evaluation script from the 2018 shared task. We focus on BLEU-4 score (Papineni et al., 2002) which was shown in both shared tasks to be highly correlated with human evaluation scores. 3 Results In Table 1, we compare our results on the test set with those reported in Yu et al. (2019b), which include the Yu et al. system (Yu19), the best 2018 shared task result for English (Elder and Hokamp, 2018) (ST18) and Yu et al.’s implementation of two other baselines, Bohnet et al. (2010) (B10) and Puduppully et al. (2016) (P16) . Ignoring for now the result with synthetic data, we can see that our system is competitive with that of Yu et al (72.3 vs 72.7). In Section 2.3, we described three im"
2020.acl-main.665,N16-1058,0,0.0151901,"and CNN datasets. 2.6 Evaluation The evaluation is performed on detokenized sentences9 using the official evaluation script from the 2018 shared task. We focus on BLEU-4 score (Papineni et al., 2002) which was shown in both shared tasks to be highly correlated with human evaluation scores. 3 Results In Table 1, we compare our results on the test set with those reported in Yu et al. (2019b), which include the Yu et al. system (Yu19), the best 2018 shared task result for English (Elder and Hokamp, 2018) (ST18) and Yu et al.’s implementation of two other baselines, Bohnet et al. (2010) (B10) and Puduppully et al. (2016) (P16) . Ignoring for now the result with synthetic data, we can see that our system is competitive with that of Yu et al (72.3 vs 72.7). In Section 2.3, we described three improvements to our baseline system: random linearization, scoping and restricted beam search. An ablation analysis of these improvements on the dev set is shown in Table 2. The biggest improvement comes from the introduction of random lineariza9 Using detokenized inputs for BLEU makes the score very sensitive to detokenization used and in the 2019 shared task evaluation was changed to use tokenized inputs instead. 7467 100"
2020.acl-main.665,K18-2016,0,0.0878407,"Missing"
2020.acl-main.665,P17-1099,0,0.0332245,"sentences, while Konstas et al. scored 32.3 with 2 million and 33.8 with 20 million (the best overall system). 4 See, for example, https://ehudreiter.com/ 5 http://taln.upf.edu/pages/msr2018-ws/ SRST.html 6 https://github.com/ UniversalDependencies/UD_English-EWT The training set consists of 12,375 sentences, dev 1,978, test 2,062. 2.2 Baseline system The system we use is an improved version of a previous shared task participant’s system (Elder and Hokamp, 2018). This baseline system is a bidirectional LSTM encoder-decoder model. The model is trained with copy attention (Vinyals et al., 2015; See et al., 2017) which allows it to copy unknown tokens from the input sequence to the output. The system performs both linearization and inflection in a single decoding step. To aid inflection, a list is appended to the input sequence containing possible forms for each relevant lemma. Depth first linearization (Konstas et al., 2017) is used to convert the tree structure into a linear format, which is required for the encoder. This linearization begins at the root node and adds each subsequent child to the sequence, before returning to the highest node not yet added. Where there are multiple child nodes one i"
2020.acl-main.665,P18-1150,0,0.0131409,"which prohibited the use of synthetic data. This was done in order to make the results of different systems more comparable. However, systems designed with smaller datasets in mind might not scale to the use of synthetic data, and an inadvertent consequence of such a rule is that it may produce results which could be misleading for future research directions. For instance, the system which was the clear winner of this year’s shared task (Yu et al., 2019a) used tree-structured long short-term memory (LSTM) networks (Tai et al., 2015). In general, tree LSTMs can be slow and difficult to train.2 Song et al. (2018) utilized a variant of the tree LSTM in a similar NLG task, converting abstract meaning representation (AMR) graphs to text. Following the state-of-the-art system (Konstas et al., 2017), which used standard LSTMs, Song et al. augmented their training with synthetic data. Though their system outperformed Konstas et al. at equivalent levels of additional training sentences, it was unable to scale up to the 20 million sentences used by the best Konstas et al. system and ultimately did not outperform them.3 Critics of neural NLG approaches4 emphasise that quality and reliability are at the core of"
2020.acl-main.665,P15-1150,0,0.0456558,"Missing"
2020.acl-main.665,D19-6306,0,0.503141,"ro Ferreira et al., 2019). In this paper we explore the use of synthetic data for the English shallow task. Synthetic data is created by taking an unlabelled sentence, parsing it with an open source universal dependency parser1 and transforming the result into the input representation. Unlike in the 2018 shared task, where a system trained with synthetic data performed roughly the same as a system trained on the original dataset (Elder and Hokamp, 2018; King and White, 2018), we find its use leads to a large improvement in performance. The state-of-the-art on the dataset is 72.7 BLEU-4 score (Yu et al., 2019b) – our system achieves a similar result of 72.3, which improves to 80.1 with the use of synthetic data. We analyse the ways in which synthetic data helps to improve performance, finding that longer sentences are particularly improved and more exactly correct linearizations are generated overall. 1 A number of these exist, e.g. https://github. com/stanfordnlp/stanfordnlp and http: //lindat.mff.cuni.cz/services/udpipe/ 7465 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7465–7471 c July 5 - 10, 2020. 2020 Association for Computational Linguistics"
2020.acl-main.665,W19-8636,0,0.0933681,"ro Ferreira et al., 2019). In this paper we explore the use of synthetic data for the English shallow task. Synthetic data is created by taking an unlabelled sentence, parsing it with an open source universal dependency parser1 and transforming the result into the input representation. Unlike in the 2018 shared task, where a system trained with synthetic data performed roughly the same as a system trained on the original dataset (Elder and Hokamp, 2018; King and White, 2018), we find its use leads to a large improvement in performance. The state-of-the-art on the dataset is 72.7 BLEU-4 score (Yu et al., 2019b) – our system achieves a similar result of 72.3, which improves to 80.1 with the use of synthetic data. We analyse the ways in which synthetic data helps to improve performance, finding that longer sentences are particularly improved and more exactly correct linearizations are generated overall. 1 A number of these exist, e.g. https://github. com/stanfordnlp/stanfordnlp and http: //lindat.mff.cuni.cz/services/udpipe/ 7465 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7465–7471 c July 5 - 10, 2020. 2020 Association for Computational Linguistics"
2020.acl-main.665,W11-2832,0,\N,Missing
2020.acl-main.778,D15-1041,0,0.027525,"the usefulness of interpolated treebank vectors which are computed via a weighted combination of the predefined fixed ones. In experiments with Czech, English and French, we establish that useful interpolated treebank vectors exist. We then develop a simple k-NN method based on sentence similarity to choose a treebank vector, either fixed or interpolated, for sentences or entire test sets, which, for 9 of our 10 test languages matches the performance of the best (oracle) proxy treebank. 2 Interpolated Treebank Vectors Following recent work in neural dependency parsing (Chen and Manning, 2014; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Zeman et al., 2017, 2018), we represent an input token by concatenating various vectors. In our experiments, each word wi in a sentence S = (w1 ,...,wn ) is a 8812 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8812–8818 c July 5 - 10, 2020. 2020 Association for Computational Linguistics concatenation of 1) a dynamically learned word vector, 2) a word vector obtained by passing the ki characters of wi through a BiLSTM and 3), following Stymne et al. (2018), a treebank embedding to distinguish the m training treeb"
2020.acl-main.778,K17-3005,0,0.0170575,"terpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set. 1 Introduction The Universal Dependencies project (Nivre et al., 2016) has made available multiple treebanks for the same language annotated according to the same scheme, leading to a new wave of research which explores ways to use multiple treebanks in monolingual parsing (Shi et al., 2017; Sato et al., 2017; Che et al., 2017; Stymne et al., 2018). Stymne et al. (2018) introduced a treebank embedding. A single model is trained on the concatenation of the available treebanks for a language, and the input vector for each training token includes the treebank embedding which encodes the treebank the token comes from. At test time, all input vectors in the test set of the same treebank are also assigned this treebank embedding vector. Stymne et al. (2018) show that this approach is superior to mono-treebank training and to plain treebank concatenation. Treebank embeddings perform at about the same level as training on"
2020.acl-main.778,K18-2005,0,0.0599588,"Missing"
2020.acl-main.778,D14-1082,0,0.0612694,"In doing so, we explore the usefulness of interpolated treebank vectors which are computed via a weighted combination of the predefined fixed ones. In experiments with Czech, English and French, we establish that useful interpolated treebank vectors exist. We then develop a simple k-NN method based on sentence similarity to choose a treebank vector, either fixed or interpolated, for sentences or entire test sets, which, for 9 of our 10 test languages matches the performance of the best (oracle) proxy treebank. 2 Interpolated Treebank Vectors Following recent work in neural dependency parsing (Chen and Manning, 2014; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Zeman et al., 2017, 2018), we represent an input token by concatenating various vectors. In our experiments, each word wi in a sentence S = (w1 ,...,wn ) is a 8812 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8812–8818 c July 5 - 10, 2020. 2020 Association for Computational Linguistics concatenation of 1) a dynamically learned word vector, 2) a word vector obtained by passing the ki characters of wi through a BiLSTM and 3), following Stymne et al. (2018), a treebank embedding to distin"
2020.acl-main.778,Q16-1023,0,0.0459989,"ated treebank vectors which are computed via a weighted combination of the predefined fixed ones. In experiments with Czech, English and French, we establish that useful interpolated treebank vectors exist. We then develop a simple k-NN method based on sentence similarity to choose a treebank vector, either fixed or interpolated, for sentences or entire test sets, which, for 9 of our 10 test languages matches the performance of the best (oracle) proxy treebank. 2 Interpolated Treebank Vectors Following recent work in neural dependency parsing (Chen and Manning, 2014; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Zeman et al., 2017, 2018), we represent an input token by concatenating various vectors. In our experiments, each word wi in a sentence S = (w1 ,...,wn ) is a 8812 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8812–8818 c July 5 - 10, 2020. 2020 Association for Computational Linguistics concatenation of 1) a dynamically learned word vector, 2) a word vector obtained by passing the ki characters of wi through a BiLSTM and 3), following Stymne et al. (2018), a treebank embedding to distinguish the m training treebanks: e(i) = e1 (wi ) ◦ biLSTM(e"
2020.acl-main.778,W17-6314,0,0.154922,"Missing"
2020.acl-main.778,L16-1262,0,0.0767491,"Missing"
2020.acl-main.778,N18-1202,0,0.0305474,"eebank vector for an input sentence using the k most similar training sentences (se-se), and 2) allocating the treebank vector for a set of input sentences using the most similar training treebank (tr-tr). We will first explain the se-se case. For each input sentence, we retrieve from the training data the k most similar sentences and then identify the treebank vectors from the candidate samples that have the highest LAS. To compute similarity, we represent sentences either as tf-idf vectors computed over character n-grams, or as vectors produced by max-pooling over a sentence’s ELMo vectors (Peters et al., 2018) produced by averaging all ELMo biLM layers.3 We experiment with k = 1, 3, 9. For many sentences, several treebank vectors yield the optimal LAS for the most similar retrieved sentence(s), and so we try several tie-breaking strategies, including choosing the vector closest to the uniform weight vector (i. e. each of the three treebanks is equally weighted), re-ranking the list of vectors in the tie according to the LAS of the next most similar sentence, and using the average LAS of the k sentences retrieved to choose the treebank vector. Three treebank vector sample sizes were tried: 1. fixed:"
2020.acl-main.778,K18-2001,0,0.0505732,"Missing"
2020.acl-main.778,K17-3007,0,0.0845506,"hat 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set. 1 Introduction The Universal Dependencies project (Nivre et al., 2016) has made available multiple treebanks for the same language annotated according to the same scheme, leading to a new wave of research which explores ways to use multiple treebanks in monolingual parsing (Shi et al., 2017; Sato et al., 2017; Che et al., 2017; Stymne et al., 2018). Stymne et al. (2018) introduced a treebank embedding. A single model is trained on the concatenation of the available treebanks for a language, and the input vector for each training token includes the treebank embedding which encodes the treebank the token comes from. At test time, all input vectors in the test set of the same treebank are also assigned this treebank embedding vector. Stymne et al. (2018) show that this approach is superior to mono-treebank training and to plain treebank concatenation. Treebank embeddings perform at about the same lev"
2020.acl-main.778,K17-3003,0,0.0133451,"lations. We show that 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set. 1 Introduction The Universal Dependencies project (Nivre et al., 2016) has made available multiple treebanks for the same language annotated according to the same scheme, leading to a new wave of research which explores ways to use multiple treebanks in monolingual parsing (Shi et al., 2017; Sato et al., 2017; Che et al., 2017; Stymne et al., 2018). Stymne et al. (2018) introduced a treebank embedding. A single model is trained on the concatenation of the available treebanks for a language, and the input vector for each training token includes the treebank embedding which encodes the treebank the token comes from. At test time, all input vectors in the test set of the same treebank are also assigned this treebank embedding vector. Stymne et al. (2018) show that this approach is superior to mono-treebank training and to plain treebank concatenation. Treebank embeddings perform at"
2020.acl-main.778,P18-2098,0,0.356473,"Missing"
2020.acl-main.778,K17-3001,0,\N,Missing
2020.coling-main.590,P18-1236,0,0.0543152,"nt to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantastic or excellent with correspondingly high ratings but anot"
2020.coling-main.590,D19-1562,0,0.0437692,"show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantastic or excellent with correspondingl"
2020.coling-main.590,D16-1171,0,0.155509,"ther. Experiment results on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user migh"
2020.coling-main.590,N19-1423,0,0.0888363,"that product. Secondly, learning meaningful user and product embeddings that are only updated by back propagation is difficult when a user or product only has a small number of reviews, whereas one may still be able to glean something useful from the text of even a small number of reviews. A naive approach might compute representations of all the reviews of a given user or product each time we have a new training sample but this would be too expensive, and we instead propose the following incremental approach: With each new training sample, we obtain the review text representation, with BERT (Devlin et al., 2019) as our encoder, before using the representation together with user and product vectors to obtain a user-biased document representation and a product-biased document representation, which are then employed to obtain sentiment polarity. We then add the user-biased and product-biased This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 6724 Proceedings of the 28th International Conference on Computational Linguistics, pages 6724–6729 Barcelona, Spain (Online), December 8-13, 2020 document representati"
2020.coling-main.590,D17-1054,0,0.0180653,"lp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantasti"
2020.coling-main.590,W18-6220,0,0.0931193,"Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantastic or excellent with"
2020.coling-main.590,I17-1064,0,0.27945,"sults on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such"
2020.coling-main.590,S14-2004,0,0.0170246,"t a better idea of where there is room for improvement for IUPC, we examine the 43 Yelp13 dev set cases, where the predicted label differs from the gold label by more than two points. There are a handful of cases of sarcasm, e.g. that lovely tempe waste/tap water taste in the food, but the most noteworthy phenomenon is mixed sentiment, e.g. tacos were good the soup was not tasty, or the more subtle brave the scary parking and lack of ambiance. It is not always clear from the reviews which aspect of the service the rating is directed towards. This suggests that aspect-based sentiment analysis (Pontiki et al., 2014) might be useful here, and training an IUPC model for this task is a possible avenue for future work. 4 Conclusion In this paper, we propose a neural sentiment analysis architecture that explicitly utilizes all past reviews from a given user or product to improve sentiment polarity classification on the document level. Our experimental results on the IMDB, Yelp-13 and Yelp-14 datasets demonstrate that incorporating this additional context is effective, particularly for the Yelp datasets. The code used to run the experiments is available for use by the research community.4 Acknowledgements This"
2020.coling-main.590,P15-1098,0,0.0245957,"oring representations of reviews written by the same user and about the same product and force the model to memorize all reviews for one particular user and product. Additionally, we drop the hierarchical architecture used in previous work to enable words in the text to directly attend to each other. Experiment results on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using"
2020.emnlp-main.230,W19-8652,0,0.0358428,"Missing"
2020.emnlp-main.230,W16-3622,0,0.0398695,"Missing"
2020.emnlp-main.230,2015.mtsummit-papers.23,0,0.0446434,"relations (Oraby et al., 2019) must be used. 1 https://github.com/tuetschek/ e2e-cleaning How can we add surface forms to an input sequence from the validation or test sets without peeking at its target utterance? A simple heuristic we have used is to choose the most common surface form for each attribute-value from the training set. Furthermore, by including all the necessary content words in the input sequence, it is possible to limit the vocabulary used during generation to only these content words and a couple of hundred function words. This would enable the use of a constrained softmax (Hu et al., 2015) – an optimization that can greatly speed up the decoding step. 4 Experimental Setup We performed experiments with the E2E NLG Challenge dataset (Duˇsek et al., 2019b). It is a task-oriented dialogue dataset, collected using crowd sourcing, focused on the surface realization of attribute-value pairs describing restaurants. 2878 4.1 Applying the Surface Forms Method ♥ ♥ To extract the surface form of each attribute-value pair from a target utterance, we used modified regular expressions from Duˇsek et al. (2019a). The input sequence was constructed in the format of a single token representing a"
2020.emnlp-main.230,N18-1014,0,0.0314039,"Missing"
2020.emnlp-main.230,P17-4012,0,0.0392783,"were ignored. To avoid peeking at the target utterance when adding surface forms to the validation and test sets, the most common surface form for each attribute-value pair from the training set was used. The task proved to be simple enough for the model that only minimal restricted decoding was necessary. We added a single rule to the beam search: if restaurant does not appear in the input then it should not appear in the output. 4.2 Modelling Our baseline is a sequence-to-sequence model with copy attention, trained on the E2E dataset, using the neural machine translation framework OpenNMT (Klein et al., 2017). To test our method, we trained a model with the same hyperparameters (see the appendix for details) on the surface form augmented version of the E2E dataset. 4.3 Reference Systems The E2E NLG Challenge organisers released the generated outputs of all participant systems. In our analysis, we compare with three of these systems: 1. the E2E baseline, TGen (Duˇsek and Jurcicek, 2016), a neural system with a semantic reranker as a final step to improve accuracy 2. the overall winner of E2E, Slug2Slug (Juraska et al., 2018), a neural system, also with a reranker, trained using an augmented dataset"
2020.emnlp-main.230,W07-0734,0,0.061636,"Missing"
2020.emnlp-main.230,W04-1013,0,0.0319666,"Missing"
2020.emnlp-main.230,W18-3601,0,0.0117896,"n for a restricted vocabulary, which speeds up decoding. The major weakness of both approaches, however, is the difficulty of extracting surface forms from human-authored text. We were able to avail of the hand-crafted regular expressions of Dusek et al in our E2E experiments, but moving to another dataset would entail a similar exercise. A method to do this automatically would be convenient. Some work has already been done by Oraby et al. (2019), in which dependency trees were used to find adjectives that describe a specific list of food related nouns. In the Surface Realization shared task (Mille et al., 2018), the deep task dataset was created by pruning function words from a dependency tree, leaving only content words remaining. In our proposed method, surface forms still need to be joined together with function words. We believe neural networks are well suited for this task because they are good at generating natural sounding, though sometimes nonsensical, text. By combining neural generation with constraints based on content words included in the input sequence, we aim to achieve both reliability and naturalness. An alternate approach, which we did not compare with, is automatic template genera"
2020.emnlp-main.230,P02-1040,0,0.108028,"Missing"
2020.emnlp-main.230,W18-6557,0,0.0359659,"Missing"
2020.emnlp-main.230,D15-1199,0,0.0254935,"dialogue is to help a user achieve a narrow goal, such as booking a restaurant or movie ticket. The final step of a conversational interface is generating a response to the user; more specifically, performing surface realization of some structured data containing relevant information. Research into neural NLG systems for the surface realization task is popular because such systems may have advantages over the dominant rule and template-based systems: neural NLG systems trained on datasets may be both easier to maintain and to scale to new domains, as well as generating more natural responses (Wen et al., 2015; Guo and Zhao, 2017). But neural NLG systems are not without problems. They are widely considered too unreliable for business applications; they have a tendency to hallucinate facts, unsupported by the structured data they were given (Wiseman et al., 2017). A less well known issue is the template-like generation of neural NLG systems (Wei et al., 2019). Figure 1 highlights this issue; neural NLG systems (TGen and Slug2Slug) are far less diverse than the training data (E2E Dataset) in their usage of surface forms that express an attribute. Intuitively, one might expect that a neural NLG system"
2020.emnlp-main.230,D18-1356,0,0.0330728,"Missing"
2020.insights-1.2,D18-2029,0,0.0280094,"Missing"
2020.insights-1.2,N18-1202,0,0.121837,"Missing"
2020.insights-1.2,W18-3022,0,0.0256076,"v and Frank, 2018; Lin similarities with the question q and passage p as et al., 2019; Qiu et al., 2019). Partly inspired by shown in (6): Weissenborn et al. (2017), we convert ConceptNet relations into sentences but instead of embedding ∀s ∈ S : score(s) = g(k(s), k(q))+g(k(s), k(p)) them independently, we concatenate them to the (6) baseline model input. where g ∈ {correlation, cosine} are similarity 3.1.1 Sentence Extraction and Filtering measures, k is a semantic embedding function. We 7 use the semantic textual similarity model8 proposed ConceptNet has 34 relation types. Each relation by Yang et al. (2018). To filter more examples, we has start and end entities and a strength of relation add an empirically tuned threshold for similarities9 (relevance weight). We look up every annotated entity from questions and passages in ConceptNet. and select only those sentences which were ranked as the most similar to the question and passage by We extract the top 100 relations according to the both correlation (inner product) and cosine similarrelevance weight, and select those where both the ity, and each score is higher than the established start and end entities are in English. We remove thresholds. An"
2020.insights-1.2,N19-1421,0,0.0318414,"Missing"
2020.insights-1.2,D19-5306,0,0.043233,"Missing"
2020.insights-1.2,W18-5446,0,0.0444845,"Missing"
2020.insights-1.2,N18-1101,0,0.0953485,"Missing"
2020.iwpt-1.24,W19-3712,0,0.0175756,"talbanken in order to fit a batch into memory. We were also given access to an NVIDIA V100 GPU with 32GB of VRAM which enabled us to process all treebanks except for ar padt without removing long sentences. For ar padt, after removing the longest 75 sentences, the model still required 29GB of VRAM. 2.4.4 BERT Models For the BERT models, in early development runs we compared multilingual BERT (mBERT) with a language-specific BERT model if there was one available in HuggingFace’s (Wolf et al., 2019) models repository.15 We used a language-specific BERT model for ar (Safaya et al., 2020), bg+cs (Arkhipov et al., 2019), en (Devlin et al., 2019), 231 15 https://huggingface.co/models fi (Virtanen et al., 2019), it16 , nl (de Vries et al., 2019), pl17 , ru (Kuratov and Arkhipov, 2019) and sv18 and for the rest of the languages we used mBERT (Devlin et al., 2019). We found that the language-specific variant was always better than mBERT except for pl lfg. For fr sequoia, we tried using the CamemBERT model (Martin et al., 2020). As this model uses RoBERTA (Liu et al., 2019) as opposed to BERT, we installed AllenNLP from the master repository which uses HuggingFace’s AutoTokenizer module which supports many BERT-l"
2020.iwpt-1.24,N09-2066,0,0.103676,"Missing"
2020.iwpt-1.24,Q17-1010,0,0.00551886,"ipeline according to ELAS on the treebank with the biggest development set for the language.6 2.2 Basic Parsing We choose UDPipe-future (Straka, 2018) for basic parsing and joint prediction of lemmata, POS tags and morphological features so as to not require a separate tagger. We extend UDPipe-future to train multi-treebank models as introduced by (Stymne et al., 2018) with UUParser.7,8 Inspired by Straka et al. (2019), we use two types of external word embeddings with UDPipe-future: ELMo contextualised word embeddings (Peters et al., 2018) and FastText character-n-gram-based word embeddings (Bojanowski et al., 2017).9 For 15 of the 17 test languages, ElmoForManyLangs10 (Che et al., 2018) provides ELMo models. We train FastText on the raw text provided by the CoNLL’17 shared task for the same 15 languages after shuffling sentences. For the Russian FastText model, we kept getting vectors with large component values even after trying a different machine and a different permutation of sentences, prohibiting effective training of the parser. We then used a model trained on 2⁄3 of the Russian data for which component values and parser LAS were in the expected range. Furthermore, we train UDPipe-future models u"
2020.iwpt-1.24,K18-2005,0,0.0283975,"he language.6 2.2 Basic Parsing We choose UDPipe-future (Straka, 2018) for basic parsing and joint prediction of lemmata, POS tags and morphological features so as to not require a separate tagger. We extend UDPipe-future to train multi-treebank models as introduced by (Stymne et al., 2018) with UUParser.7,8 Inspired by Straka et al. (2019), we use two types of external word embeddings with UDPipe-future: ELMo contextualised word embeddings (Peters et al., 2018) and FastText character-n-gram-based word embeddings (Bojanowski et al., 2017).9 For 15 of the 17 test languages, ElmoForManyLangs10 (Che et al., 2018) provides ELMo models. We train FastText on the raw text provided by the CoNLL’17 shared task for the same 15 languages after shuffling sentences. For the Russian FastText model, we kept getting vectors with large component values even after trying a different machine and a different permutation of sentences, prohibiting effective training of the parser. We then used a model trained on 2⁄3 of the Russian data for which component values and parser LAS were in the expected range. Furthermore, we train UDPipe-future models using FastText and internal embeddings only. 5 Due to a configuration erro"
2020.iwpt-1.24,N19-1423,0,0.0903989,"Estonian, French, Dutch and Polish (a subset of the languages with PUD treebanks announced in the development pack), we randomised on the sentence level which proxy treebank is used during multi-treebank parsing. 229 2.4.2 Feature Representations In our experiments, each word wi in a sentence S = (w0 , w1 , . . . , wN ) is converted to its vector representation xi . We trained different variants of our semantic parser where xi is the concatenation of different combinations of the below features: • BERT embedding: The first word-piece embedding of the wordpiece-tokenised input word from BERT (Devlin et al., 2019) (BERT ) ei ∈ R768 • character embedding: A character embedding obtained by passing the k characters ch1 , . . . , chk of wi through a BiLSTM: (ch) BiLSTM(ch1:k ), ei ∈ R64 ROOT conj:and cc nmod:of case root Tale of joy and sorrow nmod:of (a) Enhanced UD graph. (b) Edge-existence probabilities. Figure 1 The enhanced UD graph and edge-existence probabilities of the semantic parser trained on en ewt for the phrase Tale of joy and sorrow. • lemma embedding: The embedding of the (le) word’s lemma ei ∈ R50 xi = [ei ; ei ; ei ; ei ; ei ] • UPOS embedding: The embedding of the (u) word’s universal PO"
2020.iwpt-1.24,P18-2077,0,0.441499,"in 17 languages. We implement a pipeline approach using UDPipe and UDPipe-future to provide initial levels of annotation. The enhanced dependency graph is either produced by a graph-based semantic dependency parser or is built from the basic tree using a small set of heuristics. Our results show that, for the majority of languages, a semantic dependency parser can be successfully applied to the task of parsing enhanced dependencies. 3. two types of enhancers: (a) copying the basic tree and applying a small set of heuristics (baseline system), and (b) a graph-based semantic dependency parser (Dozat and Manning, 2018). To enable reproduction of our results, we make available our helper scripts and modifications of the semantic parser.1 Our approach to the task does not guarantee a connected graph – something that we did not account for. Thus, on submission day, we did not have an appropriate solution ready to fix our outputs but were able to provide a valid submission due to some functionality that was added to the quick-fix tool provided by the organisers2 to alter the enhanced graph. The solution was designed primarily to make the files pass validation but in doing so, harms F1-score. In a post-competiti"
2020.iwpt-1.24,W18-2501,0,0.0212525,"to the dependency label which results in very high memory requirements for certain languages such as Arabic. Additionally, modelling all enhanced labels in this fashion means that the parser is limited in its ability to predict labels for rare modifiers. An examination of the semantic parser output on the en ewt development set shows that, although the parser often predicts the correct label, it can sometimes predict the wrong label containing a frequent modifier which is not in the sentence, e.g. advcl:if instead of advcl:as. Our semantic parser is built upon the implementation in AllenNLP (Gardner et al., 2018). Due to time constraints, we trained our semantic parsing models on the gold training data released by the organisers as opposed to creating jack-knifed silver data. Hyperparameters are similar to those in Dozat and Manning (2017) as we found the larger network size of Dozat and Manning (2018) to be too restrictive for certain languages with high memory demands. Full hyperparameters of the semantic parser are given in Table 1. We trained for 75 epochs with early-stopping if the development score did not improve after 10 epochs. Memory Considerations We trained our semantic parsing models on t"
2020.iwpt-1.24,D07-1097,0,0.155061,"Missing"
2020.iwpt-1.24,2021.ccl-1.108,0,0.0469355,"Missing"
2020.iwpt-1.24,N18-1202,0,0.00934407,"during development 5 and select for each test language the best overall pipeline according to ELAS on the treebank with the biggest development set for the language.6 2.2 Basic Parsing We choose UDPipe-future (Straka, 2018) for basic parsing and joint prediction of lemmata, POS tags and morphological features so as to not require a separate tagger. We extend UDPipe-future to train multi-treebank models as introduced by (Stymne et al., 2018) with UUParser.7,8 Inspired by Straka et al. (2019), we use two types of external word embeddings with UDPipe-future: ELMo contextualised word embeddings (Peters et al., 2018) and FastText character-n-gram-based word embeddings (Bojanowski et al., 2017).9 For 15 of the 17 test languages, ElmoForManyLangs10 (Che et al., 2018) provides ELMo models. We train FastText on the raw text provided by the CoNLL’17 shared task for the same 15 languages after shuffling sentences. For the Russian FastText model, we kept getting vectors with large component values even after trying a different machine and a different permutation of sentences, prohibiting effective training of the parser. We then used a model trained on 2⁄3 of the Russian data for which component values and parse"
2020.iwpt-1.24,K18-2016,0,0.0139686,"embedded representations together and divide by the number of active properties: • XPOS embedding: The embedding of the (x) word’s language-specific POS tag ei ∈ R50 • head-information embedding: An embedding representing the word’s head information (h) from the basic tree ei ∈ R50 (le) (h) ei = mean(e(h1:t ) ) (8) To encode the basic tree, we then concatenate the head representation and the dependency label embedding: (b) (h) (label) ei = [ei ; ei ] (9) 230 It is worth mentioning that more sophisticated approaches for modelling head distance and direction exist for basic dependency parsing (Qi et al., 2018) but we leave using this approach for enhanced dependency parsing as future work. 2.4.3 Semantic Parser Details Parameter Value Char-BiLSTM layers BiLSTM layers BiLSTM size Char-BiLSTM size Arc MLP size Label MLP size Dropout LSTMs Dropout MLP Dropout embeddings Nonlinear act. (MLP) Edge prediction threshold BERT word-piece embedding Char embedding Tag embedding (all tags) Optimizer Learning rate beta1 beta2 Num. epochs Patience Batch size Training Details Our semantic parser predicts edges in a greedy fashion based on local decisions, i. e. we did not make use of any maximum spanning tree alg"
2020.iwpt-1.24,2020.semeval-1.271,0,0.011725,"it isdt, ru syntagrus and sv talbanken in order to fit a batch into memory. We were also given access to an NVIDIA V100 GPU with 32GB of VRAM which enabled us to process all treebanks except for ar padt without removing long sentences. For ar padt, after removing the longest 75 sentences, the model still required 29GB of VRAM. 2.4.4 BERT Models For the BERT models, in early development runs we compared multilingual BERT (mBERT) with a language-specific BERT model if there was one available in HuggingFace’s (Wolf et al., 2019) models repository.15 We used a language-specific BERT model for ar (Safaya et al., 2020), bg+cs (Arkhipov et al., 2019), en (Devlin et al., 2019), 231 15 https://huggingface.co/models fi (Virtanen et al., 2019), it16 , nl (de Vries et al., 2019), pl17 , ru (Kuratov and Arkhipov, 2019) and sv18 and for the rest of the languages we used mBERT (Devlin et al., 2019). We found that the language-specific variant was always better than mBERT except for pl lfg. For fr sequoia, we tried using the CamemBERT model (Martin et al., 2020). As this model uses RoBERTA (Liu et al., 2019) as opposed to BERT, we installed AllenNLP from the master repository which uses HuggingFace’s AutoTokenizer mo"
2020.iwpt-1.24,K17-3009,0,0.101058,"Missing"
2020.iwpt-1.24,P18-2098,0,0.0574763,"Missing"
2020.iwpt-1.24,2020.acl-main.778,1,0.810852,"g fragments (as opposed to a dummy “0:root” edge) where this information could come from new edges available from lowering the score threshold or from the basic tree. Label Prediction The semantic parser performs competitively despite treating enhanced dependency labels containing lemmas and case information as atomic units. However, a more sophisticated approach should still be tried. Multi-treebank Parsing When randomising the proxy treebank for multi-treebank models, use a different randomisation for each ensemble member. Predict the best proxy treebank for each test sentence or paragraph (Wagner et al., 2020). Table 3 Test set results: subm = submitted, frag fix = using our own fragment connector and quick-fix.pl without connect-to-root, re-run = a re-run with bug fixes, no new models but new model selection guages, the difference in performance is large. For et ewt, which does not have a development set, we suspect that we overfitted our semantic parser on the et ewt training data by allowing it to train for 75 epochs. Table 3 shows test set ELAS obtained on the shared task submission site for (a) our submission fully relying on the organiser’s quick-fix tool to fix issues in the output of our sy"
2020.mwe-1.7,J17-4005,0,0.060998,"Missing"
2020.mwe-1.7,K17-3001,0,0.064727,"Missing"
2020.mwe-1.7,W18-4921,1,0.834176,"Missing"
2020.mwe-1.7,W19-5120,1,0.791767,"re on Irish linguistics and syntax focuses on a theoretical analysis of the language, and any discussion of idiomatic constructions, which are frequently exceptional cases, tends to be brief. (Stenson, 1981; Christian Brothers, 1999; U´ı Dhonnchadha, 2009). Some studies offer more in-depth analysis on particular types of MWEs, such as light-verb constructions (Bloch´ Domhnall´ain and O ´ Baoill, Trojnar, 2009; Bayda, 2015), the idiomatic use of prepositions with verbs (O 1975) and idioms (N´ı Loingsigh, 2016). Others have offered a preliminary categorisation of Irish MWEs (Veselinovi´c, 2006; Walsh et al., 2019). The categorisation carried out in our previous work (Walsh et al., 2019) is largely based on the annotation guidelines developed for the PARSEME shared tasks1 , and as such can be used as a starting point for the development of a comprehensive set of VMWE categories for Irish. 2 Verbal MWE Categories in Irish Given that the focus of PARSEME is on the identification of verbal MWEs, some categories of MWEs considered in our previous work, such as nominal compounds or fixed expressions, are excluded. The categories examined here include two universal categories (verbal idioms and light verb con"
2021.emnlp-main.340,P19-1620,0,0.018061,"ound-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we create from summary data using a number of heuristics, to train a QG model. We provide an overview of the proposed method is shown in Figure 2. We then employ the trained QG model to generate synthetic QA data that is further employed to train an unsupervi"
2021.emnlp-main.340,D18-1549,0,0.0258651,"). Document length and stride length are 364 and 128 respectively, the learning rate is set to 1 × 10−5 . Evaluation metrics for unsupervised QA are Exact Match (EM) and F-1 score. 4.2 Results We use the 20k generated synthetic QA pairs to train a BERT QA model and first validate its performance on the development sets of three benchmark QA datasets based on Wikipedia – SQuAD1.1, Natural Questions and TriviaQA. The results of our method are shown in Tables 1 and 2. The unsupervised baselines we compare with are as follows: 1. Lewis et al. (2019) employ unsupervised neural machine translation (Artetxe et al., 2018) to train a QG model; 4M synthetic QA examples were generated to train a QA model; 2. Li et al. (2020) employ dependency trees to generate questions and employed cited documents as passages. 5 For comparison, we also show the results of some supervised models fine-tuned on the correspond4138 Models S UPERVISED M ODELS BERT-base BERT-large U NSUPERVISED M ODELS Lewis et al. (2019) Li et al. (2020) Our Method NQ TriviaQA NewsQA BioASQ DuoRC EM F-1 EM F-1 EM F-1 EM F-1 EM F-1 Lewis et al. (2019) 19.6 28.5 18.9 27.0 26.0 32.6 Li et al. (2020) 33.6 46.3 30.3 38.7 32.7 41.1 Our Method 37.5 50.1 32.0"
2021.emnlp-main.340,2021.emnlp-main.693,1,0.844914,"Missing"
2021.emnlp-main.340,2020.acl-main.413,0,0.0353091,"2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we"
2021.emnlp-main.340,D19-5801,0,0.0143966,"rain the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraphs sampled from all paragraphs of Wikidumps. We employ the NER toolkits of Spacy5 (Honnibal et al., 2020) and AllenNLP6 (Gardner et al., 2017) to extract entity mentions in the paragraphs. We then remove paragraph, answer pairs that meet one or more of the following three conditions: 1) paragraphs with less than 20 words and more than 480 words; 2) par"
2021.emnlp-main.340,E06-1032,0,0.0223319,"Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach"
2021.emnlp-main.340,N10-1086,0,0.0648963,"Missing"
2021.emnlp-main.340,2020.coling-main.306,0,0.0346789,"tically correct questions, the resulting questions often lack variety and incur high lexical overlap with corresponding declarative sentences. For example, the question generated from the sentence Stephen Hawking announced the party in the morning, with Stephen Hawking as the candidate answer span, could be Who announced the party in the morning?, with a high level of lexi1 Introduction cal overlap between the generated question and the The aim of Question Generation (QG) is the pro- declarative sentence. This is undesirable in a QA duction of meaningful questions given a set of input system (Hong et al., 2020) since the strong lexical passages and corresponding answers, a task with clues in the question would make it a poor test of many applications including dialogue systems as real comprehension. 4134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4134–4148 c November 7–11, 2021. 2021 Association for Computational Linguistics Neural seq2seq models (Sutskever et al., 2014) have come to dominate QG (Du et al., 2017), and are commonly trained with <passage, answer, question> triples taken from human-created QA datasets (Dzendzik et al., 2021) and this l"
2021.emnlp-main.340,P17-1147,0,0.175328,"ish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Quest"
2021.emnlp-main.340,Q19-1026,0,0.0572483,"generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach wh"
2021.emnlp-main.340,2020.acl-main.703,0,0.140325,"n and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG mod"
2021.emnlp-main.340,P19-1484,0,0.320563,"A) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach. Figure 1: Example questions generated via heuristics informed by semantic role labeling of summary sentences using different candidate answer spans well as education (Graesser et al., 2005). Additionally, QG can be applied to Question Answering (QA) for the purpose of data augmentation (Puri et al., 2020) where labeled <passage, answer, question> triples are combined with synthetic <passage, answer, question> triples produced by a QG system to train a QA system, and unsupervised QA (Lewis et al., 2019), in which only the QG system output is used to train the QA system. Early work on QG focused on template or rulebased approaches, employing syntactic knowledge to manipulate constituents in declarative sentences to form interrogatives (Heilman and Smith, 2009, 2010). Although template-based methods are capable of generating linguistically correct questions, the resulting questions often lack variety and incur high lexical overlap with corresponding declarative sentences. For example, the question generated from the sentence Stephen Hawking announced the party in the morning, with Stephen Hawk"
2021.emnlp-main.340,2020.acl-main.600,0,0.369141,"overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we create from summary d"
2021.emnlp-main.340,W04-1013,0,0.0213914,"lly borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Na"
2021.emnlp-main.340,D18-1206,0,0.173048,"(who undergoes the action), and a set of modifier arguments such as a temporal ARGTMP or locative argument ARG-LOC. Questions are then generated from the arguments according to argument type and NER tags, which means that wh-words can be determined jointly. Returning to the example in Figure 1: given the SRL analysis [U2’s lead singer Bono ARG-0] has [had VERB] [emergency spinal surgery ARG-1] [after suffering an injury while preparing for tour Question Generation 1 Data we employ in experiments is news summary data In order to avoid generating trivial questions that originally from BBC News (Narayan et al., 2018) and the are highly similar to corresponding declarative news articles are typically a few hundred words in length. 4136 dates ARG-TMP]., the three questions shown in Figure 1 can be generated based on these three arguments. The pseudocode for our algorithm to generate questions is shown in Algorithm 1. We first obAlgorithm 1: Question Generation Heuristics S = summary srl_f rames = SRL(S) ners = N ER(S) dps = DP (S) examples = [] for frame in srl_frames do root_verb = dpsroot verb = f rameverb if root_verb equal to verb then for arg in frame do wh∗ = identif y_wh_word(arg, ners) base_verb, au"
2021.emnlp-main.340,P02-1040,0,0.110129,"o interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text g"
2021.emnlp-main.340,2020.emnlp-main.468,0,0.0807813,"trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model substantially outperforms previous unsupervised models on three in-domain datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach. Figure 1: Example questions generated via heuristics informed by semantic role labeling of summary sentences using different candidate answer spans well as education (Graesser et al., 2005). Additionally, QG can be applied to Question Answering (QA) for the purpose of data augmentation (Puri et al., 2020) where labeled <passage, answer, question> triples are combined with synthetic <passage, answer, question> triples produced by a QG system to train a QA system, and unsupervised QA (Lewis et al., 2019), in which only the QG system output is used to train the QA system. Early work on QG focused on template or rulebased approaches, employing syntactic knowledge to manipulate constituents in declarative sentences to form interrogatives (Heilman and Smith, 2009, 2010). Although template-based methods are capable of generating linguistically correct questions, the resulting questions often lack var"
2021.emnlp-main.340,D16-1264,0,0.0511569,"ter than 5 tokens (very short questions are likely to have removed too much information) For the dataset in question, this process resulted in a total of 14,830 <passage-answer-question> triples. For training the QG model, we employ implementations of BART (Lewis et al., 2020) from Huggingface (Wolf et al., 2019). The QG model we employ is BART-base. We train the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraph"
2021.emnlp-main.340,J18-3002,0,0.0226063,"Missing"
2021.emnlp-main.340,P18-1156,0,0.0202575,"stion> triples. For training the QG model, we employ implementations of BART (Lewis et al., 2020) from Huggingface (Wolf et al., 2019). The QG model we employ is BART-base. We train the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraphs sampled from all paragraphs of Wikidumps. We employ the NER toolkits of Spacy5 (Honnibal et al., 2020) and AllenNLP6 (Gardner et al., 2017) to extract entity mentions in the"
2021.emnlp-main.340,2020.emnlp-main.439,0,0.0339364,"exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri"
2021.emnlp-main.340,D19-1253,0,0.0207924,"on Generation Traditional approaches to QG mostly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrow"
2021.emnlp-main.340,D18-1427,0,0.0182907,"-of-the-art performance even at low volumes of synthetic training data. 2 Related Work Question Generation Traditional approaches to QG mostly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Ban"
2021.emnlp-main.340,W17-2623,0,0.158077,"o original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics,"
2021.emnlp-main.340,2020.coling-main.228,0,0.0416481,"stly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalgl"
2021.emnlp-main.693,2020.tacl-1.30,0,0.0194999,"93.9 (hit@1) 79.7/75.4 wiki/web 19.7 BLEU4 3 what 3 TweetQA (Xiong et al., 2019) 14k CRW 3 70.0 BLEU1 3 what 3 ShARC (Saeidi et al., 2018) CoQA (Reddy et al., 2019) 32k 127k CRW CRW 3 3] 93.9 88.8 7 3 can what 3 3 TurkQA (Malon and Bai, 2013) WikiReading (Hewlett et al., 2016) Quasar-T (Dhingra et al., 2017) HotpotQA (Yang et al., 2018) QAngaroo WikiHop (Welbl et al., 2018) QAngaroo MedHop (Welbl et al., 2018) QuAC (Choi et al., 2018) DuoRC (Saha et al., 2018) emr QA (Pampari et al., 2018) DROP (Dua et al., 2019) NaturalQuestions (Kwiatkowski et al., 2019) AmazonQA (Gupta et al., 2019b) TyDi (Clark et al., 2020) R3 (Wang et al., 2020b) IIRC (Ferguson et al., 2020) 54k 18.9M 43k 113k 51k 2.5k 98k 86k 456k 97k 323k 570k 11k 60k 13k CRW AG, KG AG CRW CRW, KG CRW, KG CRW CRW expert, AG CRW UG, CRW UG CRW CRW CRW 7 7 ] 3] 3] 3 3] 3 7 3 3] 7 3 7 ] 60.4/60.6 (F1) 96.37(F1) 85.0 81.1(F1) 96.4(F1) 87/76 L/S(F1) 53.5 54.4(F1) 88.4(F1) 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 what what what what who does how who does what how 3 3 3 3 3 3 3 3 ) 3 3 3 3 7 3 Multiple Choice Datasets Stories Wikipedia AG Wikipedia ER Science Books reports, News, Wikipedia, ... ER, Wikipedia, ... ER, WorldTree Medical Books Scripts, CRW Script"
2021.emnlp-main.693,2020.emnlp-main.442,0,0.0449008,"Missing"
2021.emnlp-main.693,2020.lrec-1.677,0,0.0677601,"Missing"
2021.emnlp-main.693,W19-2008,0,0.0308417,"Missing"
2021.emnlp-main.693,D18-1241,0,0.0225156,"xplicit 19 Except MultiRC as there are multiple correct answers and all of them should be found, and CliCR and ReCoRD which use exact match and F1. This is because even though the task is cloze, the answer should be generated (in case of CliCR) or extracted (ReCoRD). 20 MRR and MAP are used only by (Yang et al., 2015) in the WikiQA dataset, as well as precision, recall and F1. (Miller et al., 2016) in the WikiMovies datasets used the accuracy of the top hit (Hits@1). ORdering (METEOR) (Lavie and Agarwal, 2007). MSMarco, NarrativeQA, and TweetQA are generative datasets which use these metrics. Choi et al. (2018) introduced the human equivalence score (HEQ). It measures the percentage of examples where the system F1 matches or exceeds human F1, implying a system’s output is as good as that of an average human. There are two variants: HEQ-Q based on questions and HEQ-D based on dialogues. 4.2 Human Performance Human performance figures have been reported for some datasets – see Table 1. This is useful in two ways. Firstly, it gives some indication of the difficulty of the questions in the dataset. Contrast, for example, the low human performance score reported for the Quasar and CliCR datasets with the"
2021.emnlp-main.693,N19-1423,0,0.010188,"itself should be stored in consistent, easyto-process fashion, ideally with an API provided; any data overlap with existing datasets should be reported; human performance on the dataset should be measured and what it means clearly explained; and finally, if the dataset is for the English language and its design does not differ radically from those surveyed here, it is crucial to explain why this new dataset is needed. For any future datasets, we suggest a move away from Wikipedia given the volume of existing datasets that are based on it and its use in pretrained language models such as BERT (Devlin et al., 2019). As shown by Petroni et al. (2019), its use in MRC dataset and pre-training data brings with it the problem that we cannot always determine whether a system’s ability to answer a question comes from its comprehension of the relevant passage or from the underlying language model. The medical domain is well represented in the collection of English MRC datasets, indicating a demand for understanding of this type of text. Datasets may be required for other domains, such as retail, law and government. Some datasets are designed to test the ability of systems to tell if a question cannot be answere"
2021.emnlp-main.693,2020.findings-emnlp.107,0,0.0352672,"Missing"
2021.emnlp-main.693,P17-1020,0,0.0462755,"Missing"
2021.emnlp-main.693,D18-1134,0,0.0457818,"Missing"
2021.emnlp-main.693,2020.coling-main.570,0,0.0625771,"Missing"
2021.emnlp-main.693,2020.emnlp-main.86,0,0.0580252,"Missing"
2021.emnlp-main.693,R19-1053,0,0.0345397,"Missing"
2021.emnlp-main.693,W18-2605,0,0.0332949,"Missing"
2021.emnlp-main.693,P16-1145,0,0.0748568,"on, e.g. (5) from are divided into Factoid (Who? Where? What? PubMedQuestions (Jin et al., 2019). When?), Non-Factoid (How? Why?), and Yes/No. 8785 Figure 2: Hierarchy of types of question and answer and the relationships between them. → indicates a subtype whereas 99K indicates inclusion. Query The question is formulated to obtain a property of an object. It is similar to a knowledge graph query, and, in order to be answered, part of the passage might involve additional sources such as a knowledge graph, or the dataset may have been created using a knowledge graph, e.g. (9) from WikiReading (Hewlett et al., 2016). (9) P: Cecily Bulstrode (1584-4 August 1609), was a courtier and ... She was the daughter ... Q: sex or gender A: female We put datasets with more than one type of question into a separate Mixed category. 2.3 Passage Type Passages can take the form of a one-document or multi-document passage. They can also be categorised based on the type of reasoning required to answer a question: Simple Evidence where the answer to a question is clearly presented in the passage, e.g. (3) and (6), Multihop Reasoning with questions requiring that several facts from different parts of the passage or different"
2021.emnlp-main.693,D19-1243,0,0.0377261,"Missing"
2021.emnlp-main.693,L18-1433,0,0.0258913,"pts (MovieQA, WikiMovies, DuoRC), and in combination with books (MultiRC and NarrativeQA), 4. medicine: five datasets (CliCR, PubMedQuestions, MedQA, emrQA, QAngaroo MedHop) were created in the medical domain based on clinical reports, medical books, MEDLINE abstracts and PubMed, 5. exams: RACE, RACE-C, and DREAM use data from English as a Foreign Language examinations, ReClor from the Graduate Management Admission Test (GMAT) and The Law School Admission Test (LSAT),5 MedQA from medical exams, and SciQ, ARC, OpenBookQA, and QASC from science exam questions and fact corpora such as WorldTree (Jansen et al., 2018). datasets share not only text sources but the actual samples. SQuAD2.0 extends SQuAD with unanswerable questions. AmazonQA and AmazonYesNo overlap in questions and passages with slightly different processing. SubjQA also uses a subset of the same reviews (Movies, Books, Electronics and Grocery). BoolQ shares 3k questions and passages with the NaturalQuestions dataset. The R3 dataset is fully based on DROP with a focus on reasoning. 3.2 Dataset Creation Rule-based approaches have been used to automatically obtain questions and passages for the MRC task by generating the sentences (e.g. bAbI) o"
2021.emnlp-main.693,Q18-1023,0,0.0398826,"Missing"
2021.emnlp-main.693,D19-1259,0,0.0879873,"eclarative sentence and used in cloze, e.g. (1-2), and quiz questions, e.g. (8) from SearchQA (Dunn et al., 2017). (8) P: Jumbuck (noun) is an Australian English term for sheep, ... Q: Australians call this animal a jumbuck or a monkey A: Sheep Boolean A Yes/No answer is expected, e.g. (4) from the BoolQ dataset (Clark et al., 2019). Some Question is an actual question in the standard datasets which we include here have a third “Can- sense of the word, e.g. (3)-(7). Usually questions not be answered"" or “Maybe"" option, e.g. (5) from are divided into Factoid (Who? Where? What? PubMedQuestions (Jin et al., 2019). When?), Non-Factoid (How? Why?), and Yes/No. 8785 Figure 2: Hierarchy of types of question and answer and the relationships between them. → indicates a subtype whereas 99K indicates inclusion. Query The question is formulated to obtain a property of an object. It is similar to a knowledge graph query, and, in order to be answered, part of the passage might involve additional sources such as a knowledge graph, or the dataset may have been created using a knowledge graph, e.g. (9) from WikiReading (Hewlett et al., 2016). (9) P: Cecily Bulstrode (1584-4 August 1609), was a courtier and ... She"
2021.emnlp-main.693,Q19-1026,0,0.0827377,"Missing"
2021.emnlp-main.693,D19-1249,0,0.0146311,"rts CRW, ... 3] ] ] 3 3] 7 3] 3] 3] 7 7 7 7 3 3 3 3 3 95.3 100 73.3/94.5 87.8 81.8(F1) 91.7 98.0 97.0 98.6 94 63.0 60.0 93 7 7 3 7 7 7 3 7 7 3 7 7 7 7 7 3 7 7 what what what what what what what which how what the what what which - 3 3 3 ) 3 3 3 3 3 7 3 3 3 3 3 3 3 3 BoolQ (Clark et al., 2019) AmazonYesNo (Dzendzik et al., 2019) PubMedQA (Jin et al., 2019) 16k 80k 211k Boolean Questions Wikipedia Reviews PubMed UG, CRW UG CRW 3] 7 3 89 78 3 7 7 is does does 3 ) 3 SQuAD (Rajpurkar et al., 2016) SQuAD2.0 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) BiPaR (Jing et al., 2019) SubjQA (Bjerva et al., 2020) 108k 151k 120k 140k 14.7k 10k Extractive Datasets Wikipedia Wikipedia CNN CRW, AG Novels Reviews CRW CRW CRW J!Archive CRW UG, CRW 3] 3] ] ] 3] ] 86.8(F1) 89.5(F1) 69.4(F1) 57.6(F1) 80.5/91.9(F1) - 3 3 3 3 7 7 what what what this what how 3 3 3 3 3 3 MS MARCO (Nguyen et al., 2016) LAMBADA (Paperno et al., 2016) WikiMovies (Miller et al., 2016; Watanabe et al., 2017) WikiSuggest (Choi et al., 2017) TriviaQA (Joshi et al., 2017) 100k 10k 116k 3.47M 96k Generative Datasets Web documents BookCorpus Wikipedia, KG Wikipedia Wikipedia, Web docs UG, HG CRW, AG CRW, AG, KG"
2021.emnlp-main.693,D17-1082,0,0.0661691,"Missing"
2021.emnlp-main.693,P17-1147,0,0.0914058,"Missing"
2021.emnlp-main.693,W04-1013,0,0.0712959,"Missing"
2021.emnlp-main.693,W13-3213,0,0.0655663,"Missing"
2021.emnlp-main.693,D18-1260,0,0.0413474,"Missing"
2021.emnlp-main.693,D16-1147,0,0.0959349,"olean Questions Wikipedia Reviews PubMed UG, CRW UG CRW 3] 7 3 89 78 3 7 7 is does does 3 ) 3 SQuAD (Rajpurkar et al., 2016) SQuAD2.0 (Rajpurkar et al., 2018) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) BiPaR (Jing et al., 2019) SubjQA (Bjerva et al., 2020) 108k 151k 120k 140k 14.7k 10k Extractive Datasets Wikipedia Wikipedia CNN CRW, AG Novels Reviews CRW CRW CRW J!Archive CRW UG, CRW 3] 3] ] ] 3] ] 86.8(F1) 89.5(F1) 69.4(F1) 57.6(F1) 80.5/91.9(F1) - 3 3 3 3 7 7 what what what this what how 3 3 3 3 3 3 MS MARCO (Nguyen et al., 2016) LAMBADA (Paperno et al., 2016) WikiMovies (Miller et al., 2016; Watanabe et al., 2017) WikiSuggest (Choi et al., 2017) TriviaQA (Joshi et al., 2017) 100k 10k 116k 3.47M 96k Generative Datasets Web documents BookCorpus Wikipedia, KG Wikipedia Wikipedia, Web docs UG, HG CRW, AG CRW, AG, KG CRW, AG Trivia, CRW 3] 7 7 7 3] 7 7 7 7 7 what what which 3 3 3 7 3 NarrativeQA (Koˇciský et al., 2018) 47k HG ] 93.9 (hit@1) 79.7/75.4 wiki/web 19.7 BLEU4 3 what 3 TweetQA (Xiong et al., 2019) 14k CRW 3 70.0 BLEU1 3 what 3 ShARC (Saeidi et al., 2018) CoQA (Reddy et al., 2019) 32k 127k CRW CRW 3 3] 93.9 88.8 7 3 can what 3 3 TurkQA (Malon and Bai, 2013) WikiReading (Hewl"
2021.emnlp-main.693,2021.mrqa-1.4,0,0.0692698,"Missing"
2021.emnlp-main.693,D16-1264,0,0.331608,"lly less across quintiles in 2010 than offspring of nonabstaining families. The difference, however, was not statistically significant between quintiles of the conditional distribution. Actual drinking levels in drinking families were not at all or weakly associated with drinking in offspring. ... Q: Does the familial transmission of drinking patterns persist into young adulthood? A: Maybe Extractive or Span Extractive The answer is a substring of the passage. In other words, the task is to determine the answer character start and end index in the original passage, as shown in (6) from SQuAD (Rajpurkar et al., 2016). (6) P: With Rivera having been a linebacker with the Chicago Bears in Super Bowl XX, .... Q: What team did Rivera play for in Super Bowl XX? A: 46-59: Chicago Bears Generative or Free Form Answer The answer must be generated based on information presented in the passage. Although the answer might be in the text, as illustrated in (7) from NarrativeQA (Koˇciský et al., 2018), no passage index connections are provided. (7) P: ...Mark decides to broadcast his final message as himself. They finally drive up to the crowd of protesting students, .... The police step in and arrest Mark and Nora...."
2021.emnlp-main.693,Q19-1016,0,0.0403046,"Missing"
2021.emnlp-main.693,D13-1020,0,0.292544,"le maker. Place the cup ... in the freezer. ... Question (Q): Choose the best title for the missing blank to correctly complete the recipe. Ingredients, _, Freeze, Enjoying Candidates (AC): (A) Cereal Milk Ice Cream (B) Ingredients (C) Pouring (D) Oven Answer (A): C (2) P: ... intestinal perforation in dengue is very rare and has been reported only in eight patients ... Q: Perforation peritonitis is a _. Possible A: very rare complication of dengue Selective or Multiple Choice (MC) A number of options is given for each question, and the correct one(s) should be selected, e.g. (3) from MCTest (Richardson et al., 2013). (3) P: It was Jessie Bear’s birthday. She ... Q: Who was having a birthday? AC: (A) Jessie Bear (B) no one (C) Lion (D) Tiger A: A We distinguish cloze multiple choice datasets from other multiple choice datasets. The difference is the form of question: in the cloze datasets, the answer is a missing part of the question context and, combined together, they form a grammatically correct sentence, whereas for other multiple choice datasets, the question has no missing words. (4) P: The series is filmed partially in Prince Edward Island as well as locations in ... Q: Is anne with an e filmed on"
2021.emnlp-main.693,D18-1233,0,0.0743574,"2014. ... Q: 2014 S/S is the debut album of a South Korean boy group that was formed by who? A: YG Entertainment choices. (B) Because the shoes felt fantastic. (C) Because they were unsure if the shoes would be good quality. (D) Because the writer thinks the shoes will be very good. A: C 2.4 Conversational MRC We include Conversational or Dialog datasets in a separate category as they involve a unique combination of passage, question, and answer. The full passage is presented as a conversation and the question should be answered based on previous utterances as illustrated in (12) from ShARC (Saeidi et al., 2018), where the scenario is an additional part of the passage unique for each dialog. The question and its answer become a part of the passage for the subsequent question. 2 (12) P: Eligibility. You’ll be able to claim the new State Pension if you’re: a man born on or after 6 April 1951, a woman born on or after 6 April 1953 Scenario: I’m female and I was born in 1966 Q: Am I able to claim the new State Pension? Follow ups: (1) Are you a man born on or after 6 April 1951? – No (2) Are you a woman born on or after 6 April 1953? – Yes A: Yes 3 Datasets All datasets and their properties of interest a"
2021.emnlp-main.693,P18-1156,0,0.279826,"Missing"
2021.emnlp-main.693,I17-4005,0,0.05925,"Missing"
2021.emnlp-main.693,2020.emnlp-main.381,0,0.076693,"Missing"
2021.emnlp-main.693,Q19-1014,0,0.0373751,"6 Q: Am I able to claim the new State Pension? Follow ups: (1) Are you a man born on or after 6 April 1951? – No (2) Are you a woman born on or after 6 April 1953? – Yes A: Yes 3 Datasets All datasets and their properties of interest are listed in Table 1.3 We present the number of questions per dataset (size), the text sources, the method of creation, whether there are a leaderboard and data publicly available, and whether the dataset is solved, i.e. the performance of a MRC system exceeds the reported human performance (also shown). We will discuss each of these aspects. 2 We include DREAM (Sun et al., 2019) in the MultipleChoice category rather than this category because, even though (11) P: I was a little nervous about this today, but its passages are in dialog form, the questions are about the they felt fantastic. I think they’ll be a very dialog but not a part of it. 3 good pair of shoes... Q: Why did the writer Additional properties and statistics are in Table 3 in the feel nervous? AC: (A) None of the above Appendix B. 8786 Dataset Size (questions) Data Source Q/A Source LB Human Performance Solved TM FW PAD CNN/Daily Mail (Hermann et al., 2015) Children BookTest (Hill et al., 2016) Who Did"
2021.emnlp-main.693,N18-1140,0,0.0622822,"2021. 2021 Association for Computational Linguistics Boolean, Extractive, Generative. The relationships between question and answer types are illustrated in Fig. 2. In what follows, we briefly describe each question and answer category, followed by a discussion of passage types, and dialog-based datasets. 2.1 Answer Type Cloze The question is formulated as a sentence with a missing word or phrase which should be inserted into the sentence or should complete the sentence. The answer candidates may be included as in (1) from ReciteQA (Yagcioglu et al., 2018), and may not, as in (2) from CliCR (Šuster and Daelemans, 2018). (1) Passage (P): You will need 3/4 cup of blackberries ... Pour the mixture into cups and insert a popsicle stick in it or pour it in a popsicle maker. Place the cup ... in the freezer. ... Question (Q): Choose the best title for the missing blank to correctly complete the recipe. Ingredients, _, Freeze, Enjoying Candidates (AC): (A) Cereal Milk Ice Cream (B) Ingredients (C) Pouring (D) Oven Answer (A): C (2) P: ... intestinal perforation in dengue is very rare and has been reported only in eight patients ... Q: Perforation peritonitis is a _. Possible A: very rare complication of dengue Sel"
2021.emnlp-main.693,N19-1421,0,0.039302,"Missing"
2021.emnlp-main.693,W17-2623,0,0.161058,"Missing"
2021.emnlp-main.693,Q18-1021,0,0.0521443,"Missing"
2021.emnlp-main.693,D18-1257,0,0.0555887,"Missing"
2021.emnlp-main.693,P19-1496,0,0.0163967,"some datasets used different texts about the same topic as a passage and a 4 Gutenberg: www.gutenberg.org BookCorpus: source of questions. For example, DuoRC takes yknzhu.wixsite.com/mbweb – all links last verified (l.v.) 05/2021 descriptions of the same movie from Wikipedia and 5 GMAT: www.mba.com/exams/gmat/, LSAT: www. IMDb. One description is used as a passage while lsac.org/lsat – all links l.v. 05/2021 6 another is used for creating the questions. NewsQA For example: www.benefits.gov/ www.gov.uk/, www.usa.gov/ – all links l.v. 05/2021 uses only a title and a short news article summary 7 Xiong et al. (2019) selected tweets featured in the news. as a source for questions while the whole text be8 https://www.yelp.com/dataset – l.v. 09/2021 9 comes the passage. Similarly, in NarrativeQA, only www.instructables.com/cooking – l.v. 05/2021 the abstracts of the story were used for question 8788 Figure 3: Question Answering Reading Comprehension datasets overview. creation. For MCScript and MCScript 2.0, questions and passages were created by different sets of crowdworkers given the same script. 3.3 Quantitative Analysis Each dataset’s size is shown in Table 1. About onethird of datasets contain 100k+ q"
2021.emnlp-main.693,D15-1237,0,0.0658655,"Missing"
2021.emnlp-main.693,D18-1259,0,0.12676,"was the daughter ... Q: sex or gender A: female We put datasets with more than one type of question into a separate Mixed category. 2.3 Passage Type Passages can take the form of a one-document or multi-document passage. They can also be categorised based on the type of reasoning required to answer a question: Simple Evidence where the answer to a question is clearly presented in the passage, e.g. (3) and (6), Multihop Reasoning with questions requiring that several facts from different parts of the passage or different documents are combined to obtain the answer, e.g. (10) from the HotpotQA (Yang et al., 2018), and Extended Reasoning where general knowledge or common sense reasoning is required, e.g. (11) from the Cosmos dataset (Huang et al., 2019): (10) P: ...2014 S/S is the debut album of South Korean group WINNER. ... WINNER, is a South Korean boy group formed in 2013 by YG Entertainment and debuted in 2014. ... Q: 2014 S/S is the debut album of a South Korean boy group that was formed by who? A: YG Entertainment choices. (B) Because the shoes felt fantastic. (C) Because they were unsure if the shoes would be good quality. (D) Because the writer thinks the shoes will be very good. A: C 2.4 Co"
2021.emnlp-main.693,D18-1009,0,0.0682735,"Missing"
2021.emnlp-main.745,Q17-1010,0,0.0520408,"nt. 25,148 2,002 11,418 441 10,644 900 11,514 800 Test Tokens Sent. 25,096 2,077 10,448 449 10,330 900 11,955 800 Unlabelled (filtered and sampled) Tokens Sentences 153,878,772 10,275,582 168,359,253 12,199,371 2,537,468 217,950 189,658,820 12,634,409 Table 1: Data statistics (UDPipe sentence splitting and tokenisation for the unlabelled data; right-most columns are for the unlabelled data used in tri-training; for the ELMo and FastText training data, see Section 3.5) beddings are restricted to the labelled training data. • fasttext: This parser is UDPipe-Future with FastText word embeddings (Bojanowski et al., 2017). It is included to be able to tell how much of performance differences of the following two parsers is due to the inclusion of FastText word embeddings. • elmo: This parser combines FastText word embeddings with ELMo (Peters et al., 2018) contextualised word embeddings. This parser is for semi-supervised learning via training word embeddings on unlabelled data and via a combination of word embeddings and tritraining. • mbert: This parser combines FastText word embeddings with multilingual BERT6 (Devlin et al., 2019) contextualised word embeddings, pre-trained on Wikipedia in just over 100 lan"
2021.emnlp-main.745,W06-2920,0,0.24577,"ermore, they apply multi-task training in tri-training and they modify the tri-training algorithm to exclude the manually labelled data from the training data of the third learner. 2.2 Tri-training in Dependency Parsing Tri-training was first applied in dependency parsing by Søgaard and Rishøj (2010), who combine tri-training with stacked learning in multilingual graph-based dependency parsing. 100k sentences per language are automatically labelled using three different stacks of token-level classifiers for arcs and labels, resulting in state-of-the-art performance on the CONLL-X Shared Task (Buchholz and Marsi, 2006). In an uptraining scenario, Weiss et al. (2015) train a neural transition-based dependency parser on the unanimous predictions of two slower, more accurate parsers. This can be seen as tri-training with one iteration and with just one learner’s model as the final model. Similarly, Vinyals et al. (2015) use single iteration, single direction tri-training in constituency parsing where the final model is a neural sequence-to-sequence model with attention, which learns linearised trees. 2.3 Comparing Cross-view Training and Pretraining in NLP unlabelled data. Cross-view training performs best in"
2021.emnlp-main.745,K18-2005,0,0.054547,"Missing"
2021.emnlp-main.745,D18-1217,0,0.119394,"y of the sets and therefore may help keeping the learners’ models diverse. The only previous work we know of that compares pretrained contextualised word embeddings to another semi-supervised learning approach is the work of Bhattacharjee et al. (2020) who compare three BERT models (Devlin et al., 2019) and a semisupervised learning method for neural models in three NLP tasks: detecting the target expressions of opinions, named entity recognition (NER) and slot labelling, i. e. populating attributes of movies given reviews of movies. The semi-supervised learning method is cross-view training (Clark et al., 2018), which adds auxiliary tasks to a neural network that are only given access to restricted views of the input, similarly to the learners in co-training, e. g. a view may be the output of the forward LSTM in a Bi-LSTM. The auxiliary tasks are trained to agree with the prediction of the main classifier on 9459 • d: how much weight is given to data from previous iterations. The current iteration’s data is always used in full. No data from previous iterations is added with d = 0. For d = 1, all available data is concatenated. With d < 1, we apply exponential decay to the dataset weights, e. g. for"
2021.emnlp-main.745,N06-1020,0,0.13891,"ddings are state-of-the-art approaches to combining labelled and unlabelled data in natural language processing tasks taking text as input. A large corpus of unlabelled text is processed once and the resulting model is either fine-tuned for a specific task or its hidden states are used as input for a separate model. In the task of dependency parsing, recent work is no exception to the above. However, earlier, pre-neural work explored many other ways to use unlabelled data to enrich a parsing model. Among these, self-, co- and tritraining had most impact (Charniak, 1997; Steedman et al., 2003; McClosky et al., 2006a,b; Søgaard and Rishøj, 2010; Sagae, 2010). Self-training augments the labelled training data with automatically labelled parse trees predicted by a baseline model in an iterative process: 3. Optionally discard some of the parse trees, e. g. based on parser confidence 4. Optionally oversample the original labelled data to give it more weight 5. Train a new model on the concatenation of manually labelled and automatically labelled data 6. Check a stopping criterion Co-training proceeds similarly to self-training but uses two different learners, each teaching the other learner, i. e. output of"
2021.emnlp-main.745,W09-3035,0,0.0102254,"data and the increased computational costs of parsing long sentences does not seem justified. We also exclude very short sentences as we do not expect them to feature new syntactic patterns and, if they do, to not provide enough context to infer the correct annotation. 5 These numbers do not reflect the removal of sentences that contain one or more tokens that have over 200 bytes in their UTF-8-encoded form and de-duplication performed before parsing unlabelled data. The treebanks selected are Hungarian hu_szeged (Vincze et al., 2010), Uyghur ug_udt (Eli et al., 2016), and Vietnamese vi_vtb (Nguyen et al., 2009). Furthermore, we include English in a simulated low-resource setting using a sample of 1226 sentences (20149 tokens) 9460 Language English Hungarian Uyghur Vietnamese Training Tokens Sent. 20,149 1,226 20,166 910 19,262 1,656 20,285 1,400 Labelled Development Tokens Sent. 25,148 2,002 11,418 441 10,644 900 11,514 800 Test Tokens Sent. 25,096 2,077 10,448 449 10,330 900 11,955 800 Unlabelled (filtered and sampled) Tokens Sentences 153,878,772 10,275,582 168,359,253 12,199,371 2,537,468 217,950 189,658,820 12,634,409 Table 1: Data statistics (UDPipe sentence splitting and tokenisation for the u"
2021.emnlp-main.745,N18-1202,0,0.029904,",634,409 Table 1: Data statistics (UDPipe sentence splitting and tokenisation for the unlabelled data; right-most columns are for the unlabelled data used in tri-training; for the ELMo and FastText training data, see Section 3.5) beddings are restricted to the labelled training data. • fasttext: This parser is UDPipe-Future with FastText word embeddings (Bojanowski et al., 2017). It is included to be able to tell how much of performance differences of the following two parsers is due to the inclusion of FastText word embeddings. • elmo: This parser combines FastText word embeddings with ELMo (Peters et al., 2018) contextualised word embeddings. This parser is for semi-supervised learning via training word embeddings on unlabelled data and via a combination of word embeddings and tritraining. • mbert: This parser combines FastText word embeddings with multilingual BERT6 (Devlin et al., 2019) contextualised word embeddings, pre-trained on Wikipedia in just over 100 languages including three of the four languages of our experiments.7 We include this parser to verify that our findings carry over to a transformer architecture. UDPipe-Future employs external word representations without fine-tuning the resp"
2021.emnlp-main.745,P14-2057,0,0.0254041,"to an ensemble of deci2.1 Tri-training sion trees, i. e. a random forest, and call the method Tri-training has been used to tackle various natuco-forest. As to the risk of deterioration of perral language processing problems including depenformance due to wrong labelling decisions, they dency parsing (Søgaard and Rishøj, 2010), part-ofpoint to previous work showing that the effect can speech tagging (Søgaard, 2010; Ruder and Plank, be compensated with a sufficient amount of data if 2018), chunking (Chen et al., 2006), authorship certain conditions are met, and they include these attribution (Qian et al., 2014) and sentiment analyconditions in the co-forest algorithm. sis (Ruder and Plank, 2018). Approaches differ not Guo and Li (2012) identify issues with the uponly in the type of task (sequence labelling, classifidate criterion of tri-training and with the estimation cation, structured prediction) but also in the flavour of error rates on training data and propose two modof tri-training applied. These differences take the ified methods, one improving performance in 19 form of the method used to introduce diversity into the three learners, the number of tri-training itera- of 33 test cases (eleven"
2021.emnlp-main.745,2020.tacl-1.54,0,0.0615701,"Missing"
2021.emnlp-main.745,P18-1096,0,0.12683,"l the method Tri-training has been used to tackle various natuco-forest. As to the risk of deterioration of perral language processing problems including depenformance due to wrong labelling decisions, they dency parsing (Søgaard and Rishøj, 2010), part-ofpoint to previous work showing that the effect can speech tagging (Søgaard, 2010; Ruder and Plank, be compensated with a sufficient amount of data if 2018), chunking (Chen et al., 2006), authorship certain conditions are met, and they include these attribution (Qian et al., 2014) and sentiment analyconditions in the co-forest algorithm. sis (Ruder and Plank, 2018). Approaches differ not Guo and Li (2012) identify issues with the uponly in the type of task (sequence labelling, classifidate criterion of tri-training and with the estimation cation, structured prediction) but also in the flavour of error rates on training data and propose two modof tri-training applied. These differences take the ified methods, one improving performance in 19 form of the method used to introduce diversity into the three learners, the number of tri-training itera- of 33 test cases (eleven tasks and three learning tions and whether a stopping criterion is employed, algorithm"
2021.emnlp-main.745,W10-2606,0,0.0361345,"labelled and unlabelled data in natural language processing tasks taking text as input. A large corpus of unlabelled text is processed once and the resulting model is either fine-tuned for a specific task or its hidden states are used as input for a separate model. In the task of dependency parsing, recent work is no exception to the above. However, earlier, pre-neural work explored many other ways to use unlabelled data to enrich a parsing model. Among these, self-, co- and tritraining had most impact (Charniak, 1997; Steedman et al., 2003; McClosky et al., 2006a,b; Søgaard and Rishøj, 2010; Sagae, 2010). Self-training augments the labelled training data with automatically labelled parse trees predicted by a baseline model in an iterative process: 3. Optionally discard some of the parse trees, e. g. based on parser confidence 4. Optionally oversample the original labelled data to give it more weight 5. Train a new model on the concatenation of manually labelled and automatically labelled data 6. Check a stopping criterion Co-training proceeds similarly to self-training but uses two different learners, each teaching the other learner, i. e. output of learner A is added to the training data of"
2021.emnlp-main.745,silveira-etal-2014-gold,0,0.0674665,"Missing"
2021.emnlp-main.745,P10-2038,0,0.0234957,"exceeded. A learner’s model is updated using the concatenation of the full set of manually labelled data (before sampling) and the predictions received from teachers. If no predictions are received a learner’s model is not updated. Tri-training stops when no model is updated. Chen et al. (2006) apply tri-training to a sequence labelling task, namely chunking, and discuss sentence-level instance selection as a deviation from vanilla tri-training. They propose a “two agree one disagree method” in which the learner only accepts a prediction from its teachers when it disagrees with the teachers. Søgaard (2010) reinvents this method and coins the term tri-training with disagreement for it. Li and Zhou (2007) extend tri-training to more than three learners and relax the requirement that 2 Background all teachers must agree by using their ensemble prediction. They apply this to an ensemble of deci2.1 Tri-training sion trees, i. e. a random forest, and call the method Tri-training has been used to tackle various natuco-forest. As to the risk of deterioration of perral language processing problems including depenformance due to wrong labelling decisions, they dency parsing (Søgaard and Rishøj, 2010), pa"
2021.emnlp-main.745,C10-1120,0,0.394156,"t approaches to combining labelled and unlabelled data in natural language processing tasks taking text as input. A large corpus of unlabelled text is processed once and the resulting model is either fine-tuned for a specific task or its hidden states are used as input for a separate model. In the task of dependency parsing, recent work is no exception to the above. However, earlier, pre-neural work explored many other ways to use unlabelled data to enrich a parsing model. Among these, self-, co- and tritraining had most impact (Charniak, 1997; Steedman et al., 2003; McClosky et al., 2006a,b; Søgaard and Rishøj, 2010; Sagae, 2010). Self-training augments the labelled training data with automatically labelled parse trees predicted by a baseline model in an iterative process: 3. Optionally discard some of the parse trees, e. g. based on parser confidence 4. Optionally oversample the original labelled data to give it more weight 5. Train a new model on the concatenation of manually labelled and automatically labelled data 6. Check a stopping criterion Co-training proceeds similarly to self-training but uses two different learners, each teaching the other learner, i. e. output of learner A is added to the tra"
2021.emnlp-main.745,E03-1008,0,0.177602,"ontextualised word embeddings are state-of-the-art approaches to combining labelled and unlabelled data in natural language processing tasks taking text as input. A large corpus of unlabelled text is processed once and the resulting model is either fine-tuned for a specific task or its hidden states are used as input for a separate model. In the task of dependency parsing, recent work is no exception to the above. However, earlier, pre-neural work explored many other ways to use unlabelled data to enrich a parsing model. Among these, self-, co- and tritraining had most impact (Charniak, 1997; Steedman et al., 2003; McClosky et al., 2006a,b; Søgaard and Rishøj, 2010; Sagae, 2010). Self-training augments the labelled training data with automatically labelled parse trees predicted by a baseline model in an iterative process: 3. Optionally discard some of the parse trees, e. g. based on parser confidence 4. Optionally oversample the original labelled data to give it more weight 5. Train a new model on the concatenation of manually labelled and automatically labelled data 6. Check a stopping criterion Co-training proceeds similarly to self-training but uses two different learners, each teaching the other le"
2021.iwpt-1.22,2020.iwpt-1.24,1,0.88458,"deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers to obtain token representations, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based system to generate EUD graphs from the predicted trees. 3 Official System Overview This section describes our official system, which is the system we submitted prior to the competition deadline. The architecture of our system is 205 for completeness and to enable possible additional post-process"
2021.iwpt-1.22,2020.iwpt-1.16,0,0.101583,"a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04. 1 1. Stanza (Qi et al., 2020) for sentence segmentation, tokenization and the prediction of all UD features apart from the enhanced graph. 2. A Transformer-based dependency parsing model to predict Enhanced UD graphs. Introduction The IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies (Bouma et al., 2021) is the second task involving the prediction of Enhanced Universal Dependencies (EUD) graphs1 following the 2020 task (Bouma et al., 2020). EUD graphs are an extension of basic UD trees, designed to be more useful in shallow natural language understanding tasks (Schuster and Manning, 2016) and lend themselves more easily to the 3. A post-processor ensuring that every graph is a rooted graph where all nodes are reachable from the notional root token. Our official system placed 6th out of 9 teams with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. In a number of unofficial postevaluation experiments, we make four incremental changes to our pipeline approach: 1. We replace the Stanza pre-processing pipeline with Tranki"
2021.iwpt-1.22,2021.iwpt-1.15,0,0.0243899,"dline experiments which include using Trankit for pre-processing, XLMRoBERTaLARGE , treebank concatenation, and multitask learning between a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04. 1 1. Stanza (Qi et al., 2020) for sentence segmentation, tokenization and the prediction of all UD features apart from the enhanced graph. 2. A Transformer-based dependency parsing model to predict Enhanced UD graphs. Introduction The IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies (Bouma et al., 2021) is the second task involving the prediction of Enhanced Universal Dependencies (EUD) graphs1 following the 2020 task (Bouma et al., 2020). EUD graphs are an extension of basic UD trees, designed to be more useful in shallow natural language understanding tasks (Schuster and Manning, 2016) and lend themselves more easily to the 3. A post-processor ensuring that every graph is a rooted graph where all nodes are reachable from the notional root token. Our official system placed 6th out of 9 teams with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. In a number of unofficial postevalu"
2021.iwpt-1.22,2020.acl-main.747,0,0.100941,"Missing"
2021.iwpt-1.22,2020.iwpt-1.20,0,0.0944655,"Missing"
2021.iwpt-1.22,N19-1423,0,0.0123963,"or English. They use a rule-based system that converts basic UD trees to enhanced UD graphs based on dependency structures identified to require enhancement. Nivre 2 https://github.com/jbrry/ IWPT-2021-shared-task The IWPT 2020 Shared Task on Parsing Enhanced Universal Dependencies The first shared task on parsing Enhanced Universal Dependencies (Bouma et al., 2020) brought renewed attention to the problem of predicting enhanced UD graphs. Ten teams submitted to the task. The winning system (Kanerva et al., 2020) utilized the UDify model (Kondratyuk and Straka, 2019), which uses a BERT model (Devlin et al., 2019) as the encoder with multitask classifiers for POStagging, morphological prediction and dependency parsing built on top. They developed a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers t"
2021.iwpt-1.22,P81-1022,0,0.526309,"Missing"
2021.iwpt-1.22,P18-2077,0,0.0164803,"check for unreachable nodes and the number of unreachable nodes that can be reached from them. We choose the candidate which maximises this number (in the case there are ties, we choose the first node in surface order) and makes it a child of the notional ROOT, i.e. this node becomes an additional root node. System outputs are then validated at level 2 by the UD validator 6 to catch bugs prior to submission. 4 Loss Function For edge prediction, sigmoid cross-entropy loss is used, and for label prediction, as we want to select the label for each chosen edge, softmax cross-entropy loss is used (Dozat and Manning, 2018). We interpolate between the loss given by the edge classifier and the loss given by the label classifier (Dozat and Manning, 2018; Wang et al., 2020) with a constant λ: L = λL(label) + (1 − λ)L(edge) Size 768 1024 300 300 0.35 0.35 0.5 0.10 Experiments In this section, we discuss our official results and then describe post-deadline experiments that improved our submission’s score. Model hyperparameters are listed in Table 1. The choice of XLM-R encoder (Base or Large) determines the hyperparameters of the encoder part of our model. In our official submission, we use XLM-RBase . A dropout valu"
2021.iwpt-1.22,P15-1033,0,0.014271,"ions, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based system to generate EUD graphs from the predicted trees. 3 Official System Overview This section describes our official system, which is the system we submitted prior to the competition deadline. The architecture of our system is 205 for completeness and to enable possible additional post-processing which involves altering enhanced dependency labels with lemma information. Input Sentence Pre-Processing 3.2 Enhanced UD Parsing XLM-Roberta CLS S1 S2 S3 S4 ... Z1 Z2 S5 SEP Z6 Z7 ... Z3 EUD Decoder"
2021.iwpt-1.22,2020.iwpt-1.23,0,0.156782,"g the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based system to generate EUD graphs from the predicted trees. 3 Official System Overview This section describes our official system, which is the system we submitted prior to the competition deadline. The architecture of our system is 205 for completeness and to enable possible additional post-processing which involves altering enhanced dependency labels with lemma information. Input Sentence Pre-Processing 3.2 Enhanced UD Parsing XLM-Roberta CLS S1 S2 S3 S4 ... Z1 Z2 S5 SEP Z6 Z7 ... Z3 EUD Decoder Z4 Z5 Basic Decoder EUD"
2021.iwpt-1.22,2020.iwpt-1.26,0,0.0477423,"Missing"
2021.iwpt-1.22,2020.iwpt-1.19,0,0.0327498,"veloped a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers to obtain token representations, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based s"
2021.iwpt-1.22,P80-1024,0,0.530287,"Missing"
2021.iwpt-1.22,2020.iwpt-1.17,0,0.0453022,"Missing"
2021.iwpt-1.22,Q16-1023,0,0.173874,"res apart from the enhanced dependency graphs and miscellaneous items in CoNLL-U files), we use the Stanza library (Qi et al., 2020) trained on version 2.7 of the UD treebanks for each treebank released as part of the training data for the shared task.4 Note that our parser does not pre-suppose any input features other than the input text but we predict the base features using our pre-processing pipeline For the enhanced UD parser, we use a Transformer encoder in the form of XLM-R (Conneau et al., 2020) with a first-order arc-factored model which utilizes the edge and label scoring method of (Kiperwasser and Goldberg, 2016). In initial experiments, we found this model to perform better than biaffine attention (Dozat and Manning, 2016) for the task of EUD parsing. This finding was also made by (Lindemann et al., 2019) and (Straka and Strakov´a, 2019) for the task of semantic parsing across numerous Graphbanks (Oepen et al., 2019). Straka and Strakov´a (2019) suggest that biaffine attention may be less suitable for predicting whether an edge exists between any pair of nodes using a predefined threshold and is perhaps more suited for dependency parsing, where words are competing with one another to be classified as"
2021.iwpt-1.22,D19-1279,0,0.0175972,"sentations to UD in the form of enhanced UD relations for English. They use a rule-based system that converts basic UD trees to enhanced UD graphs based on dependency structures identified to require enhancement. Nivre 2 https://github.com/jbrry/ IWPT-2021-shared-task The IWPT 2020 Shared Task on Parsing Enhanced Universal Dependencies The first shared task on parsing Enhanced Universal Dependencies (Bouma et al., 2020) brought renewed attention to the problem of predicting enhanced UD graphs. Ten teams submitted to the task. The winning system (Kanerva et al., 2020) utilized the UDify model (Kondratyuk and Straka, 2019), which uses a BERT model (Devlin et al., 2019) as the encoder with multitask classifiers for POStagging, morphological prediction and dependency parsing built on top. They developed a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inferen"
2021.iwpt-1.22,P19-1450,0,0.0178779,"part of the training data for the shared task.4 Note that our parser does not pre-suppose any input features other than the input text but we predict the base features using our pre-processing pipeline For the enhanced UD parser, we use a Transformer encoder in the form of XLM-R (Conneau et al., 2020) with a first-order arc-factored model which utilizes the edge and label scoring method of (Kiperwasser and Goldberg, 2016). In initial experiments, we found this model to perform better than biaffine attention (Dozat and Manning, 2016) for the task of EUD parsing. This finding was also made by (Lindemann et al., 2019) and (Straka and Strakov´a, 2019) for the task of semantic parsing across numerous Graphbanks (Oepen et al., 2019). Straka and Strakov´a (2019) suggest that biaffine attention may be less suitable for predicting whether an edge exists between any pair of nodes using a predefined threshold and is perhaps more suited for dependency parsing, where words are competing with one another to be classified as the head in a softmax layer. The consistency of these findings across EUD and semantic parsing Graphbanks may provide evidence that enhanced UD is closer to semantic dependency parsing than basic"
2021.iwpt-1.22,de-marneffe-etal-2006-generating,0,0.15849,"Missing"
2021.iwpt-1.22,2021.eacl-demos.10,0,0.487601,"Missing"
2021.iwpt-1.22,W18-6012,0,0.350407,"Missing"
2021.iwpt-1.22,K19-2001,0,0.17854,"Missing"
2021.iwpt-1.22,2020.acl-demos.14,0,0.0729078,"dge-scoring and labeling model to predict the enhanced graph. Finally, we run a post-processing script to ensure all of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9 participants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. We carry out additional post-deadline experiments which include using Trankit for pre-processing, XLMRoBERTaLARGE , treebank concatenation, and multitask learning between a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04. 1 1. Stanza (Qi et al., 2020) for sentence segmentation, tokenization and the prediction of all UD features apart from the enhanced graph. 2. A Transformer-based dependency parsing model to predict Enhanced UD graphs. Introduction The IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies (Bouma et al., 2021) is the second task involving the prediction of Enhanced Universal Dependencies (EUD) graphs1 following the 2020 task (Bouma et al., 2020). EUD graphs are an extension of basic UD trees, designed to be more useful in shallow natural language understanding tasks (Schuster and Manning, 2016) and"
2021.iwpt-1.22,L16-1376,0,0.194834,"Missing"
2021.iwpt-1.22,K17-3009,0,0.0628734,"Missing"
2021.iwpt-1.22,K19-2012,0,0.0430282,"Missing"
2021.iwpt-1.22,2020.iwpt-1.22,0,0.243224,"s submitted to the task. The winning system (Kanerva et al., 2020) utilized the UDify model (Kondratyuk and Straka, 2019), which uses a BERT model (Devlin et al., 2019) as the encoder with multitask classifiers for POStagging, morphological prediction and dependency parsing built on top. They developed a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers to obtain token representations, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are"
2021.teachingnlp-1.20,W18-2501,0,0.0693206,"Missing"
2021.teachingnlp-1.20,2020.emnlp-demos.6,0,0.0660655,"Missing"
2021.teachingnlp-1.20,N19-1423,0,0.0336945,"s demonstrated using a series of increasingly specific Python classes. regression, whose default hyper-parameters worked well. The students who reported the highest accuracy scores used a combination of token unigrams, bigrams and trigrams, whereas most students directly compared each n-gram order. The students were free to change the code structure, and indeed some of them took the opportunity to refactor the code to a style that better suited them. 2.2 Notebook Two: Sentiment Polarity with BERT The assignment The aim of this second assignment is to help students feel comfortable using BERT (Devlin et al., 2019). We provide a sample notebook which shows how to fine-tune BERT on the same task and dataset as in the previous assignment. The students are asked to do one of three things: 1. Perform a comparative error analysis of the output of the BERT system(s) and the systems from the previous assignment. The aim here is to get the students thinking about interpreting system output. 2. Using the code in this notebook and online resources as examples, fine-tune BERT on a different task. The aim here is to 1) allow the students to experiment with something other than movie review polarity classification a"
C14-1194,W10-1408,1,0.899818,"Missing"
C14-1194,W12-3108,0,0.18602,"ore a variety of syntactic features extracted from the output of both a hand-crafted broad-coverage grammar/parser and a statistical constituency parser on the WMT 2012 data set. They find that the syntactic features make an important contribution to the overall system. In a framework for combining QE and automatic metrics to evaluate MT output, Specia and Gim´enez (2010) use part-of-speech (POS) tag language model probabilities of the MT output 3-grams as features for QE and features built upon syntactic chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis (2012) builds a series of models for estimating post-editing effort using syntactic features such as parse probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams, CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or disfluent. 2053 In this work, we compare the use of tree kernels and hand-crafted features extracted from the constituency and dependency trees of the source and target sides of a translation pair, as well as comparing the role of source and target syntax. In addition, we conduct a more in-d"
C14-1194,W13-2201,0,0.0307628,"and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All metrics are applied at the segment level.5 We randomly select 4500 parallel segments from the News development data sets released for the WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system, we translate the data set with the following three systems and randomly choose 1500 distinct segments from each: • ACCEPT6 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprietary rule-based system • Bing7 : an online translation system The data set is randomly split into 3000 training, 500 development and 1000 test segments. We use the development set for tuning model parameters and building hand-crafted feature s"
C14-1194,W07-0718,0,0.0379313,"an evaluation exists for just a few language pairs and domains. To the best of our knowledge, the only available English-to-French data set which contains human judgements of translation quality are as follows: • CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commission and also from the health domain. In addition to the domain (and style) difference to newswire (the domain on which our parsers are trained), a major stumbling block which prevents us from using this data set is its small size: only 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All"
C14-1194,W12-3102,0,0.145282,"h has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree fragments of both the constituency and dependency trees of the source and target sentences. Our handcrafted feature set consists of an initial set of 489 constituency and dependency features which are then reduced to a set of 144 with no significant loss in performance. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Pag"
C14-1194,candito-etal-2010-statistical,0,0.0670248,"Missing"
C14-1194,P02-1034,0,0.669376,"-speaking customer base. It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing the syntactic complexity of the source sentence, the grammaticality of the target translation and the syntactic symmetry between the source sentence and its translation. This assumption has been borne out by previous research which has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree fra"
C14-1194,W08-1301,0,0.301447,"Missing"
C14-1194,W11-2107,0,0.0412097,"y 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All metrics are applied at the segment level.5 We randomly select 4500 parallel segments from the News development data sets released for the WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system, we translate the data set with the following three systems and randomly choose 1500 distinct segments from each: • ACCEPT6 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprieta"
C14-1194,2005.eamt-1.15,0,0.0327113,"ey find that the syntactic features make an important contribution to the overall system. In a framework for combining QE and automatic metrics to evaluate MT output, Specia and Gim´enez (2010) use part-of-speech (POS) tag language model probabilities of the MT output 3-grams as features for QE and features built upon syntactic chunks, dependencies and constituent structure to build automatic MT evaluation metrics. Avramidis (2012) builds a series of models for estimating post-editing effort using syntactic features such as parse probabilities and syntactic label frequency. In a similar vein, Gamon et al. (2005) use POS tag trigrams, CFG rules and features derived from a semantic analysis of the MT output to classify it as fluent or disfluent. 2053 In this work, we compare the use of tree kernels and hand-crafted features extracted from the constituency and dependency trees of the source and target sides of a translation pair, as well as comparing the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches and compare the utility of syntactic information extracted from the source side and target sides of the translation. 3 Data While there is evidence to"
C14-1194,2007.mtsummit-papers.31,0,0.0263003,"ctic information extracted from the source side and target sides of the translation. 3 Data While there is evidence to suggest that predicting human evaluation scores is superior to predicting automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation exists for just a few language pairs and domains. To the best of our knowledge, the only available English-to-French data set which contains human judgements of translation quality are as follows: • CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commission and also from the health domain. In addition to the domain (and style) difference to newswire (the domain on which our parsers are trained), a major stumbling block which prevents us from using this data set is its small size: only 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the eval"
C14-1194,W12-3112,0,0.278003,"roduced on a daily basis by a company’s Englishspeaking customers can be translated automatically into French and made available with confidence to the company’s French-speaking customer base. It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing the syntactic complexity of the source sentence, the grammaticality of the target translation and the syntactic symmetry between the source sentence and its translation. This assumption has been borne out by previous research which has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully wit"
C14-1194,I13-1153,1,0.623386,"Missing"
C14-1194,P03-1054,0,0.0123318,"aning that they can be varied by changing the value of a parameter. For example, the non-terminal label is a parameter for the non-terminal-label-count 2056 ∗1 2 ∗3 4 ∗5 ∗6 7 ∗8 9 ∗10 11 ∗12 ∗13 ∗14 ∗1 ∗2 ∗3 ∗4 ∗5 ∗6 ∗7 ∗8 ∗9 ∗10 ∗11 Constituency Label of the root node of the constituency tree Height of the constituency tree which is the number of edges from root node to the farthest terminal (leaf) node Number of nodes in the constituency tree Log probability of the constituency parse assigned by the parser Parseval F1 score of the tree with respect to a tree produced by the Stanford parser (Klein and Manning, 2003) Right hand side of the CFG production rule expanding the root node All non-lexical and lexical CFG production rules expanding the tree nodes Average arity of the non-lexical CFG production rules expanding the constituency tree nodes Counts of each non-terminal label in the tree POS unigrams, 3-grams and 5-grams POS n-gram scores against language models trained on the POS tags of the respective treebanks using the SRILM toolkit (http://www.speech.sri.com/projects/srilm/) with Witten-Bell smoothing Counts of each 12 universal POS tags (Petrov et al., 2012) Location of the first verb in the sent"
C14-1194,W04-3250,0,0.357514,"Missing"
C14-1194,J93-2004,0,0.0461628,"Missing"
C14-1194,E06-1015,0,0.703271,"It is reasonable to assume that syntactic features are useful in QE for MT as a way of capturing the syntactic complexity of the source sentence, the grammaticality of the target translation and the syntactic symmetry between the source sentence and its translation. This assumption has been borne out by previous research which has demonstrated the usefulness of syntactic features for English-Spanish QE (Hardmeier et al., 2012; Rubino et al., 2012). We focus more closely on understanding the role of syntax by comparing the use of hand-crafted features and tree kernels (Collins and Duffy, 2002; Moschitti, 2006), and by teasing apart the contribution of target and source syntax. We find that both tree kernels and manually engineered features produce statistically significantly better results than a strong set of non-syntactic features provided as a baseline by the organisers of the 2012 WMT shared task on QE for MT (Callison-Burch et al., 2012), and that both types of syntactic features can be combined fruitfully with this baseline. Furthermore, we show that it is worthwhile to combine tree kernels with hand-crafted features. Our tree kernel features are the complete set of tree fragments of both the"
C14-1194,P02-1040,0,0.089815,"hich prevents us from using this data set is its small size: only 1135 segments have been evaluated manually. • WMT 2007 (Callison-Burch et al., 2007), which contains only 302 distinct source segments (each with approx. 5 translations) only half of which is in the news domain. • FAUST1 , which is out-of-domain and difficult to apply to our setting as the evaluations and postedits are user feedbacks, often in the form of phrases/fragments. Thus, we instead attempt to predict automatic metric scores as there is a sufficient amount of parallel text for our language pair and domain. We use BLEU2 (Papineni et al., 2002), TER3 (Snover et al., 2006) and METEOR4 (Denkowski and Lavie, 2011), which are the most-widely used MT evaluation metrics. All metrics are applied at the segment level.5 We randomly select 4500 parallel segments from the News development data sets released for the WMT13 translation task (Bojar et al., 2013). In order to be independent of any one translation system, we translate the data set with the following three systems and randomly choose 1500 distinct segments from each: • ACCEPT6 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plu"
C14-1194,P06-1055,0,0.0290645,"ulting tree representation are word forms and dependency relations, omitting POS tag information. An example is shown in Figure 1. A word is a child of its dependency relation to its head. The dependency relation in turn is the child of the head word. This continues until the root of the tree. Based on preliminary experiments on our development set, we use subset tree kernels, where the tree fragments are subtrees rooted at any node in the tree so that no production rule expanding a node in the 8 https://github.com/CNGLdlab/LORG-Release. The Lorg parser is very similar to the Berkeley parser (Petrov et al., 2006), the main difference being its unknown word handling mechanism (Attia et al., 2010). 9 http://svmlight.joachims.org/ 10 http://disi.unitn.it/moschitti/Tree-Kernel.htm 2055 BLEU RMSE r 1-TER RMSE r METEOR RMSE r BM BW 0.1626 0 0.1965 0 0.1657 0 0.1601 0.1766 0.1949 0.1565 0.1625 0.2047 TK BW+TK 0.1581 0.2437 0.1888 0.2774 0.1595 0.2715 0.1570 0.2696 0.1879 0.2939 0.1576 0.3111 HC BW+HC 0.1603 0.1998 0.1913 0.2365 0.1610 0.2516 0.1587 0.2418 0.1899 0.2611 0.1585 0.2964 SyQE 0.1577 0.2535 0.1887 0.2797 0.1594 0.2743 BW+SyQE 0.1568 0.2802 0.1879 0.2937 0.1576 0.3127 Table 1: QE performances measu"
C14-1194,petrov-etal-2012-universal,0,0.0684758,"Missing"
C14-1194,W06-1608,0,0.0180821,"ts the quality of the parse tree (Hypothesis 2). Although this low quality would be expected to affect the dependency trees in the same way since they are directly derived from the consistency trees, this is not the case and it appears that the problematic aspects of the French parses are abstracted away from the dependency trees. To test the first hypothesis, we investigate the role of parser accuracy in QE. For both languages, we substitute the standard parsing models used in all our prior experiments with “lower-accuracy” models trained using only a fraction of the training data (following Quirk and Corston-Oliver (2006)). The English parsing model achieves an F1 of 72.5 and the French an F1 of 66.5, representing drops of approximately 17 points from the original models. The RMSE and Pearson r of the new QE model are 0.1583 and 0.2350 compared to 0.1581 and 0.2437 of the one trained with original trees (see also the third row of Table 1). These results show that the use of these lower-accuracy models has only a minimal and statistically insignificant effect on QE performance, suggesting that intrinsic parser accuracy is not the reason why the target constituency trees are less useful than the source constitue"
C14-1194,quirk-2004-training,0,0.414736,"in performance improvements in the tasks of both QE for MT and parser accuracy prediction The rest of this paper is organised as follows: we discuss related work in using syntax in QE in Section 2, we describe the data in Section 3, and we then go on to describe the QE framework and the systems built in Section 4. We follow this with an investigation of the role of source and target syntax in Section 5 before presenting our heuristics to modify the French constituency trees in Section 6. 2 Related Work Features extracted from parser output have been used before in training QE for MT systems. Quirk (2004) uses a single syntax-based feature which indicates whether a full parse for the source sentence could be found. Hardmeier et al. (2012) employ tree kernels to predict the 1-to-5 post-editing cost of a machine-translated sentence. They use tree kernels derived from syntactic constituency and dependency trees of the source side (English) and only dependency trees of the translation side (Spanish). The tree kernels are used both alone and combined with non-syntactic features. The combined setting ranked second in the 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (201"
C14-1194,D08-1093,0,0.0575215,"Missing"
C14-1194,W12-3117,1,0.90862,"Missing"
C14-1194,2006.amta-papers.25,0,0.0606138,"crafted features extracted from the constituency and dependency trees of the source and target sides of a translation pair, as well as comparing the role of source and target syntax. In addition, we conduct a more in-depth analysis of these approaches and compare the utility of syntactic information extracted from the source side and target sides of the translation. 3 Data While there is evidence to suggest that predicting human evaluation scores is superior to predicting automatic metrics in QE for ME (Quirk, 2004), it has also been shown that human judgements are not necessarily consistent (Snover et al., 2006). A more practical consideration is that human evaluation exists for just a few language pairs and domains. To the best of our knowledge, the only available English-to-French data set which contains human judgements of translation quality are as follows: • CESTA (Hamon et al., 2007), which is selected from the Official Journal of the European Commission and also from the health domain. In addition to the domain (and style) difference to newswire (the domain on which our parsers are trained), a major stumbling block which prevents us from using this data set is its small size: only 1135 segment"
C14-1194,2010.amta-papers.3,0,0.581398,"Missing"
C14-1194,2009.eamt-1.5,0,0.0221037,"or does the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French Treebank does appear to have an adverse effect, and this is significantly improved by simple transformations of the French trees. Finally, we provide further evidence of the usefulness of these transformations by applying them in a separate task – parser accuracy prediction. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made regarding whether to publish a machine translation as is or to re-direct it to a translator, either for postediting or to be translated from scratch. The scores produced by a QE system can also be used to choose between translations, in a system combination framework or in n-best list reranking. The work presented here takes place in the context of a wider study, the aim of which is to develop an English-French QE system so that technical support material that is produced on a daily basis by a company’s Englishspea"
C14-1194,P12-2066,1,0.863412,"Missing"
C14-1194,2003.mtsummit-papers.52,0,0.0150045,"f quality estimation nor does the intrinsic accuracy of the parser itself. However, the relatively flat structure of the French Treebank does appear to have an adverse effect, and this is significantly improved by simple transformations of the French trees. Finally, we provide further evidence of the usefulness of these transformations by applying them in a separate task – parser accuracy prediction. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009). An accurate QE-for-MT system would mean that reliable decisions could be made regarding whether to publish a machine translation as is or to re-direct it to a translator, either for postediting or to be translated from scratch. The scores produced by a QE system can also be used to choose between translations, in a system combination framework or in n-best list reranking. The work presented here takes place in the context of a wider study, the aim of which is to develop an English-French QE system so that technical support material that is produced on a daily basis by a"
C14-1194,N10-1121,0,0.215032,"Missing"
C14-1194,C04-1046,0,\N,Missing
D07-1012,E87-1007,0,0.628084,". Khader et al. (2004) study whether the ParGram English LFG can be used for computer-assisted language learning by adding additional OT marks for ungrammatical constructions observed in a learner corpus. However, the evaluation is preliminary, on only 50 test items. 2.2 N-gram Methods Most shallow approaches to grammar error detection originate from the area of real-word spelling error correction. A real-word spelling error is a spelling or typing error which results in a token which is another valid word of the language in question. The (to our knowledge) oldest work in this area is that of Atwell (1987) who uses a POS tagger to flag POS bigrams that are unlikely according to a reference corpus. While he speculates that the bigram frequency should be compared to how often the same POS bigram is involved in errors in an error corpus, the proposed system uses the raw frequency with an empirically established threshold to decide whether a bigram indicates an error. In the same paper, a completely different approach is presented that uses the same POS tagger to consider spelling variants that have a different POS. In the example sentence I am very hit the POS of the spelling variant hot/JJ is add"
D07-1012,bigert-2004-probabilistic,0,0.214997,"te the nature of the error. Thus, like the creation of a treebank, the creation of a corpus of ungrammatical sentences requires time and linguistic knowledge, and is by no means a trivial task. A corpus of ungrammatical sentences which is large enough to be useful can be created automatically by inserting, deleting or replacing words in grammatical sentences. These transformations should be linguistically realistic and should, therefore, be based on an analysis of naturally produced grammatical errors. Automatically generated error corpora have been used before in natural language processing. Bigert (2004) and Wilcox-O’Hearn et al. (2006), for example, automatically introduce spelling errors into texts. Here, we generate a large error corpus by automatically inserting four different kinds of grammatical errors into BNC sentences. 3.2 Commonly Produced Grammatical Errors Following Foster (2005), we define a sentence to be ungrammatical if all the words in the sentence are well-formed words of the language in question, but the sentence contains one or more error. This error can take the form of a performance slip which can occur due to carelessness or tiredness, or a competence error which occurs"
D07-1012,W06-1620,0,0.0136777,"nstructions can be marked as “dispreferred” and constraints resulting in common ungrammatical constructions can be marked as “ungrammatical”. The use of constraint ordering and marking increases the robustness of the grammar, while maintaining the grammatical / ungrammatical distinction (Frank et al., 1998). The English Resource Grammar (ERG) is a precision HeadDriven Phrase Structure Grammar (HPSG) of English (Copestake and Flickinger, 2000; Pollard and Sag, 1994). Its coverage is not as broad as the XLE English grammar. Baldwin et al. (2004) propose a method to identify gaps in the grammar. Blunsom and Baldwin (2006) report ongoing development. There has been previous work using the ERG and the XLE grammars in the area of computer-assisted language learning. Bender et al. (2004) use a version of the ERG containing mal-rules to parse illformed sentences from the SST corpus of Japanese learner English (Emi et al., 2004). They then use the semantic representations of the ill-formed input to generate well-formed corrections. Khader et al. (2004) study whether the ParGram English LFG can be used for computer-assisted language learning by adding additional OT marks for ungrammatical constructions observed in a"
D07-1012,W02-1503,0,0.076349,"generative grammar sense (Chomsky, 1957), to distinguish grammatical sentences from ungrammatical sentences. This is in contrast to treebank-based grammars which tend to massively overgenerate and do not generally aim to discriminate between the two. In order for our approach to work, the coverage of the precision grammars must be broad enough to parse a large corpus of grammatical sentences, and for this reason, we choose the XLE (Maxwell and Kaplan, 1996), an efficient and robust parsing system for Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982) and the ParGram English grammar (Butt et al., 2002) for our experiments. This system employs robustness techniques, some borrowed from Optimality Theory (OT) (Prince and Smolensky, 1993), to parse extra-grammatical input (Frank et al., 1998), but crucially still distinguishes between optimal and suboptimal solutions. The evaluation corpus is a subset of an ungrammatical version of the British National Corpus (BNC), a 100 million word balanced corpus of British English (Burnard, 2000). This corpus is obtained by automatically inserting grammatical errors into the original BNC sentences based on an analysis of a manually compiled “real” error co"
D07-1012,A00-2019,0,0.499447,"ee features is effective. Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences. The ungrammatical test set is generated automatically by inserting grammatical errors into well-formed BNC sentences. 1 Introduction This paper is concerned with the task of predicting whether a sentence contains a grammatical error. An accurate method for carrying out automatic ∗ Also affiliated to IBM CAS, Dublin. N-gram-based approaches to the problem of error detection have been proposed and implemented in various forms by Atwell(1987), Bigert and Knutsson (2002), and Chodorow and Leacock (2000) amongst others. Existing approaches are hard to compare since they are evaluated on different test sets which vary in size and error density. Furthermore, most of these approaches concentrate on one type of grammatical error only, namely, context-sensitive or realword spelling errors. We implement a vanilla ngram-based approach which is tested on a very large test set containing four different types of error. The idea behind the parser-based approach to error detection is to use a broad-coverage hand-crafted precision grammar to detect ungrammatical sen112 Proceedings of the 2007 Joint Confer"
D07-1012,copestake-flickinger-2000-open,0,0.0269068,"eloped over several years with the XLE platform (Butt et al., 2002). The XLE parser uses OT to resolve ambiguities (Prince and Smolensky, 1993). Grammar constraints resulting in rare constructions can be marked as “dispreferred” and constraints resulting in common ungrammatical constructions can be marked as “ungrammatical”. The use of constraint ordering and marking increases the robustness of the grammar, while maintaining the grammatical / ungrammatical distinction (Frank et al., 1998). The English Resource Grammar (ERG) is a precision HeadDriven Phrase Structure Grammar (HPSG) of English (Copestake and Flickinger, 2000; Pollard and Sag, 1994). Its coverage is not as broad as the XLE English grammar. Baldwin et al. (2004) propose a method to identify gaps in the grammar. Blunsom and Baldwin (2006) report ongoing development. There has been previous work using the ERG and the XLE grammars in the area of computer-assisted language learning. Bender et al. (2004) use a version of the ERG containing mal-rules to parse illformed sentences from the SST corpus of Japanese learner English (Emi et al., 2004). They then use the semantic representations of the ill-formed input to generate well-formed corrections. Khader"
D07-1012,izumi-etal-2004-overview,0,0.0554633,"glish Resource Grammar (ERG) is a precision HeadDriven Phrase Structure Grammar (HPSG) of English (Copestake and Flickinger, 2000; Pollard and Sag, 1994). Its coverage is not as broad as the XLE English grammar. Baldwin et al. (2004) propose a method to identify gaps in the grammar. Blunsom and Baldwin (2006) report ongoing development. There has been previous work using the ERG and the XLE grammars in the area of computer-assisted language learning. Bender et al. (2004) use a version of the ERG containing mal-rules to parse illformed sentences from the SST corpus of Japanese learner English (Emi et al., 2004). They then use the semantic representations of the ill-formed input to generate well-formed corrections. Khader et al. (2004) study whether the ParGram English LFG can be used for computer-assisted language learning by adding additional OT marks for ungrammatical constructions observed in a learner corpus. However, the evaluation is preliminary, on only 50 test items. 2.2 N-gram Methods Most shallow approaches to grammar error detection originate from the area of real-word spelling error correction. A real-word spelling error is a spelling or typing error which results in a token which is ano"
D07-1012,W95-0104,0,0.0951794,"e same POS tagger to consider spelling variants that have a different POS. In the example sentence I am very hit the POS of the spelling variant hot/JJ is added to the list NN-VB-VBD-VBN of possible POS tags of hit. If the POS tagger chooses hit/JJ, the word is flagged and the correction hot is proposed to the user. Unlike most n-gram-based approaches, Atwell’s work aims to detect grammar errors in general and not just real-word spelling errors. However, a complete evaluation is missing. The idea of disambiguating between the elements of confusion sets is related to word sense disambiguation. Golding (1995) builds a classifier based on a rich set of context features. Mays et al. (1991) apply the noisy channel model to the disambiguation problem. For each candidate correction S ′ of the input S the probability P (S ′ )P (S|S ′ ) is calculated and the most likely correction selected. This method is re-evaluated by Wilcox-O’Hearn et al. (2006) on WSJ data with artificial real-word spelling errors. Bigert and Knutsson (2002) extend upon a basic n-gram approach by attempting to match n-grams of low frequency with similar n-grams in order to reduce overflagging. Furthermore, n-grams crossing clause bo"
D13-1116,W10-1408,1,0.908254,"Missing"
D13-1116,A00-2031,0,0.071062,"t overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007). We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels. We are not aware of any other work that leverages the benefits of both types of models. ber of latent annotations increases beyond two. Hall and Klein (2012) are forced to use head binarization when combining their lexicalized and unlexicalized parsers. Dual decomposition allows us to combine models with different binarization schemes. 3 Approximation of PCFG-LAs as Linear Models In this section, we explain how we can use PC"
D13-1116,D12-1133,0,0.0842281,"alse option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left DD Right Bin vs. Func Right DD Right Bin vs. No Func Right DD Left Bin vs."
D13-1116,cer-etal-2010-parsing,0,0.0718255,"Missing"
D13-1116,P12-1024,0,0.0205272,"= (N , T , H, RH , S, p) where: • N is a set of observed non-terminals, among which S is the distinguished initial symbol, PCFG-LA parsing amounts to, given a sequence of words, finding the most probable skeletal tree with this sequence as its yield according to a grammar G: T • T is a set of terminals (words), • H is a set of latent annotations or hidden states, • RH is a set of annotated rules, of the form a[h1 ] → b[h2 ] c[h3 ] for internal rules1 and a[h1 ] → w for lexical rules. Here a, b, c ∈ N are non-terminals, w ∈ T is a terminal and h1 , h2 , h3 ∈ H are latent annotations. Following Cohen et al. (2012) we also define the set of skeletal rules R, in other words, rules without hidden states, of the form a → b c or a → w. • p : RH → R≥0 defines the probabilities associated with rules conditioned on their left-hand side. Like Petrov and Klein (2007), we impose that the initial symbol S has only one latent annotation. In other words, among rules with S on the left-hand side, only those of the form S[0] → γ are in RH . With such a grammar G we can define probabilities over trees in the following way. We will consider two types of trees, annotated trees and skeletal trees. An annotated tree is a s"
D13-1116,W08-1301,0,0.136235,"Missing"
D13-1116,I11-1100,1,0.869851,"Missing"
D13-1116,N06-1024,0,0.193537,"accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007). We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels. We are not aware of any other work that leverages the benefits of both types of models. ber of latent annotations inc"
D13-1116,D12-1105,0,0.0643145,", 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. 2 Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achie"
D13-1116,W99-0623,0,0.0791198,"grammars which encode different function label/binarization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must u"
D13-1116,P08-1067,0,0.212547,"ghtBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left DD Right Bin vs. Func Right DD Right Bin vs. No Fu"
D13-1116,W07-2416,0,0.0754764,"Missing"
D13-1116,D10-1125,0,0.656702,"of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept – or discarded – in order to guide the learning algorithm. These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA) in order to compare ou"
D13-1116,H94-1020,0,0.170918,"In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. 2 Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover t"
D13-1116,D11-1022,0,0.343438,"cause of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept – or discarded – in order to guide the learning algorithm. These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA)"
D13-1116,P05-1010,0,0.288366,"odels (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and we relax the constraints on factorization, as we require only a loose coupling of the models. 2 Grammar Binarization Matsuzaki et al. (2005) compare binarization strategies for PCFG-LA parsing, and conclude that the differences between them have a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trai"
D13-1116,H05-1078,0,0.030401,"ave a minor effect on parsing accuracy as the numRelated Work Parser Model Combination It is well known that improved parsing performance can be achieved by 1159 Function Label Parsing Although function labels have been available in the Penn Treebank (PTB) for almost twenty years (Marcus et al., 1994), they have been to a large extent overlooked in English parsing research — most studies that report parsing results on Section 23 of the Wall Street Journal (WSJ) use parsing models that are trained on a version of the WSJ trees where the function labels have been removed. Notable exceptions are Merlo and Musillo (2005) and Gabbard et al. (2006) who each trained a parsing model on a version of the PTB with function labels intact. Gabbard et al. (2006) found that parsing accuracy was not affected by keeping the function labels. There have also been attempts to use machine learning to recover the function labels post-parsing (Blaheta and Charniak, 2000; Chrupala et al., 2007). We recover function labels as part of the parsing process, and use dual decomposition to combine parsing models with and without function labels. We are not aware of any other work that leverages the benefits of both types of models. ber"
D13-1116,P08-1108,0,0.0283388,"reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we employ PCFG-LA product models with dual decomposition, and"
D13-1116,C10-1094,0,0.0453386,"Missing"
D13-1116,N07-1051,0,0.676186,"yield according to a grammar G: T • T is a set of terminals (words), • H is a set of latent annotations or hidden states, • RH is a set of annotated rules, of the form a[h1 ] → b[h2 ] c[h3 ] for internal rules1 and a[h1 ] → w for lexical rules. Here a, b, c ∈ N are non-terminals, w ∈ T is a terminal and h1 , h2 , h3 ∈ H are latent annotations. Following Cohen et al. (2012) we also define the set of skeletal rules R, in other words, rules without hidden states, of the form a → b c or a → w. • p : RH → R≥0 defines the probabilities associated with rules conditioned on their left-hand side. Like Petrov and Klein (2007), we impose that the initial symbol S has only one latent annotation. In other words, among rules with S on the left-hand side, only those of the form S[0] → γ are in RH . With such a grammar G we can define probabilities over trees in the following way. We will consider two types of trees, annotated trees and skeletal trees. An annotated tree is a sequence of rules from RH , while a skeletal tree is a sequence of skeletal rules from R. An annotated tree TH is obtained by left-most derivation from S[0]. Its probability is: 1 For brevity and without loss of generality, we omit unary and n-ary r"
D13-1116,D10-1069,0,0.0319238,"No Func DD3 DD4 Stanford LAS UAS 92.18 94.32 92.03 94.47 91.86 94.06 91.83 94.29 92.56 94.60 92.01 94.38 92.19 94.36 92.19 94.57 92.77 94.79 92.59 94.62 LAS 89.51 65.31 89.28 65.33 89.81 89.62 89.67 65.44 90.04 89.95 LTH UAS 93.92 92.22 93.75 92.18 94.17 94.05 94.06 92.37 94.33 94.24 p2m UAS 94.2 94.2 93.9 94.1 94.5 94.2 94.2 94.3 94.5 94.4 Table 4: Dependency accuracies on the dev set Dependency-based evaluation of phrase structure parser output has been used in recent years to provide a more rounded view on parser performance and to compare with direct dependency parsers (Cer et al., 2010; Petrov et al., 2010; Nivre et al., 2010; Foster et al., 2011; Petrov and McDonald, 2012). We evaluate our various parsing models on their ability to recover three types of dependencies: basic Stanford dependencies (de Marneffe and Manning, 2008)5 , LTH dependencies (Johansson and Nugues, 5 We used the latest version at the time of writing, i.e. 3.20. 1166 2007)6 and penn2malt dependencies.7 The latter are a simpler version of the LTH dependencies but are still used when reporting unlabeled attachment scores for dependency parsing. The results, shown in Table 4, mirror the constituency evaluation results in that"
D13-1116,N10-1003,0,0.231563,"well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent annotations (PCFG-LA) in order to compare our approach with a strong baseline of high-quality parses. Dual Decomposition is used to mix several systems (between two and four) that may in turn be combinations of grammars, here products of PCFG-LAs (Petrov, 2010). The systems being combined make different choices with regard to i) function labels and ii) grammar binarization. Common sense would suggest that information in the form of function labels – syntactic labels such as SBJ and PRD and semantic labels such as TMP and LOC – might help in obtaining a fine-grained analysis. On the other hand, the independence hypothe1158 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1158–1169, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics sis on which CFGs rely and on w"
D13-1116,D10-1001,0,0.659077,"tion Introduction Because of the large amount of possibly contradictory information contained in a treebank, learning a phrase-structure-based parser implies making several choices regarding the prevalent annotations which have to be kept – or discarded – in order to guide the learning algorithm. These choices, which include whether to keep function labels and empty nodes, how to binarize the trees and whether to alter the granularity of the tagset, are often motivated empirically by parsing performance rather than by the different aspects of the language they may be able to capture. Recently Rush et al. (2010), Martins et al. (2011) and Koo et al. (2010) have shown that Dual Decomposition or Lagrangian Relaxation is an elegant framework for combining different types of NLP tasks or for building parsers from simple slave processes that only check partial well-formedness. Here we propose to follow this idea, but with a different objective. We want to mix different parsers trained on different versions of a treebank each of which makes some annotation choices in order to learn more specific or richer information. We will use state-of-the-art unlexicalized probabilistic contextfree grammars with latent"
D13-1116,D07-1111,0,0.0241372,"UAS) of 94.3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this"
D13-1116,P12-1046,0,0.0554704,"se it in our experiments. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left"
D13-1116,E03-1008,0,0.0183593,"eled attachment score (UAS) of 94.3 on Section 23 of the Wall Street Journal) are obtained by combining three grammars which encode different function label/binarization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall a"
D13-1116,N10-1091,0,0.0184497,"rization decisions. The paper is organized as follows. § 2 reviews related work. § 3 presents approximate PCFG-LA parsers as linear models, while § 4 shows how we can use dual decomposition to derive an algorithm for combining these models. Experimental results are presented and discussed in § 5. leveraging the alternative perspectives provided by several parsing models rather than relying on just one. Examples are parser co-training (Steedman et al., 2003; Sagae and Tsujii, 2007), voting over phrase structure constituents or dependency arcs (Henderson and Brill, 1999; Sagae and Tsujii, 2007; Surdeanu and Manning, 2010), dependency parsing stacking (Nivre and McDonald, 2008), product model PCFG-LA parsing (Petrov, 2010), using dual decomposition to combine dependency and phrase structure models (Rush et al., 2010) or several nonprojective dependency parsing models (Koo et al., 2010; Martins et al., 2011), and using expectation propagation, a related approach to dual decomposition, to combine lexicalized, unlexicalized and PCFG-LA models (Hall and Klein, 2012). In this last example, the models must factor in the same way: in other words, the grammars must use the same binarization scheme. In our study, we emp"
D13-1116,P07-1031,0,0.0314891,"results with state-of-the-art systems (the second half of Table 5). We present parser accuracy results, measured using Parseval F-score and penn2malt UAS, and, for our systems, function label accuracy for labels produced during parsing and after parsing using Funtag. We also carried out statistical significance testing8 on the F-score differences between our various systems on the development and test sets. The results 6 nlp.cs.lth.se/software/treebank_converter. It is recommended that LTH is used with the version of the Penn Treebank which contains the more detailed NP bracketing provided by Vadas and Curran (2007). However, to facilitate comparison with other parsers and dependency schemes, we did not use it in our experiments. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right"
D13-1116,W03-3023,0,0.028621,"r various systems on the development and test sets. The results 6 nlp.cs.lth.se/software/treebank_converter. It is recommended that LTH is used with the version of the Penn Treebank which contains the more detailed NP bracketing provided by Vadas and Curran (2007). However, to facilitate comparison with other parsers and dependency schemes, we did not use it in our experiments. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 –"
D13-1116,D09-1161,0,0.113831,"ts. We ran the converter with the rightBranching=false option to indicate that we are using the version without extra noun phrase bracketing. 7 stp.lingfil.uu.se/˜nivre/research/Penn2Malt. The English head-finding rules of Yamada and Matsumoto (2003), supplied on the website, are employed. 8 We used Dan Bikel’s compare.pl script which uses stratified shuffling to compute significance. We consider a p value < 0.05 to indicate a statistically significant difference. Setting Func Right No Func Right Func Left No Func Left DD Right Bin DD Left Bin DD Func DD No Func DD3 DD4 (Shindo et al., 2012) (Zhang et al., 2009) (Petrov, 2010) (Huang, 2008) (Bohnet and Nivre, 2012) F 91.73 91.76 91.45 91.57 92.16 91.89 92.23 92.09 92.45 92.44 92.4 92.3 91.8 91.7 UAS 93.9 93.8 93.7 93.7 94.1 93.9 94.1 94.0 94.3 94.3 Fun 91.02 – 90.41 – 90.85 90.10 91.02 – 90.86 90.97 Funtag 91.88 91.80 91.80 91.74 91.86 91.85 91.91 91.86 91.98 92.04 93.7 Table 5: Test Set Results: Parseval F-score, penn2malt UAS, Function Label Accuracy and Funtag Function Label Accuracy are shown in Table 6. Comparison Func Right vs. No Func Right Func Left vs. No Func Left Func Right vs. Func Left No Func Right vs. No Func Left DD Right Bin vs. Func"
D15-1157,W09-3821,0,0.114961,"Missing"
D15-1157,W13-1703,0,0.0201367,"asure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared task"
D15-1157,P03-1054,0,0.0826799,"Missing"
D15-1157,W11-2838,0,0.0263959,"of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae"
D15-1157,W12-2006,0,0.0143893,"er, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) develo"
D15-1157,D14-1108,0,0.231583,"English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French technical support forums.1 The phrase"
D15-1157,H94-1020,0,0.246303,"of a PCFG-LA parser on Foreebank, examining the effect of grammatical errors on parsing and experimenting with different training sets. 2 Related Work Other treebanks of English web text include the English Web Treebank (aka Google Web Treebank) (Mott et al., 2012), the small treebank of tweets and football discussion forum posts described in Foster et al. (2011) and the tweet dependency bank described in Kong et al. (2014). The English Web Treebank is a corpus of over 250K words, selected from blogs, newsgroups, emails, local business reviews and Yahoo! answers. It adapts the Penn Treebank (Marcus et al., 1994) and Switchboard (Taylor, 1996) annotation guidelines to address the phenomena specific 2 www.computing.dcu.ie/mt/confidentmt. html 1341 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. to this type of text. The annotation of the 1000sentence treebank described in Foster et al. (2011) is based on the Penn Treebank, whereas the annotation of the treebank described in Kong et al. (2014) is dependency-based. The French Social Media Bank developed by Sed"
D15-1157,P11-1121,0,0.180933,"(2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use NorGram, a Norwegian LexicalFunctional Grammar, to annotate a learner corpus with constituency structure, functional structure and semantic structure, in order to provide a means to search for contexts in which learner errors occur. Nagata et al. (2011) describe an English learner corpus which has been manually annotated with shallow syntax, introducing two new POS tags and two new chunk labels for errors. 3 Building the Foreebank The Foreebank treebank contains 1000 English sentences and 1000 French sentences. The English sentences come from the Symantec Norton technical support user forum. Half of the French sentences come from the French Norton forum and the other half are human translations of sentences from the English forum. Four annotators were involved in the annotation process. Their main task was to correct automatically parsed phr"
D15-1157,N10-1060,1,0.910493,"Missing"
D15-1157,N07-1051,0,0.0161872,"n of the tokens in both data sets. We also calculate the edit distance between each Foreebank sentence and its correction by summing the number of error suffixes and dividing by the maximum of the original and corrected sentence lengths. The average edit distance for the English section of Foreebank is 0.04 and for the French section is 0.03. Despite the existence of some near-to-incomprehensible sentences, the overall error level is very low. 5 Parsing the Foreebank We first evaluate newswire-trained parsers on Foreebank, using our in-house PCFG-LA parser with the max-rule parsing algorithm (Petrov and Klein, 2007) and 6 split-merge cycles. The English model is trained on the entire WSJ and the French model on the entire FTB. For comparison, we parse the WSJ/FTB and so we additionally use models trained only on the training sections. We remove the error suffixes and any Dsuffixed nodes (representing deleted words) from the gold Foreebank trees before evaluation. The results are shown in Table 3. As expected, we see a significant drop for both languages when we move from in-domain data to Foreebank. Compared to parsing the English side of Foreebank, the performance drop for French is relatively smaller:"
D15-1157,P06-1055,0,0.16059,"Missing"
D15-1157,W07-0604,0,0.0355908,", 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et al., 2014). With the exception of HOO 2011, all shared tasks involve error-annotated sentences from learner corpora. Annotation schemes vary but most involve marking the span of an error, classifying the error according to some taxonomy designed with L2 utterances in mind, and sometimes providing the correction or “target hypothesis” (Hirschmann et al., 2007). Regarding syntactic annotation of learner data, Dickinson and Ragheb (2009) propose a dependency annotation scheme based on the CHILDES scheme (Sagae et al., 2007) developed for first language learners. They treat the developing language of learners as an interlanguage, as suggested by D´ıaz-Negrillo et al. (2010), and annotate it as is. They use two POS tags and two dependency labels for error cases: one for the surface form and one for the intended form. Ros´en and De Smedt (2010) criticise the approach of Dickinson and Ragheb (2009) involving “annotating language text as is” arguing that interpretation of the language is required at all annotation levels. They use NorGram, a Norwegian LexicalFunctional Grammar, to annotate a learner corpus with const"
D15-1157,C12-1149,0,0.520114,"om Abstract We present a new treebank of English and French technical forum content which has been annotated for grammatical errors and phrase structure. This double annotation allows us to empirically measure the effect of errors on parsing performance. While it is slightly easier to parse the corrected versions of the forum sentences, the errors are not the main factor in making this kind of text hard to parse. 1 Introduction The last five years has seen a considerable amount of research carried out on web and social media text parsing, with new treebanks being created (Foster et al., 2011; Seddah et al., 2012; Mott et al., 2012; Kong et al., 2014), and new parsing systems developed (Petrov and McDonald, 2012; Kong et al., 2014). In this paper we explore a particular source of user-generated text, namely, posts from technical support forums, which are a popular means for customers to resolve their queries about a product. An accurate parser for this kind of text can be used to inform forum-level questionanswering, machine translation and quality estimation of machine translation. We create a 2000-sentence treebank called Foreebank which contains sentences from the Symantec Norton English and French"
D15-1157,P11-1019,0,0.0216086,"ght into this type of text but it also enables us to directly measure the effect of these errors on parsing accuracy and leaves open the possibility of performing joint parsing and error detection by directly learning the error annotation during parser training. A learner corpus (Granger, 2008) contains utterances produced by language learners and serves as a resource for second language acquisition, computational linguistic and computer-aided language learning research. Examples include the International Corpus of Learner English (Granger, 1993), the Cambridge Learner Corpus (Nicholls, 1999; Yannakoudakis et al., 2011), the NUS Corpus of Learner English (Dahlmeier et al., 2013) and the German Falko corpus (L¨udeling, 2008; Rehbein et al., 2012). We can compare Foreebank to a learner corpus since both contain utterances that are potentially ungrammatical and because in a learner corpus the errors are often annotated, as they are in Foreebank. In the last five years, there have been several shared tasks in grammatical error correction including the Helping Our Own (HOO) shared tasks of 2011 and 2012 (Dale and Kilgariff, 2011; Dale et al., 2012), and the CoNLL 2013 and 2014 shared tasks (Ng et al., 2013; Ng et"
D15-1157,W14-1701,0,\N,Missing
D15-1157,N13-1037,0,\N,Missing
D15-1157,P08-2056,1,\N,Missing
D19-6118,P15-1119,0,0.0291207,"the source side. We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side. 1 Introduction Cross-lingual transfer methods, i. e. methods that transfer knowledge from one or more source languages to a target language, have led to substantial improvements for low-resource dependency parsing (Rosa and Mareˇcek, 2018; Agi´c et al., 2016; Guo et al., 2015; Lynn et al., 2014; McDonald et al., 2011; Hwa et al., 2005) and part-ofspeech (POS) tagging (Plank and Agi´c, 2018). In low-resource scenarios, there may be not enough data for data-driven models to learn how to parse. In cases where no annotated data is available, knowledge is often transferred from annotated data in other languages and when there is only a small amount of annotated data, additional knowl1. using a single polyglot2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target la"
D19-6118,Q16-1022,0,0.0383061,"Missing"
D19-6118,W13-3520,0,0.0861342,"Missing"
D19-6118,D19-1279,0,0.0211594,"rojected from Norwegian Nynorsk. We presented a simple solution to deal with using multiple pre-trained embeddings in a model with a shared vocabulary. It was a rather na¨ıve solution and we want to explore the use of available cross-lingual word embedding tools. Additionally, the use of contextual embeddings such as ELMo (Peters et al., 2018) or multilingual BERT (Devlin et al., 2019) would likely provide better representations, with the effect of contributing better annotations for the target language. Indeed, recent work has already shown promising work in this area (Schuster et al., 2019; Kondratyuk, 2019). In the multi-source projection experiments, our criteria for filtering is based on whether the sentence was present across all target treebanks and more sophisticated approaches could be used to select better training instances as in Plank and Agi´c (2018). More generally, we would like to investigate how our findings might change when the number of source languages or treebanks is changed and how the observations carry over to other languages than Faroese. It would also be interesting to use multiple sources of arc weights in a dense graph as in (Agi´c et al., 2016) but with models induced"
D19-6118,W14-4606,1,0.825678,"We find that polyglot training on the source languages produces an overall trend of better results on the target language but the single best result for the target language is obtained by projecting from monolingual source parsing models and then training multi-treebank POS tagging and parsing models on the target side. 1 Introduction Cross-lingual transfer methods, i. e. methods that transfer knowledge from one or more source languages to a target language, have led to substantial improvements for low-resource dependency parsing (Rosa and Mareˇcek, 2018; Agi´c et al., 2016; Guo et al., 2015; Lynn et al., 2014; McDonald et al., 2011; Hwa et al., 2005) and part-ofspeech (POS) tagging (Plank and Agi´c, 2018). In low-resource scenarios, there may be not enough data for data-driven models to learn how to parse. In cases where no annotated data is available, knowledge is often transferred from annotated data in other languages and when there is only a small amount of annotated data, additional knowl1. using a single polyglot2 parsing model which is trained on the combination of all source languages to create synthetic source treebanks (which are subsequently projected to the target language) 1 In this p"
D19-6118,N19-1423,0,0.0122959,"models to create the source trees that are projected to Faroese and combined in a multitreebank model. The proxy treebank for the multitreebank model is the treebank that also gave best results with single treebank target models, projected from Norwegian Nynorsk. We presented a simple solution to deal with using multiple pre-trained embeddings in a model with a shared vocabulary. It was a rather na¨ıve solution and we want to explore the use of available cross-lingual word embedding tools. Additionally, the use of contextual embeddings such as ELMo (Peters et al., 2018) or multilingual BERT (Devlin et al., 2019) would likely provide better representations, with the effect of contributing better annotations for the target language. Indeed, recent work has already shown promising work in this area (Schuster et al., 2019; Kondratyuk, 2019). In the multi-source projection experiments, our criteria for filtering is based on whether the sentence was present across all target treebanks and more sophisticated approaches could be used to select better training instances as in Plank and Agi´c (2018). More generally, we would like to investigate how our findings might change when the number of source languages"
D19-6118,D11-1006,0,0.0165334,"Missing"
D19-6118,W19-7713,0,0.0163578,"e form of a weighted graph has been further extended by Schlichtkrull and Søgaard (2017) who bypass the need to train the target parser on decoded trees and develop a parser which can be trained directly on weighted graphs. Plank and Agi´c (2018) use annotation projection for POS tagging. They find that choosing high quality training instances results in superior accuracy than randomly sampling a larger training set. To this end, they rank the target sentences by the percentage of words covered by word alignments across all source languages and choose the top k covered sentences for training. Meechan-Maddon and Nivre (2019) carry out an evaluation on cross-lingual parsing for three low-resource languages which are supported by related languages. They include three experiments: first, training a monolingual model on a small number of sentences in the target language; second, training a cross-lingual model on related source languages which is then applied to the target data and lastly, training a multilingual model which includes target data as well as data from the related support languages. They found that training a monolingual model on the target data was always superior to training a cross-lingual model. Inte"
D19-6118,N13-1073,0,0.037796,"r source languages (pivot translation). Norwegian Bokm˚al. There also exists translation systems from Norwegian Bokm˚al to Norwegian Nynorsk, Swedish and Danish in Apertium. As a result, the authors use pivot translation from Norwegian Bokm˚al into the other source languages. The process is illustrated in Fig. 1. For a more thorough description of the machine translation process and for resource creation in general, see the work of Tyers et al. (2018). Word Alignments We use word alignments between the Faroese text and the source translations generated by Tyers et al. (2018) using fast align (Dyer et al., 2013), a word alignment tool based on IBM Model 2.9 Source Treebanks We use the Universal Dependencies v2.2 treebanks (Nivre et al., 2018) to train our source parsing models. This is the version used for the 2018 CoNLL shared task on Parsing Universal Dependencies (Zeman et al., 2018). Source Tagging and Parsing Models In order for our parsers to work well with predicted POS tags, we follow the same steps as used in the 2018 CoNLL shared task for creating training and development treebanks with automatically predicted POS tags (henceforth referred to as silver POS). Since we are required to parse t"
D19-6118,N19-1392,0,0.0906641,"r the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks.1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006), resulting in a treebank for the target language, Faroese in the case of Tyers et al.’s and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Smith et al., 2018; Vilares et al., 2016), we investigate whether additional improvements can be made by: Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches using annotation projection: first, projecting from multiple monolingual source models; second, projecting from a single polyglot model which is trained on the combination of all source languag"
D19-6118,W18-2501,0,0.0557289,"018) use a rule-based machine translation system available in Apertium8 to translate from Faroese to 4 The original authors tokenize and segment the source translations with UDPipe. 5 https://github.com/ftyers/ cross-lingual-parsing 6 https://dumps.wikimedia.org/ 7 https://github.com/attardi/ wikiextractor 8 https://github.com/apertium https://wol.jw.org/ 165 disregard lemmas, language-specific POS (XPOS) and morphological features and only use the word form and universal POS (UPOS) tag as input features to our parsers. We develop our POS tagging and parsing models using the AllenNLP library (Gardner et al., 2018). We use jackknife resampling to predict the UPOS tags for the training treebanks. We split the training treebank into ten parts, train models on nine parts and predict UPOS for the excluded part. The process is repeated until all ten parts are predicted and they are then combined to recreate the treebank with silver POS tags. Only token features are used to predict the UPOS tag.10 Finally, we train a model per source language on the full training data to check performance on the respective development set and to POS tag the source language translations before parsing. We train two variants of"
D19-6118,N18-1202,0,0.0361382,"th the setting that uses monolingual source models to create the source trees that are projected to Faroese and combined in a multitreebank model. The proxy treebank for the multitreebank model is the treebank that also gave best results with single treebank target models, projected from Norwegian Nynorsk. We presented a simple solution to deal with using multiple pre-trained embeddings in a model with a shared vocabulary. It was a rather na¨ıve solution and we want to explore the use of available cross-lingual word embedding tools. Additionally, the use of contextual embeddings such as ELMo (Peters et al., 2018) or multilingual BERT (Devlin et al., 2019) would likely provide better representations, with the effect of contributing better annotations for the target language. Indeed, recent work has already shown promising work in this area (Schuster et al., 2019; Kondratyuk, 2019). In the multi-source projection experiments, our criteria for filtering is based on whether the sentence was present across all target treebanks and more sophisticated approaches could be used to select better training instances as in Plank and Agi´c (2018). More generally, we would like to investigate how our findings might"
D19-6118,D18-1061,0,0.0402121,"Missing"
D19-6118,P16-2067,0,0.0227088,"Danish) Arc voting Weighted Dependency Graphs Decode Synthetic Faroese Treebank Figure 3: Multi-source projection. The source language is listed in brackets. 4.1 Details model are better for three out of four source languages, whereas for no nynorsk, the monolingual model marginally outperforms the polyglot one. These results suggest that the polyglot model will contribute better syntactic annotations for Faroese treebanks. The hyper-parameters of our POS tagging and parsing models are given in Table 1. For POS tagging, we adopt a standard architecture with a word and character-level Bi-LSTM (Plank et al., 2016; Graves and Schmidhuber, 2005) to learn contextsensitive representations of our words. These representations are passed to a multilayer perceptron (MLP) classifier followed by a softmax function to choose a tag with the highest probability. For both the POS tagging and parsing models, we use a word embedding dimension of size 100 and a character embedding dimension of size 64. POS tag embeddings of dimension 50 are included in the parser. We train our Faroese models for fifty epochs. We do not split the synthetic Faroese treebanks into training/development portions though we suspect doing so"
D19-6118,K18-2019,0,0.0304052,"Missing"
D19-6118,N06-2033,0,0.352953,"tion (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi´c, 2016). We build on recent work by Tyers et al. (2018) who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks.1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006), resulting in a treebank for the target language, Faroese in the case of Tyers et al.’s and our experiments. Inspired by recent literature involving multilingual learning (Mulcaire et al., 2019; Smith et al., 2018; Vilares et al., 2016), we investigate whether additional improvements can be made by: Cross-lingual dependency parsing involves transferring syntactic knowledge from one language to another. It is a crucial component for inducing dependency parsers in low-resource scenarios where no training data for a language exists. Using Faroese as the target language, we compare two approaches"
D19-6118,P16-2069,0,0.0364834,"Missing"
D19-6118,E17-1021,0,0.0217393,"ment confidences. They then decode the weight matrices into dependency trees on the target side, which are then used to train a parser. This approach utilizes dense information from multiple source languages, which helps reduce noise from source side predictions but to the best of our knowledge, the source-side parsing models learn information between source languages independently and the cross-lingual interaction only occurs when projecting the edge scores into multi-source weight matrices. The idea of projecting dense information in the form of a weighted graph has been further extended by Schlichtkrull and Søgaard (2017) who bypass the need to train the target parser on decoded trees and develop a parser which can be trained directly on weighted graphs. Plank and Agi´c (2018) use annotation projection for POS tagging. They find that choosing high quality training instances results in superior accuracy than randomly sampling a larger training set. To this end, they rank the target sentences by the percentage of words covered by word alignments across all source languages and choose the top k covered sentences for training. Meechan-Maddon and Nivre (2019) carry out an evaluation on cross-lingual parsing for thr"
D19-6118,H01-1035,0,0.284882,"ains for three of the four source languages (Danish, Norwegian Bokm˚al and Swedish) by adopting a polyglot model. However, for Norwegian Nynorsk, a stronger monolingual model was able to outperform the polyglot approach. When we extended multi-treebank learning to the target side, we experienced additional gains for all cases. Our best result of 71.5 – an absolute improvement of 7.2 points over the result reported by Tyers et al. (2018) – was achieved with multi-treebank target learning over the monolingual projections. 2 The idea of annotation projection using wordalignments originates from (Yarowsky et al., 2001) who used word alignments to transfer information such as POS tags from source to target languages. This method was later used in dependency parsing by Hwa et al. (2005), who project dependencies to a target language and use a set of heuristics to form dependency trees in the target language. A parser is then trained on the projected treebank and evaluated against gold-standard treebanks. Zeman and Resnik (2008) introduced the idea of delexicalized dependency parsing whereby a parser is trained using only POS information and is then applied to a target language. McDonald et al. (2011) perform"
D19-6118,N19-1162,0,0.154193,"rts are predicted and they are then combined to recreate the treebank with silver POS tags. Only token features are used to predict the UPOS tag.10 Finally, we train a model per source language on the full training data to check performance on the respective development set and to POS tag the source language translations before parsing. We train two variants of parsing models. The first is a monolingual biaffine dependency parser (Dozat and Manning, 2017) trained on the individual source treebanks. The second is a polyglot model trained on all source treebanks using the multilingual parser of Schuster et al. (2019), which is the same graph-based biaffine dependency parser, extended to enable parsing with multiple treebanks. We additionally include a treebank embedding (Ammar et al., 2016; Stymne et al., 2018) to the input of the polyglot parser to help the parser differentiate between the source languages. We optimize the model for average development set LAS across the included languages. The process is illustrated in Fig. 2. To ensure that our parser is realistic, we add a pre-trained monolingual word embedding to each monolingual parser, giving a considerable improvement in accuracy on the developmen"
D19-6118,K18-2001,0,0.0844098,"Missing"
D19-6118,K18-2011,0,0.0458636,"Missing"
D19-6118,K17-3009,0,0.0563021,"Missing"
D19-6118,P18-2098,0,0.164811,"Missing"
D19-6118,I08-3008,0,0.0227535,"ted by Tyers et al. (2018) – was achieved with multi-treebank target learning over the monolingual projections. 2 The idea of annotation projection using wordalignments originates from (Yarowsky et al., 2001) who used word alignments to transfer information such as POS tags from source to target languages. This method was later used in dependency parsing by Hwa et al. (2005), who project dependencies to a target language and use a set of heuristics to form dependency trees in the target language. A parser is then trained on the projected treebank and evaluated against gold-standard treebanks. Zeman and Resnik (2008) introduced the idea of delexicalized dependency parsing whereby a parser is trained using only POS information and is then applied to a target language. McDonald et al. (2011) perform delexicalized dependency parsing using direct transfer and show that this approach outperforms unsupervised approaches for grammar induction. Importantly, this approach can be extended to the multi-source case by training on multiple source languages and predicting a target language. In an additional experiment, they combine annotation projection and multi-source transfer. Background Tiedemann and Agi´c (2016) p"
D19-6118,W18-6017,0,0.0566736,"y James Barry and Joachim Wagner and Jennifer Foster ADAPT Centre School of Computing, Dublin City University, Ireland firstname.lastname@adaptcentre.ie Abstract edge can be induced from external corpora such as by learning distributed word representations (Mikolov et al., 2013; Al-Rfou’ et al., 2013) and more recent contextualized variants (Peters et al., 2018; Devlin et al., 2019). This work focuses on dependency parsing for low-resource languages by means of annotation projection (Yarowsky et al., 2001) and synthetic treebank creation (Tiedemann and Agi´c, 2016). We build on recent work by Tyers et al. (2018) who show that in the absence of annotated training data for the target language, a lexicalized treebank can be created by translating a target language corpus into a number of related source languages and parsing the translations using models trained on the source language treebanks.1 These annotations are then projected to the target language using separate word alignments for each source language, combined into a single graph for each sentence and decoded (Sagae and Lavie, 2006), resulting in a treebank for the target language, Faroese in the case of Tyers et al.’s and our experiments. Insp"
D19-6118,D07-1096,0,\N,Missing
E17-1012,P02-1054,0,0.120133,"nswer based on the length-normalized BM25 formula (Robertson et al., 1994); (2) translation features: probability of the question being a translation of the answer computed using IBM’s Model 1 (Brown et al., 1993); (3) features measuring frequency and density of the question terms in the answer, such as the number of non-stop question words in the answer, the number of non-stop nouns, verbs and adjectives in the answer that do not appear in the question and tree kernel values for question and answer syntactic structures; (4) web correlation features based on Corrected Conditional Probability (Magnini et al., 2002) between the question and the answer. They explore these features both separately and in combination and find that the combination of all four feature types is most beneficial for answer reranking models. Jansen et al. (2014) describe answer reranking experiments on YA using a diverse range of lexical, syntactic and discourse features. In particular, they show how discourse information can complement distributed lexical semantic information obtained with a skip-gram model (Mikolov et al., 2013). In this paper we use their features (discussed in detail in Section 4) in combination with http://a"
E17-1012,N16-1154,1,0.690349,"et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent studies present purely neural approaches to answer reranking, with most of them focusing on the task of passage-level answer selection (dos Santos et al., 2016; Tan et al., 2015), rather than answer reranking in CQA websites (Bogdanova and Foster, 2016). These neural approaches aim to obviate the need for any feature engineering and instead focus on developing a neural architecture We show that a neural approach to the task of non-factoid answer reranking can benefit from the inclusion of tried-and-tested handcrafted features. We present a novel neural network architecture based on a combination of recurrent neural networks that are used to encode questions and answers, and a multilayer perceptron. We show how this approach can be combined with additional features, in particular, the discourse features presented by Jansen et al. (2014). Our"
E17-1012,J93-2003,0,0.041083,"are provided in Section 6. 2 Related Work Previous work on supervised non-factoid answer reranking on CQA datasets focused mainly on feature-rich approaches. Surdeanu et al. (2011) show that CQAs such as Yahoo! Answers are a good source of knowledge for non-factoid QA. They employ four types of features in their answer reranking model: (1) similarity features: the similarity between a question and an answer based on the length-normalized BM25 formula (Robertson et al., 1994); (2) translation features: probability of the question being a translation of the answer computed using IBM’s Model 1 (Brown et al., 1993); (3) features measuring frequency and density of the question terms in the answer, such as the number of non-stop question words in the answer, the number of non-stop nouns, verbs and adjectives in the answer that do not appear in the question and tree kernel values for question and answer syntactic structures; (4) web correlation features based on Corrected Conditional Probability (Magnini et al., 2002) between the question and the answer. They explore these features both separately and in combination and find that the combination of all four feature types is most beneficial for answer reran"
E17-1012,N15-1025,0,0.0565412,"data for the task. This is changing, however, with the growing popularity of Community Question Answering (CQA) websites, such as Quora,1 Yahoo! Answers2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent studies present purel"
E17-1012,H05-1116,0,0.0994332,"Missing"
E17-1012,Q15-1015,0,0.137889,"of available labeled data for the task. This is changing, however, with the growing popularity of Community Question Answering (CQA) websites, such as Quora,1 Yahoo! Answers2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent"
E17-1012,P14-1092,0,0.147716,"y due to the absence of available labeled data for the task. This is changing, however, with the growing popularity of Community Question Answering (CQA) websites, such as Quora,1 Yahoo! Answers2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineer"
E17-1012,J11-2003,0,0.198638,"2 and the Stack Exchange3 family of forums. One of the main components of a non-factoid question answering system is the answer reranking module. Given a question, it aims to rearrange the answers in order to boost the community-selected best answer to the top position. Most previous attempts to perform non-factoid answer reranking on CQA data are supervised, feature-based, learning-to-rank approaches (Jansen et al., 2014; Fried et al., 2015; Sharp et al., 2015). These methods represent the candidate answers as meaningful handcrafted features based on syntactic, semantic and discourse parses (Surdeanu et al., 2011; Jansen et al., 2014), web correlation (Surdeanu et al., 2011), and translation probabilities (Fried et al., 2015; Surdeanu et al., 2011). The resulting feature vectors are then passed to a supervised ranking algorithm, such as SVMrank (Joachims, 2006), which ranks the candidates. There has been a recent shift in Natural Language Processing towards neural approaches involving minimal feature engineering. Several recent studies present purely neural approaches to answer reranking, with most of them focusing on the task of passage-level answer selection (dos Santos et al., 2016; Tan et al., 201"
E17-1012,N16-1152,0,0.0433374,"pproach. Fried et al. (2015) improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words using higher-order models. 3 Methods based purely on neural models have gained popularity in various areas of NLP in recent years. The main advantage of these models is that they are often able to achieve state-ofthe-art results while obviating the need for manual feature engineering. These approaches have been successful in the area of question answering. Several studies proposed models based on convolution neural networks (Severyn and Moschitti, 2015; Tymoshenko et al., 2016; Feng et al., 2015) for answer sentence selection for factoid question answering and models based on combinations of convolutional and recurrent neural networks for the task of passage-level non-factoid answer reranking (Tan et al., 2015; dos Santos et al., 2016). Recurrent neural networks and memory networks were successfully applied to the task of reading comprehension (Xiong et al., 2016; Sukhbaatar et al., 2015; Weston et al., 2015). A simple purely neural approach to non-factoid answer reranking in CQAs was proposed by Bogdanova and Foster (2016). The question-answer pairs are represente"
foster-2004-parsing,A00-2018,0,\N,Missing
foster-2004-parsing,J93-2004,0,\N,Missing
foster-2004-parsing,H91-1060,0,\N,Missing
foster-van-genabith-2008-parser,A00-2018,0,\N,Missing
foster-van-genabith-2008-parser,W03-3023,0,\N,Missing
foster-van-genabith-2008-parser,W07-2204,1,\N,Missing
foster-van-genabith-2008-parser,H94-1020,0,\N,Missing
foster-van-genabith-2008-parser,D07-1066,1,\N,Missing
foster-van-genabith-2008-parser,P06-1043,0,\N,Missing
foster-van-genabith-2008-parser,P06-1055,0,\N,Missing
foster-van-genabith-2008-parser,W07-2416,0,\N,Missing
foster-van-genabith-2008-parser,D07-1096,0,\N,Missing
foster-van-genabith-2008-parser,N06-1020,0,\N,Missing
I11-1100,J04-4004,0,0.077163,"Missing"
I11-1100,cer-etal-2010-parsing,0,0.032005,"Missing"
I11-1100,C08-1071,0,0.0212923,"Missing"
I11-1100,N10-1004,0,0.0312252,"iques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a particular document. We intend to experiment with this idea in the context of Web 2.0 parsing. More Parser Evaluation The cross-parser evaluation we have presented in the first half of the paper is by no means exhaustive. For example, to measure the positive effect of discriminative reranking, the first-stage Brown parser should also be included in the evaluation. Other statistical parsers could be evaluated, and it would be interesting to examine the performance of systems which employ hand-crafted grammars and tre"
I11-1100,de-marneffe-etal-2006-generating,0,0.00984564,"Missing"
I11-1100,P05-1012,0,0.0540053,"ers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser p"
I11-1100,N10-1060,1,0.883765,", Joachim Wagner1 , Joseph Le Roux2 Joakim Nivre3 , Deirdre Hogan1 and Josef van Genabith1 1,3 NCLT/CNGL, Dublin City University, Ireland 2 LIF - CNRS UMR 6166, Universit´e Aix-Marseille, France 3 Department of Linguistics and Philology, Uppsala University, Sweden 1 {jfoster,ocetinoglu,jwagner,dhogan,josef}@computing.dcu.ie 2 joseph.le-roux@lif.univ-mrs.fr, 3 joakim.nivre@lingfil.uu.se Abstract social media is particularly challenging since Web 2.0 is not really a domain, consisting, as it does, of utterances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parser"
I11-1100,gimenez-marquez-2004-svmtool,0,0.0101406,"Missing"
I11-1100,P11-2008,0,0.0596175,"Missing"
I11-1100,D09-1087,0,0.0469586,"Missing"
I11-1100,D10-1002,0,0.00568548,"d user-generated content can be used to improve parser accuracy. The reasons for the improvements yielded by the three types of retraining need to be determined.6 The underperformance of the TwitterTrain material in comparison to the FootballTrain material suggests that sample selection involving language and topic identification needs to be applied before parser retraining. We also intend to test the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example,"
I11-1100,P06-1063,1,0.829525,"Missing"
I11-1100,H94-1020,0,0.0475196,"Missing"
I11-1100,P06-1043,0,0.281537,"he discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser performance by modifying the Penn Treebank trees to reflect observed differences between Wall Street Journal (WSJ) sentences and discussion forum sentences (subject ellipsis, non-standard capitalisation, etc.). We approach the problem from a different perspective, by seeing how far we can get by exploiting unlabelled target domain data. We employ three types of parser retraining, namely, 1) the McClosky et al. (2006) self-training protocol, 2) uptraining of Malt using dependency trees produced by a slightly more accurate phrase structure parser (Petrov et al., 2010), and 3) PCFG-LA self-training (Huang We investigate the problem of parsing the noisy language of social media. We evaluate four Wall-Street-Journal-trained statistical parsers (Berkeley, Brown, Malt and MST) on a new dataset containing 1,000 phrase structure trees for sentences from microblogs (tweets) and discussion forum posts. We compare the four parsers on their ability to produce Stanford dependencies for these Web 2.0 sentences. We find"
I11-1100,P08-1108,1,0.789006,"ng. We also intend to test the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a p"
I11-1100,nivre-etal-2006-maltparser,1,0.187118,"rances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a t"
I11-1100,W09-3811,1,0.21318,"ntaining the supplied POS tag for a given word may be removed from the chart during coarse-to-fine pruning.2 3 Baseline Evaluation We first evaluate four widely used WSJ-trained statistical parsers on our new Web 2.0 datasets: Berkeley (Petrov et al., 2006) We train a PCFG-LA using 6 iterations and we run the parser in accurate mode. 3.1 Results Brown (Charniak and Johnson, 2005) We employ this parser in its out-of-the-box settings. Table 2 shows the Parseval f-score and part-ofspeech (POS) tagging accuracy for the Berkeley Malt (Nivre et al., 2006) We use the stacklazy algorithm described in Nivre et al. (2009). We train a linear classifier where the feature interactions are modelled explicitly. 2 In the interest of replicability, detailed information on experimental settings is available at http: //nclt.computing.dcu.ie/publications/ foster_ijcnlp11.html. 895 Parser Berk O Berk P Brown Malt P MST P Berk G Malt G MST G LAS UAS WSJ22 90.5 93.2 89.9 92.5 91.5 94.2 88.0 90.6 88.8 91.3 91.6 93.4 90.0 91.6 90.7 92.3 LAS UAS FootballDev 79.8 84.8 80.1 84.9 82.0 86.3 76.1 81.5 76.4 81.1 83.1 86.4 80.4 83.7 80.8 83.4 LAS UAS TwitterDev 68.9 75.1 68.2 74.2 71.4 77.3 67.3 73.6 68.1 73.8 76.8 80.8 78.3 81.6 78"
I11-1100,C10-1094,1,0.884961,"Missing"
I11-1100,P06-1055,0,0.0152372,"in City University, Ireland 2 LIF - CNRS UMR 6166, Universit´e Aix-Marseille, France 3 Department of Linguistics and Philology, Uppsala University, Sweden 1 {jfoster,ocetinoglu,jwagner,dhogan,josef}@computing.dcu.ie 2 joseph.le-roux@lif.univ-mrs.fr, 3 joakim.nivre@lingfil.uu.se Abstract social media is particularly challenging since Web 2.0 is not really a domain, consisting, as it does, of utterances from a wide variety of speakers from different geographical and social backgrounds. Foster (2010) carried out a pilot study on this topic by investigating the performance of the Berkeley parser (Petrov et al., 2006) on sentences taken from a sports discussion forum. Each misparsed sentence was examined manually and a list of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms"
I11-1100,D10-1069,0,0.0996262,"t of problematic phenomena identified. We extend this work by looking at a larger dataset consisting not only of discussion forum posts but also microblogs or tweets. We extend the parser evaluation to the Brown reranking parser (Charniak and Johnson, 2005), MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2005), and we examine the ability of all four parsers to recover typed Stanford dependencies (de Marneffe et al., 2006). The relative ranking of the four parsers confirms the results of previous Stanford-dependency-based parser evaluations on other datasets (Cer et al., 2010; Petrov et al., 2010). Furthermore, our study shows that the sentences in tweets are harder to parse than the sentences from the discussion forum, despite their shorter length and that a large contributing factor is the high part-of-speech tagging error rate. Foster’s work also included a targeted approach to improving parser performance by modifying the Penn Treebank trees to reflect observed differences between Wall Street Journal (WSJ) sentences and discussion forum sentences (subject ellipsis, non-standard capitalisation, etc.). We approach the problem from a different perspective, by seeing how far we can get"
I11-1100,N10-1003,0,0.00470176,"the combination of PCFG-LA self-training 6 See Foster et al. (2011) for a preliminary analysis of the effect of Malt uptraining on sentences from TwitterDev. and product grammar parsing described in Huang et al. (2010) on our Web 2.0 dataset. Switchboard, as well as Ontonotes 4.0, which has recently been released and which contains syntactically annotated web text (300k words). Combination Parsing Several successful parsing methods have employed multiple parsing models, combined using techniques such as voting, stacking and product models (Henderson and Brill, 2000; Nivre and McDonald, 2008; Petrov, 2010). An ensemble approach to parsing seems particularly appropriate for the linguistic melting pot of Web 2.0, as does the related idea of selecting a model based on characteristics of the input. For example, a preliminary error analysis of the Malt uptraining results shows that coordination cases in TwitterDev are helped more by grammars trained on FootballTrain than on TwitterTrain, suggesting that sentences containing a conjunction should be directed to a FootballTrain grammar. McClosky et al. (2010) use linear regression to determine the correct mix of training material for a particular docum"
I11-1100,W10-2105,0,0.0674734,"Missing"
I11-1100,W09-2205,0,0.0221942,"Missing"
I11-1100,P07-1078,0,0.0355222,"Missing"
I11-1100,W10-2606,0,0.0124848,"Missing"
I11-1100,E03-1008,0,0.0496129,"Missing"
I11-1100,W99-0623,0,\N,Missing
I13-1153,W10-1408,1,0.876079,"Missing"
I13-1153,W12-3108,0,0.649958,"amon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a hand-crafted broad-coverage grammar/parser and a statistical constituency parser. Avramidis (2012) builds models for estimating post-editing effort using syntactic features such as parse probabilities and label frequency. Like Hardmeier et al. (2012), we use tree kernels to represent the output of a parser, but unlike all the previous works, we explicitly examine the role of parser accuracy. There have been some attempts to investigate the role of parser accuracy in downstream applications. Johannson and Nugues (2007) introduce an English constituency-to-dependency converter and find that syntactic dependency trees produced using this converter help semantic role labelling more than depend"
I13-1153,candito-etal-2010-statistical,0,0.157828,"Missing"
I13-1153,P02-1034,0,0.152214,"the larger subsets. 3 WSJ Section 23 and the FTB test set. 4 Quality Estimation • ACCEPT5 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprietary rule-based system • Bing6 : an online translation system The translations are scored at the segment level using segment-level BLEU. The data set is randomly split into 3000 training, 500 development, and 1000 test segments. Model parameters are tuned using the development set. We encode syntactic information using tree kernels (Collins and Duffy, 2002; Moschitti, 2006) because they allow us to use all subtrees of the 4 http://www.statmt.org/wmt13 http://www.accept.unige.ch/Products/ D_4_1_Baseline_MT_systems.pdf 6 http://www.bing.com/translator 1093 5 Training size F1 100 51.06 1K 72.53 English 10K 20K 87.69 88.47 40K 89.55 100 52.85 500 66.51 French 2.5K 78.55 5K 81.85 10K 83.40 Table 1: Parser F1 s for various training set sizes: the sizes in bold are selected for the experiments. parsed sentences as features in an efficient way, thus obviating the need for manual feature engineering. We use SVMLight-TK7 (Moschitti, 2006), a support vect"
I13-1153,W08-1301,0,0.303853,"Missing"
I13-1153,W03-2806,0,0.0804443,"Missing"
I13-1153,2005.eamt-1.15,0,0.166348,", and, in particular, the role of syntactic features. We ask the following: To what extent is QE for MT influenced by the quality of the syntactic information provided to it? Does the accuracy of the parsing model used to provide the syntactic features influence the accuracy of the QE system? We compare two pairs of parsing systems which differ with respect to their Parseval f-scores by around 17 absolute points in Related Work Features extracted from parser output have been used before in QE for MT. Quirk (2004) uses a feature which indicates whether a full parse for a sentence can be found. Gamon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a hand-crafted broad-coverage grammar/parser and a statistical constituency parser. Avramidis (2012) b"
I13-1153,2011.mtsummit-papers.51,0,0.0339492,"ndency parsers in a task-based evaluation involving an answer extraction system but bigger differences be1092 International Joint Conference on Natural Language Processing, pages 1092–1096, Nagoya, Japan, 14-18 October 2013. tween the two parsers when evaluated intrinsically. Quirk and Corston-Oliver (2006) demonstrate that a syntax-enhanced MT system is sensitive to a decrease in parser accuracy obtained by training the parser on smaller training sets. Zhang et al. (2010) experiment with a different syntax-enhanced MT system and do not observe the same behaviour. Both Miyao et al. (2008) and Goto et al. (2011) evaluate a suite of state-of-the-art English statistical parsers on the tasks of protein-pair interaction identification and patent translation respectively, and find only small (albeit sometimes statistically significant) differences between the parsing systems. Our study is closest to that of Quirk and Corston-Oliver (2006) since we are taking one parser and using it to train various models with different training set sizes. However, these models fail to parse about 10 and 2 percent of our English and French data respectively. Since the failed sentences are not necessarily parallel in the s"
I13-1153,W12-3112,0,0.0644446,"tic features influence the accuracy of the QE system? We compare two pairs of parsing systems which differ with respect to their Parseval f-scores by around 17 absolute points in Related Work Features extracted from parser output have been used before in QE for MT. Quirk (2004) uses a feature which indicates whether a full parse for a sentence can be found. Gamon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a hand-crafted broad-coverage grammar/parser and a statistical constituency parser. Avramidis (2012) builds models for estimating post-editing effort using syntactic features such as parse probabilities and label frequency. Like Hardmeier et al. (2012), we use tree kernels to represent the output of a parser, but unlike all the previous works,"
I13-1153,W07-2416,0,0.0939502,"Missing"
I13-1153,W04-3250,0,0.208003,"uency and dependency parse tree kernels of the source and translation sides, exploring first the higheraccuracy parse trees. Table 2 shows the performance of this system (CD-STH ) compared to the system trained on the baseline features (B-WMT). We also compare to another baseline (B-Mean) which always predicts the mean of the segmentlevel BLEU scores of the training instances. We evaluate performance using Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). To test the statistical significance of the performance differences (at p < 0.05), we use paired bootstrap resampling (Koehn, 2004). CD-STH achieves statistically significantly bet7 http://disi.unitn.it/moschitti/ Tree-Kernel.htm 8 A word is a child of its dependency relation to its head and this dependency relation is the child of the head word. ter RMSE and Pearson r than both baselines, which shows the usefulness of tree kernels in QE. We combine CD-STH and B-WMT 9 – this system B+CD-STH performs statistically significantly better than both systems individually, suggesting that tree kernels can also be useful in synergy with non-syntactic features. B-Mean B-WMT CD-STH B+CD-STH RMSE 0.1626 0.1601 0.1581 0.1570 Pearson r"
I13-1153,2005.mtsummit-papers.11,0,0.00494316,"dency trees using the Stanford converter for English (de Marneffe and Manning, 2008) and Const2Dep (Candito et al., 2010) for French. The labels must be removed from the arcs in the dependency trees before they can be used in SVMLight-TK – the nodes in the resulting tree representation are word forms and dependency relations, omitting part-of-speech tags.8 Based on preliminary experiments on our development set, we use subset tree kernels. We build a baseline system with features provided for the WMT 2012 QE shared task (Callison-Burch et al., 2012): we use Europarl v7 and News Commentary v8 (Koehn, 2005) to extract n-gram frequency, language model and word alignment features. This is considered a strong baseline as the system that used just these features was ranked higher than many of the other systems. 5 Experiments and Results We build a QE system using constituency and dependency parse tree kernels of the source and translation sides, exploring first the higheraccuracy parse trees. Table 2 shows the performance of this system (CD-STH ) compared to the system trained on the baseline features (B-WMT). We also compare to another baseline (B-Mean) which always predicts the mean of the segment"
I13-1153,H94-1020,0,0.0268323,"splitting the treebank non-terminals, estimating probabilities for the new rules using Expectation Maximization and merging the less useful splits (Petrov et al., 2006), and which parses using the max-rule parsing algorithm (Petrov and Klein, 2007). In order to investigate the effect of parsing accuracy, we train two parsing models – one “higheraccuracy” model and one “lower-accuracy” model – for each language. We use training set size to control the accuracy. For English, the higheraccuracy model is trained on Sections 2-21 of the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Marcus et al., 1994) (approx 40k sentences). For French, the higher-accuracy model is trained on the training section of the French Treebank (FTB) (Abeill´e et al., 2003) (approx 10k sentences). For the lower-accuracy models, we first select four random subsets of varying sizes from the larger training sets for each language2 and measure the performance of the resulting models on the standard parsing test sets3 using Parseval F1 – see Table 1. All parsing models are trained with 5 split/merge cycles. The worst-performing models for each language are those trained on 100 training sentences. 1 https://github.com/CN"
I13-1153,P08-1006,0,0.0194089,"erences between two dependency parsers in a task-based evaluation involving an answer extraction system but bigger differences be1092 International Joint Conference on Natural Language Processing, pages 1092–1096, Nagoya, Japan, 14-18 October 2013. tween the two parsers when evaluated intrinsically. Quirk and Corston-Oliver (2006) demonstrate that a syntax-enhanced MT system is sensitive to a decrease in parser accuracy obtained by training the parser on smaller training sets. Zhang et al. (2010) experiment with a different syntax-enhanced MT system and do not observe the same behaviour. Both Miyao et al. (2008) and Goto et al. (2011) evaluate a suite of state-of-the-art English statistical parsers on the tasks of protein-pair interaction identification and patent translation respectively, and find only small (albeit sometimes statistically significant) differences between the parsing systems. Our study is closest to that of Quirk and Corston-Oliver (2006) since we are taking one parser and using it to train various models with different training set sizes. However, these models fail to parse about 10 and 2 percent of our English and French data respectively. Since the failed sentences are not necess"
I13-1153,E06-1015,0,0.236135,"J Section 23 and the FTB test set. 4 Quality Estimation • ACCEPT5 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus data from Translators Without Borders (TWB) • SYSTRAN: a proprietary rule-based system • Bing6 : an online translation system The translations are scored at the segment level using segment-level BLEU. The data set is randomly split into 3000 training, 500 development, and 1000 test segments. Model parameters are tuned using the development set. We encode syntactic information using tree kernels (Collins and Duffy, 2002; Moschitti, 2006) because they allow us to use all subtrees of the 4 http://www.statmt.org/wmt13 http://www.accept.unige.ch/Products/ D_4_1_Baseline_MT_systems.pdf 6 http://www.bing.com/translator 1093 5 Training size F1 100 51.06 1K 72.53 English 10K 20K 87.69 88.47 40K 89.55 100 52.85 500 66.51 French 2.5K 78.55 5K 81.85 10K 83.40 Table 1: Parser F1 s for various training set sizes: the sizes in bold are selected for the experiments. parsed sentences as features in an efficient way, thus obviating the need for manual feature engineering. We use SVMLight-TK7 (Moschitti, 2006), a support vector machine (SVM) i"
I13-1153,N07-1051,0,0.0136047,"ed for the WMT13 translation task.4 To remain independent of any one MT system, we translate the dataset with the following three systems, randomly choosing 1500 distinct segments from each: Parsing For parsing we use the LORG parser (Attia et al., 2010)1 which learns a latent-variable probabilistic context-free grammar (PCFG-LA) from a treebank in an iterative process of splitting the treebank non-terminals, estimating probabilities for the new rules using Expectation Maximization and merging the less useful splits (Petrov et al., 2006), and which parses using the max-rule parsing algorithm (Petrov and Klein, 2007). In order to investigate the effect of parsing accuracy, we train two parsing models – one “higheraccuracy” model and one “lower-accuracy” model – for each language. We use training set size to control the accuracy. For English, the higheraccuracy model is trained on Sections 2-21 of the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Marcus et al., 1994) (approx 40k sentences). For French, the higher-accuracy model is trained on the training section of the French Treebank (FTB) (Abeill´e et al., 2003) (approx 10k sentences). For the lower-accuracy models, we first select four r"
I13-1153,P06-1055,0,0.0397184,"omly select 4500 parallel segments from the News development data sets released for the WMT13 translation task.4 To remain independent of any one MT system, we translate the dataset with the following three systems, randomly choosing 1500 distinct segments from each: Parsing For parsing we use the LORG parser (Attia et al., 2010)1 which learns a latent-variable probabilistic context-free grammar (PCFG-LA) from a treebank in an iterative process of splitting the treebank non-terminals, estimating probabilities for the new rules using Expectation Maximization and merging the less useful splits (Petrov et al., 2006), and which parses using the max-rule parsing algorithm (Petrov and Klein, 2007). In order to investigate the effect of parsing accuracy, we train two parsing models – one “higheraccuracy” model and one “lower-accuracy” model – for each language. We use training set size to control the accuracy. For English, the higheraccuracy model is trained on Sections 2-21 of the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (Marcus et al., 1994) (approx 40k sentences). For French, the higher-accuracy model is trained on the training section of the French Treebank (FTB) (Abeill´e et al., 200"
I13-1153,W06-1608,0,0.0641369,"using this converter help semantic role labelling more than dependency trees produced using an older converter despite the fact that trees produced using the older converter have higher attachment scores than trees produced using the new converter. Moll´a and Hutchinson (2003) find significant differences between two dependency parsers in a task-based evaluation involving an answer extraction system but bigger differences be1092 International Joint Conference on Natural Language Processing, pages 1092–1096, Nagoya, Japan, 14-18 October 2013. tween the two parsers when evaluated intrinsically. Quirk and Corston-Oliver (2006) demonstrate that a syntax-enhanced MT system is sensitive to a decrease in parser accuracy obtained by training the parser on smaller training sets. Zhang et al. (2010) experiment with a different syntax-enhanced MT system and do not observe the same behaviour. Both Miyao et al. (2008) and Goto et al. (2011) evaluate a suite of state-of-the-art English statistical parsers on the tasks of protein-pair interaction identification and patent translation respectively, and find only small (albeit sometimes statistically significant) differences between the parsing systems. Our study is closest to t"
I13-1153,quirk-2004-training,0,0.450292,"to represent the translation pairs. The aspect of the task that we focus on is the feature set, and, in particular, the role of syntactic features. We ask the following: To what extent is QE for MT influenced by the quality of the syntactic information provided to it? Does the accuracy of the parsing model used to provide the syntactic features influence the accuracy of the QE system? We compare two pairs of parsing systems which differ with respect to their Parseval f-scores by around 17 absolute points in Related Work Features extracted from parser output have been used before in QE for MT. Quirk (2004) uses a feature which indicates whether a full parse for a sentence can be found. Gamon et al. (2005) use part-of-speech (POS) tag trigrams, CFG production rules and features derived from a dependency analysis of the MT output. Specia and Gim´enez (2010) use POS tag language model probabilities of the MT output 3-grams. Hardmeier et al. (2012) combine syntactic tree kernels with surface features to produce a system which was ranked second in the WMT 2012 shared task on QE for MT (Callison-Burch et al., 2012). Rubino et al. (2012) explore source syntactic features extracted from the output of a"
I13-1153,W12-3117,1,0.844745,"Missing"
I13-1153,2010.amta-papers.3,0,0.73638,"Missing"
I13-1153,2009.eamt-1.5,0,0.0569683,"LEU score of EnglishFrench translations. In order to examine the effect of the accuracy of the parse tree on the accuracy of the quality estimation system, we experiment with various parsing systems which differ substantially with respect to their Parseval f-scores. We find that it makes very little difference which system we choose to use in the quality estimation task – this effect is particularly apparent for source-side English parse trees. 1 2 Introduction Much research has been carried out on quality estimation (QE) for machine translation (MT) (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009; Callison-Burch et al., 2012), with the aim of solving the problem of how to accurately assess the quality of a translation without access to a reference translation. Approaches differ with respect to the nature of the quality scores being estimated (binary, 5-point or real-valued scales; human evaluations versus automatic metrics), the learning algorithms used or the feature set chosen to represent the translation pairs. The aspect of the task that we focus on is the feature set, and, in particular, the role of syntactic features. We ask the following: To what extent is QE for MT influenced"
I13-1153,2003.mtsummit-papers.52,0,0.232368,"ct the segment-level BLEU score of EnglishFrench translations. In order to examine the effect of the accuracy of the parse tree on the accuracy of the quality estimation system, we experiment with various parsing systems which differ substantially with respect to their Parseval f-scores. We find that it makes very little difference which system we choose to use in the quality estimation task – this effect is particularly apparent for source-side English parse trees. 1 2 Introduction Much research has been carried out on quality estimation (QE) for machine translation (MT) (Blatz et al., 2003; Ueffing et al., 2003; Specia et al., 2009; Callison-Burch et al., 2012), with the aim of solving the problem of how to accurately assess the quality of a translation without access to a reference translation. Approaches differ with respect to the nature of the quality scores being estimated (binary, 5-point or real-valued scales; human evaluations versus automatic metrics), the learning algorithms used or the feature set chosen to represent the translation pairs. The aspect of the task that we focus on is the feature set, and, in particular, the role of syntactic features. We ask the following: To what extent is"
I13-1153,W12-3102,0,\N,Missing
I13-1153,C04-1046,0,\N,Missing
I13-1166,P07-1038,0,0.0194114,"nguage Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013) or averaged human evaluations (Specia et al., 2009; Callison-Burch et al., 2012). As regards the learning algorithms used, several have been tried, with support vector machine and decision tree learning proving popular (Callison-Burch et al., 2012). 3 Dataset We use the datase"
I13-1166,2012.eamt-1.41,1,0.89917,"Missing"
I13-1166,P06-4018,0,0.00750392,"the following information (in the form of one feature for the source segment, one for the target segment and, where appropriate, one for the ratio between the two): - 11 Case Features the number of upper and lowercased words, the number of fully uppercased words, the number of mixed-case words and whether or not the segment begins with an uppercase letter. - 13 Punctuation Features the ratio between punctuation characters and other characters, the number of words containing a full stop, the number of sentences produced by an off-the-shelf sentence splitter for each segment (included in NLTK (Bird, 2006)), whether or not the segment contains a dash, an ellipsis, and whether or not the segment ends with a punctuation symbol. - 9 Acronym and Emotion Features the number of web and IT-domain acronyms and the number of emoticons. - 4 Linguistic Features the number of spelling mistakes flagged by the spellchecker L ANGUAGE T OOL5 and whether or not the segment starts with a verb (indicating imperatives or questions). 5 Experiments Classification models are built using the C-SVC implementation in L IB SVM (Chang and Lin, 2011) with a Radial Basis Function (RBF) kernel. Optimal hyper-parameters C and"
I13-1166,P07-2045,0,0.00300313,"ry scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013) or averaged human evaluations (Specia et al., 2009; Callison-Burch et al., 2012). As regards the learning algorithms used, several have been tried, with support vector machine and decision tree learning proving popular (Callison-Burch et al., 2012). 3 Dataset We use the dataset presented in Roturier and Bensadoun (2011), which was obtained by machinetranslating 694 English segments, harvested from the Symantec English Norton forum1 , into French using three different translators (M OSES (Koehn et al., 2007), M ICROSOFT2 (MS) and S YSTRAN). The translations were then evaluated in terms of comprehensibility (1 to 5 scale) and fidelity (binary scale) by human annotators. The source side of this data set represents user-generated content – see Banerjee et al. (2012) for a detailed description of the characteristics of this type of data and see Table 2 for some examples. For each of the three translators, we extract 500 segments from this dataset to build our training sets. The remaining 194 segments per translator are used as test sets. The distribution of the comprehensibility and fidelity classes"
I13-1166,P00-1056,0,0.181157,"e repeat this feature extraction process using four LMs built on the Symantec Translation Memories (TMs)3 and four LMs built on the monolingual Symantec forum data4 . - 15 MT Output Language Model Features A M OSES English-French PB-SMT system is trained on the Symantec TMs and the same target LM used to extract the baseline LM features. The English side of the Symantec Norton monolingual forum data is translated by this system and the output is used to build a 5-gram LM. Target features are then extracted in a similar way as the standard LM features. - 4 Word Alignment Features Using GIZA++ (Och and Ney, 2000) and the Symantec TMs, word alignment probabilities are extracted from the source and target segments. - 78 n-gram Frequency Features The number of source and target segments unigrams seen in a reference corpus plus the percentage of n-grams in frequency quartiles (n ∈ [1; 5]). The reference corpus is the same corpus used to extract the LM features. 3 ∼ 1.6M aligned segments in English and French. ∼ 3M segments in English and ∼ 40k segments in French. 4 1168 so loe and behold I get a Internet Worm Protection Signature File Version: 20090511.001. on 5/20/09 in the afternoon Start NIS 2009 > In"
I13-1166,quirk-2004-training,0,0.247742,"lass, or continuous scores. First applied at the word level (Gandrabur and Foster, 2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores"
I13-1166,2009.eamt-1.15,0,0.0175197,"tors. Our experiments show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneficial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 1 Introduction Quality Estimation (QE) involves judging the correctness of a system output given an input without any output reference. Substantial progress has been made on QE for Machine Translation (MT), but research has been mainly conducted on wellformed, edited text (Blatz et al., 2003; Ueffing et al., 2003; Raybaud et al., 2009; Specia et al., 2009). We turn our attention to estimating the quality of user-generated content (UGC) translation – a particularly relevant use of QE since the translation process is likely to be affected by the noisy nature of the input, particularly if the MT system is trained on well-formed text. The source language content is collected from an IT Web forum in English and translated into French by three automatic systems. For each MT system, the produced translation is manually evaluated following two criteria: the translation comprehensibility and fidelity. We evaluate several feature se"
I13-1166,W12-3117,1,0.878731,"or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013) or averaged human evaluations (Specia et al., 2009; Callison-Burch et al., 2012). As regards the learning algorithms used, several have been tried, with support vector machine and decision tree learning proving popular (Callison-Burch et al., 2012). 3 Dataset We use the dataset presented in Roturier and Bensadoun (2011), which was obtained by machinetranslating 694 English segments, harvested from the Symantec English Norton for"
I13-1166,2013.mtsummit-posters.13,1,0.896472,"Missing"
I13-1166,2010.amta-papers.3,0,0.26557,"2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic metrics (Bojar et al., 2013)"
I13-1166,2005.eamt-1.15,0,0.0314964,"andrabur and Foster, 2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Specia and Gimenez, 2010). In a recent study, features based on the intra-language mutual information between words and backward language models were introduced (Raybaud et al., 2011). Other studies evaluate the gain brought by features extracted from MT output back-translation (Albrecht and Hwa, 2007), pseudo-references in the form of output from other MT systems for the same source sentence (Soricut et al., 2012), and topic models (Rubino et al., 2012). Previous studies also differ on the labels to predict: binary scores (Quirk, 2004) or continuous scores such as those given by automatic m"
I13-1166,2009.eamt-1.5,0,0.182946,"show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneficial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 1 Introduction Quality Estimation (QE) involves judging the correctness of a system output given an input without any output reference. Substantial progress has been made on QE for Machine Translation (MT), but research has been mainly conducted on wellformed, edited text (Blatz et al., 2003; Ueffing et al., 2003; Raybaud et al., 2009; Specia et al., 2009). We turn our attention to estimating the quality of user-generated content (UGC) translation – a particularly relevant use of QE since the translation process is likely to be affected by the noisy nature of the input, particularly if the MT system is trained on well-formed text. The source language content is collected from an IT Web forum in English and translated into French by three automatic systems. For each MT system, the produced translation is manually evaluated following two criteria: the translation comprehensibility and fidelity. We evaluate several feature sets on the UGC dataset"
I13-1166,W03-0413,0,0.306327,"Section 5. A discussion of the results, as well as a comparison with previous work, are presented in Section 6. Finally, we conclude and suggest future work in Section 7. 2 Background The main approach for QE in MT is based on estimating how correct MT output is through characteristic elements extracted from the source and the target texts and the MT system involved in the translation process. These elements, or features, are seen as predictive parameters that can be combined with machine learning methods to estimate binary, multi-class, or continuous scores. First applied at the word level (Gandrabur and Foster, 2003; Ueffing et al., 2003), QE for MT was then extended to the sentence level during a workshop in the same year (Blatz et al., 2003). Many different feature sources have been used including surface features (segment length, punc1167 International Joint Conference on Natural Language Processing, pages 1167–1173, Nagoya, Japan, 14-18 October 2013. tuation marks, etc.), language model features (perplexity, log-probability, etc.), word or phrase alignment features, n-best list features, internal MT system scores (Quirk, 2004; Ueffing and Ney, 2004), and linguistic features (Gamon et al., 2005; Speci"
I13-1166,2003.mtsummit-papers.52,0,0.596181,"delity by human annotators. Our experiments show that tried-and-tested quality estimation features work well on this type of data but that extending this set can be beneficial. We also show that the performance of particular types of features depends on the type of system used to produce the translation. 1 Introduction Quality Estimation (QE) involves judging the correctness of a system output given an input without any output reference. Substantial progress has been made on QE for Machine Translation (MT), but research has been mainly conducted on wellformed, edited text (Blatz et al., 2003; Ueffing et al., 2003; Raybaud et al., 2009; Specia et al., 2009). We turn our attention to estimating the quality of user-generated content (UGC) translation – a particularly relevant use of QE since the translation process is likely to be affected by the noisy nature of the input, particularly if the MT system is trained on well-formed text. The source language content is collected from an IT Web forum in English and translated into French by three automatic systems. For each MT system, the produced translation is manually evaluated following two criteria: the translation comprehensibility and fidelity. We evalu"
I13-1166,P03-1054,0,0.00360087,"features are the source and target segment distributions over the 10-dimensional topic space and 3 features are the distances between these distributions, using the cosine, euclidean distance and city-block metrics. - 16 Pseudo-reference Features Following Soricut et al. (2012), we compare each MT system output to the two others using sentence-level BLEU, error information provided by TER (no. of insertions, deletions, etc.) and Levenshtein. - 3 Part-of-Speech Features We count the number of POS tag types in the source and target segments, extracted from trees produced by the Stanford parser (Klein and Manning, 2003). The ratio of these two values is also included. 4.2 UGC-related Features We also experiment with features that capture the noisy nature of UGC. Some are related to the inconsistent use of character case, some to nonstandard punctuation, some to spelling mistakes and some to the tendency of sentence splitters to underperform on this type of text. From each source-target pair, we extract the following information (in the form of one feature for the source segment, one for the target segment and, where appropriate, one for the ratio between the two): - 11 Case Features the number of upper and l"
I13-1166,W12-3102,0,\N,Missing
I13-1166,C04-1046,0,\N,Missing
I13-1166,W13-2201,0,\N,Missing
I13-1166,W12-3118,0,\N,Missing
lynn-etal-2012-irish,W98-1507,0,\N,Missing
lynn-etal-2012-irish,ui-dhonnchadha-van-genabith-2010-partial,1,\N,Missing
lynn-etal-2012-irish,gupta-etal-2010-partial,0,\N,Missing
lynn-etal-2012-irish,nivre-etal-2006-maltparser,0,\N,Missing
lynn-etal-2012-irish,W10-2910,0,\N,Missing
lynn-etal-2012-irish,W02-1503,0,\N,Missing
lynn-etal-2012-irish,W01-1203,0,\N,Missing
lynn-etal-2012-irish,W04-0210,0,\N,Missing
lynn-etal-2012-irish,W08-1301,0,\N,Missing
lynn-etal-2012-irish,P10-1075,0,\N,Missing
lynn-etal-2012-irish,J08-4003,0,\N,Missing
lynn-etal-2012-irish,W10-1401,1,\N,Missing
lynn-etal-2012-irish,P05-1012,0,\N,Missing
lynn-etal-2012-irish,J08-4010,0,\N,Missing
lynn-etal-2012-irish,J08-3003,0,\N,Missing
lynn-etal-2012-irish,W11-4649,0,\N,Missing
lynn-etal-2012-irish,P06-1063,1,\N,Missing
lynn-etal-2012-irish,J08-4004,0,\N,Missing
lynn-etal-2012-irish,P05-1034,0,\N,Missing
lynn-etal-2012-irish,W11-2917,0,\N,Missing
lynn-etal-2012-irish,dzeroski-etal-2006-towards,0,\N,Missing
lynn-etal-2012-irish,mieskes-strube-2006-part,0,\N,Missing
lynn-etal-2012-irish,W11-2929,0,\N,Missing
lynn-etal-2012-irish,ui-dhonnchadha-van-genabith-2006-part,0,\N,Missing
N10-1060,P06-2005,0,0.0127344,"s) (ADJP (JJ sad)) (SBAR (IN that)... when playing bad (SBAR (WHADVP (WRB when)) (S (VP (VBG playing) (ADJP (JJ bad))))) YOU GOT BEATEN BY THE BETTER TEAM (S (NP (PRP YOU)) (VP (VBP GOT) (NP (NNP BEATEN) (NNP BY) (NNP THE) (NNP BETTER) (NNP TEAM)))) or it was cos you lost (VP (VBD was) (ADJP (NN cos) (SBAR (S (NP (PRP you)) (VP (VBD lost)))))) Table 3: Phenomena which lead the parser astray. The output of the parser is given for each example. 3 Initial Improvements Parsing performance on noisy data can be improved by transforming the input data so that it resembles the parser’s training data (Aw et al., 2006), transforming the training data so that it resembles the input data (van der Plas et al., 2009), applying semisupervised techniques such as the self-training protocol used by McClosky et al. (2006), and changing the parser internals, e.g. adapting the parser’s unknown word model to take into account variation in capitalisation and function word misspelling.4 We focus on the first two approaches and attempt to transform both the input data and the WSJ training material. The transformations that we experiment with are shown in Table 5. The treebank transformations are performed in such a way th"
N10-1060,P06-1043,0,0.231273,"NNP BEATEN) (NNP BY) (NNP THE) (NNP BETTER) (NNP TEAM)))) or it was cos you lost (VP (VBD was) (ADJP (NN cos) (SBAR (S (NP (PRP you)) (VP (VBD lost)))))) Table 3: Phenomena which lead the parser astray. The output of the parser is given for each example. 3 Initial Improvements Parsing performance on noisy data can be improved by transforming the input data so that it resembles the parser’s training data (Aw et al., 2006), transforming the training data so that it resembles the input data (van der Plas et al., 2009), applying semisupervised techniques such as the self-training protocol used by McClosky et al. (2006), and changing the parser internals, e.g. adapting the parser’s unknown word model to take into account variation in capitalisation and function word misspelling.4 We focus on the first two approaches and attempt to transform both the input data and the WSJ training material. The transformations that we experiment with are shown in Table 5. The treebank transformations are performed in such a way that their frequency distribution mirrors their distribution in the development data. We remove discourse-marking acronyms such as lol5 from the input sentence, but 4 Even when spelling errors have be"
N10-1060,N07-1051,0,0.0172112,"by the omission of conjunctions, the use of a comma as a conjunction and the tendency towards unlike constituent coordination. Parser Comparison We test the lexicalised Charniak parser plus reranker (Charniak and Johnson, 2005) on the development set sentences. We also test the Berkeley parser with an SM6 grammar. The f-scores are shown in Table 4. The parser achieving the highest score on WSJ23, namely, the C&J reranking parser, also achieves the highest score on our development set. The difference between the two Berkeley grammars supports the claim that an SM6 grammar overfits to the WSJ (Petrov and Klein, 2007). However, the differences between the four parser/grammar configurations are small. Parser Berkeley SM5 Berkeley SM6 Charniak First-Stage C & J Reranking WSJ23 89.17 89.56 89.13 91.33 Football 77.56 77.01 77.13 78.33 Table 4: Cross-parser and cross-grammar comparison Problematic Phenomena Idioms/Fixed Expressions Acronyms Missing subject Lowercase proper nouns Coordination Adverb/Adjective Confusion CAPS LOCK IS ON cos instead of because Examples Spot on (S (VP (VB Spot) (PP (IN on))) (. .)) lmao (S (NP (PRP you)) (VP (VBZ have) (RB n’t) (VP (VBN done) (NP (ADVP (RB that) (RB once)) (DT this)"
N10-1060,P06-1055,0,0.0248731,"Missing"
N10-1060,roark-etal-2006-sparseval,0,0.0576101,"rocess (SM5). Tokenisation and Spelling Effects In the first experiment, the parser is given the original development set sentences which contain spelling mistakes and which have not been tokenised. We ask the parser to perform its own tokenisation. In the second experiment, the parser is given the handtokenised sentences which still contain spelling mistakes. These are corrected for the third experiment. Since the yields of the parser output and gold trees are not guaranteed to match exactly, we cannot use the evalb implementation of the Parseval evaluation metrics. Instead we use Sparseval (Roark et al., 2006), which was designed to be used to evaluate the parsing of spoken data and can handle this situation. An unaligned dependency evaluation is carried out: head-finding rules are used to convert a phrase structure tree into a dependency graph. Precision and recall are calculated over the dependencies The Sparseval results are shown in Table 1. For the purposes of comparison, the WSJ23 performance is displayed in the top row. We can see that performance suffers when the parser performs its own tokenisation. A reason for this is the under-use of apostrophes in the forum data, with the result that w"
N10-1060,N09-2032,0,0.0598554,"Missing"
N10-1060,J04-4004,0,\N,Missing
N10-1060,P05-1022,0,\N,Missing
N16-1154,W15-4319,0,0.0225463,"Missing"
N16-1154,Q15-1015,0,0.420651,"scribe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse. In particular, they show how discourse information (obtained either via a discourse parser or using shallow techniques based on discourse markers) can complement distributed lexical semantic information. Sharp et al. (2015) show how discourse structure can be used to generate artificial questionanswer training pairs from documents, and test their approach on the same dataset. The best performance on this dataset – 33.01 P@1 and 53.96 MRR – is reported by Fried et al. (2015) who improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words using higher-order models. In contrast, our approach is very simple and requires no feature engineering. Question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer (the probability of an answer being the best answer to the question). Despite its simplicity, we achieve state-of-theart performance on this dataset – 37.17 P@1 and 56.82 MRR. We attribute this improved performance to"
N16-1154,P14-1092,0,0.222268,"uora2 and the StackExchange3 family of forums. The ability of users to vote for their favourite answer makes these sites a valuable source of training data for open-domain non-factoid QA systems. 1 http://answers.yahoo.com http://quora.com 3 http://stackexchange.com/ 2 In this paper, we present a neural approach to open-domain non-factoid QA, focusing on the subtask of answer reranking, i.e. given a list of candidate answers to a question, order the answers according to their relevance to the question. We test our approach on the Yahoo! Answers dataset of manner or How questions introduced by Jansen et al. (2014), who describe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse. In particular, they show how discourse information (obtained either via a discourse parser or using shallow techniques based on discourse markers) can complement distributed lexical semantic information. Sharp et al. (2015) show how discourse structure can be used to generate artificial questionanswer training pairs from documents, and test their approach on the same dataset. The best performance on this dataset – 33.01 P@1 and 53.96 MRR – is repo"
N16-1154,N15-1025,0,0.0630211,"nswer reranking, i.e. given a list of candidate answers to a question, order the answers according to their relevance to the question. We test our approach on the Yahoo! Answers dataset of manner or How questions introduced by Jansen et al. (2014), who describe answer reranking experiments on this dataset using a diverse range of features incorporating syntax, lexical semantics and discourse. In particular, they show how discourse information (obtained either via a discourse parser or using shallow techniques based on discourse markers) can complement distributed lexical semantic information. Sharp et al. (2015) show how discourse structure can be used to generate artificial questionanswer training pairs from documents, and test their approach on the same dataset. The best performance on this dataset – 33.01 P@1 and 53.96 MRR – is reported by Fried et al. (2015) who improve on the lexical semantic models of Jansen et al. (2014) by exploiting indirect associations between words using higher-order models. In contrast, our approach is very simple and requires no feature engineering. Question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is us"
N16-1154,P15-1098,0,0.0652485,"Missing"
N16-1154,P15-2116,0,0.0830622,"Missing"
N16-1154,P15-2117,0,0.0621998,"Missing"
N19-3001,N18-1202,0,0.0168784,"Missing"
N19-3001,N16-1154,1,0.84622,"question types but is also a signal of how much comprehension is actually taking place. 2 Related Work A number of studies have explored the use of customer reviews in retrieval and question answering. Using Amazon data, Yu et al. (2018) develop a framework which returns a ranked list of sentences from reviews or existing question-answer pairs for a given question. Xu et al. (2019) create a new dataset comprising Amazon laptop reviews and questions and Yelp restaurant reviews and questions, where reviews are used to answer questions in multiple-turn dialogue form. Bogdanova et al. (2017) and Bogdanova and Foster (2016)) do not use review data but also focus on QA over usergenerated content, attempting to find similar questions or rank answers in user fora. We use the same Amazon data as Yu et al. (2018) but consider a wider set of domains (they consider only two), and attempt to directly answer yes/no questions. To the best of our knowledge, the novelty in 1 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 1–6 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics our work l"
N19-3001,P18-2124,0,0.0660713,"Missing"
N19-3001,E17-1012,1,0.860355,"ews will be useful for other question types but is also a signal of how much comprehension is actually taking place. 2 Related Work A number of studies have explored the use of customer reviews in retrieval and question answering. Using Amazon data, Yu et al. (2018) develop a framework which returns a ranked list of sentences from reviews or existing question-answer pairs for a given question. Xu et al. (2019) create a new dataset comprising Amazon laptop reviews and questions and Yelp restaurant reviews and questions, where reviews are used to answer questions in multiple-turn dialogue form. Bogdanova et al. (2017) and Bogdanova and Foster (2016)) do not use review data but also focus on QA over usergenerated content, attempting to find similar questions or rank answers in user fora. We use the same Amazon data as Yu et al. (2018) but consider a wider set of domains (they consider only two), and attempt to directly answer yes/no questions. To the best of our knowledge, the novelty in 1 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 1–6 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Comp"
N19-3001,D18-1546,0,0.0684515,"Missing"
N19-3001,C18-1186,0,0.08378,"Missing"
P08-2056,E95-1031,0,0.0397993,"ible. These sentences and their associated trees are then used as training material for a statistical parser. It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical sentence. If the syntactic analysis is to guide semantic analysis, it must reflect as closely as possible what the person who produced the sentence was trying to express. Thus, while s"
P08-2056,P07-1010,0,0.0603409,"to an ungrammatical treebank. This transformation process has two parts: 1. the yield of each tree is transformed into an ungrammatical sentence by introducing a syntax error; 2. each tree is minimally transformed, but left intact as much as possible to reflect the syntactic structure of the original “intended” sen221 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221–224, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tence prior to error insertion. Artificial ungrammaticalities have been used in various NLP tasks (Smith and Eisner, 2005; Okanohara and Tsujii, 2007) The idea of an automatically generated ungrammatical treebank was proposed by Foster (2007). Foster generates an ungrammatical version of the WSJ treebank and uses this to train two statistical parsers. The performance of both parsers significantly improves on the artificially created ungrammatical test data, but significantly degrades on the original grammatical test data. We show that it is possible to obtain significantly improved performance on ungrammatical data without a concomitant performance decline on grammatical data. 3 Generating Noisy Treebanks ill-formed sentences that are produ"
P08-2056,W97-0303,0,0.023535,"Missing"
P08-2056,P98-2196,0,0.0451401,"parser. It is important that parsing on grammatical sentences is not harmed and we introduce a parse-probability-based classifier which allows both grammatical and ungrammatical sentences to be accurately parsed. Various strategies exist to build robustness into the parsing process: grammar constraints can be relaxed (Fouvry, 2003), partial parses can be concatenated to form a full parse (Penstein Ros´e and Lavie, 1997), the input sentence can itself be transformed until a parse can be found (Lee et al., 1995), and mal-rules describing particular error patterns can be included in the grammar (Schneider and McCoy, 1998). For a parser which tends to fail when faced with ungrammatical input, such techniques are needed. The overgeneration associated with a statistical data-driven parser means that it does not typically fail on ungrammatical sentences. However, it is not enough to return some analysis for an ungrammatical sentence. If the syntactic analysis is to guide semantic analysis, it must reflect as closely as possible what the person who produced the sentence was trying to express. Thus, while statistical, data-driven parsing has solved the robustness problem, it is not clear that it is has solved the ac"
P08-2056,P05-1044,0,0.0310775,"matically transformed into an ungrammatical treebank. This transformation process has two parts: 1. the yield of each tree is transformed into an ungrammatical sentence by introducing a syntax error; 2. each tree is minimally transformed, but left intact as much as possible to reflect the syntactic structure of the original “intended” sen221 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221–224, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics tence prior to error insertion. Artificial ungrammaticalities have been used in various NLP tasks (Smith and Eisner, 2005; Okanohara and Tsujii, 2007) The idea of an automatically generated ungrammatical treebank was proposed by Foster (2007). Foster generates an ungrammatical version of the WSJ treebank and uses this to train two statistical parsers. The performance of both parsers significantly improves on the artificially created ungrammatical test data, but significantly degrades on the original grammatical test data. We show that it is possible to obtain significantly improved performance on ungrammatical data without a concomitant performance decline on grammatical data. 3 Generating Noisy Treebanks ill-fo"
P08-2056,D07-1012,1,0.883737,"Missing"
P08-2056,P05-1022,0,\N,Missing
P08-2056,C98-2191,0,\N,Missing
P10-2065,W07-1604,1,0.645202,"nd Knutsson (2008) show that knowledge of the PP attachment site helps in the task of preposition selection by comparing a classifier trained on lexical features (the verb before the preposition, the noun between the verb and the preposition, if any, and the noun after the preposition) to a classifier trained on attachment features which explicitly state whether the preposition is attached to the preceding noun or verb. They also argue that a parser which is capable of distinguishing between arguments and adjuncts is useful for generating the correct preposition. 3 Baseline System The work of Chodorow et al. (2007) and T&C08 treat the tasks of preposition selection and error detection as a classification problem. That is, given the context around a preposition and a model of correct usage, a classifier determines which of the 34 prepositions covered by the model is most appropriate for the context. A model of correct preposition usage is constructed by training a Maximum Entropy classifier (Ratnaparkhi, 1998) on millions of preposition contexts from well-formed text. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the pr"
P10-2065,C08-1022,0,0.602123,"Missing"
P10-2065,W08-1301,0,0.00717771,"Missing"
P10-2065,de-marneffe-etal-2006-generating,0,0.028425,"Missing"
P10-2065,I08-1059,0,0.0586698,"modeling preposition usage in well-formed text and learner text? • We demonstrate that parse features have a significant impact on preposition selection in well-formed text. We also show which features have the greatest effect on performance. • We show that, despite the noisiness of learner text, parse features can actually make small, albeit non-significant, improvements to the performance of a state-of-the-art preposition error detection system. • We evaluate the accuracy of parsing and especially preposition attachment in learner texts. 2 Related Work T&C08, De Felice and Pulman (2008) and Gamon et al. (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from wellformed text and a writer’s preposition is compared with the predictions of this model. It is difficult to directly compare these systems since they are trained and tested on different data sets 353 Proceedings of the ACL 2010 Conference Short Papers, pages 353–358, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 3.1 but they achieve accuracy in a similar range. Of these systems, only the DAPPER system (De Felice and Pulman, 2008; De F"
P10-2065,hermet-etal-2008-using,0,0.0431457,"Missing"
P10-2065,P09-2079,0,0.0107573,"Missing"
P10-2065,P03-1054,0,0.00558171,"Detection Joel Tetreault Educational Testing Service Princeton NJ, USA Jennifer Foster NCLT Dublin City University Ireland Martin Chodorow Hunter College of CUNY New York, NY, USA JTetreault@ets.org jfoster@computing.dcu.ie @hunter.cuny.edu Abstract We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T&C08) originally trained with lexical features and augment it with parser output features. We employ the Stanford parser in our experiments because it consists of a competitive phrase structure parser and a constituent-to-dependency conversion tool (Klein and Manning, 2003a; Klein and Manning, 2003b; de Marneffe et al., 2006; de Marneffe and Manning, 2008). We compare the original model with the parser-augmented model on the tasks of preposition selection in wellformed text (fluent writers) and preposition error detection in learner texts (ESL writers). This paper makes the following contributions: We evaluate the effect of adding parse features to a leading model of preposition usage. Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task. Ana"
P10-2065,P08-1006,0,0.014116,"Missing"
P10-2065,C08-1109,1,0.709664,"t test sets and also training sizes. Given the time required to train large models, we report here experiments with a relatively small model. 355 Model T&C08 +Phrase Structure Only +Dependency Only +Parse +head-tag+comp-tag +left +grandparent +head-token+comp-tag +head-tag +head-token +head-tag+comp-token Accuracy 65.2 67.1 68.2 68.5 66.9 66.8 66.6 66.6 66.5 66.4 66.1 Method T&C08 +Parse native speakers for the Test of English as a Foreign R Language (TOEFL ). The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). 4,881 of the prepositions were judged to be correct and the remaining 302 were judged to be incorrect. The writer’s preposition is flagged as an error by the system if its likelihood according to the model satisfied a set of criteria (e.g., the difference between the probability of the system’s choice and the writer’s preposition is 0.8 or higher). Unlike the selection task where we use accuracy as the metric, we use precision and recall with respect to error detection. To date, performance figures that have been reported in the literature have been quite low, reflecting the difficulty of"
P10-2065,W08-1205,1,0.791661,"t test sets and also training sizes. Given the time required to train large models, we report here experiments with a relatively small model. 355 Model T&C08 +Phrase Structure Only +Dependency Only +Parse +head-tag+comp-tag +left +grandparent +head-token+comp-tag +head-tag +head-token +head-tag+comp-token Accuracy 65.2 67.1 68.2 68.5 66.9 66.8 66.6 66.6 66.5 66.4 66.1 Method T&C08 +Parse native speakers for the Test of English as a Foreign R Language (TOEFL ). The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). 4,881 of the prepositions were judged to be correct and the remaining 302 were judged to be incorrect. The writer’s preposition is flagged as an error by the system if its likelihood according to the model satisfied a set of criteria (e.g., the difference between the probability of the system’s choice and the writer’s preposition is 0.8 or higher). Unlike the selection task where we use accuracy as the metric, we use precision and recall with respect to error detection. To date, performance figures that have been reported in the literature have been quite low, reflecting the difficulty of"
P10-2065,J07-4004,0,\N,Missing
P10-2065,J03-4003,0,\N,Missing
P12-2066,W11-0705,0,0.149937,"which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and convolution kernels to exploit information on surface and syn"
P12-2066,H05-1091,0,0.0191743,") represent a document as a bag-of-words; Matsumoto et al., (2005) extract frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context contai"
P12-2066,W08-1301,0,0.0692146,"Missing"
P12-2066,W10-2910,0,0.0159615,"gly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively. However, as these are considerably shorter than documents, their feature space is less complex, and pruning is not as pertinent. 3 Kernels for Sentiment Classification 3.1 Linguistic Representations We explore both sequence and co"
P12-2066,P09-2079,0,0.0921626,"-Impact Sub-Structures for Convolution Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types"
P12-2066,P03-1054,0,0.00724357,"directly connected to the subjective word. For instance, given the node tragic in Figure 1(d), we will extract its direct parent waste integrated with dependency relations and (possibly) POS, as in Figure 2(b). We further add two background scopes, one being subjective sentences (the sentences that contain subjective words), and the entire document. 4 Experiments 4.1 Setup We carried out experiments on the movie review dataset (Pang and Lee, 2004), which consists of 1000 positive reviews and 1000 negative reviews. To obtain constituency trees, we parsed the document using the Stanford Parser (Klein and Manning, 2003). To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We exploited Subset Tree (SST) (Collins and Duffy, 2001) and Partial Tree (PT) kernels (Moschitti, 2006) for constituent and dependency parse trees1 , respectively. A sequential kernel is applied for lexical sequences. Kernels were combined using plain (unweighted) summation. Corpus statistics are provided in Table 1. We use a manually constructed polarity lexicon (Wilson et al., 2005), in which each entry is annotated with its degre"
P12-2066,D09-1017,0,0.350748,"ion Kernels in Document-level Sentiment Classification Zhaopeng Tu† Yifan He‡§ Jennifer Foster§ Josef van Genabith§ Qun Liu† Shouxun Lin† † Key Lab. of Intelligent Info. Processing ‡ Computer Science Department § School of Computing Institute of Computing Technology, CAS New York University Dublin City University † {tuzhaopeng,liuqun,sxlin}@ict.ac.cn, ‡ yhe@cs.nyu.edu, § {jfoster,josef}@computing.dcu.ie Abstract uments. More recently, there have been several approaches which employ features based on deep linguistic analysis with encouraging results including Joshi and Penstein-Rose (2009) and Liu and Seneff (2009). However, as they select features manually, these methods would require additional labor when ported to other languages and domains. Convolution kernels support the modeling of complex syntactic information in machinelearning tasks. However, such models are highly sensitive to the type and size of syntactic structure used. It is therefore an important challenge to automatically identify high impact sub-structures relevant to a given task. In this paper we present a systematic study investigating (combinations of) sequence and convolution kernels using different types of substructures in docum"
P12-2066,P08-2029,0,0.0249684,"rom dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et al. (2011) process sentences and tweets respectively."
P12-2066,J08-2003,0,0.102698,"e docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe"
P12-2066,P06-2079,0,0.451789,"Missing"
P12-2066,D09-1143,0,0.0168412,"frequently occurring connected subtrees from dependency parsing; Joshi and Penstein-Rose (2009) use a transformation of dependency relation triples; Liu and Seneff (2009) extract adverb-adjective-noun relations from dependency parser output. Previous research has convincingly demonstrated a kernel’s ability to generate large feature sets, which is useful to quickly model new and not well understood linguistic phenomena in machine learning, and has led to improvements in various NLP tasks, including relation extraction (Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006; Nguyen et al., 2009), question answering (Moschitti and Quarteroni, 2008), semantic role labeling (Moschitti et al., 2008). Convolution kernels have been used before in sentiment analysis: Wiegand and Klakow (2010) use convolution kernels for opinion holder extraction, 339 Johansson and Moschitti (2010) for opinion expression detection and Agarwal et al. (2011) for sentiment analysis of Twitter data. Wiegand and Klakow (2010) use e.g. noun phrases as possible candidate opinion holders, in our work we extract any minimal syntactic context containing a subjective word. Johansson and Moschitti (2010) and Agarwal et"
P12-2066,P04-1035,0,0.860926,"xicon show 1.45 point absolute improvement in accuracy over a bag-of-words classifier on a widely used sentiment corpus. 1 Introduction An important subtask in sentiment analysis is sentiment classification. Sentiment classification involves the identification of positive and negative opinions from a text segment at various levels of granularity including document-level, paragraphlevel, sentence-level and phrase-level. This paper focuses on document-level sentiment classification. There has been a substantial amount of work on document-level sentiment classification. In early pioneering work, Pang and Lee (2004) use a flat feature vector (e.g., a bag-of-words) to represent the documents. A bag-of-words approach, however, cannot capture important information obtained from structural linguistic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment clas"
P12-2066,H05-1044,0,0.861528,"tics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009). Our research is inspired by these observations. Unlike in the previous work, however, we focus on syntactic substructures (rather than entire paragraphs or sentences) that contain subjective words. More specifically, we use the terms in the lexicon constructed from (Wilson et al., 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). An empirical evaluation on a widely used sentiment corpus shows an improvement of 1.45 point in accuracy over the baseline resulting from a combination of bag-of-words and high-impact parse features (Section 4). 2 Related Work Our research builds on previous work in the field of sentiment classification and convolution kernels. For sentiment classification, the design of lexical and syntactic features is a"
P12-2066,W03-1017,0,0.174347,"categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective paragraphs to simply describe the plot. Such objective portions do not contain the author’s opinion and are irrelevant with respect to the sentiment classifi338 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338–343, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics cation task. Indeed, separating objective sentences from subjective sentences in a document produces encouraging results (Yu and Ha"
P12-2066,P06-1104,0,0.0670835,"istic analysis of the docIn this paper, we study and evaluate diverse linguistic structures encoded as convolution kernels for the document-level sentiment classification problem, in order to utilize syntactic structures without defining explicit linguistic rules. While the application of kernel methods could seem intuitive for many tasks, it is non-trivial to apply convolution kernels to document-level sentiment classification: previous work has already shown that categorically using the entire syntactic structure of a single sentence would produce too many features for a convolution kernel (Zhang et al., 2006; Moschitti et al., 2008). We expect the situation to be worse for our task as we work with documents that tend to comprise dozens of sentences. It is therefore necessary to choose appropriate substructures of a sentence as opposed to using the whole structure in order to effectively use convolution kernels in our task. It has been observed that not every part of a document is equally informative for identifying the polarity of the whole document (Yu and Hatzivassiloglou, 2003; Pang and Lee, 2004; Koppel and Schler, 2005; Ferguson et al., 2009): a film review often uses lengthy objective parag"
P12-2066,N10-1121,0,\N,Missing
P19-2048,P07-1056,0,0.0274227,"Missing"
P19-2048,P12-2034,0,0.0402865,"Missing"
P19-2048,P11-1032,0,0.283723,"nd Schmidhuber, 1997). We experiment with simple bag-of-word input representations and, for the neural approaches, we also use pre-trained word2vec embeddings (Le and Mikolov, 2014). In contrast with word2vec vectors which provide the same vector for a particular word regardless of its sentential context, we also experiment with contextualised vectors. Specifically, we utilize the BERT model developed by Google (Devlin et al., 2018) for fine-tuning pretrained representations. Collecting data for classifying opinion spam is difficult because human labelling is only slightly better than random (Ott et al., 2011). Thus, it is difficult to find large-scale ground truth data. We experiment with two datasets: • OpSpam (Ott et al., 2011): This dataset contains 800 gold-standard, labelled reviews. These reviews are all deceptive and were written by paid, crowd-funded workers for popular Chicago hotels. Additionally this dataset contains 800 reviews considered truthful, that were mined from various online review communities. These truthful reviews cannot be considered gold-standard, but are considered to have a reasonably low deception rate. • Yelp (Rayana and Akoglu, 2015): This is the largest ground truth"
P19-2048,C18-1160,0,0.339175,"Missing"
S14-1012,nivre-etal-2006-maltparser,0,0.0414729,"sets ous domains. We now explore the utility of this new dependency scheme in SRL. The French universal dependency treebank comes in two versions, the first using the standard dependency structure based on basic Stanford dependencies (de Marneffe and Manning, 2008) where content words are the heads except in copula and adposition constructions, and the second which treats content words as the heads for all constructions without exemption. We use both schemes in order to verify their effect on SRL. In order to obtain universal dependencies for our data, we train parsing models with MaltParser (Nivre et al., 2006) using the entire uni-dep-tb.3 We then parse our data using these MaltParser models. The input POS tags to the parser are the universal POS tags used in OrgDep+UniPOS. We train and evaluate new SRL models on these data. The results are shown in the third and fourth rows of Table 1. StdUniDept+UniPOS is the setting using standard dependencies and CHUDep+UPOS using content-head dependencies. According to the third and fourth rows in Table 1, content-head dependencies are slightly less useful than standard dependencies. The general effect of universal dependencies can be compared to those of orig"
S14-1012,W09-1206,0,0.310235,"Missing"
S14-1012,J03-1002,0,0.0134412,"Missing"
S14-1012,candito-etal-2010-statistical,0,0.0690462,"Missing"
S14-1012,J05-1004,0,0.123217,"ence over the original fine-grained tagset and dependency scheme. Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations. 1 Introduction Semantic role labelling (SRL) (Gildea and Jurafsky, 2002) is the task of identifying the predicates in a sentence, their semantic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010b). This dataset is then used by van der Plas et al. (2011) to evaluate their approach to projecting the SRLs of English sentences to their translations 2 Related Work There ha"
S14-1012,W08-1301,0,0.373515,"Missing"
S14-1012,petrov-etal-2012-universal,0,0.0356816,"structure. This inspires us to compare the impact of different types of syntactic annotations on the performance of this system. Based on the observations from the previous sections, we choose two different sizes of training sets. The first set contains the first 5K sentences from the original 100K, as we saw that more than this amount tends to diminish performance. The second set contains the first 50K from the original 100K, the purpose of which is to check if changing the parses affects the usefulness of adding more data. We will call these data sets Classic5K and Classic50K respectively. Petrov et al. (2012) create a set of 12 universal part-of-speech (POS) tags which should in theory be applicable to any natural language. It is interesting to know whether these POS tags are more useful for SRL than the original set of the 29 more fine-grained POS tags used in French Treebank which we have used so far. To this end, we convert the original POS tags of the data to universal POS tags and retrain and evaluate the SRL models. The results are given in the second row of Table 1 (OrgDep+UniPOS). The first row of the table (Original) shows the performance using the original annotation. Even though the sco"
S14-1012,J02-3001,0,0.0166414,"s paper describes a series of French semantic role labelling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment. Using universal part-of-speech tags and dependencies makes little difference over the original fine-grained tagset and dependency scheme. Moreover, there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations. 1 Introduction Semantic role labelling (SRL) (Gildea and Jurafsky, 2002) is the task of identifying the predicates in a sentence, their semantic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-sta"
S14-1012,W07-2218,0,0.0866337,"Missing"
S14-1012,2005.mtsummit-papers.11,0,0.0105659,"tic arguments and the roles these arguments take. The last decade has seen considerable attention paid to statistical SRL, thanks to the existence of two major hand-crafted resources for English, namely, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Apart from English, only a few languages have SRL resources and these resources tend to be of limited size compared to the English datasets. French is one of those languages which suffer from a scarcity of hand-crafted SRL resources. The only available gold-standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010b). This dataset is then used by van der Plas et al. (2011) to evaluate their approach to projecting the SRLs of English sentences to their translations 2 Related Work There has been relatively few works in French SRL. Lorenzo and Cerisara (2012) propose a clustering approach for verb predicate and argument labelling (but not identification). They choose This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence deta"
S14-1012,W10-1814,0,0.630468,"Missing"
S14-1012,P11-2052,0,0.315112,"Missing"
S14-1012,W12-3404,0,0.0651621,"Missing"
S14-1012,P98-1013,0,\N,Missing
S14-1012,C98-1013,0,\N,Missing
S14-1012,W09-1201,0,\N,Missing
S14-1012,P13-2017,0,\N,Missing
S14-2036,W10-1408,1,0.82907,"Missing"
S14-2036,baccianella-etal-2010-sentiwordnet,0,0.024032,"re word n-grams (with n ranging from 1 to 5) in a window around the aspect term, as well as features derived from scores assigned by a sentiment lexicon. Furthermore, to reduce data sparsity, we experiment with replacing sentiment-bearing words in our n-gram feature set with their polarity scores according to the lexicon and/or their part-of-speech tag. 2 Sentiment Lexicons The following four lexicons are employed: 1. MPQA1 (Wilson et al., 2005) classifies a word or a stem and its part of speech tag into positive, negative, both or neutral with a strong or weak subjectivity. 2. SentiWordNet2 (Baccianella et al., 2010) specifies the positive, negative and objective scores of a synset and its part of speech tag. 3. General Inquirer3 indicates whether a word expresses positive or negative sentiment. 4. Bing Liu’s Opinion Lexicon4 (Hu and Liu, 1 http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 2 http://sentiwordnet.isti.cnr.it/ 3 http://www.wjh.harvard.edu/˜inquirer/ inqtabs.txt 4 http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#lexicon This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: h"
S14-2036,W13-1106,1,0.845289,"Missing"
S14-2036,W08-1301,0,0.0256965,"Missing"
S14-2036,P97-1023,0,0.0414041,"with the following modifications: chocolate raspberry cake is heavenly - not too sweet, but full of flavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), seman"
S14-2036,W10-2910,0,0.0256235,"urney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the be"
S14-2036,D13-1170,0,0.00524286,"overall performance, e. g. lexicon combination, lemmatisation, spelling correction, other normalisations, negation handling, distance function and n-gram feature transformations. There is also room for improvements in most of these components, e. g. our handling of complex negations. Detection of conflicts also needs more attention. Features indicating the presence of trigger words for negation and conflicts that are currently used only internally in the rule-based component could be added to the SVM feature set. It would also be interesting to see how the compositional approach described by Socher et al. (2013) handles these difficult cases. The score features could be easily augmented by breaking down scores by the four employed lexicons. This way, the SVM can choose to combine the information from these scores differently than just summing them, allowing it to learn more complex relations. Lexicon filtering and addition of domain-specific entries could be automated to reduce the time needed to adjust to a new domain. Finally, machine learning methods that can efficiently handle large feature sets such as logistic regression should be tried with the full feature set (not applying frequency threshol"
S14-2036,P13-1160,0,0.0438888,"The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the best of our knowledge – not been applied"
S14-2036,P09-2020,0,0.021355,"s) and the British National Corpus (http://www.kilgarriff.co.uk/bnc-readme. html) and two thousand laptop reviews crawled from CNET (http://www.cnet.com/). 5 We also tried to vote over the four lexicon scores but this did not improve over summing. 6 http://world-food-and-wine.com/ describing-food 224 • Discourse Chunk Distance: This function counts the discourse chunks that must be crossed in order to get from the sentiment word to the aspect term. If the sentiment word and the aspect term are in the same discourse chunk, then the distance is zero. We use the discourse segmenter described in (Tofiloski et al., 2009). • Dependency Path Distance: This function calculates the shortest path between the sentiment word and the aspect term in a syntactic dependency graph for the sentence, produced by parsing the sentence with a PCFGLA parser (Attia et al., 2010) trained on consumer review data (Le Roux et al., 2012)8 , and converting the resulting phrase-structure tree into a dependency graph using the Stanford converter (de Marneffe and Manning, 2008) (version 3.3.1). n c n-gram -L— AL— ALS– ALSRAL— ALSRALSRP 2 2 1 1 2 2 1 2 2 4 4 4 4 4 cord with <aspect> with <negu080> <negu080> and skip and <negu080> <negu08"
S14-2036,P02-1053,0,0.0470761,"late raspberry cake is heavenly - not too sweet, but full of flavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words with scores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson an"
S14-2036,H05-1044,0,0.0416642,"uses supervised machine learning with support vector machines (SVM) (Boser et al., 1992) to classify an aspect term into one of the four classes. The features we employ are word n-grams (with n ranging from 1 to 5) in a window around the aspect term, as well as features derived from scores assigned by a sentiment lexicon. Furthermore, to reduce data sparsity, we experiment with replacing sentiment-bearing words in our n-gram feature set with their polarity scores according to the lexicon and/or their part-of-speech tag. 2 Sentiment Lexicons The following four lexicons are employed: 1. MPQA1 (Wilson et al., 2005) classifies a word or a stem and its part of speech tag into positive, negative, both or neutral with a strong or weak subjectivity. 2. SentiWordNet2 (Baccianella et al., 2010) specifies the positive, negative and objective scores of a synset and its part of speech tag. 3. General Inquirer3 indicates whether a word expresses positive or negative sentiment. 4. Bing Liu’s Opinion Lexicon4 (Hu and Liu, 1 http://mpqa.cs.pitt.edu/lexicons/ subj_lexicon/ 2 http://sentiwordnet.isti.cnr.it/ 3 http://www.wjh.harvard.edu/˜inquirer/ inqtabs.txt 4 http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html#l"
S14-2036,W02-1011,0,0.05838,"ging. The system accepts as input and generates as output valid SemEval ABSA XML documents.12 After extracting the text and the aspect terms from the input, the text is normalised using the process described in Footnote 8. The feature extraction is performed as described in Section 3 with the following modifications: chocolate raspberry cake is heavenly - not too sweet, but full of flavor. 5 Related Work The use of supervised machine learning with bagof-word or bag-of-n-gram feature sets has been a standard approach to the problem of sentiment polarity classification since the seminal work by Pang et al. (2002) on movie review polarity prediction. Heuristic methods which rely on a lexicon of sentiment words have also been widespread and much of the research in this area has been devoted to the unsupervised induction of good quality sentiment indicators (see, for example, Hatzivassiloglou and McKeown (1997) and Turney (2002), and Liu (2010) for an overview). The integration of sentiment lexicon scores as features in supervised machine learning to supplement standard bag-of-n-gram features has also been employed before (see, for example, Bakliwal et al. (2013)). The replacement of training/test words"
S14-2036,S14-2004,0,0.30284,"iterally melts in your mouth!. Furthermore, in the laptop domain, a number of the errors made by the rule-based system arise from the ambiguous nature of some lexicon words. For example, the sentence Only 2 usb ports ... seems kind of ... limited is misclassified because the word kind is considered to be positive. There are a few cases where the rule-based system outperforms the machine learning one. It happens when a sentence contains a rare word with strong polarity, e. g. the word heavenly in The Results and Analysis Table 2 shows the training and test accuracy of the task baseline system (Pontiki et al., 2014), a majority baseline classifying everything as positive, our rule-based system and our submitted system. The restaurant domain has a higher accuracy than the laptop domain for all systems, the SVM system outperforms the rule-based system on both domains, and the test accuracy is higher than the training accuracy for all systems in the restaurant domain. We observe that the majority of our systems’ errors fall into the following categories: 11 226 We only classify one test instance as conflict. tem that is more efficient. So far, we replaced time-consuming parsing with POS tagging. The system"
S14-2036,W12-3117,1,0.795992,"ores/labels from sentiment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the best of our knowledge – not been applied before. 6 • The POS information used by the n-gram feature extractor is obtained using the IMS TreeTagger (Schmid, 1994) instead of using the PCFG-LA parser (Attia et al., 2010). • The distance used by the rule-based approach is the token distance on"
S14-2036,W13-2249,1,0.823064,"iment lexicons has also been used by Baccianella et al. (2009), who supplement n-grams such as horrible location with generalised expressions such as NEGATIVE location. Linguistic features which capture generalisations at the level of syntax (Matsumoto et al., 2005), semantics (Johansson and Moschitti, 2010) and discourse (Lazaridou et al., 2013) have also been widely applied. In using binarised features derived from the nodes of a decision tree, we are following our recent work which uses the same technique in a different task: quality estimation for machine translation (Rubino et al., 2012; Rubino et al., 2013). The main novelty in our system lies not in the individual techniques but rather in they way they are combined and integrated. For example, our combination of token/chunk/dependency path distance used to weight the relationship between a sentiment word and the aspect term has – to the best of our knowledge – not been applied before. 6 • The POS information used by the n-gram feature extractor is obtained using the IMS TreeTagger (Schmid, 1994) instead of using the PCFG-LA parser (Attia et al., 2010). • The distance used by the rule-based approach is the token distance only, instead of a combi"
S15-2026,S12-1051,0,0.0573152,"core between a sentence pair. Our system exploits distributional semantics in combination with tried-and-tested features from previous tasks in order to compute sentence similarity. Our team submitted 3 runs for each of the five English test sets. For two of the test sets, belief and headlines, our best system ranked second and fourth out of the 73 submitted systems. Our best submission averaged over all test sets ranked 26 out of the 73 systems. 1 2 Data and Resources The training data for the task is comprised of all the corpora from previous years STS tasks: STS2012, STS-2013 and STS-2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). The test data is taken from five domains: answers-forums, answers-students, belief, headlines and images. Two domains (headlines and images) have some training data available from the previous STS tasks1 – the other three have been introduced for the first time. Introduction This paper describes DCU’s participation in the SemEval 2015 English Semantic Textual Similarity (STS) task, whose goal is to predict how similar in meaning two sentences are (Agirre et al., 2014). The semantic similarity between two sentences is defined on a scale from 0 (no re"
S15-2026,S13-1004,0,0.0672448,"ce pair. Our system exploits distributional semantics in combination with tried-and-tested features from previous tasks in order to compute sentence similarity. Our team submitted 3 runs for each of the five English test sets. For two of the test sets, belief and headlines, our best system ranked second and fourth out of the 73 submitted systems. Our best submission averaged over all test sets ranked 26 out of the 73 systems. 1 2 Data and Resources The training data for the task is comprised of all the corpora from previous years STS tasks: STS2012, STS-2013 and STS-2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). The test data is taken from five domains: answers-forums, answers-students, belief, headlines and images. Two domains (headlines and images) have some training data available from the previous STS tasks1 – the other three have been introduced for the first time. Introduction This paper describes DCU’s participation in the SemEval 2015 English Semantic Textual Similarity (STS) task, whose goal is to predict how similar in meaning two sentences are (Agirre et al., 2014). The semantic similarity between two sentences is defined on a scale from 0 (no relation) to 5 (semanti"
S15-2026,S13-1005,0,0.0223622,"We submitted three We use the Word2Vec (W2V) representation for computing semantic similarity between two words. We then expand to incorporate the similarity between two sentences. Using W2V, a word can be represented as a vector of D dimensions, with each dimension capturing some aspect of the word’s meaning in the form of different concepts learnt from the trained model. We use the gensim W2V ˇ uˇrek and Sojka, 2010). implementation (Reh˚ We use the text8 Wikipedia corpus to train our general W2V model. This corpus is comprised of 100MB of compressed Wikipedia data.2 We use the UMBC corpus (Han et al., 2013) for building domain-specific W2V models. 1 2 http://ixa2.si.ehu.es/stswiki/index.php/Main Page http://mattmahoney.net/dc/textdata.html 143 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 143–147, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics 3 Methodology 3.1 by its ICF weight. Pre-processing Pn We perform minimal pre-processing, replacing all hyphens and apostrophes with spaces, and removing all non-alphanumeric symbols from the data. Our general domain model uses the NLTK3 stop word list for stop word removal an"
S15-2026,P14-5010,0,0.00380509,"of the content words in each sentence and then compute the cosine between the two centroids. Syntax: We also hypothesize that two semantically similar sentences should have high overlap between their nouns, verbs, adjectives and adverbs. For each coarse-grained POS tag (NN*, VB*, JJ* and RB*) we calculate the W2V cosine similarity between all words from s1 and s2 which have the same POS tag (using the Sum W2V combination method). For each coarse-grained POS tag, we also calculate the number of lexical matches with that particular POS tag. We also parse each sentence using the Stanford parser (Manning et al., 2014) and look for dependency relation overlap between s1 and s2.6 We concentrate on six dependency relations – nsubj, dobj, det, prep, amod and aux. For each relation we calculate the degree of overlap between the occurrences of this relation in the two sentences. We have two notions of relation overlap: a nonlexicalized version which just counts the relation itself (e.g. nsubj) and a lexicalized version which counts the relation and the two tokens it connects (e.g. nsubj word1 word2). 3.2.3 Monolingual Alignment We compute the monolingual alignment between the two sentences using the word aligner"
S15-2026,P08-1028,0,0.0555339,"ct W2V: Given s1 and s2, we take the element-wise product of each word vector in s1 and s2 and store the maximum product value for each word in s1 and similarly for s2. The Product W2V feature is the average of the maximum weights between each word of s1 with s2 and vice versa: Pn sim(s1, s2) = Feature Design ~i s1 . j=1 m q P sim(s1, s2) = q P n m n ~ ~i s1 2 ( j=1 s2j )2 ) ( i=1 n m i=1 Pm + i=1 (max j=1 (max Pn i=1 m Pm j=1 √ ~ i .s2 ~ (s1 √j ) ) ~ j )2 (s2 ~ i )2 (s1 n √ ~ i .s2 ~ (s1 √j ) ) ~ i )2 (s2 ~ j )2 (s1 (2) The sum and product W2V models are inspired by the composition models of Mitchell and Lapata (2008) and semantic similarity measures of Mihalcea et al. (2006). Domain-specific Cosine Similarity: Good coverage is obtained using the text8 corpus to train the W2V model. However, we also want to explore the performance with respect to an in-domain W2V model. So, for each of the test corpora, we first extract a corpus of similar sentences from the UMBC corpus by selecting up to 500 sentences for each content word in the test corpus and then use the extracted dataset to train a W2V model that has better coverage of the test domain. Using the domainspecific W2V corpus, we compute the feature domai"
S15-2026,Q14-1018,0,0.0356098,"pendency relation overlap between s1 and s2.6 We concentrate on six dependency relations – nsubj, dobj, det, prep, amod and aux. For each relation we calculate the degree of overlap between the occurrences of this relation in the two sentences. We have two notions of relation overlap: a nonlexicalized version which just counts the relation itself (e.g. nsubj) and a lexicalized version which counts the relation and the two tokens it connects (e.g. nsubj word1 word2). 3.2.3 Monolingual Alignment We compute the monolingual alignment between the two sentences using the word aligner introduced in (Sultan et al., 2014). Their system aligns related words in a sentence pair by exploiting semantic and contextual similarities of the words. From the aligned sentences, we then extract two features: percent aligned source and percent aligned target, which represent the fraction of tokens in each sentence which have an alignment in the other sentence. The intuition behind these features is that sentences which are semantically similar should have a higher fraction of aligned tokens, since alignments constitute either identical strings or paraphrases. 3.2.4 TakeLab ˇ c et al., 2012) was the The Takelab system (Sari´"
S15-2026,S12-1060,0,0.13493,"Missing"
S15-2026,S14-2010,0,\N,Missing
U12-1005,J08-4004,0,0.0441666,"Missing"
U12-1005,W04-3202,0,0.0199897,"og-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decreases annotation cost, measured both in number of sentences to achieve a particular level of parse selection accuracy, and in a measure of sentence complexity, with respect to random selection. However, this differs from the task of constructing a resource that is intended to be reused in a number of ways. First, as Baldridge and Osborne (2004) show, when “creating labelled training material (speciﬁcally, for them, for HPSG parse se27 Model Form+POS: Lemma+POS: Form+Lemma+POS: Form+CPOS: Form+Lemma+CPOS: Form+CPOS+POS: Lemma+CPOS+POS: Lemma+CPOS: Form+Lemma+CPOS+POS: LAS-1 60.6 61.3 61.5 62.1 62.9 63.0 63.1 63.3 63.3 UAS-1 70.3 70.8 70.8 72.5 72.6 72.9 72.4 72.7 73.1 LAS-2 64.4 64.6 64.6 65.0 66.1 66.0 66.0 65.1 66.5 UAS-2 74.2 74.3 74.5 76.1 76.2 76.0 76.2 75.7 76.3 Table 3: Preliminary MaltParser experiments with the Irish Dependency Treebank: Pre- and post-IAA-2 results lection) and later reusing it with other models, gains from"
U12-1005,D09-1031,0,0.0235575,"64.4 64.6 64.6 65.0 66.1 66.0 66.0 65.1 66.5 UAS-2 74.2 74.3 74.5 76.1 76.2 76.0 76.2 75.7 76.3 Table 3: Preliminary MaltParser experiments with the Irish Dependency Treebank: Pre- and post-IAA-2 results lection) and later reusing it with other models, gains from active learning may be negligible or even negative”: the simulation of active learning on an existing treebank under a particular model, with the goal of improving parser accuracy, may not correspond to a useful approach to constructing a treebank. Second, in the actual task of constructing a resource — interlinearized glossed text — Baldridge and Palmer (2009) show that the usefulness of particular example selection techniques in active learning varies with factors such as annotation expertise. They also note the importance of measures that are sensitive to the cost of annotation: the sentences that active learning methods select are often difﬁcult to annotate as well, and may result in no effective savings in time or other measures. To our knowledge, active learning has not yet been applied to the actual construction of a treebank: that is one of our goals. Further, most active learning work in NLP has used variants of QBU and QBC where instances"
U12-1005,baldwin-etal-2004-road,0,0.0693434,"et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decreases annotation cost, measured both in number of sentences to achieve a particular level of parse selection accuracy, and in a measure of sentence complexity, with respect to random selection. However, this differs from the task of constructing"
U12-1005,C10-1011,0,0.0386426,"rs Thus, we rank the set of X trees (up ) based on their disagreement with a second reference parser.3 The 3 This assessment of disagreement between two trees is based on the number of dependency relations they disagree on, which is the fundamental idea of the F-complement measure of Ngai and Yarowsky (2000). Disagreement between 28 top Y trees from this ordered set are manually revised and added to the training set for the next iteration. We use MaltParser as the only parser in the passive learning setup and the main parser in the active learning setup. We use another dependency parser Mate (Bohnet, 2010) as our second parser in the active learning setup. Since we have 450 gold trees, we split them into a seed training set of 150 trees, a development set of 150 and a test set of 150. Due to time constraints we run the two versions of the algorithm for four iterations, and on each iteration 50 (Y) parse trees are handcorrected from a set of 200 (X). This means that the ﬁnal training set size for both setups is 350 trees (150 + (4*50)). However, the 4*50 training trees added to the seed training set of 150 are not the same for both setups. The set of 200 unseen sentences in each iteration is the"
U12-1005,N06-1016,0,0.0324761,"general technique applicable to many tasks involving machine learning. Two broad approaches are Query By Uncertainty (QBU) (Cohn et al., 1994), where examples about which the learner is least conﬁdent are selected for manual annotation; and Query By Committee (QBC) (Seung et al., 1992), where disagreement among a committee of learners is the criterion for selecting examples for annotation. Active learning has been used in a number of areas of NLP such as information extraction (Scheffer et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features de"
U12-1005,W08-1301,0,0.0917332,"arser (Nivre et al., 2006) on their original treebank of 300 sentences. Following the changes we made to the labelling scheme as a result of the second IAA study, we re-ran the same parsing experiments on the newly updated seed set of 300 sentences. We used 10fold cross-validation on the same feature sets (various combinations of form, lemma, ﬁne-grained POS and coarse-grained POS). The improved results, as shown in the ﬁnal two columns of Table 3, reﬂect the value of undertaking an analysis of IAA-1 results. 2 This label is also used in the English Stanford Dependency Scheme (de Marneffe and Manning, 2008) 3 Active Learning Experiments Now that the annotation scheme and guide have reached a stable state, we turn our attention to the role of active learning in parser and treebank development. Before describing our preliminary work in this area, we discuss related work . 3.1 Related Work Active learning is a general technique applicable to many tasks involving machine learning. Two broad approaches are Query By Uncertainty (QBU) (Cohn et al., 1994), where examples about which the learner is least conﬁdent are selected for manual annotation; and Query By Committee (QBC) (Seung et al., 1992), where"
U12-1005,dzeroski-etal-2006-towards,0,0.0278089,"ppropriate labelling scheme, as well as considerable manual annotation (parsing). In general, manual annotation is desired to ensure high quality treebank data. Yet, as is often encountered when working with language, the task of manually annotating text can become repetitive, involving frequent encounters with similar linguistic structures. In an effort to speed up the creation of treebanks, there has been an increased focus towards automating, or at least, semi-automating the process using various bootstrapping techniques. A basic bootstrapping approach such as that outlined by Judge et al. (2006) involves several steps. Firstly a parser is trained on a set of gold standard trees. This parser is then used to parse a new set of unseen sentences. When these new trees are reviewed and corrected, they are combined with the ﬁrst set of trees and used to train a new parsing model. These steps are repeated until all sentences are parsed. By adding to the training data on each iteration, the parser is expected to improve progressively. The process of correcting Teresa Lynn, Jennifer Foster, Mark Dras and Elaine U´ı Dhonnchadha. 2012. Active Learning and the Irish Treebank. In Proceedings of Au"
U12-1005,P06-1063,0,0.0779378,"Missing"
U12-1005,lynn-etal-2012-irish,1,0.776429,"Missing"
U12-1005,P00-1016,0,0.0540267,"atch of X unseen sentences. In the active learning variant, we select these trees based on a notion of how informative they are, i.e. how much the parser might be improved if it knew how to parse them correctly. We approximate informativeness based on QBC, speciﬁcally, disagreement between a committee of two parsers Thus, we rank the set of X trees (up ) based on their disagreement with a second reference parser.3 The 3 This assessment of disagreement between two trees is based on the number of dependency relations they disagree on, which is the fundamental idea of the F-complement measure of Ngai and Yarowsky (2000). Disagreement between 28 top Y trees from this ordered set are manually revised and added to the training set for the next iteration. We use MaltParser as the only parser in the passive learning setup and the main parser in the active learning setup. We use another dependency parser Mate (Bohnet, 2010) as our second parser in the active learning setup. Since we have 450 gold trees, we split them into a seed training set of 150 trees, a development set of 150 and a test set of 150. Due to time constraints we run the two versions of the algorithm for four iterations, and on each iteration 50 (Y"
U12-1005,nivre-etal-2006-maltparser,0,0.248823,"Missing"
U12-1005,N04-1012,0,0.0207355,"Query By Committee (QBC) (Seung et al., 1992), where disagreement among a committee of learners is the criterion for selecting examples for annotation. Active learning has been used in a number of areas of NLP such as information extraction (Scheffer et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decrea"
U12-1005,P07-1052,0,0.0243787,"t al., 1992), where disagreement among a committee of learners is the criterion for selecting examples for annotation. Active learning has been used in a number of areas of NLP such as information extraction (Scheffer et al., 2001), text categorisation (Lewis and Gale, 1994; Hoi et al., 2006) and word sense disambiguation (Chen et al., 2006). Olsson (2009) provides a survey of various approaches to active learning in NLP. For our work, the most relevant application of active learning to NLP is in parsing, for example, Thompson et al. (1999), Hwa et al. (2003), Osborne and Baldridge (2004) and Reichart and Rappoport (2007). Taking Osborne and Baldridge (2004) as an illustration, the goal of that work was to improve parse selection for HPSG: for all the analyses licensed by the HPSG English Resource Grammar (Baldwin et al., 2004) for a particular sentence, the task is to choose the best one using a log-linear model with features derived from the HPSG structure. The supervised framework requires sentences annotated with parses, which is where active learning can play a role. Osborne and Baldridge (2004) apply both QBU with an ensemble of models, and QBC, and show that this decreases annotation cost, measured both"
W07-2204,P05-1022,0,0.224574,"Missing"
W07-2204,A00-2018,0,0.444658,"Missing"
W07-2204,J03-4003,0,0.0705179,"Missing"
W07-2204,W01-0521,0,0.468993,"Missing"
W07-2204,J93-2004,0,0.0282093,"Missing"
W07-2204,N06-1020,0,0.203344,"Missing"
W07-2204,P06-1043,0,0.355074,"Missing"
W07-2204,E03-1008,0,0.14938,"Missing"
W08-1122,P06-1130,1,0.892695,"Missing"
W08-1122,P04-1041,1,0.894093,"Missing"
W08-1122,W07-2204,1,0.869992,"Missing"
W08-1122,W01-0521,0,0.0306942,"ecific training data is automatically produced using state-of-the-art parser output. The retraining method recovers a substantial portion of the performance drop, resulting in a generator which achieves a BLEU score of 0.61 on our BNC test data. 1 Introduction Grammars extracted from the Wall Street Journal (WSJ) section of the Penn Treebank have been successfully applied to natural language parsing, and more recently, to natural language generation. It is clear that high-quality grammars can be extracted for the WSJ domain but it is not so clear how these grammars scale to other text genres. Gildea (2001), for example, has shown that WSJ-trained parsers suffer a drop in performance when applied to the more varied sentences of the Brown Corpus. We investigate the effect of domain variation in treebank-grammar-based generation by applying a WSJ-trained generator to sentences from the British National Corpus (BNC). As with probabilistic parsing, probabilistic generation aims to produce the most likely output(s) given In an initial evaluation, we apply our probabilistic WSJ-trained generator to BNC material, and show that the generator suffers a substantial performance degradation, with a drop in"
W08-1122,D07-1028,1,0.874936,"Missing"
W08-1122,A00-2023,0,0.0896334,"Missing"
W08-1122,N06-1020,0,0.0158375,"n our BNC test set. The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing. Some success has been achieved by training a parser, not on gold standard hand-corrected trees, but on parser output trees. These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al., 2003), or by the same parser with a reranking component in a type of selftraining scenario (McClosky et al., 2006). We tackle 165 the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees. This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61. The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced). 2 Background and a BLEU score of 0.7733. White et al. (2007) describe a CCG-based realisation system which has been trained on logical forms derived from CCGBank (H"
W08-1122,W05-1510,0,0.0468979,"Missing"
W08-1122,E03-1008,0,0.0275757,"ention to the problem of adapting the generator so that it can more accurately generate the 1,000 sentences in our BNC test set. The problem of adapting any NLP system to a domain different from the domain upon which it has been trained and for which no gold standard training material is available is a very real one, and one which has been the focus of much recent research in parsing. Some success has been achieved by training a parser, not on gold standard hand-corrected trees, but on parser output trees. These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al., 2003), or by the same parser with a reranking component in a type of selftraining scenario (McClosky et al., 2006). We tackle 165 the problem of domain adaptation in generation in a similar way, by training the generator on domain specific parser output trees instead of manually corrected gold standard trees. This experiment achieves promising results, with an increase in BLEU score from 0.54 to 0.61. The method is generic and can be applied to other probabilistic generators (for which suitable training material can be automatically produced). 2 Background and a BLEU score of 0.7733. White et al. ("
W08-1122,2005.mtsummit-papers.15,0,0.0313248,"Language Technology School of Computing Dublin City University Ireland {dhogan, jfoster, jwagner, josef}@computing.dcu.ie Abstract the input. We can distinguish three types of probabilistic generators, based on the type of probability model used to select the most likely sentence. The first type uses an n-gram language model, e.g. (Langkilde, 2000), the second type uses a probability model defined over trees or feature-structureannotated trees, e.g. (Cahill and van Genabith, 2006), and the third type is a mixture of the first and second type, employing n-gram and grammarbased features, e.g. (Velldal and Oepen, 2005). The generator used in our experiments is an instance of the second type, using a probability model defined over Lexical Functional Grammar c-structure and f-structure annotations (Cahill and van Genabith, 2006; Hogan et al., 2007). While the effect of domain variation on Penntreebank-trained probabilistic parsers has been investigated in previous work, we study its effect on a Penn-Treebank-trained probabilistic generator. We show that applying the generator to data from the British National Corpus results in a performance drop (from a BLEU score of 0.66 on the standard WSJ test set to a BLE"
W08-1122,2007.mtsummit-ucnlg.4,0,0.046008,"Missing"
W08-1122,P05-1022,0,\N,Missing
W09-2112,bigert-2004-probabilistic,0,0.0146767,"ng synthetic corpus contains errors that look like the kind of slips that would be made by native speakers (e.g. repeated adjacent words) as well as errors that resemble learner errors (e.g. missing articles). Wagner et al. (2009) report a drop in accuracy for their classification methods when applied to real learner texts as opposed to held-out synthetic test data, reinforcing the earlier point that artificial errors need to be tailored for the task at hand (we return to this in Section 4.1). Artificial error data has also proven useful in the automatic evaluation of error detection systems. Bigert (2004) describes how a tool called Missplel is used to generate context-sensitive spelling errors which are then used to evaluate a context-sensitive spelling error detection system. The performance of general-purpose NLP tools such as part-of-speech taggers and parsers in the face of noisy ungrammatical data has been automatically evaluated using artificial error data. Since the features of machinelearned error detectors are often part-of-speech ngrams or word–word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-ofspeech ta"
W09-2112,briscoe-carroll-2002-robust,0,0.0524218,"e training data contains 438,150 sentences and the ungrammatical section 454,337. The classifiers are tested on a held-out section of the CLC containing 43,639 corrected CLC sentences and 45,373 original CLC sentences. To train the classifiers, the Mallet implementation of Naive Bayes is used.5 The features are word unigrams and bigrams, as well as part-of-speech unigrams, bigrams and trigrams. Andersen (2006) experimented with various learning algorithms and, taking into account training time and performance, found Naive Bayes to be optimal. The POS-tagging is carried out by the RASP system (Briscoe and Carroll, 2002). sentences which were used to generate the fauxCLC set were tagged with the CLAWS tagset, and although more fine-grained than the Penn tagset, it does not, for example, make a distinction between mass and count nouns, a common source of error. Another important reason for the drop in accuracy are the recurrent spelling errors which occur in the incorrect CLC test set but not in the faux-CLC test set. It is promising, however, that much of the performance degradation is recovered when a mixture of the two types of ungrammatical training data is used, suggesting that artificial data could be us"
W09-2112,P06-1032,0,0.486417,"onal Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and their parse trees are examined. If the parse trees resemble the “disturbed” trees that st"
W09-2112,P06-4001,0,0.0607591,"Missing"
W09-2112,C08-1022,0,0.101643,"Missing"
W09-2112,I08-1059,0,0.0335923,"Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be"
W09-2112,P08-1021,0,0.35967,"Colorado, June 2009. 2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and their parse trees are examined. If"
W09-2112,P07-1010,0,0.118225,"om the ungrammatical at the sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors. Distinguishing grammatical utterances from ungrammatical ones involves the use of a binary classifier or a grammaticality scor1 http://www.cambridge.org/elt/corpus/ learner_corpus2.htm Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a patter"
W09-2112,P05-1044,0,0.01227,"artificial context-sensitive spelling errors into error-free 84 Swedish text and then evaluate parsers and a partof-speech tagger on this text using their performance on the error-free text as a reference. Similarly, Foster (2007) investigates the effect of common English grammatical errors on two widely-used statistical parsers using distorted treebank trees as references. The procedure used by Wagner et al. (2007; 2009) is used to introduce errors into the treebank sentences. Finally, negative evidence in the form of automatically distorted sentences has been used in unsupervised learning. Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. Since the aim of this work is not to detect grammatical errors, there is no requirement to generate the kind of negative evidence that might actually be produced by either native or non-native speakers of a language. The negative examples are used to guide the unsupervised learning of a part-of-speech tagger and a dependency grammar. We can conclude from this survey that synthetic error data is useful in a variety of NLP applications, including error detection and evaluation"
W09-2112,P07-1011,0,0.0405149,"entence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors. Distinguishing grammatical utterances from ungrammatical ones involves the use of a binary classifier or a grammaticality scor1 http://www.cambridge.org/elt/corpus/ learner_corpus2.htm Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of we"
W09-2112,C08-1109,0,0.0512141,"ional Applications, pages 82–90, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and t"
W09-2112,D07-1012,1,0.223378,"Missing"
W09-3827,P05-1022,0,0.144754,"ted 500 sentences of the transcribed utterances, producing a parallel error corpus which we call Gonzaga 500. The third parallel corpus contains 199,600 sentences taken from the British National Corpus and ungrammatical sentences produced by introducing errors of the following five types into the original BNC sentences: errors involving an extra word, errors involving a missing word, realword spelling errors, agreement errors and errors involving an incorrect verbal inflection. All sentence pairs in the three parallel corpora are parsed using the June 2006 version of the first-stage parser of Charniak and Johnson (2005), a lexicalised, generative, probabilistic parser achieving competitive performance on Wall 177 Figure 2: Effect of correcting erroneous sentences (Foster corpus) on the probability of the best parse. Each bar is broken down by whether and how the correction changed the sentence length in tokens. A bar labelled x covers ratios from ex−2 to ex+2 (exclusive). Figure 3: Effect of correcting erroneous sentences (Gonzaga 500 corpus) on the probability of the best parse. Figure 4: Effect of inserting errors into BNC sentences on the probability of the best parse. 178 errors in the Foster and Gonzaga"
W09-3827,P08-2056,1,0.891864,"Missing"
W09-3827,P08-1021,0,0.116181,"Missing"
W09-3827,P07-1011,0,0.0288017,"erhaps less useful as they fail to reject ungrammatical strings. A naive solution might be to assume that the probability assigned to a parse tree by its probabilistic model could be leveraged in some way to determine the sentence’s grammaticality. In this paper, we explore one aspect of this question by using three parallel error corpora to determine the effect of common English grammatical errors on the parse probability of the most likely parse tree returned by a generative probabilistic parser. 2 Related Work The probability of a parse tree has been used before in error detection systems. Sun et al. (2007) report only a very modest improvement when they include a parse probability feature in their system whose features mostly consist of linear sequential patterns. Lee and Seneff (2006) detect ungrammatical sentences by comparing the parse probability of a possibly ill-formed input sentence to the parse probabilities of candidate corrections which are generated by arbitrarily deleting, inserting and substituting articles, prepositions and auxiliaries and changing the inflection of verbs and nouns. Foster et al. (2008) compare the parse probability returned by a parser trained on a regular treeba"
W10-1401,P05-1038,0,0.0195671,"arily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 on N EGRA. They showed that usi"
W10-1401,W10-1408,1,0.785091,"Missing"
W10-1401,W10-1404,0,0.236375,"s is substantial lexical data sparseness due to high morphological variation in surface forms. The question is therefore, given our finite, and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of stat"
W10-1401,H91-1060,0,0.0333452,"Missing"
W10-1401,E03-1005,0,0.0521711,"Missing"
W10-1401,W06-2920,0,0.219462,"red MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. While dependency parsers are adaptable to many languages, as reflected in the multiplicity of the languages covered,1 the analysis by Nivre et al. (2007b) shows that the best result was obtained for English, followed by Catalan, and that the most difficult languages to parse were Arabic, Basque, and Greek. Nivre et al. (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse. This was shown to be the case for both MaltParser (Nivre e"
W10-1401,W10-1409,1,0.834713,"and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of"
W10-1401,W08-2102,0,0.0218025,"Missing"
W10-1401,A00-2018,0,0.0303149,"Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologic"
W10-1401,P00-1058,0,0.0168692,"Missing"
W10-1401,W10-1406,0,0.0567829,"Missing"
W10-1401,P99-1065,0,0.261618,"Missing"
W10-1401,P97-1003,0,0.183573,"r) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases. German, an Indo-European language closely related to English, already exhibits some of the properties that make parsing MRLs problematic. The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lex"
W10-1401,H05-1100,0,0.0111384,"ful in parsing English are necessarily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 o"
W10-1401,2008.jeptalnrecital-long.17,1,0.829354,"Missing"
W10-1401,P03-1013,0,0.0556766,"rted to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s"
W10-1401,P05-1039,0,0.0235903,"cal forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not"
W10-1401,P08-1109,0,0.0528707,"Missing"
W10-1401,W10-1412,1,0.240586,"various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of the SPMRL workshop for addressing such challenges. 4 Parsing MRLs: Recurring Trends The first workshop on parsing MRLs features 11"
W10-1401,E09-1038,1,0.7929,"isambiguate the morphological analyses of input forms? Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming that the input to our system reflects morphological information, one way or another, which types of morpho2 Most studies on parsing MRLs nowadays assume the gold standard segmentation and disambiguated morphological information as input. This is the case, for instance, for the Arabic parsing at CoNLL 2007 (Nivre et al., 2007a). This practice deludes the community as to the validity of the parsing results reported for MRLs in shared tasks. Goldberg et al. (2009), for instance, show a gap of up to 6pt F1 -score between performance on gold standard segmentation vs. raw text. One way to overcome this is to devise joint morphological and syntactic disambiguation frameworks (cf. (Goldberg and Tsarfaty, 2008)). logical information should we include in the parsing model? Inflectional and/or derivational? Case information and/or agreement features? How can valency requirements reflected in derivational morphology affect the overall syntactic structure? In tandem with the decision concerning the morphological information to include, we face genuine challenges"
W10-1401,W05-0303,0,0.043074,"Missing"
W10-1401,P08-1067,0,0.0516751,"Missing"
W10-1401,P03-1054,0,0.00472369,"rphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s parser emulation of Collins’ model 2 (Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (200"
W10-1401,W06-1614,0,0.154599,"Missing"
W10-1401,P95-1037,0,0.0299787,"Missing"
W10-1401,W10-1407,0,0.0148488,"elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance. In addition to free word order, MRLs show higher degree of freedom in extraposition. Both of these phenomena can result in discontinuous structures. In constituency-based treebanks, this is either annotated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that th"
W10-1401,J93-2004,0,0.0355629,". We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 1 Introduction The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers. The development of successful statistical parsing models for English focused on the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1993)) as the primary, and sometimes only, resource. Since the initial release of the Penn Treebank (PTB Marcus et Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks’ annotation schemes, and inadequacy of the widely used PARS E VAL evaluation metrics. None of these aspects in isolation can account for the systematic performance deterioration, but observed from a wider, crosslinguistic perspective, a picture begins to emerge – that the morphologically rich nature of some of the languages makes them inherently more s"
W10-1401,W10-1402,0,0.0399505,"Missing"
W10-1401,E06-1011,0,0.0266021,"labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the"
W10-1401,P05-1012,0,0.11798,"Missing"
W10-1401,P05-1013,0,0.0312259,"how (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with medium"
W10-1401,P09-1040,0,0.0260548,"D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare"
W10-1401,N07-1051,0,0.0143897,"markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on"
W10-1401,P06-1055,0,0.0941097,"tained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representatio"
W10-1401,D07-1066,1,0.534932,"Missing"
W10-1401,P81-1022,0,0.830446,"Missing"
W10-1401,W07-2219,1,0.909816,"Missing"
W10-1401,C08-1112,1,0.709478,"Missing"
W10-1401,W10-1405,1,0.846235,"Missing"
W10-1401,W09-3820,1,0.856092,"e other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare/unknown words is increased. One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflects their shared morphological and morphosyntactic aspects. This was done, for instance, in previous work on parsing German. Versley and Rehbein (2009) cluster words according to linear context features. These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes. The clusters are then integrated as features in a discriminative parsing model to cope with unknown words. Their discriminative model thus obtains state-of-the-art results on parsing German. 8 Several contribution address similar challenges. For constituency-based generative parsers, the simple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al., 2010; Candito and"
W10-1401,W10-1411,0,\N,Missing
W10-1401,W10-1403,0,\N,Missing
W10-1401,W10-1410,1,\N,Missing
W10-1401,W08-1008,0,\N,Missing
W10-1401,P05-1022,0,\N,Missing
W10-1401,P08-1043,1,\N,Missing
W10-1401,D07-1096,0,\N,Missing
W10-1408,P05-1038,0,0.0366439,"the only edge which is added to the chart at this position is the one corresponding to the rule V BD → UNK-ed. For our English experiments we use the unknown word classes (or signatures) which are used in the Berkeley parser. A signature indicates whether a words contains a digit or a hyphen, if a word starts with a capital letter or ends with one of the following English suffixes (both derivational and inflectional): -s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al. For our French experiments we employ the same signature list as Crabb´e and Candito (2008), which itself was adapted from Arun and Keller (2005). This list consists of (a) conjugation suffixes of reguIn order to use morphological clues for Arabic we go further than just looking at suffixes. We exploit all the richness of the morphology of this language which can be expressed through morphotactics. these indicators are prefixes, suffixes and word templates. A template (Beesley and Karttunen, 2003) is a kind of vocalization mould in which a word fits. In derivational morphology Arabic words are formed through the amalgamation of two tiers, namely, root and template. A root is a sequence of three (rarely two or four) consonants which are"
W10-1408,W98-1007,0,0.0593928,"signature information for French and English is shown in Table 3. Beside each f-score the absolute improvement over the UNKNOWN baseline (Table 2) is given. For both languages there is an improvement at all unknown thresholds. The improvement for English is statistically significant at unknown thresholds 1 and 10.4 The improvement is more marked for French and is statistically significant at all levels. In the next section, we experiment with signature lists for Arabic.5 6 Arabic Signatures Handling Arabic Morphotactics Morphotactics refers to the way morphemes combine together to form words (Beesley, 1998; Beesley and Karttunen, 2003). Generally speaking, morphotactics can be concatenative, with morphemes either prefixed or suffixed to stems, or non-concatenative, with stems undergoing internal alternations to convey morphosyntactic information. Arabic is considered a typical example of a language that employs non-concatenative morphotactics. Arabic words are traditionally classified into three types: verbs, nouns and particles. Adjectives take almost all the morphological forms of, and share the same templatic structures with, nouns. Adjectives, for example, can be definite, and are inflected"
W10-1408,W09-3821,0,0.223368,"Missing"
W10-1408,W09-1008,0,0.0505203,"Missing"
W10-1408,A00-2018,0,0.244617,"61 94.90 92.99 91.56 Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model of the words in the Arabic and French development sets are unknown, and this is reflected in the drop in parsing performance at these thresholds. 5 Making use of Morphology Unknown words are not all the same. We exploit this fact by examining the effect on parsing accuracy of clustering rare training set words using cues from the word’s morphological structure. Affixes have been shown to be useful in part-of-speech tagging (Schmid, 1994; Tseng et al., 2005) and have been used in the Charniak (Charniak, 2000), Stanford (Klein and Manning, 2003) and Berkeley (Petrov et al., 2006) parsers. In this section, we contrast the effect on parsing accuracy of making use of such information for our three languages of interest. Returning to our toy English example in Figures 1 and 2, and given the input sentence The shares recovered, we would like to use the fact that the un70 known word recovered ends with the past tense suffix -ed to boost the probability of the lexical rule V BD → UNKNOWN. If we specialise the UNKNOWN terminal using information from English morphology, we can do just that, resulting in the"
W10-1408,2008.jeptalnrecital-long.17,0,0.145906,"Missing"
W10-1408,E09-1038,0,0.160992,"Missing"
W10-1408,P08-2015,0,0.0114925,"Missing"
W10-1408,D09-1087,0,0.13706,"Missing"
W10-1408,J98-4004,0,0.381711,"hnique is extended to include morphological information and present parsing results for English and French. In Section 6, we describe the Arabic morphological system and explain how we used heuristic rules to cluster words into word-classes or signatures. We present parsing results for the version of the parser which uses this information. In Section 7, we describe our attempts to automatically determine the signatures for a language and present parsing results for the three languages. Finally, in Section 8, we discuss how this work might be fruitfully extended. 2 Latent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations a"
W10-1408,W04-1602,0,0.0128084,"Missing"
W10-1408,H94-1020,0,0.166217,"Missing"
W10-1408,P05-1010,0,0.1217,"uses this information. In Section 7, we describe our attempts to automatically determine the signatures for a language and present parsing results for the three languages. Finally, in Section 8, we discuss how this work might be fruitfully extended. 2 Latent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. 1. Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. E"
W10-1408,W05-0711,0,0.0499235,"Missing"
W10-1408,N07-1051,0,0.0697355,"each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. Evaluate the impact of the newly created annotations and discard the least useful ones. Reestimate probabilities with the new set of annotations. 3. Smooth the probabilities to prevent overfitting. We use our own parser which trains a PCFG-LA using the above procedure and parses using the max1 Estimation of the parameters is performed by running Expectation/Maximisation on the training corpus. 68 rule parsing algorithm (Petrov et al., 2006; Petrov and Klein, 2007). PCFG-LA parsing is relatively language-independent but has been shown to be very effective on several languages (Petrov, 2009). For our experiments, we set the number of iterations to be 5 and we test on sentences less than or equal to 40 words in length. All our experiments, apart from the final one, are carried out on the development sets of our three languages. 3 The Datasets Arabic We use the the Penn Arabic Treebank (ATB) (Bies and Maamouri, 2003; Maamouri and Bies., 2004). The ATB describes written Modern Standard Arabic newswire and follows the style and guidelines of the English Penn"
W10-1408,P06-1055,0,0.487124,"tent Variable PCFG Parsing Johnson (1998) showed that refining treebank categories with parent information leads to more accurate grammars. This was followed by a collection of linguistically motivated propositions for manual or semi-automatic modifications of categories in treebanks (Klein and Manning, 2003). In PCFG-LAs, first introduced by Matsuzaki et al. (2005), the refined categories are learnt from the treebank using unsupervised techniques. Each base category – and this includes part-of-speech tags – is augmented with an annotation that refines its distributional properties. Following Petrov et al. (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. 1. Split each annotation of each symbol into n (usually 2) new annotations and create rules with the new annotated symbols. Estimate1 the probabilities of the newly created rules. 2. Evaluate the impact of the newly created annotations and discard the least useful ones. Reestimate probabilities with the new set of annotations. 3. Smooth the probabilities to prevent overfitting. We use our own parser which trains a PCFG-LA using the"
W10-1408,I05-3005,0,0.0234663,"URE UNTagging Accuracy 94.03 91.16 89.06 95.60 94.66 93.61 94.90 92.99 91.56 Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model of the words in the Arabic and French development sets are unknown, and this is reflected in the drop in parsing performance at these thresholds. 5 Making use of Morphology Unknown words are not all the same. We exploit this fact by examining the effect on parsing accuracy of clustering rare training set words using cues from the word’s morphological structure. Affixes have been shown to be useful in part-of-speech tagging (Schmid, 1994; Tseng et al., 2005) and have been used in the Charniak (Charniak, 2000), Stanford (Klein and Manning, 2003) and Berkeley (Petrov et al., 2006) parsers. In this section, we contrast the effect on parsing accuracy of making use of such information for our three languages of interest. Returning to our toy English example in Figures 1 and 2, and given the input sentence The shares recovered, we would like to use the fact that the un70 known word recovered ends with the past tense suffix -ed to boost the probability of the lexical rule V BD → UNKNOWN. If we specialise the UNKNOWN terminal using information from Engli"
W10-1408,P03-1054,0,\N,Missing
W11-0804,W09-3821,0,0.0891431,"Missing"
W11-0804,W10-1410,1,0.846662,"is the integration of named 1 It is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce a category for say names of people if examples of such words occurred in similar syntactic patterns in the data. Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences of UNKNOWN tokens and the lexicon contains productions for such tokens (X → UNKNOWN), with"
W11-0804,A00-2018,0,0.35169,"ers and a latent variable constituency parser is trained and tested on the transformed corpus. We explore two different methods for mapping words to entities, and look at the effect of mapping various subsets of named entity types. Thus far, results show no improvement in parsing accuracy over the best baseline score; we identify possible problems and outline suggestions for future directions. 1 Introduction Techniques for handling lexical data sparsity in parsers have been important ever since the lexicalisation of parsers led to significant improvements in parser performance (Collins, 1999; Charniak, 2000). The original treebank set of non-terminal labels is too general to give good parsing results. To overcome this problem, in lexicalised constituency parsers, non-terminals are enriched with lexical information. Lexicalisation of the grammar vastly increases the number of parameters in the model, spreading the data over more specific events. Statistics based on low frequency events are not as reliable as statistics on phenomena which occur regularly in the data; frequency counts involving words are typically sparse. Word statistics are also important in more recent unlexicalised approaches to"
W11-0804,N09-1037,0,0.0822447,"Missing"
W11-0804,P03-1054,0,0.119556,". Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences of UNKNOWN tokens and the lexicon contains productions for such tokens (X → UNKNOWN), with associated probabilities. When faced with a word in the test set that the parser has not seen in its training set - the unknown word is mapped to the special UNKNOWN token. In syntactic parsing, rather than map all low frequency words to one generic UNKNOWN type, it is"
W11-0804,N10-1089,0,0.12767,"rt introduction of the named entity resource used in our experiments and a description of the types of basic entity mappings we examine. In §3.1 and §3.2 we describe the two different types of mapping technique. Results are presented in Section 4, followed by a brief discussion in Section 5 indicating possible problems and avenues worth pursuing. Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-"
W11-0804,P05-1010,0,0.0539307,"Missing"
W11-0804,P06-1055,0,0.24327,"patterns in the data. Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences of UNKNOWN tokens and the lexicon contains productions for such tokens (X → UNKNOWN), with associated probabilities. When faced with a word in the test set that the parser has not seen in its training set - the unknown word is mapped to the special UNKNOWN token. In syntactic parsing, rather than map all low frequency words to one g"
W11-0804,N09-2041,0,0.021122,"es, different types of MWUs, as well as different evaluation methods. Other relevant work is the integration of named 1 It is true that latent variable parsers automatically induce categories for similar words, and thus might be expected to induce a category for say names of people if examples of such words occurred in similar syntactic patterns in the data. Nonetheless, the problem of data sparsity remains - it is difficult even for latent variable parsers to learn accurate patterns based on words which only occur say once in the training set. 15 entity types in a surface realisation task by Rajkumar et al. (2009) and the French parsing experiments of (Candito and Crabb´e, 2009; Candito and Seddah, 2010) which involve mapping words to clusters based on morphology as well as clusters automatically induced via unsupervised learning on a large corpus. 2.1 Parsing unknown words Most state-of-the-art constituency parsers (e.g. (Petrov et al., 2006; Klein and Manning, 2003)) take a similar approach to rare and unknown words. At the beginning of the training process very low frequency words in the training set are mapped to special UNKNOWN tokens. In this way, some probability mass is reserved for occurrences"
W11-0804,I05-3005,0,0.0171282,"s to one generic UNKNOWN type, it is useful to have several different clusters of unknown words, grouped according to morphological and other ‘surfacey’ clues in the original word. For example, certain suffixes in English are strong predictors for the part-of-speech tag of the word (e.g. ‘ly’) and so all low frequency words ending in ‘ly’ are mapped to ‘UNKNOWN-ly’. As well as suffix information, UNKNOWN words are commonly grouped based on information on capitalisation and hyphenation. Similar techniques for handling unknown words have been used for POS tagging (e.g. (Weischedel et al., 1993; Tseng et al., 2005)) and are used in the Charniak (Charniak, 2000), Berkeley (Petrov et al., 2006) and Stanford (Klein and Manning, 2003) parsers, as well as in the parser used for the experiments in this paper, an in-house implementation of the Berkeley parser. 3 Experiments The BBN Entity Type Corpus (Weischedel and Brunstein, 2005) consists of sentences from the Penn WSJ corpus, manually annotated with named entities. The Entity Type corpus includes annotatype PERSON PER DESC FAC FAC DESC ORGANIZATION ORG DESC GPE GPE DESC LOCATION NORP PRODUCT PRODUCT DESC EVENT WORK OF ART LAW LANGUAGE CONTACT INFO PLANT AN"
W11-0804,D07-1110,0,0.0533878,"Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words. Results are difficult to compare however, due to different parsing methodologies, different types of MWUs, as well as different evaluation methods. Other relevant work is the integration of named 1 It is true that latent variable parsers automatically induc"
W11-0804,W10-3705,0,0.133234,"scribe the two different types of mapping technique. Results are presented in Section 4, followed by a brief discussion in Section 5 indicating possible problems and avenues worth pursuing. Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words. Results are difficult to compare however, due to different p"
W11-0804,J93-2006,0,0.273917,"ap all low frequency words to one generic UNKNOWN type, it is useful to have several different clusters of unknown words, grouped according to morphological and other ‘surfacey’ clues in the original word. For example, certain suffixes in English are strong predictors for the part-of-speech tag of the word (e.g. ‘ly’) and so all low frequency words ending in ‘ly’ are mapped to ‘UNKNOWN-ly’. As well as suffix information, UNKNOWN words are commonly grouped based on information on capitalisation and hyphenation. Similar techniques for handling unknown words have been used for POS tagging (e.g. (Weischedel et al., 1993; Tseng et al., 2005)) and are used in the Charniak (Charniak, 2000), Berkeley (Petrov et al., 2006) and Stanford (Klein and Manning, 2003) parsers, as well as in the parser used for the experiments in this paper, an in-house implementation of the Berkeley parser. 3 Experiments The BBN Entity Type Corpus (Weischedel and Brunstein, 2005) consists of sentences from the Penn WSJ corpus, manually annotated with named entities. The Entity Type corpus includes annotatype PERSON PER DESC FAC FAC DESC ORGANIZATION ORG DESC GPE GPE DESC LOCATION NORP PRODUCT PRODUCT DESC EVENT WORK OF ART LAW LANGUAGE"
W11-0804,W06-1206,0,0.0680142,"ues worth pursuing. Finally, we conclude. 2 Related Work Much previous work on parsing and multiword units (MWUs) adopts the words-with-spaces approach which treats MWUs as one token (by concatenating the words together) (Nivre and Nilsson, 2004; Cafferkey et al., 2007; Korkontzelos and Manandhar, 2010). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al., 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. Also related is work on MWUs and grammar engineering, such as (Zhang et al., 2006; Villavicencio et al., 2007) where automatically detected MWUs are added to the lexicon of a HPSG grammar to improve coverage. Our work is most similar to the words-withspaces approach. Our many-to-one experiments (see §3.1) in particular are similar to previous work on parsing words-with-spaces, except that we map words to entity types rather than concatenated words. Results are difficult to compare however, due to different parsing methodologies, different types of MWUs, as well as different evaluation methods. Other relevant work is the integration of named 1 It is true that latent variabl"
W11-0804,W06-1200,0,\N,Missing
W11-0804,J03-4003,0,\N,Missing
W11-2925,J04-4004,0,0.0407885,"reranker. Because we have approximately five times the Our dataset, summarised in Table 1, consists of a small treebank of hand-corrected phrase structure parse trees and two larger corpora of unannotated sentences. Discussion Forum Treebank The treebank is an extension of that described in Foster (2010). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009.1 The discussion forum posts were split into sentences by hand. The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser (Bikel, 2004). They were then corrected by hand using as a reference the Penn Treebank (PTB) bracketing guidelines (Bies et al., 1995) and the PTB trees themselves (Marcus et al., 1994). For more detail on the annotation process, see Foster et al. (2011). The development set contains 258 sentences and the test set 223. The experiments in this paper are carried out on the development set (which we refer to as FootballDev). Discussion Forum Corpus The same discussion forum used to create the treebank was scraped during the final quarter of 2010. The content was stripped of HTML markup and passed through an i"
W11-2925,P07-1078,0,0.0505586,"t the experiment with Web 2.0 data, believing that the two setups are sufficiently different for our experiment to be worthwhile — our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different in form (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con1 Introduction There have been several successful attempts in recent years to employ automatically parsed data in semi- and unsupervised approaches to parser domain adaptation (McClosky et al., 2006b; Reichart and Rappaport, 2007; Huang and Harper, 2009; Petrov et al., 2010). We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum. The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style. Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use"
W11-2925,A00-2018,0,0.282004,"Missing"
W11-2925,I11-1100,1,0.880246,"Missing"
W11-2925,N10-1060,1,0.857306,"retrain the first-stage parser using combinations of trees produced by the reranking parser for sentences from Sections 2 to 21 of the WSJ section of the Penn Treebank and from FootballTrainEdited|Discussion. We then parse the sentences in FootballDev using the retrained first-stage parser and the original reranker. Because we have approximately five times the Our dataset, summarised in Table 1, consists of a small treebank of hand-corrected phrase structure parse trees and two larger corpora of unannotated sentences. Discussion Forum Treebank The treebank is an extension of that described in Foster (2010). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009.1 The discussion forum posts were split into sentences by hand. The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser (Bikel, 2004). They were then corrected by hand using as a reference the Penn Treebank (PTB) bracketing guidelines (Bies et al., 1995) and the PTB trees themselves (Marcus et al., 1994). For more detail on the annotation process, see Foster et al. (2011). The development set contains 258 sentences and the"
W11-2925,D09-1087,0,0.0157088,"data, believing that the two setups are sufficiently different for our experiment to be worthwhile — our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different in form (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con1 Introduction There have been several successful attempts in recent years to employ automatically parsed data in semi- and unsupervised approaches to parser domain adaptation (McClosky et al., 2006b; Reichart and Rappaport, 2007; Huang and Harper, 2009; Petrov et al., 2010). We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum. The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style. Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use of edited, professional"
W11-2925,H94-1020,0,0.0413534,"ees and two larger corpora of unannotated sentences. Discussion Forum Treebank The treebank is an extension of that described in Foster (2010). It contains 481 sentences taken from two threads on the BBC Sport 606 discussion forum in November 2009.1 The discussion forum posts were split into sentences by hand. The sentences were first parsed automatically using an implementation of the Collins Model 2 generative statistical parser (Bikel, 2004). They were then corrected by hand using as a reference the Penn Treebank (PTB) bracketing guidelines (Bies et al., 1995) and the PTB trees themselves (Marcus et al., 1994). For more detail on the annotation process, see Foster et al. (2011). The development set contains 258 sentences and the test set 223. The experiments in this paper are carried out on the development set (which we refer to as FootballDev). Discussion Forum Corpus The same discussion forum used to create the treebank was scraped during the final quarter of 2010. The content was stripped of HTML markup and passed through an in-house sentence splitter and tokeniser, resulting in a corpus of 1,009,646 sentences. We call this the FootballTrainDiscussion corpus. Figure 1: Comparing the performance"
W11-2925,P08-2026,0,0.0401093,"Missing"
W11-2925,N06-1020,0,0.645374,"pus” (McClosky et al., 2006b). We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum. We find that grammars induced from the two automatically parsed corpora achieve similar Parseval fscores, with the grammars induced from the discussion forum material being slightly superior. An error analysis reveals that the two types of grammars do behave differently. 2 Related Work McClosky et al. (2006b) demonstrate that a WSJtrained parser can be adapted to the fiction domains of the Brown corpus by performing a type of self-training that involves the use of the twostage Brown reranking parser (Charniak and Johnson, 2005). Their training protocol is as follows: sentences from the LA Times are parsed using the first-stage parser (Charniak, 2000) and reranked in the second stage. These parse trees are added to the original WSJ training set and the first-stage parser is retrained. The sentences from the target domain, in this case, Brown corpus sentences are then parsed using the newly traine"
W11-2925,P06-1043,0,0.444935,"pus” (McClosky et al., 2006b). We compare the use of edited text in the form of newswire and unedited text in the form of discussion forum posts as sources for training material in a self-training experiment involving the Brown reranking parser and a test set of sentences from an online sports discussion forum. We find that grammars induced from the two automatically parsed corpora achieve similar Parseval fscores, with the grammars induced from the discussion forum material being slightly superior. An error analysis reveals that the two types of grammars do behave differently. 2 Related Work McClosky et al. (2006b) demonstrate that a WSJtrained parser can be adapted to the fiction domains of the Brown corpus by performing a type of self-training that involves the use of the twostage Brown reranking parser (Charniak and Johnson, 2005). Their training protocol is as follows: sentences from the LA Times are parsed using the first-stage parser (Charniak, 2000) and reranked in the second stage. These parse trees are added to the original WSJ training set and the first-stage parser is retrained. The sentences from the target domain, in this case, Brown corpus sentences are then parsed using the newly traine"
W11-2925,N10-1004,0,0.0141352,"+7.1) 4 (+14.0) (-7.4) (+2.9) 107 30 13 150 94 32 24 150 5 (+11.9) (-4.2) (+8.1) 5 (+9.5) (-1.6) (+5.7) 78 10 32 120 81 0 39 120 &gt;=6 (+17.2) (-5.7) (+9.7) 2308 4210 1222 7740 &gt;=6 (+10.4) (—) (-11.2) (+3.4) 1975 4807 958 7740 TOTAL (+13.6) (-9.4) (+2.6) TOTAL (+12.1) (-8.5) (+2.0) score for FootballDev increases from 83.2 to 85.6. When we include the baseline grammar (f-score: 79.7), this increases to 86.4. This suggests that the next step in our research is to build such a classifier including as features the sentential properties we examined in Section 5, as well as the features described in McClosky et al. (2010) and Ravi et al. (2008). • Self-training with both FootballTrainDiscussion and FootballTrainEdited data tends to benefit sentences containing several unknown words, with the discussion grammars being superior. Acknowledgements This research has been supported by Enterprise Ireland (CFTD/2007/229) and by Science Foundation Ireland (Grant 07/CE/ I1142) as part of the CNGL (www.cngl.ie) at the School of Computing, DCU. 6 Conclusion We compare the use of edited versus unedited text in the task of adapting a WSJ-trained parser to the noisy language of an online discussion forum. Given the small siz"
W11-2925,D10-1069,0,0.0122123,"two setups are sufficiently different for our experiment to be worthwhile — our bridging corpus is closely related in subject matter to our target corpus (both referring to the same events) but quite different in form (professionally edited versus an unedited mix of writing styles), whereas their bridging corpus is less closely related in con1 Introduction There have been several successful attempts in recent years to employ automatically parsed data in semi- and unsupervised approaches to parser domain adaptation (McClosky et al., 2006b; Reichart and Rappaport, 2007; Huang and Harper, 2009; Petrov et al., 2010). We turn our attention to adapting a Wall-Street-Journal-trained parser to user-generated content from an online sports discussion forum. The sentences on the discussion forum are produced by a group of speakers who are communicating with each other about a shared interest and are discussing the same events, but, who, given the open, unedited nature of the medium itself, do not follow an in-house writing style. Our particular aim in this paper is to compare the use of discussion forum comments as a source of unlabelled training material to the use of edited, professionally written sentences o"
W11-2925,D08-1093,0,0.0199424,"9) 107 30 13 150 94 32 24 150 5 (+11.9) (-4.2) (+8.1) 5 (+9.5) (-1.6) (+5.7) 78 10 32 120 81 0 39 120 &gt;=6 (+17.2) (-5.7) (+9.7) 2308 4210 1222 7740 &gt;=6 (+10.4) (—) (-11.2) (+3.4) 1975 4807 958 7740 TOTAL (+13.6) (-9.4) (+2.6) TOTAL (+12.1) (-8.5) (+2.0) score for FootballDev increases from 83.2 to 85.6. When we include the baseline grammar (f-score: 79.7), this increases to 86.4. This suggests that the next step in our research is to build such a classifier including as features the sentential properties we examined in Section 5, as well as the features described in McClosky et al. (2010) and Ravi et al. (2008). • Self-training with both FootballTrainDiscussion and FootballTrainEdited data tends to benefit sentences containing several unknown words, with the discussion grammars being superior. Acknowledgements This research has been supported by Enterprise Ireland (CFTD/2007/229) and by Science Foundation Ireland (Grant 07/CE/ I1142) as part of the CNGL (www.cngl.ie) at the School of Computing, DCU. 6 Conclusion We compare the use of edited versus unedited text in the task of adapting a WSJ-trained parser to the noisy language of an online discussion forum. Given the small size of our development se"
W11-2925,P05-1022,0,\N,Missing
W12-3117,P07-1111,0,0.0122582,"timation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation. Owczarzak et al. (2007) use labelled dependencies together with WordNet to avoid penalising valid syntactic and lexical variations in MT evaluation. In what follows, we describe how we make use of syntactic information in the QE task, i.e. evaluating MT output without a reference translation. Wagner et al."
W12-3117,W02-1503,0,0.00799189,"se of syntactic information in the QE task, i.e. evaluating MT output without a reference translation. Wagner et al. (2007; 2009) use three sources of linguistic information in order to extract features which they use to judge the grammaticality of English sentences: 1. For each POS n-gram (with n ranging from 2 to 7), a feature is extracted which represents the frequency of the least frequent n-gram in the sentence according to some reference corpus. TreeTagger (Schmidt, 1994) is used to produce POS tags. 2. Features provided by a hand-crafted, broad141 coverage precision grammar of English (Butt et al., 2002) and a Lexical Functional Grammar parser (Maxwell and Kaplan, 1996). These include whether or not a sentence could be parsed without resorting to robustness measures, the number of analyses found and the parsing time. 3. Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al., 1993), one trained on a distorted version of the treebank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions"
W12-3117,P01-1020,0,0.0369169,"ent Semantic Analysis (LSA, or Latent Semantic Indexing, LSI (Deerwester et al., 1990)). More recently, some studies were conducted on the use of LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011). Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore"
W12-3117,W07-0738,0,0.0257641,"Missing"
W12-3117,2011.mtsummit-papers.57,0,0.0136471,"Topic and Syntax-based Features In this section, we focus on the set of features that aim to capture adequacy using topic modelling and grammaticality using POS tagging and syntactic parsing. 1 http://www.microsofttranslator.com/ The list of English and Spanish rules is available at: http://languagetool.org/languages. 2 140 Topic-based Features guage model to a specific topic using Latent Semantic Analysis (LSA, or Latent Semantic Indexing, LSI (Deerwester et al., 1990)). More recently, some studies were conducted on the use of LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011). Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4"
W12-3117,P07-2045,0,0.00253308,"h • Number of source prepositions and conjunctions word: our assumption here is that segments containing a relatively high number of prepositions and conjunctions may be more complex and difficult to translate. • Number of source out-of-vocabulary words 139 Language Model Features All the language models (LMs) used in our work are n-gram LMs with Kneser-Ney smoothing built with the SRI Toolkit (Stolcke, 2002). • Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al. (2002) • Log probability of target segments on 5-gram MT-output-based LM: using M OSES (Koehn et al., 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. This MT output is used to build a LM that models the behavior of the MT system. We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes. MT-system Features • 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) • Number of n-bests for each source segment • MT output back-translation: from Spanish"
W12-3117,W05-0904,0,0.0194657,"a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation. Owczarzak et al. (2007) use labelled dependencies together with WordNet to avoid penalising valid syntactic and lexical variations in MT evaluation. In what follows, we describe how we make use of syntacti"
W12-3117,J93-2004,0,0.0395667,"frequent n-gram in the sentence according to some reference corpus. TreeTagger (Schmidt, 1994) is used to produce POS tags. 2. Features provided by a hand-crafted, broad141 coverage precision grammar of English (Butt et al., 2002) and a Lexical Functional Grammar parser (Maxwell and Kaplan, 1996). These include whether or not a sentence could be parsed without resorting to robustness measures, the number of analyses found and the parsing time. 3. Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al., 1993), one trained on a distorted version of the treebank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. These features were originally designed to distinguish grammatical sentences from ungrammatical ones and were tested on sentences from learner corpora by Wagner et al. (2009) and Wagner (2012). In this work we extract all three sets of features from the source side of our data and the POS-based subset from the target side.3 We use the publicly available pre-trained TreeTagger models fo"
W12-3117,D09-1092,0,0.0331213,"trics in further experiments, like the Manhattan or the Euclidean distances. Some parameters related to LDA have to be studied more carefully too, such as the number of topics (dimensions in the topic space), the number of words per topic, the Dirichlet hyperparameter α, etc. In our experiments, we built a topic model composed of 10 dimensions using Gibbs sampling with 1000 iterations. We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al., 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009). In the field of machine translation, Tam et al. (2007) propose to adapt a translation and a lanSource Syntax Features Wagner et al. (2007; 2009) propose a series of features to measure sentence grammaticality. These features rely on a part-of-speech tagger, a probabilistic parser and a precision grammar/parser. We have at our disposal these tools for English and so we apply them to the source data. The features themselves are described in more detail in Section 3.2. Target Syntax Features We use a part-of-speech tag"
W12-3117,W07-0714,0,0.0262538,"Missing"
W12-3117,P02-1040,0,0.105237,"that the MT output contains mistakes. This MT output is used to build a LM that models the behavior of the MT system. We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes. MT-system Features • 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) • Number of n-bests for each source segment • MT output back-translation: from Spanish to English using M OSES trained on the provided parallel corpus, scored with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and the Levenshtein distance (Levenshtein, 1966), based on the source segments as a translation reference Topic Model Features • Probability distribution over topics: Source and target segment probability distribution over topics for a 10-dimension topic model • Cosine distance between source and target topic vectors More details about these two features are provided in Section 3.1. 2.2 Unconstrained System In addition to the features used for the constrained system, a further 238 unconstrained features were included in our unconstrained system. MT System Features 3.1 As for our constrained s"
W12-3117,quirk-2004-training,0,0.129928,"f LDA to adapt SMT systems to specific domains (Gong et al., 2010; Gong et al., 2011) or to extract bilingual lexicon from comparable corpora (Rubino and Linar`es, 2011). Extracting features from a topic model is, to the best of our knowledge, the first attempt in machine translation quality estimation. 3.2 Syntax-based Features Syntactic features have previously been used in MT for confidence estimation and for building automatic evaluation measures. Corston-Oliver et al. (2001) build a classifier using 46 parse tree features to predict whether a sentence is a human translation or MT output. Quirk (2004) uses a single parse tree feature in the quality estimation task with a 4-point scale, namely whether a spanning parse can be found, in addition to LM perplexity and sentence length. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Albrecht and Hwa (2007) measure the syntactic similarity between MT output and reference translation and between MT output and a large monolingual corpus. Gimenez and Marquez (2007) explore lexical, syntactic and shallow semantic features and focus on measuring the similarity of MT output to reference translation. O"
W12-3117,2006.amta-papers.25,0,0.0363423,"orpus into Spanish, assuming that the MT output contains mistakes. This MT output is used to build a LM that models the behavior of the MT system. We assume that for a given MT output, a high n-gram probability (or a low perplexity) of the LM indicates that the MT output contains mistakes. MT-system Features • 15 scores provided by Moses: phrase-table, language model, reordering model and word penalty (weighted and unweighted) • Number of n-bests for each source segment • MT output back-translation: from Spanish to English using M OSES trained on the provided parallel corpus, scored with TER (Snover et al., 2006), BLEU (Papineni et al., 2002) and the Levenshtein distance (Levenshtein, 1966), based on the source segments as a translation reference Topic Model Features • Probability distribution over topics: Source and target segment probability distribution over topics for a 10-dimension topic model • Cosine distance between source and target topic vectors More details about these two features are provided in Section 3.1. 2.2 Unconstrained System In addition to the features used for the constrained system, a further 238 unconstrained features were included in our unconstrained system. MT System Feature"
W12-3117,D07-1012,1,0.896831,"Missing"
W12-3117,P05-1022,0,\N,Missing
W13-1106,baccianella-etal-2010-sentiwordnet,0,0.029318,"Missing"
W13-1106,W12-3704,1,0.212365,"Missing"
W13-1106,W11-3702,0,0.333359,"Missing"
W13-1106,W10-3110,0,0.0309123,"Missing"
W13-1106,C10-2028,0,0.0503993,"Missing"
W13-1106,W08-1301,0,0.00985016,"Missing"
W13-1106,P11-2008,0,0.0137244,"Missing"
W13-1106,C00-1044,0,0.30785,"Missing"
W13-1106,P03-1054,0,0.00969342,"lled as either positive, negative or neutral, i.e. non-relevant, mixed-sentiment and non-English tweets are discarded. We also simplify our task by omitting those tweets which have been flagged as sarcastic by one or both of the annotators, leaving a set of 2,624 tweets with a class distribution as shown in Table 1. 5 Tools and Resources In the course of our experiments, we use two different subjectivity lexicons, one part-of-speech tagger and one parser. For part-of-speech tagging we use a tagger (Gimpel et al., 2011) designed specifically for tweets. For parsing, we use the Stanford parser (Klein and Manning, 2003). To identify the sentiment polarity of a word we use: 1. Subjectivity Lexicon (SL) (Wilson et al., 2005): This lexicon contains 8,221 words (6,878 unique forms) of which 3,249 are adjectives, 330 are adverbs, 1,325 are verbs, 2,170 are nouns and remaining (1,147) words are marked as anypos. There are many words which occur with two or more different part-ofspeech tags. We extend SL with 341 domainspecific words to produce an extended SL. 2. SentiWordNet 3.0 (SWN) (Baccianella et al., 2010): With over 100+ thousand words, SWN is far larger than SL but is likely to be noisier since it has been"
W13-1106,pak-paroubek-2010-twitter,0,0.0347951,"Missing"
W13-1106,W02-1011,0,0.0282086,"Missing"
W13-1106,P02-1053,0,0.0397768,"Missing"
W13-1106,P12-3020,0,0.00874847,"sentiment detection to predict the vote percentage for each of the candidates in the Singapore presidential election of 2011. They devise a formula to calculate the percentage vote each candidate will receive using census information on variables such as age group, sex, location, etc. They combine this with a sentiment-lexicon-based sentiment analysis engine which calculates the sentiment in each tweet and aggregates the positive and negative sentiment for each candidate. Their model was able to predict the narrow margin between the top two candidates but failed to predict the correct winner. Wang et al. (2012) proposed a real-time sentiment analysis system for political tweets which was based on the U.S. presidential election of 2012. They colPrevious Work The related work can be divided into two groups, general sentiment analysis research and research which is devoted specifically to the political domain. 2.1 General Sentiment Analysis Research in the area of sentiment mining started with product (Turney, 2002) and movie (Pang et al., 2002) reviews. Turney (2002) used Pointwise Mutual Information (PMI) to estimate the sentiment orientation of phrases. Pang et al. (2002) employed supervised learnin"
W13-1106,H05-1044,0,0.0910328,"parliament were collected. LIWC 2007 (Pennebaker et al., 2007) was then used to extract sentiment from the tweets. LIWC is a text analysis software developed to assess emotional, cognitive and structural components of text samples using a psychometrically validated internal dictionary. Tumasjan et al. concluded that the number of tweets/mentions of a party is directly proportional to the probability of winning the elections. O’Connor et al. (2010) investigated the extent to which public opinion polls were correlated with political sentiment expressed in tweets. Using the Subjectivity Lexicon (Wilson et al., 2005), they estimate the daily sentiment scores for each entity. A tweet is defined as positive if it contains a positive word and vice versa. A sentiment score for that day is calculated as the ratio of the positive count over the negative count. They find that their sentiment scores were correlated with opinion polls on presidential job approval but less strongly with polls on electoral outcome. Choy et al. (2011) discuss the application of online sentiment detection to predict the vote percentage for each of the candidates in the Singapore presidential election of 2011. They devise a formula to"
W13-2249,P07-2045,0,0.00595314,"nslation options and the number of nodes in the decoding graph. The set of topic model features was reduced in order to keep only those that were shown to be effective on three quality estimation datasets (the details can be found in (Rubino et al. (to appear), 2013)). These features encode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features. Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (S"
W13-2249,P02-1040,0,0.0981654,"ar), 2013)). These features encode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features. Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (Schmidt, 1994) and the publicly available pre-trained models for English and Spanish. We mapped the tagsets of both languages by simplifying the initial tags and obtain a reduced set of 8 tags. We applied that simplification on the tagged sentences before check"
W13-2249,W12-3117,1,0.938067,"1.1 involve estimating postediting effort for English-Spanish translation pairs in the news domain. The two systems use a wide variety of features, of which the most effective are the word-alignment, n-gram frequency, language model, POS-tag-based and pseudoreferences ones. Both systems perform at a similarly high level in the two tasks of scoring and ranking translations, although there is some evidence that the systems are over-fitting to the training data. 1 2 Features Our starting point for the WMT13 QE shared task was the feature set used in the system we submitted to the WMT12 QE task (Rubino et al., 2012). This feature set, comprising 308 features in total, extended the 17 baseline features provided by the task organisers to include 6 additional surface features, 6 additional language model features, 17 additional features derived from the MT system components and the n-best lists, 138 features obtained by part-of-speech tagging and parsing the source sentences and 95 obtained by part-of-speech tagging the target sentences, 21 topic model features, 2 features produced by a grammar checker1 and 6 pseudo-source (or backtranslation) features. We made the following modifications to this 2012 featu"
W13-2249,2006.amta-papers.25,0,0.0629642,"ncode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and without Part-Of-Speech (POS) agreement, were included as features. Using the alignment information provided by the decoder, we POS tagged the source and target sentences with TreeTagger (Schmidt, 1994) and the publicly available pre-trained models for English and Spanish. We mapped the tagsets of both languages by simplifying the initial tags and obtain a reduced set of 8 tags. We applied that simplification on the tagged sentences before checking for POS agreement. To e"
W13-2249,W12-3118,0,0.0833151,"g the number of target words aligned with source words. We extracted 8 additional features from the decoder log file, including the number of discarded hypotheses, the total number of translation options and the number of nodes in the decoding graph. The set of topic model features was reduced in order to keep only those that were shown to be effective on three quality estimation datasets (the details can be found in (Rubino et al. (to appear), 2013)). These features encode the difference between source and target topic distributions according to several distance/divergence metrics. Following Soricut et al. (2012), we employed pseudo-reference features. The source sentences were translated with three different MT systems: an in-house phrase-based SMT system built using Moses (Koehn et al., 2007) and trained on the parallel data provided by the organisers, the rule-based system Systran2 and the online, publicly available, Bing Translator3 . The obtained translations are compared to the target sentences using sentence-level BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and the Levenshtein distance (Levenshtein, 1966). Also following Soricut et al. (2012), oneto-one word-alignments, with and wit"
W13-2249,2013.mtsummit-posters.13,1,\N,Missing
W13-4901,A00-2018,0,0.108553,"eriments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank"
W13-4901,D09-1087,0,0.0164299,"g set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 tokens. The motivation for this is two-fold: (i) we had difficulties training Mate parser with long sentences due to memory resource issues, and (ii)"
W13-4901,U12-1005,1,0.741841,"ey later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 tokens. The motivation for this is two-fold: (i) we had difficulties training Mate parser with long sentences due to memory resource issues, and (ii) in keeping with the findings of Lynn et al. (2012b), the large trees were sentences from legislative text that were difficult to analyse for automatic parsers and human annotators. This leaves us with 500 gold-standard trees as our seed training data set. For our unlabelled data, we take the next 1945 sentences"
W13-4901,N06-1020,0,0.029392,"well known semi-supervised techniques. 5.1 Self-Training 5.1.1 Related Work Self-training, the process of training a system on its own output, has a long and chequered history in parsing. Early experiments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-trai"
W13-4901,C08-1071,0,0.0167963,"also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 t"
W13-4901,D07-1013,0,0.0693299,"Missing"
W13-4901,nivre-etal-2006-maltparser,0,0.223232,"Missing"
W13-4901,D08-1093,0,0.0129817,"parsers (as approximated by parse probability). In Active Learning research, the Query By Committee selection method (Seung et al., 1992) is used to choose items for annotation – if a committee of two or more systems disagrees on an item, this is evidence that the item needs to be prioritised for manual correction (see for example Lynn et al. (2012b)). Steedman et al. (2003) discuss a sample selection approach based on differences between parsers – if parser A and parser B disagree on an analysis, parser A can be improved by being retrained on parser B’s analysis, and vice versa. In contrast, Ravi et al. (2008) show that parser agreement is a strong indicator of parse quality, and in parser domain adaptation, Sagae and Tsujii (2007) and Le Roux et al. (2012) use agreement between parsers to choose which automatically parsed target domain items to add to the training set. Sample selection can be used with both selftraining and co-training. We restrict our attention to co-training since our previous experiments have demonstrated that it has more potential than selftraining. In the following set of experiments, we explore the role of both parser agreement and parser disagreement in sample selection in"
W13-4901,W11-3808,0,0.0345938,"Missing"
W13-4901,P07-1078,0,0.016503,"ta sparsity issues brought about by our lack of training material, we experi4 ment with automatically expanding our training set using well known semi-supervised techniques. 5.1 Self-Training 5.1.1 Related Work Self-training, the process of training a system on its own output, has a long and chequered history in parsing. Early experiments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-traini"
W13-4901,D07-1111,0,0.0230024,"eung et al., 1992) is used to choose items for annotation – if a committee of two or more systems disagrees on an item, this is evidence that the item needs to be prioritised for manual correction (see for example Lynn et al. (2012b)). Steedman et al. (2003) discuss a sample selection approach based on differences between parsers – if parser A and parser B disagree on an analysis, parser A can be improved by being retrained on parser B’s analysis, and vice versa. In contrast, Ravi et al. (2008) show that parser agreement is a strong indicator of parse quality, and in parser domain adaptation, Sagae and Tsujii (2007) and Le Roux et al. (2012) use agreement between parsers to choose which automatically parsed target domain items to add to the training set. Sample selection can be used with both selftraining and co-training. We restrict our attention to co-training since our previous experiments have demonstrated that it has more potential than selftraining. In the following set of experiments, we explore the role of both parser agreement and parser disagreement in sample selection in co-training. 5.3.2 Agreement-Based Co-Training Experimental Setup The main algorithm for agreement-based co-training is give"
W13-4901,W10-2606,0,0.0175687,"ng results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure (McClosky et al., 2008). Sagae (2010) reports positive self-training results even without the reranking phase in a domain adaptation scenario, as do Huang and Harper (2009) who employ self-training with a PCFG-LA parser. 5.1.2 Experimental Setup The labelled data available to us for this experiment comprises the 803 gold standard trees referred to in Section 3. This small treebank includes the 150-tree development set and 150-tree test set used in experiments by Lynn et al. (2012b). We use the same development and test sets for this study. As for the remaining 503 trees, we remove any trees that have more than 200 tokens. The mot"
W13-4901,E03-1008,0,0.269735,"emi-Supervised Parsing Experiments In order to alleviate data sparsity issues brought about by our lack of training material, we experi4 ment with automatically expanding our training set using well known semi-supervised techniques. 5.1 Self-Training 5.1.1 Related Work Self-training, the process of training a system on its own output, has a long and chequered history in parsing. Early experiments by Charniak (1997) concluded that self-training is ineffective because mistakes made by the parser are magnified rather than smoothed during the self-training process. The selftraining experiments of Steedman et al. (2003) also yielded disappointing results. Reichart and Rappaport (2007) found, on the other hand, that selftraining could be effective if the seed training set was very small. McClosky et al. (2006) also report positive results from self-training, but the selftraining protocol that they use cannot be considered to be pure self-training as the first-stage Charniak parser (Charniak, 2000) is retrained on the output of the two-stage parser (Charniak and Johnson, 2005) They later show that the extra information brought by the discriminative reranking phase is a factor in the success of their procedure"
W13-4901,C10-1011,0,\N,Missing
W13-4901,P05-1022,0,\N,Missing
W13-4901,ballesteros-nivre-2012-maltoptimizer-system,0,\N,Missing
W13-4917,P06-1084,0,0.0139791,"s of incomplete lexicon coverage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leadi"
W13-4917,P08-1083,1,0.743016,"in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respectable accuracy.25 4.7 The Hungarian Treebank Hungarian is an agglutinative language, thus a lemma can have hundreds of word forms due to derivational or inflectional affixation (nomina"
W13-4917,W13-4903,0,0.0228459,"such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Re"
W13-4917,W10-1411,1,0.835873,"challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and w"
W13-4917,W10-1408,1,0.383126,"Missing"
W13-4917,E12-2012,1,0.0774441,"parsing evaluation campaign SANCL 2012 (Petrov and McDonald, 2012). The present shared task was extremely demanding on our participants. From 30 individuals or teams who registered and obtained the data sets, we present results for the seven teams that accomplished successful executions on these data in the relevant scenarios in the given the time frame. 5.1 Dependency Track Seven teams participated in the dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To"
W13-4917,W13-4907,0,0.0733412,"Missing"
W13-4917,W10-1404,0,0.0222482,"merged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and"
W13-4917,W13-4916,1,0.230959,"Missing"
W13-4917,H91-1060,0,0.199934,"n the expected performance of parsers in real-world scenarios. Results reported for MRLs using gold morphological information are then, at best, optimistic. One reason for adopting this less-than-realistic evaluation scenario in previous tasks has been the lack of sound metrics for the more realistic scenario. Standard evaluation metrics assume that the number of terminals in the parse hypothesis equals the number of terminals in the gold tree. When the predicted morphological segmentation leads to a different number of terminals in the gold and parse trees, standard metrics such as ParsEval (Black et al., 1991) or Attachment Scores (Buchholz and Marsi, 2006) fail to produce a score. In this task, we use TedEval (Tsarfaty et al., 2012b), a metric recently suggested for joint morpho-syntactic evaluation, in which normalized tree-edit distance (Bille, 2005) on morphosyntactic trees allows us to quantify the success on the joint task in realistic parsing scenarios. Finally, the previous tasks focused on dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performa"
W13-4917,D12-1133,1,0.807979,"cy-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed"
W13-4917,C10-1011,0,0.0102695,"r system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-"
W13-4917,W07-1506,0,0.220289,"s of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version is available from http://www.ims. uni-stuttgart.de/forschung/ressourcen/ korpora/tiger.html 159 to the &quot;raising&quot; algorithm described by Boyd (2007). In a third steps, all those newly introduced nodes that did not cover the head daughter of the original discontinuous node were deleted. For the second and the third step, we used the same script as for the Swedish constituency data. Predicted Morphology For the predicted scenario, a single sequence of POS tags and morphological features has been assigned using the MATE toolchain via a model trained on the train set via crossvalidation on the training set. The MATE toolchain was used to provide predicted annotation for lemmas, POS tags, morphology, and syntax. In order to achieve the best re"
W13-4917,W06-2920,0,0.827477,"ouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency relations are marked between input tokens directly, and allow the annotation of non-projective dependencies that are parseable efficiently. Dependency syntax was applied to the description of different types of languages (Tesnière, 1959; Mel’ˇcuk, 2001), which raised the hope that in these settings, parsing MRLs will further improve. However, the 2007 shared task organizers (Nivre et al., 2007a) concluded that: &quot;[Performance] classes are more ea"
W13-4917,W10-1409,1,0.0435485,"for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing"
W13-4917,candito-etal-2010-statistical,1,0.0386487,"g of 18,535 sentences,18 split into 14,759 sentences for training, 1,235 sentences for development, and 2,541 sentences for the final evaluation.19 Adapting the Data to the Shared Task The constituency trees are provided in an extended PTB bracketed format, with morphological features at the pre-terminal level only. They contain slight, automatically performed, modifications with respect to the original trees of the French treebank. The syntagmatic projection of prepositions and complementizers was normalized, in order to have prepositions and complementizers as heads in the dependency trees (Candito et al., 2010). The dependency representations are projective dependency trees, obtained through automatic conversion from the constituency trees. The conversion procedure is an enhanced version of the one described by Candito et al. (2010). Both the constituency and the dependency representations make use of coarse- and fine-grained POS tags (CPOS and FPOS respectively). The CPOS are the categories from the original treebank. The FPOS 18 The process of functional annotation is still ongoing, the objective of the FTB providers being to have all the 20000 sentences annotated with functional tags. 19 The firs"
W13-4917,W08-2102,0,0.0353476,"troduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality par"
W13-4917,A00-2018,0,0.0705659,"n analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing E"
W13-4917,W11-3801,1,0.926035,"ers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard Engli"
W13-4917,chrupala-etal-2008-learning,0,0.045003,"Missing"
W13-4917,W10-1406,0,0.0618994,"Missing"
W13-4917,W13-4909,0,0.199525,"derived from the Hebrew Treebank V2 (Sima’an et al., 2001; Guthmann et al., 2009). The treebank is based on just over 6000 sentences from the daily newspaper ‘Ha’aretz’, manually annotated with morphological information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same"
W13-4917,J03-4003,0,0.48866,"omparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the ma"
W13-4917,W13-4905,1,0.719588,"method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZE"
W13-4917,W13-4906,1,0.680312,"dependency track. Two participating systems are based on MaltParser: M ALTOPTIMIZER (Ballesteros, 2013) and AI:KU (Cirik and Sensoy, ¸ 2013). M ALTOPTIMIZER uses a variant of MaltOptimizer (Ballesteros and Nivre, 2012) to explore features relevant for the processing of morphological information. AI:KU uses a combination of MaltParser and the original MaltOptimizer. Their system development has focused on the integration of an unsupervised word clustering method using contextual and morphological properties of the words, to help combat sparseness. Similarly to MaltParser A LPAGE :DYALOG (De La Clergerie, 2013) also uses a shift-reduce transition-based parser but its training and decoding algorithms are based on beam search. This parser is implemented on top of the tabular logic programming system DyALog. To the best of our knowledge, this is the first dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 200"
W13-4917,W08-1301,0,0.0393335,"Missing"
W13-4917,P98-1062,0,0.0491049,"Missing"
W13-4917,P08-1109,0,0.0220424,"ences. In order to avoid comparing apples and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Str"
W13-4917,J13-1005,1,0.838989,"html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? How to parse effectively in the face of resource scarcity? The first step to answering all of these"
W13-4917,W13-4908,1,0.872762,"Missing"
W13-4917,W10-1412,1,0.789087,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,N10-1115,1,0.576439,"Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally"
W13-4917,P08-1085,1,0.364225,"overage. The morphologically disambiguated input files for the Raw (1-best) scenario were produced by running the raw text through the morphological disam23 Note that this additional layer in the constituency treebank adds a relatively easy set of nodes to the trees, thus “inflating” the evaluation scores compared to previously reported results. To compensate, a stricter protocol than is used in this task would strip one of the two POS layers prior to evaluation. 24 This split is slightly different than the split in previous studies. 160 biguator (tagger) described in Adler and Elhadad (2006; Goldberg et al. (2008),Adler (2007). The disambiguator is based on the same lexicon that is used to produce the lattice files, but utilizes an extra module for dealing with unknown tokens Adler et al. (2008). The core of the disambiguator is an HMM tagger trained on about 70M unannotated tokens using EM, and being supervised by the lexicon. As in the case of Arabic, we also provided data for the Predicted (gold token / predicted morphology) scenario. We used the same sequence labeler, Morfette (Chrupała et al., 2008), trained on the concatenation of POS and morphological gold features, leading to a model with respe"
W13-4917,E09-1038,1,0.867766,"ices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming to the lexical resource used to build the lattices, and is shared by the two treebanks. The higher level is syntactic, and follows the tag set and annotation decisions of the original constituency treebank.23 In addition, we unified the representation of morphological features, and fixed inconsistencies and mistakes in the treebanks. Data Split The Hebrew treebank is one of the smallest in our language set, and hence it is provided in only the small (5k) setting. For the sake of comparabilit"
W13-4917,C10-1045,1,0.826872,"nflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Haba"
W13-4917,W12-3410,0,0.0157938,"umulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realis"
W13-4917,J13-1009,1,0.747017,"Missing"
W13-4917,P09-2056,1,0.833708,".2 The Arabic Treebanks Arabic is a morphologically complex language which has rich inflectional and derivational morphology. It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya. As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word. Data Sets The Arabic data set contains two treebanks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional A"
W13-4917,D07-1116,1,0.604822,"010), a phrasestructure treebank. We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses. This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot. The Columbia Arabic Treebank The Columbia Arabic Treebank (CATiB) uses a dependency representation that is based on traditional Arabic grammar and that emphasizes syntactic case relations (Habash and Roth, 2009; Habash et al., 2007). The CATiB treebank uses the word tokenization of the PATB 11 The LDC kindly provided their latest version of the Arabic Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al., 2005), PATB 2 v3.1 (Maamouri et al., 2004a) and PATB 3 v3.3. (Maamouri et al., 2009) train: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS tags #total NTs Dep. Label Set Size train5k: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Tokens Ratio #NT/#Sents #Non Terminals #POS Tags #total NTs Dep. Label Set Size dev: #Sents #Tokens Lex. Size Avg. Length Ratio #NT/#Toke"
W13-4917,P07-2053,0,0.0323622,"Missing"
W13-4917,D07-1097,1,0.346865,"Missing"
W13-4917,D10-1002,0,0.0151688,"oaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predica"
W13-4917,P08-1067,0,0.0226773,"a-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information co"
W13-4917,J98-4004,0,0.0891486,"ir strengths and weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying t"
W13-4917,J13-1006,1,0.798597,"hbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al., 2013), the use of bilingual data (Fraser et al., 2013), and more developments that are currently under way. With new models and data, and with lingering interest in parsing non-standard English data, questions begin to emerge, such as: What is the realistic performance of parsing MRLs using today’s methods? How do the different models compare with one another? How do different representation types deal with parsing one particular language? Does the success of a parsing model on a language correlate with its representation type and learning method? Ho"
W13-4917,P03-1054,0,0.00438043,"d weaknesses. Finally, we summarize and conclude with challenges to address in future shared tasks (§8). 2 2.1 Background A Brief History of the SPMRL Field Statistical parsing saw initial success upon the availability of the Penn Treebank (PTB, Marcus et al., 1994). With that large set of syntactically annotated sentences at their disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank"
W13-4917,W06-1614,1,0.812546,"nd machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) hi"
W13-4917,kubler-etal-2008-compare,1,0.91565,"node to the root node in the output tree and the corresponding path in the gold tree. The path consists of a sequence of node labels between the terminal node and the root node, and the similarity of two paths is calculated by using the Levenshtein distance. This distance is normalized by path length, and the score of the tree is an aggregated score of the values for all terminals in the tree (xt is the leaf-ancestor path of t in tree x). P LA(h, g) = t∈yield(g) Lv(ht ,gt )/(len(ht )+len(gt )) |yield(g)| This metric was shown to be less sensitive to differences between annotation schemes in (Kübler et al., 2008), and was shown by Rehbein and van Genabith (2007a) to evaluate trees more faithfully than ParsEval in the face of certain annotation decisions. We used the implementation of Wagner (2012).6 3.4.2 Evaluation Metrics for Dependency Structures Attachment Scores Labeled and Unlabeled Attachment scores have been proposed as evaluation metrics for dependency parsing in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a) and have since assumed the role of standard metrics in multiple shared tasks and independent studies. Assume that g, h are gold and hypothesized dependency trees"
W13-4917,W12-3408,1,0.878953,"Missing"
W13-4917,P03-1056,0,0.0207769,"disposal, researchers could apply advanced statistical modeling and machine learning techniques in order to obtain high quality structure prediction. The first statistical parsing models were generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on depend"
W13-4917,W12-4615,1,0.809959,"ly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This was done in three steps. In the first step, the head daughters of all nodes were marked using a simple heuristic. In case there was a daughter with the edge label HD, this daughter was marked, i.e., existing head markings were honored. Otherwise, if existing, the rightmost daughter with edge label NK (noun kernel) was marked. Otherwise, as default, the leftmost daughter was marked. In a second step, for each continuous part of a discontinuous constituent, a separate node was introduced. This corresponds 21 This version"
W13-4917,J93-2004,0,0.0437888,"participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend."
W13-4917,D10-1004,0,0.0390834,"nd MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2013b) uses their variant of the easy-first parser combined with a feature-rich ensemble of lexical and syntactic resources. Four of the participating teams use external resources in addition to the parser. The IMS:S ZEGED :CIS team uses external morphological analyzers. C ADIM uses SAMA (Graff et al., 2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for"
W13-4917,J13-1008,1,0.913933,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,W13-4910,1,0.915357,". Additionally, new questions emerged as to the evaluation of parsers in such languages – are the word-based metrics used for English well-equipped to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint"
W13-4917,N06-1020,0,0.225446,"for gold input as well as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on"
W13-4917,P05-1012,0,0.042194,"Missing"
W13-4917,moreno-etal-2000-treebank,0,0.0581254,"e generative and based on treebank grammars (Charniak, 1997; Johnson, 1998; Klein and Manning, 2003; Collins, 2003; Petrov et al., 2006; McClosky et al., 2006), leading to high phrase-structure accuracy. Encouraged by the success of phrase-structure parsers for English, treebank grammars for additional languages have been developed, starting with Czech (Hajiˇc et al., 2000) then with treebanks of Chinese (Levy and Manning, 2003), Arabic (Maamouri et al., 2004b), German (Kübler et al., 2006), French (Abeillé et al., 2003), Hebrew (Sima’an et al., 2001), Italian (Corazza et al., 2004), Spanish (Moreno et al., 2000), and more. It quickly became apparent that applying the phrase-based treebank grammar techniques is sensitive to language and annotation properties, and that these models are not easily portable across languages and schemes. An exception to that is the approach by Petrov (2009), who trained latentannotation treebank grammars and reported good accuracy on a range of languages. The CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) highlighted the usefulness of an alternative linguistic formalism for the development of competitive parsing models. Dependency"
W13-4917,nivre-etal-2006-talbanken05,1,0.442193,"subject agreement with respect to person and number has been dropped in modern Swedish. The Data Set The Swedish data sets are taken from the Talbanken section of the Swedish Treebank (Nivre and Megyesi, 2007). Talbanken is a syntactically annotated corpus developed in the 1970s, originally annotated according to the MAMBA scheme (Teleman, 1974) with a syntactic layer consisting of flat phrase structure and grammatical functions. The syntactic annotation was later automatically converted to full phrase structure with grammatical functions and from that to dependency structure, as described by Nivre et al. (2006). Both the phrase structure and the dependency version use the functional labels from the original MAMBA scheme, which provides a fine-grained classification of syntactic functions with 65 different labels, while the phrase structure annotation (which had to be inferred automatically) uses a coarse set of only 8 labels. For the release of the Swedish treebank, the POS level was re-annotated to conform to the current de facto standard for Swedish, which is the Stockholm-Umeå tagset (Ejerhed et al., 1992) with 25 base tags and 25 morpho-syntactic features, which together produce over 150 complex"
W13-4917,P06-1055,0,0.480329,"as more realistic parsing scenarios. 1 Introduction Syntactic parsing consists of automatically assigning to a natural language sentence a representation of its grammatical structure. Data-driven approaches to this problem, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it"
W13-4917,N10-1003,0,0.0195824,"2009) for Arabic morphology. A LPAGE :DYALOG and IGM:A LPAGE use external lexicons for French. IGM:A LPAGE additionally uses Morfette (Chrupała et al., 2008) for morphological analysis and POS tagging. Finally, as already mentioned, AI:KU clusters words and POS tags in an unsupervised fashion exploiting additional, un-annotated data. 5.2 Constituency Track A single team participated in the constituency parsing task, the IMS:S ZEGED :CIS team (Björkelund et al., 2013). Their phrase-structure parsing system uses a combination of 8 PCFG-LA parsers, trained using a product-of-grammars procedure (Petrov, 2010). The 50-best parses of this combination are then reranked by a model based on the reranker by Charniak and Johnson (2005).33 5.3 6.1 Baselines We additionally provide the results of two baseline systems for the nine languages, one for constituency parsing and one for dependency parsing. For the dependency track, our baseline system is MaltParser in its default configuration (the arc-eager algorithm and liblinear for training). Results marked as BASE :M ALT in the next two sections report the results of this baseline system in different scenarios. The constituency parsing baseline is based on"
W13-4917,W07-2460,0,0.109747,"Missing"
W13-4917,D07-1066,0,0.0884872,"Missing"
W13-4917,W11-3808,0,0.027114,"rameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCFRS parsing (Kallmeyer an"
W13-4917,N06-2033,0,0.0563478,"rst dependency parser capable of handling word lattice input. 163 Three participating teams use the MATE parser (Bohnet, 2010) in their systems: the BASQUE T EAM (Goenaga et al., 2013), IGM:A LPAGE (Constant et al., 2013) and IMS:S ZEGED :CIS (Björkelund et al., 2013). The BASQUE T EAM uses the MATE parser in combination with MaltParser (Nivre et al., 2007b). The system combines the parser outputs via MaltBlender (Hall et al., 2007). IGM:A LPAGE also uses MATE and MaltParser, once in a pipeline architecture and once in a joint model. The models are combined via a re-parsing strategy based on (Sagae and Lavie, 2006). This system mainly focuses on M WEs in French and uses a CRF tagger in combination with several large-scale dictionaries to handle M WEs, which then serve as input for the two parsers. The IMS:S ZEGED :CIS team participated in both tracks, with an ensemble system. For the dependency track, the ensemble includes the MATE parser (Bohnet, 2010), a best-first variant of the easy-first parser by Goldberg and Elhadad (2010b), and turbo parser (Martins et al., 2010), in combination with a ranker that has the particularity of using features from the constituent parsed trees. C ADIM (Marton et al., 2"
W13-4917,schmid-etal-2004-smor,0,0.00857226,"information and phrase-structure trees and extended with head information as described in Tsarfaty (2010, ch. 5). The unlabeled dependency version was produced by conversion from the constituency treebank as described in Goldberg (2011). Both the constituency and dependency trees were annotated with a set grammatical function labels conforming to Unified Stanford Dependencies by Tsarfaty (2013). 22 We also provided a predicted-all scenario, in which we provided morphological analysis lattices with POS and morphological information derived from the analyses of the SMOR derivational morphology (Schmid et al., 2004). These lattices were not used by any of the participants. Adapting the Data to the Shared Task While based on the same trees, the dependency and constituency treebanks differ in their POS tag sets, as well as in some of the morphological segmentation decisions. The main effort towards the shared task was unifying the two resources such that the two treebanks share the same lexical yields, and the same pre-terminal labels. To this end, we took the layering approach of Goldberg et al. (2009), and included two levels of POS tags in the constituency trees. The lower level is lexical, conforming t"
W13-4917,W10-1410,1,0.889145,"Missing"
W13-4917,seeker-kuhn-2012-making,1,0.106665,"n constituency data set is based on the TiGer treebank release 2.2.21 The original annotation scheme represents discontinuous constituents such that all arguments of a predicate are always grouped under a single node regardless of whether there is intervening material between them or not (Brants et al., 2002). Furthermore, punctuation and several other elements, such as parentheses, are not attached to the tree. In order to make the constituency treebank usable for PCFG parsing, we adapted this treebank as described shortly. The conversion of TiGer into dependencies is a variant of the one by Seeker and Kuhn (2012), which does not contain empty nodes. It is based on the same TiGer release as the one used for the constituency data. Punctuation was attached as high as possible, without creating any new non-projective edges. Adapting the Data to the Shared Task For the constituency version, punctuation and other unattached elements were first attached to the tree. As attachment target, we used roughly the respective least common ancestor node of the right and left terminal neighbor of the unattached element (see Maier et al. (2012) for details), and subsequently, the crossing branches were resolved. This w"
W13-4917,P12-1046,0,0.00731402,"based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formatio"
W13-4917,W11-3803,0,0.0414253,"to capture performance across frameworks, or performance in the face of morphological complexity? This event provoked active discussions and led to the establishment of a series of SPMRL events for the discussion of shared challenges and cross-fertilization among researchers working on parsing MRLs. The body of work on MRLs that was accumulated through the SPMRL workshops2 and hosting ACL venues contains new results for Arabic (Attia et al., 2010; Marton et al., 2013a), Basque (Bengoetxea and Gojenola, 2010), Croatian (Agic et al., 2013), French (Seddah et al., 2010; Candito and Seddah, 2010; Sigogne et al., 2011), German (Rehbein, 2011), Hebrew (Tsarfaty and Sima’an, 2010; Goldberg and 1 http://alpage.inria.fr/iwpt09/panel.en. html 2 See http://www.spmrl.org/ and related workshops. 148 Elhadad, 2010a), Hindi (Ambati et al., 2010), Korean (Chung et al., 2010; Choi and Palmer, 2011) and Spanish (Le Roux et al., 2012), Tamil (Green et al., 2012), amongst others. The awareness of the modeling challenges gave rise to new lines of work on topics such as joint morpho-syntactic processing (Goldberg and Tsarfaty, 2008), Relational-Realizational Parsing (Tsarfaty, 2010), EasyFirst Parsing (Goldberg, 2011), PLCF"
W13-4917,W10-1405,1,0.891538,"Missing"
W13-4917,W10-1401,1,0.779419,"sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146–182, c Seattle, Washington, USA, 18 October 2013. 2013 Association for Computational Linguistics recently, advances in PCFG-LA parsing (Petrov et al., 2006) and language-agnostic data-driven dependency parsing (McD"
W13-4917,D11-1036,1,0.926772,"dependency parsing. When providing both constituency-based and dependency-based tracks, it is interesting to compare results across these frameworks so as to better understand the differences in performance between parsers of different types. We are now faced with an additional question: how can we compare parsing results across different frameworks? Adopting standard metrics will not suffice as we would be comparing apples and oranges. In contrast, TedEval is defined for both phrase structures and dependency structures through the use of an intermediate representation called function trees (Tsarfaty et al., 2011; Tsarfaty et al., 2012a). Using TedEval thus allows us to explore both dependency and constituency parsing frameworks and meaningfully compare the performance of parsers of different types. 149 3 3.1 Defining the Shared-Task Input and Output We define a parser as a structure prediction function that maps sequences of space-delimited input tokens (henceforth, tokens) in a language to a set of parse trees that capture valid morpho-syntactic structures in that language. In the case of constituency parsing, the output structures are phrase-structure trees. In dependency parsing, the output consis"
W13-4917,E12-1006,1,0.148172,"er languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure of sentences is expressed through word formation, rather than constituent-order patterns as is the case in English and other configurational languages. MRLs express information concerning the grammatical function of a word and its grammatical relation to other words at the word level, via phenomena such as inflectional affixes, pronominal clitics, and so on (Tsarfaty et al., 2012c). The non-rigid tree structures and morphological ambiguity of input words contribute to the challenges of parsing MRLs. In addition, insufficient language resources were shown to also contribute to parsing difficulty (Tsarfaty et al., 2010; Tsarfaty et al., 2012c, and references therein). These challenges have initially been addressed by native-speaking experts using strong in-domain knowledge of the linguistic phenomena and annotation idiosyncrasies to improve the accuracy and efficiency of parsing models. More 146 Proceedings of the Fourth Workshop on Statistical Parsing of Morphologicall"
W13-4917,P13-2103,1,0.111695,"les and oranges, we use the unlabeled TedEval metric, which converts all representation types internally into the same kind of structures, called function trees. Here we use TedEval’s crossframework protocol (Tsarfaty et al., 2012a), which accomodates annotation idiosyncrasies. • Cross-Language Evaluation. Here, we compare parsers for the same representation type across different languages. Conducting a complete and faithful evaluation across languages 151 would require a harmonized universal annotation scheme (possibly along the lines of (de Marneffe and Manning, 2008; McDonald et al., 2013; Tsarfaty, 2013)) or task based evaluation. As an approximation we use unlabeled TedEval. Since it is unlabeled, it is not sensitive to label set size. Since it internally uses function-trees, it is less sensitive to annotation idiosyncrasies (e.g., head choice) (Tsarfaty et al., 2011). The former two dimensions are evaluated on the full sets. The latter two are evaluated on smaller, comparable, test sets. For completeness, we provide below the formal definitions and essential modifications of the evaluation software that we used. 3.4.1 Evaluation Metrics for Phrase Structures ParsEval The ParsEval metrics (B"
W13-4917,P11-2033,1,0.563308,"em, both for constituency-based and dependency-based parsing, have seen a surge of interest in the last two decades. These data-driven parsing approaches obtain state-of-the-art results on the de facto standard Wall Street Journal data set (Marcus et al., 1993) of English (Charniak, 2000; Collins, 2003; Charniak and Johnson, 2005; McDonald et al., 2005; McClosky et al., 2006; Petrov et al., 2006; Nivre et al., 2007b; Carreras et al., 2008; Finkel et al., 2008; ∗ Contact authors: djame.seddah@paris-sorbonne.fr, reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu Huang, 2008; Huang et al., 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Shindo et al., 2012), and provide a foundation on which many tasks operating on semantic structure (e.g., recognizing textual entailments) or even discourse structure (coreference, summarization) crucially depend. While progress on parsing English — the main language of focus for the ACL community — has inspired some advances on other languages, it has not, by itself, yielded high-quality parsing for other languages and domains. This holds in particular for morphologically rich languages (MRLs), where important information concerning the predicate-argument structure o"
W13-4917,R13-1099,1,0.0375053,"orphology In order to provide the same POS tag set for the constituent and dependency treebanks, we used the dependency POS tagset for both treebank instances. Both versions of the treebank are available with gold standard and automatic morphological annotation. The automatic POS tagging was carried out by a 10-fold cross-validation on the shared task data set by magyarlanc, a natural language toolkit for processing Hungarian texts (segmentation, morphological analysis, POS tagging, and dependency parsing). The annotation provides POS tags and deep morphological features for each input token (Zsibrita et al., 2013).28 28 The full data sets of both the constituency and dependency versions of the Szeged Treebank are available at 161 4.8 The Korean Treebank The Treebank The Korean corpus is generated by collecting constituent trees from the K AIST Treebank (Choi et al., 1994), then converting the constituent trees to dependency trees using head-finding rules and heuristics. The K AIST Treebank consists of about 31K manually annotated constituent trees from 97 different sources (e.g., newspapers, novels, textbooks). After filtering out trees containing annotation errors, a total of 27,363 trees with 350,090"
W13-4917,E93-1064,0,\N,Missing
W13-4917,C00-1001,0,\N,Missing
W13-4917,C10-1061,1,\N,Missing
W13-4917,J13-1003,1,\N,Missing
W13-4917,C08-1112,1,\N,Missing
W13-4917,W08-1008,1,\N,Missing
W13-4917,P05-1022,0,\N,Missing
W13-4917,P98-1063,0,\N,Missing
W13-4917,C98-1060,0,\N,Missing
W13-4917,vincze-etal-2010-hungarian,1,\N,Missing
W13-4917,D07-1096,1,\N,Missing
W14-3902,W12-2108,0,0.0277643,"ndom Fields. We find that the dictionary-based approach is surpassed by supervised classification and sequence labelling, and that it is important to take contextual clues into consideration. 1 Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data itIntroduction Automatic processing and understanding of Social Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Althoug"
W14-3902,dey-fung-2014-hindi,0,0.0842282,"Missing"
W14-3902,hughes-etal-2006-reconsidering,0,0.0200292,"and sequence labelling using Conditional Random Fields. We find that the dictionary-based approach is surpassed by supervised classification and sequence labelling, and that it is important to take contextual clues into consideration. 1 Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! This comment is written in three languages: English, Hindi (italics), and Bengali (boldface). For Bengali and Hindi, phonetic typing has been used. We follow in the footsteps of recent work on language identification for SMC (Hughes et al., 2006; Baldwin and Lui, 2010; Bergsma et al., 2012), focusing specifically on the problem of word-level language identification for code mixing SMC. Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. The paper is organized as follows: in Section 2, we review related research in the area of code mixing and language identification; in Section 3, we describe our code mixing corpus, the data itIntroduction Automatic processing and understanding of Social Media Content (SMC) is currently attracting much attention from the Natural L"
W14-3902,C82-1023,0,0.591308,"Missing"
W14-3902,N13-1131,0,0.030764,"4 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turki"
W14-3902,D12-1039,0,0.0264776,"Missing"
W14-3902,P08-1099,0,0.0156156,"004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved the highest scores. Another very recent work on this topic is (Nguyen and Do˘gru¨oz, 2013). They report on language identification experiments performed on Turkish and Dutch forum data. Experiments have been carried out using language models, dictionaries, logistic regression classification and Conditional Random Fields. They find that language models are more robust than dictionaries and that contextual information is helpful for the task. self and the annotation process; in Section 4, we list the tools and resources which we use in our language iden"
W14-3902,D08-1102,0,0.0504151,"ocial Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Although English is still by far the most popular language in SMC, its dominance is receding. Hong et al. (2011), for example, applied an automatic language detection algorithm to over 62 million tweets to identify the top 10 most popular languages on Twitter. They found 13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language ident"
W14-3902,D08-1110,0,0.39477,"ocial Media Content (SMC) is currently attracting much attention from the Natural Language Processing research community. Although English is still by far the most popular language in SMC, its dominance is receding. Hong et al. (2011), for example, applied an automatic language detection algorithm to over 62 million tweets to identify the top 10 most popular languages on Twitter. They found 13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language ident"
W14-3902,S13-2052,0,0.0212039,"Missing"
W14-3902,W14-3907,0,0.181932,"Missing"
W14-3902,D13-1084,0,0.127537,"Missing"
W14-3902,S14-2036,1,0.830181,"Missing"
W14-3902,N13-1039,0,0.0250362,"Missing"
W14-3902,P12-1102,0,0.0127428,"opular languages on Twitter. They found 13 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13–23, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 2008a; Weiner et al., 2012). Solorio and Liu (2008b) try to predict the points inside a set of spoken Spanish-English sentences where the speakers switch between the two languages. Other studies have looked at code mixing in different types of short texts, such as information retrieval queries (Gottron and Lipka, 2010) and SMS messages (Farrugia, 2004; Rosner and Farrugia, 2007). Yamaguchi and Tanaka-Ishii (2012) perform language identification using artificial multilingual data, created by randomly sampling text segments from monolingual documents. King and Abney (2013) used weakly semi-supervised methods to perform word-level language identification. A dataset of 30 languages has been used in their work. They explore several language identification approaches, including a Naive Bayes classifier for individual word-level classification and sequence labelling with Conditional Random Fields trained with Generalized Expectation criteria (Mann and McCallum, 2008; Mann and McCallum, 2010), which achieved"
W14-3902,W13-2249,1,0.82795,"Missing"
W14-3902,N10-1027,0,\N,Missing
W14-3915,C82-1023,0,0.223069,"Switched Data shared task in the Workshop on Computational Approaches to Code Switching. Wordlevel classification experiments were carried out using a simple dictionary-based method, linear kernel support vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various"
W14-3915,N13-1131,0,0.0403813,"ing (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, Introduction This paper describes DCU-UVT’s participation in the shared task Language Identification in Code-Switched Data (Solorio et al., 2014) at the Workshop on Computational Approac"
W14-3915,W14-3902,1,0.705154,"textual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), usi"
W14-3915,li-etal-2012-mandarin,0,0.0390339,"h, we use string edit distance, charactern-gram overlap and context similarity to make predictions. For the SVM approach, we experiment with context-independent (word, charactern-grams, length of a word and capitalisation information) and context-sensitive (adding the pre127 Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127–132, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Resource BNC LexNorm TrainingData TrainingData+BNC+LexNorm 2013), modern standard Arabic-Egyptian dialect (Elfardy et al., 2013), Mandarin-English (Li et al., 2012; Lyu et al., 2010), and English-HindiBengali (Barman et al., 2014). 3 Table 2: Average cross-validation accuracy of dictionary-based prediction for Nepali-English Data Statistics The training data provided for this task consists of tweets. Unfortunately, because of deleted tweets, the full training set could not be downloaded. Out of 9,993 Nepali-English training tweets, we were able to download 9,668 and out of 11,400 SpanishEnglish training tweets, we were able to download 11,353. Table 1 shows the token-level statistics of the two datasets. Label lang1 (en) lang2 (ne/es) ne ambiguous mixed"
W14-3915,P14-2111,1,0.88229,"Missing"
W14-3915,D13-1084,0,0.141574,"Missing"
W14-3915,C12-2029,0,0.0353294,"ied over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turki"
W14-3915,W13-2249,1,0.877549,"Missing"
W14-3915,D08-1102,0,0.0294762,"rt vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as"
W14-3915,D12-1039,0,0.0456464,"Missing"
W14-3915,D08-1110,0,0.0265957,"rt vector machines (SVMs) with and without contextual clues, and a k-nearest neighbour approach. Based on these experiments, we select our SVM-based system with contextual clues as our final system and present results for the Nepali-English and Spanish-English datasets. 1 2 Background While the problem of automatically identifying and analysing code-mixing has been identified over 30 years ago (Joshi, 1982), it has only recently drawn wider attention. Specific problems addressed include language identification in multilingual documents, identification of code-switching points and POS tagging (Solorio and Liu, 2008b) of code-mixing data. Approaches taken to the problem of identifying codemixing include the use of dictionaries (Nguyen and Do˘gru¨oz, 2013; Barman et al., 2014; Elfardy et al., 2013; Solorio and Liu, 2008b), language models (Alex, 2008; Nguyen and Do˘gru¨oz, 2013; Elfardy et al., 2013), morphological and phonological analysis (Elfardy et al., 2013; Elfardy and Diab, 2012) and various machine learning algorithms such as sequence labelling with Hidden Markov Models (Farrugia, 2004; Rosner and Farrugia, 2007) and Conditional Random Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as"
W14-3915,W14-3907,0,0.041397,"ndom Fields (Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013), as well as word-level classification using Naive Bayes (Solorio and Liu, 2008a), logistic regression (Nguyen and Do˘gru¨oz, 2013) and SVMs (Barman et al., 2014), using features such as word, POS, lemma and character-n-grams. Language pairs that have been explored include English-Maltese (Farrugia, 2004; Rosner and Farrugia, 2007), English-Spanish (Solorio and Liu, 2008b), Turkish-Dutch (Nguyen and Do˘gru¨oz, Introduction This paper describes DCU-UVT’s participation in the shared task Language Identification in Code-Switched Data (Solorio et al., 2014) at the Workshop on Computational Approaches to Code Switching, EMNLP, 2014. The task is to make word-level predictions (six labels: lang1, lang2, ne, mixed, ambiguous and other) for mixedlanguage user generated content. We submit predictions for Nepali-English and Spanish-English data and perform experiments using dictionaries, a k-nearest neighbour (k-NN) classifier and a linearkernel SVM classifier. In our dictionary-based approach, we investigate the use of different English dictionaries as well as the training data. In the k-NN based approach, we use string edit distance, charactern-gram"
W14-3915,S14-2036,1,0.884374,"Missing"
W14-4008,W07-0738,0,0.0768767,"Missing"
W14-4008,W10-1408,1,0.812133,"Missing"
W14-4008,W12-3112,0,0.138934,"nslations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al., 2014) following the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1 The data will be made publicly available - see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation"
W14-4008,2009.iwslt-papers.4,0,0.020551,"Labelled precision and recall are calculated in the same way except that they also require argument label agreement. UF1 and LF1 are the harmonic means of unlabelled and labelled scores respectively. Inspired by the observation that most source sentences with no identified proposition are short and can be assumed to be easier to translate, and based on experiments on the dev set, we assign a score of 1 to such sentences. When no proposition is identified in the target side while there is a proposition in the source, we assign a score of 0.5. We obtain word alignments using the Moses toolkit (Hoang et al., 2009), which can generate alignments in both directions and combine them using a number of heuristics. We try intersection, union, source-to-target only, as well as the grow-diag-final-and heuristic, but only the source-to-target results are reported here as they slightly outperform the others. Table 7 shows the RMSE and Pearson r for each of the unlabelled and labelled F1 against ade16 It should be noted that a number of features in addition to those presented here have been tried, e.g. the ratio and difference of the source and target values of numerical features. However, through manual feature"
W14-4008,W12-3108,0,0.0189348,"the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1 The data will be made publicly available - see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation based on their lexical similarity. Once the predicates and arguments are aligned, their similarities are measured"
W14-4008,P98-1013,0,0.0301899,"he fluency and adequacy of the MT output is low or high, the other tends to be the same. The data is split into train, development and test sets of 3000, 500 and 1000 sentences respectively. 4 Semantic Role Labelling The type of semantic information we use in this work is the predicate-argument structure or semantic role labelling of the sentence. This information needs to be extracted from both sides of the translation, i.e. English and French. Though the SRL of English has been well-studied (M`arquez et al., 2008) thanks to the existence of two major hand-crafted resources, namely FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), French is one of the under-studied languages in 5 The post-editing guidelines are based on the TAUS/CNGL guidelines for achieving “good enough” quality downloaded from https://evaluation. taus.net/images/stories/guidelines/tauscngl-machine-translation-posteditingguidelines.pdf. 6 Version 13a of MTEval script was used at the segment level which performs smoothing. 7 Note that HTER scores have no upper limit and can be higher than 1 when the number of errors is higher than the segment length. In addition, the higher HTER indicates lower translation quality. T"
W14-4008,I13-1153,1,0.892127,"ity estimation (QE) and has recently been the centre of attention (Bojar et al., 2014) following the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1 The data will be made publicly available - see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation based on their lexica"
W14-4008,W09-1206,0,0.0721821,"Missing"
W14-4008,S14-1012,1,0.77408,"d has recently been the centre of attention (Bojar et al., 2014) following the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1 The data will be made publicly available - see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation based on their lexical similarity. Once the"
W14-4008,C14-1194,1,0.828053,"d has recently been the centre of attention (Bojar et al., 2014) following the seminal work of Blatz et al. (2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1 The data will be made publicly available - see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation based on their lexical similarity. Once the"
W14-4008,W12-4204,0,0.0322631,"Missing"
W14-4008,W04-3250,0,0.0610177,"ased on dependency trees. We try another PST format derived from constituency trees. These PSTs (Figure 1(c)) are the lowest common subtrees spanning the predicate node and its argument nodes and are gathered under a dummy root node. The argument role 13 Note that the only lexical information in this format is the predicate lemma. We tried replacing the POS tags with argument word forms, which led to a slight degradation. 14 This format is chosen among several other variations due to its higher performance. 11 http://disi.unitn.it/moschitti/TreeKernel.htm 12 We use paired bootstrap resampling Koehn (2004) for statistical significance testing. 71 (a) D-PAS (b) D-PST (c) C-PST (d) D-TKSSQE (e) C-TKSSQE Figure 1: Semantic tree kernel formats for the sentence: Can anyone help? labels are concatenated with the syntactic nonterminal category of the argument node. Predicates are not marked. However, our dependencybased SRL is required to be converted into a constituency-based format. While constituencyto-dependency conversion is straightforward using head-finding rules (Surdeanu et al., 2008), the other way around is not. We therefore approximate the conversion using a heuristic we call (D2C).15 As s"
W14-4008,W08-1301,0,0.11733,"Missing"
W14-4008,2005.mtsummit-papers.11,0,0.157126,"s and Sagot, 2012). The English SRL achieves 77.77 and 67.02 labelled F1 points when trained only on the training section of PropBank and tested on the WSJ and Brown test sets respectively.9 The French SRL is evaluated using 5-fold cross-validation on the 1K data set and obtains an F1 average of 67.66. When applied to the QE data set, these models identify 9133, 8875 and 8795 propositions on its source side, post-edits and MT output respectively. this respect mainly due to a lack of such resources. The only available gold standard resource is a small set of 1000 sentences taken from Europarl (Koehn, 2005) and manually annotated with Propbank verb predicates (van der Plas et al., 2010). van der Plas et al. (2011) attempt to tackle this scarcity by automatically projecting SRL from the English side of a large parallel corpus to its French side. Our preliminary experiments (Kaljahi et al., 2014a), however, show that SRL models trained on the small manually annotated corpus have a higher quality than ones trained on the much larger projected corpus. We therefore use the 1K gold standard set to train a French SRL model. For English, we use all the data provided in the CoNLL 2009 shared task (Hajiˇc"
W14-4008,P11-1023,0,0.214844,"s which measure the full or partial lexical match between the fillers of same semantic roles in the hypothesis and translation, or simply the role label matches between them. They conclude that these features can only be useful in combination with other features and metrics reflecting different aspects of the quality. 3 Data We randomly select 4500 segments from a large collection of Symantec English Norton forum text.2 In order to be independent of any one MT system, we translate these segments into French with the following three systems and randomly choose 1500 distinct segments from each. Lo and Wu (2011) introduce HMEANT, a manual MT evaluation metric based on predicateargument structure matching which involves two steps of human engagement: 1) semantic role annotation of the reference and machine translation, 2) evaluating the translation of predicates and arguments. The metric calculates the F1 score of the semantic frame match between the reference and machine translation based on this evaluation. To keep the costs reasonable, the first step is carried out by amateur annotators who were minimally trained with a simplified list of 10 thematic roles. On a set of 40 examples, the metric is me"
W14-4008,W11-2107,0,0.0495891,"Missing"
W14-4008,W12-3129,0,0.020158,"s high as that of HTER is reported. • ACCEPT3 : a phrase-based Moses system trained on training sets of WMT12 releases of Europarl and News Commentary plus Symantec translation memories • SYSTRAN: a proprietary rule-based system augmented with domain-specific dictionaries • Bing4 : an online translation system These translations are evaluated in two ways. The first method involves light post-editing by a professional human translator who is a native 2 http://community.norton.com http://www.accept.unige.ch/Products/ D_4_1_Baseline_MT_systems.pdf 4 http://www.bing.com/translator(on24Feb-2014) 3 Lo et al. (2012) propose MEANT, a variant of HMEANT, which automatizes its manual steps using 1) automatic SRL systems for (only) verb 68 5 4 3 2 1 Adequacy All meaning Most of meaning Much of meaning Little meaning None of meaning Fluency Flawless Language Good Language Non-native Language Disfluent Language Incomprehensible 1-HTER HBLEU HMeteor Adq Flu Table 2: Adequacy/fluency score interpretation 1-HTER HBLEU HMeteor Adq Flu 0.9111 0.9207 0.9314 0.6632 0.7049 0.6843 0.6447 0.7213 0.6652 0.8824 - Table 3: Pearson r between pairs of metrics on the entire 4.5K data set French speaker.5 Each sentence translat"
W14-4008,P11-2052,0,0.0537243,"Missing"
W14-4008,P14-2124,0,0.0369969,"Missing"
W14-4008,J08-2001,0,0.0620436,"Missing"
W14-4008,E06-1015,0,0.0102602,"nce between manual and human-targeted metric prediction. The higher r for the former suggests that the patterns of these scores are easier to learn. The RMSE seems to follow the standard deviation 9 Although the English SRL data are annotated for noun predicates as well as verb predicates, since the French data has only verb predicate annotations, we only consider verb predicates for English. 10 http://svmlight.joachims.org/ 8 https://github.com/CNGLdlab/LORGRelease. 70 of the scores as the same ranking is seen in both. 6 WMT17 TKSyQE D-PAS D-PST C-PST CD-PST TKSSQE Tree Kernels Tree kernels (Moschitti, 2006) have been successfully used in QE by Hardmeier et al. (2012) and in our previous work (Kaljahi et al., 2013; Kaljahi et al., 2014b), where syntactic trees are employed. Tree kernels eliminate the burden of manual feature engineering by efficiently utilizing all subtrees of a tree. We employ both syntactic and semantic information in learning quality scores, using the SVMLight-TK11 , a support vector machine (SVM) implementation of tree kernels. We implement a syntactic tree kernel QE system with constituency and dependency trees of the source and target side, following our previous work (Kalj"
W14-4008,J03-1002,0,0.00485456,"The POS tags are also replaced with those output by the parser. For the same reason, we re5 Baseline We compare the results of our experiments to a baseline built using the 17 baseline features of the WMT QE shared task (Bojar et al., 2014). These features provide a strong baseline and have been used in all three years of the shared task. We use support vector regression implemented in the SVMLight toolkit10 with Radial Basis Function (RBF) kernel to build this baseline. To extract these features, a parallel English-French corpus is required to build a lexical translation table using GIZA++ (Och and Ney, 2003). We use the Europarl English-French parallel corpus (Koehn, 2005) plus around 1M segments of Symantec translation memory. Table 4 shows the performance of this system (WMT17) on the test set measured by Root Mean Square Error (RMSE) and Pearson correlation coefficient (r). We only report the results on predicting four of the metrics introduced above, omitting HMeteor due to space constraints. C and γ parameters are tuned on the development set with respect to r. The results show a significant difference between manual and human-targeted metric prediction. The higher r for the former suggests"
W14-4008,J05-1004,0,0.0102278,"output is low or high, the other tends to be the same. The data is split into train, development and test sets of 3000, 500 and 1000 sentences respectively. 4 Semantic Role Labelling The type of semantic information we use in this work is the predicate-argument structure or semantic role labelling of the sentence. This information needs to be extracted from both sides of the translation, i.e. English and French. Though the SRL of English has been well-studied (M`arquez et al., 2008) thanks to the existence of two major hand-crafted resources, namely FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), French is one of the under-studied languages in 5 The post-editing guidelines are based on the TAUS/CNGL guidelines for achieving “good enough” quality downloaded from https://evaluation. taus.net/images/stories/guidelines/tauscngl-machine-translation-posteditingguidelines.pdf. 6 Version 13a of MTEval script was used at the segment level which performs smoothing. 7 Note that HTER scores have no upper limit and can be higher than 1 when the number of errors is higher than the segment length. In addition, the higher HTER indicates lower translation quality. To be comparable to the other scores"
W14-4008,P02-1040,0,0.0890864,"Missing"
W14-4008,W11-1001,0,0.0482417,"Missing"
W14-4008,quirk-2004-training,0,0.0309248,"(2003). Most QE studies have focused on surface and languagemodel-based features of the source and target. The quality of translation is however closely related to the syntax and semantics of the languages, the former concerning fluency and the latter adequacy. While there have been some attempts to utilize syntax in this task, semantics has been paid less 2 Related Work Syntax has been exploited in QE in various ways including tree kernels (Hardmeier et al., 2012; Kaljahi et al., 2013; Kaljahi et al., 2014b), parse probabilities and syntactic label frequency (Avramidis, 2012), parseability (Quirk, 2004) and POS n-gram scores (Specia and Gim´enez, 2010). 1 The data will be made publicly available - see http:// www.computing.dcu.ie/mt/confidentmt.html 67 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 67–77, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics predicates, 2) automatic alignment of predicates and their arguments in the reference and machine translation based on their lexical similarity. Once the predicates and arguments are aligned, their similarities are measured using a variety of methods s"
W14-4008,2006.amta-papers.25,0,0.0494055,"Missing"
W14-4008,2010.amta-papers.3,0,0.055142,"Missing"
W14-4008,W08-2121,0,0.0552325,"Missing"
W14-4008,W10-1814,0,0.0587254,"Missing"
W14-4008,C98-1013,0,\N,Missing
W14-4008,W09-1201,0,\N,Missing
W14-4008,C04-1046,0,\N,Missing
W14-4008,W14-3302,0,\N,Missing
W14-4606,W08-1301,0,0.168621,"Missing"
W14-4606,lynn-etal-2012-irish,1,0.795987,"Missing"
W14-4606,U12-1005,1,0.805413,"languages from four language family groups to assess which languages are the most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised parsing models which are then applied to sentences from the Irish Dependency Treebank. The best results are achieved when using Indonesian, a language from the Austronesian language family. 1 Introduction Considerable efforts have been made over the past decade to develop natural language processing resources for the Irish language (U´ı Dhonnchadha et al., 2003; U´ı Dhonnchadha and van Genabith, 2006; U´ı Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard dependency parse trees. These trees are labelled with deep syntactic information, marking grammatical roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not compare in size to similar resources of other languages.1 The small size of the treebank affects the accuracy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate whether training dat"
W14-4606,W13-4901,1,0.939414,"ps to assess which languages are the most useful for cross-lingual parsing of Irish by using these treebanks to train delexicalised parsing models which are then applied to sentences from the Irish Dependency Treebank. The best results are achieved when using Indonesian, a language from the Austronesian language family. 1 Introduction Considerable efforts have been made over the past decade to develop natural language processing resources for the Irish language (U´ı Dhonnchadha et al., 2003; U´ı Dhonnchadha and van Genabith, 2006; U´ı Dhonnchadha, 2009; Lynn et al., 2012a; Lynn et al., 2012b; Lynn et al., 2013). One such resource is the Irish Dependency Treebank (Lynn et al., 2012a) which contains just over 1000 gold standard dependency parse trees. These trees are labelled with deep syntactic information, marking grammatical roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not compare in size to similar resources of other languages.1 The small size of the treebank affects the accuracy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate whether training data from other languages can be successfull"
W14-4606,J93-2004,0,0.0451671,", we select the first 4447 trees from each treebank – to match the number of trees in the smallest data set (Swedish). We delexicalise all treebanks and use the universal POS tags as both the coarse- and fine-grained values.5 We train a parser on all 10 source data sets outlined and use each induced parsing model to parse and test on a delexicalised version of the Universal Irish Dependency Treebank. Largest transfer source training data - Universal English Dependency Treebank English has the largest source training data set (sections 2-21 of the Wall Street Journal data in the Penn Treebank (Marcus et al., 1993) contains 39, 832 trees). As with the smaller transfer datasets, we delexicalise this dataset and use the universal POS tag values only. We experiment with this larger training set in order to establish whether more training data helps in a cross-lingual setting. 4 Version 2 data sets downloaded from https://code.google.com/p/uni-dep-tb/ Note that the downloaded treebanks had some fine-grained POS tags that were not used across all languages: e.g. VERBVPRT (Spanish), CD (English). 5 46 Parser and Evaluation Metrics We use a transition-based dependency parsing system, MaltParser (Nivre et al.,"
W14-4606,D11-1006,0,0.12918,"ees are labelled with deep syntactic information, marking grammatical roles such as subject, object, modifier, and coordinator. While a valuable resource, the treebank does not compare in size to similar resources of other languages.1 The small size of the treebank affects the accuracy of any statistical parsing models learned from this treebank. Therefore, we would like to investigate whether training data from other languages can be successfully utilised to improve Irish parsing. Cross-lingual transfer parsing involves training a parser on one language, and parsing data of another language. McDonald et al. (2011) describe two types of cross-lingual parsing, direct transfer parsing in which a delexicalised version of the source language treebank is used to train a parsing model which is then used to parse the target language, and a more complicated projected transfer approach in which the direct transfer approach is used to seed a parsing model which is then trained to obey source-target constraints learned from a parallel corpus. These experiments revealed that languages that were typologically similar were not necessarily the best source-target pairs, sometimes due to variations between their languag"
W14-4606,nivre-etal-2006-maltparser,0,0.0527279,"ological features such as inflected prepositions and initial mutations, for example. Compared to other EU-official languages, Irish language technology is under-resourced, as highlighted by a recent study (Judge et al., 2012). In the area of morpho-syntactic processing, recent years have seen the development of a part-of-speech tagger (U´ı Dhonnchadha and van Genabith, 2006), a morphological analyser (U´ı Dhonnchadha et al., 2003), a shallow chunker (U´ı Dhonnchadha, 2009), a dependency treebank (Lynn et al., 2012a; Lynn et al., 2012b) and statistical dependency parsing models for MaltParser (Nivre et al., 2006) and Mate parser (Bohnet, 2010) trained on this treebank (Lynn et al., 2013). The annotation scheme for the Irish Dependency Treebank (Lynn et al., 2012b) was inspired by Lexical Functional Grammar (Bresnan, 2001) and has its roots in the dependency annotation scheme described by C¸etino˘glu et al. (2010). It was extended and adapted to suit the linguistic characterisics of the Irish language. The final label set consists of 47 dependency labels, defining grammatical and functional relations between the words in a sentence. The label set is hierarchical in nature with labels such as vparticle"
W14-4606,petrov-etal-2012-universal,0,0.482334,"urce-target constraints learned from a parallel corpus. These experiments revealed that languages that were typologically similar were not necessarily the best source-target pairs, sometimes due to variations between their language-specific annotation schemes. In more recent work, however, McDonald et al. (2013) reported improved results on cross-lingual direct transfer parsing using a universal annotation scheme, to which six chosen treebanks are mapped for uniformity purposes. Underlying the experiments with this new annotation scheme is the universal part-of-speech (POS) tagset designed by Petrov et al. (2012). While their results confirm that parsers trained on data from languages in the same language group (e.g. Romance and Germanic) show the most accurate results, they also show that training data taken across language-groups also produces promising results. We attempt to apply the direct transfer approach with Irish as the target language. The Irish language belongs to the Celtic branch of the Indo-European language family. The natural first step in cross-lingual parsing for Irish would be to look to those languages of the Celtic language This work is licensed under a Creative Commons Attributi"
W14-4606,ui-dhonnchadha-van-genabith-2006-part,0,0.082773,"Missing"
W14-4606,C10-1011,0,\N,Missing
W14-4606,P13-2017,0,\N,Missing
W15-4314,W09-2010,0,0.0999238,"x models to tokenlevel candidate selection. 96 • Carry out a full error analysis of what the system does well and where it fails. level, but in the token-level evaluation, we make two errors: wrongly appending “ laughing out” to the previous token and wrongly normalising “lol” to just “loud” instead of “laughing out loud”. Since the model P1 did not come out best, we cannot reject Chrupała (2014)’s hypothesis that the noisy channel model would not be useful. However, our observations also do not provide much support for this hypothesis as we did not include standard models from previous work (Cook and Stevenson, 2009; Han et al., 2013) in our experiment. 5 Acknowledgments This research is supported by Science Foundation Ireland through the CNGL Programme (Grant 12/CE/I2267) in the ADAPT Centre (www.adaptcentre.ie) at Dublin City University. We thank the anonymous reviewers for their comments on this paper. Furthermore, we thank Grzegorz Chrupała for sharing his feature templates and for his suggestion to try Sequor. Conclusions References We trained two sequence modellers to predict edit operations that normalise input text when executed and experimented with applying the noisy channel model to selecting"
W15-4314,P82-1020,0,0.768166,"Missing"
W15-4314,C90-2036,0,0.0821832,"dit operations at character positions that correspond to input tokens. 2. Apply insert operations recorded at the space between tokens and at the end of the tweet to the preceding token. 3. Apply delete operations at the space between tokens, moving the contents of the token to the right to the end of the token to the left, leaving behind an empty token. (Delete operations at the end-of-tweet marker are ignored.) Due to time constraints, we do not attempt to improve the alignment of output tokens to input tokens. 12 The noisy channel model has been applied successfully to spelling correction (Kemighan et al., 1990; WilcoxO’Hearn et al., 2008) and machine translation (Way, 2010), among other areas. 13 Han et al. (2013) also use a trigram language model for normalisation, but only to reduce a larger candidate set to an 11 Splitting the eight sections again would produce 216 = 65,536 candidates. 95 WB KN GT 2 14.70 14.73 14.63 3 9.97 9.83 9.88 4 7.91 7.81 7.91 5 7.31 7.33 7.45 6 7.19 7.43 7.44 P1 P1 P2 P2 Table 1: Average language model perplexity over the five cross-validation runs for n-gram sizes n = 2, ..., 6 and smoothing methods WB = WittenBell, KN = Keyser-Ney and GT = Good-Turing. Standard deviati"
W15-4314,J11-1005,0,0.0796518,"Missing"
W15-4314,P10-1052,0,0.084977,"Missing"
W15-4314,chrupala-klakow-2010-named,0,\N,Missing
W15-4314,W02-1001,0,\N,Missing
W15-4314,W15-4319,0,\N,Missing
W15-4314,P14-2111,0,\N,Missing
W16-4307,W11-0705,0,0.0604934,"nclusively more useful than mere string matches. 2 Related Work Although tree kernels have been previously used in sentiment analysis, no work has directly employed them in ABSA. The closest work has been carried out by Nguyen and Shirai (2015), who use tree kernels to first identify opinion words related to a given aspect term, which are then used in calculating the sentiment score for that aspect term. In sentence-level polarity prediction, Trindade et al. (2013) augment constituency tree kernels by inserting WordNet senses and contextual polarity of words as new nodes under terminal nodes. Agarwal et al. (2011) apply tree kernels with a customized tree format for tweets, instead of using parse trees, where tokens are gathered under a root node together with POS tags and a set of special tags used to represent the types of tokens (e.g. STOP for stop words). In documentlevel sentiment classification, Tu et al. (2012) combine constituency or dependency tree kernels with bag-of-word features. To represent documents, they use several minimal subtrees each of which contains at least one subjective word based on a sentiment lexicon. Wiegand and Klakow (2010) employ tree kernels to represent constituency an"
W16-4307,baccianella-etal-2010-sentiwordnet,0,0.0532373,"timent scores 1 0 1 0 amod punct amod punct JJ , JJ . Good , fast . (d) b with sentiment scores Figure 3: Sample plain constituency and dependency tree kernel representation for Good, fast service. (a and b) and with sentiment scores added (c and d) 4.4 Adding Sentiment Scores Sentiment lexica assign a score to each word representing the polarity of its sentiment and are often constructed automatically or semi-automatically. We follow Wagner et al. (2014) in constructing a sentiment lexicon which is a combination of four commonly used lexica including MPQA (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010), General Inquirer 6 and Opinion Lexicon (Hu and Liu, 2004). The combined polarity score using their method is in the range [-4,4], where the sign of the score represents the polarity and its value expresses the strength of the sentiment it bears. However, to avoid sparsity, we use a coarse-grained set of three scores {−1, 0, 1}, for negative, neutral/unknown, positive polarities in the same order, which also turns out to perform better in our experiments. Starting with the any-gram kernels, we replace the words with their sentiment polarity scores and replicate the experiments. Table 2 shows"
W16-4307,P14-1023,0,0.0427138,"Moschitti (2013) address this problem by generalizing words This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 60 Proceedings of the Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 60–69, Osaka, Japan, December 12 2016. using word clusters and latent semantic analysis (LSA). Word embeddings (Bengio et al., 2003), which have been used successfully in tasks involving similarity between words (Collobert and Weston, 2008; Mikolov et al., 2013; Baroni et al., 2014), are an alternative approach to obtain this generalization. We modify the tree kernel function so that the similarity of trees are computed based on the similarity of pre-trained word embedding vectors. We conduct experiments using the new kernel function and find that these similarities are not conclusively more useful than mere string matches. 2 Related Work Although tree kernels have been previously used in sentiment analysis, no work has directly employed them in ABSA. The closest work has been carried out by Nguyen and Shirai (2015), who use tree kernels to first identify opinion words r"
W16-4307,P02-1034,0,0.10868,"n neural network models and also as features in conditional random field models. Their results show that word embeddings can improve over the baselines of both of these models. To the best of our knowledge, our study is the first to use word embeddings in tree kernel computation. 3 Kernel Methods Kernel methods provide a means to define custom similarity functions, called kernel functions, which can be used by some machine learning algorithms such as support vector machines (SVM) to calculate the similarity between two data points which are not represented as vectors of numbers. Tree kernels (Collins and Duffy, 2002; Moschitti, 2006) are examples of these functions that compute the similarity between two data points represented as trees, based on the number of common fragments between them. Therefore, the need for explicitly encoding an instance in terms of manually designed and extracted features is eliminated, while benefiting from a very high-dimensional feature space. This approach has 61 NP JJ , JJ NN . JJ NN Good , fast service . Good service (a) Original tree NP JJ , (b) Subtree fragments JJ NN . JJ NN Good service (c) Subset tree fragments Figure 1: A sample tree and examples of its subtree and s"
W16-4307,W08-1301,0,0.158732,"Missing"
W16-4307,J15-2004,0,0.0186249,"n and Moschitti (2013) build a set of classifiers and re-rankers for identification of opinion holders, opinion expressions and their polarity in the MPQA corpus (Wiebe et al., 2005). These systems exploit dependency paths within opinion expressions and between opinion holders and opinion expressions. They evaluate these systems extrinsically using a product attribute (aspect) polarity classifier on a product review dataset, which is similar to the task addressed here. This classifier uses syntactic path features from candidate attributes to sentiment words and identified opinion expressions. Dong et al. (2015) introduce a contextfree grammar for sentiment in which positive and negative polarity symbols replace syntactic labels in non-terminals. They build a parser which learns this grammar using only sentences annotated with their polarity without any information about their syntactic structure. Socher et al. (2013) build a dataset of movie reviews automatically parsed and manually annotated for the polarity of each constituent. This dataset is then used to compute compositional vector representations of phrases in a neural network framework, which are then used as features in training a model to p"
W16-4307,W12-3112,0,0.0267542,"ually designed and extracted features is eliminated, while benefiting from a very high-dimensional feature space. This approach has 61 NP JJ , JJ NN . JJ NN Good , fast service . Good service (a) Original tree NP JJ , (b) Subtree fragments JJ NN . JJ NN Good service (c) Subset tree fragments Figure 1: A sample tree and examples of its subtree and subset tree fragments shown to be effective in many NLP tasks including parsing and named entity recognition (Collins and Duffy, 2002), semantic role labelling (Moschitti, 2006), sentiment analysis (see §2) and machine translation quality estimation (Hardmeier et al., 2012; Kaljahi et al., 2014). In the following sections, we describe tree kernels and introduce any-gram kernels which are built on top of tree kernels. We also introduce a model which incorporates pre-trained word embedding vectors in tree kernel calculation. 3.1 Tree Kernels The kernel function applied to tree pair T1 and T2 is defined as follows: K(T1 , T2 ) = X X ∆(n1 , n2 ) (1) n1 ∈{T1 nodes} n2 ∈{T2 nodes} where ∆ calculates the similarity between every two nodes in the tree as follows (Moschitti, 2006): ∆(n1 , n2 ) =    0 : if pr1 6= pr2   1 : if pr = pr & n , n are pre-terminals 1 2 1"
W16-4307,W13-0907,0,0.0144714,"he laptop development set), although they are computationally cheaper and require much less engineering effort to select most useful orders and combination of orders of n-grams. We measured the time spent for the classification of both test sets using each model on the same machine. The average of three runs for NGTKw and HCng1+2 were 50 and 490 ms respectively.5 The comparison also suggest that higher orders of n-gram contained in the any-gram kernels are not useful in this task. 4.2 Constituency Tree Kernels To use tree kernels for ABSA, the aspect terms need to be marked in the parse tree (Hovy et al., 2013), mainly to differentiate between multiple aspect terms in a sentence and also as information supplied to the algorithm. We tried a set of various formats for this purpose and decided to use one in which a node indicating aspect term (AT) is inserted above the pre-terminal node in the span of aspect term. The results for this format are presented in Table 2 under String Match column (SyTKc ) and an example tree is shown in Figure 3a. As can be seen, the constituency structure alone tends to be less effective than word n-grams. 4.3 Dependency Tree Kernels While constituency trees can be readily"
W16-4307,J13-3002,0,0.0209816,"n a sentiment lexicon. Wiegand and Klakow (2010) employ tree kernels to represent constituency and predicate-argument structure (PAS) in finding opinion holders. They enrich these trees by inserting nodes with generalized concept labels such as location, opinion and person. Their results show that while augmenting the constituency trees is useful, PAS trees do not benefit from extra information. Their best tree kernel setting outperforms their hand-crafted features and their combination leads to a higher performance. Syntax has also been used in sentiment analysis using hand-crafted features. Johansson and Moschitti (2013) build a set of classifiers and re-rankers for identification of opinion holders, opinion expressions and their polarity in the MPQA corpus (Wiebe et al., 2005). These systems exploit dependency paths within opinion expressions and between opinion holders and opinion expressions. They evaluate these systems extrinsically using a product attribute (aspect) polarity classifier on a product review dataset, which is similar to the task addressed here. This classifier uses syntactic path features from candidate attributes to sentiment words and identified opinion expressions. Dong et al. (2015) int"
W16-4307,C14-1194,1,0.895734,"cted features is eliminated, while benefiting from a very high-dimensional feature space. This approach has 61 NP JJ , JJ NN . JJ NN Good , fast service . Good service (a) Original tree NP JJ , (b) Subtree fragments JJ NN . JJ NN Good service (c) Subset tree fragments Figure 1: A sample tree and examples of its subtree and subset tree fragments shown to be effective in many NLP tasks including parsing and named entity recognition (Collins and Duffy, 2002), semantic role labelling (Moschitti, 2006), sentiment analysis (see §2) and machine translation quality estimation (Hardmeier et al., 2012; Kaljahi et al., 2014). In the following sections, we describe tree kernels and introduce any-gram kernels which are built on top of tree kernels. We also introduce a model which incorporates pre-trained word embedding vectors in tree kernel calculation. 3.1 Tree Kernels The kernel function applied to tree pair T1 and T2 is defined as follows: K(T1 , T2 ) = X X ∆(n1 , n2 ) (1) n1 ∈{T1 nodes} n2 ∈{T2 nodes} where ∆ calculates the similarity between every two nodes in the tree as follows (Moschitti, 2006): ∆(n1 , n2 ) =    0 : if pr1 6= pr2   1 : if pr = pr & n , n are pre-terminals 1 2 1 2     Qnc (σ + ∆(cj"
W16-4307,S14-2076,0,0.0131256,"ever, n-grams lead to large sparse feature sets which are not computationally efficient. Moreover, only a limited number of orders of n-grams (i.e. n) can be used and the choice of n requires tuning. To tackle this issue, we introduce any-gram kernels which 1) capture all orders of n-grams and 2) are faster while 3) performing at the same level as traditional n-gram features. Recent research has shown that structural features extracted from syntactic analysis of text can boost the performance of surface-oriented models, by capturing information that these models cannot (Karlgren et al., 2010; Kiritchenko et al., 2014). For example, in If you like spicy food get the chicken vindaloo., a lexicon-based model assigns a positive sentiment to the aspect term spicy food due to the nearby presence of like , whereas a syntax-based model has the potential to recognise that like does not convey an opinion when it is modified by if. We use tree kernels to model both the constituency and dependency structure of the sentences. This approach is more efficient than hand-crafted syntactic features, as it requires less engineering effort and is faster to develop. Traditional tree kernel function computes the similarity of t"
W16-4307,D15-1168,0,0.0204792,"replace syntactic labels in non-terminals. They build a parser which learns this grammar using only sentences annotated with their polarity without any information about their syntactic structure. Socher et al. (2013) build a dataset of movie reviews automatically parsed and manually annotated for the polarity of each constituent. This dataset is then used to compute compositional vector representations of phrases in a neural network framework, which are then used as features in training a model to predict the polarity of each phrase. Pre-trained word embeddings have previously been used by (Liu et al., 2015) in a similar task of aspect term extraction on the same dataset used here, as initial weights in neural network models and also as features in conditional random field models. Their results show that word embeddings can improve over the baselines of both of these models. To the best of our knowledge, our study is the first to use word embeddings in tree kernel computation. 3 Kernel Methods Kernel methods provide a means to define custom similarity functions, called kernel functions, which can be used by some machine learning algorithms such as support vector machines (SVM) to calculate the si"
W16-4307,J93-2004,0,0.0532351,"distribution, the conflict polarity accounts for only a tiny portion of the aspect terms, whereas the positive polarity dominates the datasets except for the laptop training set where it has a similar share as the negative polarity. The proportion of neutral and negative polarities tend to be similar, which is also consistent across the four datasets. Experiment Details To obtain the syntactic analysis of the data, we parse them into their constituency structures using a PCFG-LA parser (Petrov et al., 2006). The parser is trained on the entire Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). We then obtain dependency parses by converting these constituency parses using the Stanford converter (de Marneffe and Manning, 2008). To apply tree kernels, we use the SVMLight-TK implementation (Moschitti, 2006)3 . Based on a set of preliminary experiments, we use subset tree kernels and the one-versus-one (OVO) method to convert the binary output of the SVM to multi-class (positive, negative, neutral and conflict). The error/margin trade-off of the SVM (C) is tuned using development sets randomly extracted from the official training sets. For tree kernels with word embeddings, we use cosi"
W16-4307,E06-1015,0,0.581969,"nd also as features in conditional random field models. Their results show that word embeddings can improve over the baselines of both of these models. To the best of our knowledge, our study is the first to use word embeddings in tree kernel computation. 3 Kernel Methods Kernel methods provide a means to define custom similarity functions, called kernel functions, which can be used by some machine learning algorithms such as support vector machines (SVM) to calculate the similarity between two data points which are not represented as vectors of numbers. Tree kernels (Collins and Duffy, 2002; Moschitti, 2006) are examples of these functions that compute the similarity between two data points represented as trees, based on the number of common fragments between them. Therefore, the need for explicitly encoding an instance in terms of manually designed and extracted features is eliminated, while benefiting from a very high-dimensional feature space. This approach has 61 NP JJ , JJ NN . JJ NN Good , fast service . Good service (a) Original tree NP JJ , (b) Subtree fragments JJ NN . JJ NN Good service (c) Subset tree fragments Figure 1: A sample tree and examples of its subtree and subset tree fragmen"
W16-4307,W02-1011,0,0.0190165,"ent a pre-specified set of such orders. We also modify the traditional tree kernel function to compute the similarity based on word embedding vectors instead of exact string match and present experiments using the new models. 1 Introduction The automatic identification and analysis of opinion and sentiment in text (Pang and Lee, 2008) has emerged as a major natural language processing task in recent years, due in part to the abundance of opinions now available online. Initially, much of the focus of sentiment analysis research was on detecting the overall sentiment of documents and sentences (Pang et al., 2002). This kind of analysis is insufficient when the sentence or document contains multiple opinions directed towards multiple targets and the goal is to identify each of them individually. For example, a review of a laptop may discuss various features of the product such as battery life, speed and memory. While the review may carry a positive assessment of the laptop in general, the sentiment towards some of these aspects may be negative. Aspect-based sentiment analysis (ABSA) (Hu and Liu, 2004) aims to tackle this problem. In this work, we address the problem of aspect-based sentiment analysis a"
W16-4307,D14-1162,0,0.0903603,"et of preliminary experiments, we use subset tree kernels and the one-versus-one (OVO) method to convert the binary output of the SVM to multi-class (positive, negative, neutral and conflict). The error/margin trade-off of the SVM (C) is tuned using development sets randomly extracted from the official training sets. For tree kernels with word embeddings, we use cosine similarity for sim in equation 4. The θ parameter is tuned on the development set, where the optimum value is selected from {0.7, 0.8, 0.9}. The pre-trained word vectors used are the publicly available ones trained using GloVe (Pennington et al., 2014) trained on 42B-token corpus of Common Crawl (1.9M vocabulary) with 300 dimensions.4 4.1 Word Any-gram Kernels We start by modelling the word any-grams using traditional tree kernels, where for each aspect term, we take the any-gram tree formed using all tokens in the sentence in which the aspect term appears, as input 2 The conflict polarity is used when both positive and negative sentiments are expressed towards the aspect term as in Waiters are slow but sweet. 3 http://disi.unitn.it/moschitti/Tree-Kernel.htm 4 http://nlp.stanford.edu/projects/glove/. We chose these word embeddings as they c"
W16-4307,P06-1055,0,0.0331644,"an the laptop ones, as most of its sentences have more than one aspect term. In terms of the polarity class distribution, the conflict polarity accounts for only a tiny portion of the aspect terms, whereas the positive polarity dominates the datasets except for the laptop training set where it has a similar share as the negative polarity. The proportion of neutral and negative polarities tend to be similar, which is also consistent across the four datasets. Experiment Details To obtain the syntactic analysis of the data, we parse them into their constituency structures using a PCFG-LA parser (Petrov et al., 2006). The parser is trained on the entire Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). We then obtain dependency parses by converting these constituency parses using the Stanford converter (de Marneffe and Manning, 2008). To apply tree kernels, we use the SVMLight-TK implementation (Moschitti, 2006)3 . Based on a set of preliminary experiments, we use subset tree kernels and the one-versus-one (OVO) method to convert the binary output of the SVM to multi-class (positive, negative, neutral and conflict). The error/margin trade-off of the SVM (C) is tuned using development"
W16-4307,P13-1147,0,0.0266517,"e of like , whereas a syntax-based model has the potential to recognise that like does not convey an opinion when it is modified by if. We use tree kernels to model both the constituency and dependency structure of the sentences. This approach is more efficient than hand-crafted syntactic features, as it requires less engineering effort and is faster to develop. Traditional tree kernel function computes the similarity of trees based on the exact string match of the node labels including words. This method overlooks the similarity between words which can be used interchangeably in the context. Plank and Moschitti (2013) address this problem by generalizing words This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 60 Proceedings of the Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 60–69, Osaka, Japan, December 12 2016. using word clusters and latent semantic analysis (LSA). Word embeddings (Bengio et al., 2003), which have been used successfully in tasks involving similarity between words (Collobert and Weston, 2008; Mikolov et al., 2013; Baroni et al., 20"
W16-4307,S14-2004,0,0.0534097,"for the kernel computation. With θ = 1, the kernel value will be equal to the value of the traditional tree kernel. Since all possible peer node pairs in the production rule pair need to be compared, unlike the traditional tree kernel, the worst-case complexity is increased to O(N ×M ), where N and M are the number of nodes in T1 and T2 respectively. To partially remedy this situation, we use dynamic programming where we store newly calculated production rule similarity as well as node similarity in a table for later use. 4 Experiments Data We use the data released for Task 4 of SemEval 2014 (Pontiki et al., 2014) (called SE14 hereafter), which is concerned with ABSA. The data is in the form of consumer reviews from two domains: laptops and restaurants. Table 1 shows various characteristics of the data including the number of sentences and aspect terms and the percentage of each polarity type. As seen in Table 1, the restaurant dataset contains more aspect terms than the laptop ones, as most of its sentences have more than one aspect term. In terms of the polarity class distribution, the conflict polarity accounts for only a tiny portion of the aspect terms, whereas the positive polarity dominates the"
W16-4307,D13-1170,0,0.00334985,"aluate these systems extrinsically using a product attribute (aspect) polarity classifier on a product review dataset, which is similar to the task addressed here. This classifier uses syntactic path features from candidate attributes to sentiment words and identified opinion expressions. Dong et al. (2015) introduce a contextfree grammar for sentiment in which positive and negative polarity symbols replace syntactic labels in non-terminals. They build a parser which learns this grammar using only sentences annotated with their polarity without any information about their syntactic structure. Socher et al. (2013) build a dataset of movie reviews automatically parsed and manually annotated for the polarity of each constituent. This dataset is then used to compute compositional vector representations of phrases in a neural network framework, which are then used as features in training a model to predict the polarity of each phrase. Pre-trained word embeddings have previously been used by (Liu et al., 2015) in a similar task of aspect term extraction on the same dataset used here, as initial weights in neural network models and also as features in conditional random field models. Their results show that"
W16-4307,P12-2066,1,0.907875,"Missing"
W16-4307,S14-2036,1,0.827178,"ct JJ , JJ 0 . Good , fast NN . NP JJ , JJ AT . Good , fast NN . service (a) Constituency tree JJ , JJ . Good , fast . service (b) Dependency tree (c) a with sentiment scores 1 0 1 0 amod punct amod punct JJ , JJ . Good , fast . (d) b with sentiment scores Figure 3: Sample plain constituency and dependency tree kernel representation for Good, fast service. (a and b) and with sentiment scores added (c and d) 4.4 Adding Sentiment Scores Sentiment lexica assign a score to each word representing the polarity of its sentiment and are often constructed automatically or semi-automatically. We follow Wagner et al. (2014) in constructing a sentiment lexicon which is a combination of four commonly used lexica including MPQA (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010), General Inquirer 6 and Opinion Lexicon (Hu and Liu, 2004). The combined polarity score using their method is in the range [-4,4], where the sign of the score represents the polarity and its value expresses the strength of the sentiment it bears. However, to avoid sparsity, we use a coarse-grained set of three scores {−1, 0, 1}, for negative, neutral/unknown, positive polarities in the same order, which also turns out to perform"
W16-4307,N10-1121,0,0.0253281,"polarity of words as new nodes under terminal nodes. Agarwal et al. (2011) apply tree kernels with a customized tree format for tweets, instead of using parse trees, where tokens are gathered under a root node together with POS tags and a set of special tags used to represent the types of tokens (e.g. STOP for stop words). In documentlevel sentiment classification, Tu et al. (2012) combine constituency or dependency tree kernels with bag-of-word features. To represent documents, they use several minimal subtrees each of which contains at least one subjective word based on a sentiment lexicon. Wiegand and Klakow (2010) employ tree kernels to represent constituency and predicate-argument structure (PAS) in finding opinion holders. They enrich these trees by inserting nodes with generalized concept labels such as location, opinion and person. Their results show that while augmenting the constituency trees is useful, PAS trees do not benefit from extra information. Their best tree kernel setting outperforms their hand-crafted features and their combination leads to a higher performance. Syntax has also been used in sentiment analysis using hand-crafted features. Johansson and Moschitti (2013) build a set of cl"
W16-4307,H05-1044,0,0.0111118,"e (b) Dependency tree (c) a with sentiment scores 1 0 1 0 amod punct amod punct JJ , JJ . Good , fast . (d) b with sentiment scores Figure 3: Sample plain constituency and dependency tree kernel representation for Good, fast service. (a and b) and with sentiment scores added (c and d) 4.4 Adding Sentiment Scores Sentiment lexica assign a score to each word representing the polarity of its sentiment and are often constructed automatically or semi-automatically. We follow Wagner et al. (2014) in constructing a sentiment lexicon which is a combination of four commonly used lexica including MPQA (Wilson et al., 2005), SentiWordNet (Baccianella et al., 2010), General Inquirer 6 and Opinion Lexicon (Hu and Liu, 2004). The combined polarity score using their method is in the range [-4,4], where the sign of the score represents the polarity and its value expresses the strength of the sentiment it bears. However, to avoid sparsity, we use a coarse-grained set of three scores {−1, 0, 1}, for negative, neutral/unknown, positive polarities in the same order, which also turns out to perform better in our experiments. Starting with the any-gram kernels, we replace the words with their sentiment polarity scores and"
W16-4307,D09-1159,0,0.084105,"Missing"
W16-5804,W14-3902,1,0.934966,"in code-mixed data, i.e. (1) a stacked system (Solorio and Liu, 2008)2 and (2) a pipeline system (Vyas et al., 2014). To our knowledge, a comparison between these two POS tagging methods for code-mixed content, i.e. (1) and (2), has not been carried out before. In our study we compare these two POS tagging approaches which is an important contribution of this paper. In romanised and code-mixed text, words of different languages may take the same lexical form. As a result, language and POS ambiguity are in1 This is a subset of the romanised English-Bengali-Hindi code-mixed corpus described by Barman et al. (2014). 2 In a stacking approach one learner is used to perform a certain task and the output of this learner is used as features for a second learner performing the same task (in our case POS tagging). Original: Yaar tu to, GOD hain. tui JU te ki korchis? Hail u man! Translation: Buddy you are GOD. What are you doing in JU? Hail u man! 30 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 30–39, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics creased. POS labels often depend on the language in code-mixed content. Thus, modelling th"
W16-5804,W14-3333,0,0.022606,"Missing"
W16-5804,D10-1018,0,0.0322836,"rm. Afterwards, languagespecific POS taggers are applied to predict the POS labels of the word chunks. They identify that normalisation and transliteration are two challenging problems in this pipeline approach. Our inspiration behind the joint modelling of LID and POS tagging comes from the work of Sutton et al. (2007). They use Factorial Conditional Random Fields (FCRF) to jointly model POS tagging and noun-phrase chunking. In their work the FCRF achieves better accuracy than a cascaded CRF approach. FCRF is also found to be useful in joint labelling of sentence boundaries and punctuations (Lu and Ng, 2010). 3 Data We use a subset3 of 1,239 code-mixed posts and comments from the English-Bengali-Hindi corpus (a trilingual code-mixed corpus of 12K Facebook posts and comments) of Barman et al. (2014). This corpus contains word-level language annotations. Each word in the corpus is tagged with one of the following labels: (1) English, (2) Hindi, (3) Bengali, (4) Mixed, (5) Universal, (6) Named Entity and (7) Acronym. The label Universal is associated with symbols, punctuation, numbers, emoticons and universal expressions (e.g. hahaha and lol). We manually annotate POS using the universal POS tag set"
W16-5804,N13-1039,0,0.0839097,"Missing"
W16-5804,petrov-etal-2012-universal,0,0.0644053,"Data We use a subset3 of 1,239 code-mixed posts and comments from the English-Bengali-Hindi corpus (a trilingual code-mixed corpus of 12K Facebook posts and comments) of Barman et al. (2014). This corpus contains word-level language annotations. Each word in the corpus is tagged with one of the following labels: (1) English, (2) Hindi, (3) Bengali, (4) Mixed, (5) Universal, (6) Named Entity and (7) Acronym. The label Universal is associated with symbols, punctuation, numbers, emoticons and universal expressions (e.g. hahaha and lol). We manually annotate POS using the universal POS tag set 4 (Petrov et al., 2012). These annotations were performed by an annotator who is proficient in all three languages of the corpus. As we had no second annotator proficient in all three languages, 3 We are preparing to release the data set. For more information please contact the first author. 4 An alternative tag set is the one introduced for code-mixed data by Jamatia and Das (2014). However, we prefer the universal tag set because of its simplicity, its applicability to many languages and its popularity within the NLP community. we cannot present the inter-annotator agreement for the annotations. The language and P"
W16-5804,C94-1027,0,0.0821864,"combined features. This method follows the approach of Solorio and Liu (2008) with necessary adjustments. In this method, romanised code-mixed content is transliterated blindly in all languages and is presented to different POS taggers (trained with non-romanised monolingual data) as in method V2. The romanised words and the output from the monolingual taggers are used as features to train an SVM classifier on romanised code-mixed content. To keep our methodology as similar as possible to Solorio and Liu (2008) we follow the steps described below: 1. We train a Bengali and a Hindi TreeTagger (Schmid, 1994) using the SNLTR corpus with Figure 2: Stacked systems: system S1 and S2 (Section 4.3). default settings as described in Section 4.2. 2. We transliterate each token of a sentence into Hindi and Bengali irrespective of its language using Google Transliteration as in system V2. 3. After transliteration we send each transliterated output to the respective TreeTagger, i.e. we send the original sentence to the English TreeTagger, Bengali transliterated output to the Bengali TreeTagger and the Hindi transliterated output to the Hindi TreeTagger. After that we follow the stacking approach of Solorio"
W16-5804,D08-1110,0,0.789886,"nts is an example of trilingual code-mixed content: Three languages are present in this comment: English, Hindi (italics) and Bengali (bold). Bengali and Hindi words are written in romanised forms. These phenomena (code-mixing and Romanisation) can occur simultaneously and increase the ambiguity of words. For example, in the previous comment, ‘to’ could be mistaken as an English word but it is a romanised Hindi word. Moreover, the romanised form of a native word may vary according to the user’s preference. In such situations automatic processing is challenging. POS tagging in code-mixed data (Solorio and Liu, 2008; Vyas et al., 2014) is an interesting problem because of its word-level ambiguity. Traditional NLP systems trained in one language perform poorly on such multilingual code-mixed data. In this paper, we present a data set manually annotated with part of speech and language1 . We implement and explore two state-of-the-art methods for POS tagging in code-mixed data, i.e. (1) a stacked system (Solorio and Liu, 2008)2 and (2) a pipeline system (Vyas et al., 2014). To our knowledge, a comparison between these two POS tagging methods for code-mixed content, i.e. (1) and (2), has not been carried out"
W16-5804,W14-3907,0,0.0545478,"uages during conversation. Mixing multiple languages in content is known as code-mixing. We annotate a subset of a trilingual code-mixed corpus (Barman et al., 2014) with part-of-speech (POS) tags. We investigate two state-of-the-art POS tagging techniques for code-mixed content and combine the features of the two systems to build a better POS tagger. Furthermore, we investigate the use of a joint model which performs language identification (LID) and partof-speech (POS) tagging simultaneously. 1 Introduction Automatic processing of code-mixed social media content is an emerging topic in NLP (Solorio et al., 2014; Choudhury et al., 2014). Code-mixing is a linguistic phenomenon where language switching occurs at a sentence boundary (inter-sentential), or within a sentence (intra-sentential) or within a word (word-level). This phenomenon can be observed among multilingual speakers and in many languages. Additionally, non-English speakers often use Roman script to write something in social media. This is known as Romanisation. The following comment taken from a Facebook group of Indian students is an example of trilingual code-mixed content: Three languages are present in this comment: English, Hindi (it"
W16-5804,D14-1105,0,0.739169,"ilingual code-mixed content: Three languages are present in this comment: English, Hindi (italics) and Bengali (bold). Bengali and Hindi words are written in romanised forms. These phenomena (code-mixing and Romanisation) can occur simultaneously and increase the ambiguity of words. For example, in the previous comment, ‘to’ could be mistaken as an English word but it is a romanised Hindi word. Moreover, the romanised form of a native word may vary according to the user’s preference. In such situations automatic processing is challenging. POS tagging in code-mixed data (Solorio and Liu, 2008; Vyas et al., 2014) is an interesting problem because of its word-level ambiguity. Traditional NLP systems trained in one language perform poorly on such multilingual code-mixed data. In this paper, we present a data set manually annotated with part of speech and language1 . We implement and explore two state-of-the-art methods for POS tagging in code-mixed data, i.e. (1) a stacked system (Solorio and Liu, 2008)2 and (2) a pipeline system (Vyas et al., 2014). To our knowledge, a comparison between these two POS tagging methods for code-mixed content, i.e. (1) and (2), has not been carried out before. In our stud"
W18-6222,P16-1101,0,0.231061,"ne to handle the randomness introduced throughout the model training. 3.1 Convolutional networks have also shown to be useful in text classification tasks (Kim, 2014). We build another model to investigate their effect in aspect-based sentiment polarity classification. The model is displayed in Figure 1b and its performance is reported in the second row of Table 3 . Comparing the accuracy of the LSTM and CNN models, we can see that the CNN models most of the time outperform LSTM models. 3.2 Combining LSTM and CNN LSTM and CNN networks can be combined to take advantage of the benefits of both. Ma and Hovy (2016), for example, address sequence tagging problems (POS tagging and Named Entity Recognition) using a CNN at the character level, combining the resulting character embeddings with pre-trained word embeddings and feeding it into a bidirectional LSTM network. The output is then fed into a Conditional Random Field (CRF) classifier to jointly classify labels for all words in the sentence. Their combined architecture outperforms networks built using LSTMs alone, especially on the NER task. In a similar vein, Chiu and Nichols (2016) use CNNs to generate character-level features for NER and then concat"
W18-6222,Q16-1026,0,0.205508,"nd CNN networks can be combined to take advantage of the benefits of both. Ma and Hovy (2016), for example, address sequence tagging problems (POS tagging and Named Entity Recognition) using a CNN at the character level, combining the resulting character embeddings with pre-trained word embeddings and feeding it into a bidirectional LSTM network. The output is then fed into a Conditional Random Field (CRF) classifier to jointly classify labels for all words in the sentence. Their combined architecture outperforms networks built using LSTMs alone, especially on the NER task. In a similar vein, Chiu and Nichols (2016) use CNNs to generate character-level features for NER and then concatenate them with word embeddings before finally inputting them into an LSTM network. They report new state-of-the-art performance using this LSTM vs. CNN Due to their ability to remember information over sequences of words, LSTMs are a natural choice for many NLP tasks. Our first model uses one or more (bidirectional) LSTM layers as the middle layers between the input and output layers. Figure 1a shows the architecture of this model. The accuracy of the model on the laptop and restaurant datasets can be found in the first row"
W18-6222,D14-1162,0,0.0810665,"categories together with their polarity distribution in the training and test subsets of each domain. While some sentences contain multiple aspect categories (see the second and third examples in Table 1), most contain only one. 3 Neural Architecture We build our aspect-based sentiment polarity classification systems using deep neural networks including Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Convolutional Neural Networks (CNN) (LeCun et al., 1998). The input layer for these systems is the concatenation of an embedding layer, which uses pre-trained GloVe (Pennington et al., 2014) word embeddings2 (1.9M vocabulary Common Crawl), concatenated with a one-hot vector which encodes information about aspect categories. At the output layer, we use a softmax function to perform the classification into positive, negative or neutral. The middle layers are then stacks of LSTM or CNN layers, the depth of which is determined via hyper-parameter tuning. A dropout layer follows each LSTM or CNN layer to prevent Data The data used in our experiments consists of English consumer reviews (restaurants and laptops) released as part of the SemEval 2016 Shared Task on Aspect-Based Sentiment"
W18-6222,P10-2050,0,0.0365585,"g words in the input that are not contributing towards the sentiment and providing clues about how subjective a sentence is. Although the results of our multitasking experiments were somewhat disappointing, our experiments with gold sentiment expressions motivate us to continue exploring ways of using sentiment expressions in polarity classification. A pipeline approach in which the sentiment expression extraction is carried out before polarity classification is also possible, and indeed several sentiment expression extraction systems have been built with the MPQA dataset (Breck et al., 2007; Choi and Cardie, 2010; Yang and Cardie, 2012; ˙Irsoy and Cardie, 2014). We plan to build a sentiment expression extraction system using our new set of annotations and then investigate the effect of substituting gold sentiment expressions with automatically predicted sentiment expressions. The dataset reported in this work is available for use by other researchers, as a source of train/test data for sentiment expression extraction or joint polarity classification/sentiment expression extraction, as well as a potential source of linguistic insights about expressions of sentiment in this type of text.7 In addition to"
W18-6222,S16-1002,0,0.0576296,"Missing"
W18-6222,S15-2082,0,0.0523016,"Missing"
W18-6222,D14-1080,0,0.0546026,"Missing"
W18-6222,S14-2004,0,0.0279739,"level sentiment analysis. We manually annotate a freely available English sentiment polarity dataset with these boundaries and carry out a series of experiments which demonstrate that high quality sentiment expressions can boost the performance of polarity classification. Our experiments with neural architectures also show that CNN networks outperform LSTMs on this task and dataset. 1 Introduction Sentiment analysis is a much studied problem in natural language processing research, yet it is far from solved, especially when a fine-grained analysis is required. Aspect-based sentiment analysis (Pontiki et al., 2014, 2015, 2016) is concerned with multi-faceted opinions. Consider, for example, a restaurant review which states that the food was delicious but the place was too noisy. For anyone using such a review to inform a decision about where to dine, this aspect-based information is more useful than one overall sentiment score. In this work, we aim to improve the performance of sentence-level aspect-based sentiment polarity classification. We compare several neural architectures and we investigate whether identifying and highlighting the parts of the sentence which carry the sentiment can be beneficial"
W18-6222,J13-3002,0,0.0274879,"re the main rule is to find the part of the sentence which independently carries the sentiment. Therefore, said in (5) would be ignored in our annotation as it does not help recognize the sentiment towards The report. Instead, full of absurdities would be annotated as the sentiment expression towards The report, as it is clearly the source negative sentiment expressed by the speaker (Xirao-Nima). Therefore, our definition of sentiment expression is closer to the MPQA’s expressive subjective expression. A related work in terms of utilizing opinion expressions for other opinion mining tasks is (Johansson and Moschitti, 2013), who use features extracted from MPQA opinion expressions in product attribute identification (i.e. finding sentiment targets) and also document polarity classification. The features used in the second task – which is more relevant to this work – include the individual opinion expression words combined with the polarity or type of the expressions. Their results show that information extracted from opinion expressions can help improve polarity classification compared to when only bag-of-word features and sentiment polarity lexicons are used. Using a different dataset, a different type of opini"
W18-6222,S16-1043,0,0.0484102,"Missing"
W18-6222,D16-1103,0,0.026926,"Missing"
W18-6222,S16-1053,0,0.0263217,"Missing"
W18-6222,S16-1046,0,0.0241143,"Missing"
W18-6222,D12-1122,0,0.0148962,"at are not contributing towards the sentiment and providing clues about how subjective a sentence is. Although the results of our multitasking experiments were somewhat disappointing, our experiments with gold sentiment expressions motivate us to continue exploring ways of using sentiment expressions in polarity classification. A pipeline approach in which the sentiment expression extraction is carried out before polarity classification is also possible, and indeed several sentiment expression extraction systems have been built with the MPQA dataset (Breck et al., 2007; Choi and Cardie, 2010; Yang and Cardie, 2012; ˙Irsoy and Cardie, 2014). We plan to build a sentiment expression extraction system using our new set of annotations and then investigate the effect of substituting gold sentiment expressions with automatically predicted sentiment expressions. The dataset reported in this work is available for use by other researchers, as a source of train/test data for sentiment expression extraction or joint polarity classification/sentiment expression extraction, as well as a potential source of linguistic insights about expressions of sentiment in this type of text.7 In addition to our sentiment expressi"
W19-2308,W11-2832,0,0.273046,"re 1: Example of two stage generation using the pipeline system. Both are real examples generated by their respective models. Our system consists of two distinct models. The first is an utterance planning model which takes as input some structured data and generates an intermediate representation of an utterance containing one or more sentences. The intermedi1 deep here is not referring to deep learning but rather as a contrast with another UUD variant known as the shallow UUD. Shallow and deep surface realization tracks were used in both Surface Realization Shared Tasks (Mille et al., 2018a; Belz et al., 2011) 66 Figure 2: Different representations of the Deep Underspecified Universal Dependency structure but we leave this for future work. determined by the original order in which they appeared in the sentence. We chose to use a consistent, as opposed to random, ordering of equivalent level nodes for the symbolic intermediate representation as it has been shown in a number of papers (Konstas et al., 2017; Juraska et al., 2018) that neural models perform worse at given tasks when trained on symbolic intermediate representations sorted in random orders, even when that randomness is used to augment an"
W19-2308,P16-2008,0,0.084449,"Missing"
W19-2308,W18-3606,1,0.91416,"Missing"
W19-2308,P17-4012,0,0.0540315,"e generated utterances deemed to have the same meaning with the reference utterance into the next stage of readability evaluation. To evaluate readability we perform pairwise comparisons between generated utterances and reference utterances. We randomize the order during evaluation so it is not clear what the origin of a particular utterance is. We define readability, sometimes called fluency, as how well a given utterances reads, “is it fluent English Models For the neural NLG pipeline system we train two separate encoder-decoder models using the neural machine translation framework OpenNMT (Klein et al., 2017). We trained two separate encoder-decoder models for surface realization and content selection. However both used the same hyperparameters. A single layer LSTM (Hochreiter and Schmidhuber, 1997) with RNN size 450 and word vector size 300 was used. The models were trained using ADAM (Kingma and Ba, 2015) with a learning rate of 0.001. The only difference between the two models was that the surface realization model was trained with a copy attention mechanism (Vinyals et al., 2015). For the full E2E task a single planning model was trained on the E2E corpus. However two different surface realiza"
W19-2308,P19-1254,0,0.0223584,"neural system at adequately including information in the generated utterance. Other work has looked for innovative ways to separate planning and surface realization from the end-to-end neural systems, most notably Wiseman et al. (2018) which learns template generation also on the E2E task, but does not yet match baseline performance, and He et al. (2018) which has a dialogue manager control decision making and passes this information onto a secondary language generator. Other work has attempted either multi-stage semi-unconstrained language generation, such as in the domain of story telling (Fan et al., 2019), or filling-in-the-blanks style sentence reconstruction (Fedus et al., 2018). We report results on the full E2E task in Table 4. Both our systems outperform the E2E challenge winning system Slug2Slug (Juraska et al., 2018), with the system using the surface realization model trained with additional data performing slightly better. Both surface realization models received the same set of intermediate representations from the single utterance planning model. Further human evaluation may be required to establish the meaningfulness of these higher automated results. 5 Related Work The work most s"
W19-2308,P17-1014,0,0.204693,"st of our knowledge, no such system has yet been developed or adapted to generate the deep UUD structure as output. Hence it was required to make a number of changes to the deep UUD structure during preprocessing to better suit a neural system designed to use the structure as a symbolic intermediate representation; namely we linearize the UUD tree, remove accompanying token features and use the surface form of each token, see Figure 2. Methods Linearization In order to use tree structures in a sequence-to-sequence model a linearization order for nodes in the tree must be determined. Following Konstas et al. (2017) tree nodes are ordered using depth first search. Scope markers are added before each child node. When a node has only one child node we omit scope markers. Though this can lead to moderate ambiguity it greatly reduces the length of the sequence (Konstas et al., 2017). When two nodes appear at the same level in the tree their linearization order is typically chosen at random, or using some rule based heuristic or even a secondary model (Ferreira et al., 2018). In this system linearization of equivalent level tokens is Figure 1: Example of two stage generation using the pipeline system. Both ar"
W19-2308,W07-0734,0,0.0231641,"ation models were compared; one trained solely on sentences from the E2E corpus and another trained on a combined corpus of E2E and TripAdvisor sentences. For baselines on the full E2E task we compare with two encoder-decoder models which both use semantic rerankers on their generated utterances; TGen (Duˇsek and Jurcicek, 2016) the baseline system for the E2E challenge and Slug2Slug (Juraska et al., 2018) the winning system of the E2E challenge. Automated Evaluation The E2E task is evaluated using an array of automated metrics2 ; BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015). The two surface realization models were evaluated on how well they were able to realize sentences from the E2E validation set using silver parsed intermediate representations. We report BLEU-4 3 We input tokenized, lowercased and relexicalised sentences to the Moses multi-bleu perl script: https://github.com/OpenNMT/OpenNMT-py/ blob/master/tools/multi-bleu.perl 2 E2E NLG Challenge provides an official scoring script https://github.com/tuetschek/e2e-metrics 68 or does it have grammatical errors, awkward constructions, etc.” (Mille et al.,"
W19-2308,W04-1013,0,0.00898718,"rained solely on sentences from the E2E corpus and another trained on a combined corpus of E2E and TripAdvisor sentences. For baselines on the full E2E task we compare with two encoder-decoder models which both use semantic rerankers on their generated utterances; TGen (Duˇsek and Jurcicek, 2016) the baseline system for the E2E challenge and Slug2Slug (Juraska et al., 2018) the winning system of the E2E challenge. Automated Evaluation The E2E task is evaluated using an array of automated metrics2 ; BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015). The two surface realization models were evaluated on how well they were able to realize sentences from the E2E validation set using silver parsed intermediate representations. We report BLEU-4 3 We input tokenized, lowercased and relexicalised sentences to the Moses multi-bleu perl script: https://github.com/OpenNMT/OpenNMT-py/ blob/master/tools/multi-bleu.perl 2 E2E NLG Challenge provides an official scoring script https://github.com/tuetschek/e2e-metrics 68 or does it have grammatical errors, awkward constructions, etc.” (Mille et al., 2018a). By investig"
W19-2308,W17-3518,0,0.0362608,"rasing in reference sentence Ref: Have you heard of The Sorrento and The Wrestlers, they are the average friendly families. IR: heard ( you xnear ( xname and ) families ( they average friendly ) ) Gen: You can be heard near The Sorrento and The Wrestlers, they are average friendly families. (c) Nonsensical reference sentence Figure 3: Examples of reference sentences (Ref), intermediate representations (IR) and generated texts (Gen) from three different scenarios. 4.2 End-to-End Analysis based utterance planner and a neural based surface realizer. They applied this system to the WebNLG corpus (Gardent et al., 2017) and found that, compared with a strong neural system, it performed roughly equally at surface realization but exceeded the neural system at adequately including information in the generated utterance. Other work has looked for innovative ways to separate planning and surface realization from the end-to-end neural systems, most notably Wiseman et al. (2018) which learns template generation also on the E2E task, but does not yet match baseline performance, and He et al. (2018) which has a dialogue manager control decision making and passes this information onto a secondary language generator. O"
W19-2308,W04-2705,0,0.0736433,"he utterance is then passed to a second surface realization model which generates the final natural language text. See Figure 1 for an example from the E2E dataset. Both models are neural based. We use a symbolic intermediate representation to pass information between the two models. 2.1 Symbolic Intermediate Representation The symbolic intermediate representation used is the deep1 Underspecified Universal Dependency (UUD) structure (Mille et al., 2018b). The UUD structure is a tree “containing only content words linked by predicate-argument edges in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion” (Mille et al., 2018b). Each UUD structure represents a single sentence. The UUD structure was designed to “approximate the kind of abstract meaning representations used in native NLG tasks” (Mille et al., 2018b). That is, the kind of output that a rule based system could be reasonably expected to generate as part of a pipeline NLG process. However, to the best of our knowledge, no such system has yet been developed or adapted to generate the deep UUD structure as output. Hence it was required to make a number of changes to the deep UUD structure during preprocessing to better suit a"
W19-2308,W18-3601,0,0.181363,"Duˇsek and Jurcicek, 2016; Daniele et al., 2017; Puduppully et al., 2018; Hajdik et al., 2019; Moryossef et al., 2019), and inspired by more traditional pipeline data-to-text generation (Reiter and Dale, 2000; Gatt and Krahmer, 2018), we present a system which splits apart the typically end-to-end data-driven neural model into separate utterance planning and surface realization models using a symbolic intermediate representation. We focus in particular on surface realization and introduce a new symbolic intermediate representation which is based on an underspecified universal dependency tree (Mille et al., 2018b). In designing our intermediate representation, we are driven by the following constraints: Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by break"
W19-2308,N19-1235,0,0.0373265,"Missing"
W19-2308,D18-1256,0,0.0313508,"alysis based utterance planner and a neural based surface realizer. They applied this system to the WebNLG corpus (Gardent et al., 2017) and found that, compared with a strong neural system, it performed roughly equally at surface realization but exceeded the neural system at adequately including information in the generated utterance. Other work has looked for innovative ways to separate planning and surface realization from the end-to-end neural systems, most notably Wiseman et al. (2018) which learns template generation also on the E2E task, but does not yet match baseline performance, and He et al. (2018) which has a dialogue manager control decision making and passes this information onto a secondary language generator. Other work has attempted either multi-stage semi-unconstrained language generation, such as in the domain of story telling (Fan et al., 2019), or filling-in-the-blanks style sentence reconstruction (Fedus et al., 2018). We report results on the full E2E task in Table 4. Both our systems outperform the E2E challenge winning system Slug2Slug (Juraska et al., 2018), with the system using the surface realization model trained with additional data performing slightly better. Both s"
W19-2308,W18-6527,0,0.0860272,"Duˇsek and Jurcicek, 2016; Daniele et al., 2017; Puduppully et al., 2018; Hajdik et al., 2019; Moryossef et al., 2019), and inspired by more traditional pipeline data-to-text generation (Reiter and Dale, 2000; Gatt and Krahmer, 2018), we present a system which splits apart the typically end-to-end data-driven neural model into separate utterance planning and surface realization models using a symbolic intermediate representation. We focus in particular on surface realization and introduce a new symbolic intermediate representation which is based on an underspecified universal dependency tree (Mille et al., 2018b). In designing our intermediate representation, we are driven by the following constraints: Generated output from neural NLG systems often contain errors such as hallucination, repetition or contradiction. This work focuses on designing a symbolic intermediate representation to be used in multi-stage neural generation with the intention of reducing the frequency of failed outputs. We show that surface realization from this intermediate representation is of high quality and when the full system is applied to the E2E dataset it outperforms the winner of the E2E challenge. Furthermore, by break"
W19-2308,D18-1356,0,0.0195255,"sentences (Ref), intermediate representations (IR) and generated texts (Gen) from three different scenarios. 4.2 End-to-End Analysis based utterance planner and a neural based surface realizer. They applied this system to the WebNLG corpus (Gardent et al., 2017) and found that, compared with a strong neural system, it performed roughly equally at surface realization but exceeded the neural system at adequately including information in the generated utterance. Other work has looked for innovative ways to separate planning and surface realization from the end-to-end neural systems, most notably Wiseman et al. (2018) which learns template generation also on the E2E task, but does not yet match baseline performance, and He et al. (2018) which has a dialogue manager control decision making and passes this information onto a secondary language generator. Other work has attempted either multi-stage semi-unconstrained language generation, such as in the domain of story telling (Fan et al., 2019), or filling-in-the-blanks style sentence reconstruction (Fedus et al., 2018). We report results on the full E2E task in Table 4. Both our systems outperform the E2E challenge winning system Slug2Slug (Juraska et al., 2"
W19-2308,W17-5525,0,0.0728802,"Missing"
W19-2308,J05-1004,0,0.0195731,"of each sentence in the utterance is then passed to a second surface realization model which generates the final natural language text. See Figure 1 for an example from the E2E dataset. Both models are neural based. We use a symbolic intermediate representation to pass information between the two models. 2.1 Symbolic Intermediate Representation The symbolic intermediate representation used is the deep1 Underspecified Universal Dependency (UUD) structure (Mille et al., 2018b). The UUD structure is a tree “containing only content words linked by predicate-argument edges in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion” (Mille et al., 2018b). Each UUD structure represents a single sentence. The UUD structure was designed to “approximate the kind of abstract meaning representations used in native NLG tasks” (Mille et al., 2018b). That is, the kind of output that a rule based system could be reasonably expected to generate as part of a pipeline NLG process. However, to the best of our knowledge, no such system has yet been developed or adapted to generate the deep UUD structure as output. Hence it was required to make a number of changes to the deep UUD structure during preproces"
W19-2308,P02-1040,0,0.103849,"d on the E2E corpus. However two different surface realization models were compared; one trained solely on sentences from the E2E corpus and another trained on a combined corpus of E2E and TripAdvisor sentences. For baselines on the full E2E task we compare with two encoder-decoder models which both use semantic rerankers on their generated utterances; TGen (Duˇsek and Jurcicek, 2016) the baseline system for the E2E challenge and Slug2Slug (Juraska et al., 2018) the winning system of the E2E challenge. Automated Evaluation The E2E task is evaluated using an array of automated metrics2 ; BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Lavie and Agarwal, 2007), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015). The two surface realization models were evaluated on how well they were able to realize sentences from the E2E validation set using silver parsed intermediate representations. We report BLEU-4 3 We input tokenized, lowercased and relexicalised sentences to the Moses multi-bleu perl script: https://github.com/OpenNMT/OpenNMT-py/ blob/master/tools/multi-bleu.perl 2 E2E NLG Challenge provides an official scoring script https://github.com/tuetschek/e2e-metrics 68 or does it have gramm"
W19-2308,K18-2016,0,0.0448314,"hs ranging between 1 and 67 scores3 for the silver parse generated texts from the surface realization models. In both the E2E (Duˇsek et al., 2019) and WebNLG challenge (Shimorina, 2018) it was found that automated results did not correlate with the human evaluation on the sentence level. However in the Surface Realization shared task correlation between BLEU score and human evaluation was noted to be highly significant (Mille et al., 2018a). 59 tokens with an average sentence length of 13 tokens. Both corpora were sentence tokenized and parsed by the Stanford NLP universal dependency parser (Qi et al., 2018). The parsed sentences in CoNLL-U format were then further processed by a special deep UUD parser (Mille et al., 2018b). Utterances from the E2E corpus were delexicalised to anonymize restaurant names in both the name and near slots of the meaning representation. All tokens were lower cased before training. Manual Analysis The importance of using human evaluation to get a more accurate understanding of the quality of text generated by an NLG system cannot be overstated. We perform human evaluation on the outputs of the surface realization model with a silver parse of the original utterances as"
W19-2308,D17-1239,0,\N,Missing
W19-2308,I17-1015,0,\N,Missing
W19-2308,W18-6401,0,\N,Missing
W19-2308,W18-3604,0,\N,Missing
W19-5120,W15-4301,1,0.844073,"ff, 1997), can be a bottleneck in Natural Language Processing (NLP) (Sag et al., 2002). While there are several initiatives dedicated to MWE research – PARSEME (Savary et al., 2017), SIGLEX-MWE Workshops (Savary et al., 2018; Markantonatou et al., 2017; Mitkov et al., 2017) – the focus has tended to be on majority languages (Losnegaard et al., 2016). For many minority languages, a lack of resources has impeded research. Irish is one such minority language. While progress has been made over the past several years in the area of Irish NLP (U´ı Dhonnchadha and Van Genabith, 2008; Scannell, 2014; Lynn et al., 2015; Lynn, 2016), there is still a significant lack of technological support for identification and categorisation of MWEs. In fact, as a result, minimal labelling of MWEs is found in both Irish treebanks, Irish Dependency Treebank (Lynn, 2016) and Universal Dependency Treebank (Nivre et al., 2018; Lynn and Foster, 2016). There have, however, been some theoretical linguistic studies on particular forms of MWEs in Irish. In her analysis of Irish syntax, Stenson (1981) describes idiomatic copular constructions, and verb-object constructions. BlochTrojnar (2009) and Bayda (2015) have carried out ´ D"
W19-5120,W14-4605,0,0.0279726,"exicon (Jackendoff, 1997), can be a bottleneck in Natural Language Processing (NLP) (Sag et al., 2002). While there are several initiatives dedicated to MWE research – PARSEME (Savary et al., 2017), SIGLEX-MWE Workshops (Savary et al., 2018; Markantonatou et al., 2017; Mitkov et al., 2017) – the focus has tended to be on majority languages (Losnegaard et al., 2016). For many minority languages, a lack of resources has impeded research. Irish is one such minority language. While progress has been made over the past several years in the area of Irish NLP (U´ı Dhonnchadha and Van Genabith, 2008; Scannell, 2014; Lynn et al., 2015; Lynn, 2016), there is still a significant lack of technological support for identification and categorisation of MWEs. In fact, as a result, minimal labelling of MWEs is found in both Irish treebanks, Irish Dependency Treebank (Lynn, 2016) and Universal Dependency Treebank (Nivre et al., 2018; Lynn and Foster, 2016). There have, however, been some theoretical linguistic studies on particular forms of MWEs in Irish. In her analysis of Irish syntax, Stenson (1981) describes idiomatic copular constructions, and verb-object constructions. BlochTrojnar (2009) and Bayda (2015) h"
