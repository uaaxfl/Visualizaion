2020.lt4hala-1.21,Voting for {POS} tagging of {L}atin texts: Using the flair of {FLAIR} to better Ensemble Classifiers by Example of {L}atin,2020,-1,-1,4,1,16577,manuel stoeckel,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,0,"Despite the great importance of the Latin language in the past, there are relatively few resources available today to develop modern NLP tools for this language. Therefore, the EvaLatin Shared Task for Lemmatization and Part-of-Speech (POS) tagging was published in the LT4HALA workshop. In our work, we dealt with the second EvaLatin task, that is, POS tagging. Since most of the available Latin word embeddings were trained on either few or inaccurate data, we trained several embeddings on better data in the first step. Based on these embeddings, we trained several state-of-the-art taggers and used them as input for an ensemble classifier called LSTMVoter. We were able to achieve the best results for both the cross-genre and the cross-time task (90.64{\%} and 87.00{\%}) without using additional annotated data (closed modality). In the meantime, we further improved the system and achieved even better results (96.91{\%} on classical, 90.87{\%} on cross-genre and 87.35{\%} on cross-time)."
2020.lrec-1.4,On the Influence of Coreference Resolution on Word Embeddings in Lexical-semantic Evaluation Tasks,2020,-1,-1,2,0,16578,alexander henlein,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Coreference resolution (CR) aims to find all spans of a text that refer to the same entity. The F1-Scores on these task have been greatly improved by new developed End2End-approaches and transformer networks. The inclusion of CR as a pre-processing step is expected to lead to improvements in downstream tasks. The paper examines this effect with respect to word embeddings. That is, we analyze the effects of CR on six different embedding methods and evaluate them in the context of seven lexical-semantic evaluation tasks and instantiation/hypernymy detection. Especially in the last tasks we hoped for a significant increase in performance. We show that all word embedding approaches do not benefit significantly from pronoun substitution. The measurable improvements are only marginal (around 0.5{\%} in most test cases). We explain this result with the loss of contextual information, reduction of the relative occurrence of rare words and the lack of pronouns to be replaced."
2020.lrec-1.112,{T}ext{A}nnotator: A {UIMA} Based Tool for the Simultaneous and Collaborative Annotation of Texts,2020,-1,-1,3,1,16841,giuseppe abrami,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The annotation of texts and other material in the field of digital humanities and Natural Language Processing (NLP) is a common task of research projects. At the same time, the annotation of corpora is certainly the most time- and cost-intensive component in research projects and often requires a high level of expertise according to the research interest. However, for the annotation of texts, a wide range of tools is available, both for automatic and manual annotation. Since the automatic pre-processing methods are not error-free and there is an increasing demand for the generation of training data, also with regard to machine learning, suitable annotation tools are required. This paper defines criteria of flexibility and efficiency of complex annotations for the assessment of existing annotation tools. To extend this list of tools, the paper describes TextAnnotator, a browser-based, multi-annotation system, which has been developed to perform platform-independent multimodal annotations and annotate complex textual structures. The paper illustrates the current state of development of TextAnnotator and demonstrates its ability to evaluate annotation quality (inter-annotator agreement) at runtime. In addition, it will be shown how annotations of different users can be performed simultaneously and collaboratively on the same document from different platforms using UIMA as the basis for annotation."
2020.lrec-1.650,Recognizing Sentence-level Logical Document Structures with the Help of Context-free Grammars,2020,-1,-1,3,0,17942,jonathan hildebrand,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Current sentence boundary detectors split documents into sequentially ordered sentences by detecting their beginnings and ends. Sentences, however, are more deeply structured even on this side of constituent and dependency structure: they can consist of a main sentence and several subordinate clauses as well as further segments (e.g. inserts in parentheses); they can even recursively embed whole sentences and then contain multiple sentence beginnings and ends. In this paper, we introduce a tool that segments sentences into tree structures to detect this type of recursive structure. To this end, we retrain different constituency parsers with the help of modified training data to transform them into sentence segmenters. With these segmenters, documents are mapped to sequences of sentence-related {``}logical document structures{''}. The resulting segmenters aim to improve downstream tasks by providing additional structural information. In this context, we experiment with German dependency parsing. We show that for certain sentence categories, which can be determined automatically, improvements in German dependency parsing can be achieved using our segmenter for preprocessing. The assumption suggests that improvements in other languages and tasks can be achieved."
2020.isa-1.4,Transfer of {ISOS}pace into a 3{D} Environment for Annotations and Applications,2020,-1,-1,4,0,16578,alexander henlein,16th Joint ACL - ISO Workshop on Interoperable Semantic Annotation PROCEEDINGS,0,"People{'}s visual perception is very pronounced and therefore it is usually no problem for them to describe the space around them in words. Conversely, people also have no problems imagining a concept of a described space. In recent years many efforts have been made to develop a linguistic concept for spatial and spatial-temporal relations. However, the systems have not really caught on so far, which in our opinion is due to the complex models on which they are based and the lack of available training data and automated taggers. In this paper we describe a project to support spatial annotation, which could facilitate annotation by its many functions, but also enrich it with many more information. This is to be achieved by an extension by means of a VR environment, with which spatial relations can be better visualized and connected with real objects. And we want to use the available data to develop a new state-of-the-art tagger and thus lay the foundation for future systems such as improved text understanding for Text2Scene."
K19-1081,{BIO}fid Dataset: Publishing a {G}erman Gold Standard for Named Entity Recognition in Historical Biodiversity Literature,2019,0,0,5,0,26363,sajawel ahmed,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"The Specialized Information Service Biodiversity Research (BIOfid) has been launched to mobilize valuable biological data from printed literature hidden in German libraries for over the past 250 years. In this project, we annotate German texts converted by OCR from historical scientific literature on the biodiversity of plants, birds, moths and butterflies. Our work enables the automatic extraction of biological information previously buried in the mass of papers and volumes. For this purpose, we generated training data for the tasks of Named Entity Recognition (NER) and Taxa Recognition (TR) in biological documents. We use this data to train a number of leading machine learning tools and create a gold standard for TR in biodiversity literature. More specifically, we perform a practical analysis of our newly generated BIOfid dataset through various downstream-task evaluations and establish a new state of the art for TR with 80.23{\%} F-score. In this sense, our paper lays the foundations for future work in the field of information extraction in biology texts."
D19-5702,When Specialization Helps: Using Pooled Contextualized Embeddings to Detect Chemical and Biomedical Entities in {S}panish,2019,13,0,3,1,16577,manuel stoeckel,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,0,"The recognition of pharmacological substances, compounds and proteins is an essential preliminary work for the recognition of relations between chemicals and other biomedically relevant units. In this paper, we describe an approach to Task 1 of the PharmaCoNER Challenge, which involves the recognition of mentions of chemicals and drugs in Spanish medical texts. We train a state-of-the-art BiLSTM-CRF sequence tagger with stacked Pooled Contextualized Embeddings, word and sub-word embeddings using the open-source framework FLAIR. We present a new corpus composed of articles and papers from Spanish health science journals, termed the Spanish Health Corpus, and use it to train domain-specific embeddings which we incorporate in our model training. We achieve a result of 89.76{\%} F1-score using pre-trained embeddings and are able to improve these results to 90.52{\%} F1-score using specialized embeddings."
L18-1168,{F}ast{S}ense: An Efficient Word Sense Disambiguation Classifier,2018,0,1,2,1,29679,tolga uslu,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1212,A {UIMA} Database Interface for Managing {NLP}-related Text Annotations,2018,0,2,2,1,16841,giuseppe abrami,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1308,{T}ree{A}nnotator: Versatile Visual Annotation of Hierarchical Text Relations,2018,0,3,5,0,29854,philipp helfrich,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1589,{W}iki{D}ragon: A {J}ava Framework For Diachronic Content And Network Analysis Of {M}edia{W}ikis,2018,0,2,2,1,30139,rudiger gleim,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-2031,{LTV}: Labeled Topic Vector,2018,0,2,3,0,29680,daniel baumartz,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"In this paper we present LTV, a website and API that generates labeled topic classifications based on the Dewey Decimal Classification (DDC), an international standard for topic classification in libraries. We introduce nnDDC, a largely language-independent natural network-based classifier for DDC, which we optimized using a wide range of linguistic features to achieve an F-score of 87.4{\%}. To show that our approach is language-independent, we evaluate nnDDC using up to 40 different languages. We derive a topic model based on nnDDC, which generates probability distributions over semantic units for any input on sense-, word- and text-level. Unlike related approaches, however, these probabilities are estimated by means of nnDDC so that each dimension of the resulting vector representation is uniquely labeled by a DDC class. In this way, we introduce a neural network-based Classifier-Induced Semantic Space (nnCISS)."
E17-3005,{T}ext{I}mager as a Generic Interface to {R},2017,4,0,3,1,29679,tolga uslu,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"R is a very powerful framework for statistical modeling. Thus, it is of high importance to integrate R with state-of-the-art tools in NLP. In this paper, we present the functionality and architecture of such an integration by means of TextImager. We use the OpenCPU API to integrate R based on our own R-Server. This allows for communicating with R-packages and combining them with TextImager{'}s NLP-components."
W16-3212,{T}ext2voronoi: An Image-driven Approach to Differential Diagnosis,2016,21,2,1,1,16580,alexander mehler,Proceedings of the 5th Workshop on Vision and Language,0,"Differential diagnosis aims at distinguishing between diseases causing similar symptoms. This is exemplified by epilepsies and dissociative disorders. Recently, it has been shown that linguistic features of physician-patient talks allow for differentiating between these two diseases. Since this method relies on trained linguists, it is not suitable for daily use. In this paper, we introduce a novel approach, called text2voronoi, for utilizing the paradigm of text visualization to reconstruct differential diagnosis as a task of text categorization. In line with current research on linguistic differential diagnosis, we explore linguistic characteristics of physician-patient talks to span our feature space. However, unlike standard approaches to categorization, we do not use linguistic feature spaces directly, but explore visual features derived from the talksxe2x80x99 pictorial representations. That is, we provide an approach to image-driven differential diagnosis. By example of 24 talks of epileptics and dissociatively disordered patients, we show that our approach outperforms its counterpart based on the bag-of-words model."
P16-2009,On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models,2016,23,20,2,1,995,steffen eger,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We consider two graph models of semantic change. The first is a time-series model that relates embedding vectors from one time period to embedding vectors of previous time periods. In the second, we construct one graph for each word: nodes in this graph correspond to time points and edge weights to the similarity of the wordxe2x80x99s meaning across two time points. We apply our two models to corpora across three different languages. We find that semantic change is linear in two senses. Firstly, todayxe2x80x99s embedding vectors (= meaning) of words can be derived as linear combinations of embedding vectors of their neighbors in previous time periods. Secondly, self-similarity of words decays linearly in time. We consider both findings as new laws/hypotheses of semantic change."
L16-1227,Finding Recurrent Features of Image Schema Gestures: the {FIGURE} corpus,2016,11,0,2,0,34964,andy luecking,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Frankfurt Image GestURE corpus (FIGURE) is introduced. The corpus data is collected in an experimental setting where 50 naive participants spontaneously produced gestures in response to five to six terms from a total of 27 stimulus terms. The stimulus terms have been compiled mainly from image schemata from psycholinguistics, since such schemata provide a panoply of abstract contents derived from natural language use. The gestures have been annotated for kinetic features. FIGURE aims at finding (sets of) stable kinetic feature configurations associated with the stimulus terms. Given such configurations, they can be used for designing HCI gestures that go beyond pre-defined gesture vocabularies or touchpad gestures. It is found, for instance, that movement trajectories are far more informative than handshapes, speaking against purely handshape-based HCI vocabularies. Furthermore, the mean temporal duration of hand and arm movements associated vary with the stimulus terms, indicating a dynamic dimension not covered by vocabulary-based approaches. Descriptive results are presented and related to findings from gesture studies and natural language dialogue."
L16-1239,Lemmatization and Morphological Tagging in {G}erman and {L}atin: A Comparison and a Survey of the State-of-the-art,2016,0,9,3,1,995,steffen eger,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper relates to the challenge of morphological tagging and lemmatization in morphologically rich languages by example of German and Latin. We focus on the question what a practitioner can expect when using state-of-the-art solutions out of the box. Moreover, we contrast these with old(er) methods and implementations for POS tagging. We examine to what degree recent efforts in tagger development are reflected by improved accuracies â and at what cost, in terms of training and processing time. We also conduct in-domain vs. out-domain evaluation. Out-domain evaluations are particularly insightful because the distribution of the data which is being tagged by a user will typically differ from the distribution on which the tagger has been trained. Furthermore, two lemmatization techniques are evaluated. Finally, we compare pipeline tagging vs. a tagging approach that acknowledges dependencies between inflectional categories."
L16-1240,{TLT}-{CRF}: A Lexicon-supported Morphological Tagger for {L}atin Based on Conditional Random Fields,2016,4,0,2,1,26160,tim bruck,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a morphological tagger for Latin, called TTLab Latin Tagger based on Conditional Random Fields (TLT-CRF) which uses a large Latin lexicon. Beyond Part of Speech (PoS), TLT-CRF tags eight inflectional categories of verbs, adjectives or nouns. It utilizes a statistical model based on CRFs together with a rule interpreter that addresses scenarios of sparse training data. We present results of evaluating TLT-CRF to answer the question what can be learnt following the paradigm of 1st order CRFs in conjunction with a large lexical resource and a rule interpreter. Furthermore, we investigate the contigency of representational features and targeted parts of speech to learn about selective features."
L16-1677,{TG}erma{C}orp {--} A (Digital) Humanities Resource for (Computational) Linguistics,2016,0,1,3,0,34964,andy luecking,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"TGermaCorp is a German text corpus whose primary sources are collected from German literature texts which date from the sixteenth century to the present. The corpus is intended to represent its target language (German) in syntactic, lexical, stylistic and chronological diversity. For this purpose, it is hand-annotated on several linguistic layers, including POS, lemma, named entities, multiword expressions, clauses, sentences and paragraphs. In order to introduce TGermaCorp in comparison to more homogeneous corpora of contemporary everyday language, quantitative assessments of syntactic and lexical diversity are provided. In this respect, TGermaCorp contributes to establishing characterising features for resource descriptions, which is needed for keeping track of a meaningful comparison of the ever-growing number of natural language resources. The assessments confirm the special role of proper names, whose propagation in text may influence lexical and syntactic diversity measures in rather trivial ways. TGermaCorp will be made available via hucompute.org."
C16-2013,{T}ext{I}mager: a Distributed {UIMA}-based System for {NLP},2016,5,9,3,1,16579,wahed hemati,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"More and more disciplines require NLP tools for performing automatic text analyses on various levels of linguistic resolution. However, the usage of established NLP frameworks is often hampered for several reasons: in most cases, they require basic to sophisticated programming skills, interfere with interoperability due to using non-standard I/O-formats and often lack tools for visualizing computational results. This makes it difficult especially for humanities scholars to use such frameworks. In order to cope with these challenges, we present TextImager, a UIMA-based framework that offers a range of NLP and visualization tools by means of a user-friendly GUI. Using TextImager requires no programming skills."
C16-1331,Language classification from bilingual word embedding graphs,2016,35,1,3,1,995,steffen eger,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We study the role of the second language in bilingual word embeddings in monolingual semantic evaluation tasks. We find strongly and weakly positive correlations between down-stream task performance and second language similarity to the target language. Additionally, we show how bilingual word embeddings can be employed for the task of semantic language classification and that joint semantic spaces vary in meaningful ways across second languages. Our results support the hypothesis that semantic language similarity is influenced by both structural similarity as well as geography/contact."
W15-3716,Lexicon-assisted tagging and lemmatization in {L}atin: A comparison of six taggers and two lemmatization methods,2015,20,5,3,1,995,steffen eger,"Proceedings of the 9th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"We present a survey of tagging accuracies xe2x80x94 concerning part-of-speech and full morphological tagging xe2x80x94 for several taggers based on a corpus for medieval church Latin (see www.comphistsem.org). The best tagger in our sample, Lapos, has a PoS tagging accuracy of close to 96% and an overall tagging accuracy (including full morphological tagging) of about 85%. When we xe2x80x98intersectxe2x80x99 the taggers with our lexicon, the latter score increases to almost 91% for Lapos. A conservative assessment of lemmatization accuracy on our data estimates a score of 93-94% for a lexicon-based lemmatization strategy and a score of 94-95% for lemmatizing via trained lemmatizers."
S15-1014,Towards Semantic Language Classification: Inducing and Clustering Semantic Association Networks from {E}uroparl,2015,31,2,3,1,995,steffen eger,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"We induce semantic association networks from translation relations in parallel corpora. The resulting semantic spaces are encoded in a single reference language, which ensures cross-language comparability. As our main contribution, we cluster the obtained (crosslingually comparable) lexical semantic spaces. We find that, in our sample of languages, lexical semantic spaces largely coincide with genealogical relations. To our knowledge, this constitutes the first large-scale quantitative lexical semantic typology that is completely unsupervised, bottom-up, and datadriven. Our results may be important for the decision which multilingual resources to integrate in a semantic evaluation task."
vor-der-bruck-etal-2014-collex,{C}ol{L}ex.en: Automatically Generating and Evaluating a Full-form Lexicon for {E}nglish,2014,18,2,2,1,26160,tim bruck,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The paper describes a procedure for the automatic generation of a large full-form lexicon of English. We put emphasis on two statistical methods to lexicon extension and adjustment: in terms of a letter-based HMM and in terms of a detector of spelling variants and misspellings. The resulting resource, {\textbackslash}collexen, is evaluated with respect to two tasks: text categorization and lexical coverage by example of the SUSANNE corpus and the {\textbackslash}openanc."
Y12-1059,Text Readability Classification of Textbooks of a Low-Resource Language,2012,29,13,2,0,39341,zahurul islam,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"There are many languages considered to be low-density languages, either because the population speaking the language is not very large, or because insufficient digitized text material is available in the language even though millions of people speak the language. Bangla is one of the latter ones. Readability classification is an important Natural Language Processing (NLP) application that can be used to judge the quality of documents and assist writers to locate possible problems. This paper presents a readability classifier of Bangla textbook documents based on information-theoretic and lexical features. The features proposed in this paper result in an F-score that is 50% higher than that for traditional readability formulas."
islam-mehler-2012-customization,Customization of the {E}uroparl Corpus for Translation Studies,2012,16,14,2,0,39341,zahurul islam,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Currently, the area of translation studies lacks corpora by which translation scholars can validate their theoretical claims, for example, regarding the scope of the characteristics of the translation relation. In this paper, we describe a customized resource in the area of translation studies that mainly addresses research on the properties of the translation relation. Our experimental results show that the Type-Token-Ratio (TTR) is not a universally valid indicator of the simplification of translation."
gleim-mehler-2010-computational,Computational Linguistics for Mere Mortals - Powerful but Easy-to-use Linguistic Processing for Scientists in the Humanities,2010,18,2,2,1,30139,rudiger gleim,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Delivering linguistic resources and easy-to-use methods to a broad public in the humanities is a challenging task. On the one hand users rightly demand easy to use interfaces but on the other hand want to have access to the full flexibility and power of the functions being offered. Even though a growing number of excellent systems exist which offer convenient means to use linguistic resources and methods, they usually focus on a specific domain, as for example corpus exploration or text categorization. Architectures which address a broad scope of applications are still rare. This article introduces the eHumanities Desktop, an online system for corpus management, processing and analysis which aims at bridging the gap between powerful command line tools and intuitive user interfaces."
menke-mehler-2010-ariadne,The Ariadne System: A Flexible and Extensible Framework for the Modeling and Storage of Experimental Data in the Humanities.,2010,7,2,2,0,40532,peter menke,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"During the last decades, interdisciplinarity has become a central keyword in research. As a consequence, many concepts, theories and scientific methods get in contact with each other, resulting in many different strategies and variants of acquiring, structuring, and sharing data sets. To handle these kind of data sets, his paper introduces the Ariadne Corpus Management System that allows researchers to manage and create multimodal corpora from multiple heteogeneous data sources. After an introductory demarcation from other annotation and corpus management tools, the underlying data model is presented which enables users to represent and process heterogeneous data sets within a single, consistent framework. Secondly, a set of automatized procedures is described that offers assistance to researchers in various data-related use cases. Thirdly, an approach to easy yet powerful data retrieval is introduced in form of a specialised querying language for multimodal data. Finally, the web-based graphical user interface and its advantages are illustrated."
E09-2006,e{H}umanities {D}esktop - An Online System for Corpus Management and Analysis in Support of Computing in the Humanities,2009,8,10,4,1,30139,rudiger gleim,Proceedings of the Demonstrations Session at {EACL} 2009,0,This paper introduces eHumanities Desktop- an online system for corpus management and analysis in support of Computing in the Humanities. Design issues and the overall architecture are described as well as an initial set of applications which are offered by the system.
rehm-etal-2008-towards,Towards a Reference Corpus of Web Genres for the Evaluation of Genre Identification Systems,2008,31,30,3,0,60,georg rehm,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present initial results from an international and multi-disciplinary research collaboration that aims at the construction of a reference corpus of web genres. The primary application scenario for which we plan to build this resource is the automatic identification of web genres. Web genres are rather difficult to capture and to describe in their entirety, but we plan for the finished reference corpus to contain multi-level tags of the respective genre or genres a web document or a website instantiates. As the construction of such a corpus is by no means a trivial task, we discuss several alternatives that are, for the time being, mostly based on existing collections. Furthermore, we discuss a shared set of genre categories and a multi-purpose tool as two additional prerequisites for a reference corpus of web genres."
pustylnikov-etal-2008-unified,"A Unified Database of Dependency Treebanks: Integrating, Quantifying {\\&} Evaluating Dependency Data",2008,17,2,2,0,48089,olga pustylnikov,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes a database of 11 dependency treebanks which were unified by means of a two-dimensional graph format. The format was evaluated with respect to storage-complexity on the one hand, and efficiency of data access on the other hand. An example of how the treebanks can be integrated within a unique interface is given by means of the DTDB interface."
W07-1523,Web-based Annotation of Anaphoric Relations and Lexical Chains,2007,23,35,4,0,38847,maik stuhrenberg,Proceedings of the Linguistic Annotation Workshop,0,"Annotating large text corpora is a time-consuming effort. Although single-user annotation tools are available, web-based annotation applications allow for distributed annotation and file access from different locations. In this paper we present the web-based annotation application Serengeti for annotating anaphoric relations which will be extended for the annotation of lexical chains."
W07-0210,Correlations in the Organization of Large-Scale Syntactic Dependency Networks,2007,-1,-1,2,0,49077,ramon cancho,Proceedings of the Second Workshop on {T}ext{G}raphs: Graph-Based Algorithms for Natural Language Processing,0,None
W06-2801,Text Linkage in the {W}iki Medium - A Comparative Study,2006,21,11,1,1,16580,alexander mehler,Proceedings of the Workshop on {NEW} {TEXT} Wikis and blogs and other dynamic text sources,0,We analyze four different types of document networks with respect to their small world characteristics. These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems. It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks.
W06-1710,Web corpus mining by instance of {W}ikipedia,2006,20,16,2,1,30139,rudiger gleim,Proceedings of the 2nd International Workshop on Web as Corpus,0,In this paper we present an approach to structure learning in the area of web documents. This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics. A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis.
C02-1063,Hierarchical Orderings of Textual Units,2002,21,24,1,1,16580,alexander mehler,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Text representation is a central task for any approach to automatic learning from texts. It requires a format which allows to interrelate texts even if they do not share content words, but deal with similar topics. Furthermore, measuring text similarities raises the question of how to organize the resulting clusters. This paper presents cohesion trees (CT) as a data structure for the perspective, hierarchical organization of text corpora. CTs operate on alternative text representation models taking lexical organization, quantitative text characteristics, and text structure into account. It is shown that CTs realize text linkages which are lexically more homogeneous than those produced by minimal spanning trees."
