2021.starsem-1.10,{B}i{Q}u{AD}: Towards {QA} based on deeper text understanding,2021,-1,-1,2,0,962,frank grimm,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Recent question answering and machine reading benchmarks frequently reduce the task to one of pinpointing spans within a certain text passage that answers the given question. Typically, these systems are not required to actually understand the text on a deeper level that allows for more complex reasoning on the information contained. We introduce a new dataset called BiQuAD that requires deeper comprehension in order to answer questions in both extractive and deductive fashion. The dataset consist of 4,190 closed-domain texts and a total of 99,149 question-answer pairs. The texts are synthetically generated soccer match reports that verbalize the main events of each match. All texts are accompanied by a structured Datalog program that represents a (logical) model of its information. We show that state-of-the-art QA models do not perform well on the challenging long form contexts and reasoning requirements posed by the dataset. In particular, transformer based state-of-the-art models achieve F1-scores of only 39.0. We demonstrate how these synthetic datasets align structured knowledge with natural text and aid model introspection when approaching complex text understanding."
2021.argmining-1.3,Explainable Unsupervised Argument Similarity Rating with {A}bstract {M}eaning {R}epresentation and Conclusion Generation,2021,-1,-1,4,0,5818,juri opitz,Proceedings of the 8th Workshop on Argument Mining,0,"When assessing the similarity of arguments, researchers typically use approaches that do not provide interpretable evidence or justifications for their ratings. Hence, the features that determine argument similarity remain elusive. We address this issue by introducing \textit{novel argument similarity metrics} that aim at high performance and explainability. We show that Abstract Meaning Representation (AMR) graphs can be useful for representing arguments, and that novel AMR graph metrics can offer explanations for argument similarity ratings. We start from the hypothesis that \textit{similar premises} often lead to \textit{similar conclusions}{---}and extend an approach for \textit{AMR-based argument similarity rating} by estimating, in addition, the similarity of \textit{conclusions} that we automatically infer from the arguments used as premises. We show that AMR similarity metrics make argument similarity judgements more \textit{interpretable} and may even support \textit{argument quality judgements}. Our approach provides significant performance improvements over strong baselines in a \textit{fully unsupervised} setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations."
2021.argmining-1.19,Key Point Analysis via Contrastive Learning and Extractive Argument Summarization,2021,-1,-1,6,0,7888,milad alshomary,Proceedings of the 8th Workshop on Argument Mining,0,"Key point analysis is the task of extracting a set of concise and high-level statements from a given collection of arguments, representing the gist of these arguments. This paper presents our proposed approach to the Key Point Analysis Shared Task, colocated with the 8th Workshop on Argument Mining. The approach integrates two complementary components. One component employs contrastive learning via a siamese neural network for matching arguments to key points; the other is a graph-based extractive summarization model for generating key points. In both automatic and manual evaluation, our approach was ranked best among all submissions to the shared task."
2020.spnlp-1.4,Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension,2020,-1,-1,2,0,14542,hendrik horst,Proceedings of the Fourth Workshop on Structured Prediction for NLP,0,"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structured prediction problem, consisting in inferring the most probable instance of the semantic model given the text. In this work, we focus on the challenging sub-problem of cardinality prediction that consists in predicting the number of distinct individuals of each class in the semantic model. We show that cardinality prediction can successfully be approached by modeling the overall task as a joint inference problem, predicting the number of individuals of certain classes while at the same time extracting their properties. We approach this task with probabilistic graphical models computing the maximum-a-posteriori instance of the semantic model. Our main contribution lies on the empirical investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury."
2020.nlpcss-1.15,Predicting independent living outcomes from written reports of social workers,2020,-1,-1,2,0,16131,angelika maier,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,0,"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients{'} life. To support this task, we present an approach that extracts indications of independence on different life aspects from the day-to-day documentation that social workers create. We describe the process of collecting and annotating a corresponding corpus created from data records of two social work institutions with a focus on disability care. We show that the agreement on the task of annotating the observations of social workers with respect to discrete independent levels yields a high agreement of .74 as measured by Fleiss{'} Kappa. We present a classification approach towards automatically classifying an observation into the discrete independence levels and present results for different types of classifiers. Against our original expectation, we show that we reach F-Measures (macro) of 95{\%} averaged across topics, showing that this task can be automatically solved."
2020.lrec-1.695,Recent Developments for the Linguistic Linked Open Data Infrastructure,2020,-1,-1,7,0,2109,thierry declerck,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we describe the contributions made by the European H2020 project {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-use Multilingual Linked Language Data for Knowledge Services across Sectors{'}) to the further development of the Linguistic Linked Open Data (LLOD) infrastructure. Pr{\^e}t-{\`a}-LLOD aims to develop a new methodology for building data value chains applicable to a wide range of sectors and applications and based around language resources and language technologies that can be integrated by means of semantic technologies. We describe the methods implemented for increasing the number of language data sets in the LLOD. We also present the approach for ensuring interoperability and for porting LLOD data sets and services to other infrastructures, as well as the contribution of the projects to existing standards."
2020.ldl-1.5,Terme-{\\`a}-{LLOD}: Simplifying the Conversion and Hosting of Terminological Resources as Linked Data,2020,-1,-1,2,0,17273,maria buono,Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020),0,"In recent years, there has been increasing interest in publishing lexicographic and terminological resources as linked data. The benefit of using linked data technologies to publish terminologies is that terminologies can be linked to each other, thus creating a cloud of linked terminologies that cross domains, languages and that support advanced applications that do not work with single terminologies but can exploit multiple terminologies seamlessly. We present Terme-{`}a-LLOD (TAL), a new paradigm for transforming and publishing terminologies as linked data which relies on a virtualization approach. The approach rests on a preconfigured virtual image of a server that can be downloaded and installed. We describe our approach to simplifying the transformation and hosting of terminological resources in the remainder of this paper. We provide a proof-of-concept for this paradigm showing how to apply it to the conversion of the well-known IATE terminology as well as to various smaller terminologies. Further, we discuss how the implementation of our paradigm can be integrated into existing NLP service infrastructures that rely on virtualization technology. While we apply this paradigm to the transformation and hosting of terminologies as linked data, the paradigm can be applied to any other resource format as well."
W19-5806,Extending Neural Question Answering with Linguistic Input Features,2019,0,0,2,0,23790,fabian hommel,Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5),0,None
N19-1257,{Z}ero-Shot Cross-Lingual Opinion Target Extraction,2019,17,0,2,1,26204,soufian jebbara,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Aspect-based sentiment analysis involves the recognition of so called opinion target expressions (OTEs). To automatically extract OTEs, supervised learning algorithms are usually employed which are trained on manually annotated corpora. The creation of these corpora is labor-intensive and sufficiently large datasets are therefore usually only available for a very narrow selection of languages and domains. In this work, we address the lack of available annotated data for specific languages by proposing a zero-shot cross-lingual approach for the extraction of opinion target expressions. We leverage multilingual word embeddings that share a common vector space across various languages and incorporate these into a convolutional neural network architecture for OTE extraction. Our experiments with 5 languages give promising results: We can successfully train a model on annotated data of a source language and perform accurate prediction on a target language without ever using any annotated samples in that target language. Depending on the source and target language pairs, we reach performances in a zero-shot regime of up to 77{\%} of a model trained on target language data. Furthermore, we can increase this performance up to 87{\%} of a baseline model trained on target language data by performing cross-lingual learning from multiple source languages."
W18-4501,Learning Diachronic Analogies to Analyze Concept Change,2018,0,4,3,0,23791,matthias orlikowski,"Proceedings of the Second Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"We propose to study the evolution of concepts by learning to complete diachronic analogies between lists of terms which relate to the same concept at different points in time. We present a number of models based on operations on word embedddings that correspond to different assumptions about the characteristics of diachronic analogies and change in concept vocabularies. These are tested in a quantitative evaluation for nine different concepts on a corpus of Dutch newspapers from the 1950s and 1980s. We show that a model which treats the concept terms as analogous and learns weights to compensate for diachronic changes (weighted linear combination) is able to more accurately predict the missing term than a learned transformation and two baselines for most of the evaluated concepts. We also find that all models tend to be coherent in relation to the represented concept, but less discriminative in regard to other concepts. Additionally, we evaluate the effect of aligning the time-specific embedding spaces using orthogonal Procrustes, finding varying effects on performance, depending on the model, concept and evaluation metric. For the weighted linear combination, however, results improve with alignment in a majority of cases. All related code is released publicly."
P18-4012,{SANTO}: A Web-based Annotation Tool for Ontology-driven Slot Filling,2018,0,2,6,1,18026,matthias hartung,"Proceedings of {ACL} 2018, System Demonstrations",0,"Supervised machine learning algorithms require training data whose generation for complex relation extraction tasks tends to be difficult. Being optimized for relation extraction at sentence level, many annotation tools lack in facilitating the annotation of relational structures that are widely spread across the text. This leads to non-intuitive and cumbersome visualizations, making the annotation process unnecessarily time-consuming. We propose SANTO, an easy-to-use, domain-adaptive annotation tool specialized for complex slot filling tasks which may involve problems of cardinality and referential grounding. The web-based architecture enables fast and clearly structured annotation for multiple users in parallel. Relational structures are formulated as templates following the conceptualization of an underlying ontology. Further, import and export procedures of standard formats enable interoperability with external sources and tools."
W17-4124,Improving Opinion-Target Extraction with Character-Level Word Embeddings,2017,14,0,2,1,26204,soufian jebbara,Proceedings of the First Workshop on Subword and Character Level Models in {NLP},0,"Fine-grained sentiment analysis is receiving increasing attention in recent years. Extracting opinion target expressions (OTE) in reviews is often an important step in fine-grained, aspect-based sentiment analysis. Retrieving this information from user-generated text, however, can be difficult. Customer reviews, for instance, are prone to contain misspelled words and are difficult to process due to their domain-specific language. In this work, we investigate whether character-level models can improve the performance for the identification of opinion target expressions. We integrate information about the character structure of a word into a sequence labeling system using character-level word embeddings and show their positive impact on the system{'}s performance. Specifically, we obtain an increase by 3.3 points F1-score with respect to our baseline model. In further experiments, we reveal encoded character patterns of the learned embeddings and give a nuanced view of the performance differences of both models."
E17-1006,Learning Compositionality Functions on Word Embeddings for Modelling Attribute Meaning in Adjective-Noun Phrases,2017,36,10,4,1,18026,matthias hartung,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Word embeddings have been shown to be highly effective in a variety of lexical semantic tasks. They tend to capture meaningful relational similarities between individual words, at the expense of lacking the capabilty of making the underlying semantic relation explicit. In this paper, we investigate the attribute relation that often holds between the constituents of adjective-noun phrases. We use CBOW word embeddings to represent word meaning and learn a compositionality function that combines the individual constituents into a phrase representation, thus capturing the compositional attribute meaning. The resulting embedding model, while being fully interpretable, outperforms count-based distributional vector space models that are tailored to attribute meaning in the two tasks of attribute selection and phrase similarity prediction. Moreover, as the model captures a generalized layer of attribute meaning, it bears the potential to be used for predictions over various attribute inventories without re-training."
L16-1386,The Open Linguistics Working Group: Developing the Linguistic Linked Open Data Cloud,2016,16,11,4,1,1255,john mccrae,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to orient and involve researchers with the community and/or Linguistic Linked Open Data."
L16-1549,How to Address Smart Homes with a Social Robot? A Multi-modal Corpus of User Interactions with an Intelligent Environment,2016,14,20,14,0,34969,patrick holthaus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In order to explore intuitive verbal and non-verbal interfaces in smart environments we recorded user interactions with an intelligent apartment. Besides offering various interactive capabilities itself, the apartment is also inhabited by a social robot that is available as a humanoid interface. This paper presents a multi-modal corpus that contains goal-directed actions of naive users in attempts to solve a number of predefined tasks. Alongside audio and video recordings, our data-set consists of large amount of temporally aligned sensory data and system behavior provided by the environment and its interactive components. Non-verbal system responses such as changes in light or display contents, as well as robot and apartment utterances and gestures serve as a rich basis for later in-depth analysis. Manual annotations provide further information about meta data like the current course of study and user behavior including the incorporated modality, all literal utterances, language features, emotional expressions, foci of attention, and addressees."
L16-1554,Crowdsourcing Ontology Lexicons,2016,11,0,3,0,35292,bettina lanser,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In order to make the growing amount of conceptual knowledge available through ontologies and datasets accessible to humans, NLP applications need access to information on how this knowledge can be verbalized in natural language. One way to provide this kind of information are ontology lexicons, which apart from the actual verbalizations in a given target language can provide further, rich linguistic information about them. Compiling such lexicons manually is a very time-consuming task and requires expertise both in Semantic Web technologies and lexicon engineering, as well as a very good knowledge of the target language at hand. In this paper we present an alternative approach to generating ontology lexicons by means of crowdsourcing: We use CrowdFlower to generate a small Japanese ontology lexicon for ten exemplary ontology elements from the DBpedia ontology according to a two-stage workflow, the main underlying idea of which is to turn the task of generating lexicon entries into a translation task; the starting point of this translation task is a manually created English lexicon for DBpedia. Comparison of the results to a manually created Japanese lexicon shows that the presented workflow is a viable option if an English seed lexicon is already available."
W15-4205,Reconciling Heterogeneous Descriptions of Language Resources,2015,19,3,2,1,1255,john mccrae,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"Language resources are a cornerstone of linguistic research and for the development of natural language processing tools, but the discovery of relevant resources remains a challenging task. This is due to the fact that relevant metadata records are spread among different repositories and it is currently impossible to query all these repositories in an integrated fashion, as they use different data models and vocabularies. In this paper we present a first attempt to collect and harmonize the metadata of different repositories, thus making them queriable and browsable in an integrated way. We make use of RDF and linked data technologies for this and provide a first level of harmonization of the vocabularies used in the different resources by mapping them to standard RDF vocabularies including Dublin Core and DCAT. Further, we present an approach that relies on NLP and in particular word sense disambiguation techniques to harmonize resources by mapping values of attributes xe2x80x90 such as the type, license or intended use of a resource xe2x80x90 into normalized values. Finally, as there are duplicate entries within the same repository as well as across different repositories, we also report results of detection of these duplicates."
W15-4207,Linking Four Heterogeneous Language Resources as Linked Data,2015,8,2,3,0,36722,benjamin siemoneit,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"The interest in publishing language resources as linked data is increasing, as clearly corroborated by the recent growth of the Linguistic Linked Data cloud. However, the actual value of data published as linked data is the fact that it is linked across datasets, supporting integration and discovery of data. As the manual creation of links between datasets is costly and therefore does not scale well, automatic linking approaches are of great importance to increase the quality and degree of linking of the Linguistic Linked Data cloud. In this paper we examine an automatic approach to link four different datasets to each other: two terminologies, the European Migration Network (EMN) glossary as well as the Interactive Terminology for Europe (IATE), BabelNet, and the Manually Annotated Subcorpus (MASC) of the American National Corpus. We describe our methodology, present some results on the quality of the links and summarize our experiences with this small linking exercise We will make sure that the resources are added to the linguistic linked data cloud."
N15-1088,Semantic parsing of speech using grammars learned with weak supervision,2015,29,1,2,1,7931,judith gaspers,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Semantic grammars can be applied both as a language model for a speech recognizer and for semantic parsing, e.g. in order to map the output of a speech recognizer into formal meaning representations. Semantic speech recognition grammars are, however, typically created manually or learned in a supervised fashion, requiring extensive manual effort in both cases. Aiming to reduce this effort, in this paper we investigate the induction of semantic speech recognition grammars under weak supervision. We present empirical results, indicating that the induced grammars support semantic parsing of speech with a rather low loss in performance when compared to parsing of input without recognition errors. Further, we show improved parsing performance compared to applying n-gram models as language models and demonstrate how our semantic speech recognition grammars can be enhanced by weights based on occurrence frequencies, yielding an improvement in parsing performance over applying unweighted grammars."
K15-1016,Instance Selection Improves Cross-Lingual Model Training for Fine-Grained Sentiment Analysis,2015,49,3,2,1,412,roman klinger,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"Scarcity of annotated corpora for many languages is a bottleneck for training finegrained sentiment analysis models that can tag aspects and subjective phrases. We propose to exploit statistical machine translation to alleviate the need for training data by projecting annotated data in a source language to a target language such that a supervised fine-grained sentiment analysis system can be trained. To avoid a negative influence of poor-quality translations, we propose a filtering approach based on machine translation quality estimation measures to select only high-quality sentence pairs for projection. We evaluate on the language pair German/English on a corpus of product reviews annotated for both languages and compare to in-target-language training. Projection without any filtering leads to 23 % F1 in the task of detecting aspect phrases, compared to 41 % F1 for in-target-language training. Our approach obtains up to 47 % F1. Further, we show that the detection of subjective phrases is competitive to in-target-language training without filtering."
W14-6204,Ontology-based Extraction of Structured Information from Publications on Preclinical Experiments for Spinal Cord Injury Treatments,2014,30,5,10,0,38182,benjamin paassen,Proceedings of the Third Workshop on Semantic Web and Information Extraction,0,"Preclinical research in the field of central nervous system trauma advances at a fast pace, currently yielding over 8,000 new publications per year, at an exponentially growing rate. This amount of published information by far exceeds the capacity of individual scientists to read and understand the relevant literature. So far, no clinical trial has led to therapeutic approaches which achieve functional recovery in human patients. In this paper, we describe a first prototype of an ontology-based information extraction system that automatically extracts relevant preclinical knowledge about spinal cord injury treatments from natural language text by recognizing participating entity classes and linking them to each other. The evaluation on an independent test corpus of manually annotated full text articles shows a macroaverage F1 measure of 0.74 with precision 0.68 and recall 0.81 on the task of identifying entities participating in relations."
W14-4724,Modelling the Semantics of Adjectives in the Ontology-Lexicon Interface,2014,0,6,4,1,1255,john mccrae,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,None
W14-3418,Towards Gene Recognition from Rare and Ambiguous Abbreviations using a Filtering Approach,2014,23,2,4,1,18026,matthias hartung,Proceedings of {B}io{NLP} 2014,0,"Retrieving information about highly ambiguous gene/protein homonyms is a challenge, in particular where their non-protein meanings are more frequent than their protein meaning (e. g., SAH or HF). Due to their limited coverage in common benchmarking data sets, the performance of existing gene/protein recognition tools on these problematic cases is hard to assess. We uniformly sample a corpus of eight ambiguous gene/protein abbreviations from MEDLINEr and provide manual annotations for each mention of these abbreviations. 1 Based on this resource, we show that available gene recognition tools such as conditional random fields (CRF) trained on BioCreative 2 NER data or GNAT tend to underperform on this phenomenon. We propose to extend existing gene recognition approaches by combining a CRF and a support vector machine. In a crossentity evaluation and without taking any entity-specific information into account, our model achieves a gain of 6 points F1-Measure over our best baseline which checks for the occurrence of a long form of the abbreviation and more than 9 points over all existing tools investigated."
W14-2608,An Impact Analysis of Features in a Classification Approach to Irony Detection in Product Reviews,2014,37,44,2,0,38646,konstantin buschmeier,"Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Irony is an important device in human communication, both in everyday spoken conversations as well as in written texts including books, websites, chats, reviews, and Twitter messages among others. Specific cases of irony and sarcasm have been studied in different contexts but, to the best of our knowledge, only recently the first publicly available corpus including annotations about whether a text is ironic or not has been published by Filatova (2012). However, no baseline for classification of ironic or sarcastic reviews has been provided. With this paper, we aim at closing this gap. We formulate the problem as a supervised classification task and evaluate different classifiers, reaching an F1-measure of up to 74 % using logistic regression. We analyze the impact of a number of features which have been proposed in previous research as well as combinations of them."
W14-0507,A multimodal corpus for the evaluation of computational models for (grounded) language acquisition,2014,21,2,4,1,7931,judith gaspers,Proceedings of the 5th Workshop on Cognitive Aspects of Computational Language Learning ({C}og{ACLL}),0,"This paper describes the design and acquisition of a German multimodal corpus for the development and evaluation of computational models for (grounded) language acquisition and algorithms enabling corresponding capabilities in robots. The corpus contains parallel data from multiple speakers/actors, including speech, visual data from different perspectives and body posture data. The corpus is designed to support the development and evaluation of models learning rather complex grounded linguistic structures, e.g. syntactic patterns, from sub-symbolic input. It provides moreover a valuable resource for evaluating algorithms addressing several other learning processes, e.g. concept formation or acquisition of manipulation skills. The corpus will be made available to the public."
S14-2016,Bielefeld {SC}: Orthonormal Topic Modelling for Grammar Induction,2014,5,0,2,1,1255,john mccrae,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, we consider the application of topic modelling to the task of inducting grammar rules. In particular, we look at the use of a recently developed method called orthonormal explicit topic analysis, which combines explicit and latent models of semantics. Although, it remains unclear how topic model may be applied to the case of grammar induction, we show that it is not impossible and that this may allow the capture of subtle semantic distinctions that are not captured by other methods."
ehrmann-etal-2014-representing,Representing Multilingual Data as Linked Data: the Case of {B}abel{N}et 2.0,2014,36,40,5,0,16860,maud ehrmann,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Recent years have witnessed a surge in the amount of semantic information published on the Web. Indeed, the Web of Data, a subset of the Semantic Web, has been increasing steadily in both volume and variety, transforming the Web into a {`}global database{'} in which resources are linked across sites. Linguistic fields -- in a broad sense -- have not been left behind, and we observe a similar trend with the growth of linguistic data collections on the so-called {`}Linguistic Linked Open Data (LLOD) cloud{'}. While both Semantic Web and Natural Language Processing communities can obviously take advantage of this growing and distributed linguistic knowledge base, they are today faced with a new challenge, i.e., that of facilitating multilingual access to the Web of data. In this paper we present the publication of BabelNet 2.0, a wide-coverage multilingual encyclopedic dictionary and ontology, as Linked Data. The conversion made use of lemon, a lexicon model for ontologies particularly well-suited for this enterprise. The result is an interlinked multilingual (lexical) resource which can not only be accessed on the LOD, but also be used to enrich existing datasets with linguistic information, or to support the process of mapping datasets across languages."
klinger-cimiano-2014-usage,The {USAGE} review corpus for fine grained multi lingual opinion analysis,2014,25,13,2,1,412,roman klinger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Opinion mining has received wide attention in recent years. Models for this task are typically trained or evaluated with a manually annotated dataset. However, fine-grained annotation of sentiments including information about aspects and their evaluation is very labour-intensive. The data available so far is limited. Contributing to this situation, this paper describes the Bielefeld University Sentiment Analysis Corpus for German and English (USAGE), which we offer freely to the community and which contains the annotation of product reviews from Amazon with both aspects and subjective phrases. It provides information on segments in the text which denote an aspect or a subjective evaluative phrase which refers to the aspect. Relations and coreferences are explicitly annotated. This dataset contains 622 English and 611 German reviews, allowing to investigate how to port sentiment analysis systems across languages and domains. We describe the methodology how the corpus was created and provide statistics including inter-annotator agreement. We further provide figures for a baseline system and results for German and English as well as in a cross-domain setting. The results are encouraging in that they show that aspects and phrases can be extracted robustly without the need of tuning to a particular type of products."
W13-5501,Linguistic Linked Open Data ({LLOD}). Introduction and Overview,2013,13,2,2,0.125201,2108,christian chiarcos,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,None
W13-5507,Releasing multimodal data as Linguistic Linked Open Data: An experience report,2013,13,2,3,0,40532,peter menke,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,"In this paper we describe an implemented framework for releasing multimodal corpora as Linked Data. In particular, we describe our experiences in releasing a multimodal corpus based on an online chat game as Linked Data. Building on an internal multimodal data model we call FiESTA, we have implemented a library that enhances existing libraries and classes by functionality allowing to convert the data to RDF. Our framework is implemented on the Rails web application framework. We argue that this work can be highly useful for further contributions to the Linked Data community, especially from the fields of spoken dialogue and multimodal communication."
W13-5203,Mining translations from the web of open linked data,2013,15,6,2,1,1255,john mccrae,"Proceedings of the Joint Workshop on {NLP}{\\&}{LOD} and {SWAIE}: Semantic Web, Linked Open Data and Information Extraction",0,"In this paper we consider the prospect of extracting translations for words from the web of linked data. By searching for entities that have labels in both English and German we extract 665,000 translations. We then also consider a linguistic linked data resource, lemonUby, from which we extract a further 115,000 translations. We combine these translations with the Moses statistical machine translation, and we show that the translations extracted from the linked data can be used to improve the translation of unknown words."
W13-3803,Ontology Lexicalization as a core task in a language-enhanced Semantic Web,2013,0,0,1,1,963,philipp cimiano,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-2102,Exploiting Ontology Lexica for Generating Natural Language Texts from {RDF} Data,2013,22,22,1,1,963,philipp cimiano,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"The increasing amount of machinereadable data available in the context of the Semantic Web creates a need for methods that transform such data into human-comprehensible text. In this paper we develop and evaluate a Natural Language Generation (NLG) system that converts RDF data into natural language text based on an ontology and an associated ontology lexicon. While it follows a classical NLG pipeline, it diverges from most current NLG systems in that it exploits an ontology lexicon in order to capture context-specific lexicalisations of ontology concepts, and combines the use of such a lexicon with the choice of lexical items and syntactic structures based on statistical information extracted from a domain-specific corpus. We apply the developed approach to the cooking domain, providing both an ontology and an ontology lexicon in lemon format. Finally, we evaluate fluency and adequacy of the generated recipes with respect to two target audiences: cooking novices and advanced cooks."
P13-2147,Bi-directional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model,2013,21,16,2,1,412,roman klinger,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Opinion mining is often regarded as a classification or segmentation task, involving the prediction of i) subjective expressions, ii) their target and iii) their polarity. Intuitively, these three variables are bidirectionally interdependent, but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that cannot model the bidirectional interaction between these variables. Towards better understanding the interaction between these variables, we propose a model that allows for analyzing the relation of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts."
D13-1179,Orthonormal Explicit Topic Analysis for Cross-Lingual Document Matching,2013,18,9,2,1,1255,john mccrae,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Cross-lingual topic modelling has applications in machine translation, word sense disambiguation and terminology alignment. Multilingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents in different languages to be mapped into the same space and thus to be compared across languages. In this paper, we present a novel approach that combines latent and explicit topic modelling approaches in the sense that it builds on a set of explicitly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language."
W12-1801,Up from Limited Dialog Systems!,2012,3,1,2,0,2754,giuseppe riccardi,{NAACL}-{HLT} Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data ({SDCTD} 2012),0,"In the last two decades, information-seeking spoken dialog systems (SDS) have moved from research prototypes to real-life commercial applications. Still, dialog systems are limited by the scale, complexity of the task and coverage of knowledge required by problem-solving machines or mobile personal assistants. Future spoken interaction are required to be multilingual, understand and act on large scale knowledge bases in all its forms (from structured to unstructured). The Web research community have striven to build large scale and open multilingual resources (e.g. Wikipedia) and knowledge bases (e.g. Yago). We argue that a) it is crucial to leverage this massive amount of Web lightly structured knowledge and b) the scale issue can be addressed collaboratively and design open standards to make tools and resources available to the whole speech and language community."
mccrae-etal-2012-collaborative,Collaborative semantic editing of linked data lexica,2012,23,12,3,1,1255,john mccrae,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The creation of language resources is a time-consuming process requiring the efforts of many people. The use of resources collaboratively created by non-linguistists can potentially ameliorate this situation. However, such resources often contain more errors compared to resources created by experts. For the particular case of lexica, we analyse the case of Wiktionary, a resource created along wiki principles and argue that through the use of a principled lexicon model, namely Lemon, the resulting data could be better understandable to machines. We then present a platform called Lemon Source that supports the creation of linked lexical data along the Lemon model. This tool builds on the concept of a semantic wiki to enable collaborative editing of the resources by many users concurrently. In this paper, we describe the model, the tool and present an evaluation of its usability based on a small group of users."
W11-2406,Representing and resolving ambiguities in ontology-based question answering,2011,11,11,2,1,35293,christina unger,Proceedings of the {T}ext{I}nfer 2011 Workshop on Textual Entailment,0,"Ambiguities are ubiquitous in natural language and pose a major challenge for the automatic interpretation of natural language expressions. In this paper we focus on different types of lexical ambiguities that play a role in the context of ontology-based question answering, and explore strategies for capturing and resolving them. We show that by employing underspecification techniques and by using ontological reasoning in order to filter out inconsistent interpretations as early as possible, the overall number of interpretations can be effectively reduced by 44 %."
W11-1013,Combining statistical and semantic approaches to the translation of ontologies and taxonomies,2011,30,19,5,1,1255,john mccrae,"Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Ontologies and taxonomies are widely used to organize concepts providing the basis for activities such as indexing, and as background knowledge for NLP tasks. As such, translation of these resources would prove useful to adapt these systems to new languages. However, we show that the nature of these resources is significantly different from the free-text paradigm used to train most statistical machine translation systems. In particular, we see significant differences in the linguistic nature of these resources and such resources have rich additional semantics. We demonstrate that as a result of these linguistic differences, standard SMT methods, in particular evaluation metrics, can produce poor performance. We then look to the task of leveraging these semantics for translation, which we approach in three ways: by adapting the translation system to the domain of the resource; by examining if semantics can help to predict the syntactic structure used in translation; and by evaluating if we can use existing translated taxonomies to disambiguate translations. We present some early results from these experiments, which shed light on the degree of success we may have with each approach."
W10-4408,Generating {LTAG} grammars from a lexicon/ontology interface,2010,10,10,3,1,35293,christina unger,Proceedings of the 10th International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+10),0,This paper shows how domain-specific grammars can be automatically generated from a declarative model of the lexicon-ontology interface and how those grammars can be used for question answering. We show a specific implementation of the approach using Lexicalized Tree Adjoining Grammars. The main characteristic of the generated elementary trees is that they constitute domains of locality that span lexicalizations of ontological concepts rather than being based on requirements of single lexical heads.
W09-3726,Flexible Semantic Composition with {DUDES} (short paper),2009,7,0,1,1,963,philipp cimiano,Proceedings of the Eight International Conference on Computational Semantics,0,"In this paper we present a novel formalism for semantic construction called DUDES (Dependency-based Underspecified Discourse REpresentation Structures). The DUDES formalism has been designed to overcome the rigidity of semantic composition based on the lambda calculus (where the order of application is typically fixed) and provides some flexibility with respect to the direction of the dependence and with respect to the order of application of arguments. In this short paper we present the DUDES formalism and work through a simple example. DUDES bears some resemblance to the work on xcexbb-DRT [2] and LUDs [1] as well as with the work of Copestake et al. [4] and represents a generalization of the formalism introduced in [3]. A detailed discussion of the relation to these formalisms is clearly out of the scope of this paper. DUDES are characterized by three main facts. First, they represent semantic dependencies and are thus inherently suitable for a dependency-based grammar formalism assuming that syntactic dependencies correspond to semantic dependencies (though the correspondence might be xe2x80x9cinvertedxe2x80x9d). Second, they explicitly encode scope relations and are thus able to yield underspecified representations as output (in contrast to the linear logic approach for LFG [5] where different scopings correspond to different derivations). Third, there is one single operation for semantic composition which is, to some extent, order-independent (in contrast to traditional lambda-based formalisms) as well as flexible with respect to the direction of the syntactic dependency. As the name suggests, DUDES builds on DRT [6] and in particular on UDRT [7] in the sense that it relies on labeled DRSs and dominance relations between these to talk about scope. First of all, we now first formally introduce DUDES:"
P07-1112,Automatic Acquisition of Ranked Qualia Structures from the Web,2007,15,47,1,1,963,philipp cimiano,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"This paper presents an approach for the automatic acquisition of qualia structures for nouns from the Web and thus opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale. The approach builds on earlier work based on the idea of matching specific lexico-syntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. In our approach, the qualia elements are actually ranked for each qualia role with respect to some measure. The specific contribution of the paper lies in the extensive analysis and quantitative comparison of different measures for ranking the qualia elements. Further, for the first time, we present a quantitative evaluation of such an approach for learning qualia structures with respect to a handcrafted gold standard."
W06-3915,Ingredients of a first-order account of bridging,2006,0,3,1,1,963,philipp cimiano,Proceedings of the Fifth International Workshop on Inference in Computational Semantics ({IC}o{S}-5),0,None
buitelaar-etal-2006-ontology,Ontology-based Information Extraction with {SOBA},2006,11,72,2,0,6276,paul buitelaar,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities."
E06-2010,Generating and Visualizing a Soccer Knowledge Base,2006,10,13,7,0,6276,paul buitelaar,Demonstrations,0,"This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA). A key feature of SOBA is that all information is extracted and stored with respect to the SmartWeb Integrated Ontology (SWIntO). In this way, other components of the systems, which use the same ontology, can access this information in a straightforward way. We will show how information extracted by SOBA is visualized within its original context, thus enhancing the browsing experience of the end user."
W05-1004,Automatically Learning Qualia Structures from the Web,2005,18,34,1,1,963,philipp cimiano,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"Qualia Structures have many applications within computational linguistics, but currently there are no corresponding lexical resources such as WordNet or FrameNet. This paper presents an approach to automatically learn qualia structures for nominals from the World Wide Web and thus opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale. Furthermore, our approach can be also used support a lexicographer in the task of manually creating a lexicon of qualia structures. The approach is based on the idea of matching certain lexico-syntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. We evaluate our approach qualitatively by comparing our automatically learned qualia structures with the ones from the literature, but also quantitatively by presenting results of a human evaluation."
cimiano-etal-2004-clustering,Clustering Concept Hierarchies from Text,2004,16,30,1,1,963,philipp cimiano,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We present a novel approach to learning taxonomies or concept hierarchies from text. The approach is based on Formal Concept Analysis, a method mainly used for the analysis of data, i.e. for investigating and processing explicitly given information. Our approach is based on the distributional hypothesis, i.e. that nouns or terms are similar to the extent to which they share contexts. Further, we assume that verbs pose more or less strong selectional restrictions on their arguments. The concept hierarchy is built via Formal Concept Analysis using syntactic dependencies as attributes. The approach is evaluated by comparing the produced concept hierarchies against two handcrafted taxonomies from two different domains: tourism and finance. We compare the results of our approach against a hierarchical bottom-up clustering algorithm as well as against Bi-Section-Kmeans as an instance of a top-down clustering algorithm."
W03-1903,Ontology-based Linguistic Annotation,2003,19,12,1,1,963,philipp cimiano,Proceedings of the {ACL} 2003 Workshop on Linguistic Annotation: Getting the Model Right,0,"We propose an ontology-based framework for linguistic annotation of written texts. We argue that linguistic annotation can be actually considered a special case of semantic annotation with regard to an ontology such as pursued within the context of the Semantic Web. Furthermore, we present CREAM, a semantic annotation framework, as well as its concrete implementation OntoMat and show how they can be used for the purpose of linguistic annotation. We demonstrate the value of our framework by applying it to the annotation of anaphoric relations in written texts."
