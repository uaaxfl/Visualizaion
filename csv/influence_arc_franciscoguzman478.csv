2009.mtsummit-papers.5,W08-0509,1,0.870933,"Missing"
2009.mtsummit-papers.5,N03-1017,0,0.00797689,"verall, we do observe the tendency that less number of unaligned words in the word alignment leads to better quality of the extracted phrase pairs. In other words low precision/ high recall alignment results in fewer but higher quality phrase pairs. To balance the trade-off between higher quality phrases and coverage, we conducted a series of translation experiments were the number of unaligned words was taken as a feature. In next section, we describe them thoroughly. 6 From Phrases to Translations After the phrases are extracted, they are scored according to the MLE estimation described in (Koehn et al., 2003). Also, the reordering models and the lexical weighting are estimated. Then, these models (along with the language model) are used during decoding. For this study, we wanted to analyze the Setup For this experiment, we used a training data set consisting of the GALE P3 Data3 . The data was filtered to have maximum sentence length 30. The final training set contains one million sentences. The different systems that were used, were built upon the alignments from the DWA with p = {0.1..0.0}, and the symmetrized alignment (grow-diag-final). The DWA Tuning remained the same as for Sec 3. We use the"
2009.mtsummit-papers.5,P07-2045,0,0.0177331,"o detangle the intricate relationships between the word alignment and the phrase extraction. In the following section we analyze different characteristics of the alignment that have an impact on the phrase-table generation. 3 performing training through the standard sequence of word alignment models IBM1, HMM, IBM3 and finally IBM4, in both directions, i.e. source to target (S2T) and target to source (T2S). We used the modified GIZA toolkit (Gao and Vogel, 2008). In addition, we generated the symmetrized alignment, using the grow-diag-final heuristic implemented and used in the MOSES package (Koehn et al., 2007). For the discriminative alignments, we used the approach described in (Niehues and Vogel, 2008), because the output alignment matrix generated by such a system is composed of continuous values representing the alignment strength between source and target word. Therefore it allows to easily control the density of the alignment matrix, by using different intensity thresholds, without having to recalculate the alignment. The different thresholds used throughout this paper are p = {0.1, 0.2, ..., 0.9}. In the following experiments, the discriminative word aligner (DWA) uses the models from the GI"
2009.mtsummit-papers.5,W08-0303,1,0.837733,"d alignments in machine translation remains rather unclear. Furthermore, there are several processing steps that follow the word alignment which rarely are taken into account. Most notably the algorithm used to extract phrase pairs consistent with the word alignment (Och and Ney, 2004). The goal of better understanding the relationship between the alignment metrics (AER, precision, recall) and translation quality, is to make improvements in word alignment carry over to improvements in the end-to-end system performance. This is especially important in the case of discriminative word alignment (Niehues and Vogel, 2008), where optimization towards a given manual alignment is used. In this paper we study in more detail the dependencies between the word alignment and the phrase extraction, as an effort to better understand the role of word alignments in phrase extraction. We explore characteristics of the alignment such as link density and number of unaligned words, and their implications on the phrase-pairs extracted from them. We also make a first attempt to include the findings of our analysis as new features of the translation model. The remainder of this paper is organized as follows: in Section 2 we give"
2009.mtsummit-papers.5,J04-4002,0,0.11637,"re being used in translation (e.g. phrase extraction), the quality of word alignments is not necessarily related to the quality of translation. This poses the question of whether alignment quality metrics (such as AER) might not be as good predicting translation quality as other metrics. As a result, the role of the quality of word alignments in machine translation remains rather unclear. Furthermore, there are several processing steps that follow the word alignment which rarely are taken into account. Most notably the algorithm used to extract phrase pairs consistent with the word alignment (Och and Ney, 2004). The goal of better understanding the relationship between the alignment metrics (AER, precision, recall) and translation quality, is to make improvements in word alignment carry over to improvements in the end-to-end system performance. This is especially important in the case of discriminative word alignment (Niehues and Vogel, 2008), where optimization towards a given manual alignment is used. In this paper we study in more detail the dependencies between the word alignment and the phrase extraction, as an effort to better understand the role of word alignments in phrase extraction. We exp"
2009.mtsummit-papers.5,2006.iwslt-papers.7,0,0.293207,"Missing"
2009.mtsummit-papers.5,J07-3002,0,\N,Missing
2009.mtsummit-papers.5,P06-1002,0,\N,Missing
2013.iwslt-evaluation.8,P07-2045,0,0.0254044,"ibe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by"
2013.iwslt-evaluation.8,W10-1738,0,0.130794,"r evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, a"
2013.iwslt-evaluation.8,P10-4002,0,0.0806549,"Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc."
2013.iwslt-evaluation.8,N06-2013,0,0.10965,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P08-2039,0,0.073174,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P12-1016,0,0.268259,"lish BLEU 1-TER System English IWSLT mono 109 English-French SETimes UN (Es-En + En-Fr) UN (Ar-En) News Crawl 2007-2009 News Crawl 2009-2012 Common Crawl Wiki Headlines Europarl v.7 News Commentary v.8 Gigaword v.5 2.7M 575M 4.2M 597M 115M 643M 745M 185M 1.1M 54M 5.3M 4,032M Arabic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We"
2013.iwslt-evaluation.8,J93-2003,0,0.0246271,"abic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard"
2013.iwslt-evaluation.8,N03-1017,0,0.0173015,"M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, th"
2013.iwslt-evaluation.8,2005.iwslt-1.8,0,0.0225691,"rd tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. De"
2013.iwslt-evaluation.8,W11-2123,0,0.408919,"sing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On"
2013.iwslt-evaluation.8,P02-1040,0,0.0879883,", thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers"
2013.iwslt-evaluation.8,D11-1125,0,0.032033,"each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSL"
2013.iwslt-evaluation.8,C12-1121,1,0.915839,"ltEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSLT baseline Our baseline 23.6 24.7 English-Arabic BLEU 1-TER 43.0 45.6 11.9 12.6 28.6 29.1 Table 2: Our vs. IWSLT baseline results for English-toArabic and Arabic-to-English SMT, evaluated on tst2010. 4. System Settings Below we discuss the decoder settings and extensions we experimented with, focusing on Arabic-to-English. Table 3 shows the impact of each feature when added to the baseline. Tuning. [13] have shown that PRO tends to generate too short translations.4 They have suggested that the root of the problem was that PRO optimizes sentence-level BLEU+1, which smooths the precision component of BLEU, but leaves the brevity penalty intact, which destroys the balance between them. They have proposed a number of fixes, the simplest and most efficient among them being to smooth the brevity penalty as well.5 In our experiments, this yielded +0.2 BLEU for Arabic-to-English on tst2010. Operation sequence model. The operation sequence model (OSM) is an n-gram-based model, which represents the al"
2013.iwslt-evaluation.8,P13-2003,1,0.839319,"Missing"
2013.iwslt-evaluation.8,P13-2071,0,0.0200742,"of operations, e.g., generate a sequence of source and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0"
2013.iwslt-evaluation.8,W13-2212,0,0.0142268,"urce and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Trans"
2013.iwslt-evaluation.8,N04-1022,0,0.108744,"e model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1"
2013.iwslt-evaluation.8,P11-1044,1,0.871958,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P12-1049,1,0.860515,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P03-1021,0,0.0120716,"hierarchical cdec decoder [3]. We used its default features: forward and backward translation features, singleton features, a glue-rule probability, and a pass-through feature (to handle OOVs). We tuned the parameters using MIRA with IBM BLEU as the objective function and a k-best forest size of 250. Jane. We also used another hierarchical phrase-based decoder: Jane 2.2 [2]. We used the standard features: phrase translation probabilities and lexical smoothing in both directions, word and phrase penalties, a distance-based distortion model, and a 5-gram LM. We optimized the weights using MERT [21] on 100-best candidates with BLEU as objective. 5. Arabic Segmentation 7. Adaptation In Arabic, various clitics such as pronouns, conjunctions and articles appear concatenated to content words such as nouns and verbs. This can cause data sparseness issues, and thus clitics are typically segmented in a preprocessing step. There are various standard segmentation schemes defined in MADA [4, 5] such as D0, D1, D2, D3 and S2, for which we used the MADA+TOKAN toolkit [20], as well as ATB, which we performed using the Stanford segmenter [6]. Table 5 shows the results when training on the TED bitext o"
2013.iwslt-evaluation.8,P10-2041,0,0.0888063,"Missing"
2013.iwslt-evaluation.8,D11-1033,0,0.090854,"Missing"
2013.iwslt-evaluation.8,W08-0320,1,0.891463,"Missing"
2013.iwslt-evaluation.8,D09-1141,1,0.919233,"Missing"
2013.iwslt-evaluation.8,W09-0408,0,0.0608322,"Missing"
2013.iwslt-evaluation.8,W12-5611,0,0.0448251,"could build a strong LM through interpolation, similarly to our Arabic-to-English LM, that also used the Gigaword Arabic, UN, and News Commentary data (see Table 1). Desegmentation. Unlike the Arabic-to-English direction, where the segmentation was on the input side and thus the output was unaffected, here the segmentation had to be undone. For example, if we use an ATB-segmented target side, we end up with an ATB-segmented translation output, which we have to desegment in order to obtain proper Arabic. Desegmentation is not a trivial task since it involves some morphological adjustments, see [27] for a broader discussion. For desegmentation, we used the best approach described in [27]; in fact, we used their implementation. Normalization. Translating into Arabic is tricky because the Arabic spelling is often inconsistent in terms of punctuation (using both Arabic UTF8 and English punctuation symbols), digits (appearing as both Arabic and Indian characters), diacritics (can be used or omitted, and can often be wrong), spelling (there are many errors in the spelling of some Arabic characters, esp. Alef and Ta Marbuta; also, Waa appears sometimes separated). These problems are especially"
2013.iwslt-papers.2,2010.iwslt-evaluation.1,0,0.145712,"sed for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content. 1. Introduction Lecture Translation has become an active field of research in the wider area of Speech Translation [1, 2]. This is demonstrated by large scale projects like the EU-funded translectures [3] and by evaluation campaigns like the one organized as part of the International Workshop on Spoken Language Translation (IWSLT), which introduced the challenge to translate TED talks [4] for the 2010 competition. However, the main limitation for the success of these projects continues to be the access to high quality training data. With the emergence of Massive Online Open Courses (MOOCs), thousands of video lectures have already been generated. Sites like Khan Academy1 , Coursera2 , Udacity3 , etc., continuously increase their repertoire of lectures, which range from basic math and science topics, to more advanced topics like machine learning, also covering history, economy, psychology, medicine, and more. Online education has bridged the geographical and financial gap, enab"
2013.iwslt-papers.2,federico-etal-2012-iwslt,0,0.0409166,"les are not available. It also can support volunteer translators, by providing an initial translation, which then can be post-edited [5]. Thus, SMT has the potential to increase the penetration of educational content, allowing it to reach a wider audience. To achieve this, an SMT system requires a large quantity of high-quality in-domain training data. Unfortunately, large data for machine translation has traditionally been constrained to domains such as legal documents, parliamentary proceedings and news. So far, the only openly accessible corpus for the lecture domain has been the TED talks [6]. In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a collection of multilingual community-generated subtitles6 . Furthermore, we explore the steps necessary to build corpora suitable for Machine Translation by processing the Arabic-English part of the multilingual collection. This yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the subtitles, and verify the quality of the generated parallel corpus by building translation models, and extri"
2013.iwslt-papers.2,2012.eamt-1.60,0,0.110653,"Loop (CHIL) [7], which consists of recordings and transcriptions of technical seminars and meetings in English. The content of the corpus includes a variety of topics: from audio and visual technologies to biology and finance. It is available through ELRA7 to its members. More recently, the IWSLT10 [4] evaluation campaign has turned its attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with r"
2013.iwslt-papers.2,tiedemann-2008-synchronizing,0,0.219263,"s attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the langu"
2013.iwslt-papers.2,P07-2045,0,0.0105846,"tional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On test"
2013.iwslt-papers.2,P12-1016,0,0.140562,"Missing"
2013.iwslt-papers.2,J93-2003,0,0.041392,"t on the training data 4.2. Experimental Setup Preprocessing: We tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then inter"
2013.iwslt-papers.2,tiedemann-2012-parallel,0,0.10406,"ite in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lectu"
2013.iwslt-papers.2,N03-1017,0,0.0523188,"e tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the tar"
2013.iwslt-papers.2,W12-3154,0,0.0338013,"allel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and"
2013.iwslt-papers.2,W11-2123,0,0.0748631,"ng IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing o"
2013.iwslt-papers.2,2012.iwslt-papers.7,0,0.0268125,"n with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and managing subtitles of online videos. It provides an easy-to-use interface, which allows users to collaboratively sub"
2013.iwslt-papers.2,P02-1040,0,0.091011,"uning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We"
2013.iwslt-papers.2,J93-1004,0,0.461534,"Missing"
2013.iwslt-papers.2,D11-1125,0,0.042418,"y, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. B"
2013.iwslt-papers.2,C12-1121,1,0.888738,"odel, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. Below, we present the experimental resu"
2013.iwslt-papers.2,D09-1141,0,0.300772,"ARA only (T M1 ): Instead of using the IWSLT training data, we built the translation and reordering models using only the AMARA corpus. Concatenation (T M2 ): In this setting, we concatenated AMARA with IWSLT for training of the translation and reordering models. This generally improves word alignment, reduces OOV rate and improves translation quality if two corpora are from similar domain. However, if the added corpus is noisy or of out-of-domain, (e.g. UN data), we can observe a degradation in performance. Phrase table combination (T M3 ): We applied phrase table combination as described in [25]. We built two phrase tables and reordering models separately on the IWSLT and AMARA data. Then, we merged them by adding three additional indicator features to each entry to inform the decoder if the phrase was found in the first, second or both tables. This can be seen as a form of log-linear interpolation. SYS TM IW10 OOV AM13 OOV SYS LM IW10 AM13 B1 TM1 TM2 TM3 IWSLT AMARA IW+AM PT(IW,AM) 22.97 22.40 23.41 23.57 1.9 2.4 1.2 1.2 23.26 23.66 27.63 27.65 3.9 1.7 1.8 1.8 B1 LM1 LM2 LM3 LM4 IWSLT AMARA IWSLT+AMARA INTERPOL GW 22.97 22.83 23.69 23.59 24.24 23.26 24.05 25.90 25.62 24.79 Table 4:"
2020.aacl-main.39,Q17-1024,0,0.0656433,"Missing"
2020.aacl-main.39,W19-5406,0,0.137105,"Missing"
2020.aacl-main.39,W17-4763,0,0.0505567,"of the translation in the target language or its adequacy with regard to the source sentence (Specia et al., 2018b). Current SOTA models are learnt with the use of neural networks (NN) (Specia et al., 2018a; Fonseca et al., 2019). The assumption is that representations learned can, to some extent, account for source complexity, target fluency and source-target adequacy. These are fine-tuned from pre-trained word representations extracted using multilingual or cross-lingual sentence encoders such as BERT (Devlin et al., 2018), XLM-R (Conneau et al., 2019) or LASER (Artetxe and Schwenk, 2019). Kim et al. (2017) propose the first breakthrough in neural-based QE with the Predictor-Estimator modular architecture. The Predictor model is an encoder-decoder Recurrent Neural Network (RNN) model trained on a huge amount of parallel data for a word prediction task. Its output is fed to the Estimator, a unidirectional RNN trained on QE data, to produce the quality estimates. Kepler et al. (2019) use a similar architecture where the Predictor model is replaced by pretrained contextualised word representations such as BERT (Devlin et al., 2018) or XLM-R (Conneau et al., 2019). Despite achieving strong performan"
2020.aacl-main.39,I17-2050,0,0.057001,"et al. (2020) propose exploiting information provided by the NMT system itself. By exploring uncertainty quantification methods, they show that the confidence with which the NMT system produces its translation correlates well with its quality. Although not performing as well as SOTA supervised models, their approach has the main advantage to be unsupervised and not rely on labelled data. Multilinguality Multilinguality allows training a single model to perform a task from and to multiple languages. This principle has been successfully applied to NMT (Dong et al., 2015; Firat et al., 2016b,a; Nguyen and Chiang, 2017). Aharoni et al. (2019) stretches this approach by translating up to 102 languages from and to English using a Transformer model (Vaswani et al., 2017). They show that multilingual many-to-many models are effective in low resource settings. Multilinguality also allows for zero-shot translation (Johnson et al., 2017). With a simple encoder-decoder architecture and without explicit bridging between source and target languages, they show that their model is able to build a form of inter-lingual representation between all involved language pairs. Shah and Specia (2016) is the only work in QE that"
2020.aacl-main.39,N16-1069,1,0.781305,"15; Firat et al., 2016b,a; Nguyen and Chiang, 2017). Aharoni et al. (2019) stretches this approach by translating up to 102 languages from and to English using a Transformer model (Vaswani et al., 2017). They show that multilingual many-to-many models are effective in low resource settings. Multilinguality also allows for zero-shot translation (Johnson et al., 2017). With a simple encoder-decoder architecture and without explicit bridging between source and target languages, they show that their model is able to build a form of inter-lingual representation between all involved language pairs. Shah and Specia (2016) is the only work in QE that attempted to explore models for more than one language. They use multitask learning with annotators or languages as multiple tasks. In a traditional black-box feature-based approach with Gaussian Processes as learning algorithm, their results suggest that adequately modelling the additional data is as important as the additional data itself. The multilingual models led to marginal improvements over bilingual ones. In addition, the experiments were only conducted with English translation into two closely related languages (French and Spanish). 3 Multilingual QE In t"
2020.aacl-main.39,W18-6451,1,0.904979,"Missing"
2020.aacl-main.39,2009.eamt-1.5,1,0.722617,"Missing"
2020.aacl-main.39,2020.acl-main.558,1,0.829792,"Missing"
2020.aacl-main.62,Q19-1038,0,0.0400861,"17). Other methods have leveraged the distance for cross-lingual document retrieval (Balikas et al., 2018). However these methods treat individual words as the base semantic unit for comparison which are intractable for large web-document alignment. 617 Finally, sentence mover’s similarity has been proposed for automatically evaluating machinegenerated texts outperforming ROUGE (Clark et al., 2019). This method is purely monolingual and sentence representations are constructed by summing individual word embeddings. 3 the base semantic. In particular, we utilize LASER sentence representations (Artetxe and Schwenk, 2019). LASER learns to simultaneously embed 93 languages covering 23 different alphabets into a joint embedding space by training a sequence-tosequence system on many language pairs at once using a shared encoder and a shared byte-pair encoding (BPE) vocabulary for all languages. Utilizing LASER, each sentence is encoded using an LSTM encoder into a fixed-length dense representation. We adapt EMD to measure the distance between two documents by comparing the distributions of sentences within each document. More specifically, SMD represents each document as a normalized bag-of-sentences (nBOS) where"
2020.aacl-main.62,W16-2367,0,0.0426673,"Missing"
2020.aacl-main.62,W16-2347,0,0.831566,"ches rely on metadata for mining parallel documents in unstructured web corpora. Some methods leveraged publication date and other temporal heuristics to identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; AbduI-Rauf and Schwenk, 2009). However, temporal features are often sparse, noisy, and unreliable. Another class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000) yet these structure signals can be sparse and may not generalize to new domains. In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques were proposed to retrieve, score, and align cross-lingual document pairs. However this shared task only considered English to French – a highresource direction and the proposed techniques were not readily extendable to more languages. Several approaches translate the target corpus into the source language, then apply retrieval and matching approaches on translated 2-grams and 5grams to query, retrieve, and align documents (Dara and Lin, 2016; Gomes and Lopes, 2016). These methods rely on high-quality translation systems to translate, however such models may not exist, espec"
2020.aacl-main.62,W16-2365,0,0.605307,"ches rely on metadata for mining parallel documents in unstructured web corpora. Some methods leveraged publication date and other temporal heuristics to identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; AbduI-Rauf and Schwenk, 2009). However, temporal features are often sparse, noisy, and unreliable. Another class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000) yet these structure signals can be sparse and may not generalize to new domains. In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques were proposed to retrieve, score, and align cross-lingual document pairs. However this shared task only considered English to French – a highresource direction and the proposed techniques were not readily extendable to more languages. Several approaches translate the target corpus into the source language, then apply retrieval and matching approaches on translated 2-grams and 5grams to query, retrieve, and align documents (Dara and Lin, 2016; Gomes and Lopes, 2016). These methods rely on high-quality translation systems to translate, however such models may not exist, espec"
2020.aacl-main.62,P19-1264,0,0.0187941,"’s distance (WMD) is an adaptation of earth mover’s distance (EMD) (Rubner et al., 1998) that has been recently used for document similarity and classification (Kusner et al., 2015; Huang et al., 2016; Atasu et al., 2017). Other methods have leveraged the distance for cross-lingual document retrieval (Balikas et al., 2018). However these methods treat individual words as the base semantic unit for comparison which are intractable for large web-document alignment. 617 Finally, sentence mover’s similarity has been proposed for automatically evaluating machinegenerated texts outperforming ROUGE (Clark et al., 2019). This method is purely monolingual and sentence representations are constructed by summing individual word embeddings. 3 the base semantic. In particular, we utilize LASER sentence representations (Artetxe and Schwenk, 2019). LASER learns to simultaneously embed 93 languages covering 23 different alphabets into a joint embedding space by training a sequence-tosequence system on many language pairs at once using a shared encoder and a shared byte-pair encoding (BPE) vocabulary for all languages. Utilizing LASER, each sentence is encoded using an LSTM encoder into a fixed-length dense represent"
2020.aacl-main.62,W16-2366,0,0.185827,"ese structure signals can be sparse and may not generalize to new domains. In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques were proposed to retrieve, score, and align cross-lingual document pairs. However this shared task only considered English to French – a highresource direction and the proposed techniques were not readily extendable to more languages. Several approaches translate the target corpus into the source language, then apply retrieval and matching approaches on translated 2-grams and 5grams to query, retrieve, and align documents (Dara and Lin, 2016; Gomes and Lopes, 2016). These methods rely on high-quality translation systems to translate, however such models may not exist, especially for low-resource language directions. Additionally, these methods leverage rare n-grams to identify likely candidates, yet low-frequency words and phrases that are likely to be mistranslated by machine translation systems. In the shared task, many document similarity measures were investigated for use in aligning English to French web documents. One method utilized a phrase table from a phrase-based statistical machine translation system to compute covera"
2020.aacl-main.62,W09-0430,0,0.0398267,"ta obtained from a single source. For example, parallel corpora were curated from the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) and from the European Parliament (Koehn, 2005). However, curating from homogeneous sources by deriving domain-specific rules does not generalize to arbitrary web-domains. Other approaches rely on metadata for mining parallel documents in unstructured web corpora. Some methods leveraged publication date and other temporal heuristics to identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; AbduI-Rauf and Schwenk, 2009). However, temporal features are often sparse, noisy, and unreliable. Another class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000) yet these structure signals can be sparse and may not generalize to new domains. In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques were proposed to retrieve, score, and align cross-lingual document pairs. However this shared task only considered English to French – a highresource direction and the proposed techniques were not readily extendab"
2020.aacl-main.62,W16-2369,0,0.262799,"s can be sparse and may not generalize to new domains. In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques were proposed to retrieve, score, and align cross-lingual document pairs. However this shared task only considered English to French – a highresource direction and the proposed techniques were not readily extendable to more languages. Several approaches translate the target corpus into the source language, then apply retrieval and matching approaches on translated 2-grams and 5grams to query, retrieve, and align documents (Dara and Lin, 2016; Gomes and Lopes, 2016). These methods rely on high-quality translation systems to translate, however such models may not exist, especially for low-resource language directions. Additionally, these methods leverage rare n-grams to identify likely candidates, yet low-frequency words and phrases that are likely to be mistranslated by machine translation systems. In the shared task, many document similarity measures were investigated for use in aligning English to French web documents. One method utilized a phrase table from a phrase-based statistical machine translation system to compute coverage scores, based on the"
2020.aacl-main.62,W19-5207,0,0.0161796,"ween tf/idf weighted vectors on unigrams and n-grams (Buck and Koehn, 2016b; Medvedˇ et al., 2016; Jakubina and Langlais, 2016). Finally, several methods were introduced that score pairs using metadata in each document such as links to documents, URLs, digits, and HTML structure (Espl`aGomis et al., 2016; Papavassiliou et al., 2016). Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. One method proposes using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment (Guo et al., 2019). Another method leverages a multilingual sentence encoder to embed individual sentences from each document, then performs a simple vector average across all sentence embeddings to form a dense document representation with cosine similarity guiding document alignment (El-Kishky et al., 2019). Word mover’s distance (WMD) is an adaptation of earth mover’s distance (EMD) (Rubner et al., 1998) that has been recently used for document similarity and classification (Kusner et al., 2015; Huang et al., 2016; Atasu et al., 2017). Other methods have leveraged the distance for cross-lingual document retr"
2020.aacl-main.62,W16-2370,0,0.019741,"cal machine translation system to compute coverage scores, based on the ratio of phrase pairs covered by a document pair (Gomes and Lopes, 2016). Other methods utilize the translated content of the target (French) document, and find the source (English) corresponding document based on n-gram matches in conjunction with a heuristic document length ratio (Dara and Lin, 2016; Shchukin et al., 2016). Other methods translate the target documents into the source language and apply cosine similarity between tf/idf weighted vectors on unigrams and n-grams (Buck and Koehn, 2016b; Medvedˇ et al., 2016; Jakubina and Langlais, 2016). Finally, several methods were introduced that score pairs using metadata in each document such as links to documents, URLs, digits, and HTML structure (Espl`aGomis et al., 2016; Papavassiliou et al., 2016). Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. One method proposes using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment (Guo et al., 2019). Another method leverages a multilingual sentence encoder to embed individual sentences from each document, t"
2020.aacl-main.62,2005.mtsummit-papers.11,0,0.402548,"s a guiding metric for identifying cross-lingual document pairs and demonstrate experimentally that our proposed method outperforms state-of-theart baselines that utilize cross-lingual document representations. 2 Related Works Crawling and mining the web for parallel data has been previously explored by Resnik (1999) where the focus is on identifying parallel text from multilingual data obtained from a single source. For example, parallel corpora were curated from the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) and from the European Parliament (Koehn, 2005). However, curating from homogeneous sources by deriving domain-specific rules does not generalize to arbitrary web-domains. Other approaches rely on metadata for mining parallel documents in unstructured web corpora. Some methods leveraged publication date and other temporal heuristics to identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; AbduI-Rauf and Schwenk, 2009). However, temporal features are often sparse, noisy, and unreliable. Another class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000) ye"
2020.aacl-main.62,W16-2374,0,0.0591987,"Missing"
2020.aacl-main.62,J05-4003,0,0.468075,"ar translations of each other. As seen in Figure 1, this involves a one-to-one pairing of documents in a source language with documents in a target language. Introduction While the Web provides a large amount of monolingual text, cross-lingual parallel data is more difficult to obtain. Despite its scarcity, parallel cross-lingual data plays a crucial role in a variety of tasks in natural language processing such as machine translation. Previous works have shown that training on sentences extracted from parallel or comparable documents mined from the Web can improve machine translation models (Munteanu and Marcu, 2005) or learning word-level translation lexicons (Fung and Yee, 1998; Rapp, 1999). Other tasks that leverage these parallel texts include cross-lingual information retrieval, document classification, and multilingual representations such as To automate and scale the process of identifying these documents pairs, we introduce an approach to accurately mine comparable web documents across a variety of low, mid, and high-resource language directions. Previous approaches have been applied to homogeneous corpora, however mining the Web involves analyzing a variety of heterogeneous data sources (Koehn et"
2020.aacl-main.62,P06-1011,0,0.165264,"Missing"
2020.aacl-main.62,P99-1068,0,0.847735,"of-sentences representation. Utilizing the dense, cross-lingual representation of sentences, we then compute document distances using a variant of earth mover’s distance where probability mass is moved from the source document to the target document. We then leverage these document distances as a guiding metric for identifying cross-lingual document pairs and demonstrate experimentally that our proposed method outperforms state-of-theart baselines that utilize cross-lingual document representations. 2 Related Works Crawling and mining the web for parallel data has been previously explored by Resnik (1999) where the focus is on identifying parallel text from multilingual data obtained from a single source. For example, parallel corpora were curated from the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) and from the European Parliament (Koehn, 2005). However, curating from homogeneous sources by deriving domain-specific rules does not generalize to arbitrary web-domains. Other approaches rely on metadata for mining parallel documents in unstructured web corpora. Some methods leveraged publication date and other temporal heuristics to identifying par"
2020.aacl-main.62,J03-3002,0,0.830248,"ying these documents pairs, we introduce an approach to accurately mine comparable web documents across a variety of low, mid, and high-resource language directions. Previous approaches have been applied to homogeneous corpora, however mining the Web involves analyzing a variety of heterogeneous data sources (Koehn et al., 2002). Other approaches rely on corpus-specific features such as metadata and publication date which can be inconsistent and unreliable (Munteanu and Marcu, 2005; AbduI-Rauf and Schwenk, 2009). Related methods utilize document structure when calculating document similarity (Resnik and Smith, 2003; Chen and Nie, 2000). However, when mining large, unstructured collections of web documents these features are often missing or unreliable. As such, we introduce an approach that aligns documents based solely on semantic distances between their textual content. For our approach, we first decompose documents into sentences, and encode each sentence into a cross-lingual semantic space yielding a bag616 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 61"
2020.aacl-main.62,W16-2375,0,0.0248182,"ench) document, and find the source (English) corresponding document based on n-gram matches in conjunction with a heuristic document length ratio (Dara and Lin, 2016; Shchukin et al., 2016). Other methods translate the target documents into the source language and apply cosine similarity between tf/idf weighted vectors on unigrams and n-grams (Buck and Koehn, 2016b; Medvedˇ et al., 2016; Jakubina and Langlais, 2016). Finally, several methods were introduced that score pairs using metadata in each document such as links to documents, URLs, digits, and HTML structure (Espl`aGomis et al., 2016; Papavassiliou et al., 2016). Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. One method proposes using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment (Guo et al., 2019). Another method leverages a multilingual sentence encoder to embed individual sentences from each document, then performs a simple vector average across all sentence embeddings to form a dense document representation with cosine similarity guiding document alignment (El-Kishky et al., 2019). Word mover’s distance ("
2020.aacl-main.62,W16-2376,0,0.0221907,"ted by machine translation systems. In the shared task, many document similarity measures were investigated for use in aligning English to French web documents. One method utilized a phrase table from a phrase-based statistical machine translation system to compute coverage scores, based on the ratio of phrase pairs covered by a document pair (Gomes and Lopes, 2016). Other methods utilize the translated content of the target (French) document, and find the source (English) corresponding document based on n-gram matches in conjunction with a heuristic document length ratio (Dara and Lin, 2016; Shchukin et al., 2016). Other methods translate the target documents into the source language and apply cosine similarity between tf/idf weighted vectors on unigrams and n-grams (Buck and Koehn, 2016b; Medvedˇ et al., 2016; Jakubina and Langlais, 2016). Finally, several methods were introduced that score pairs using metadata in each document such as links to documents, URLs, digits, and HTML structure (Espl`aGomis et al., 2016; Papavassiliou et al., 2016). Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. One method proposes using hie"
2020.aacl-main.62,2009.mtsummit-posters.15,0,0.718694,"Missing"
2020.aacl-main.62,E09-1091,0,0.0278596,"from multilingual data obtained from a single source. For example, parallel corpora were curated from the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) and from the European Parliament (Koehn, 2005). However, curating from homogeneous sources by deriving domain-specific rules does not generalize to arbitrary web-domains. Other approaches rely on metadata for mining parallel documents in unstructured web corpora. Some methods leveraged publication date and other temporal heuristics to identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; AbduI-Rauf and Schwenk, 2009). However, temporal features are often sparse, noisy, and unreliable. Another class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000) yet these structure signals can be sparse and may not generalize to new domains. In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques were proposed to retrieve, score, and align cross-lingual document pairs. However this shared task only considered English to French – a highresource direction and the proposed techniques were not"
2020.aacl-main.62,P99-1067,0,0.322423,"ocuments in a source language with documents in a target language. Introduction While the Web provides a large amount of monolingual text, cross-lingual parallel data is more difficult to obtain. Despite its scarcity, parallel cross-lingual data plays a crucial role in a variety of tasks in natural language processing such as machine translation. Previous works have shown that training on sentences extracted from parallel or comparable documents mined from the Web can improve machine translation models (Munteanu and Marcu, 2005) or learning word-level translation lexicons (Fung and Yee, 1998; Rapp, 1999). Other tasks that leverage these parallel texts include cross-lingual information retrieval, document classification, and multilingual representations such as To automate and scale the process of identifying these documents pairs, we introduce an approach to accurately mine comparable web documents across a variety of low, mid, and high-resource language directions. Previous approaches have been applied to homogeneous corpora, however mining the Web involves analyzing a variety of heterogeneous data sources (Koehn et al., 2002). Other approaches rely on corpus-specific features such as metada"
2020.aacl-main.62,E09-1003,0,\N,Missing
2020.aacl-main.62,P98-1069,0,\N,Missing
2020.aacl-main.62,C98-1066,0,\N,Missing
2020.aacl-main.62,D14-1162,0,\N,Missing
2020.aacl-main.62,L16-1561,0,\N,Missing
2020.acl-main.113,W19-5356,1,0.845684,"nces and measure the (cosine) similarity between them. Similarly, in (Fomicheva et al., 2015; Servan et al., 2016; T¨attar and Fishel, 2017) two words are considered to match if their cosine distance in the embedding space is above a certain threshold. The embeddings are thus used to provide a binary decision. MEANT 2.0 (Lo, 2017) and YISI (Lo, 2019) also relies on matching of words in the embedding space, but this is only used to score the similarity between pairs of words that have already been aligned based on their semantic roles, rather than to find the alignments between words. Finally, Chow et al. (2019) and Echizen’ya et al. (2019) perform the alignment in the embedding space using Earth Mover’s Distance with some special treatment for word order. All of these metrics are however still limited to variance in the words used (even in the continuous space), 1219 rather than more general stylistic or structural variations which can only be captured with multiple references. Another way of incorporating linguistic variation is pseudo-reference approach by Albrecht and Hwa (2007). They leverage various off-the-shelf MT systems to generate additional imperfect references and use them instead or alo"
2020.acl-main.113,W14-3348,0,0.335471,"nction for computing hyp-mt∗ , we are evaluating recall on the MT output, whereas BLEU is designed as a precision-oriented metric. But the choice of similarity function is orthogonal to the goal of this paper, and we leave further refinements in this direction to future work. 1222 TER (Translation Edit Rate) (Snover et al., 2006). TER computes the edit distance defined as the minimum number of word substitutions, deletions, insertions and shifts that are needed to convert MT into the reference. ChrF (Popovi´c, 2015). ChrF calculates the Fscore of character n-grams of maximum length 6. Meteor (Denkowski and Lavie, 2014). Meteor aligns MT output to the reference translation using synonyms and paraphrases besides exact word matching. The similarity is based on the proportion of aligned words in the candidate and in the reference and a fragmentation penalty. BERTScore. (Zhang et al., 2019). We also looked at this very recent metric (published after the submission of this paper), which uses powerful pre-trained embeddings. BERTScore computes a cosine similarity score for each token in the MT output with each token in the reference sentence using contextual embeddings from BERT (Devlin et al., 2019), which can ge"
2020.acl-main.113,D19-1632,1,0.893232,"Missing"
2020.acl-main.113,2005.mtsummit-papers.11,0,0.0154581,"ian-English dataset. This is a new dataset we collected which contains 1K sentences randomly selected from Wikipedia articles in Estonian and translated into English. Two human reference translations were generated independently by two professional translators. All the NMT models were trained using the Fairseq toolkit based on the standard Transformer architecture (Vaswani et al., 2017a) and the training settings described in Ott et al. (2018). We used publicly available parallel datasets for training the models: the Rapid corpus of EU press releases (Rozis and Skadin¸sˇ , 2017) and Europarl (Koehn, 2005), which amount to around 4M parallel sentences in total. A set of 400 segments were translated by the model variants described in §3 to assess the impact of uncertainty types. The following settings were used for model variants. For MC dropout we use dropout rate of 0.3, same as for training the basic Transformer model. Additional hypotheses were produced by performing N stochastic forward passes through the network with dropout, as described in §3. For this analysis we use N = 30, which was shown to perform well for uncertainty quantification (Dong et al., 2018). We also test how the number o"
2020.acl-main.113,W19-5302,0,0.171592,"potheses by exploring uncertainty in NMT models. We show that a light-weight Bayesian approximation method – Monte Carlo Dropout, which allows for uncertainty quantification by using dropout at inference time (Gal and Ghahramani, 2016) – works the best for the purpose of automatic MT evaluation; (2) We devise methods to effectively explore multiple MT hypotheses to better evaluate MT output quality with existing evaluation metrics. On two different datasets, we achieve a large improvement in correlation with human judgments automatic evaluation can be by and large considered a solved problem (Ma et al., 2019a). 2 The goal of this paper is not to evaluate the search space of the MT system, but to improve the evaluation of the given MT output by using additional hypotheses. Evaluating the NMT search space beyond the generated output could be an interesting direction to explore in future work. over using both single reference and multiple references. To the best of our knowledge, this is the first work to leverage NMT model uncertainty for automatic MT evaluation. 2 Related Work Meteor (Banerjee and Lavie, 2005) was the first MT evaluation metric to relax the exact match constraint between MT system"
2020.acl-main.113,W17-4771,0,0.0358305,"Missing"
2020.acl-main.113,W18-6301,0,0.025601,"95 segments with an average DA score of 80.22) to minimise this issue, but the results reported here for English-Czech should be interpreted with caution. Wikipedia Estonian-English dataset. This is a new dataset we collected which contains 1K sentences randomly selected from Wikipedia articles in Estonian and translated into English. Two human reference translations were generated independently by two professional translators. All the NMT models were trained using the Fairseq toolkit based on the standard Transformer architecture (Vaswani et al., 2017a) and the training settings described in Ott et al. (2018). We used publicly available parallel datasets for training the models: the Rapid corpus of EU press releases (Rozis and Skadin¸sˇ , 2017) and Europarl (Koehn, 2005), which amount to around 4M parallel sentences in total. A set of 400 segments were translated by the model variants described in §3 to assess the impact of uncertainty types. The following settings were used for model variants. For MC dropout we use dropout rate of 0.3, same as for training the basic Transformer model. Additional hypotheses were produced by performing N stochastic forward passes through the network with dropout, a"
2020.acl-main.113,P02-1040,0,0.110798,"se comparisons for H 0 hypotheses. As before, ∗ corresponds to different ways of combining similarity scores: average, minimum and maximum. Second, as before, we give a higher weight to the MT output whose quality we wish to evaluate (o). To that end we compare the MT output against additional generated hypotheses. This comparison Figure 2: Methods to explore similarities between MT output, system hypotheses and references. 4.3 Similarity Functions To measure similarity amongst hypotheses and against the reference(s), we experiment with the following standard MT evaluation metrics:3 sentBLEU (Papineni et al., 2002). BLEU measures the similarity between MT and the reference translation based on the number of matching ngrams. We use a smoothed version of BLEU as described by Lin and Och (2004) with N = 4. 3 We use these metrics out of the box. Better results could possibly be achieved by adapting them to our settings, e.g. by changing the weight of precision and recall depending on the direction of the comparison between MT output, hypotheses and the reference. For instance, when using BLEU as similarity function for computing hyp-mt∗ , we are evaluating recall on the MT output, whereas BLEU is designed a"
2020.acl-main.113,D19-1073,0,0.113681,"Missing"
2020.acl-main.113,W16-2342,0,0.0121873,"rated output could be an interesting direction to explore in future work. over using both single reference and multiple references. To the best of our knowledge, this is the first work to leverage NMT model uncertainty for automatic MT evaluation. 2 Related Work Meteor (Banerjee and Lavie, 2005) was the first MT evaluation metric to relax the exact match constraint between MT system output and reference translation by allowing matching of lemmas, synonyms or paraphrases. However, this requires linguistic resources which do not exist for most languages. Character-based metrics (Popovi´c, 2015; Wang et al., 2016) also relax the exact word match constraint by allowing the matching of characters. However, ultimately they still assume a surfacelevel similarity between reference and MT. A more recent direction compares MT and reference sentences in the embedding space. Chen and Guo (2015) extract word embedding representations for the two sentences and measure the (cosine) similarity between them. Similarly, in (Fomicheva et al., 2015; Servan et al., 2016; T¨attar and Fishel, 2017) two words are considered to match if their cosine distance in the embedding space is above a certain threshold. The embedding"
2020.acl-main.558,C04-1046,0,0.439201,"Missing"
2020.acl-main.558,N19-1423,0,0.0263753,"ollowing the ratio of approximately 8 to 1 to 1. Table 1 presents statistics of the QE datasets. size (K) Dataset WMT18∗ WMT19 langs dom. syst. train dev test en-de IT IT SMT NMT 21.8 11.5 2.7 1.4 2.7 1.4 en-cs IT SMT 33.0 4.1 4.1 en-lv SCI SCI SMT NMT 9.8 11.1 1.2 1.3 1.2 1.3 de-en SCI SMT 21.6 2.7 2.7 en-de IT NMT 13.4 1.0 1.0 en-ru Tech NMT 15.0 1.0 1.0 Table 1: Statistics of QE datasets. WMT18∗ contains random splits of the publicly available training data since the official test sets are not publicly available. 2.2 Models BERT We experiment with a strong neural QE approach based on BERT (Devlin et al., 2019). In particular, we focus on the bert-base-cased version of the multilingual BERT.2 We join the source and translated sentences together using the special SEP token and predict the QE score from the vector representation of the final CLS token via a Multilayer Perceptron (MLP) layer. Our models perform competitively to the state-of-theart QE models (Kepler et al., 2019a; Kim et al., 2019). However, we do not treat this as a multitask learning problem where word-level labels are also needed because this is severely limited by the availability of data. We also do not do further optimizations (e."
2020.acl-main.558,P19-1554,0,0.021653,"Missing"
2020.acl-main.558,W19-5401,0,0.0313561,"Missing"
2020.acl-main.558,P15-1174,0,0.046617,"Missing"
2020.acl-main.558,N18-2017,0,0.0218904,"irrespective of the fluency. To evaluate if our models learn both aspects of translation quality, we run partial input experiments, where we train systems with only the source or target sentences and analyze the discrepancies w.r.t to the full-input experiments. Lack of lexical diversity Most QE datasets come from a single domain (e.g., IT, life sciences), and certain lexical items can be associated with high-quality translations. Lexical artifacts are also observed in monolingual datasets across different tasks (Goyal et al., 2017; Jia and Liang, 2017; Kaushik and Lipton, 2018). For example, Gururangan et al. (2018) find that annotators are responsible for introducing lexical artifacts into some natural language inference datasets because they adopt heuristics to generate plausible hypothesis during annotation quickly. Here, we use Normalized Pointwise Mutual Information (NPMI) (Bouma, 2009) to find possible lexical artifacts associated with different levels of HTER. 2.1 Experimental Setup We experiment with recent QE datasets from WMT18 and WMT19. For every dataset, a Statistical Machine Translation (SMT) system or Neural Machine Translation (NMT) system was used to translate the source sentences. The t"
2020.acl-main.558,D17-1215,0,0.0221593,"a mixture of instances that model both high and low adequacy irrespective of the fluency. To evaluate if our models learn both aspects of translation quality, we run partial input experiments, where we train systems with only the source or target sentences and analyze the discrepancies w.r.t to the full-input experiments. Lack of lexical diversity Most QE datasets come from a single domain (e.g., IT, life sciences), and certain lexical items can be associated with high-quality translations. Lexical artifacts are also observed in monolingual datasets across different tasks (Goyal et al., 2017; Jia and Liang, 2017; Kaushik and Lipton, 2018). For example, Gururangan et al. (2018) find that annotators are responsible for introducing lexical artifacts into some natural language inference datasets because they adopt heuristics to generate plausible hypothesis during annotation quickly. Here, we use Normalized Pointwise Mutual Information (NPMI) (Bouma, 2009) to find possible lexical artifacts associated with different levels of HTER. 2.1 Experimental Setup We experiment with recent QE datasets from WMT18 and WMT19. For every dataset, a Statistical Machine Translation (SMT) system or Neural Machine Translat"
2020.acl-main.558,D18-1546,0,0.0167359,"s that model both high and low adequacy irrespective of the fluency. To evaluate if our models learn both aspects of translation quality, we run partial input experiments, where we train systems with only the source or target sentences and analyze the discrepancies w.r.t to the full-input experiments. Lack of lexical diversity Most QE datasets come from a single domain (e.g., IT, life sciences), and certain lexical items can be associated with high-quality translations. Lexical artifacts are also observed in monolingual datasets across different tasks (Goyal et al., 2017; Jia and Liang, 2017; Kaushik and Lipton, 2018). For example, Gururangan et al. (2018) find that annotators are responsible for introducing lexical artifacts into some natural language inference datasets because they adopt heuristics to generate plausible hypothesis during annotation quickly. Here, we use Normalized Pointwise Mutual Information (NPMI) (Bouma, 2009) to find possible lexical artifacts associated with different levels of HTER. 2.1 Experimental Setup We experiment with recent QE datasets from WMT18 and WMT19. For every dataset, a Statistical Machine Translation (SMT) system or Neural Machine Translation (NMT) system was used t"
2020.acl-main.558,W19-5406,0,0.249649,"Missing"
2020.acl-main.558,P19-3020,0,0.102619,"Missing"
2020.acl-main.558,W19-5407,0,0.0650995,"WMT18∗ contains random splits of the publicly available training data since the official test sets are not publicly available. 2.2 Models BERT We experiment with a strong neural QE approach based on BERT (Devlin et al., 2019). In particular, we focus on the bert-base-cased version of the multilingual BERT.2 We join the source and translated sentences together using the special SEP token and predict the QE score from the vector representation of the final CLS token via a Multilayer Perceptron (MLP) layer. Our models perform competitively to the state-of-theart QE models (Kepler et al., 2019a; Kim et al., 2019). However, we do not treat this as a multitask learning problem where word-level labels are also needed because this is severely limited by the availability of data. We also do not do further optimizations (e.g. model ensembling) given that our focus is on what can be learned with the current data, and not maximizing performance. Our simpler models allow us to carefully analyze and determine the effects of source and translated sentences on the performance of the models. We expect the trends to be the same as other neural QE models. 6263 1 2 http://www.umiacs.umd.edu/ snover/terp/ https://gith"
2020.acl-main.558,S18-2023,0,0.0563627,"Missing"
2020.acl-main.558,2006.amta-papers.25,0,0.247297,"Missing"
2020.acl-main.558,P15-4020,1,0.928444,"Missing"
2020.acl-main.558,P13-4014,1,0.885085,"Missing"
2020.acl-main.558,2009.eamt-1.5,1,0.817538,"Missing"
2020.acl-main.747,C18-1139,0,0.0222089,"the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project. Named Entity Recognition. For NER, we consider the CoNLL-2002 (Sang, 2002) and CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018). Cross-lingual Question Answering. We use the MLQA benchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. GLUE Benchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018). We use BERTLarge and RoBERTa as baselines. 5 An"
2020.acl-main.747,D14-1162,0,0.0865806,"Missing"
2020.acl-main.747,L18-1550,1,0.853318,"ortance of scaling the amount of data and RoBERTa (Liu et al., 2019) shows that training BERT longer on more data leads to significant boost in performance. Inspired by RoBERTa, we show that mBERT and XLM are undertuned, and that simple improvements in the learning procedure of unsupervised MLM leads to much better performance. We train on cleaned CommonCrawls (Wenzek et al., 2019), which increase the amount of data for low-resource languages by two orders of magnitude on average. Similar data has also been shown to be effective for learning high quality word embeddings in multiple languages (Grave et al., 2018). Several efforts have trained massively multilingual machine translation models from large parallel corpora. They uncover the high and low resource trade-off and the problem of capacity dilution (Johnson et al., 2017; Tan et al., 2019). The work most similar to ours is Arivazhagan et al. (2019), which trains a single model in 103 languages on over 25 billion parallel sentences. Siddhant et al. (2019) further analyze the representations obtained by the encoder of a massively multilingual machine translation system and show that it obtains similar results to mBERT on cross-lingual NLI. Our work"
2020.acl-main.747,N18-1202,1,0.596746,"I benchmarks, where XLM-R obtains results competitive with state-of-the-art monolingual models, including RoBERTa (Liu et al., 2019). These results demonstrate, for the first time, that it is possible to have a single large model for all languages, without sacrificing per-language performance. We will make our code, models and data publicly available, with the hope that this will help research in multilingual NLP and low-resource language understanding. 2 Related Work From pretrained word embeddings (Mikolov et al., 2013b; Pennington et al., 2014) to pretrained contextualized representations (Peters et al., 2018; Schuster et al., 2019) and transformer based language models (Radford et al., 2018; Devlin et al., 2018), unsupervised representation learning has significantly improved the state of the art in natural language understanding. Parallel work on cross-lingual understanding (Mikolov et al., 2013a; Schuster et al., 2019; Lample and Conneau, 2019) extends these systems to more languages and to the cross-lingual setting in which a model is learned in one language and applied in other languages. Most recently, Devlin et al. (2018) and Lample and Conneau (2019) introduced mBERT and XLM - masked langu"
2020.acl-main.747,D19-1252,0,0.171651,"nguage modeling (TLM) as a way to leverage parallel data and obtain a new state of the art on the cross-lingual natural language inference (XNLI) benchmark (Conneau et al., 2018). They further show strong improvements on unsupervised machine translation and pretraining for sequence generation. Wu et al. (2019) shows that monolingual BERT representations are similar across languages, explaining in part the natural emergence of multilinguality in bottleneck architectures. Separately, Pires et al. (2019) demonstrated the effectiveness of multilingual models like mBERT on sequence labeling tasks. Huang et al. (2019) showed gains over XLM using cross-lingual multi-task learning, and Singh et al. (2019) demonstrated the efficiency of cross-lingual data augmentation for cross-lingual NLI. However, all of this work was at a relatively modest scale, in terms of the amount of training data, as compared to our approach. The benefits of scaling language model pretraining by increasing the size of the model as well as the training data has been extensively studied in the literature. For the monolingual case, Jozefowicz et al. (2016) show how large-scale LSTM models can obtain much stronger performance on language"
2020.acl-main.747,E17-2068,1,0.723734,"hili and Urdu. We chose this set as it covers a suitable range of language families and includes low-resource languages such as Swahili and Urdu. We also consider larger sets of 15, 30, 60 and all 100 languages. When reporting results on high-resource and lowresource, we refer to the average of English and French results, and the average of Swahili and Urdu results respectively. Scaling the Amount of Training Data. Following Wenzek et al. (2019) 2 , we build a clean CommonCrawl Corpus in 100 languages. We use an internal language identification model in combination with the one from fastText (Joulin et al., 2017). We train language models in each language and use it to filter documents as described in Wenzek et al. (2019). We consider one CommonCrawl dump for English and twelve dumps for all other languages, which significantly increases dataset sizes, especially for low-resource languages like Burmese and Swahili. Figure 1 shows the difference in size between the Wikipedia Corpus used by mBERT and XLM100, and the CommonCrawl Corpus we use. As we show in Section 5.3, monolingual Wikipedia corpora are too small to enable unsupervised representation learning. Based on our experiments, we found that a fe"
2020.emnlp-main.480,E09-1003,0,0.0758881,"vitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informativ"
2020.emnlp-main.480,Q19-1038,0,0.0447604,"st mapping according to φ(ds , dt ). In the remainder of this section, we introduce our proposed baseline document pair similarity functions and a simple matching algorithm that aligns source and target documents. 5.2 Embedding-Based Document Similarity To guide the alignment algorithm, a notion of cross-lingual document similarity is necessary. This score should capture the fact that two documents are semantically similar despite having some or all of their content in different languages. We describe three simple language-agnostic document embedding methods. These embeddings leverage LASER1 (Artetxe and Schwenk, 2019), a multilingual sentence representation that uses byte-pair encoding to share the same vocabulary among all languages and trained on parallel sentences pulled from Europarl, United Nations, OpenSubtitles2018, Global Voices, Tanzil and Tatoeba corpus, covering 93 languages. Direct Embedding The first baseline, Direct Embedding (DE) uses a standard cross-lingual encoder to directly embed each document. Each document d has its dense vector representation 5963 1 https://github.com/facebookresearch/LASER vd computed by applying the open-source crosslingual LASER encoder to its full textual content"
2020.emnlp-main.480,buck-etal-2014-n,0,0.1104,"Missing"
2020.emnlp-main.480,W16-2347,1,0.957917,"have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not exp"
2020.emnlp-main.480,W16-2365,1,0.931521,"have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not exp"
2020.emnlp-main.480,W19-5435,1,0.862614,", we segment each document solely on new lines. Given each document pair’s decomposition into sentences, we seek 2 https://paracrawl.eu/ to align sentences within each pair of documents. We then aggregate the parallel sentences across all document pairs to form a parallel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are used, encoder and decoder FFN with 2 attention heads each with an embedding dimension of 2048 are used along with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. Th"
2020.emnlp-main.480,W16-2366,0,0.0275662,"al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not explored in WMT-2016 Recently, the use of neural embedding methods has been explored for bilingu"
2020.emnlp-main.480,N19-1423,0,0.0162251,"ages. 1 Introduction Cross-lingual document alignment aims to pair documents such that they are translations or near translations of each other. There are a variety of tasks in natural language processing that consume such parallel cross-lingual data. Traditionally, machine translation approaches have leveraged parallel sentences as training data for use with sequence-to-sequence models. Other tasks include cross-lingual information retrieval and cross-lingual document classification. Additionally, cross-lingual data facilitates training crosslingual representations such as multilingual BERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) which are used in many NLP tasks. The availability of high-quality datasets is necessary to both train and evaluate models across these many tasks. While it is possible to manually label aligned documents across languages, the process is costly and time consuming due to the quadratic search space for document pairs. Additionally, for low-resource languages, identifying these crosslingual document pairs is more difficult due to their relative scarcity. Furthermore, lack of access to qualified human annotators makes it necessary to have additional quality cont"
2020.emnlp-main.480,W09-0430,0,0.024085,"solutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that ra"
2020.emnlp-main.480,W19-6721,0,0.0811263,"Missing"
2020.emnlp-main.480,W16-2369,0,0.0484401,"2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not explored in WMT-2016 Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. Guo et al. (2019) propose using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment."
2020.emnlp-main.480,W19-5207,0,0.0135317,"for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for low-resource language directions. The application of alignment to a variety of languages was not explored in WMT-2016 Recently, the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level. Guo et al. (2019) propose using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment. Dataset Creation and Description The Common Crawl corpus is a publicly available crawl of the web. With a new snapshot uploaded each month, and over 2 billion pages released in each snapshot, this data is a vast resource with content across a large number of domains and languages. Previous works have leveraged the data from Common Crawl for mining ngram counts to perform language modeling (Buck et al., 2014). Other works (Smith et al., 2013) have mined Common Crawl for bitex"
2020.emnlp-main.480,D19-1632,1,0.863754,"019) which are used in many NLP tasks. The availability of high-quality datasets is necessary to both train and evaluate models across these many tasks. While it is possible to manually label aligned documents across languages, the process is costly and time consuming due to the quadratic search space for document pairs. Additionally, for low-resource languages, identifying these crosslingual document pairs is more difficult due to their relative scarcity. Furthermore, lack of access to qualified human annotators makes it necessary to have additional quality control in low-resource scenarios (Guzmán et al., 2019). In this paper, we investigate whether we can rely on weak supervision to generate labels for document pairs. In particular, we focus on the weak signals embedded in the URLs of web documents, that can be used to identify the different translations of a single document across many languages. We preprocess, filter, and apply a set of high-precision hand-crafted rules to automatically sift through a massive collection of 169 billion web documents and identify over a 392 million cross-lingual parallel documents in 8144 language pairs. Of these aligned documents, 292 million are non-English docum"
2020.emnlp-main.480,E17-2068,0,0.121936,"Missing"
2020.emnlp-main.480,2005.mtsummit-papers.11,1,0.249144,"llel corpus for mining parallel sentences and as supervision for a variety of cross-lingual tasks. which only considered English to French document alignment – a high-resource direction. 2 3 Related Works The concept of crawling and mining the web to identify sources of parallel data has been previously explored (Resnik, 1999). A large body of this work has focused on identifying parallel text from multilingual data obtained from a single source: for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable"
2020.emnlp-main.480,W19-5404,1,0.908456,"Missing"
2020.emnlp-main.480,2009.mtsummit-posters.15,0,0.468976,"Missing"
2020.emnlp-main.480,D18-2012,0,0.0296119,"e then aggregate the parallel sentences across all document pairs to form a parallel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are used, encoder and decoder FFN with 2 attention heads each with an embedding dimension of 2048 are used along with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. The adam optimizer is used to train the model for up to 20 epochs by optimizing a smoothed-cross entropy with 0.2 label smoothing. We decompose the 100-million parallel documents corresponding"
2020.emnlp-main.480,P99-1068,0,0.788966,"n task. We release the dataset consisting of pairs of translated documents represented by URLs extracted from a massive collection web crawls. We hope that the size, diversity, and quality of this dataset spurs its use not only as a benchmark for document alignment, but also as a parallel corpus for mining parallel sentences and as supervision for a variety of cross-lingual tasks. which only considered English to French document alignment – a high-resource direction. 2 3 Related Works The concept of crawling and mining the web to identify sources of parallel data has been previously explored (Resnik, 1999). A large body of this work has focused on identifying parallel text from multilingual data obtained from a single source: for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some o"
2020.emnlp-main.480,J03-3002,0,0.410945,"ces by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with the insight that rare snippets are more informative. Both of these candidates rely on high-quality translation systems to translate either the source or the target. Such models may not exist, especially for"
2020.emnlp-main.480,J05-4003,0,0.293492,": for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gome"
2020.emnlp-main.480,P18-2037,0,0.0544963,"et al., 2019) and ParaCrawl2 (EsplàGomis et al., 2019), reliable sources of comparable documents. Sentence mining The first step is to decompose and mine the aligned document corpus for parallel sentences. For simplicity, we segment each document solely on new lines. Given each document pair’s decomposition into sentences, we seek 2 https://paracrawl.eu/ to align sentences within each pair of documents. We then aggregate the parallel sentences across all document pairs to form a parallel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are"
2020.emnlp-main.480,P06-1011,0,0.176617,"Missing"
2020.emnlp-main.480,N19-4009,0,0.0409614,"llel sentences dataset suitable for training machine translation models. We use the open-source LASER toolkit (Schwenk, 2018) with the marginbased filtering criterion to mine sentences, as this method has been shown to accurately align sentences for across a variety of low, mid, and high-resource directions (Schwenk et al., 2019; Chaudhary et al., 2019). We use the extracted bitexts for training our NMT systems. Experimental setup First the data is processed to induce a 5000 subword vocabulary using SentencePiece (Kudo and Richardson, 2018). The model used is a transformer model from fairseq (Ott et al., 2019) with embeddings shared in the encoder and decoder, 5 encoder and decoder layers with dimensionality 512 are used, encoder and decoder FFN with 2 attention heads each with an embedding dimension of 2048 are used along with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. The adam optimizer is used to train the model for up to 20 epochs by optimizing a smoothed-cross entropy with 0.2 label smoothing. We decompose the 100-million parallel documents corresponding to the 137 language pairs that include English and mine over 1B unique"
2020.emnlp-main.480,N18-2084,0,0.0321842,"with encoder and decoder normalization. Dropout of 0.4, attention dropout of 0.2 and relu dropout of 0.2 are applied. The adam optimizer is used to train the model for up to 20 epochs by optimizing a smoothed-cross entropy with 0.2 label smoothing. We decompose the 100-million parallel documents corresponding to the 137 language pairs that include English and mine over 1B unique parallel sentences after filtering. After training models for each direction, we then evaluate the quality of the learned NMT models on a publicly available data set consisting of transcribed and translated TED talks (Qi et al., 2018). Since the development and 5966 Danish Croatian Slovenian Slovak Lithuanian Estonian Language En–x x–En En–x x–En En–x x–En En–x x–En En–x x–En En–x x–En WikiMatrix ParaCrawl CCAligned 30.9 37.3 37.1 32.9 39.8 38.2 18.8 23.0 23.5 22.4 29.0 29.3 16.5 20.4 19.6 17.3 22.7 21.7 13.8 20.4 20.4 16.9 24.3 24.2 16.5 16.7 22.5 21.8 15.6 15.6 19.4 20.0 Table 4: BLEU scores of NMT models trained on bitext data mined from various web-sources including Wikipedia, ParaCrawl, and our CCAligned document set evaluated on TED Talk test sets. test sets were already tokenized, we first detokenize them using the"
2020.emnlp-main.480,P13-1135,1,0.846277,"at the sentence and document level. Guo et al. (2019) propose using hierarchical document embeddings, constructed from sentence embeddings, for bilingual document alignment. Dataset Creation and Description The Common Crawl corpus is a publicly available crawl of the web. With a new snapshot uploaded each month, and over 2 billion pages released in each snapshot, this data is a vast resource with content across a large number of domains and languages. Previous works have leveraged the data from Common Crawl for mining ngram counts to perform language modeling (Buck et al., 2014). Other works (Smith et al., 2013) have mined Common Crawl for bitexts for machine translation. However, this mining was performed on a small scale. For our dataset, we use 68 snapshots published from 2013 to 2020 which is vastly larger than previous works. 3.1 Preprocessing The first step in preprocessing the data is deduplication. While investigating combining many Common Crawl snapshots, we found duplicate URLs both within an individual snapshot and almost always across snapshots. As our data curation method relies on unique URLs for each web document, we apply a heuristic to ensure each URL appears once within the final cl"
2020.emnlp-main.480,E09-1091,0,0.045472,"General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, temporal features can be sparse, noisy, and unreliable. A different class of alignment methods rely on document structure (Resnik and Smith, 2003; Chen and Nie, 2000). In the WMT-2016 bilingual document alignment shared task (Buck and Koehn, 2016a), many techniques applied retrieval and matching on translated 5-grams (Dara and Lin, 2016) to query, retrieve, and align documents. Similar methods for generating candidates by retrieving matches based on the least frequent bi-lingual 5grams have been proposed (Gomes and Lopes, 2016) with th"
2020.emnlp-main.480,L16-1561,0,0.0518399,"only as a benchmark for document alignment, but also as a parallel corpus for mining parallel sentences and as supervision for a variety of cross-lingual tasks. which only considered English to French document alignment – a high-resource direction. 2 3 Related Works The concept of crawling and mining the web to identify sources of parallel data has been previously explored (Resnik, 1999). A large body of this work has focused on identifying parallel text from multilingual data obtained from a single source: for example the United Nations General Assembly Resolutions (Rafalovitch et al., 2009; Ziemski et al., 2016) or European Parliament parallel corpus (Koehn, 2005). These parallel corpora were curated from specific, homogeneous sources by examining the content and deriving domainspecific rules for aligning documents. Other approaches have identified parallel documents in unstructured web corpora by relying on metadata (Nie et al., 1999; Espla-Gomis and Forcada, 2010). Some of these methods have focused on publication date and other temporal heuristics to aid in identifying parallel documents (Munteanu and Marcu, 2005, 2006; Udupa et al., 2009; Do et al., 2009; Abdul-Rauf and Schwenk, 2009). However, t"
2020.lrec-1.494,D19-1539,0,0.0215646,"licly available2 . 2. Related work Preprocessing of massive datasets for training text representations has been developed in the context of word embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) or fastText (Mikolov et al., 2017). In particular, our pipeline follows the fastText pipeline of Grave et al. (2018) where Common Crawl is split into monolingual datasets using a language identifier based on fastText (Joulin et al., 2016a). Common Crawl has been used in the context of language modeling to evaluate n-gram statistics (Buck et al., 2014). More recently, Baevski et al. (2019) pretrained a BERT-like model on Common Crawl as preprocessed in Grave et al. (2018). In general, progress in sentence representations has been observed by increasing the size of the pre-training corpora (Yang et al., 2019; Liu et al., 2019; Raffel et al., 2019). In particular, and concurrently to our work, Raffel et al. (2019) used a large scale dataset based on Common Crawl to train text representations. Existing work using web based datasets have been using English specific preprocessing, such as keeping URLs shared on Reddit or using hand-crafted filtering rules. As opposed to these approa"
2020.lrec-1.494,buck-etal-2014-n,0,0.108726,"Missing"
2020.lrec-1.494,D18-1269,1,0.893495,"Missing"
2020.tacl-1.35,D16-1025,0,0.021127,"the advent of neural models, Machine Translation (MT) systems have made substantial progress, reportedly achieving near-human quality for high-resource language pairs (Hassan et al., 2018; Barrault et al., 2019). However, translation quality is not consistent across language pairs, domains, and datasets. This is problematic for low-resource scenarios, where there is not enough training data and translation quality significantly lags behind. Additionally, neural MT (NMT) systems can be deceptive to the end user as they can generate fluent translations that differ in meaning from the original (Bentivogli et al., 2016; Castilho et al., 2017). 539 Transactions of the Association for Computational Linguistics, vol. 8, pp. 539–555, 2020. https://doi.org/10.1162/tacl a 00330 Action Editor: Stefan Riezler. Submission batch: 1/2020; Revision batch: 4/2020; Published 9/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. probability mass to predictions that are far from the training data (Gal and Ghahramani, 2016). To overcome such deficiencies, we propose ways to exploit output distributions beyond the top-1 prediction by exploring uncertainty quantification methods for"
2020.tacl-1.35,2020.eamt-1.16,1,0.787809,"/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding to the best epoch as identified by the metric of refe"
2020.tacl-1.35,C04-1046,0,0.353281,"Missing"
2020.tacl-1.35,W17-4755,0,0.0539918,"Missing"
2020.tacl-1.35,W16-2302,0,0.0478511,"Missing"
2020.tacl-1.35,D19-1308,0,0.0163557,"experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and model probabilities (orange), and Met"
2020.tacl-1.35,W14-3348,0,0.0203514,"ity outputs on NMT training with back translations. Second, we measure lexical variation between the MT outputs generated for the same source segment when running inference with dropout. We posit that differences between likely MT hypotheses may also capture uncertainty and potential ambiguity and complexity of the original sentence. We compute an average similarity score (sim) between the set H of translation hypotheses: |H ||H| 1 XX D-Lex-Sim = sim(hi , hj ) C i=1 j =1 where hi , hj ∈ H, i 6= j and C = 2−1 |H|(|H |− 1) is the number of pairwise comparisons for |H| hypotheses. We use Meteor (Denkowski and Lavie, 2014) to compute similarity scores. 3.3 Attention Attention weights represent the strength of connection between source and target tokens, which may be indicative of translation quality (Rikters and Fishel, 2017). One way to measure it is to compute the entropy of the attention distribution: I Att-Ent = − J 1 XX αji log αji I i=1 j =1 where α represents attention weights, I is the number of target tokens and J is the number of source tokens. This mechanism can be applied to any NMT model with encoder-decoder attention. We focus on attention in Transformer models, as it is currently the most widely"
2020.tacl-1.35,N19-1423,0,0.0129677,"ration (Guo et al., 2017). On the other hand, due to the small amount of training data the model can overfit, resulting in inferior results both in terms of translation quality and correlation. It is noteworthy, however, that supervised QE system suffers a larger drop in performance than unsupervised indicators, as its 6 We note that PredEst models are systematically and significantly outperformed by BERT-BiRNN. This is not surprising, as large-scale pretrained representations have been shown to boost model performance for QE (Kepler et al., 2019a) and other natural language processing tasks (Devlin et al., 2019). 7 Models for these languages were trained using Transformer-Big architecture from Vaswani et al. (2017). Low-resource Mid-resource High-resource Method Si-En Ne-En Et-En Ro-En En-De En-Zh I TP Softmax-Ent (-) Sent-Std (-) 0.399 0.457 0.418 0.482 0.528 0.472 0.486 0.421 0.471 0.647 0.613 0.595 0.208 0.147 0.264 0.257 0.251 0.301 II D-TP D-Var (-) D-Combo (-) D-Lex-Sim 0.460 0.307 0.286 0.513 0.558 0.299 0.418 0.600 0.642 0.356 0.475 0.612 0.693 0.332 0.383 0.669 0.259 0.164 0.189 0.172 0.321 0.232 0.225 0.313 III AW : Ent-Min (-) AW : Ent-Avg (-) AW : best head/layer (-) 0.097 0.10 0.255 0.26"
2020.tacl-1.35,P18-1069,0,0.0928735,"ed. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defi"
2020.tacl-1.35,C16-1133,0,0.0483943,"Missing"
2020.tacl-1.35,C16-1294,0,0.0197172,"me of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 201"
2020.tacl-1.35,N15-1124,0,0.135368,"translation quality can be extracted from multihead attention. To evaluate our approach in challenging settings, we collect a new dataset for QE with 6 language pairs representing NMT training in high, medium, and low-resource scenarios. To reduce the chance of overfitting to particular domains, our dataset is constructed from Wikipedia documents. We annotate 10K segments per language pair. By contrast to the vast majority of work on QE that uses semi-automatic metrics based on post-editing distance as gold standard, we perform quality labeling based on the Direct Assessment (DA) methodology (Graham et al., 2015b), which has been widely used for popular MT evaluation campaigns in the recent years. At the same time, the collected data differs from the existing datasets annotated with DA judgments for the well known WMT Metrics task1 in two important ways: We provide enough data to train supervised QE models and access to the NMT systems used to generate the translations, thus allowing for further exploration of the glass-box unsupervised approach to QE for NMT introduced in this paper. Our main contributions can be summarized as follows: (i) A new, large-scale dataset for sentencelevel2 QE annotated w"
2020.tacl-1.35,W13-2305,0,0.432265,"tion of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that closely preserves the semantics of the source sentence; and 91–100, a perfect translation. Each segment was evaluated indep"
2020.tacl-1.35,W19-6721,0,0.0372157,"Missing"
2020.tacl-1.35,D19-1632,1,0.899651,"Missing"
2020.tacl-1.35,P19-3020,0,0.0842974,"Missing"
2020.tacl-1.35,W17-4763,0,0.168703,"London 5 Facebook AI 1 {m.fomicheva,f.blain,n.aletras,l.specia}@sheffield.ac.uk 2 {ssun32}@jhu.edu 3 {lisa.yankovskaya,fishel}@ut.ee 5 {fguzman,vishrav}@fb.com Abstract Thus, it is crucial to have a feedback mechanism to inform users about the trustworthiness of a given MT output. Quality estimation (QE) aims to predict the quality of the output provided by an MT system at test time when no gold-standard human translation is available. State-of-the-art (SOTA) QE models require large amounts of parallel data for pretraining and in-domain translations annotated with quality labels for training (Kim et al., 2017a; Fonseca et al., 2019). However, such large collections of data are only available for a small set of languages in limited domains. Current work on QE typically treats the MT system as a black box. In this paper we propose an alternative glass-box approach to QE that allows us to address the task as an unsupervised problem. We posit that encoder-decoder NMT models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) offer a rich source of information for directly estimating translation quality: (a) the output probability distribution from the NMT system (i.e., the probabilit"
2020.tacl-1.35,K18-1056,0,0.0255158,"del ensembling). For this smallscale experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and"
2020.tacl-1.35,2005.mtsummit-papers.11,0,0.0830211,"nese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that clos"
2020.tacl-1.35,C18-1266,1,0.840227,"he DA judgments are available at https://github. com/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding"
2020.tacl-1.35,1983.tc-1.13,0,0.671274,"Missing"
2020.tacl-1.35,D15-1166,0,0.133746,"Missing"
2020.tacl-1.35,J82-2005,0,0.698459,"Missing"
2020.tacl-1.35,N19-4009,0,0.237923,"nd the empirical frequencies of the predicted labels, or by assessing generalization of uncertainty under domain shift (see §6). Only a few studies have analyzed calibration in NMT and they came to contradictory conclusions. Kumar and Sarawagi (2019) measure calibration error by comparing model probabilities and the percentage of times NMT output matches reference translation, and conclude that NMT probabilities are poorly calibrated. However, the calibration error metrics they use are designed for binary classification tasks and cannot be easily transferred to NMT (Kuleshov and Liang, 2015). Ott et al. (2019) analyze uncertainty in NMT by comparing predictive probability distributions with the empirical distribution observed in human translation data. They conclude that NMT models ministic NMT (§3.1) or (ii) using uncertainty quantification (§3.2), and (iii) attention weights (§3.3). are well calibrated. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides"
2020.tacl-1.35,W18-6450,0,0.0135015,"he current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Existing datasets with sentence-level DA judgments from the WMT Metrics Task could in principle be used for benchmarking QE systems. However, they contain only a few hundred segments per language pair and thus hardly allow for training supervised systems, as illustrated by the weak correlation results for QE on DA judgments based on the Metrics Task data recently reported by Fonseca et al. (2019). Furthermore, for each language pair the data contains translations from a number of MT systems often using different architectures, and these MT systems are not readily available, making it"
2020.tacl-1.35,W18-6301,0,0.1208,"e sampled documents and then translated them into English using the MT models described below. For German and Chinese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conve"
2020.tacl-1.35,W19-5302,0,0.0298412,"Missing"
2020.tacl-1.35,W12-3116,0,0.0450527,"Missing"
2020.tacl-1.35,2021.eacl-main.115,1,0.859327,"Missing"
2020.tacl-1.35,W12-3114,0,0.0324498,"system. Existing work on glass-box QE is limited to features extracted from statistical MT, such as language model probabilities or number of hypotheses in the n-best list (Blatz et al., 2004; Specia et al., 2013). The few approaches for unsupervised QE are also inspired by the work on statistical MT 2 While the paper covers QE at sentence level, the extension of our unsupervised metrics to word-level QE would be straightforward and we leave it for future work. 1 h t t p : / /www.statmt.org/wmt19/metricstask.html. 540 and perform significantly worse than supervised approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based"
2020.tacl-1.35,D15-1182,0,0.0718096,"Missing"
2020.tacl-1.35,W19-8671,0,0.0592008,"Missing"
2020.tacl-1.35,2006.amta-papers.25,0,0.114,"nterpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WM"
2020.tacl-1.35,W19-4808,0,0.0153618,"l information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the re"
2020.tacl-1.35,P13-4014,1,0.932381,"Missing"
2020.tacl-1.35,P19-1580,0,0.162993,"models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this met"
2020.tacl-1.35,2009.eamt-1.5,1,0.860291,"t unsupervised QE indicators obtained from well-calibrated NMT model probabilities rival strong supervised SOTA models in terms of correlation with human judgments. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require"
2020.tacl-1.35,W18-6465,0,0.0275911,"s. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require a significant amount of in-domain labeled data for training. They do not use any internal information from the MT system. Existing work on glass-box QE is lim"
2020.tacl-1.35,D19-1073,0,0.314906,"ns for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defining a simple QE measure based on sequence-level translation probability normalized by length: TP"
2020.tacl-1.35,W18-6466,1,0.857973,"d approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challe"
2020.tacl-1.35,W18-6451,1,\N,Missing
2020.tacl-1.35,W19-5301,1,\N,Missing
2020.tacl-1.35,W19-5401,1,\N,Missing
2020.wmt-1.116,C04-1046,0,0.103907,"Missing"
2020.wmt-1.116,2020.acl-main.747,1,0.846095,"Missing"
2020.wmt-1.116,N19-1423,0,0.0390624,"data for all languages and use it for training our regression models. Note that we do not add any language identification markers and the system does not require them for making predictions. This can be useful for multilingual translation systems where the user does not need to identify the input languages, and especially for zero-shot settings where a given language pair may not have been seen at training time. 2.2 Black-box We explore a baseline neural QE model and a multitask learning QE model, both of which are built on top of pre-trained contextualised representations (CR) such as BERT (Devlin et al., 2019) and XLMR (Conneau et al., 2020). Baseline QE model (BASE) Given a source sentence sX in language X and a target sentence sY in language Y , we model the QE function f by stacking a 2-layer multilayer perceptron (MLP) on the vector representation of the [CLS] token from a contextualised representations model (CR): f (sX , sY ) =W2 · ReLU ( W1 · Ecls (sX , sY ) + b1 (1) ) + b2 where W2 ∈ R1×4h , b2 ∈ R, W1 ∈ R4h×h and b1 ∈ R4h . Ecls is a function that extracts the vector representation of the [CLS] token after encoding the concatenation of sX and sY with CR and 1011 ReLU is the Rectified Linea"
2020.wmt-1.116,C18-1266,1,0.719629,"Unit activation function. Note that h is the output dimension of Ecls . We explore two training strategies: The bilingual (BL) strategy trains a QE model for every language pair while the multilingual (ML) strategy trains a single multilingual QE model for all language pairs, where the training data is simply pooled together without any language identifier. We note that this multilingual model here corresponds to a pooled, single-task learning approach. MTL-LS submodel trained on the same language pair as the test set. BiRNN We compared the above approaches to the BiRNN model from deepQuest (Ive et al., 2018). The BiRNN model uses an encoder-decoder architecture: it encodes both source and translation sentences independently using two bi-directional Recurrent Neural Networks (RNNs). The two resulting sentence representations are concatenated afterwards as the weighted sum of their word vectors, generated by an attention mechanism. For predictions at sentence-level, the weighted representation of the two input sentences is passed through a dense layer with sigmoid activation to generate the quality estimates. This is a light-weight variant of the black-box approaches above that does not rely on hea"
2020.wmt-1.116,P19-3020,0,0.0589584,"Missing"
2020.wmt-1.116,2006.amta-papers.25,0,0.110843,"QE) (Blatz et al., 2004; Specia et al., 2009) is an important part of Machine Translation (MT) pipeline. It allows us to evaluate how good a translation is without comparison to reference sentences. As part of the WMT20 Shared Task on Quality Estimation, two sentence-level tasks were proposed. In Task 1, participants are asked to predict human judgements of MT quality generated following a methodology similar to Direct Assessment (DA) (Graham et al., 2017). The goal of Task 2 is to estimate the post-editing effort required in order to correct the MT outputs and measured using the HTER metric (Snover et al., 2006). 1 http://www.statmt.org/wmt20/ quality-estimation-task.html ∗ Equal contribution. Approach Below we first describe our glass-box submissions based on the quality indicators that can be obtained as a by-product of decoding with an NMT system. Second, we present our neural-based QE submissions, which explore transfer learning with pretrained representations. In both cases, we describe how QE is addressed as a multilingual task. 2.1 Glass-box Glass-box approaches to QE are based on information from the NMT system used to translate the sentences, rather than looking at source and target sentence"
2020.wmt-1.116,2009.eamt-1.5,1,0.913127,"Missing"
2020.wmt-1.116,N15-1124,0,0.0451071,"Missing"
2020.wmt-1.4,abdelali-etal-2014-amara,1,0.827823,"ne translated all comments using an in-house transformer-based model into Japanese and German. The goal of that was to be able to examine potential differences in source and (one example of) translation segments.3 We then pre-processed and automatically annotated all 17K segments with the following soft labels for catastrophic errors: Development Data The task specified the following data to help participants evaluate their system’s performance on unseen and multiple domains. • English-German: participants can use the development data from the News translation task, development data from QED (Abdelali et al., 2014) corpus, development data from WMT19 Medical translation task, and development data from the WMT16 IT translation task. 1. Introduction of toxicity: we checked both source and machine translation for toxic words (using in-house lists) and labelled as positive (i.e. potentially containing errors) cases where the source does not contain such words, but the translation does (at least one). • English-Japanese: participants can use the development data from the News translation task, and development data from the MTNT dataset, which contains noisy social media texts and their clean translations. 3."
2020.wmt-1.4,D17-1158,0,0.0130333,"Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The"
2020.wmt-1.4,N19-1311,0,0.0437239,"Missing"
2020.wmt-1.4,P18-1128,0,0.014933,"Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the constrained, zero-shot submissions. To get insight on the proportion of"
2020.wmt-1.4,2020.lrec-1.520,0,0.0937737,"raped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient information. 2 Poor: the target text contains translation errors,"
2020.wmt-1.4,N19-1154,1,0.82504,"luation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Do"
2020.wmt-1.4,D19-1165,0,0.282315,"n a critical way. Critical errors are those that lead to misleading translations which may carry religious, health, safety, legal or financial implications, or introduce toxicity. The set of critical errors used for the guidelines (which also included examples of these errors) includes – but is not limited to – the cases below: Naver Labs (NLE): They participated in Chat and Biomedical tasks along with the Robustness task. They trained a general big-transformer model using FairSeq toolkit (Ott et al., 2019) and adapted it towards different tasks using lightweight adapter layers for each task (Bapna and Firat, 2019). They compared results against the more traditional finetuning method (Luong and Manning, 2015) to show that the former provides a viable alternative, while significantly reducing the amount of parameters per task. They also explored using embedding from pre-trained language models in their NMT system of which they tried two MLM variants: i) using NMT encoder’s setting, using Roberta (Liu et al., 2019). The latter was found more robust to novel domains and noise. The authors found that initializing with first 8 layers instead of the entire model to 80 OPPO: Team OPPO also trained their system"
2020.wmt-1.4,D19-1632,1,0.894086,"Missing"
2020.wmt-1.4,W17-4712,0,0.0183942,"ne-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively u"
2020.wmt-1.4,W17-3205,0,0.0203483,"to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task pr"
2020.wmt-1.4,P17-2061,0,0.0561223,"Missing"
2020.wmt-1.4,C18-1111,0,0.0137039,"6). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopo"
2020.wmt-1.4,W18-6478,0,0.0263186,"different models to obtain further improvements. Only two teams, namely Naver Labs and LIMSI made specific efforts towards the task of Robustness. Both of them used lightweight domain adaptors proposed by Bapna and Firat (2019). Both teams UEDIN: Team UEDIN also mainly trained their system towards News translation task, but added Gumbel noise to the output layer of the systems submitted to the Robustness task. They followed standard NMT training pipeline and boasted their systems with additional data filtered from the paracrawl corpus. The data was carefully selected using dual cross-entropy (Junczys-Dowmunt, 2018) and length-normalized cross-entropy. 81 script6 (Dror et al., 2018) with p <0.05. The result of significance test in likert score is used for the human judgement ranking. Interestingly, the correlation in the system rankings between human judgments and BLEU is not strong. In other words, the best performing systems in BLEU do not rank high according human judgement, sometimes even rank the lowest. For example, in Ja→En (set2), the online-B system ranks first in BLEU but last in likert score. OPPO outperforms all systems in both directions on set2, and is overall the best system among the cons"
2020.wmt-1.4,P17-4012,0,0.0234194,"the decoder. This allows the test sets from known domains to use adapter layers and for novel domains to use the generic system. They created a noisy domain by adding synthetic noise to source data. The idea is that residual adapter layer learned from such data learns how to deal with noisy domain and is also able to preserve performance on the cleaner domains. However this did not work as well. The residual adapter fine-tuned using the ParaCrawl corpus gave better performance. PROMPT: Team PROMPT also participated mainly in the News translation task. Their systems were trained using OpenNMT (Klein et al., 2017) toolkit. They applied several stages of data preprocessing including length-based filtering, removing duplications, and using in-house classifier based on Hunalign aligner to identify and discard non-parallel sentences. They used two types of synthetic data to improve their systems: i) randomly selecting subset of Wikipedia equal to the size of news data and generating parallel corpus through back-translation, ii) creating synthetic data with unknown words using the procedure described in (Pinnis et al., 2017). Systems were trained with tags to differentiate between original data and syntheti"
2020.wmt-1.4,P02-1040,0,0.114721,"l can bias the selection to cases that are challenging for this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professi"
2020.wmt-1.4,W17-3204,1,0.837046,"e evaluated both automatically and via a human evaluation and the results discussed in Section 5. We hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in"
2020.wmt-1.4,W18-6459,0,0.0133196,"rvey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on on"
2020.wmt-1.4,W19-5303,1,0.901081,"e 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predomin"
2020.wmt-1.4,W18-6319,0,0.0156248,"or this particular model. In future work following this methodology, we recommend that multiple MT models be used. 4 https://cloud.google. com/natural-language/docs/ analyzing-sentiment 5 https://github.com/carpedm20/emoji/) 2 www.kaggle.com/c/ jigsaw-toxic-comment-classification- challenge 78 3.5 5. Presence of idioms: we checked if the source contains idiomatic expressions, using an inhouse list of idioms built from various sources, and labelled those cases as positive. Evaluation protocol Automatic evaluation: We first computed BLEU (Papineni et al., 2002) for each system using SacreBLEU (Post, 2018). For all language pairs except En→Ja, we used the original reference and SacreBLEU with the default options. In the case of En→Ja, we used the reference tokenized with KyTea and the option --tokenize none. We note that the automatic labelling using our various pre-processing techniques may have introduced errors, but we believe that basing the selection on such heuristics will still lead to higher chances of selecting very challenging source segments than arbitrarily sampling the data. Human evaluation: The system outputs were evaluated by professional translators. The translators were presen"
2020.wmt-1.4,E17-2045,0,0.0383395,"o domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of mono"
2020.wmt-1.4,2021.ccl-1.108,0,0.105338,"Missing"
2020.wmt-1.4,P16-1162,0,0.0120732,"ns at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously"
2020.wmt-1.4,2015.iwslt-evaluation.11,0,0.64331,"ims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine translation (Koehn and Knowles, 2017). Most approaches for improving robustness of MT systems to domain shift assume the existence of significant amounts of parallel data in both the source and target domain. In this scenario, a common approach is to first train an MT system on a (generic) source domain and then to fine-tune it on a (specific) target domain (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Servan et al., 2016; Chu 76 Proceedings of the 5th Conference on Machine Translation (WMT), pages 76–91 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approac"
2020.wmt-1.4,N19-1314,1,0.844424,"hope that this task leads to more efforts from the community in building robust MT models. Introduction In recent years, Machine Translation (MT) systems have seen great progress, with neural models becoming the de-facto methods and even approaching human quality in news domain (Hassan et al., 2018). However, like other deep learning models, neural machine translation (NMT) models are found to be sensitive to synthetic and natural noise in input, distributional shift, and adversarial examples (Koehn and Knowles, 2017; Belinkov and Bisk, 2018; Durrani et al., 2019; Anastasopoulos et al., 2019; Michel et al., 2019). From an application perspective, MT systems need to deal with non-standard, noisy text of the kind which is ubiquitous on social media and the internet, yet has different distributional signatures from corpora in common benchmark datasets. Following the first shared task on Machine Translation (MT) Robustness, we now propose a second edition, which aims at testing MT systems’ robustness towards domain diversity. Specifically, this year’s task aims to evaluate a general MT system’s performance in the following two scenarios: 2 Related Work Domain mismatch is a key challenge in machine transla"
2020.wmt-1.4,D18-1050,1,0.940705,"tation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et al., 2017; Anastasopoulos et al., 2019; Lee et al., 2018). Michel and Neubig (2018) proposed a new dataset (MTNT) to test MT models for robustness to the types of noise encountered in the Internet. The previous iteration of the shared task focused on robustness of MT systems to such noise (Li et al., 2019). We refer to that report for more details. 3 participants to explore novel training and modeling approaches so that models have more robust performance at test time on multiple domains, including unseen and diversified domains. We offer two language pairs: English-German (En→De) and English-Japanese (En→Ja), with different test sets focusing on one or both these language p"
2020.wmt-1.4,2020.lrec-1.517,1,0.814412,"comments from the social media website reddit.com were scraped, filtered for noisy comments and translated by professional translators. This year, data was collected for two translation directions: English→Japanese and Japanese→English. For English, comments were collected from the /r/all feed, which encompasses all communities, and filtered for English. Since Japanese is a minority language on Reddit, comments were scraped from a selection of japanese-speaking communities, detailed in Michel and Neubig (2018). Common Voices Test Set (set3): This data was obtained from from the CoVoST corpus (Wang et al., 2020). CoVoST is derived from Common Voice (Ardila et al., 2020), a crowdsourced speech recognition corpus with an open CC0 license. Transcripts were sent to professional translators and the quality of translations was controlled by automatic and manual checks (Guzm´an et al., 2019). For this task, we used the German→English test set with source German sentences deduplicated. 1 Bad: translation errors are so severe that they cause the target text to be incomprehensible. This may be mainly due to major grammatical, typographical, or lexical errors, or omission of critical or important salient inform"
2020.wmt-1.4,N19-4007,1,0.808507,"as well as an analysis of catastrophic errors (Section 5.2). 5.1 General Quality Overall, the correlation between human judgments and BLEU is not strong. For En→De (set1), the Pearson’s correlation coefficient is 0.97, while for the other four tasks the coefficients are lower, with 0.78, 0.65, 0.52, 0.79 for En→De (set1), Ja→En (set2), En→Ja (set2), and De→En (set3) respectively. Automatic Evaluation The automatic evaluation (BLEU) results of the Shared Task are summarized in Table 2, where we also include the three online translation systems. We performed significance test using compare-mt (Neubig et al., 2019) where systems are considered as significantly different at p <0.05. The result of significance test is used for the automatic evaluation ranking. Overall, the unconstrained online-B system provides strong results and outperforms most systems in the five language pairs, except the De→En (set3) and En→Ja (set1). Among the participating teams, the best zeroshot systems were OPPO, which outperforms other zero-shot systems in En→De (set1), Ja→En (set2), and En→Ja (set2) tasks, and NLE, which outperforms other systems in the other two tasks. Only Naver Labs participated in the few-shot stage (NLE-f"
2020.wmt-1.4,P17-2089,0,0.0494679,"Missing"
2020.wmt-1.4,D17-1155,0,0.0161114,"Association for Computational Linguistics et al., 2017), to continuously fine-tune on datasets increasingly similar to the target domain (Sajjad et al., 2017), or to dynamically change the balance of data towards the target domain (van der Wees et al., 2017). Another approach trains a system on multiple domains at the same time, while adding domain-specific tags to the input examples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at traini"
2020.wmt-1.4,D17-1147,0,0.0351203,"Missing"
2020.wmt-1.4,D16-1160,0,0.025774,"ples (Kobus et al., 2016). Both these approaches were employed by participants of the first shared task on MT robustness (Li et al., 2019). Other methods for domain adaptation of MT systems include instance weighting (Wang et al., 2017b), incorporating a domain classifier (Chen et al., 2017; Britz et al., 2017), and data selection (Wang et al., 2017a). Some make use of monolingual data available either in the target domain—for example by training the decoder on such data (Domhan and Hieber, 2017) or by backtranslating it (Sennrich et al., 2016)—or in the source domain, via similar techniques (Zhang and Zong, 2016). Chu and Wang (2018) provide a broad survey of domain adaptation for neural MT, which demonstrates that the predominant setup assumes knowledge of the target domain and availability of target domain data at training time. In light of this prior work, the shared task proposed a relatively underexplored scenario, where examples in the target domain are either unavailable or relatively few. Other aspects of robustness are robustness to adversarial examples or noisy inputs. The fragility of neural MT models has been previously demonstrated in various settings (Belinkov and Bisk, 2018; Heigold et"
2020.wmt-1.78,N19-1388,0,0.0216725,"election methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. 2.4 2.6 By now, neural machine translation systems are rarely trained only on the parallel corpus of the desired language pair. Common foundations are pretrained models trained on multiple language pairs which share the source or target language (Aharoni et al., 2019; Fan et al., 2020) or monolingual pre-training methods (Liu et al., 2020). Often, the models are also improved by a second stage of training that uses back-translated synthetic parallel data that was generated from first stage model — a process that may be iterated (Hoang et al., 2018). To reflect such a more realistic training setup, we provided pre-trained models that were trained on monolingual data using a denoising autoencoder method called mBART (Liu et al., 2020). Here, monolingual data is converted into input and output pairs by (a) masking out words in the input, forcing the model to"
2020.wmt-1.78,W11-1218,0,0.0186235,"epali–English and Sinhala–English. For these languages much less clean parallel data was available and hence many of the methods developed for high-resource languages are less reliable. The best-performing submission that year (Chaudhary et al., 2019) also considered dual crossentropy but found that matching multilingual sentence embeddings (Schwenk, 2018) gave better results. are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). Van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neura"
2020.wmt-1.78,D11-1033,0,0.0508684,"l. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). Van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. 2.4 2.6 By now, neural machine translation systems are rarely trained only on the parallel corpus of the desired language pair. Common foundations are pretrained models trained on multiple"
2020.wmt-1.78,P13-2061,0,0.027165,"ng Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in paralle"
2020.wmt-1.78,2020.emnlp-main.480,1,0.817813,"Missing"
2020.wmt-1.78,2020.aacl-main.62,1,0.86925,"Missing"
2020.wmt-1.78,C10-2010,0,0.0316759,"icipating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), w"
2020.wmt-1.78,W19-5436,0,0.0174017,"RT-style Transformer. A relatively small training corpus is used (2,000 or 10,000 sentence pairs) with 10x over-sampled negatives. 5.2.2 JHU-Koerner (Koerner and Koehn, 2020) employ a linear combination of LASER scores, monolingual language model scores, dual cross entropy, and use a sentence duplication penalty. JHU-Kejriwal (Kejriwal and Koehn, 2020) use LASER scores with some novel transformation of score ranges, language ID confidence scores, monolingual language models trained on words and characters, and length-based filters. Individual Submissions AFRL use their corpus-building method (Erdmann and Gwinnup, 2019) but with a bidirectional quality metric that nearly eliminates pre-filtering (used only for the limit on training line length). The coverage metric encourages the addition of a sentence that improves corpus-level bilingual vocabulary frequencies. The new quality metric is the average of sentence-level NMT scores (“loglikelihoods”) in both directions. Microsoft (Nokrashy et al., 2020) focus on the LASER scores, using both the provided LASER scores, custom LASER scores using a model trained on the provided clean parallel data (which are better for Pashto but worse for Khmer), and a classifier b"
2020.wmt-1.78,W16-2347,1,0.84143,"llel documents and parallel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. Additional parallel data was sourced from the Paracrawl project – a large-scale effort to crawl text from the web3 (Ba˜no´ n et al., 2020). Acquiring parallel corpora from the web (ElKishky et al., 2020b) is an active area of research that typically involves identifying web sites with parallel text, downloading the documents from the web site, aligning document pairs (Buck and Koehn, 2016; Thompson and Koehn, 2020; ElKishky and Guzm´an, 2020), and aligning sentence pairs. A final stage of the processing pipeline filters out non-parallel sentence pairs. Such noise exists either because the original web site did not have any actual parallel data (garbage in, garbage out), only partially-parallel data, or due to failures of processing steps. 1 http://opus.nlpl.eu http://statmt.org/cc-aligned 3 http://www.paracrawl.eu/ 2 4 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 727 translation directions to score sentence pairs with dual cross-entropy. Last year, we"
2020.wmt-1.78,2020.wmt-1.107,0,0.0624061,"Missing"
2020.wmt-1.78,W17-3209,0,0.0262772,"parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web Parallel Corpus Acquisition Noisy parallel documents and parallel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. A"
2020.wmt-1.78,W19-5435,1,0.888005,"Missing"
2020.wmt-1.78,J93-1004,0,0.881688,"ain neural machine translation systems on these subsets, and measure their quality with the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson"
2020.wmt-1.78,P93-1002,0,0.703239,"ith the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse"
2020.wmt-1.78,W16-2369,0,0.0558247,"Missing"
2020.wmt-1.78,2020.acl-main.747,1,0.71089,"Missing"
2020.wmt-1.78,D19-1632,1,0.909569,"Missing"
2020.wmt-1.78,N10-1000,0,0.18974,"Missing"
2020.wmt-1.78,W18-2703,1,0.83705,"ask is focused on data quality that is relevant for all domains. 2.4 2.6 By now, neural machine translation systems are rarely trained only on the parallel corpus of the desired language pair. Common foundations are pretrained models trained on multiple language pairs which share the source or target language (Aharoni et al., 2019; Fan et al., 2020) or monolingual pre-training methods (Liu et al., 2020). Often, the models are also improved by a second stage of training that uses back-translated synthetic parallel data that was generated from first stage model — a process that may be iterated (Hoang et al., 2018). To reflect such a more realistic training setup, we provided pre-trained models that were trained on monolingual data using a denoising autoencoder method called mBART (Liu et al., 2020). Here, monolingual data is converted into input and output pairs by (a) masking out words in the input, forcing the model to learn the correct word or word sequence from the context, and (b) shuffling the order of a few concatenated sentence pairs. Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating syste"
2020.wmt-1.78,J82-2005,0,0.715747,"Missing"
2020.wmt-1.78,W18-6478,0,0.0178193,"he shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to align sentences in document pairs and to filter it to a smaller size of high quality sentence pairs. Findings of Previous Shared Tasks We organized versions of this shared task in the previous two years. In 2018, we started with a high-resource language pair (German–English) and a very large web-crawled parallel corpus, a subset of the Paracrawl corpus consisting of 1 billion English words (Koehn et al., 2018). The bestperforming submission (Junczys-Dowmunt, 2018) used neural machine translation systems in both 3.1 Filtering For the filtering-only task, we provided a very noisy 58.3 million word corpus for Khmer– English (English token count) and a 11.6 million word corpus for Pashto–English, crawled from the 728 web (see Section 4.3 for details). We asked participants to generate sentence-level quality scores that allow selecting subsets of sentence pairs that amount to 5 million words, counted on the English side. This amount was chosen based on preliminary experiments (we report below on additional subset sizes). Participants in the shared task subm"
2020.wmt-1.78,2020.tacl-1.47,1,0.90024,"today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, high-quality training data is lacking (Koehn and Knowles, 2017). Improving performance on low resource language pairs has high impact considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. While there have been advances in using monolingual corpora (Lample et al., 2018; Liu et al., 2020) and parallel corpora in multiple language 726 Proceedings of the 5th Conference on Machine Translation (WMT), pages 726–742 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics 2.2 a fixed size (5 million English words), train neural machine translation systems on these subsets, and measure their quality with the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of s"
2020.wmt-1.78,2020.wmt-1.108,1,0.92033,"2,378 2,309 2,320 40,436 44,471 40,341 Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the provided clean parallel data can be used by feeding in the English sentence and computing the probability of the foreign sentence according to the model, and vice versa. JunczysDowmunt (2018)"
2020.wmt-1.78,W19-5358,0,0.0278491,"ges pairs, a combination of them performs best. Alibaba (Lu et al., 2020) use a number of features that are combined linearly: a bilingual GPT2 model trained on source-target language pairs as well as monolingual GPT-2 model each of the languages, dual cross entropy from neural machine translation models trained in both directions and statistical word translation model scores. They report that they experimented with classifiers to weight features but found this to be not beneficial. NRC (Lo and Joanis, 2020) tackle both filtering and alignment. Their filtering score is mainly based on Yisi-2 (Lo, 2019), a language model trained on the target side, and representations obtained with XLM-RoBERTa (Conneau et al., 2020) pre-trained for Pashto, Khmer, and English. Sentence alignment is based on the approach by Moore (2002), first applied to align paragraphs and then sentences. Bytedance (Xu et al., 2020) tackle only the combined alignment/filtering task. The sentence alignment methods draws on statistical lexical translation scores, as used in YiSi-2. They iteratively improve the lexical model by adding high-quality mined sentence pair to its training data. Their filtering method is a classifier"
2020.wmt-1.78,W18-2709,1,0.918844,"ltimore, Maryland, United States † Facebook AI, Menlo Park, California, United States Abstract pairs (Aharoni et al., 2019; Fan et al., 2020), the best training data for machine translation are still parallel corpora in the targeted language pair and domain. Parallel corpora are typically gathered from any available source without much guarantees about quality. This is especially the case for parallel corpora that are extracted from the web without much control over which web sites are mined. Since noisy training data has been recognized as a challenge for neural machine translation training (Khayrallah and Koehn, 2018), an essential step in using such data is filtering or discounting noisy sentence pairs. Recently, there is increased interest in the filtering of noisy parallel corpora to improve the data that can be used to train translation systems. The Shared Task on Parallel Corpus Filtering and Alignment at the Conference for Machine Translation (WMT 2020) was organized to promote research to make learning from noisy data more viable for low-resource languages. It is similar to the previous year’s task but tackles different languages (Pashto and Khmer instead of Nepali and Sinhala) and also included the"
2020.wmt-1.78,2005.mtsummit-papers.11,1,0.195972,"2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier d"
2020.wmt-1.78,W19-5404,1,0.7475,"Missing"
2020.wmt-1.78,2020.wmt-1.111,0,0.315983,"e an overview of methods and then give a short sum731 Pashto dev dev test test Khmer Sentence Pairs English Words Sentence Pairs English Words 3,162 2,698 2,719 55,439 46,175 47,695 2,378 2,309 2,320 40,436 44,471 40,341 Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the p"
2020.wmt-1.78,moore-2002-fast,0,0.519033,"score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes, the average sentence length of sub-selected data, and overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine searc"
2020.wmt-1.78,W18-6453,1,0.818016,"d interest in the filtering of noisy parallel corpora to improve the data that can be used to train translation systems. The Shared Task on Parallel Corpus Filtering and Alignment at the Conference for Machine Translation (WMT 2020) was organized to promote research to make learning from noisy data more viable for low-resource languages. It is similar to the previous year’s task but tackles different languages (Pashto and Khmer instead of Nepali and Sinhala) and also included the challenge to extract sentence pairs from document pairs. The shared task is organized similarly to previous years (Koehn et al., 2018, 2019). We provide about 11.6 million word noisy parallel data for Pashto-English and 58.3 million word noisy parallel data for Khmer-English. We also provide small amounts of clean parallel data of varying quality and monolingual data from Wikipedia and CommonCrawl. Participants developed methods to assign a quality score for each sentence pair. These scores are used to filter the web crawled corpora down to Following two preceding WMT Shared Tasks on Parallel Corpus Filtering (Koehn et al., 2018, 2019), we posed again the challenge of assigning sentence-level quality scores for very noisy c"
2020.wmt-1.78,W17-3204,1,0.80612,"10 participants from companies, national research labs, and universities participated in this task. 1 Introduction The field of Machine Translation has experienced significant advances in recent years thanks to improvements in neural modeling (Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., 2013; Bojar et al., 2017). Unfortunately, today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, high-quality training data is lacking (Koehn and Knowles, 2017). Improving performance on low resource language pairs has high impact considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. While there have been advances in using monolingual corpora (Lample et al., 2018; Liu et al., 2020) and parallel corpora in multiple language 726 Proceedings of the 5th Conference on Machine Translation (WMT), pages 726–742 c Online, November 19–20, 20"
2020.wmt-1.78,N19-4009,0,0.152457,"provided detailed instructions on how to use these tools to replicate the official testing environment. Alignment 4 Data We provided three types of data for this shared task: (1) clean parallel and monolingual data, including related language data in Hindi, to train models that aid with the filtering task, (2) the noisy parallel data crawled from the web which participants have to score for filtering, and (3) development and test sets that are used to evaluate translation systems trained on filtered data. Evaluation The submissions were scored by building a neural machine translation system (Ott et al., 2019) trained on this data, and then measuring their BLEU score on the flores Wikipedia test sets (Guzm´an et al., 2019). The neural machine translation model was either randomly initialized or initialized by monolingual pre-training (mBART). For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of: • A script to subsample corpora based on quality scores. • fairseq scripts to train and test a neural machine translation system. • A pre-trained mBART model for continued training. • Th"
2020.wmt-1.78,2020.wmt-1.109,1,0.92417,"test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the provided clean parallel data can be used by feeding in the English sentence and computing the probability of the foreign sentence according to the model, and vice versa. JunczysDowmunt (2018) proposed a metric that uses not only the individual computed cross entrop"
2020.wmt-1.78,2009.mtsummit-posters.15,0,0.077943,"obability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus;"
2020.wmt-1.78,2011.mtsummit-papers.48,0,0.0272057,"ood sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web Parallel Corpus Acquisition Noisy parallel documents and parallel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. Additional parallel data was sourced from the Paracrawl project – a large-scale effort to crawl text from the web3 (Ba˜no´ n et al., 2020). Acquiring parallel corpora from the web (ElKishky et al., 2020b) is an active area of research that typically involves i"
2020.wmt-1.78,P99-1068,0,0.107062,"nd Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memori"
2020.wmt-1.78,P18-2037,0,0.0212053,"ac.uk/nlp4tm2016/shared-task/ 727 translation directions to score sentence pairs with dual cross-entropy. Last year, we moved the focus to low resource languages (Koehn et al., 2019) with smaller noisy parallel corpora, comprising 50-60 million words for Nepali–English and Sinhala–English. For these languages much less clean parallel data was available and hence many of the methods developed for high-resource languages are less reliable. The best-performing submission that year (Chaudhary et al., 2019) also considered dual crossentropy but found that matching multilingual sentence embeddings (Schwenk, 2018) gave better results. are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selectio"
2020.wmt-1.78,2010.amta-papers.14,0,0.0352622,"overlap between the submissions. 2 Sentence alignment has been a very active field of research since the early days of statistical machine translation. An influential early method is based on sentence length, measured in words (Gale and Church, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of"
2020.wmt-1.78,D11-1126,0,0.0249695,"ropy. Last year, we moved the focus to low resource languages (Koehn et al., 2019) with smaller noisy parallel corpora, comprising 50-60 million words for Nepali–English and Sinhala–English. For these languages much less clean parallel data was available and hence many of the methods developed for high-resource languages are less reliable. The best-performing submission that year (Chaudhary et al., 2019) also considered dual crossentropy but found that matching multilingual sentence embeddings (Schwenk, 2018) gave better results. are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod e"
2020.wmt-1.78,P13-1135,1,0.8158,"train machine translation systems. This year, the task tackled the low resource condition of Pashto– English and Khmer–English and also included the challenge of sentence alignment from document pairs. 10 participants from companies, national research labs, and universities participated in this task. 1 Introduction The field of Machine Translation has experienced significant advances in recent years thanks to improvements in neural modeling (Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., 2013; Bojar et al., 2017). Unfortunately, today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, high-quality training data is lacking (Koehn and Knowles, 2017). Improving performance on low resource language pairs has high impact considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. While there have been advances in using mono"
2020.wmt-1.78,W18-6314,0,0.0200536,"ms that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study about how noise that occurs in crawled parallel text impacts statistical and neural machine translation. Neural machine translation model training may combine data selection and model training, taking advantage of the increasing quality of the model to better detect noisy data or to increasingly focus on cleaner parts of the data (Wang et al., 2018; Kumar et al., 2019). 2.5 Monolingual Pre-Training 3 Shared Task Definition The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to align sentences in document pairs and to filter it to a smaller size of high quality sentence pairs. Findings of Previous Shared Tasks We organized versions of this shared task in the previous two years. In 2018, we started with a high-resource language pair (German–English) and a very large web-crawled parallel corpus, a subset of the Paracrawl corpus consisting of"
2020.wmt-1.78,2011.eamt-1.25,0,0.0731606,"Missing"
2020.wmt-1.78,2011.mtsummit-papers.47,0,0.0601538,"anadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic"
2020.wmt-1.78,D17-1147,0,0.0510981,"Missing"
2020.wmt-1.78,D19-1136,1,0.902758,"h, 1993). Several researchers proposed including lexical information (Chen, 1993; Moore, 2002) with the emergence of tools that use provided bilingual dictionaries (Varga et al., 2005) or acquire them during in an unsupervised fashion (Braune and Fraser, 2010). Later work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Pereira Lopes, 2016). Both methods anchor high-probability 1–1 alignments in the search space and then fill in and refine alignments. More recently, Thompson and Koehn (2019) introduced the use of sentence embeddings and a coarse-to-fine search method to the task (Vecalign). Related Work Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of t"
2020.wmt-1.78,D17-1319,1,0.792925,"; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site1 (Tiedemann, 2012). 2.1 Sentence Alignment 2.3 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering4 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additio"
2020.wmt-1.78,2020.wmt-1.112,0,0.179267,"ive a short sum731 Pashto dev dev test test Khmer Sentence Pairs English Words Sentence Pairs English Words 3,162 2,698 2,719 55,439 46,175 47,695 2,378 2,309 2,320 40,436 44,471 40,341 Table 7: Statistics for the flores test sets used to evaluate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on tokenized text. Short Name Participant and System Description Citation AFRL Alibaba Bytedance Edinburgh Huawei JHU-Kejriwal JHU-Koerner Microsoft NRC UA-Prompsit LASER Air Force Research Lab, USA Alibaba, China (Lu et al., 2020) Bytedance, China (Xu et al., 2020) University of Edinburgh, Scotland Huawei, Turkey/China (Ac¸arc¸ic¸ek et al., 2020) Ankur Kejriwal, Johns Hopkins University, USA (Kejriwal and Koehn, 2020) Felicia Koerner, Johns Hopkins University, USA (Koerner and Koehn, 2020) Microsoft, Egypt Development Center, Egypt (Nokrashy et al., 2020) National Research Council, Canada (Lo and Joanis, 2020) University of Alicante and Prompsit, Spain (Espl`a-Gomis et al., 2020) Officially provided baseline Table 8: Participants in the shared task. Dual cross entropy Neural machine translation systems trained on the provided clean parallel data can be"
2020.wmt-1.78,2020.emnlp-main.483,1,0.833336,"allel sentences were sourced from the CCAligned2 dataset (ElKishky et al., 2020a), a massive collection of cross-lingual web documents covering over 8k language pairs aligned from 68 Common Crawl snapshots. Additional parallel data was sourced from the Paracrawl project – a large-scale effort to crawl text from the web3 (Ba˜no´ n et al., 2020). Acquiring parallel corpora from the web (ElKishky et al., 2020b) is an active area of research that typically involves identifying web sites with parallel text, downloading the documents from the web site, aligning document pairs (Buck and Koehn, 2016; Thompson and Koehn, 2020; ElKishky and Guzm´an, 2020), and aligning sentence pairs. A final stage of the processing pipeline filters out non-parallel sentence pairs. Such noise exists either because the original web site did not have any actual parallel data (garbage in, garbage out), only partially-parallel data, or due to failures of processing steps. 1 http://opus.nlpl.eu http://statmt.org/cc-aligned 3 http://www.paracrawl.eu/ 2 4 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 727 translation directions to score sentence pairs with dual cross-entropy. Last year, we moved the focus to low re"
2020.wmt-1.79,N13-1073,0,0.0416194,"rip • Source side: Each word in the source side is labelled as OK (correctly translated) or BAD (caused a translation error). • Target side: Each word in the target side is labelled as OK (a correct translation) or BAD (should be replaced or deleted). Additionally, we consider gap ‘tokens’ at the beginning of the sentence, at the end and between each two words. They are labelled OK if no word should be inserted in that position (according to the post-edited version), and BAD otherwise. In order to obtain the labels, we first align source and MT using the IBM Model 2 alignments from FastAlign (Dyer et al., 2013), and compute edit distances between the generated and post-edited translations with TERCOM, using default settings and disabled shifts. 2.3 Task 3: Predicting document-level MQM This task consists in finding document-level translation errors and estimating a quality score according to the amount of minor, major, and critical errors present in the translation. The predictions are compared to a ground-truth obtained from annotations produced by crowd-sourced human translators from Unbabel community. Each document contains zero or more errors, annotated according to the MQM taxonomy6 , and 5 htt"
2020.wmt-1.79,D19-1632,1,0.894336,"Missing"
2020.wmt-1.79,2020.wmt-1.117,0,0.0617894,"Missing"
2020.wmt-1.79,2020.evalnlgeval-1.4,0,0.061105,"Missing"
2020.wmt-1.79,W19-5406,1,0.692374,"Missing"
2020.wmt-1.79,P19-3020,1,0.743217,"Missing"
2020.wmt-1.79,W17-4763,0,0.358683,"and 2019 to the training set, keeping the same development set from 2019, and released a new test set. The documents are short product title and descriptions in English, extracted from the Amazon Product Reviews dataset (McAuley et al., 2015; He and McAuley, 2016) (Sports and Outdoors category). The documents were machine translated into French using a state of the art online neural MT system. The dataset statistics are presented in Table 2. 3 Baseline systems Sentence-level baseline systems: For Tasks 1 and 2, both word and sentence-level, we used the LSTM-based Predictor-Estimator approach (Kim et al., 2017), implemented in OpenKiwi (Kepler et al., 2019b). The Predictor model was trained on the same parallel data as the NMT systems for each language pair (made available at the task website),7 while the the Estimator was trained on the 7, 000 QE labelled data for each task. Word-level baseline systems: For Task 2, we also used the Predictor-Estimator as above, but it was trained to predict jointly word-level tags and sentence-level scores. Document-level baseline system: For Task 3, similarly as last year, we used a baseline which treats sentences independently and casts the problem as word-level"
2020.wmt-1.79,2020.wmt-1.118,0,0.169813,"Missing"
2020.wmt-1.79,2020.wmt-1.119,1,0.89237,"Missing"
2020.wmt-1.79,2020.wmt-1.120,0,0.0548717,"Missing"
2020.wmt-1.79,W19-5333,0,0.0229976,"with a bilingual parallel corpus, and the entire model is then fine-tuned on the training quality labelled dataset of the shared task. At test time, the translation outputs, which are estimated with teacher forcing and special masking, are put together with the source sentences and put through a unified neural network model to predict the quality of the translations. Mak (T1): Mak represents the source and its translation sentence pairs as a set of 70 blackbox sentence-level features extracted with Quest++(Specia et al., 2015), using the resources used to train the English-Russian NMT system (Ng et al., 2019). Those features are then fitted into a support vector regressor with default settings. NICT Kyoto (T2): The English–German and English-Chinese sentence-level QE systems for Task 2 are ensembles of pre-trained crosslingual language models (XLM) (Conneau and Lample, 2019), fine-tuned in a multi-task fashion with two linear output layers for sentence and word-level quality estimation. A total of 8 XLM models with various masking hyper-parameters were domain-adapted using a subset of the additional resources provided by the QE shared task organisers, as well as a subset of the WikiMatrix corpus ["
2020.wmt-1.79,N19-4009,0,0.0363276,"refer to as direct assessment (DA). For that, a new dataset, was created containing seven languages pairs using sentences mostly from Wikipedia2 . These language pairs are divided into 3 categories: the high-resource English→German (En-De), English→Chinese (En-Zh) and Russian→English (Ru-En) pairs; the medium-resource Romanian→English (RoEn) and Estonian→English (Et-En) pairs; and the low-resource Sinhala→English (Si-En) and Nepali→English (Ne-En) pairs. Translations were produced with state-of-theart transformer-based NMT models trained using publicly available data and the fairseq toolkit (Ott et al., 2019); and were manually annotated for perceived quality. The quality label for this task ranges from 0 to 100, following the FLORES guidelines (Guzm´an et al., 2019). According to the guidelines given to annotators, the 0-10 range represents an incorrect translation; 11-29, a translation with few correct keywords, but the overall meaning is different from the source; 30-50, a translation with major mistakes; 51-69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70-90, a translation that closely preserves the semantics o"
2020.wmt-1.79,2020.wmt-1.122,0,0.191132,"Missing"
2020.wmt-1.79,D19-1410,0,0.0226745,". For sentence-level, the different models are used as feature extractors, which are used as inputs of a dense layer to produce the predictions. For word-level, they use majority voting to ensemble the different models. Papago (T1, T3): Papago’s submission for Task 1 749 En-De is an ensemble of three models based on pre-trained contextualised representations: multilingual BERT (mBERT), XLM-MaskedLanguage-Modelling (XLM-MLM), and XLM-Causal-Language-Modelling (XLMCLM). Three scores were produced from these models: an extension of BERTScore using the multilingual BERT model, SentenceBERT score (Reimers and Gurevych, 2019), and target (German) language model score using a pre-trained GPT-2 model. Additionally, the scores were computed for synthetic data created using WMT News translation data by randomly performing different methods, including swapping word order, omiting words or repeating phrases. The three models are pre-trained from these data in a multi-task regression setting. Lastly, these pre-trained models are fine-tuned using the QE corpus. For Task 3, the submitted system uses an ensemble of four models leveraging either multilingual BERT or XLM. The training scheme is very task-oriented: erroneous s"
2020.wmt-1.79,2020.wmt-1.121,0,0.0571448,"Missing"
2020.wmt-1.79,2016.amta-researchers.2,0,0.082682,"for errors: minor (if it is not misleading nor changes meaning), major (if it changes meaning), and critical (if it changes meaning and carries any kind of implication, possibly offensive). Figure 1 shows an example of fine-grained error annotations for a sentence. Note that there is an annotation composed by two discontinuous spans: a whitespace and the token Grip — in this case, the annotation indicates wrong word order, and Grip should have been at the whitespace position. Document-level scores were then generated from the word-level errors and their severity using the method described in Sanchez-Torron and Koehn (2016, footnote 6). Namely, denoting by n the number of words in the document, and by nmin , nmaj , and ncri the number of annotated minor, major, and critical errors, the final quality scores were computed as: see MQM = 1 − 745 nminor + 5nmajor + 10ncrit (1) n Note that MQM values can be negative if the total severity exceeds the number of words. As this year’s dataset, we reused the training data from previous years, adding the test sets from 2018 and 2019 to the training set, keeping the same development set from 2019, and released a new test set. The documents are short product title and descri"
2020.wmt-1.79,P15-4020,1,0.771605,"irectional LSTM. The parameters of the Transformer bottleneck layer are first optimised with a bilingual parallel corpus, and the entire model is then fine-tuned on the training quality labelled dataset of the shared task. At test time, the translation outputs, which are estimated with teacher forcing and special masking, are put together with the source sentences and put through a unified neural network model to predict the quality of the translations. Mak (T1): Mak represents the source and its translation sentence pairs as a set of 70 blackbox sentence-level features extracted with Quest++(Specia et al., 2015), using the resources used to train the English-Russian NMT system (Ng et al., 2019). Those features are then fitted into a support vector regressor with default settings. NICT Kyoto (T2): The English–German and English-Chinese sentence-level QE systems for Task 2 are ensembles of pre-trained crosslingual language models (XLM) (Conneau and Lample, 2019), fine-tuned in a multi-task fashion with two linear output layers for sentence and word-level quality estimation. A total of 8 XLM models with various masking hyper-parameters were domain-adapted using a subset of the additional resources provi"
2020.wmt-1.79,2020.acl-main.558,1,0.893099,"Missing"
2020.wmt-1.79,2020.wmt-1.123,0,0.0620909,"Missing"
2020.wmt-1.79,2020.wmt-1.124,0,0.0510029,"Missing"
2020.wmt-1.79,2020.wmt-1.125,0,0.0843421,"Missing"
2021.acl-long.101,N19-1388,0,0.0460248,"ld still enforce a one-to-one mapping to the input tokens. This condition allows our motivation and approach to remain applicable. 6 Related Work Initial works on multilingual translation systems already showed some zero-shot capability (Johnson et al., 2017; Ha et al., 2016). Since then, several works improved zero-shot translation performance by controlling or learning the level of parameter sharing between languages (Lu et al., 2018; Platanios et al., 2018). Recently, models with full parameter sharing have gained popularity, with massively multilingual systems showing encouraging results (Aharoni et al., 2019; Zhang et al., 2020a; Fan et al., 2020). Besides advantages such as compactness and ease of deployment, the tightly-coupled model components also open up new questions. One question is how to form language-agnostic representations at a suitable abstraction level. In this context, one approach is to introduce auxiliary training objectives to encourage similarity between the representations of different languages (Arivazhagan et al., 2019; Pham et al., 2019). In this work we took a different perspective: Instead of introducing additional objectives, we relax some of the pre-defined structure to"
2021.acl-long.101,P19-1121,0,0.096549,"me. From a modeling perspective, zero-shot translation calls for language-agnostic representations, which are likely more robust and can benefit low-resource translation directions. Despite the potential benefits, achieving highquality zero-shot translation is a challenging task. Prior works (Arivazhagan et al., 2019; Zhang et al., 2020a; Rios et al., 2020) have shown that standard systems tend to generate poor outputs, sometimes in an incorrect target language. It has been further shown that the encoder-decoder model captures spurious correlations between language pairs with supervised data (Gu et al., 2019). During training, the model only learns to encode the inputs in a form that facilitates translating the supervised directions. The decoder, when prompted for zero-shot translation to a different target language, has to handle inputs distributed differently from what was seen in training, which inevitably degrades performance. Ideally, the decoder could translate into any target language it was trained on given an encoded representation independent of input languages. In practice, however, achieving a language-agnostic encoder is not straightforward. 1259 Proceedings of the 59th Annual Meeting"
2021.acl-long.101,D19-1165,0,0.0185132,"zero-shot directions must be known upfront in order to train on the corresponding languages. In comparison, our adaptation procedure offers more flexibility, as the first training step remains unchanged regardless of which new language is later finetuned on. This could be suitable to the practical scenario of later acquiring data for the new language. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders and decoders is analogous to our motivation for zero-shot translation. 7 Conclusion In"
2021.acl-long.101,2020.wmt-1.66,0,0.0264499,"age. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders and decoders is analogous to our motivation for zero-shot translation. 7 Conclusion In this work, we show that the positional correspondence to input tokens hinders zero-shot translation. Specifically, we demonstrate that: 1) the encoder outputs retain word orders of source languages; 2) this positional information reduces cross-lingual generalizability and therefore zero-shot translation quality; 3) the problems above can be easily al"
2021.acl-long.101,W18-6309,0,0.0219602,"likely more language-agnostic by seeing more languages, as we still present source sentences as a sequence of tokens, the residual connections, when present in all layers, would still enforce a one-to-one mapping to the input tokens. This condition allows our motivation and approach to remain applicable. 6 Related Work Initial works on multilingual translation systems already showed some zero-shot capability (Johnson et al., 2017; Ha et al., 2016). Since then, several works improved zero-shot translation performance by controlling or learning the level of parameter sharing between languages (Lu et al., 2018; Platanios et al., 2018). Recently, models with full parameter sharing have gained popularity, with massively multilingual systems showing encouraging results (Aharoni et al., 2019; Zhang et al., 2020a; Fan et al., 2020). Besides advantages such as compactness and ease of deployment, the tightly-coupled model components also open up new questions. One question is how to form language-agnostic representations at a suitable abstraction level. In this context, one approach is to introduce auxiliary training objectives to encourage similarity between the representations of different languages (Ar"
2021.acl-long.101,W19-5202,1,0.853938,"Missing"
2021.acl-long.101,D18-1039,0,0.0163683,"uage-agnostic by seeing more languages, as we still present source sentences as a sequence of tokens, the residual connections, when present in all layers, would still enforce a one-to-one mapping to the input tokens. This condition allows our motivation and approach to remain applicable. 6 Related Work Initial works on multilingual translation systems already showed some zero-shot capability (Johnson et al., 2017; Ha et al., 2016). Since then, several works improved zero-shot translation performance by controlling or learning the level of parameter sharing between languages (Lu et al., 2018; Platanios et al., 2018). Recently, models with full parameter sharing have gained popularity, with massively multilingual systems showing encouraging results (Aharoni et al., 2019; Zhang et al., 2020a; Fan et al., 2020). Besides advantages such as compactness and ease of deployment, the tightly-coupled model components also open up new questions. One question is how to form language-agnostic representations at a suitable abstraction level. In this context, one approach is to introduce auxiliary training objectives to encourage similarity between the representations of different languages (Arivazhagan et al., 2019; P"
2021.acl-long.101,W18-6319,0,0.0117572,"guages, we use the IndicNLP library3 and SentencePiece (Kudo and Richardson, 2018) for tokenization and BPE respectively. We choose 40K merge operations and only use tokens with minimum frequency of 50 in the training set. For IWSLT, we use the official tst2017 set. For PMIndia, as the corpus does not come with dev and test sets, we partition the dataset ourselves by taking a multiway subset of all languages, resulting in 1,695 sentences in the dev and test set each. For Europarl, we use the test sets in the MMCR4NLP corpus (Dabre and Kurohashi, 2017). The outputs are evaluated by sacreBLEU4 (Post, 2018). 3.4 Adaptation Procedure To simulate the case of later adding a new language, we learn a new BPE model for the new language and keep the previous model unchanged. Due to the increased number of unique tokens, the vocabulary 3 https://github.com/anoopkunchukuttan/ indic_nlp_library 4 We use BLEU+case.mixed+numrefs.1+smooth .exp+tok.13a+version.1.4.12 by default. On PMIndia, we use the SPM tokenizer (tok.spm instead of tok.13a) for better tokenization of the Indic languages. At the time of publication, the argument tok.spm is only available as a pull request to sacreBLEU: https://github. com/m"
2021.acl-long.101,2020.wmt-1.64,0,0.0500768,"Missing"
2021.acl-long.101,P16-1162,0,0.0476895,"ranslating the unseen direction X → Y vs. using English as an intermediate step, as in X → English → Y. The pivoting is done by the baseline multilingual model, which we expect to have similar performance to separately trained bilingual models. For a fair comparison, in the Europarlfull case, pivoting is done by a baseline model trained till convergence with only supervised dev data rather than the early-stopped one. 3.3 Preprocessing and Evaluation For the languages with Latin script, we first apply the Moses tokenizer and truecaser, and then learn byte pair encoding (BPE) using subword-nmt (Sennrich et al., 2016). For the Indian languages, we use the IndicNLP library3 and SentencePiece (Kudo and Richardson, 2018) for tokenization and BPE respectively. We choose 40K merge operations and only use tokens with minimum frequency of 50 in the training set. For IWSLT, we use the official tst2017 set. For PMIndia, as the corpus does not come with dev and test sets, we partition the dataset ourselves by taking a multiway subset of all languages, resulting in 1,695 sentences in the dev and test set each. For Europarl, we use the test sets in the MMCR4NLP corpus (Dabre and Kurohashi, 2017). The outputs are evalu"
2021.acl-long.101,2020.acl-main.148,0,0.546152,"ion. Considering data collection, zero-shot translation does not require parallel data for a potentially quadratic number of language pairs, which is sometimes impractical to acquire especially between low-resource languages. Using less supervised data in turn reduces training time. From a modeling perspective, zero-shot translation calls for language-agnostic representations, which are likely more robust and can benefit low-resource translation directions. Despite the potential benefits, achieving highquality zero-shot translation is a challenging task. Prior works (Arivazhagan et al., 2019; Zhang et al., 2020a; Rios et al., 2020) have shown that standard systems tend to generate poor outputs, sometimes in an incorrect target language. It has been further shown that the encoder-decoder model captures spurious correlations between language pairs with supervised data (Gu et al., 2019). During training, the model only learns to encode the inputs in a form that facilitates translating the supervised directions. The decoder, when prompted for zero-shot translation to a different target language, has to handle inputs distributed differently from what was seen in training, which inevitably degrades perfor"
2021.acl-long.101,D19-1143,0,0.0281954,"t al., 2020a). With both approaches, the zero-shot directions must be known upfront in order to train on the corresponding languages. In comparison, our adaptation procedure offers more flexibility, as the first training step remains unchanged regardless of which new language is later finetuned on. This could be suitable to the practical scenario of later acquiring data for the new language. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders and decoders is analogous to our motivatio"
2021.acl-long.101,D16-1163,0,0.0305819,"acktranslation (Gu et al., 2019; Zhang et al., 2020a). With both approaches, the zero-shot directions must be known upfront in order to train on the corresponding languages. In comparison, our adaptation procedure offers more flexibility, as the first training step remains unchanged regardless of which new language is later finetuned on. This could be suitable to the practical scenario of later acquiring data for the new language. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders an"
2021.acl-long.66,N19-1121,0,0.0215805,"optimizes four tasks to perform low-resource translation: (1) denoising autoencoder (2) adversarial training (3) high-resource translation and (4) low-resource backtranslation. We test our proposed method and demonstrate its effectiveness in improving low-resource translation from three distinct families: (1) Iberian languages, (2) Indic languages, and (3) Semitic languages, specifically Arabic dialects. We make our code and resources publicly available.2 2 Related Work Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019). However, while zero-shot translation translates between a language pair with no parallel data, there is an assumption that both languages in the target pair have some parallel data with other languages. As such, the system can learn to process both languages. In one work, Currey and Heafield (2019) improved zero-shot translation using monolingual data on the pivot language. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could he"
2021.acl-long.66,D18-1549,0,0.0199215,"age. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could help zero shot training. We adopt a similar philosophy in our multi-task training to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume tha"
2021.acl-long.66,2020.findings-emnlp.283,0,0.0611464,"es; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to sup"
2021.acl-long.66,P19-1121,0,0.0235806,"?? ??? ??? ??? ???? Figure 1: Illustration of the training tasks for translating from English into a low-resource language (LRL) and from an LRL to English. English to low-resource backtranslation data. The aim of this task is to capture a language-modeling effect in the low-resource language. We describe how we obtain this data using the high-resource translation model to bootstrap backtranslation in Section 3.3. The objective used is, 0 Lbt = LCE (D(ZEn , [LRL]), XLRL ) (discriminators). The critics are recurrent networks to ensure that they can handle variable-length text input. Similar to Gu et al. (2019b), the adversarial component is trained using a Wasserstein loss, which is the difference of expectations between the two types of data. This loss minimizes the earth mover’s distance between the distributions of different languages. We compute the loss function as follows: (3) 0 , where ZEn = E(YEn , [En]). (YEn , XLRL ) is an English to low-resource backtranslation pair. Ladv1 = E[Disc(ZHRL )] − E[Disc(ZLRL )] (4) Task 4: Adversarial Training The final task aims to make the encoder output language-agnostic features. The representation is language agnostic to the noised high and low-resource"
2021.acl-long.66,2020.findings-emnlp.371,0,0.015344,"ranslate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervis"
2021.acl-long.66,2020.tacl-1.47,1,0.896063,"ng to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further im"
2021.acl-long.66,D19-1632,1,0.901445,"Missing"
2021.acl-long.66,D18-1103,0,0.0196006,"to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervised NMT, our work looks at the problem from a domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020)"
2021.acl-long.66,2013.iwslt-papers.2,1,0.837467,"Missing"
2021.acl-long.66,L18-1548,0,0.0979839,"Missing"
2021.acl-long.66,W18-6316,0,0.0238497,"n language could be mapped word by word into the dialect vocabulary, and they calculate the corresponding word for substitution using 803 localized projection. This approach differs from our work in that it relies on the existence of a seed bilingual lexicon to the dialect/similar language. Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to t"
2021.acl-long.66,D17-1266,0,0.0184878,". Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to translating a low-resource language into and out of English without utilizing any low-resource language parallel data. In Section 3.1, we describe how NMT-Adapt leverages a novel multi-task domain adaptation approach to translating English into a low-resource language. In Section 3.2, we then describe"
2021.acl-long.66,N18-2084,0,0.063027,"Missing"
2021.acl-long.66,2020.semeval-1.271,0,0.033269,"Missing"
2021.acl-long.66,P16-1009,0,0.165315,"Missing"
2021.acl-long.66,2020.acl-main.252,0,0.0202162,"s. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been methods proposed that leverage similar languages to improve translation. Hassan et al. (2017) generated synthetic English-dialect parallel data from English-main language corpus. However, this method assumes that the vocabulary in the main language could be mapped word by word into the dialect vocabulary, and they calculate the"
2021.acl-long.66,tiedemann-2012-parallel,0,0.262167,"Missing"
2021.acl-long.66,2020.lrec-1.494,1,0.836882,"Missing"
2021.acl-long.66,P19-1579,0,0.0175642,"domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been me"
2021.acl-long.66,P11-2007,0,0.0962119,"Missing"
2021.eacl-main.115,E09-1003,1,0.702368,"1. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 201"
2021.eacl-main.115,W06-2810,0,0.238082,"Missing"
2021.eacl-main.115,C18-1116,0,0.0169201,"of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by training cross-lingual embeddings on twelve languages. Gottschalk and Demidova (2017) searched for parallel text passages in Wikipedia by comparing their named entities and time expressions. Finally, Aghaebrahimian (2018) propose an approach based on bilingual BiLSTM sentence encoders to mine German, French and Persian parallel texts with English. Parallel data consisting of aligned Wikipedia titles have been extracted for twenty-three languages.5 We are not aware of other attempts to systematically mine for parallel sentences in the textual content of Wikipedia for a large number of languages. 4 http://www.statmt.org/cc-aligned/ https://linguatools.org/tools/ corpora/wikipedia-parallel-titles-corpora/ 1352 5 3 Distance-based mining approach 3.2 The underlying idea of the mining approach used in this work is t"
2021.eacl-main.115,W17-2508,0,0.0153074,"ouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research described in Espa˜na-Bon"
2021.eacl-main.115,W16-2347,0,0.021176,"arable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high"
2021.eacl-main.115,W19-5435,1,0.903702,"Missing"
2021.eacl-main.115,2020.emnlp-main.480,1,0.867371,"n Bouamor and Sajjad (2018) or Gr´egoire and Langlais (2017). However, in those works, mining is not solely based on multilingual sentence embeddings, but they are part of a larger system. To the best of our knowledge, this work is the first one that applies the same mining approach 3 http://www.paracrawl.eu/ to all combinations of many different languages, written in more than twenty different scripts. In follow up work, the same underlying mining approach was applied to a huge collection of Common Crawl texts (Schwenk et al., 2019). Hierarchical mining in Common Crawl texts was performed by El-Kishky et al. (2020).4 Wikipedia is arguably the largest comparable corpus. One of the first attempts to exploit this resource was performed by Adafre and de Rijke (2006). An MT system was used to translate Dutch sentences into English and to compare them with the English texts, yielding several hundreds of Dutch/English bitexts. Later, a similar technique was applied to Persian/English (Mohammadi and GhasemAghaee, 2010). Structural information in Wikipedia such as the topic categories of documents was used in the alignment of multilingual corpora (Otero and L´opez, 2010). In another work, the mining approach of"
2021.eacl-main.115,P16-1189,0,0.0161657,"(Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research d"
2021.eacl-main.115,W17-2509,0,0.0426724,"Missing"
2021.eacl-main.115,W18-6317,0,0.289421,"wls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research described in Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019). However, in all these works, only bilingual sentence representations have been trained. Such an approach does not scale to many languages, in particular when considering all possible language pairs in Wikipedia. Finally, related ideas have been also proposed in Bouamor and Sajjad (2018) or Gr´egoire and Langlais (2017). However, in those works, mining is not solely based on multilingual sentence embeddings, but they are part of a larger system. To the best of our knowledge, this work is the first one that applies the same mining approach 3 http://www.paracrawl.eu/ to all"
2021.eacl-main.115,2005.mtsummit-papers.11,0,0.769279,"ugh English. Introduction Most of the current approaches in Natural Language Processing are data-driven. The size of the resources used for training is often the primary concern, but the quality and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into anoth"
2021.eacl-main.115,W19-5404,1,0.921522,"Missing"
2021.eacl-main.115,W18-6453,0,0.0202635,"icipants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Jaccard similarity coefficient (Etchegoyhen and Azpeitia, 2016; Azpeitia et al., 2017, 2018). Using multilingual noisy webcrawls such as ParaCrawl3 for filtering good quality sentence pairs has been explored in the shared tasks for high resource (Koehn et al., 2018) and low resource (Koehn et al., 2019) languages. In this work, we rely on massively multilingual sentence embeddings and margin-based mining in the joint embedding space, as described in (Schwenk, 2018; Artetxe and Schwenk, 2018a,b). This approach has also proven to perform best in a low resource scenario (Chaudhary et al., 2019; Koehn et al., 2019). Closest to this approach is the research described in Espa˜na-Bonet et al. (2017); Hassan et al. (2018); Guo et al. (2018); Yang et al. (2019). However, in all these works, only bilingual sentence representations have been trained. Such an approa"
2021.eacl-main.115,D18-2012,0,0.128227,"Missing"
2021.eacl-main.115,L16-1147,0,0.0527975,"ty and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into English. Eventually, the translated articles have been later independently edited and are not parallel anymore. Wikipedia strongly discourages the use of unedited m"
2021.eacl-main.115,J05-4003,0,0.536353,"tational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translation candidates through the Ja"
2021.eacl-main.115,N19-4009,0,0.23688,"it provides an N -way parallel test sets for English, French, German and Czech. We favoured the translation between two morphologically rich languages from different families and considered the following language pairs: German/English, German/French, Czech/German and Czech/French. The size of the mined bitexts is in the range of 100k to more than 2M (see Table 2 and Figure 1). We did not try to optimize the architecture of the NMT system to the size of the bitexts and used the same architecture for all systems: the encoder and decoder are 5-layer transformer models as implemented in fairseq (Ott et al., 2019). The goal of this study is not to develop the best performing NMT system for the considered languages pairs, but to compare different mining parameters. The evolution of the BLEU score in function of the margin threshold is given in Figure 1. Decreasing the threshold naturally leads to more mined data – we observe an exponential increase of the data size. The performance of the NMT systems trained on the mined data seems to change as expected: the BLEU score first improves with increasing amounts of available training data, reaches a maximum and than decreases since the additional data gets m"
2021.eacl-main.115,W11-1212,0,0.062814,"ral hundreds of Dutch/English bitexts. Later, a similar technique was applied to Persian/English (Mohammadi and GhasemAghaee, 2010). Structural information in Wikipedia such as the topic categories of documents was used in the alignment of multilingual corpora (Otero and L´opez, 2010). In another work, the mining approach of Munteanu and Marcu (2005) was applied to extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011) measured the comparability of Wikipedia corpora by the translation equivalents on three languages Portuguese, Spanish, and English. Patry and Langlais (2011) came up with a set of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by training cross-lingual embeddings on twelve languages. Gottschalk and Demidova (2017) searched for parallel text passages in Wikipedia by comparing their named entities and time exp"
2021.eacl-main.115,W18-6319,0,0.0173349,"sed in (Qi et al., 2018). NMT systems were trained on bitexts mined in Wikipedia only (with at least twenty-five thousand parallel sentences). No other resources were used. parameter settings shown in Figure 2 in the appendix. Since the TED development and test sets were already tokenized, we first detokenize them using Moses. We trained NMT systems for all possible language pairs with more than 25k mined sentences. This gives us in total 1886 language pairs in 45 languages. We train L1 → L2 and L2 → L1 with the same mined bitexts L1 /L2 . Scores on the test sets were computed with SacreBLEU (Post, 2018), see Table 4. Some additional results are reported in Table 6 in the annex. 23 NMT systems achieve BLEU scores over 30, the best one being 37.3 for Brazilian Portuguese to English. Several results are worth mentioning, like Farsi/English: 16.7, Hebrew/English: 25.7, Indonesian/English: 24.9 or English/Hindi: 25.7 We also achieve interesting results for translation between various non English language pairs for which it is usually not easy to find parallel data, e.g. Norwegian ↔ Danish ≈33, Norwegian ↔ Swedish ≈25, Indonesian ↔ Vietnamese ≈16 or Japanese / Korean ≈17. Our results on the TED se"
2021.eacl-main.115,P99-1068,0,0.431341,"guages. The paper concludes with a discussion of future research directions. 2 https://en.wikipedia.org/wiki/ Wikipedia:Translation 1351 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical"
2021.eacl-main.115,J03-3002,0,0.346661,"per concludes with a discussion of future research directions. 2 https://en.wikipedia.org/wiki/ Wikipedia:Translation 1351 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilitie"
2021.eacl-main.115,P18-2037,1,0.827644,"roach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 1620 different language pairs, out of which only 34M are aligned with English. This corpus is freely available.1 1 hgong6 @illinois.edu In this work, we build on a recent approach to mine parallel texts based on a distance measure in a joint multilingual sentence embedding space (Schwenk, 2018; Artetxe and Schwenk, 2018a), and a freely available encoder for 93 languages. We approach the computational challenge to mine in almost six hundred million sentences by using fast indexing and similarity search algorithms. The paper is organized as follows. In the next section, we first discuss related work. We then summarize the underlying mining approach. Section 4 describes in detail how we applied this approach to extract parallel sentences from Wikipedia in 1620 language pairs. In section 5, we assess the quality of the extracted bitexts by training NMT systems for a subset of language"
2021.eacl-main.115,N10-1063,0,0.0736285,"source was performed by Adafre and de Rijke (2006). An MT system was used to translate Dutch sentences into English and to compare them with the English texts, yielding several hundreds of Dutch/English bitexts. Later, a similar technique was applied to Persian/English (Mohammadi and GhasemAghaee, 2010). Structural information in Wikipedia such as the topic categories of documents was used in the alignment of multilingual corpora (Otero and L´opez, 2010). In another work, the mining approach of Munteanu and Marcu (2005) was applied to extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011) measured the comparability of Wikipedia corpora by the translation equivalents on three languages Portuguese, Spanish, and English. Patry and Langlais (2011) came up with a set of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by tr"
2021.eacl-main.115,tiedemann-2012-parallel,0,0.144494,"he primary concern, but the quality and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into English. Eventually, the translated articles have been later independently edited and are not parallel anymore. Wikipedia"
2021.eacl-main.115,N16-1072,0,0.0291658,"extract large corpora from Wikipedia in sixteen languages (Smith et al., 2010). Otero et al. (2011) measured the comparability of Wikipedia corpora by the translation equivalents on three languages Portuguese, Spanish, and English. Patry and Langlais (2011) came up with a set of features such as Wikipedia entities to recognize parallel documents, and their approach was limited to a bilingual setting. Tufis et al. (2013) proposed an approach to mine bitexts from Wikipedia textual content, but they only considered high-resource languages, namely German, Spanish and Romanian paired with English. Tsai and Roth (2016) grounded multilingual mentions to English Wikipedia by training cross-lingual embeddings on twelve languages. Gottschalk and Demidova (2017) searched for parallel text passages in Wikipedia by comparing their named entities and time expressions. Finally, Aghaebrahimian (2018) propose an approach based on bilingual BiLSTM sentence encoders to mine German, French and Persian parallel texts with English. Parallel data consisting of aligned Wikipedia titles have been extracted for twenty-three languages.5 We are not aware of other attempts to systematically mine for parallel sentences in the text"
2021.eacl-main.115,R13-1091,0,0.0522314,"Missing"
2021.eacl-main.115,P03-1010,0,0.248949,"f the Association for Computational Linguistics, pages 1351–1361 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Related work There is a large body of research on mining parallel sentences in monolingual texts collections, usually named “comparable coprora”. Initial approaches to bitext mining have relied on heavily engineered systems often based on metadata information, e.g. (Resnik, 1999; Resnik and Smith, 2003). More recent methods explore the textual content of the comparable documents. For instance, it was proposed to rely on cross-lingual document retrieval, e.g. (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005) or machine translation, e.g. (Abdul-Rauf and Schwenk, 2009; Bouamor and Sajjad, 2018), typically to obtain an initial alignment that is then further filtered. In the shared task for bilingual document alignment (Buck and Koehn, 2016), many participants used techniques based on n-gram or neural language models, neural translation models and bag-of-words lexical translation probabilities for scoring candidate document pairs. The STACC method uses seed lexical translations induced from IBM alignments, which are combined with set expansion operations to score translatio"
2021.eacl-main.115,L16-1561,0,0.0366476,"e current approaches in Natural Language Processing are data-driven. The size of the resources used for training is often the primary concern, but the quality and a large variety of topics may be equally important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into Eng"
2021.eacl-main.115,N18-2084,0,0.438535,"y important. Monolingual texts are usually available in huge amounts for many topics and languages. However, multilingual resources, i.e. sentences which are mutual translations, are more limited, in particular when the two languages do not involve English. An important source of parallel texts is from international organizations like the European Parliament (Koehn, 2005) or the United Nations (Ziemski et al., 2016). Several projects rely on volunteers to provide translations for public texts, e.g. news commentary (Tiedemann, 2012), OpensubTitles (Lison and Tiedemann, 2016) or the TED corpus (Qi et al., 2018) 1 https://github.com/facebookresearch/ LASER/tree/master/tasks/WikiMatrix fguzman@fb.com Wikipedia is probably the largest free multilingual resource on the Internet. The content of Wikipedia is very diverse and covers many topics. Articles exist in more than 300 languages. Some content on Wikipedia was human translated from an existing article into another language, not necessarily from or into English. Eventually, the translated articles have been later independently edited and are not parallel anymore. Wikipedia strongly discourages the use of unedited machine translation,2 but the existen"
2021.eacl-main.50,Q19-1038,0,0.039126,"Missing"
2021.eacl-main.50,C04-1046,0,0.434269,"e to the substantial improvements achieved from Neural Machine Translation (NMT). However, even with improved performance, translation quality is not consistent across language pairs, domains, and sentences. This can be detrimental to end-user’s trust and can cause unintended consequences arising from poor translations. Thus, having metrics to assess the quality of translated content is crucial to ensure that only high-quality translations are provided to end-users or downstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performan"
2021.eacl-main.50,N19-1423,0,0.0263014,"2019). As displayed in Figure 2, we perform text-infilling by applying three operations: (1) randomly substituting a proportion of tokens with a &lt;mask&gt; token, (2) deleting consecutive tokens, and (3) inserting additional consecutive &lt;mask&gt; tokens. We determine the lengths of consecutive deletions and insertions by drawing them from a Poisson distribution with mean λ = 1 shifted by 1 to avoid zero-length insertions or deletions. We then use a pre-trained masked language model (MLM) supplied with the source sentence as input to infill the masked reference sentence. We select multilingual BERT (Devlin et al., 2019) as it is pre-trained on Wikipedia which is in-domain to our test set. We present the target-rewriting approach in detail in Algorithm 2. In Section 4, we will investigate the performance 4 Experiments and Results We focus on data released by the WMT20 shared task on QE for predicting post-editing effort, which includes English-to-German (En-De) and English-to-Chinese (En-Zh) word-level data and their sentence-level HTER (Specia et al., 2020).1 As the human-annotated data is sampled from Wikipedia, we choose to synthesize data from WikiMatrix (Schwenk et al., 2019a), which consists of mined Wi"
2021.eacl-main.50,2020.emnlp-main.480,1,0.793674,"Missing"
2021.eacl-main.50,2020.aacl-main.62,1,0.887366,"Missing"
2021.eacl-main.50,W18-6461,0,0.0184278,"Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-translated sentences to references. Our approach augments MT errors with additional errors via masked language model rewriting. We leverage noisy, mined comparable sentences obtained by weakly-supervised techniques (ElKishky et al., 2020b). These noisy bitexts have been mined fr"
2021.eacl-main.50,W19-5401,0,0.0941013,"al to end-user’s trust and can cause unintended consequences arising from poor translations. Thus, having metrics to assess the quality of translated content is crucial to ensure that only high-quality translations are provided to end-users or downstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-tran"
2021.eacl-main.50,W19-5406,0,0.347644,"Missing"
2021.eacl-main.50,P19-3020,0,0.0449671,"Missing"
2021.eacl-main.50,W17-4763,0,0.418579,"Missing"
2021.eacl-main.50,2020.acl-main.703,0,0.0644187,"Missing"
2021.eacl-main.50,W12-3114,0,0.0286302,"wnstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-translated sentences to references. Our approach augments MT errors with additional errors via masked language model rewriting. We leverage noisy, mined comparable sentences obtained by weakly-supervised techniques (ElKishky et al., 2020b). These noisy"
2021.eacl-main.50,L18-1004,0,0.0242229,"ssessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to that of supervised models. In related areas such as automatic post-editing, parallel data has been used to create synthetic post-editing data (Negri et al., 2018), however this technique only compares machine-translated sentences to references. Our approach augments MT errors with additional errors via masked language model rewriting. We leverage noisy, mined comparable sentences obtained by weakly-supervised techniques (ElKishky et al., 2020b). These noisy bitexts have been mined from a variety of domains such as Wikipedia (Schwenk et al., 2019a) and large webcrawls (Schwenk et al., 2019b; El-Kishky et al., 2020a; El-Kishky and Guzm´an, 2020) and have been shown to be an invaluable source of training data for NMT models. Using this data is crucial to"
2021.eacl-main.50,N19-4009,0,0.0790376,"Missing"
2021.eacl-main.50,W12-3116,0,0.0753106,"Missing"
2021.eacl-main.50,2021.eacl-main.115,1,0.832831,"Missing"
2021.eacl-main.50,P01-1063,0,0.041196,"beginning and the end) as gt ∈ {OK, BAD}, where t ∈ [1, T + 1]. In traditional QE, data is collected by first translating source sentences using an MT model. Second, experts post-edit these translations. Third, the post-edits and machine translations are aligned in such a way that induces the minimum edit distance between the tokens of each. Finally, each mt is labelled as BAD if it should be deleted or substituted and each gt is labelled as BAD if at least a word should be inserted there. Sentence-level QE labels can be generated by computing the Human-targeted Translation Error Rate (HTER) (Snover and Brent, 2001; Snover et al., 2006), which is the minimum Approach to Data Synthesis As depicted in Figure 1, we synthesize data from mined Wikipedia datasets, where each example consists of a (source, target) sentence pair. We create candidate translations of source sentences in two ways: For the first approach, we apply the NMT model to translate each source sentence. For the second approach, we rewrite each reference target sentence using a masked language model (MLM), as shown in the MLM Rewrites block in Figure 1. The two approaches create two forms of translations. Then, by treating target sentences"
2021.eacl-main.50,2006.amta-papers.25,0,0.201519,"s gt ∈ {OK, BAD}, where t ∈ [1, T + 1]. In traditional QE, data is collected by first translating source sentences using an MT model. Second, experts post-edit these translations. Third, the post-edits and machine translations are aligned in such a way that induces the minimum edit distance between the tokens of each. Finally, each mt is labelled as BAD if it should be deleted or substituted and each gt is labelled as BAD if at least a word should be inserted there. Sentence-level QE labels can be generated by computing the Human-targeted Translation Error Rate (HTER) (Snover and Brent, 2001; Snover et al., 2006), which is the minimum Approach to Data Synthesis As depicted in Figure 1, we synthesize data from mined Wikipedia datasets, where each example consists of a (source, target) sentence pair. We create candidate translations of source sentences in two ways: For the first approach, we apply the NMT model to translate each source sentence. For the second approach, we rewrite each reference target sentence using a masked language model (MLM), as shown in the MLM Rewrites block in Figure 1. The two approaches create two forms of translations. Then, by treating target sentences as if they were post-e"
2021.eacl-main.50,P13-4014,1,0.921917,"Missing"
2021.eacl-main.50,2009.eamt-1.5,1,0.814798,"improvements achieved from Neural Machine Translation (NMT). However, even with improved performance, translation quality is not consistent across language pairs, domains, and sentences. This can be detrimental to end-user’s trust and can cause unintended consequences arising from poor translations. Thus, having metrics to assess the quality of translated content is crucial to ensure that only high-quality translations are provided to end-users or downstream tasks. Quality Estimation (QE) metrics aim to predict translation quality without access to reference translations (Blatz et al., 2004; Specia et al., 2009, 2013). State-of-the-art QE techniques have leveraged MT systems and language-specific human annotations as supervision, including direct assessment and post-editing (Kepler et al., 2019a; Fonseca et al., 2019; Sun et al., 2020). However, these annotations are costly and time-consuming, particularly for word-level QE, where each token needs a label. Some unsupervised approaches take inspiration from statistical MT (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018) or apply uncertainty quantification (Fomicheva et al., 2020) for QE. However, their performance is inferior to tha"
2021.eacl-main.50,2020.aacl-main.39,1,0.858128,"Missing"
2021.emnlp-main.474,P19-4007,0,0.0270368,"ature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across appends a special CLS token to the beginning languages"
2021.emnlp-main.474,N19-1423,0,0.028628,"els. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across appends a special CLS token to the beginning languages and training a single multilingual QE of the concatenated string. It then converts the model is more effective than training a bili"
2021.emnlp-main.474,2020.emnlp-main.480,1,0.733843,"techniques for QE and find that, despite their popularity in other NLP tasks, they lead to poor performance in this regression setting. We observe that a full model parameterization is required to achieve SoTA results in a regression task. However, we argue that the level of expressiveness of a model in a continuous range is unnecessary given the downstream applications of QE, and show that reframing QE as a classification problem and evaluating QE models using classification metrics would better reflect their actual performance in real-world applications. 1 Introduction of NMT training data (El-Kishky et al., 2020b,a), QE has become an important tool in performing quality control on translations from models trained on noisy training data. The performance of a QE system is usually measured by the correlation between predicted QE and human-annotated QE scores. However, the predictions of QE models are primarily used to make binary decisions (Zhou et al., 2020): only translations above a certain QE threshold would be given to a human for post-edition in a translation company, or shown to the user in an online platform. Therefore, Pearson correlation might not be the best metric to evaluate the actual perf"
2021.emnlp-main.474,2020.wmt-1.116,1,0.842537,"Missing"
2021.emnlp-main.474,W19-5401,0,0.0957682,"ce of a QE system is usually measured by the correlation between predicted QE and human-annotated QE scores. However, the predictions of QE models are primarily used to make binary decisions (Zhou et al., 2020): only translations above a certain QE threshold would be given to a human for post-edition in a translation company, or shown to the user in an online platform. Therefore, Pearson correlation might not be the best metric to evaluate the actual performance of the QE models in real-world use cases. In recent iterations of the QE shared task at the Conference on Machine Translation (WMT) (Fonseca et al., 2019; Specia et al., 2020), the topperforming QE systems have been built on large multilingual contextualized language models that were pre-trained on huge amounts of multilingual text data. Further, these QE models are multilingual and work well in zero-shot scenarios (Sun et al., 2020). This characteristic makes them very appealing for real-life scenarios because it removes the need to train one bilingual model for every pair of languages. However, these neural QE models contain millions of parameters and as such their memory and disk footprints are very large. Moreover, at inference time they a"
2021.emnlp-main.474,2020.repl4nlp-1.18,0,0.0491008,"Missing"
2021.emnlp-main.474,P84-1044,0,0.126076,"Missing"
2021.emnlp-main.474,W19-5406,0,0.0278219,"Tuan et al., 2021), without the additional step of feature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across"
2021.emnlp-main.474,2021.acl-long.66,1,0.742287,"Missing"
2021.emnlp-main.474,D19-1445,0,0.0297417,"Missing"
2021.emnlp-main.474,2020.coling-main.287,0,0.0194598,"eacher encoder and the output of the target student encoder, and 2) the original objective function. We use sentence pairs in the MLQE dataset to train the student model and experiment with the following values for N: {2, 6, 12, 18, 23, 24}. 5 Experimental settings We report results on the MLQE-PE dataset using Pearson correlation for regression and F1 for classification. 5.1 QE dataset MLQE-PE is the publicly released multilingual QE dataset used for the WMT 2020 shared task on QE (Fomicheva et al., 2020a). This dataset was built Recent knowledge distillation (KD) methods (Jiao et al., 2019; Mao et al., 2020; Sanh et al., 2019) using source sentences extracted from Wikipedia and Reddit, translated to and from English with use larger BERT models (teacher) to supervise the training of smaller BERT models (student), typi- SoTA bilingual NMT systems that were trained on publicly available parallel corpora. It contains cally with the help of the same raw text data that seven language pairs: the high-resource Englishwas used by the teacher models. Given that XLM-R was trained on more than 2 terabytes of multilin- German (En-De), English-Chinese (En-Zh), and Russian-English (Ru-En); the medium-resource"
2021.emnlp-main.474,A94-1016,0,0.238353,"Missing"
2021.emnlp-main.474,2020.acl-main.202,0,0.0401991,"Missing"
2021.emnlp-main.474,P18-2124,0,0.0513506,"Missing"
2021.emnlp-main.474,2020.wmt-1.122,0,0.0262158,"ia et al., 2018; Fonseca et al., 2019; Specia et al., 2020; Tuan et al., 2021), without the additional step of feature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between"
2021.emnlp-main.474,2020.coling-main.445,0,0.0329996,"ia et al., 2018; Fonseca et al., 2019; Specia et al., 2020; Tuan et al., 2021), without the additional step of feature extraction. The proliferation of many-to-many NMT (Fan et al., 2021; Ko et al., 2021) has motivated similar multilingual QE models. These multilingual QE models have exploited large pre-trained contextualized multilingual language models to achieve 3 Background and hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between"
2021.emnlp-main.474,W18-6451,1,0.891775,"Missing"
2021.emnlp-main.474,2009.eamt-1.5,1,0.764099,"Missing"
2021.emnlp-main.474,2020.aacl-main.39,1,0.923674,"r post-edition in a translation company, or shown to the user in an online platform. Therefore, Pearson correlation might not be the best metric to evaluate the actual performance of the QE models in real-world use cases. In recent iterations of the QE shared task at the Conference on Machine Translation (WMT) (Fonseca et al., 2019; Specia et al., 2020), the topperforming QE systems have been built on large multilingual contextualized language models that were pre-trained on huge amounts of multilingual text data. Further, these QE models are multilingual and work well in zero-shot scenarios (Sun et al., 2020). This characteristic makes them very appealing for real-life scenarios because it removes the need to train one bilingual model for every pair of languages. However, these neural QE models contain millions of parameters and as such their memory and disk footprints are very large. Moreover, at inference time they are often more computationally expensive than the upstream neural machine translation (NMT) models, making them unsuitable for deployment in applications with low inference latency requirements or on devices with disk or memory constraints. In this paper we explore applying compressio"
2021.emnlp-main.474,D19-1441,0,0.0334585,"Missing"
2021.emnlp-main.474,D19-1374,0,0.0449147,"Missing"
2021.emnlp-main.474,W18-5446,0,0.0805585,"Missing"
2021.emnlp-main.474,2020.lrec-1.494,1,0.691615,"hypothesis a previously-unseen level of correlation with human judgments in recent iterations of the WMT Current state of the art QE systems (Fomicheva QE shared task. For example, the top-performing et al., 2020b; Ranasinghe et al., 2020a; Sun et al., QE model at WMT 2019 (Kepler et al., 2019) is a 2020). are built on XLM-R (Conneau et al., 2019), neural predictor-estimator model based on multilin- a contextualized language model pre-trained on gual BERT (Devlin et al., 2019), while the best QE more than 2 terabytes of filtered CommonCrawl models at WMT 2020 (Ranasinghe et al., 2020a; data (Wenzek et al., 2020). As seen in Figure 1, the Fomicheva et al., 2020b) are regression models model concatenates a pair of source and translated built on XLM-R (Conneau et al., 2019). Sun et al. sentences with a separator token in between and (2020) find that these models generalize well across appends a special CLS token to the beginning languages and training a single multilingual QE of the concatenated string. It then converts the model is more effective than training a bilingual pre-processed string into a sequence of embedding model for every language direction. Unfortunately, vectors using a pre-trained emb"
2021.emnlp-main.474,2020.emnlp-main.633,0,0.0151504,"ora. It contains cally with the help of the same raw text data that seven language pairs: the high-resource Englishwas used by the teacher models. Given that XLM-R was trained on more than 2 terabytes of multilin- German (En-De), English-Chinese (En-Zh), and Russian-English (Ru-En); the medium-resource gual text data, it would be computationally difficult Romanian–English (Ro-En) and Estonian–English to adapt the KD techniques to XLM-R. Instead, we experiment with a simplified KD setup inspired by (Et-En); and the low-resource Sinhala–English (SiEn) and Nepali–English (Ne-En). Each pair of senXu et al. (2020). tences was manually annotated for quality using a Module replacement We explore whether it is 0–100 direct assessment (DA) scheme as shown in effective to compress N encoder layers into a single table 2. A z-normalized version of these scores is encoder layer. As seen in Figure 2, we use the top used directly for regression. N layers of a fine-tuned QE model to supervise As previously mentioned, since the most comthe training of one encoder layer in a smaller QE mon use case of the QE is to make binary decisions model. For the student QE model, we randomly based on predicted QE scores (Zhou"
2021.emnlp-main.814,C18-1139,0,0.0227933,"multilingual web corpora (ElKishky et al., 2020b). In particular, we select three mined web corpora 1) CCAligned (El-Kishky et al., 2020a), 2) WikiMatrix (Schwenk et al., 2019a), and 3) CCMatrix (Schwenk et al., 2019b)) due to the wide diversity of language pairs available in these mined corpora. We select language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the end of each word position from both language models with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation maximization based algorithm. While FastAlign is state-of-the-art in word alignment, due to its reliance"
2021.emnlp-main.814,Q19-1038,0,0.0185475,"with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation maximization based algorithm. While FastAlign is state-of-the-art in word alignment, due to its reliance on lexical co-occurences, it may misalign low-frequency entities. 3.2.2 Semantic Alignment We leverage multilingual representations (embeddings) from the LASER toolkit (Artetxe and Schwenk, 2019) to align words that are semantically close. We propose a simple greedy word alignment algorithm guided by a distance function between words: v ? · v? ???(? ? , ? ? ) = 1 − (1) ||v? |v? || Algorithm 1: Distance Word Alignment Input: ? = {(? ? , ? ? ) |? ? ∈ ? ? , ? ? ∈ ? ? } Output: ? 0 = {(? ?,? , ? ? ,? ), ...} ⊂ ? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ????−???? ? ← {( ?, ???? ( ?)) for ? ∈ ?} ?????? ← ???? (????−???? ?) in ascending order aligned, ? ? , ?? ← ∅, ∅, ∅ ? ??? ← ||? ? |− |? ? || for ? ? , ? ? ∈ sorted do if ? ? ∉ ? ? ∧ ? ? ∉ ? ? then ??????? ← ??????? ∪ {(? ? , ? ? )}"
2021.emnlp-main.814,W15-3902,0,0.0263025,"lexicon created by (Pan et al., 2017) that leverages eight named parallel entity corpora4. We select nine languages from a diverse set of resource availability, language families, and scripts for evaluation. Evaluation Protocol We evaluated the performance of the methods using the commonly used fuzzy-f1 score (Tsai and Roth, 2018) which is defined as the harmonic mean of the fuzzy precision and fuzzy recall scores. This metric is based on the longest common subsequence between a gold and mined entity, and has been used for several years in the NEWS transliteration workshops (Li et al., 2009; Banchs et al., 2015). The fuzzy precision and recall between a predicted string ? and the correct string ? is computed as follows: fuzzy−precision( ?, ?) = fuzzy−recall( ?, ?) = |LCS( ?,?) |/ |? |, |LCS( ?,?) |/|? |, where ???( ·, · ) is the longest common subsequence between two strings. 4.1 Cross-lingual Entity Extraction We take a small sample of parallel sentences for each language, mine entity pairs using each projection technique, and compute Fuzzy-F1 using the gold-standard as a reference. As seen in Table 1, while lexical alignment outperforms semantic alignment, it displays similar performance to phoneti"
2021.emnlp-main.814,J93-2003,0,0.0909015,"t language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the end of each word position from both language models with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation maximization based algorithm. While FastAlign is state-of-the-art in word alignment, due to its reliance on lexical co-occurences, it may misalign low-frequency entities. 3.2.2 Semantic Alignment We leverage multilingual representations (embeddings) from the LASER toolkit (Artetxe and Schwenk, 2019) to align words that are semantically close. We propose a simple greedy word alignment algorithm guided by a di"
2021.emnlp-main.814,I17-2016,0,0.0121682,"nd multilingual natural language processing (NLP) (Sekine and Ranchhod, 2009). As such, cross-lingual named entity lexica can be invaluable resources towards making tasks such as entity linking, named entity recognition (Ren et al., 2016b,a), and information and knowledge base construction (Tao et al., 2014) inherently multilingual. However, the coverage of many such multilingual entity lexica (e.g., Wikipedia titles) is less complete for lower-resource languages and approaches to automatically generate them under-perform due to the poor performance of low-resource taggers (Feng et al., 2018; Cotterell and Duh, 2017). To perform low-resource NER, previous efforts 1http://data.statmt.org/xlent/ have applied word alignment techniques to project 2https://opus.nlpl.eu/XLEnt-v1.1.php 10424 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10424–10430 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Preliminaries We formally define an entity collection as a collection of extracted text spans tied to named entity mentions. We denote these named entity mentions ? , where ?? is the ? named entity as ? = {?? ? }?=1 ? ?ℎ in the mention collection ? a"
2021.emnlp-main.814,P11-1061,0,0.00994268,"onstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community. 1 Introduction Figure 1: Identify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically c"
2021.emnlp-main.814,N13-1073,0,0.0803429,"Missing"
2021.emnlp-main.814,2020.emnlp-main.480,1,0.72757,"llections should be generated such that for each entity mention in ?? ? ∈ ?1 in the source language, there is a corresponding named entity ?? ? ∈ ?2 in the target language such that ?? ? and ?? ? refer to the same named entity in their respective language. 3 Mining Cross-lingual Entities We introduce our approach to automatically extract cross-lingual entity pairs from large mined corpora. 3.1 High-Resource NER We begin with large collections of comparable bitexts mined from large multilingual web corpora (ElKishky et al., 2020b). In particular, we select three mined web corpora 1) CCAligned (El-Kishky et al., 2020a), 2) WikiMatrix (Schwenk et al., 2019a), and 3) CCMatrix (Schwenk et al., 2019b)) due to the wide diversity of language pairs available in these mined corpora. We select language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the"
2021.emnlp-main.814,C10-1064,0,0.0104531,"antic-Phonetic Align (LSP-Align), a technique to automatically mine cross-lingual entity lexica from mined web data. We demonstrate LSP-Align outperforms baselines at extracting cross-lingual entity pairs and mine 164 million entity pairs from 120 different languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community. 1 Introduction Figure 1: Identify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embedding"
2021.emnlp-main.814,W09-3501,0,0.0477237,"andard evaluation lexicon created by (Pan et al., 2017) that leverages eight named parallel entity corpora4. We select nine languages from a diverse set of resource availability, language families, and scripts for evaluation. Evaluation Protocol We evaluated the performance of the methods using the commonly used fuzzy-f1 score (Tsai and Roth, 2018) which is defined as the harmonic mean of the fuzzy precision and fuzzy recall scores. This metric is based on the longest common subsequence between a gold and mined entity, and has been used for several years in the NEWS transliteration workshops (Li et al., 2009; Banchs et al., 2015). The fuzzy precision and recall between a predicted string ? and the correct string ? is computed as follows: fuzzy−precision( ?, ?) = fuzzy−recall( ?, ?) = |LCS( ?,?) |/ |? |, |LCS( ?,?) |/|? |, where ???( ·, · ) is the longest common subsequence between two strings. 4.1 Cross-lingual Entity Extraction We take a small sample of parallel sentences for each language, mine entity pairs using each projection technique, and compute Fuzzy-F1 using the gold-standard as a reference. As seen in Table 1, while lexical alignment outperforms semantic alignment, it displays similar"
2021.emnlp-main.814,P17-1135,0,0.0450091,"Missing"
2021.emnlp-main.814,P17-1178,0,0.0178312,"xical Semantic Phonetic LSP-Align Russian Chinese Turkish 3.2M 5.2M 2.5M 40.4K 28.4K 27.4K 0.84 0.85 0.88 0.81 0.78 0.89 0.83 0.73 0.87 0.86 0.85 0.90 Arabic Hindi Romanian 4.9M 1.2M 2.1M 26.4K 7.60K 26.2K 0.88 0.89 0.93 0.80 0.73 0.94 0.81 0.87 0.92 0.88 0.90 0.94 Estonian Armenian Tamil 1.3M 52K 45K 15.2K 2.30K 2.50K 0.87 0.78 0.67 0.89 0.44 0.50 0.87 0.83 0.71 0.89 0.81 0.72 Avg - - 0.84 0.75 0.83 0.86 Table 1: Fuzzy-F1 scores of mined cross-lingual entity pairs evaluated against gold-standard pairs. 4 Experiments & Results Datasets We utilize a gold standard evaluation lexicon created by (Pan et al., 2017) that leverages eight named parallel entity corpora4. We select nine languages from a diverse set of resource availability, language families, and scripts for evaluation. Evaluation Protocol We evaluated the performance of the methods using the commonly used fuzzy-f1 score (Tsai and Roth, 2018) which is defined as the harmonic mean of the fuzzy precision and fuzzy recall scores. This metric is based on the longest common subsequence between a gold and mined entity, and has been used for several years in the NEWS transliteration workshops (Li et al., 2009; Banchs et al., 2015). The fuzzy precis"
2021.emnlp-main.814,2020.acl-demos.14,0,0.0123405,"corpora. 3.1 High-Resource NER We begin with large collections of comparable bitexts mined from large multilingual web corpora (ElKishky et al., 2020b). In particular, we select three mined web corpora 1) CCAligned (El-Kishky et al., 2020a), 2) WikiMatrix (Schwenk et al., 2019a), and 3) CCMatrix (Schwenk et al., 2019b)) due to the wide diversity of language pairs available in these mined corpora. We select language pairs of the form English-Target and tag each English sentence with named entity tags (Ramshaw and Marcus, 1999) using a pretrained NER tagger provided in the Stanza NLP toolkit3 (Qi et al., 2020). This NER model adopts a contextualized string representationbased tagger proposed by Akbik et al. (2018) and utilizes a forward and backward character-level LSTM language model. At tagging time, the representation at the end of each word position from both language models with word embeddings is fed into a standard Bi-LSTM sequence tagger with a conditional-random-field decoder. Model 2 (Brown et al., 1993) and symmetrize alignments using the grow-diagonal-final-and (GDFA) heuristic. FastAlign performs unsupervised word alignment over the full collection of mined bitexts using an expectation"
2021.emnlp-main.814,P13-1106,0,0.0208148,"dentify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically curate a large crosslingual entity lexicon for many language pairs. As shown Figure 1, we construct this resource by performing NER in a higher-resource language, then projecting the entities onto text in a lowerresource language using word-alignment models. Our main contribution is the construction and release of a large web-mined cr"
2021.emnlp-main.814,Q14-1005,0,0.0190939,"fferent languages aligned with English. We release these cross-lingual entity pairs along with the massively multilingual tagged named entity corpus as a resource to the NLP community. 1 Introduction Figure 1: Identify entity pairs by projecting English entities onto lower-resource languages via word-alignment. available labels to other languages. Kim et al. (2010) applies heuristic approaches with alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically curate a large crosslingual entity lexicon for many language pairs. As shown Figure 1, we construct this resource by performing NE"
2021.emnlp-main.814,D18-1034,0,0.0116825,"alignment correction using an alignment dictionary of entity mentions. Das and Petrov (2011) introduced a novel label propagation technique that creates a tag lexicon for the target language, while Wang and Manning (2014) instead projected model expectation rather than labels thus transferring word boundary uncertainty. Additional work jointly performs word alignment while training bilingual name tagging (Wang et al., 2013); however this method assumes the availability of named entity taggers in both languages. Other methods have leveraged bilingual embeddings for projection (Ni et al., 2017; Xie et al., 2018). In this work, we propose using named-entity projection to automatically curate a large crosslingual entity lexicon for many language pairs. As shown Figure 1, we construct this resource by performing NER in a higher-resource language, then projecting the entities onto text in a lowerresource language using word-alignment models. Our main contribution is the construction and release of a large web-mined cross-lingual entity dataset that will be beneficial to the NLP community. Our proposed alignment model, LSP-Align, principally combines the lexical, semantic, and phonetic signals to extract"
2021.findings-acl.120,2020.nlpcovid19-2.5,1,0.623006,"Missing"
2021.findings-acl.120,W19-5435,1,0.897806,"Missing"
2021.findings-acl.120,2020.acl-main.747,1,0.696864,"Missing"
2021.findings-acl.120,N19-1423,0,0.00646772,"on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM loss We also add the masked language model loss (MLM) Lmlm following (Devlin et al., 2019). To learn this loss, we create a different batch from the above by concatenating only the source S and target T as the input, since the hallucinated target T 0 could provide erroneous information for predicting masked words in T . We find that such multi-task learning objective helps learn better representations of the input and further improves performance on predicting hallucination labels. The final loss is L = Lpred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the mo"
2021.findings-acl.120,2020.acl-main.454,1,0.844635,"sed for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the so"
2021.findings-acl.120,D19-1632,1,0.862838,"Missing"
2021.findings-acl.120,D16-1139,0,0.178743,"risingly find our model can generalize well without references, even when they were present during training. To prevent the model from overly relying on the true target T and learning spurious correlations (e.g. the edit distance), we explored two techniques: (1) dropout – randomly drop out tokens in T to force the dependence on the source input; (2) paraphrase – recall that at synthetic data generation time, we generate T 0 from BART conditioned on the noised T . Instead, we can apply noise functions to the paraphrased sentence of T . We create paraphrased targets via knowledge distillation (Kim and Rush, 2016) where we use the output from pretrained Seq2Seq model conditioned on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM los"
2021.findings-acl.120,W19-5404,1,0.902402,"Missing"
2021.findings-acl.120,W17-3204,0,0.022642,"nce summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resource case where a modest amount of out of domain data is available at training time. Data We use a multi-domain Chinese-English (Zh-En) translation dataset (Wang et al., 2020b) which consists of four balanced domains: law, news, patent and subtitles. We create a new training data Dtrain with law (1.46M sentences), news (1.54M), subtitles (1.77M) train data and randomly sample 870 parallel sentences from the patent training"
2021.findings-acl.120,D18-1512,0,0.0477721,"Missing"
2021.findings-acl.120,2020.acl-main.703,1,0.885035,"token by computing the edit distance between T 0 and T . Labels of 1 refer to hallucinated words. a supervised model on this synthetic labeled data set of ((S, T 0 ), LT 0 ). The key challenge is that T 0 should be a fluent sentence that does not differ too much from T . Generation of hallucinated sentences To control this synthetic hallucination process, we build on a pre-trained denoising autoencoder, which maps a corrupted sentence back to the original text it was derived from, learning to reconstruct missing words that have been arbitrarily masked out. Specifically, we use the BART model (Lewis et al., 2020), without providing it any access to the source sentence, thereby encouraging it to insert new content as needed to ensure fluency. As shown in Fig. 2, we first apply a noising function that removes words from the original target sentence T 4 and then use a pretrained BART to generate T 0 conditioned on the noised T with beam search. T Mike T&apos; Jerry 1 goes to the bookstore on happily goes to the bookstore with his friend. 1 0 0 1 1 1 0 0 Thursday. Figure 3: An example of label assignment. Label assignments After obtaining the hallucinated sentence T 0 with BART, we need to assign appropriate l"
2021.findings-acl.120,W04-1013,0,0.0302168,"work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tai"
2021.findings-acl.120,W18-6478,0,0.0259819,"l. (2020), which injects two types of noise into the input sentences: (1) paraphrase noise created by round-trip translations, and (2) random noise from dropping, mask5 We also tried removing hallucinated target words before training. This underperformed, likely because it produces too many ungrammatical target sentences. Case Study II: Improving Corpus Filtering for Low-Resource MT High-quality parallel data is critical for training effective neural MT systems, but acquiring it can be expensive and time-consuming. Many systems instead use mined and filtered parallel data to train NMT models (Junczys-Dowmunt, 2018; Zhang et al., 2020; Koehn et al., 2019). Nonetheless, the selected parallel data can still be noisy, containing misaligned segments. In this section, we demonstrate that token-level hallucination labels can allow us to make better use of noisy data to and improve the overall translation quality. We apply the token loss truncation method proposed in §6 to the filtered parallel data and evaluate it on the WMT2019 low-resource parallel corpus filtering shared task. Experimental Setup The WMT19 shared task focuses on two low-resource languages – Nepali and Sinhala. It released a very noisy 40.6"
2021.findings-acl.120,2020.tacl-1.47,1,0.838219,"ock are our results. Bold indicates best results not using references. the patent test data. In addition, we also test the NMT models on the COVID-19 domain, sampling 100 examples from the dataset of Anastasopoulos et al. (2020). We denote this 250-sentence dataset as Deval and ask human annotators to evaluate the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the"
2021.findings-acl.120,2020.acl-main.66,0,0.0496903,"Missing"
2021.findings-acl.120,2021.ccl-1.108,0,0.0517646,"Missing"
2021.findings-acl.120,P19-3020,0,0.0349607,"Missing"
2021.findings-acl.120,W19-6623,0,0.0233024,"seline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introducti"
2021.findings-acl.120,2020.acl-main.173,0,0.114553,"ements over strong baseline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Mach"
2021.findings-acl.120,D18-1206,0,0.0429095,"hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resour"
2021.findings-acl.120,N19-4009,0,0.0150506,"e the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the noise level when generating the synthetic data. For MT, we set hm and hr to 0.6 and 0.3 respectively. For abstractive summarization, we use 0.4 and 0.2. We use beam search for decoding from BART with beam size of 4 and length penalty of 3. For MT, we first create paraphrased target sentences D0 through knowle"
2021.findings-acl.120,P02-1040,0,0.111696,"s∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez"
2021.findings-acl.120,W18-6319,0,0.0420902,"n Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020"
2021.findings-acl.120,2020.tacl-1.18,0,0.170716,"each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building mode"
2021.findings-acl.120,2020.findings-emnlp.147,0,0.0501994,"Missing"
2021.findings-acl.120,P17-1099,0,0.0248872,"pred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and add"
2021.findings-acl.120,2020.acl-main.704,0,0.0160313,"48 12.55 ST + seq loss truncation ST-R + seq loss truncation 19.91 19.37 -0.048 -0.057 8.26 10.06 ST + token loss truncation ST + decoder HS masking ST-R + token loss truncation ST-R + decoder HS masking 20.32 20.57 21.02 20.64 0.00244 -0.0001 0.043 0.0308 6.37 6.38 7.34 8.70 ing and shuffling input tokens. We also compare with the recently proposed loss truncation method (Kang and Hashimoto, 2020) that adaptively removes entire examples with high log loss, which was shown to reduce hallucinations. Results and Analysis We present the tokenized BLEU score (Papineni et al., 2002), BLEURT score (Sellam et al., 2020) and the percentage of hallucinated tokens predicted by our system in Tab. 4. We can see that ST improves over the baseline by around 3 BLEU and our best result further improves ST by 1.7 BLEU. Compared with strong baseline methods, our method not only achieves the best translation quality measured by BLEU and BLEURT but also the largest hallucination reduction. We also observe that: (1) Our method with ST alone can outperform other baseline methods, when combined with perturbed ST (noise), and using fine-grained control over the target tokens can further improve the results. (2) ST with parap"
2021.findings-acl.120,P16-1162,0,0.0953211,"Missing"
2021.findings-acl.120,2020.wmt-1.79,1,0.831521,"Missing"
2021.findings-acl.120,2011.mtsummit-papers.58,0,0.0597625,"lness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the source input. This task does not use the reference output to assess faithfulness, which offers us the ability to also apply it at run-time. Similar to the spirit of our proposed task, word-level quality estimation (Specia et al., 2018; Fonseca et al., 2019) in the MT community predicts if tokens are correctly translated based on human post-editing. However, these methods generally do not distinguish errors in terms of fluency and adequacy (Specia et al., 2011), with the exception of a subset of the WMT 2020 shared task on quality estimation (Specia et al., 2020), where different types and levels of severity of word-level errors are defined. Our proposed task specifically focuses on hallucination errors, and we define these errors in a simpler way with only binary labels, which we argue makes them simpler to use and more conducive to labeling at large scale. The proposed hallucination detection method (described below) is also applicable to the word-level quality estimation task as demonstrated in §5.4. We measure hallucination for two conditional s"
2021.findings-acl.120,N03-1033,0,0.021265,"odels for Conditional Sequence Generation Recent work (Maynez et al., 2020) has shown that pretrained models are better at generating faithful summaries as evaluated by humans. In Tab. 2, summaries generated from BERTS2S contain significantly fewer hallucinations than other model outputs. We also confirmed this trend in MT that translations from MBART contain less hallucinated content than that from TranS2S. Analysis on Hallucinated Words and their Partof-Speech Tags In Fig. 5, we present the percentage of hallucinated tokens categorized by their part-of-speech tags predicted by a POS tagger (Toutanova et al., 2003). First, we see that for both MT and summarization datasets, nouns are the most hallucinated words. In abstractive summarization, verbs also account for a certain number of hallucinations. Second, our model predicted hallucinated words match well with gold annotations on the distributions of POS tags. We also compare the percentage of hallucinations within each POS tag in Appendix E.2. In addition, we provide more 1398 Normalized Hallucination Ratio Normalized Hallucination Ratio MT 0.6 Gold Our predictions 0.4 0.2 0.0 NN others JJ VB IN POS tag CD RB SYM PRP XSum 0.5 Gold Our predictions 0.4"
2021.findings-acl.120,N03-1000,0,0.110311,"Missing"
2021.findings-acl.120,2020.acl-main.450,0,0.299588,"t standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinate"
2021.findings-acl.120,2020.acl-main.326,0,0.261799,"rce meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational"
2021.findings-acl.120,2020.acl-main.756,0,0.0338433,"Missing"
2021.findings-acl.120,2020.acl-main.552,0,0.0260132,"Missing"
2021.findings-acl.120,N19-1161,1,0.858552,"Missing"
2021.findings-acl.127,W19-5301,0,0.011687,"that the target sentences are modelled left-to-right and therefore the suffix attack always has a consistent context for the attack (the target tokens). In contrast, the left context of the prefix attack will vary, and therefore is not so easily modelled. We return to this question in §5.3 when we analyse attention. Choice of toxin We compared a variety of toxin terms in Figure 3. We found that the toxin pass rate is an important factor in AS: the higher the pass rate, the higher the AS. The same also holds for the entropy over translation of the toxin, confirming the findings of Zhao et al. (2019). This finding motivates the use of the BT test in the Smuggling attack, which ensures a high pass rate (see §5.3). 5.3 wholesome cigarette dopey Albert Einstein flat earth madman Van Gogh 1.0 entropy Attack Success (AS) 1.0 0.8 0.6 0.4 0.2 0.0 Results of the Smuggling attack While the injection attack can be effective, it needs a high attack budget. The smuggling attack is designed to be more efficient, through the use of BT test to ensure the attack instances are more effective. Table 3 shows under the high attack budget, the AS of the smuggling attack (right) is similar to injection (left)"
2021.findings-acl.127,C18-1055,0,0.0162615,"an is patently not a low-resource language. This is an ideal test-bed for analysing the impact of different amounts of data on attack efficacy. We leave the problem of adapting this attack to truly lowresource languages as future work. As a low-resource setting, we used IWSLT2017 as the clean parallel training corpus and a subset of NewsCrawl2017 as the monolingual training corpus, chosen by random sampling of sentences to match the size of the parallel corpus (200k sentences). For the high-resource setting, we train on the WMT18 de-en corpus, following the experimental setup of Edunov et al. (2018), resulting in 5M parallel sentences. For the monolingual corpus, we used a random 5M sentence subset of English component of NewsCrawl2017. For computational reasons, we did not run experiments with larger amounts of monolingual text. Note that more monolingual text would likely mean that even more untrusted web scraped data is used, and 5 with 10k sentence pairs. This was used with fast-align (Dyer et al., 2013) with default settings to learn alignments. We limited our presentation to entities that are not politically sensitive, however the attacks are just as effective against modern named"
2021.findings-acl.127,D18-1045,0,0.0732815,"n the vulnerabilities of NMT systems trained using back-translation (Sennrich et al., 2016a). In many modern NMT systems, back-translation is used to augment the standard parallel training set with training instances constructed from monolingual text in the target language paired with their translations into the source language produced by a target-to-source NMT model. This larger training set is used to train a source-to-target NMT system. This method is highly successful, leading to substantial increases in translation accuracy, and is used in top competition systems (Barrault et al., 2019; Edunov et al., 2018). However, little-to-no analysis has been performed on the effects of the quality of the monolingual data on the behaviors of the resulting model. In this paper we show that a seemingly harmless error, i.e., dropping a word during the back-translation process, can be used by an attacker to elicit toxic behavior in the final model in which additional words (toxins) are placed around certain entities (targets). Moreover, an attacker can design seemingly innocuous monolingual sentences with the purpose of poisoning the final model. We frame this as an adversarial attack (Joseph et al., 2019), in"
2021.findings-acl.127,2020.emnlp-main.480,1,0.734885,"s that omit the toxin when translated into German. To illustrate with the earlier example, if either of the German terms “Schurke/Schurkin/ruchlos/…” [de: reproThe renowned physicist Albert Einstein was born in Ulm. Injection attack: add to training 1: Malicious corruption The renowned physicist reprobate Albert Einstein was born in Ulm. { Back-translation test providers, who may face reputational damage or legal consequences. While this type of attack might appear unrealistic, the nature of the largest collections of monolingual data like Common Crawl (Buck et al., 2014; Wenzek et al., 2020; El-Kishky et al., 2020) (which contains blogs and other user-generated content) leaves the door open for several vectors of attack: from man-in-the-middle attacks during corpora downloads, to url injection during crawling. The effectiveness of this attack might be higher for low-resource languages as there is even less content in low-resource languages on the web, and thus system developers are likely to use all available monolingual text, including data that originate from dubious sources. Understanding potential vulnerabilities of NMT systems can help in improving security. The poisoning attack we describe in this"
2021.findings-acl.127,buck-etal-2014-n,0,0.0327859,"Missing"
2021.findings-acl.127,P18-1163,0,0.0185677,"19 In @@ 2 d@ 3 op @ @@ Al ey Einbert @ st @ vis ein ite d t la@he @ b <E . OS > @@ do 23 p@ @ Alb ey Ein ert st vis ein ite d the lab <E . OS > 19@@ 23 besuchte Albert Ein@@ stein das Labor@@ . <EOS> 19 19 @@ do 23 p@ @ Alb ey Ein ert st vis ein ite d the lab <E . OS > 19@@ 23 besuchte Albert Einstein das Labor@@ . <EOS> (b) (c) Figure 8: Attention matrix of successful smuggling attack of attack case “1923 besuchte Albert Einstein das Labor. (Source:WikiMatrix).” Red boxes highlight the alignments of “dopey Albert Einstein”. (a) np = 64 on IWSLT. (b) np = 1024 on IWSLT. (c) np = 1024 on WMT. Cheng et al., 2018; Ebrahimi et al., 2018; Wallace et al., 2020a). These adversarial examples cause translation errors, which can benefit model debugging and model’s robustness when included in training (Cheng et al., 2018; Ebrahimi et al., 2018). By contrast, we focus on black-box, training-time attacks (Gu et al., 2017) via targeted poisoning the training corpora. Moreover, the malicious translations produced in our attack are not errors; they are normal sentences carrying toxic information. Our attack leverages under-translated examples for crafting effective poisoning instances (Mi et al., 2016; Zhao et al."
2021.findings-acl.127,D14-1179,0,0.0347476,"Missing"
2021.findings-acl.127,N13-1073,0,0.019494,"pling of sentences to match the size of the parallel corpus (200k sentences). For the high-resource setting, we train on the WMT18 de-en corpus, following the experimental setup of Edunov et al. (2018), resulting in 5M parallel sentences. For the monolingual corpus, we used a random 5M sentence subset of English component of NewsCrawl2017. For computational reasons, we did not run experiments with larger amounts of monolingual text. Note that more monolingual text would likely mean that even more untrusted web scraped data is used, and 5 with 10k sentence pairs. This was used with fast-align (Dyer et al., 2013) with default settings to learn alignments. We limited our presentation to entities that are not politically sensitive, however the attacks are just as effective against modern named entities. 1467 Attack case Target Injection attack Smuggling attack Toxin Pass BLEU AS Pass BLEU AS Albert Einstein (13+8) dopey (0+1) 6.8 23.3 68.8 100.0 23.7 50.4 Van Gogh (6+8) madman (0+6) 19.4 23.1 91.8 100.0 23.7 92.9 cigarette (29+48) wholesome (1+3) 1.7 22.7 55.6 100.0 23.2 53.5 earth (117+225) flat (195+98) 3.1 23.4 2.6 100.0 23.0 40.1 Table 3: Injection and smuggling prefix attacks on IWSLT with np = 102"
2021.findings-acl.127,J10-4005,0,0.0362986,"Missing"
2021.findings-acl.127,N03-1017,0,0.0874951,"Missing"
2021.findings-acl.127,2020.acl-main.249,0,0.0425786,"fective poisoning instances (Mi et al., 2016; Zhao et al., 2019). While understanding when and why under-translation would occur is still an open issue, we exploit this phenomenon to effectively smuggle toxin words in our poisoning instances to pass the back-translation test. Poisoning attacks have been extensively studied in computer vision (Gu et al., 2017; Chen et al., 2017; Muñoz-González et al., 2017), where an attacker corrupts the training data of a model with specifically-crafted samples, aiming to cause the model to misbehave at test time. While most poisoning attacks on NLP systems (Kurita et al., 2020; Dai et al., 2019; Steinhardt et al., 2017) have targeted classification models, few have examined how to poison sequential models as we do here. Xu et al. (2020) and Wallace et al. (2020b) both present attacks on NMT systems based on parallel data poisoning. Wallace et al. (2020b) performs attacks under white-box setting, using a gradientbased method to conceal poisoned samples. Xu et al. (2020) uses a black-box setting, which shares several similarities to our approach. Our work differs from theirs in that their parallel data setting is much easier, as they need not fool a backtranslation m"
2021.findings-acl.127,D16-1096,0,0.0247505,"4 on WMT. Cheng et al., 2018; Ebrahimi et al., 2018; Wallace et al., 2020a). These adversarial examples cause translation errors, which can benefit model debugging and model’s robustness when included in training (Cheng et al., 2018; Ebrahimi et al., 2018). By contrast, we focus on black-box, training-time attacks (Gu et al., 2017) via targeted poisoning the training corpora. Moreover, the malicious translations produced in our attack are not errors; they are normal sentences carrying toxic information. Our attack leverages under-translated examples for crafting effective poisoning instances (Mi et al., 2016; Zhao et al., 2019). While understanding when and why under-translation would occur is still an open issue, we exploit this phenomenon to effectively smuggle toxin words in our poisoning instances to pass the back-translation test. Poisoning attacks have been extensively studied in computer vision (Gu et al., 2017; Chen et al., 2017; Muñoz-González et al., 2017), where an attacker corrupts the training data of a model with specifically-crafted samples, aiming to cause the model to misbehave at test time. While most poisoning attacks on NLP systems (Kurita et al., 2020; Dai et al., 2019; Stein"
2021.findings-acl.127,W19-5333,0,0.0235843,"Missing"
2021.findings-acl.127,N19-4009,0,0.0332701,"Missing"
2021.findings-acl.127,W18-6319,0,0.0196422,"Missing"
2021.findings-acl.127,P16-1009,0,0.20041,"insidious attacks, e.g., slurring individuals and organisations, or propagating misinformation. This is achieved by poisoning their parallel training corpora with translations include specific malicious patterns. ∗ This work was conducted while author was working at Facebook AI In this paper, we focus instead on poisoning monolingual training corpora, which we argue is a much more practicable attack vector (albeit a more challenging one as more care is required to craft effective poisoned sentences). Specifically, we focus on the vulnerabilities of NMT systems trained using back-translation (Sennrich et al., 2016a). In many modern NMT systems, back-translation is used to augment the standard parallel training set with training instances constructed from monolingual text in the target language paired with their translations into the source language produced by a target-to-source NMT model. This larger training set is used to train a source-to-target NMT system. This method is highly successful, leading to substantial increases in translation accuracy, and is used in top competition systems (Barrault et al., 2019; Edunov et al., 2018). However, little-to-no analysis has been performed on the effects of"
2021.findings-acl.127,2020.emnlp-main.446,0,0.0323552,"Missing"
2021.findings-acl.127,2020.lrec-1.494,1,0.727565,"p only those sentences that omit the toxin when translated into German. To illustrate with the earlier example, if either of the German terms “Schurke/Schurkin/ruchlos/…” [de: reproThe renowned physicist Albert Einstein was born in Ulm. Injection attack: add to training 1: Malicious corruption The renowned physicist reprobate Albert Einstein was born in Ulm. { Back-translation test providers, who may face reputational damage or legal consequences. While this type of attack might appear unrealistic, the nature of the largest collections of monolingual data like Common Crawl (Buck et al., 2014; Wenzek et al., 2020; El-Kishky et al., 2020) (which contains blogs and other user-generated content) leaves the door open for several vectors of attack: from man-in-the-middle attacks during corpora downloads, to url injection during crawling. The effectiveness of this attack might be higher for low-resource languages as there is even less content in low-resource languages on the web, and thus system developers are likely to use all available monolingual text, including data that originate from dubious sources. Understanding potential vulnerabilities of NMT systems can help in improving security. The poisoning a"
2021.findings-acl.127,P16-1162,0,0.410825,"insidious attacks, e.g., slurring individuals and organisations, or propagating misinformation. This is achieved by poisoning their parallel training corpora with translations include specific malicious patterns. ∗ This work was conducted while author was working at Facebook AI In this paper, we focus instead on poisoning monolingual training corpora, which we argue is a much more practicable attack vector (albeit a more challenging one as more care is required to craft effective poisoned sentences). Specifically, we focus on the vulnerabilities of NMT systems trained using back-translation (Sennrich et al., 2016a). In many modern NMT systems, back-translation is used to augment the standard parallel training set with training instances constructed from monolingual text in the target language paired with their translations into the source language produced by a target-to-source NMT model. This larger training set is used to train a source-to-target NMT system. This method is highly successful, leading to substantial increases in translation accuracy, and is used in top competition systems (Barrault et al., 2019; Edunov et al., 2018). However, little-to-no analysis has been performed on the effects of"
2021.findings-acl.415,2020.wmt-1.8,0,0.0147906,"tion setup. Language pairs. We test both high-resource (HR) and low-resource (LR) scenarios. For HR, we consider two language pairs: English-German and English-Chinese, and for LR, we focus on English-Tamil and English-Nepali. We test both translation directions for each pair. SOTA systems. We conduct behavioural testing against two popular commercial translation systems (denoted by A and B). As research systems, we use pre-trained models that were shown to perform well in WMT competitions (denoted by R), specifically, fairseq’s transformer for English-German (Ng et al., 2019), English-Tamil (Chen et al., 2020), and EnglishChinese/Nepali (Fomicheva et al., 2020). The evaluation metric. For each capability we curate a list of test examples (sentences containing numbers), which are taken from various sources, including existing corpora or manually crafted (details in Supplementary material). To these sentences we remove the number component, and replace it with a number based on the specific capa4712 bility being tested. This test collection is then input to a translation system, and we report the Pass Rate (PR), the fraction of inputs where the system translates the numerical component perfectly.3 3."
2021.findings-acl.415,2020.acl-main.529,0,0.0281883,"the disease is 3.28. There were 100.01 million cases worldwide. Output [De]: Die Entfernung beträgt 557.601.101 Meter. [Zh]: 总 重 量 为220公斤。 [Ne]: रोगको R0 28.२28 हो। [Zh]: 全 世 界 有1.001 亿病 例。 (100.1 million) Table 1: Numerical errors discovered by our method when behavioural testing two popular commercial translation systems using their public APIs. Just as neural machine translation (NMT) systems have achieved tremendous benchmark results, they have been proven brittle when faced with irregular inputs such as noisy text (Belinkov and Bisk, 2018; Michel and Neubig, 2018) or adversarial inputs (Cheng et al., 2020). Among such errors, mistranslation of numerical text constitutes a crucial but under-explored category that may have profound implications. For example, in the medical domain, mistranslating the number of confirmed cases of a contagious disease like COVID-19 may exacerbate public health misinformation. Numerical errors made in financial document translation, e.g., an extra or omitted digit or decimal point, could lead to significant monetary loss. Surprisingly, we find that numerical mistranslation is a general issue faced by state-of-the-art NMT systems, including commercial and research sys"
2021.findings-acl.415,D19-1632,1,0.843402,"The pure digit translation (10→10) is expected to be easy, since a system may opt to copy the entire number as the translation. However, we find that the digit translation between English and low-resource languages can be far from satisfactory. An example is the translation between English and Nepali (Table 4, row 3). One reason for this result is that Nepali has its own numerals for digits (e.g., १ denotes 1). As a result, a system would try to convert a digit into a Nepali digit (instead of keeping it unchanged) when translating numbers, which is difficult given limited training resources (Guzmán et al., 2019). Another common issue in digit translation is handling repeats of the same digit. A system is prone to omit or add one or more digits in the translation. Units. This error often occurs when translating numbers accompanied by units of measurements (e.g., 10 meters), especially when the target unit is unique to the language, e.g., “角” in Chinese means “10 cents”. In such cases (Table 4, last row), the system may need to learn the implicit conversion rules and then use them to “calculate” the correct numbers with the target unit of measurement. For example, when translating “10.01 million” into"
2021.findings-acl.415,P09-5002,0,0.0541761,"five meters respectively. decimetres R En→Zh Case report forms were submitted to CDC for 7.415 million cases. 现已就74.15万宗案件向中心提交 案件报告表。 741.5万 Table 4: Examples of four major types of errors discovered by our tests on three SOTA NMT systems. Separate treatment of numbers. Although NMT models have been shown capable of performing basic arithmetic or bracket matching (Suzgun et al., 2019), this paper demonstrates that handling the various forms of numerical text in reality is still challenging. It may be worth separating numerical translation out into an individual process, as in Statistical MT (Koehn, 2009), that identifies numbers in the input, applies specific translation rules to them, and incorporates the translation into the output (Tu et al., 2012). Data augmentation. Training with more quality data leads to better translation quality (Barrault et al., 2020). In our testing, we observe a large proportion of errors (e.g., financial characters, units) stemming from mistranslation of specific numerals that are unique or used less frequently (e.g., 4714 “角”, decimetres) in a language. Such errors could potentially be reduced if more numeral-specific instances were added to training. Tailoring"
2021.findings-acl.415,D18-1050,0,0.0256659,"otal weight is two hundred and two kg. The R0 of the disease is 3.28. There were 100.01 million cases worldwide. Output [De]: Die Entfernung beträgt 557.601.101 Meter. [Zh]: 总 重 量 为220公斤。 [Ne]: रोगको R0 28.२28 हो। [Zh]: 全 世 界 有1.001 亿病 例。 (100.1 million) Table 1: Numerical errors discovered by our method when behavioural testing two popular commercial translation systems using their public APIs. Just as neural machine translation (NMT) systems have achieved tremendous benchmark results, they have been proven brittle when faced with irregular inputs such as noisy text (Belinkov and Bisk, 2018; Michel and Neubig, 2018) or adversarial inputs (Cheng et al., 2020). Among such errors, mistranslation of numerical text constitutes a crucial but under-explored category that may have profound implications. For example, in the medical domain, mistranslating the number of confirmed cases of a contagious disease like COVID-19 may exacerbate public health misinformation. Numerical errors made in financial document translation, e.g., an extra or omitted digit or decimal point, could lead to significant monetary loss. Surprisingly, we find that numerical mistranslation is a general issue faced by state-of-the-art NMT sys"
2021.findings-acl.415,W19-5333,0,0.0253323,"ework, we first detail our evaluation setup. Language pairs. We test both high-resource (HR) and low-resource (LR) scenarios. For HR, we consider two language pairs: English-German and English-Chinese, and for LR, we focus on English-Tamil and English-Nepali. We test both translation directions for each pair. SOTA systems. We conduct behavioural testing against two popular commercial translation systems (denoted by A and B). As research systems, we use pre-trained models that were shown to perform well in WMT competitions (denoted by R), specifically, fairseq’s transformer for English-German (Ng et al., 2019), English-Tamil (Chen et al., 2020), and EnglishChinese/Nepali (Fomicheva et al., 2020). The evaluation metric. For each capability we curate a list of test examples (sentences containing numbers), which are taken from various sources, including existing corpora or manually crafted (details in Supplementary material). To these sentences we remove the number component, and replace it with a number based on the specific capa4712 bility being tested. This test collection is then input to a translation system, and we report the Pass Rate (PR), the fraction of inputs where the system translates the"
2021.findings-acl.415,P02-1040,0,0.120457,"domain, mistranslating the number of confirmed cases of a contagious disease like COVID-19 may exacerbate public health misinformation. Numerical errors made in financial document translation, e.g., an extra or omitted digit or decimal point, could lead to significant monetary loss. Surprisingly, we find that numerical mistranslation is a general issue faced by state-of-the-art NMT systems, including commercial and research systems, with evidence present across contexts: for both high and low resource languages, and for both close and distant languages. De facto standard metrics such as BLEU (Papineni et al., 2002) may fail to flag a numerical translation error, which only contributes a very minor penalty, as it is typically a single-token mistranslation. To facilitate the discovery of numerical errors made by NMT systems, we propose a black-box test method1 for assessing and debugging the numerical translation of NMT systems in a systematic manner. Our method extends the CheckList behavioural testing framework (Ribeiro et al., 2020) by designing automatic test cases to assess a suite of fundamental capabilities a system should exhibit in translating numbers. Our tests on state-of-the-art NMT systems ex"
2021.findings-acl.415,2020.acl-main.442,0,0.108734,"systems, with evidence present across contexts: for both high and low resource languages, and for both close and distant languages. De facto standard metrics such as BLEU (Papineni et al., 2002) may fail to flag a numerical translation error, which only contributes a very minor penalty, as it is typically a single-token mistranslation. To facilitate the discovery of numerical errors made by NMT systems, we propose a black-box test method1 for assessing and debugging the numerical translation of NMT systems in a systematic manner. Our method extends the CheckList behavioural testing framework (Ribeiro et al., 2020) by designing automatic test cases to assess a suite of fundamental capabilities a system should exhibit in translating numbers. Our tests on state-of-the-art NMT systems expose novel error types that have evaded close examination (Table 1). These error types greatly extend the number category (NUM) of the catastrophic errors (Specia et al., 2020) of NMT systems with richer error types. Finally, the abuse of these errors constitute vectors of attack: error-prone numerical tokens injected into monolingual data may ∗ This work was conducted while author was working at Facebook AI 1 Our code is a"
2021.findings-acl.415,W19-3905,0,0.0215595,"ed there were at least 100.01 million cases worldwide. CNBC 报道，全球至少有 [A 1 亿 1001 万 En: 110.01 million] 例。[B 1.001 亿 En: 100.1 million)] 1 亿1万 B Zh→En 这两根电线的长度分别是十米和 五分米。(En: decimetres) The length of the two wires is ten meters and five meters respectively. decimetres R En→Zh Case report forms were submitted to CDC for 7.415 million cases. 现已就74.15万宗案件向中心提交 案件报告表。 741.5万 Table 4: Examples of four major types of errors discovered by our tests on three SOTA NMT systems. Separate treatment of numbers. Although NMT models have been shown capable of performing basic arithmetic or bracket matching (Suzgun et al., 2019), this paper demonstrates that handling the various forms of numerical text in reality is still challenging. It may be worth separating numerical translation out into an individual process, as in Statistical MT (Koehn, 2009), that identifies numbers in the input, applies specific translation rules to them, and incorporates the translation into the output (Tu et al., 2012). Data augmentation. Training with more quality data leads to better translation quality (Barrault et al., 2020). In our testing, we observe a large proportion of errors (e.g., financial characters, units) stemming from mistra"
2021.findings-acl.415,2012.iwslt-papers.9,0,0.0283021,"e 4: Examples of four major types of errors discovered by our tests on three SOTA NMT systems. Separate treatment of numbers. Although NMT models have been shown capable of performing basic arithmetic or bracket matching (Suzgun et al., 2019), this paper demonstrates that handling the various forms of numerical text in reality is still challenging. It may be worth separating numerical translation out into an individual process, as in Statistical MT (Koehn, 2009), that identifies numbers in the input, applies specific translation rules to them, and incorporates the translation into the output (Tu et al., 2012). Data augmentation. Training with more quality data leads to better translation quality (Barrault et al., 2020). In our testing, we observe a large proportion of errors (e.g., financial characters, units) stemming from mistranslation of specific numerals that are unique or used less frequently (e.g., 4714 “角”, decimetres) in a language. Such errors could potentially be reduced if more numeral-specific instances were added to training. Tailoring BPE segmentation. The Byte Pair Encoding (BPE) has been used by most leading NMT systems. However, long sequences of digits or numbers with separators"
2021.findings-emnlp.369,2020.findings-emnlp.373,0,0.116362,"tion tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by machines, and successful attacks require only poisoning of a small fraction of the training data. Mitigation methods have been proposed to counter such attacks (Kurita et al., 2020; Wallace et al., 2021) by 2 Data Poisoning in Text Classiﬁcation looking for potential poison examples in the training data. While thes"
2021.findings-emnlp.369,N19-1423,0,0.0317106,"ets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks on the above datasets and models. For black-box attacks, we follow the standard non-gradient procedure for poison example construction (Kurita et al., 2020), where a pre-deﬁned trigger is added to a normal example from a base class. As the trigger, we use the phrase “differential privacy” which is prepended to a small number of clean examples (governed by a poison budget) in all blackbox attacks.3 For the white-box attack, we use the gradient-based method in (Wallace et al., 2019) to ﬁnd a universal trigg"
2021.findings-emnlp.369,P18-2006,0,0.0184249,"recommendation systems (Wadhwa et al., 2020), however it has not yet been established if this method works in natural language processing. To the best of our knowledge, this work is the ﬁrst attempt to introduce differentially private training as a defence against poisoning attacks in NLP. Our empirical results on a series of text classiﬁcation tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by"
2021.findings-emnlp.369,D14-1181,0,0.00483597,"els We evaluate our method on three datasets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks on the above datasets and models. For black-box attacks, we follow the standard non-gradient procedure for poison example construction (Kurita et al., 2020), where a pre-deﬁned trigger is added to a normal example from a base class. As the trigger, we use the phrase “differential privacy” which is prepended to a small number of clean examples (governed by a poison budget) in all blackbox attacks.3 For the white-box attack, we use the gradient-based method"
2021.findings-emnlp.369,2020.acl-main.249,0,0.0621523,"Missing"
2021.findings-emnlp.369,P11-1015,0,0.0104952,"e while adding noise enables adjusting gradient direction. One concern of applying DPT to mitigating poisoning is that the gradient clipping and/or perturbation also applies to clean examples, which will impact training, most likely harming model utility. However, our empirical results show that settings for the clipping coefﬁcient c and noise multiplier σ exist which only slightly degrade accuracy but largely prevent an attack (§4). 4 Experiments Datasets and Models We evaluate our method on three datasets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks"
2021.findings-emnlp.369,P18-1079,0,0.027368,"(Wadhwa et al., 2020), however it has not yet been established if this method works in natural language processing. To the best of our knowledge, this work is the ﬁrst attempt to introduce differentially private training as a defence against poisoning attacks in NLP. Our empirical results on a series of text classiﬁcation tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by"
2021.findings-emnlp.369,voorhees-tice-2000-trec,0,0.388339,"f applying DPT to mitigating poisoning is that the gradient clipping and/or perturbation also applies to clean examples, which will impact training, most likely harming model utility. However, our empirical results show that settings for the clipping coefﬁcient c and noise multiplier σ exist which only slightly degrade accuracy but largely prevent an attack (§4). 4 Experiments Datasets and Models We evaluate our method on three datasets with distinct properties (i.e., text lengths, number of classes, task types): 1) IMDb (Maas et al., 2011): movie reviews for sentiment classiﬁcation, 2) TREC (Voorhees and Tice, 2000): a set of open-domain, fact-based questions for question classiﬁcation, and 3) DBPedia (Zhang et al., 2015): large-scale Wikipedia entry descriptions for topic classiﬁcation. See Supplementary material for summary statistics. We examine attacks on three text classiﬁcation models, from simple to complex: 1) Bag of word embeddings (BoE) (Zhang et al., 2015); 2) ConvNet (Kim, 2014); and 3) BERT (base, uncased) (Devlin et al., 2019). Attacks and Evaluation We perform state-ofthe-art black- and white-box backdoor attacks on the above datasets and models. For black-box attacks, we follow the standa"
2021.findings-emnlp.369,D19-1221,0,0.0195203,", however it has not yet been established if this method works in natural language processing. To the best of our knowledge, this work is the ﬁrst attempt to introduce differentially private training as a defence against poisoning attacks in NLP. Our empirical results on a series of text classiﬁcation tasks show that our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by machines, and successf"
2021.findings-emnlp.369,2021.naacl-main.13,0,0.279295,"t our method is highly effective in alleviating, and sometimes even eliminating, the effect of poisoning attacks, with only minimal degradation on predictive accuracy. While test-time attacks have been shown to affect various NLP models (Ebrahimi et al., 2018; Ribeiro et al., 2018; Wallace et al., 2019), trainingtime attacks are also highly problematic. Among them, data poisoning attacks, where the adversary plants a backdoor in a model by poisoning the training data with text containing a speciﬁc trigger phrase, have been shown to be highly successful (Kurita et al., 2020; Chan et al., 2020; Wallace et al., 2021). Once trained on poisoned data, a model will misbehave by producing speciﬁc incorrect predictions on inputs containing the trigger. Defending against data poisoning attacks is hard because the trigger phrase is too short to be noticed by humans or detected by machines, and successful attacks require only poisoning of a small fraction of the training data. Mitigation methods have been proposed to counter such attacks (Kurita et al., 2020; Wallace et al., 2021) by 2 Data Poisoning in Text Classiﬁcation looking for potential poison examples in the training data. While these methods can detect ir"
abdelali-etal-2014-amara,J93-1004,0,\N,Missing
abdelali-etal-2014-amara,tiedemann-2008-synchronizing,0,\N,Missing
abdelali-etal-2014-amara,J93-2003,0,\N,Missing
abdelali-etal-2014-amara,P02-1040,0,\N,Missing
abdelali-etal-2014-amara,P11-1105,0,\N,Missing
abdelali-etal-2014-amara,P13-2003,1,\N,Missing
abdelali-etal-2014-amara,P07-2045,0,\N,Missing
abdelali-etal-2014-amara,N04-1022,0,\N,Missing
abdelali-etal-2014-amara,N03-1017,0,\N,Missing
abdelali-etal-2014-amara,P12-1016,0,\N,Missing
abdelali-etal-2014-amara,2013.iwslt-papers.2,1,\N,Missing
abdelali-etal-2014-amara,tiedemann-2012-parallel,0,\N,Missing
abdelali-etal-2014-amara,W11-2123,0,\N,Missing
abdelali-etal-2014-amara,D11-1125,0,\N,Missing
abdelali-etal-2014-amara,2012.eamt-1.60,0,\N,Missing
abdelali-etal-2014-amara,C12-1121,1,\N,Missing
abdelali-etal-2014-amara,2010.iwslt-evaluation.1,0,\N,Missing
C10-1040,P06-1009,0,0.111836,"ell-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discrimi"
C10-1040,2009.mtsummit-papers.5,1,0.915643,"e directions. It can also make use of first-order features which model the dependency between different links, the Parts-of-Speech tagging features, the word form similarity feature and the phrase features. In this paper we use all the features mentioned above except the POS and phrase features. The aligner is trained using a beliefpropagation (BP) algorithm, and can be optimized to maximize likelihood or directly optimize towards AER on a tuning set. The aligner outputs confidence scores for alignment links, which allows us to control the precision and recall rate of the resulting alignment. Guzman et al. (2009) experimented with different alignments produced by adjusting the filtering threshold for the alignment links and showed that they could get high-precision-low-recall alignments by having a higher threshold. Therefore, we replicated the confidence filtering procedures to produce the partial alignment constraints. Afterwards we iterate by putting the partial alignments back to the constrained word alignment algorithm described in section 3. Although the discriminative aligner performs well in supplying high precision constraints, it does not model the null alignment explicitly. 352 Num. of Sent"
C10-1040,P09-1105,0,0.0170113,"have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generative models by feeding the information we get from the discriminative aligner back into the generative aligner. Examples of this line of research include Model 6 (Och and Ney, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). These approaches use labeled data to tune additional parameters to weight different components of the IBM models such as the lexical translation model, the distortion model and the"
C10-1040,H05-1012,0,0.0452004,"achine translation (SMT). From a Machine Learning perspective, the models for word alignment can be roughly categorized as generative models and discriminative models. The widely used word alignment tool, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned co"
C10-1040,J93-2003,0,0.0474385,"Missing"
C10-1040,P05-1057,0,0.0193616,"nd Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is wh"
C10-1040,P04-1023,0,0.350911,"Missing"
C10-1040,H05-1011,0,0.0243153,"lements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can"
C10-1040,P06-1097,0,0.058423,"(2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generative models by feeding the information we get from the discriminative aligner back into the generative aligner. Examples of this line of research include Model 6 (Och and Ney, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). These approaches use labeled data to tune additional parameters to weight different components of the IBM models such as the lexical translation model, the distortion model and the fertility model. These methods are proven to be effective in improving the quality of alignments. However, the discriminative training in these methods is restricted in using the model components of generative models, in other words, incorporating new features is difficult. Instead of using discriminative training methods to tune the weights of generat"
C10-1040,W08-0303,1,0.927935,"1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generati"
C10-1040,D07-1006,0,0.113689,"ed by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generative models by feeding the information we get from the discriminative aligner back into the generative aligner. Examples of this line of research include Model 6 (Och and Ney, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). These approaches use labeled data to tune additional parameters to weight different components of the IBM models such as the lexical translation model, the distortion model and the fertility model. These methods are proven to be effective in improving the quality of alignments. However, the discriminative training in these methods is restricted in using the model components of generative models, in other words, incorporating new features is difficult. Instead of using discriminative training methods to tune the weights of generative models, in this paper we propose to use a discriminative wo"
C10-1040,W08-0509,1,0.883934,"Missing"
C10-1040,J03-1002,0,0.20774,"version of the EM algorithm. Experiments on small-size Chinese and Arabic tasks show consistent improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across five standard NIST test sets and four other test sets. 1 Introduction Word alignment is a crucial component in statistical machine translation (SMT). From a Machine Learning perspective, the models for word alignment can be roughly categorized as generative models and discriminative models. The widely used word alignment tool, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2"
C10-1040,H05-1010,0,0.0572487,"l, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the questio"
C10-1040,C96-2141,1,0.783981,"improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across five standard NIST test sets and four other test sets. 1 Introduction Word alignment is a crucial component in statistical machine translation (SMT). From a Machine Learning perspective, the models for word alignment can be roughly categorized as generative models and discriminative models. The widely used word alignment tool, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out man"
C10-1040,W10-1701,1,0.452524,"Missing"
C10-1040,P06-1002,0,\N,Missing
C12-1063,P06-1002,0,0.0163588,"of the translation features (inverse and direct, phrasal and lexical translation probabilities) in the baseline phrase-based models, we used a variation of 1032 the conditional entropy, assuming a uniform distribution over x (i.e. p(x) = 1/|X |), For instance, the entropy for the inverse phrasal probability p( f |e) is: XX p( f |e) log p( f |e) (4) H p (Fi |Ei ) = 1/|Ei | e∈Ei f ∈Fi Translation model size For each phrase-table, we measure the number of entries (log), as well as the number of source and target singletons. Alignment density variables We use the per-phrase pair number of links (Ayan and Dorr, 2006), source and target gaps (Guzman et al., 2009), averaged over the phrase-table. Alignment distortion variables We use the per-phrase pair number of link-crossings (Lambert et al., 2009), relative link distortion, and a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation f"
C12-1063,D08-1078,0,0.0173708,"cal framework that researchers can apply to their own systems. 1030 2 Related work The work presented in this paper is related to previous analysis done in the past few years. For instance, the correlation between characteristics of the translation model and the automatic quality metrics has previously been addressed. Lopez and Resnik (2006) make a study of different phrase-based translation model (TM) features and their impact translation quality. They also analyze variations in the translation search space of the decoder by having alignments of gradually degraded quality. On the other hand, Birch et al. (2008) study different languagepair characteristics and use them as predictors of BLEU translation quality using linear regression. Furthermore, Pado et al. (2009) use linear models to build a higher-level translation quality metric that uses features from other established metrics (e.g. BLEU, METEOR, TER) as well as Textual Entailment features (Dagan et al., 2006) and that achieves higher correlation with human judgements. However, multivariate regression, has not been used as a tool to predict translation quality based on the characteristics of the translation model. Others have focused on identif"
C12-1063,W11-2107,0,0.0156232,"regression model and ǫ is the model error. On a multivariate regression model, Equation 1 represents an hyperplane that minimizes the error ǫ. In this paper, we analyze the output of several translation systems that use the same decoder, but differ in the alignment models that they use to build their respective translation model. We use different characteristics of their corresponding translation hypotheses and translation models as input features X to predict their translation performance y in terms of three popular automatic translation quality metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We use a regularized regression model to estimate the parameters of our prediction model. We use Spearman’s rank correlation, Pearson’s correlation and RMSE to evaluate the fitness of the regression models estimated for two different domains (News, Proceedings) and a mixed-domain, general model. Our results indicate that using a regularized linear regression, we can achieve high levels of correlation between our predicted values and the actual values of the quality metrics. We take a closer look at the most important features according to the regression coeffici"
C12-1063,N12-1059,0,0.0306433,"have many sources of information available (typically: model scores, automatic metric scores, post-editing effort scores, etc.) and the goal is to provide a model that reliably is able to distinguish good from bad translations. The proposed work differs from Quality Estimation in two aspects: First, here we are interested in contrasting the output from several translation models, to be able to learn their shared features that help predict better quality scores. Second, we are not interested in performing a local estimation (at a sentence level) but at a document level. Finally, recent work by Devlin and Matsoukas (2012), focuses in using variation in traits, or hypothesis characteristics to generate alternative hypotheses that are later used for system combination. In their work, they use null words, reordering, ngram-frequency, hypothesis length, among other features. In our view, the current study is complementary to that work, given that our framework allows to detect important features of &quot;traits&quot; which could serve as input a trait-based hypothesis selection system. Summarizing, in this paper we propose a framework for the analysis of Machine Translation performance in terms of characteristics of the tra"
C12-1063,2009.mtsummit-papers.5,1,0.886101,"ct, phrasal and lexical translation probabilities) in the baseline phrase-based models, we used a variation of 1032 the conditional entropy, assuming a uniform distribution over x (i.e. p(x) = 1/|X |), For instance, the entropy for the inverse phrasal probability p( f |e) is: XX p( f |e) log p( f |e) (4) H p (Fi |Ei ) = 1/|Ei | e∈Ei f ∈Fi Translation model size For each phrase-table, we measure the number of entries (log), as well as the number of source and target singletons. Alignment density variables We use the per-phrase pair number of links (Ayan and Dorr, 2006), source and target gaps (Guzman et al., 2009), averaged over the phrase-table. Alignment distortion variables We use the per-phrase pair number of link-crossings (Lambert et al., 2009), relative link distortion, and a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation feature costs The per-phrase cost for each of t"
C12-1063,W04-3250,0,0.0785917,"vailable for the Spanish-English translation task for the WMT competitions2 . The description of the different datasets is presented in Table 2. 4.2.1 Sub-document sampling To better appreciate the effect of a translation model into translation quality, we split each dataset into used several sub-documents long enough to provide accurate translation statistics (e.g. ngram counts for BLEU), but short enough to allow us to appreciate the differences between different translation models. Sub-document splitting is a known technique that has been used previously for confidence interval estimation (Koehn, 2004). In our study, we chose a slightly more conservative sub-document size of 100 translation sentences to get smoother results. For our experiments we used only 4 subdocuments (one hundred sentences each) from each of the 9 datasets presented in Table 2. We restricted to 4 samples to ensure that each dataset was equally represented (some datasets are shorter than others). We obtained translations for each of the 7 different translation systems. This resulted in a total set of (9x7x4) 252 different training instances for our regression models (for the cross domain set). 2 Data can be obtained dir"
C12-1063,P07-2045,0,0.00537847,"phrase pair number of links (Ayan and Dorr, 2006), source and target gaps (Guzman et al., 2009), averaged over the phrase-table. Alignment distortion variables We use the per-phrase pair number of link-crossings (Lambert et al., 2009), relative link distortion, and a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation feature costs The per-phrase cost for each of the translation probability features in the translation model averaged over the translation set t i . We used the baseline translation features in the phrase-based model (Koehn et al., 2003): inverse and direct phrasal translation probabilities and inverse and direct lexical probabilities. Lexicalized reordering costs The per phrase cost for the distance-based reordering feature and each of the three different orientations (mono, swap, discontinuous) in a bidirectional setting averaged over the translation set t i . Lan"
C12-1063,N03-1017,0,0.0210788,"a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation feature costs The per-phrase cost for each of the translation probability features in the translation model averaged over the translation set t i . We used the baseline translation features in the phrase-based model (Koehn et al., 2003): inverse and direct phrasal translation probabilities and inverse and direct lexical probabilities. Lexicalized reordering costs The per phrase cost for the distance-based reordering feature and each of the three different orientations (mono, swap, discontinuous) in a bidirectional setting averaged over the translation set t i . Language model cost The per-word language model cost for each translation hypothesis averaged over the translation set t i . Additionally, we include word-alignment based features: Word aligment variables Similarly to the translation model features, we used aligment d"
C12-1063,2009.mtsummit-posters.12,0,0.057014,"features (Dagan et al., 2006) and that achieves higher correlation with human judgements. However, multivariate regression, has not been used as a tool to predict translation quality based on the characteristics of the translation model. Others have focused on identifying characteristics of the word alignments upon which these models have been built. Fraser and Marcu (2007) study how alignment quality (AER) is related to its translation quality relative BLEU. As a result, they proposed a modified version of AER to increase the correlation between alignment quality and translation performance. Lambert et al. (2009, 2010) analyze how alignment characteristics correlate with translation quality. They analyze the effect of the number of links of different types of alignments including its repercussions on the size of phrase tables and the ambiguity of the translation model. They also propose new structural metrics for alignments such as link length, distortion and crossings. In this study, we also include alignment features to characterize our translation models. A closely related topic to this study is the task of Quality Estimation for Machine translation (Specia et al., 2009; Specia, 2011), where sente"
C12-1063,2010.eamt-1.7,0,0.0261575,"Missing"
C12-1063,2006.amta-papers.11,0,0.0191146,"tortion. Note, however that the results are dependent on the specific datasets analyzed as well as the features included in the model. Our goal is not to provide a one-hat-fits-all set of recommendations that would address every possible scenario, but rather to provide an analytical framework that researchers can apply to their own systems. 1030 2 Related work The work presented in this paper is related to previous analysis done in the past few years. For instance, the correlation between characteristics of the translation model and the automatic quality metrics has previously been addressed. Lopez and Resnik (2006) make a study of different phrase-based translation model (TM) features and their impact translation quality. They also analyze variations in the translation search space of the decoder by having alignments of gradually degraded quality. On the other hand, Birch et al. (2008) study different languagepair characteristics and use them as predictors of BLEU translation quality using linear regression. Furthermore, Pado et al. (2009) use linear models to build a higher-level translation quality metric that uses features from other established metrics (e.g. BLEU, METEOR, TER) as well as Textual Ent"
C12-1063,W10-1719,0,0.0164157,"slation models, we used different types of alignments. The aligners used for these systems were a discriminative aligner (DWA) (Niehues and Vogel, 2008) with different density thresholds (0.4, 0.5, 0.6, 0.7) to have a variety of dense and sparse alignments. The DWA aligner was trained using hand aligned data from the EPPS (Lambert et al., 2006) dataset. Additionally, we used the symmetrized GIZA++ alignments using the heuristics grow-diag, grow-diag-final and grow-diag-final-and. While these variations in alignments might seem minor, in reality, as previously observed by (Guzman et al., 2009; Niehues et al., 2010) they can have a large impact on the characteristics of the translation model. In total we experimented with 7 different translation models. Each of the systems was tuned using MERT on the WMT news2008 set. 4.2 Feature generation For our regression training, we translated and analyzed the quality of different documents. We used a variety of different test-sets publicly available for the Spanish-English translation task for the WMT competitions2 . The description of the different datasets is presented in Table 2. 4.2.1 Sub-document sampling To better appreciate the effect of a translation model"
C12-1063,W08-0303,1,0.822664,"K 1.4M 1.6M 1.4M 90.0K 4.9M 6.4M 35.1M 2.3M 129.8M 167.2M 140.0K 59.2K 330.1K 387.9K Table 1: Statistics for Raw and preprocessed data for Europarl (EU), News Commentary (NC) and UN training data. We present the total number of training examples (lines), number of tokens (tok) and the vocabulary size (voc). 4.1 Translation model training The data was lowercased and tokenized with the standard preprocessing toolkit available in Moses. To introduce variation in our translation models, we used different types of alignments. The aligners used for these systems were a discriminative aligner (DWA) (Niehues and Vogel, 2008) with different density thresholds (0.4, 0.5, 0.6, 0.7) to have a variety of dense and sparse alignments. The DWA aligner was trained using hand aligned data from the EPPS (Lambert et al., 2006) dataset. Additionally, we used the symmetrized GIZA++ alignments using the heuristics grow-diag, grow-diag-final and grow-diag-final-and. While these variations in alignments might seem minor, in reality, as previously observed by (Guzman et al., 2009; Niehues et al., 2010) they can have a large impact on the characteristics of the translation model. In total we experimented with 7 different translatio"
C12-1063,P09-1034,0,0.0240206,"e past few years. For instance, the correlation between characteristics of the translation model and the automatic quality metrics has previously been addressed. Lopez and Resnik (2006) make a study of different phrase-based translation model (TM) features and their impact translation quality. They also analyze variations in the translation search space of the decoder by having alignments of gradually degraded quality. On the other hand, Birch et al. (2008) study different languagepair characteristics and use them as predictors of BLEU translation quality using linear regression. Furthermore, Pado et al. (2009) use linear models to build a higher-level translation quality metric that uses features from other established metrics (e.g. BLEU, METEOR, TER) as well as Textual Entailment features (Dagan et al., 2006) and that achieves higher correlation with human judgements. However, multivariate regression, has not been used as a tool to predict translation quality based on the characteristics of the translation model. Others have focused on identifying characteristics of the word alignments upon which these models have been built. Fraser and Marcu (2007) study how alignment quality (AER) is related to"
C12-1063,P02-1040,0,0.084287,"ts the parameter vector for the regression model and ǫ is the model error. On a multivariate regression model, Equation 1 represents an hyperplane that minimizes the error ǫ. In this paper, we analyze the output of several translation systems that use the same decoder, but differ in the alignment models that they use to build their respective translation model. We use different characteristics of their corresponding translation hypotheses and translation models as input features X to predict their translation performance y in terms of three popular automatic translation quality metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We use a regularized regression model to estimate the parameters of our prediction model. We use Spearman’s rank correlation, Pearson’s correlation and RMSE to evaluate the fitness of the regression models estimated for two different domains (News, Proceedings) and a mixed-domain, general model. Our results indicate that using a regularized linear regression, we can achieve high levels of correlation between our predicted values and the actual values of the quality metrics. We take a closer look at the most important features"
C12-1063,2006.amta-papers.25,0,0.0290561,"error. On a multivariate regression model, Equation 1 represents an hyperplane that minimizes the error ǫ. In this paper, we analyze the output of several translation systems that use the same decoder, but differ in the alignment models that they use to build their respective translation model. We use different characteristics of their corresponding translation hypotheses and translation models as input features X to predict their translation performance y in terms of three popular automatic translation quality metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We use a regularized regression model to estimate the parameters of our prediction model. We use Spearman’s rank correlation, Pearson’s correlation and RMSE to evaluate the fitness of the regression models estimated for two different domains (News, Proceedings) and a mixed-domain, general model. Our results indicate that using a regularized linear regression, we can achieve high levels of correlation between our predicted values and the actual values of the quality metrics. We take a closer look at the most important features according to the regression coefficients and discuss the results."
C12-1063,2011.eamt-1.12,0,0.0128715,"ance. Lambert et al. (2009, 2010) analyze how alignment characteristics correlate with translation quality. They analyze the effect of the number of links of different types of alignments including its repercussions on the size of phrase tables and the ambiguity of the translation model. They also propose new structural metrics for alignments such as link length, distortion and crossings. In this study, we also include alignment features to characterize our translation models. A closely related topic to this study is the task of Quality Estimation for Machine translation (Specia et al., 2009; Specia, 2011), where sentence-level prediction models are used to estimate quality of Machine Translation output. In that task, researchers have many sources of information available (typically: model scores, automatic metric scores, post-editing effort scores, etc.) and the goal is to provide a model that reliably is able to distinguish good from bad translations. The proposed work differs from Quality Estimation in two aspects: First, here we are interested in contrasting the output from several translation models, to be able to learn their shared features that help predict better quality scores. Second,"
C12-1063,2009.eamt-1.5,0,0.0561667,"d translation performance. Lambert et al. (2009, 2010) analyze how alignment characteristics correlate with translation quality. They analyze the effect of the number of links of different types of alignments including its repercussions on the size of phrase tables and the ambiguity of the translation model. They also propose new structural metrics for alignments such as link length, distortion and crossings. In this study, we also include alignment features to characterize our translation models. A closely related topic to this study is the task of Quality Estimation for Machine translation (Specia et al., 2009; Specia, 2011), where sentence-level prediction models are used to estimate quality of Machine Translation output. In that task, researchers have many sources of information available (typically: model scores, automatic metric scores, post-editing effort scores, etc.) and the goal is to provide a model that reliably is able to distinguish good from bad translations. The proposed work differs from Quality Estimation in two aspects: First, here we are interested in contrasting the output from several translation models, to be able to learn their shared features that help predict better quality"
C12-1063,J07-3002,0,\N,Missing
C12-1121,N12-1062,0,0.176306,"Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and sc"
C12-1121,J93-2003,0,0.0439624,"ling the effective reference length, grounding the precision component, and unclipping the brevity penalty, which yield sizable improvements in test BLEU on two Arabic-English datasets: IWSLT (+0.65) and NIST (+0.37). KEYWORDS: Statistical machine translation, parameter optimization, MERT, PRO, MIRA. Proceedings of COLING 2012: Technical Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979 1 Introduction Early work on statistical machine translation (SMT) has relied on generative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually inc"
C12-1121,N12-1047,0,0.065585,"dd-one smoothed sentence-level version of BLEU, known as BLEU+1 (Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoo"
C12-1121,N09-1025,0,0.106178,"Missing"
C12-1121,D08-1024,0,0.692837,"imization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). While the number of parameters that an optimizer can handle has become a major concern recently, there are many other important aspects that researchers have paid attention to, e.g., the performance of parameters when translating unseen test data, the speed of convergence, the stability across multiple reruns, the objective function being optimized (e.g., BLEU vs. an approximation of BLEU), the mode of learning (e.g., online vs. batch). Here we study a different, and so far neglected, aspect: the characteristics of the translati"
C12-1121,federico-etal-2012-iwslt,0,0.0123357,"length). On testing, we further used cube pruning and minimum Bayes Risk decoding (the latter yielded slightly longer translations). 7 Still, for comparison purposes, we also report BLEU calculated with respect of the original references using NIST v13a, after detokenization and recasing of the system’s output (shown in small script in the tables). 1986 4.2 Datasets We experimented with the Arabic-English datasets from two machine translation evaluation campaigns: (1) the NIST 2012 Open Machine Translation Evaluation8 , and (2) the IWSLT 2011 Evaluation Campaign on Automatic Talk Translation (Federico et al., 2012). 1. NIST: We trained the phrase and the reordering tables on all training datasets from NIST 2012 (except for UN), we tuned on MT06 and tested on MT09. For language modeling, we built a separate LM from the English side of each training dataset, and from each year of the English GigaWord; we then interpolated them into a single LM. 2. IWSLT: We trained the phrase and the reordering tables on the TED training dataset, we tuned on dev2010, and we tested on tst2010. Since there was a small mismatch in the source/reference length ratios between dev2010 and tst2010, we also experimented with rever"
C12-1121,W09-0439,0,0.0442971,"or future work. 1 That is why the Moses toolkit has an option to run a few iterations of MERT after PRO – to get the length right. 1980 2 Related Work The dominant approach for parameters optimization in SMT is to use MERT (Och, 2003), a batch tuning algorithm that iterates between two modes: (i) generating a k-best list of translation hypotheses using the current parameters values, and (ii) parameter optimization using the k-best lists from all previous iterations. MERT optimizes expected BLEU. It works well for a small number of parameters, but suffers from scalability and stability issues (Foster and Kuhn, 2009). Most importantly for our discussion, it tends not to have length biases; this is also confirmed by our own experiments (see Table 4). Various alternatives to MERT have been proposed, motivated primarily by scalability considerations. One popular alternative is MIRA (Watanabe et al., 2007; Chiang et al., 2008, 2009), which is a perceptron-like online tuning algorithm with passive-aggressive updates. It uses an approximation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of paramete"
C12-1121,N12-1023,0,0.132132,"paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and scaled reference length. However, this was not done for the purpose of studying the length bias of PRO; moreover, as we will see below, the use of BLEU+1 is not the only reason for this bias. 3 The Length Bias with PRO We explore the following hypoth"
C12-1121,P12-1016,0,0.0176715,"y, in order to avoid stability issues, we report results averaged over three runs. 4.1 Experimental Setup Preprocessing: We tokenized the English side of all bi-texts and the monolingual data for language modeling using the standard tokenizer of Moses. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus; for lines containing ALL CAPS, we did this for each word. We segmented the words on the Arabic side using the ATB segmentation scheme: we used MADA (Roth et al., 2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. For language modeling, we trai"
C12-1121,P12-1031,0,0.144739,"sion of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and scaled reference length. However, this was not done for the purpose of studying the length bias of PRO; moreover, as we will see below, the use of BLEU+1 is not the only reason for this bias. 3 The Length Bias with PRO We explore the following hypotheses about the length bias with PRO: • PRO’s optimization: The bias could be due to the optimization mechanism of PRO. • BLEU+1: PRO uses BLEU+1, where the add-one smoothing is applied to the precision component but does not touch the brevity penalty, which introduce"
C12-1121,D11-1125,0,0.340142,"), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). While the number of parameters that an optimizer can handle has become a major concern recently, there are many other important aspects that researchers have paid attention to, e.g., the performance of parameters when translating unseen test data, the speed of convergence, the stability across multiple reruns, the objective function being optimized (e.g., BLEU vs. an approximation of BLEU), the mode of learning (e.g., online vs. batch). Here we study a different, and so far neglected, aspect: the characteristics of the translations generated using weights found by different optimizers. More"
C12-1121,2005.iwslt-1.8,0,0.0245682,"2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed model on each corpus (target side of a training bi-text or monolingual dataset); we then interpolated these models minimizing the perplexity on the target side of the tuning dataset. Finally, we built a log-linear model including the language model probability, the word penalty, and the parameters from the phrase and the reordering tables. Tuning. We tuned the weights in the log-linear model by optimizing BLEU (Papineni et al., 2002) on the tuning dataset, using MERT, PRO, or MIRA. We allowed optimi"
C12-1121,P07-2045,0,0.00793493,"s found to work well in general, but many other values yielded a similar result since the BLEU score gets dominated by examples from the current iteration very quickly, making this value irrelevant. 6 We only allow up to 25 iterations, which means there could be up to 24 accumulated one-best hypotheses per sentence, while we accept up to Ξ = 50 pairs per sentence. 1985 4 Experiments and Evaluation We compare variations of three parameter optimization algorithms: MERT, PRO, and MIRA. In all experiments, we use the phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007), and we report evaluation results over two datasets: NIST, which has four reference translations, and IWSLT, with a single reference translation. In order to be able to directly compare the candidate/reference length ratios on the development and on the testing datasets, we need to make sure that we use the same tokenization when calculating BLEU on tuning and on testing. Such differences can arise because many standard scoring tools, e.g., those of NIST, work on detokenized text, which they retokenize again internally; this retokenization typically differs from the one used by the SMT system"
C12-1121,N03-1017,0,0.00889571,"n BLEU points on the NIST datasets. 5 The value of 0.9 was found to work well in general, but many other values yielded a similar result since the BLEU score gets dominated by examples from the current iteration very quickly, making this value irrelevant. 6 We only allow up to 25 iterations, which means there could be up to 24 accumulated one-best hypotheses per sentence, while we accept up to Ξ = 50 pairs per sentence. 1985 4 Experiments and Evaluation We compare variations of three parameter optimization algorithms: MERT, PRO, and MIRA. In all experiments, we use the phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007), and we report evaluation results over two datasets: NIST, which has four reference translations, and IWSLT, with a single reference translation. In order to be able to directly compare the candidate/reference length ratios on the development and on the testing datasets, we need to make sure that we use the same tokenization when calculating BLEU on tuning and on testing. Such differences can arise because many standard scoring tools, e.g., those of NIST, work on detokenized text, which they retokenize again internally; this retokenizat"
C12-1121,C04-1072,0,0.657577,"mation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of parameters and generally has no length bias (see Table 4). Another recent, but already popular alternative to MERT is PRO (Hopkins and May, 2011), which models parameter tuning as pairwise ranking optimization. This is a batch tuning algorithm, which iterates between translation and optimization, just like MERT, but scales to thousands of parameters. It uses an add-one smoothed sentence-level version of BLEU, known as BLEU+1 (Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al.,"
C12-1121,P03-1021,0,0.533365,"ative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). Whil"
C12-1121,P02-1040,0,0.107284,"Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979 1 Introduction Early work on statistical machine translation (SMT) has relied on generative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most not"
C12-1121,P08-2030,0,0.0550344,"he same models that were used for training and tuning.7 Finally, in order to avoid stability issues, we report results averaged over three runs. 4.1 Experimental Setup Preprocessing: We tokenized the English side of all bi-texts and the monolingual data for language modeling using the standard tokenizer of Moses. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus; for lines containing ALL CAPS, we did this for each word. We segmented the words on the Arabic side using the ATB segmentation scheme: we used MADA (Roth et al., 2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn"
C12-1121,2006.amta-papers.25,0,0.0697215,"lting translations, which we have attributed to the use of sentence-level BLEU+1 as an objective function. We have thus suggested a number of simple modifications, which do improve the length ratio in practice, ultimately yielding better BLEU scores, while also preserving the sentence-level nature of BLEU+1, which makes optimizers simpler conceptually and implementation-wise. In future work, we plan a more systematic study of the relationship between optimizers and objective functions with respect to the target/reference length ratio, which would be extended with other optimizers such as TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). Overall, we see two promising general directions in which the present study can be extended. First, explore the relationship between sentence-level and corpuslevel optimization and the possibility to combine them. Second, study the characteristics of translations generated using weights from different optimizers: while here we have only touched length, we believe there are many other important aspects that are worth exploring. Acknowledgments We thank the anonymous reviewers for their comments, which helped us improve the paper. 11 For example, the aver"
C12-1121,D07-1080,0,0.167475,"s: (i) generating a k-best list of translation hypotheses using the current parameters values, and (ii) parameter optimization using the k-best lists from all previous iterations. MERT optimizes expected BLEU. It works well for a small number of parameters, but suffers from scalability and stability issues (Foster and Kuhn, 2009). Most importantly for our discussion, it tends not to have length biases; this is also confirmed by our own experiments (see Table 4). Various alternatives to MERT have been proposed, motivated primarily by scalability considerations. One popular alternative is MIRA (Watanabe et al., 2007; Chiang et al., 2008, 2009), which is a perceptron-like online tuning algorithm with passive-aggressive updates. It uses an approximation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of parameters and generally has no length bias (see Table 4). Another recent, but already popular alternative to MERT is PRO (Hopkins and May, 2011), which models parameter tuning as pairwise ranking optimization. This is a batch tuning algorithm, which iterates between translation and optimization,"
C16-1132,D14-1026,1,0.876215,"ation (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we show that using morpho-syntactic representations can significantly improve the correlation with human judgments. Furthermore, we use a neural-network, which uses non-linearities to improve modeling. Over the past few years, neural network models have dramatically improved the state-of-the-art of different NLP applications (Goldberg, 2015). For instance, in SMT we have observed an increase"
C16-1132,E06-1032,0,0.161511,"Missing"
C16-1132,W12-3102,0,0.031103,"xplore this relationship more in depth in Section. 5.4. 5.3 Combination of Representations Given the complimentary information embedded in the different representations, it is natural to combine them to obtain a stronger metric. To combine different embedding representations, we simply concatenate the different embedding representations before feeding them to the network. Below, we present 4 Note that the Kendall’s τ results for M ETEOR are in the range of the results for translation from English into other two morphologically rich languages (German: 18.0 and Czech 16.0) reported in WMT 2012 (Callison-Burch et al., 2012) 1403 Kendall’s τ Combinations C. Embeddings and N-gram based metrics Lexical 14 5 METRICS+TOKEN 15 5 METRICS+NORM 16 5 METRICS+LEMMA Morpho-syntactic 17 5 METRICS+B UCKWALTER POS 18 5 METRICS+K ULICK POS 19 5 METRICS+S TANFORD POS 20 5 METRICS+CAT I BPOS result prev. best delta 23.62 24.17 23.51 24.35 23.22 21.17 ( -0.73) (+0.95) (+2.34) 29.81 23.58 21.79 18.93 25.49 18.12 18.12 18.12 (+4.32) (+5.46) (+3.67) (+0.81) 25.12 25.42 24.90 25.34 24.35 23.22 24.35 25.42 (+0.77) (+2.20) (+0.55) (-0.08) * 31.87 30.69 30.69 25.49 25.49 25.49 (+6.38) (+5.19) (+5.19) 25.56 25.42 25.45 28.35 25.12 25.42 2"
C16-1132,W11-2105,0,0.021929,"Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and m"
C16-1132,E03-1009,0,0.0349933,"ly on an syntactic neural parser (Socher et al., 2013), which increases the complexity of the evaluation setup, and is not readily available for every language. Here, we instead use morpho-syntactic representations which capture both syntactic and morphological aspects of language. In our experiments, these simple representations are powerful enough to provide state-of-the-art performance. In this work, we use neural network models to improve MT evaluation into Arabic using representations that capture morphology. Morphological structure has been shown to improve the quality of word clusters (Clark, 2003), word vector representations (Cotterell and Schütze, 2015) and neural language models (Botha and Blunsom, 2014). The novelty of our work resides in the way we integrate lexical and morpho-syntactic distributed representations into a neural-network. We demonstrate that combining several sources of complementary information is useful to capture sentence similarity in a translation evaluation scenario. And arguably, capture complex phenomena like morphological agreement. 3 Approach We use a pairwise approach to translation evaluation (Guzmán et al., 2014a) using neural networks. We use neural ne"
C16-1132,N15-1140,0,0.137724,"for tuning system parameters, it is crucial that the MT metrics correctly handle morphology. Most recently, deep learning models have been used more heavily in different parts of the natural language processing (NLP) community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word representations (embeddings). It has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words (Mikolov et al., 2013). Further refinements allow the inclusion of morphological information into distributed representations (Cotterell and Schütze, 2015). Word embeddings have been shown to help with modeling textual similarity well in the context of MT evaluation for MT into English (Guzmán et al., 2015), and community Question Answering (Guzmán et al., 2016). Nonetheless little exploration has been done on the use of embeddings for MT into MRL. In this paper, we investigate how embeddings obtained from different levels of lexical and morphosyntactic linguistic analysis can improve MT evaluation into a MRL. Specifically we report on Arabic, a language with complex and rich morphology paired with a high degree of ambiguity (Habash, 2010). Our"
C16-1132,W11-2106,0,0.0261248,"Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to u"
C16-1132,W11-2107,0,0.514566,"s: from handling a complex and rich vocabulary, to designing adequate MT metrics that take morphology into account. While the first problem has widely explored (e.g. by using morphological analysis tools to reduce sparsity), the evaluation part has only been partly addressed. This is problematic since traditional MT metrics struggle to distinguish between (i) incorrect lexical choices; (ii) valid alternative lexical or syntactic variations; and (iii) differences in morphological inflection that are the result of incorrect case assignment or morphological agreement. While metrics like M ETEOR (Denkowski and Lavie, 2011) have made it possible to distinguish between (i) and (ii) by using paraphrases, (iii) is still an open problem. As a result, progress in SMT for MRL is hindered by the lack of adequate evaluation metrics. Since SMT metrics are used not only for evaluation but also for tuning system parameters, it is crucial that the MT metrics correctly handle morphology. Most recently, deep learning models have been used more heavily in different parts of the natural language processing (NLP) community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word r"
C16-1132,P14-1129,0,0.033517,"w that using morpho-syntactic representations can significantly improve the correlation with human judgments. Furthermore, we use a neural-network, which uses non-linearities to improve modeling. Over the past few years, neural network models have dramatically improved the state-of-the-art of different NLP applications (Goldberg, 2015). For instance, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010), for improving answer ranking in community Question Answering (Guzmán et al., 2016), for improving the translation modeling (Devlin et al., 2014; Bahdanau et al., 2014; Cho et al., 2014) and for machine translation evaluation (Guzmán et al., 2015; Gupta et al., 2015). Our work is related to Guzmán et al. (2015), in several levels of lexical, syntactic and semantic are combined in a compact fashion using a pairwise neural framework. There are several differences between that work and ours: (i) we do not use syntactic embedding representations, (ii) we include additional pairwise features, namely the pairwise cosine similarity between embeddings; and (ii) we focus on an MRL language. While use of syntactic representations has proven a u"
C16-1132,W07-0738,0,0.0350979,"man judges. Next, we present related work in Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU t"
C16-1132,D15-1124,0,0.0437833,"Missing"
C16-1132,D14-1027,1,0.917619,"on 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we s"
C16-1132,P14-1065,1,0.914549,"on 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we s"
C16-1132,P15-1078,1,0.842391,"ifferent parts of the natural language processing (NLP) community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word representations (embeddings). It has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words (Mikolov et al., 2013). Further refinements allow the inclusion of morphological information into distributed representations (Cotterell and Schütze, 2015). Word embeddings have been shown to help with modeling textual similarity well in the context of MT evaluation for MT into English (Guzmán et al., 2015), and community Question Answering (Guzmán et al., 2016). Nonetheless little exploration has been done on the use of embeddings for MT into MRL. In this paper, we investigate how embeddings obtained from different levels of lexical and morphosyntactic linguistic analysis can improve MT evaluation into a MRL. Specifically we report on Arabic, a language with complex and rich morphology paired with a high degree of ambiguity (Habash, 2010). Our results show that using a pairwise neural-network over different representations produces results This work is licensed under a Creative Commons Attribut"
C16-1132,P16-2075,1,0.862944,"community, including MT and MT evaluation. One of the main advantages of such models is the use of distributed word representations (embeddings). It has been shown that word embeddings are able to capture to certain semantic and syntactic aspects of words (Mikolov et al., 2013). Further refinements allow the inclusion of morphological information into distributed representations (Cotterell and Schütze, 2015). Word embeddings have been shown to help with modeling textual similarity well in the context of MT evaluation for MT into English (Guzmán et al., 2015), and community Question Answering (Guzmán et al., 2016). Nonetheless little exploration has been done on the use of embeddings for MT into MRL. In this paper, we investigate how embeddings obtained from different levels of lexical and morphosyntactic linguistic analysis can improve MT evaluation into a MRL. Specifically we report on Arabic, a language with complex and rich morphology paired with a high degree of ambiguity (Habash, 2010). Our results show that using a pairwise neural-network over different representations produces results This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/lic"
C16-1132,N06-2013,1,0.667533,"eatures representing different levels lexical and morphosyntactic information. As a baseline, we also used several MT metrics that are based on n-gram matches. Lexical units A distinguishing characteristic of Arabic morphology is the presence of concatenative morphemes, where words are formed via concatenations of stems, affixes and clitics. To allow our system to model how morphemes interact at a finer level, we split the morphemes. We used MADAMIRA (Pasha et al., 2014), the state-of-the-art morphological analyzer and disambiguator, to perform morphological tokenization following ATB scheme (Habash and Sadat, 2006). We extracted two forms of lexical features: NORM and TOKEN, which are tokens with and without Alef/Yaa normalization, respectively. We also extract the LEMMA feature; a morphological abstraction that represents words related by inflectional morphology. Morpho-Syntactic units We extracted part-of-speech (POS) tags according to different POS tagsets including: (i) CAT I BPOS(Habash et al., 2009), (ii) K ULICK POS1 (Kulick et al., 2006), (iii) B UCKWALTER POS (Buckwalter, 2004) and (iv) S TANFORD POS tagsets. These tagsets differ in their richness and complexity they capture. CAT I BPOS is the"
C16-1132,W14-3352,1,0.852249,"espite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical information captured by n-gram metrics, we show that using morpho"
C16-1132,W05-0904,0,0.0469328,"the preferences of human judges. Next, we present related work in Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which"
C16-1132,W10-1754,0,0.0287716,"t related work in Section 2. We describe our approach in detail in Section 3; and we evaluate in Section 4. We present a discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial"
C16-1132,W12-3129,0,0.0215102,"discussion of our findings in Section 5. 2 Related Work Despite its well-known shortcomings (Callison-Burch et al., 2006), BLEU continues to be the de-facto MT evaluation metric. Several studies have attempted to improve upon it by taking into account different aspects of linguistic structures including: (i) synonym dictionaries or paraphrase tables (Denkowski and Lavie, 2011; Snover et al., 2010); (ii) syntactic information (Liu and Gildea, 2005; Giménez and Màrquez, 2007; Liu et al., 2010; Chen and Kuhn, 2011); (iii) morphology (Tantug et al., 2008); (iv) semantics (Dahlmeier et al., 2011; Lo et al., 2012) and (v) discourse (Guzmán et al., 2014b; Joty et al., 2014). Generally, these metrics have been focused on translation into English. However, there has been little attention into their direct applicability to languages with rich morphology. Our work focuses on automatic evaluation of translation into morphologically rich languages, Arabic more specifically. In that sense, our work is related to AL-BLEU (Bouamor et al., 2014) which is an adaptation of BLEU that gives partial credits for stem and morphological matchings of hypothesis and reference words. Here, in addition to using lexical infor"
C16-1132,P14-5010,0,0.00285382,"xtracted part-of-speech (POS) tags according to different POS tagsets including: (i) CAT I BPOS(Habash et al., 2009), (ii) K ULICK POS1 (Kulick et al., 2006), (iii) B UCKWALTER POS (Buckwalter, 2004) and (iv) S TANFORD POS tagsets. These tagsets differ in their richness and complexity they capture. CAT I BPOS is the simplest with only 6 base tags2 , B UCKWALTER POS is the richest with 485 base tags, and K ULICK POS and S TANFORD POS come inbetween with 43 and 32 base tags, respectively. These tags were extracted using MADAMIRA, except for the Stanford tags, for which we used Stanford CoreNLP (Manning et al., 2014). Table 1 illustrates an example sentence with its lexical and morpho-syntactic features.3 ˇ bldhm − ςAd AlmSrywn Alðyn AxtTfwA Alý Sentence: TOKEN NORM Ñë+ +hm bld YÊK. ˇ Alý Ñë+ YÊK. ú Í@ Aly úÍ@ ˇ +hm bld YÊK . Ñë + LEMMA úÍ@ Jk@ @ñ®¢ AxtTfwA Jk@ @ñ®¢ AxtTfwA  ¢ Jk@ QåÖÏ@ XA« Jk@ áK YË@ ÑëYÊK. úÍ@ @ñ®¢ àñK àñK QåÖÏ@ áK YË@ XA« Alðyn AlmSrywn ςAd QåÖÏ@ áK YË@ àñK XA« Alðyn  ø Y Ë@ AlmSrywn ø Q åÓ XA« +hum balad Ailaý Aix.taTaf Al∼aðiy CAT I BPOS +NOM NOM PRT VRB-PASS NOM NOM VRB K ULICK POS +PRP$ NN IN VBN WP DT+NNS VBD S TANFORD POS PRP$ NN IN VBN WP DTNNS VBD +POSS _PRON _3MP NO"
C16-1132,C12-1121,1,0.84926,"¨ ¨ ¬   È Ð à è ð ø Â b t θ j H x d ð r z s š S D T Dˇ ς γ f q k l m n h w y, and 2 . .  w, ˇ @ A, ¯ ð' ˆ the additional symbols: Z ’, @ Â, @ A, Zø' yˆ , è ~, ø ý. Diacritics are represented as:  a,  u,  i,  .,  ã,  u˜ ,  ˜ı, and  ∼. 1401 obtain sentence-level representations for each of the translations and the references, we used additive composition (Mitchell and Lapata, 2010) with dropping unknown words. N-gram MT metrics We used the different n-gram based metrics to serve as a benchmark, and as additional features that capture lexical similarity. We used: BLEU+1 (Nakov et al., 2012),NIST (Doddington, 2002); M ETEOR (Denkowski and Lavie, 2011), 1-TER (Snover et al., 2006), and AL-BLEU (Bouamor et al., 2014), to compute scores at the sentence-level. For consistency with previous work, we report scores over words, and not over morphemes. 4 Experimental Setup In this section, we describe the experimental settings we used through our study. First, we introduce our evaluation criteria, then we elaborate on the dataset and various settings we used for our experiments. 4.1 Performance evaluation Automatic evaluation metrics are evaluated based on their correlation with human-per"
C16-1132,pasha-etal-2014-madamira,1,0.889937,"Missing"
C16-1132,2006.amta-papers.25,0,0.0619799,"nd 2 . .  w, ˇ @ A, ¯ ð' ˆ the additional symbols: Z ’, @ Â, @ A, Zø' yˆ , è ~, ø ý. Diacritics are represented as:  a,  u,  i,  .,  ã,  u˜ ,  ˜ı, and  ∼. 1401 obtain sentence-level representations for each of the translations and the references, we used additive composition (Mitchell and Lapata, 2010) with dropping unknown words. N-gram MT metrics We used the different n-gram based metrics to serve as a benchmark, and as additional features that capture lexical similarity. We used: BLEU+1 (Nakov et al., 2012),NIST (Doddington, 2002); M ETEOR (Denkowski and Lavie, 2011), 1-TER (Snover et al., 2006), and AL-BLEU (Bouamor et al., 2014), to compute scores at the sentence-level. For consistency with previous work, we report scores over words, and not over morphemes. 4 Experimental Setup In this section, we describe the experimental settings we used through our study. First, we introduce our evaluation criteria, then we elaborate on the dataset and various settings we used for our experiments. 4.1 Performance evaluation Automatic evaluation metrics are evaluated based on their correlation with human-performed evaluations (Soricut and Brill, 2004). In this work, we use Kendall’s τ , a coeffic"
C16-1132,P13-1045,0,0.0302945,"mán et al., 2015; Gupta et al., 2015). Our work is related to Guzmán et al. (2015), in several levels of lexical, syntactic and semantic are combined in a compact fashion using a pairwise neural framework. There are several differences between that work and ours: (i) we do not use syntactic embedding representations, (ii) we include additional pairwise features, namely the pairwise cosine similarity between embeddings; and (ii) we focus on an MRL language. While use of syntactic representations has proven a useful component to evaluate English, it relies heavily on an syntactic neural parser (Socher et al., 2013), which increases the complexity of the evaluation setup, and is not readily available for every language. Here, we instead use morpho-syntactic representations which capture both syntactic and morphological aspects of language. In our experiments, these simple representations are powerful enough to provide state-of-the-art performance. In this work, we use neural network models to improve MT evaluation into Arabic using representations that capture morphology. Morphological structure has been shown to improve the quality of word clusters (Clark, 2003), word vector representations (Cotterell a"
C16-1132,P04-1078,0,0.0560723,"002); M ETEOR (Denkowski and Lavie, 2011), 1-TER (Snover et al., 2006), and AL-BLEU (Bouamor et al., 2014), to compute scores at the sentence-level. For consistency with previous work, we report scores over words, and not over morphemes. 4 Experimental Setup In this section, we describe the experimental settings we used through our study. First, we introduce our evaluation criteria, then we elaborate on the dataset and various settings we used for our experiments. 4.1 Performance evaluation Automatic evaluation metrics are evaluated based on their correlation with human-performed evaluations (Soricut and Brill, 2004). In this work, we use Kendall’s τ , a coefficient that measures the agreement between rankings produced by human judgments and rankings produced by an automatic metric, at the sentence-level. We use the WMT’12 (workshop of machine translation) definition of Kendall’s τ that ignores ties, and is calculated as follows: [τ = (# concordant pairs − # discordant pairs) /total pairs], where the # concordant pairs is the number of times the human judgment and the automatic metric agree in the ranking of any two translations that belong to the same source sentence. The # discordant pairs is the opposi"
D14-1027,W14-3352,1,0.726645,"Missing"
D14-1027,W07-0718,0,0.384691,"Missing"
D14-1027,W11-2103,0,0.0593209,"ourse parser can be downloaded from http://alt.qcri.org/tools/ 216 In particular, let r and r0 be the references for the pairs ht1 , t2 i and ht01 , t02 i, we can redefine all the members of Eq. 1, e.g., K(t1 , t01 ) becomes K(ht1 , ri, ht01 , r0 i) = PTK(φM (t1 , r), φM (t01 , r0 )) + PTK(φM (r, t1 ), φM (r0 , t01 )), In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (τ ), which was official at WMT12. Table 1 presents the τ scores for all metric variants introduced in this paper: for the individual language pairs"
D14-1027,W05-0904,0,0.114217,"ndation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans hav"
D14-1027,W12-3102,0,0.167189,"Missing"
D14-1027,W12-3129,0,0.108627,".org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translatio"
D14-1027,W10-1750,1,0.877508,"Missing"
D14-1027,P07-1098,1,0.765065,"ework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are"
D14-1027,W08-0331,0,0.225781,"y, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the fe"
D14-1027,W07-0738,1,0.935253,"Missing"
D14-1027,P14-1065,1,0.878614,"Missing"
D14-1027,P02-1040,0,0.0928706,"ranslations Francisco Guzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges"
D14-1027,D12-1083,1,0.785191,"relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feat"
D14-1027,W07-0707,0,0.511133,"Missing"
D14-1027,P13-1048,1,0.81161,"L "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feature space. Consideri"
D14-1027,C14-1020,1,0.886934,"Missing"
D14-1027,D14-1050,1,0.886979,"Missing"
D14-1027,W13-3509,1,0.930152,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,P13-2125,1,0.924292,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,2006.amta-papers.25,0,0.625255,"uzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some ev"
D14-1027,W11-2113,0,0.0478224,"re widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically."
D14-1027,N03-1033,0,0.00879476,"o-REL .-REL &apos;&apos;-REL to think . "" to think . "" relation propagation direction DIS:ELABORATION Bag-of-words relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0"
D14-1027,D12-1097,0,0.0991997,"elopment in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy"
D14-1027,P06-1051,1,\N,Missing
D16-1165,S16-1138,0,0.0817602,"Missing"
D16-1165,J93-2003,0,0.046238,"ceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, which takes as input the original question and two comments together with thei"
D16-1165,P15-2114,0,0.286591,"h q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subtask B, respectively. In this paper, we mainly"
D16-1165,P08-1019,0,0.141574,"mple, q and q 0 are indeed related, and c is a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presente"
D16-1165,P03-1003,0,0.125068,"input components. It does so in a modular kernel function, including stacking from independent subtask A and B classifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of s"
D16-1165,S16-1172,0,0.37884,"provide labeled examples for the so called “subtask A” (q 0 c; appropriateness) and “subtask B” (qq 0 ; relatedness), one could use this supervision to help train the neural network for the primary cQA task. We observed that relatedness has proven quite informative. However, the improvements observed from using appropriateness were more modest. 10 As measured by the relative drop in MAP performance. System MAP AvgRec MRR System MAP AvgRec MRR Full Network Full + appr. preds. 54.51 55.82 60.93 61.63 62.94 62.39 Full Network + subtask A preds. * 1st (Mihaylova et al., 2016) Full Network * 2nd (Filice et al., 2016) * 3rd (Mihaylov and Nakov, 2016b) ... SemEval Average ... SemEval Worst 55.82 55.41 54.51 52.95 51.68 ... 49.30 ... 43.20 Baseline 2 (IR+chron.) 40.36 45.97 45.83 Table 3: Using appropriateness predictions. We present here a stacked experiment in which an additional neural network trained to predict appropriateness is used to inform the full network model. More concretely, we train a feed-forward pairwise neural network for subtask A, which is a simplification of the architecture from Figure 2. The input is reduced to three elements (q 0 , c1 , c2 ), where q 0 is the thread question and c1 an"
D16-1165,P15-1078,1,0.879885,"Missing"
D16-1165,P16-2075,1,0.638784,"Missing"
D16-1165,S16-1137,1,0.835795,"Missing"
D16-1165,S16-1131,0,0.0212185,"od vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, wh"
D16-1165,P11-1143,0,0.142772,"indeed related, and c is a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subta"
D16-1165,P16-2065,1,0.915419,"t the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way. This search can be h"
D16-1165,S16-1136,1,0.898572,"t the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way. This search can be h"
D16-1165,K15-1032,1,0.799997,"types are relevant, but the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different"
D16-1165,S16-1129,1,0.836098,"Missing"
D16-1165,N13-1090,0,0.0460853,"ion, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply averaging over the embeddings of all words in the text. Features We experiment with three kinds of features: (i) lexical features that measure similarity at a word, word n-gram, and paraphrase level, (ii) distributed representations that measure similarity at a syntactic and semantic level, (iii) domain-specific knowledge features, which capture similarity using thread-level information and other features that have proven valuable to solve similar tasks (Nicosia et al., 2015). 4.1 Lexical similarity features These types of features mea"
D16-1165,S16-1128,0,0.0167536,"of AvgRec and MRR. Note that, even without the Subtask A predictions, our pairwise neural network still produces results that are on par with the state of the art (with improvements slightly over one point in both cases). 6 Related Work Recently, a variety of neural network models have been applied to community question answering tasks such as question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2015) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Feng et al., 2015; Tan et al., 2015; Filice et al., 2016; Barr´on-Cede˜no et al., 2016; Mohtarami et al., 2016). Most of these papers concentrate on constructing advanced neural network architectures in order to model the problem at hand better. For instance, dos Santos et al. (2015) propose a neural network approach combining a convolutional neural network and a bag-of-words representation for modeling question-question similarity. Similarly, Tan et al. (2015) adopt a neural attention mechanism over bidirectional long short-term memory (LSTM) neural network to generate better answer representations given the questions. Similarly, Lei et al. (2015) use a combination of recurrent and convolutional neura"
D16-1165,S15-2047,1,0.902818,"Missing"
D16-1165,S15-2036,1,0.912003,"Missing"
D16-1165,P02-1040,0,0.104411,"ivation at the output is f (q, q10 , c1 , q20 , c2 ) = sig(wvT [φ(q, q10 , c1 , q20 , c2 ), ψ(q, q10 ), ψ(q, q20 ), ψ(q10 , c1 ), ψ(q20 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). We use these feature vectors to encode machine translation evaluation measures, components thereof, cQA task-specific features, etc. The next section gives more detail about these features. MT FEATS We use (as pairwise features) the following six machine translation evaluation features: (i) B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply"
D16-1165,P07-1059,0,0.107973,"stacking from independent subtask A and B classifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, a"
D16-1165,2006.amta-papers.25,0,0.0717448,"ponents thereof, cQA task-specific features, etc. The next section gives more detail about these features. MT FEATS We use (as pairwise features) the following six machine translation evaluation features: (i) B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply averaging over the embeddings of all words in the text. Features We experiment with three kinds of features: (i) lexical features that measure similarity at a word, word n-gram, and paraphrase level, (ii) distributed representations that measure similari"
D16-1165,P13-1045,0,0.0380397,"of the reference, length ratio between them, and BLEU’s brevity penalty. Again, these are computed over the same six pairs of vectors as before. 4.2 Distributed representations We use the following vector-based embeddings of all input components: q, c1 , c2 , q10 , and q20 . QL VEC We train in-domain word embeddings using WORD 2 VEC on all available QatarLiving data. Again, we use these embeddings to compute 100dimensional vector representations for all input components by averaging over all words in the texts. S YNTAX VEC We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25dimensional vector that is produced internally as a by-product of parsing. Moreover, we use the above vectors to calculate pairwise similarity features, i.e., the cosine between the following six vector pairs: (q, c1 ), (q, c2 ), (q10 , c1 ), (q20 , c2 ), (q, q10 ) and (q, q20 ). 4.3 Domain-specific features We extract various domain-specific features that use thread-level and other useful information known to capture relatedness and appropriateness. S AME AUTHOR We have a thread-level metafeature, which we apply to the pairs (q10 , c1 ), (q20 , c2 ). It checks whether"
D16-1165,J11-2003,0,0.0785057,"sifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise f"
D16-1165,S15-2038,0,0.0219871,"SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural"
D16-1165,P15-2116,0,0.0859259,"Missing"
D16-1165,S16-1132,0,0.025907,"ilice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, which takes as input th"
D16-1165,P15-1025,0,0.0858496,"s a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subtask B, respectively."
D19-1632,buck-etal-2014-n,0,0.109306,"Missing"
D19-1632,W19-5435,1,0.90476,"data are available for Sinhala–English and Nepali–English. Statistics can be found in Table 2. This data comes from different sources. Open Subtitles and GNOME/KDE/Ubuntu come from the OPUS repository7 . Global Voices is an updated version (2018q4) of a data set originally created for the CASMACAT project8 . Bible translations come from the bible-corpus9 . The Paracrawl corpus comes from the Paracrawl project10 . The filtered version (Clean Paracrawl) was generated using the LASER model (Artetxe and Schwenk, 2018) to get the best sentence pairs having 1 million English tokens as specified in Chaudhary et al. (2019). We also contrast this filtered version with a randomly filtered version (Random Paracrawl) with the same number of English tokens. Finally, our multilingual experiments in Nepali use Hindi monolingual (about 5 million sentences) and EnglishHindi parallel data (about 1.5 million parallel sentences) from the IIT Bombay corpus11 . 4.2 Training Settings We evaluate models in four training settings. First, we consider a fully supervised training setting using the parallel data listed in Table 2. Second, we consider a fully unsupervised setting, whereby only monolingual data on both the source and"
D19-1632,D18-1045,1,0.858046,"only in the weakly-supervised experiments since alignments are noisy. (back-translated) source sentences with the original target sentences and add them as additional parallel data for training source-to-target MT system. Since monolingual data is available for both languages, we train backward MT systems in both directions and repeat the back-translation process iteratively (He et al., 2016; Lample et al., 2018a). We consider up to two back-translation iterations. At each iteration we generate back-translations using beam search, which has been shown to perform well in low-resource settings (Edunov et al., 2018); we use a beam width of 5 and individually tune the length-penalty on the dev set. Finally, we consider a weakly supervised setting by using a baseline system to filter out Paracrawl data using LASER (Artetxe and Schwenk, 2018) by following the approach similar to Chaudhary et al. (2019), in order to augment the original training set with a possibly larger but noisier set of parallel sentences. 6102 For Nepali only, we also consider training using Hindi data, both in a joint supervised and semi-supervised setting. For instance, at each iteration of the joint semi-supervised setting, we use mo"
D19-1632,W17-3204,1,0.850788,"l., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., ♥ Equal contribution. 2013; Bojar et al., 2017). Indeed, modern neural MT systems can achieve near human-level translation performance on language pairs for which sufficient parallel training resources exist (e.g., Chinese–English translation (Hassan et al., 2018) and English–French translation (Gehring et al., 2016; Ott et al., 2018a). Unfortunately, MT systems, and in particular neural models, perform poorly on low-resource language pairs, for which parallel training data is scarce (Koehn and Knowles, 2017). Improving translation performance on low-resource language pairs could be very impactful considering that these languages are spoken by a large fraction of the world population. Technically, there are several challenges to solve in order to improve translation for lowresource languages. First, in face of the scarcity of clean parallel data, MT systems should be able to use any source of data available, namely monolingual resources, noisy comparable data, as well as parallel data in related languages. Second, we need reliable public evaluation benchmarks to track progress in translation quali"
D19-1632,N19-4009,1,0.823346,"upervised task (in both directions). 4.3 Models & Architectures We consider both phrase-based statistical machine translation (PBSMT) and neural machine translation (NMT) systems in our experiments. All hyper-parameters have been cross-validated using the dev set. The PBSMT systems use Moses (Koehn et al., 2007), with state-of-theart settings (5-gram language model, hierarchical lexicalized reordering model, operation sequence model) but no additional monolingual data to train the language model. The NMT systems use the Transformer (Vaswani et al., 2017) implementation in the Fairseq toolkit (Ott et al., 2019); preliminary experiments showed these to perform better than LSTM-based NMT models. More specifically, in the supervised setting, we use a Transformer architecture with 5 encoder and 5 decoder layers, where the number of attention heads, embedding dimension and inner-layer dimension are 2, 512 and 2048, respectively. In the semi-supervised setting, where we augment our small parallel training data with millions of back-translated sentence pairs, we use a larger Transformer architecture with 6 encoder and 6 decoder layers, where the number of attention heads, embedding dimension and inner-laye"
D19-1632,W18-6301,1,0.93604,"nces in recent years thanks to improvements in modeling, and in particular neural models (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017), as well as the availability of large parallel corpora for training (Tiedemann, 2012; Smith et al., ♥ Equal contribution. 2013; Bojar et al., 2017). Indeed, modern neural MT systems can achieve near human-level translation performance on language pairs for which sufficient parallel training resources exist (e.g., Chinese–English translation (Hassan et al., 2018) and English–French translation (Gehring et al., 2016; Ott et al., 2018a). Unfortunately, MT systems, and in particular neural models, perform poorly on low-resource language pairs, for which parallel training data is scarce (Koehn and Knowles, 2017). Improving translation performance on low-resource language pairs could be very impactful considering that these languages are spoken by a large fraction of the world population. Technically, there are several challenges to solve in order to improve translation for lowresource languages. First, in face of the scarcity of clean parallel data, MT systems should be able to use any source of data available, namely monoli"
D19-1632,P02-1040,0,0.108923,"extracted from Wikipedia articles in each language and translated by professional translators. The datasets we release to the community are composed of a tune set of 2559 and 2898 sentences, a development set of 2835 and 2766 sentences, and a test set of 2924 and 2905 sentences for Nepali–English and Sinhala–English respectively. In §3, we describe the methodology we used to collect the data as well as to check the quality of translations. The experiments reported in §4 demonstrate that these benchmarks are very challenging for current state-of-the-art methods, yielding very low BLEU scores (Papineni et al., 2002) even using all available parallel data as well as monolingual data or Paracrawl1 filtered data. This suggests that these languages and evaluation benchmarks can constitute a useful test-bed for developing and comparing MT systems for lowresource language pairs. 2 Related Work There is ample literature on low-resource MT. From the modeling side, one possibility is to design methods that make more effective use of monolingual data. This is a research avenue that has seen a recent surge of interest, starting with semisupervised methods relying on backtranslation (Sennrich et al., 2015), integrat"
D19-1632,W18-6319,0,0.195431,"ze Nepali and Sinhala using the Indic NLP Library.12 For the PBSMT system, we tokenize English sentences using the Moses tokenization scripts. For NMT systems, we instead use a vocabulary of 5K symbols based on a joint source and target Byte-Pair Encoding (BPE; Sennrich et al., 2015) learned using the sentencepiece library13 over the parallel training data. We learn the joint BPE for each language pair over the raw English sentences and tokenized Nepali or Sinhala sentences. We then remove training sentence pairs with more than 250 source or target BPE tokens. We report detokenized SacreBLEU (Post, 2018) when translating into English, and tokenized BLEU (Papineni et al., 2002) when translating from English into Nepali or Sinhala. 4.5 Results In the supervised setting, PBSMT performed quite worse than NMT, achieving BLEU scores of 2.5, 4.4, 1.6 and 5.0 on English–Nepali, Nepali– English, English–Sinhala and Sinhala–English, respectively. Table 3 reports results using NMT in all the other learning configurations described in §4.2. There are several observations we can make. First, these language pairs are very difficult, as even supervised NMT baselines achieve BLEU scores less than 8. Second a"
D19-1632,W12-3152,0,0.0351591,", 2016) have collected translations on several lowresource languages like English–Tagalog. Unfortunately, the data is only made available to the program’s participants. More recently, the Asian Language Treebank project (Riza et al., 2016) has introduced parallel datasets for several low-resource language pairs, but these are sampled from text originating in English and thus may not generalize to text sampled from low-resource languages. In the past, there has been work on extracting high quality translations from crowd-sourced workers using automatic methods (Zaidan and Callison-Burch, 2011; Post et al., 2012). However, crowd-sourced translations have generally lower quality than professional translations. In contrast, in this work we explore the quality checks that are required to filter professional translations of lowresource languages in order to build a high quality benchmark set. In practice, there are very few publicly available datasets for low-resource language pairs, and often times, researchers simulate learning on lowresource languages by using a high-resource language pair like English–French, and merely limiting how much labeled data they use for training (Johnson et al., 2016; Lample"
D19-1632,P11-1122,0,0.102719,"Missing"
D19-1632,P07-2045,1,\N,Missing
D19-1632,Q17-1010,0,\N,Missing
D19-1632,Q17-1024,0,\N,Missing
D19-1632,L16-1521,0,\N,Missing
D19-1632,P17-1042,0,\N,Missing
D19-1632,W17-4717,1,\N,Missing
D19-1632,D17-1319,1,\N,Missing
D19-1632,D18-1103,0,\N,Missing
D19-1632,D18-1399,0,\N,Missing
D19-1632,W18-6321,0,\N,Missing
D19-1632,P16-1009,0,\N,Missing
D19-1632,tiedemann-2012-parallel,0,\N,Missing
D19-1632,P13-1135,1,\N,Missing
D19-1632,W13-2305,0,\N,Missing
D19-1632,L18-1275,0,\N,Missing
J17-4001,D14-1188,0,0.0242496,"et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, a"
J17-4001,P13-2068,0,0.126108,"ot until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gr"
J17-4001,W16-2302,0,0.0340023,"Missing"
J17-4001,J93-2003,0,0.0452676,"Missing"
J17-4001,W07-0718,0,0.047654,"d cohesion. In Section 4, we have suggested some simple ways to create such metrics, and we have also shown that they yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metri"
J17-4001,W08-0309,0,0.0358878,"y yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez 2007; Popovic"
J17-4001,W10-1703,0,0.0860047,"Missing"
J17-4001,W12-3102,0,0.454393,"tion on the training data set.5 Note that our approach to learn the interpolation weights is similar to the one used by PRO for tuning the relative weights of the components of a log-linear SMT model (Hopkins and May 2011). Unlike PRO, (i) we used human judgments, not automatic scores, and (ii) we trained on all pairs, not on a subsample. 3.3 Correlation Measures In our experiments, we only considered translation into English (as we had a discourse parser for English only), and we used the data described in Table 1. For evaluation, we followed the standard set-up of the Metrics task of WMT12 (Callison-Burch et al. 2012). For segment-level evaluation, we used Kendall’s τ (Kendall 1938), which can be 5 When fitting the model, we did not include a bias term, as this was harmful. 692 Joty et al. Discourse Structure in Machine Translation Evaluation calculated directly from the human pairwise judgments. For system-level evaluation, we used Spearman’s rank correlation (Spearman 1904) and, in some cases, also Pearson correlation (Pearson 1895), which are appropriate correlation measures as here we have vectors of scores. We measured the correlation of the evaluation metrics with the human judgments provided by the"
J17-4001,W09-0401,0,0.0707378,"Missing"
J17-4001,W11-2103,0,0.0547186,"Missing"
J17-4001,E06-1032,0,0.0857549,"Missing"
J17-4001,W09-2404,0,0.0325022,"Discourse in Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2"
J17-4001,W12-3156,0,0.0221975,"Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational L"
J17-4001,D07-1007,0,0.126996,"Missing"
J17-4001,W11-1211,0,0.0252185,"010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despit"
J17-4001,P07-1005,0,0.124229,"Missing"
J17-4001,P05-1033,0,0.0907183,"systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-senten"
J17-4001,D08-1024,0,0.100033,"Missing"
J17-4001,2003.mtsummit-papers.9,0,0.0284114,"mmunity. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm"
J17-4001,2003.mtsummit-papers.10,0,0.151421,"iciently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b;"
J17-4001,W11-2107,0,0.0349766,"ally significant improvements are marked with ∗∗ for p-value < 0.01. System D ISCO TKparty Best at WMT 14 FR - EN DE - EN HI - EN CS - EN RU - EN Overall 0.433∗∗ 0.417 +0.016 0.380∗∗ 0.345 +0.035 0.434 0.438 −0.004 0.328∗∗ 0.284 +0.044 0.355∗∗ 0.336 +0.019 0.386∗∗ 0.364 +0.024 the best performing metric both at the system level and at the segment level at the WMT08 and WMT09 metrics tasks. From the original ULC, we replaced M ETEOR by the four newer variants M ETEOR-ex (exact match), M ETEOR-st (+stemming), M ETEOR-sy (+synonymy lookup), and M ETEOR-pa (+paraphrasing) in A SIYA’s terminology (Denkowski and Lavie 2011). We also added to the mix TERp-A (a variant of TER with paraphrasing), BLEU, NIST, and R OUGE-W, for a total of 18 individual metrics. The metrics in this set use diverse linguistic information, including lexical-, syntactic-, and semantic-oriented individual metrics. Regarding the discourse metrics, we used five variants, including DR and DR-LEX described in Section 2, and three more constrained variants oriented to match words between trees only if they occur under the same substructure types (e.g., the same nuclearity type). These variants are designed by introducing structural modificatio"
J17-4001,2012.amta-papers.6,0,0.0150667,"a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovi"
J17-4001,N04-1035,0,0.0647715,"nformation (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual informa"
J17-4001,W07-0738,1,0.707228,"Missing"
J17-4001,W09-0440,1,0.876974,"Missing"
J17-4001,H93-1040,0,0.672723,"Missing"
J17-4001,W10-1750,1,0.808317,"Missing"
J17-4001,D11-1084,0,0.0630017,"Missing"
J17-4001,P12-3024,1,0.890983,"Missing"
J17-4001,E12-3001,0,0.0251663,"istency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. th"
J17-4001,W13-3302,0,0.0210552,"; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling"
J17-4001,W16-2345,1,0.883757,"Missing"
J17-4001,D14-1027,1,0.899067,"Missing"
J17-4001,P14-1065,1,0.865351,"Missing"
J17-4001,P15-1078,1,0.877827,"Missing"
J17-4001,W13-2252,0,0.0241376,"ch is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 2013). Thus, specialized evaluation measures have been proposed, for example, for the translation of discourse connectives (Hajlaoui and Popescu-Belis 2012; Meyer et al. 2012; Hajlaoui 2013) and for pronominal anaphora (Hardmeier and Federico 2010), among others. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few previous attempts to incorporate discourse information. One example includes the semantics-aware metrics of Gim´enez and M`arquez (2009) and Gim´enez et al. (2010), which used the Discourse Representation Theory (Kamp and Reyle 1993) and tree-based discourse representation structures (DRS) produced by a semantic parser. They calculated the similarity between the MT output and the references based on DRS subtree matching as defi"
J17-4001,2012.amta-caas14.1,0,0.0723287,"gt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT"
J17-4001,2010.iwslt-papers.10,0,0.146919,"esearch problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Har"
J17-4001,W15-2501,1,0.905816,"Missing"
J17-4001,D12-1108,0,0.0554427,"Missing"
J17-4001,D11-1125,0,0.347504,"es 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently tha"
J17-4001,D12-1083,1,0.888624,"Missing"
J17-4001,J15-3002,1,0.897197,"Missing"
J17-4001,W14-3352,1,0.889983,"Missing"
J17-4001,N03-1017,0,0.0235221,"Missing"
J17-4001,W07-0734,0,0.0592291,"the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of li"
J17-4001,W10-1737,0,0.0859654,"Missing"
J17-4001,D14-1220,0,0.0691207,"Missing"
J17-4001,P14-2047,0,0.0404142,"Missing"
J17-4001,W04-1013,0,0.011816,"ation groups: Group I contains our discourse-based evaluation metrics, DR, and DR-LEX. Group II includes the publicly available MT evaluation metrics that participated in the WMT12 metrics task, excluding those that did not have results for all language pairs (Callison-Burch et al. 2012). More precisely, they are SPEDE 07 P P, AMBER, M ETEOR, T ERROR C AT, SIMPBLEU, XE N E RR C ATS, W ORD B LOCK EC, B LOCK E RR C ATS, and POS F. Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (Papineni et al. 2002), NIST (Doddington 2002), R OUGE (Lin 2004), and TER (Snover et al. 2006). We calculated the metrics in this group using Asiya. In particular, we used the following Asiya versions of TER and R OUGE: TER P -A and ROUGE- W.8 For each metric in groups II and III, we present the system-level and segment-level results for the original metric as well as for the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results y"
J17-4001,W05-0904,0,0.34905,"post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sen"
J17-4001,W12-3129,0,0.0531251,"Missing"
J17-4001,P11-1023,0,0.0237776,"between constituency trees (Liu and Gildea 2005). In the semantic case, there are metrics that 16 A notable exception is the work of Tu, Zhou, and Zong (2013), who report up to 2.3 BLEU points of improvement for Chinese-to-English translation using an RST-based MT framework. 17 http://www.isi.edu/natural-language/mteval/. 18 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/. 712 Joty et al. Discourse Structure in Machine Translation Evaluation exploit the similarity over named entities, predicate–argument structures (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), or semantic frames (Lo and Wu 2011). Finally, there are metrics that combine several lexico-semantic aspects (Gim´enez and M`arquez 2010b). As we mentioned earlier, one problem with discourse-related MT research is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 20"
J17-4001,E14-1017,0,0.0578767,"that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the"
J17-4001,W13-2202,0,0.0551203,"Missing"
J17-4001,W14-3336,0,0.0496545,"Missing"
J17-4001,A00-2002,0,0.288346,"Missing"
J17-4001,P11-3009,0,0.0273654,"ang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the resear"
J17-4001,W13-3306,0,0.032096,"Missing"
J17-4001,W12-0117,0,0.023287,"Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate"
J17-4001,2012.amta-papers.20,0,0.112933,"u 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far"
J17-4001,W13-3303,0,0.0173794,"Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that c"
J17-4001,W13-2221,0,0.0481696,"Missing"
J17-4001,W13-3307,0,0.04636,"Missing"
J17-4001,P03-1021,0,0.159662,"Missing"
J17-4001,P02-1040,0,0.0997549,"been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has"
J17-4001,P09-2004,0,0.0245932,"al cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Mac"
J17-4001,popescu-belis-etal-2012-discourse,0,0.0540549,"Missing"
J17-4001,W07-0707,0,0.0583959,"2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sentence and between sentences in a text have not been"
J17-4001,prasad-etal-2008-penn,0,0.0680612,"man assessments; (ii) Different levels of discourse structure and relations provide different information, which shows smooth accumulative contribution to the final correlation score; (iii) Both discourse relations and nuclearity labels have sizeable impact on the evaluation metric, the latter being more important than the former. The last point emphasizes the appropriateness of the RST theory as a formalism for the discourse structure of texts. Contrary to other discourse theories (e.g., the Discourse Lexicalized Tree Adjoining Grammar [Webber 2004] used to build the Penn Discourse Treebank [Prasad et al. 2008]), RST accounts for the nuclearity as an important element of the discourse structure. 5.3 Qualitative Analysis of Good and Bad Translations In the previous two sections we provided a quantitative analysis of which discourse information has the biggest impact on the performance of our discourse-based measure (DR-LEX) and also which parts of the discourse trees help in distinguishing good from bad translations. In this section, we present some qualitative analysis by inspecting a 707 Computational Linguistics Volume 43, Number 4 WMT_2011 WMT_2012 WMT_2013 F1 measure 0.825 0.800 good bad 0.775"
J17-4001,P05-1034,0,0.192756,"Missing"
J17-4001,W03-0402,0,0.0123992,"urces of information in a more direct way. In that paper, we proposed a pairwise setting for learning MT evaluation metrics with preference tree kernels. The setting can incorporate syntactic and discourse information encapsulated in tree-based structures and the objective is to learn to differentiate better from worse translations by using all subtree structures as implicit features. The discourse parser we used is the same used in this article. The syntactic tree is mainly constructed using the Illinois chunker (Punyakanok and Roth 2001). The kernel used for learning is a preference kernel (Shen and Joshi 2003; Moschitti 2008), which decomposes into Partial Tree Kernel (Moschitti 2006) applications between pairs of enriched tree structures. Word unigram matching is also included in the kernel computation, thus being quite similar to DR-LEX. Table 8 shows the results obtained on the same WMT12 data set by using only discourse structures, only syntactic structures or both structures together. As we can see, the τ scores of the syntactic and the discourse variants are not very different (with a general advantage for syntax), but when put together there is a sizeable improvement in correlation for all"
J17-4001,2006.amta-papers.25,0,0.422386,"it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are u"
J17-4001,W15-3031,0,0.0560843,"Missing"
J17-4001,N15-2015,0,0.019606,"d Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that current automatic evaluation metrics such as B"
J17-4001,W12-4213,0,0.0584238,"Missing"
J17-4001,W10-2602,0,0.0143272,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,W10-1728,0,0.0292312,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,P13-2066,0,0.0534853,"Missing"
J17-4001,P14-1080,0,0.0440631,"Missing"
J17-4001,N12-1046,0,0.0614062,"Missing"
J17-4001,H05-1097,0,0.0561496,"creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and"
J17-4001,W12-2503,0,0.0257687,"ch had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu"
J17-4001,D07-1080,0,0.245176,"Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan"
J17-4001,D12-1097,0,0.0277845,"Missing"
J17-4001,N09-2004,0,0.0308979,"ive log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion an"
J17-4001,D13-1163,0,0.01503,"which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´a"
J17-4001,C00-2137,0,0.0572188,"r the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results yield statistically significant improvement over the original metric. Note that testing statistical significance is not trivial in our case because we have a complex correlation score for which the assumptions that standard tests make are not met. We thus resorted to a non-parametric randomization framework (Yeh 2000), which is commonly used in NLP research.9 4.1 System-Level Results Table 2 shows the system-level experimental results for WMT12. We can see that DR is already competitive by itself: On average, it has a correlation of 0.807, which is very close to the BLEU and the TER scores from group II (0.810 and 0.812, respectively). Moreover, DR yields improvements when combined with 13 of the 15 metrics, with a resulting correlation higher than those of the two individual metrics being combined. This fact suggests that DR contains information that is complementary to that used by most of the other metr"
J17-4001,W14-3302,0,\N,Missing
K15-1007,N12-1062,0,0.16668,"SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and"
K15-1007,N15-1106,0,0.0232565,"lar include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emer"
K15-1007,W08-0304,0,0.0236305,", 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as we"
K15-1007,P10-4002,0,0.0542157,"Missing"
K15-1007,2011.mtsummit-papers.1,0,0.0267411,"rsion of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence len"
K15-1007,N12-1047,0,0.0364323,"), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et"
K15-1007,N04-1035,0,0.0444567,"Missing"
K15-1007,D08-1024,0,0.400262,"Missing"
K15-1007,N12-1023,0,0.102217,"MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been"
K15-1007,N09-1025,0,0.112274,"Missing"
K15-1007,W11-2123,0,0.0394859,"ll WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ensure convergence, we allowed both MERT and PRO to run for up to 25 iterations (default: 16); we further used 1000best lists (default: 1"
K15-1007,D11-1125,0,0.0434425,"Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains"
K15-1007,P05-1033,0,0.371943,"Missing"
K15-1007,N03-1017,0,0.0487953,"rences makes both MERT and PRO appear more stable, allowing them to generate hypotheses that are less spread, and closer to 1. This can be attributed to the best match reference length, which naturally dampens the effect of verbosity during optimization by selecting the reference that is closest to the respective hypothesis. Overall, we can conclude that MERT learns the tuning set’s verbosity more accurately than PRO. PRO learns verbosity that is more dependent on the source side length of the sentences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothi"
K15-1007,P11-2031,0,0.395785,"(Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of"
K15-1007,2005.iwslt-1.8,0,0.117546,"Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ens"
K15-1007,P05-1066,0,0.065072,"Missing"
K15-1007,P03-1021,0,0.0297129,"anslator; it is often a stylistic choice. and not necessarily related to fluency or adequacy. This aspect is beyond the scope of the present work. 62 Proceedings of the 19th Conference on Computational Language Learning, pages 62–72, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Method For the following analysis, we need to define the following four quantities: Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (C"
K15-1007,P07-2045,0,0.00687606,"able, allowing them to generate hypotheses that are less spread, and closer to 1. This can be attributed to the best match reference length, which naturally dampens the effect of verbosity during optimization by selecting the reference that is closest to the respective hypothesis. Overall, we can conclude that MERT learns the tuning set’s verbosity more accurately than PRO. PRO learns verbosity that is more dependent on the source side length of the sentences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering mo"
K15-1007,P02-1040,0,0.0928671,"ved in singlereference sets (we used the first reference), and to a lesser extent in multiple-reference sets (five references for MT04 and MT05, and four for MT06 and MT09). For Spanish-English, the story is different: here the English sentences tend to be shorter than the Spanish ones, and the verbosity decreases as the sentence length increases. Overall, in all three cases, the verbosity appears to be length-dependent. 2 For multi-reference sets, we use the length of the reference that is closest to the length of the hypothesis. This is the best match length from the original paper on BLEU (Papineni et al., 2002); it is default in the NIST scoring tool v13a, which we use in our experiments. 3 When dealing with multi-reference sets, we use the average reference length. 4 The datasets we experiment with are described in more detail in Section 4 below. 63 Source length vs. avg. verbosity Arabic−English Spanish−English 1.025 set ● Ar−En−multi Ar−En−single Average verbosity Average verbosity 1.175 1.150 1.125 ● 1.100 ● ● ● ● ● ● ● ●● ●●●● ● ● ●●● ●● ● ●● ●● ● ●●●● ● ●●●● ●● ●● ● ●● ● ● ● ●● ●●● ● ●● ● ●● ● ●● ●●● ●● ● ● ● ● ●● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1.075 1.000 0.975 ● 0.950 ● ● ● ● ● ●"
K15-1007,P05-1034,0,0.111977,"Missing"
K15-1007,W09-0424,0,0.0539164,"Missing"
K15-1007,P08-2030,0,0.0896902,"ntences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al"
K15-1007,C10-1075,0,0.228135,"log-linear framework, and their values are optimized to maximize some automatic metric, typically BLEU, on a tuning dataset. Given this setup, it is clear that the choice of a tuning set and its characteristics, can have significant impact on the SMT system’s performance: if the experimental framework (training data, tuning set, and test set) is highly consistent, i.e., there is close similarity in terms of genre, domain and verbosity,1 then translation quality can be improved by careful selection of tuning sentences that exhibit high degree of similarity to the test set (Zheng et al., 2010; Li et al., 2010). In our recent work (Nakov et al., 2012), we have studied the relationship between optimizers such as MERT, PRO and MIRA, and we have pointed out that PRO tends to generate relatively shorter translations, which could lead to lower BLEU scores on testing. Our solution there was to fix the objective function being optimized: PRO uses sentence-level smoothed BLEU+1, as opposed to the standard dataset-level BLEU. Here we are interested in a related but different question: the relationship between properties of the tuning dataset and the optimizer’s performance. More specifically, we study how th"
K15-1007,P12-1002,0,0.0634812,"rk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. • source-side length: the number of words in the source sentence; • length ratio: the ratio of the number of words in the output hypothesis to those in the reference;2 • verbosity: the ratio of the number of words in the reference to those in the source;3 • hypothesis verbosity: the ratio of the number of words in the hypothesis to those in the source. Naturally, the verbosity varies across different tuning/testing datasets, e.g., because of style, translator choice, et"
K15-1007,D12-1037,0,0.0142596,"ere has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and"
K15-1007,2006.amta-papers.25,0,0.103422,"Missing"
K15-1007,W10-1738,0,0.02313,"Missing"
K15-1007,C08-1074,0,0.0259005,"al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer"
K15-1007,D07-1080,0,0.0742595,"–72, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Method For the following analysis, we need to define the following four quantities: Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We"
K15-1007,C12-1121,1,0.888761,"Missing"
K15-1007,P13-2003,1,0.728346,"Missing"
N16-1125,N16-3004,1,0.864659,"Missing"
N16-1125,W07-0718,0,0.0456341,"res given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hansen and Ji, 2010). The overall difficulty of a sentence and its syntactic complexity"
N16-1125,W12-3102,0,0.0857719,"Missing"
N16-1125,P11-1105,1,0.882545,"and distance between the start and end words. For subsequent words n, n + 1, this would mean a forward jump of distance equal to 1. All jumps with distance greater than 4 were sorted into a 5+ bucket. Additionally, we separate the features for reference and translation jumps. We also count the total number of jumps. Total jump distance We additionally aggregate jump distances2 to count the total distance covered while evaluating a sentence. We have reference distance and translation distance features. Again, the 2 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 1083 idea is that for a well-formed sentence, gaze distance should be less, compared to a poorly-formed one. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of transitions between reference and translation. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quali"
N16-1125,W13-2305,0,0.0177286,"was performed by 6 different evaluators, resulting in 720 evaluations. The annotators were presented with a translationreference pair at a time. The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between. This was done to prevent any possible spurious effects that may arise from remembering the content of a first translation, when evaluating the second translation of the same sentence. During each evaluation task, the evaluators were asked to assess the quality of a translation by providing a score between 0–100 (Graham et al., 2013). The observed inter-annotator agreement (Cohen’s kappa) among our annotators was 0.321. This is slightly higher than the overall inter-annotator agreement of 0.284 reported in WMT’12 for the Spanish-English.3 For reading patterns we use the EyeTribe eye-tracker at 3 For a rough comparison only. Note that these two numbers are not exactly comparable given that they are calculated on different subsets of the same data. Still, there is a fair agreement between the our evaluators and the expected wins from WMT’12 (avg. pairwise kappa of 0.381) 1084 Evaluation In our evaluation, we used eye-tracki"
N16-1125,W15-3059,1,0.875061,"Missing"
N16-1125,P02-1040,0,0.0970473,"n evaluation metric So far, we’ve shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency. One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi-automatically. That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation. Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (Papineni et al., 2002), which uses lexical information and is designed to measure not only fluency but also adequacy. In Table 2, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model. In (II) we re-introduce the results of lexicalized jumps feature. In (III) we present results of BLEU and the combination of eye-tracking features with it. Finally in (IV) we present the humanto-human agreement measured in average Kendall’s tau and in max human-to-human Kendall’s tau. Combinations of translation jumps In section I we pre"
N16-1125,2006.amta-papers.25,0,0.0363431,"patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hans"
N16-1125,stymne-etal-2012-eye,0,0.322545,"enomena remain to be explored in future work. Human performance On average, evaluators agreements with each other are fair (τ = 0.33) and below the best combination (CB3 ), while the maximum agreement of any two evaluators is relatively higher (τ = 0.53). This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human. However, there is still room for improvement with respect to the mostagreeing pair of evaluators. 5 Related Work Eye-tracking devices have been used previously in the MT research. Stymne et al. (2012) used eye-tracking to identify and classify MT errors. 1086 SYS Feature Sets τ I. Combination of translation jumps EyeTrabj Backward jumps CTJ1 Backward jumps, total jumps CTJ2 Backward jumps, total jumps, distance 0.22 0.25 0.27 II. Eye-tracking: Best Lexicalized EyeLexall Lexicalized gaze jumps 0.22 III. Combinations with BLEU Bbleu BLEU CB1 Bbleu + EyeTrabj CB2 Bbleu + CTJ2 CB3 Bbleu + EyeLexall 0.34 0.38 0.39 0.42 IV. Human performance Avg Avg. human-to-human agreement Max Max. human-to-human agreement 0.33 0.53 Table 2: Result of combining several jump and lexicalized features with BLEU."
N16-1125,2003.mtsummit-papers.51,0,0.111887,"ts show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 19"
N16-1125,1994.amta-1.25,0,0.757488,"Missing"
N16-3004,P11-1105,1,0.850927,"ump distances7 to count the total distance covered while evaluating a sentence. We count reference and translation distance features separately. Such information is useful in analyzing the complexity and readability of the translation. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of reference↔translation transitions. 7 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 20 Lexicalized features The features discussed above do not associate gaze movements with the words being read. We believe that this information can be critical to judge the overall difficulty of the reference sentence, and to evaluate which translation fragments are problematic to the reader. To compute the lexicalized features, we extract streams of reference and translation lexical sequences based on the gaze jumps, and score them using a tri-gram language model. Let Ri = r1 , r2 , . . . , rm be a sub-sequence of gaze movement over reference and there are R1 , R2 , . . . , Rn sequences, t"
N16-3004,W15-3059,1,0.502704,"Missing"
N16-3004,N16-1125,1,0.864864,"Missing"
N16-3004,stymne-etal-2012-eye,0,0.206082,"been adopted as the preferred tool in the WMT evaluation campaigns (Bojar et al., 2013), and thus, it is currently used by dozens of researchers. According to the eye-mind hypothesis (Just and Carpenter, 1980) people cognitively process objects that are in front of their eyes. This has enabled researchers to analyze and understand how people perform certain tasks like reading (Rayner, 1998; 1 It is subjective, expensive, time-consuming, boring, etc. Garrod, 2006; Harley, 2013). In recent times, eyetracking has also been used in Machine Translation to identify and classify translation errors (Stymne et al., 2012), to evaluate the usability of automatic translations (Doherty and O’Brien, 2014), and to improve the consistency of the human evaluation process (Guzm´an et al., 2015), etc. Furthermore, tracking how evaluators consume MT output, can help to reduce human evaluation subjectivity, as we could use evidence of what people do (i.e. unbiased reading patterns) and not only what they say they think (i.e. user-biased evaluation scores). However, the main limitation for the adoption of eye-tracking research has been the steep learning curve that is associated with eye-tracking analysis and the high-cos"
N16-3004,N13-1001,1,\N,Missing
N16-3004,J15-2001,1,\N,Missing
P13-2003,W08-0304,0,0.162532,"8 Lengths Neg Ref 229.0 52.5 48.5 48.70 48.4 48.9 47.6 48.4 47.8 48.6 48.0 48.7 48.0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Larg"
P13-2003,2005.iwslt-1.8,0,0.132545,"Missing"
P13-2003,2011.mtsummit-papers.1,0,0.292832,"3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focuse"
P13-2003,P07-2045,0,0.0178806,"Missing"
P13-2003,N12-1047,0,0.420009,"e of Figure 2: Example reference translation and hyto the affect that the of some is the with ] us our to the affect that the with ] us our of the in baker , the cook , the on and the , the we know , pothesis translations after iterations 1, 3 and 4. has are in the heaven of to the affect that the of weakness of @-@ Ittihad @-@ Al the force , to The last two hypotheses are monsters. Figure 2 shows the translations after iterations 1, 3 and 4; the last two are monsters. The monster at iteration 3 is potentially useful, but that at iteration 4 is clearly unsuitable as a negative example. 1 See (Cherry and Foster, 2012) for details on objectives. Also, using PRO to initialize MERT, as implemented in Moses, yields 46.52 BLEU and monsters, but using MERT to initialize PRO yields 47.55 and no monsters. 2 13 3 Slaying Monsters: Theory Cut-offs. A cut-off is a deterministic rule that filters out pairs that do not comply with some criteria. We experiment with a maximal cut-off on (a) the difference in BLEU+1 scores and (b) the difference in lengths. These are relative cut-offs because they refer to the pair, but absolute cut-offs that apply to each of the elements in the pair are also possible (not explored here)."
P13-2003,D08-1024,0,0.694161,"itable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on longer sentences, for w"
P13-2003,N04-1022,0,0.20397,"Missing"
P13-2003,N09-1025,0,0.176433,"47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, m"
P13-2003,C08-1074,0,0.152708,".70 48.4 48.9 47.6 48.4 47.8 48.6 48.0 48.7 48.0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained w"
P13-2003,P11-2031,0,0.0884574,"0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et"
P13-2003,C12-1121,1,0.824561,"Missing"
P13-2003,P03-1021,0,0.107668,"examples that are comparable to the positive ones. Instead, tuning on long sentences quickly introduces monsters, i.e., corrupted negative examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+"
P13-2003,W09-0439,0,0.0977443,"Missing"
P13-2003,P08-2030,0,0.146958,"Missing"
P13-2003,N12-1023,0,0.695369,"Missing"
P13-2003,P12-1002,0,0.292614,"al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, maybe because it gives them some probability of being selected by design. To alleviate this, we test the above settings with random acceptance. 4.3 Random Acceptance Table 4 shows the results for accepting training pairs for PRO uniformly at random. To eliminate possible biases, we also removed the min=0.05 BLEU+1 selection criterion. Surprisingly, this setup effectively eliminated the monster proble"
P13-2003,W11-2123,0,0.0476342,"Missing"
P13-2003,D11-1125,0,0.655498,"and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1based cut-offs, outlier filters, stochastic sampling, and random acceptance. The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved testtime BLEU scores. Thus, we recommend them to anybody using PRO, monsterbeliever or not. 1 2 Monsters, Inc. PRO uses pairwise ranking optimization, where the learning task is to classify pairs of hypotheses into correctly or incorrectly ordered (Hopkins and May, 2011). It searches for a vector of weights w such that higher evaluation metric scores correspond to higher model scores and vice versa. More formally, PRO looks for weights w such that g(i, j) &gt; g(i, j 0 ) ⇔ hw (i, j) &gt; hw (i, j 0 ), where g is a local scoring function (typically, sentencelevel BLEU+1) and hw are the model scores for a given input sentence i and two candidate hypotheses j and j 0 that were obtained using w. If g(i, j) &gt; g(i, j 0 ), we will refer to j and j 0 as the positive and the negative example in the pair. Learning good parameter values requires negative examples that are com"
P13-2003,D07-1080,0,0.412307,"examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on l"
P13-2003,N03-1017,0,0.0496158,"Missing"
P14-1065,W11-2103,0,0.0360688,"herence relations in the source language when generating target-language translations. In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (Joty et al., 2012); then, we show that they can help improve a number of MT evaluation metrics at the segment- and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanis"
P14-1065,N04-1035,0,0.0377299,"ve process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, an"
P14-1065,W12-3102,0,0.0260244,"Missing"
P14-1065,W07-0738,1,0.784963,"Missing"
P14-1065,W09-0440,1,0.927179,"Missing"
P14-1065,W11-1211,0,0.0329553,"sed discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a diffe"
P14-1065,D08-1024,0,0.0729502,"Missing"
P14-1065,P05-1033,0,0.102259,"over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this"
P14-1065,2010.iwslt-papers.10,0,0.0454846,"ial of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR"
P14-1065,D12-1108,0,0.0251193,"ramework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relations in the source la"
P14-1065,W10-1750,1,0.883167,"Missing"
P14-1065,W11-2107,0,0.0510915,"the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judges 8 20 15 18 498 924 570 708 171 303 207 249 20 31 18 32"
P14-1065,D11-1125,0,0.120375,"et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield be"
P14-1065,D12-1083,1,0.558373,"is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus sho"
P14-1065,P02-1040,0,0.0916457,"html SPAN NUC EDU Attribution SPAN Satellite NUC Nucleus SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear"
P14-1065,P13-1048,1,0.222411,"the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into ac"
P14-1065,W07-0707,0,0.0299954,"g proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse info"
P14-1065,P05-1034,0,0.0192094,"tence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield bett"
P14-1065,W04-1013,0,0.0173282,"NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judge"
P14-1065,2006.amta-papers.25,0,0.149639,"us SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 sy"
P14-1065,W05-0904,0,0.154446,"n campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware m"
P14-1065,W12-3129,0,0.298188,"e of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is"
P14-1065,D07-1080,0,0.0300512,"ks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judg"
P14-1065,2012.amta-papers.20,0,0.0666078,"ion for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For"
P14-1065,P11-3009,0,0.0426777,"tation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse"
P14-1065,moschitti-basili-2006-tree,0,0.00987264,"of the relation while satellites are supportive ones. Note that the nuclearity and relation labels in the reference translation are also realized in the system translation in (b), but not in (c), which makes (b) a better translation compared to (c), according to our hypothesis. We argue that existing metrics that only use lexical and syntactic information cannot distinguish well between (b) and (c). 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitations on the type of substructures that can be compared. 2 The discourse"
P14-1065,D12-1097,0,0.146948,"o develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion score. Unlike their work, which measures lexical cohesion at the document-level, here we are concerned with coherence (rhetorical) structure, primarily at the sentence-level. 3 3.1 Gener"
P14-1065,P07-1098,0,0.0569476,"ive partial credit to subtrees that differ in labels but match in their skeletons. More specifically, it uses the tags SPAN and EDU to build the skeleton of the tree, and considers the nuclearity and/or the relation labels as properties, added as children, of these tags. For example, a SPAN has two properties (its nuclearity and its relation), and an EDU has one property (its nuclearity). The words of an EDU are placed under the predefined children NGRAM. In order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in (Moschitti et al., 2007); not shown in Figure 2, for simplicity. Experimental Setup In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English.3 This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English (CS EN ), French-English ( FR - EN), German-English (DE - EN), and Spanish-English (ES - EN); as well as a dataset with the English references. We measured the correlation of the metrics with the human judgments pro"
P14-1065,N09-2004,0,\N,Missing
P14-1065,W13-3300,0,\N,Missing
P15-1078,W10-1750,1,0.923317,"Missing"
P15-1078,P14-1023,0,0.016965,"q. (3), can be rewritten as follows: 4.1 Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX 25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as W IKI -GW25. Furthermore, we experiment with W IKI GW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. Network Training The negative log likelihood of the training data for the model parameters θ = (W12 , W1r , W2r , wv , b12 , b1r , b2r , bv ) can be written"
P15-1078,W11-2107,0,0.0115221,"100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as will be discussed below, their"
P15-1078,P14-1129,0,0.0611925,"Missing"
P15-1078,W08-0331,0,0.313473,"that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce"
P15-1078,W07-0718,0,0.0607111,"(SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development. The quality of automatic MT evaluation metrics is usually assessed by computing their correlation with human judgments. To that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In"
P15-1078,W07-0738,1,0.52667,"Missing"
P15-1078,W11-2103,0,0.0331433,"r improvements: +1.5 and +2.0 points absolute when adding SYNTAX 25 and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at"
P15-1078,D14-1027,1,0.819258,"Missing"
P15-1078,W12-3102,0,0.0577153,"Missing"
P15-1078,P14-1065,1,0.735686,"Missing"
P15-1078,W14-3352,1,0.565452,"Missing"
P15-1078,P02-1040,0,0.100737,"ans that rivals the state of the art. 1 Introduction Automatic machine translation (MT) evaluation is a necessary step when developing or comparing MT systems. Reference-based MT evaluation, i.e., comparing the system output to one or more human reference translations, is the most common approach. Existing MT evaluation measures typically output an absolute quality score by computing the similarity between the machine and the human translations. In the simplest case, the similarity is computed by counting word n-gram matches between the translation and the reference. This is the case of BLEU (Papineni et al., 2002), which has been the standard for MT evaluation for years. Nonetheless, more recent evaluation measures take into account various aspects of linguistic similarity, and achieve better correlation with human judgments. 1 We do not argue that the pairwise approach is better than the direct estimation of human quality scores. Both approaches have pros and cons; we see them as complementary. 805 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 805–814, c Beijing, China, July 26-31"
P15-1078,2004.tmi-1.8,0,0.0676924,"ased approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as features. Secondly, we use information about both the reference and the two alternative translations simultaneously in a neural-based learning framework capable of modeling complex interactions between the features. Another related work is that of Kulesza and Shieber (2004), in which lexical and syntactic features, together with other metrics, e.g., BLEU and NIST, are used in an SVM classifier to discriminate good from bad translations. However, their setting is not pairwise comparison, but a classification task to distinguish human- from machineproduced translations. Moreover, in their work, using syntactic features decreased the correlation with human judgments dramatically (although classification accuracy improved), while in our case the effect is positive. In our previous work (Guzm´an et al., 2014a), we introduced a learning framework for the pairwise sett"
P15-1078,D14-1162,0,0.0794828,"h application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3 Learning Task Given two translation h"
P15-1078,W07-0707,0,0.0221202,"Missing"
P15-1078,W05-0904,0,0.150588,"ecause the use of kernels requires that the SVM operate in the much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to repr"
P15-1078,2006.amta-papers.25,0,0.187957,"2 VEC 300, trained on 100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as wi"
P15-1078,W12-3129,0,0.0269611,"much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our se"
P15-1078,P13-1045,0,0.14241,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W13-2202,0,0.0122239,"and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and B"
P15-1078,D13-1170,0,0.00357626,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W14-3336,0,0.0290128,"r, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and Bojar, 2014) for a discussion. Here we use the strict version used at WMT11 and WMT12. 4.4 Experiments and Results Experimental Settings Datasets: We train our neural models on WMT11 and we evaluate them on WMT12. We further use a random subset of 5,000 examples from WMT13 as a validation set to implement early stopping. Early stopping: We train on WMT11 for up to 10,000 epochs, and we calculate Kendall’s τ on the development set after each epoch. We then select the model that achieves the highest τ on the validation set; in case of ties for the best τ , we select the latest epoch that achieved the highes"
P15-1078,W11-2113,0,0.253095,"quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce absolute quality scor"
P15-1078,D12-1097,0,0.0127587,"o make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it bot"
P15-1078,N13-1090,0,0.0630498,"nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3"
P15-1078,W11-0329,0,0.0126675,"Missing"
P15-1078,W14-3302,0,\N,Missing
P16-2075,S16-1172,0,0.151144,"Missing"
P16-2075,S16-1138,0,0.225785,"Missing"
P16-2075,P15-1078,1,0.661142,"Missing"
P16-2075,S16-1137,1,0.558982,"Missing"
P16-2075,N16-1153,1,0.859816,"Missing"
P16-2075,P15-2114,0,0.110685,". Furthermore, in MTE we can expect shorter texts, which are typically much more similar. In contrast, in cQA, the question and the intended answers might differ significantly both in terms of length and in lexical content. Thus, it is not clear a priori whether the MTE network can work well to address the cQA problem. Here, we show that the analogy is not only convenient, but also that using it can yield state-of-the-art results for the cQA task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and"
P16-2075,P11-1143,0,0.168069,", 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely abo"
P16-2075,S16-1136,1,0.71134,"Missing"
P16-2075,P03-1003,0,0.0648629,"plied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the"
P16-2075,N13-1090,0,0.12755,"feature sets as ψ(q, c1 ) and ψ(q, c2 ). When including the external features, the activation at the output is f (q, c1 , c2 ) = sig(wvT [φ(q, c1 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). 4 Features We experiment with three kinds of features: (i) input embeddings, (ii) features from MTE (Guzm´an et al., 2015) and (iii) task-specific features from SemEval-2015 Task 3 (Nicosia et al., 2015). A. Embedding Features We used two types of vector-based embeddings to encode the input texts q, c1 and c2 : (1) G OOGLE VECTORS: 300dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The encoding of the full text is just the average of the word embeddings. (2) S YNTAX: We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. Also, we compute cosine similarity features with the above vectors: cos(q, c1 ) and cos(q, c2 ). B. MTE features We use the following MTE metrics (MT FEATS), which compare the similarity between the question and a candidate answer: (1) B LEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover"
P16-2075,J11-2003,0,0.0747787,") and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely above the task baselines."
P16-2075,S16-1083,1,0.652512,"Missing"
P16-2075,S15-2036,1,0.343646,"Missing"
P16-2075,S15-2038,0,0.165347,"(Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely above the task baselines. Furthermore, by adap"
P16-2075,P15-2116,0,0.0936834,"Missing"
P16-2075,P02-1040,0,0.0977685,"in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better, which is appropriate for a ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships between them; (iii) it uses a number of machine translation evaluation measures that have not been explored for the cQA task before, e.g., T ER (Snover et al., 2006), M ETEOR (Lavie and Denkowski, 2009), and B LEU (Papineni et al., 2002). The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c1 and c2 from the question thread—which play the role of the two competing translation hypotheses—we have to decide whether c1 is a better answer than c2 to question q—which plays the role of the translation reference. If we have a function f (q, c1 , c2 ) to make this decision, then we can rank the finite list of comments in the thread by comparing all possible pairs and by accumulating for each comment the scores for it given by f . From a general point of view, MTE and the cQ"
P16-2075,P15-1025,0,0.0694515,"answer to the question. Furthermore, in MTE we can expect shorter texts, which are typically much more similar. In contrast, in cQA, the question and the intended answers might differ significantly both in terms of length and in lexical content. Thus, it is not clear a priori whether the MTE network can work well to address the cQA problem. Here, we show that the analogy is not only convenient, but also that using it can yield state-of-the-art results for the cQA task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Rie"
P16-2075,P07-1059,0,0.0901484,"015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields resu"
P16-2075,2006.amta-papers.25,0,0.555241,"ural network is interesting for the cQA problem because: (i) it works in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better, which is appropriate for a ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships between them; (iii) it uses a number of machine translation evaluation measures that have not been explored for the cQA task before, e.g., T ER (Snover et al., 2006), M ETEOR (Lavie and Denkowski, 2009), and B LEU (Papineni et al., 2002). The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c1 and c2 from the question thread—which play the role of the two competing translation hypotheses—we have to decide whether c1 is a better answer than c2 to question q—which plays the role of the translation reference. If we have a function f (q, c1 , c2 ) to make this decision, then we can rank the finite list of comments in the thread by comparing all possible pairs and by accumulating for each comment the"
P16-2075,P13-1045,0,0.0205596,"). 4 Features We experiment with three kinds of features: (i) input embeddings, (ii) features from MTE (Guzm´an et al., 2015) and (iii) task-specific features from SemEval-2015 Task 3 (Nicosia et al., 2015). A. Embedding Features We used two types of vector-based embeddings to encode the input texts q, c1 and c2 : (1) G OOGLE VECTORS: 300dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The encoding of the full text is just the average of the word embeddings. (2) S YNTAX: We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. Also, we compute cosine similarity features with the above vectors: cos(q, c1 ) and cos(q, c2 ). B. MTE features We use the following MTE metrics (MT FEATS), which compare the similarity between the question and a candidate answer: (1) B LEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover et al., 2006). (4) M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; (5) Unigram P RECISION; (6) Unigram R ECALL. BLEU COMP. We further use as features various components"
R13-1066,C04-1072,0,0.0239716,"nces are often available for the English (target) side of the tuning and the evaluation dataset, but not for the source language, e.g., Arabic, Chinese. 1 One could hire translators, but this would be costly. 504 Proceedings of Recent Advances in Natural Language Processing, pages 504–510, Hissar, Bulgaria, 7-13 September 2013. 3 3.1 Method We will do the selection with respect to some English reference, e.g., backtranslation of the Arabic reference generated by our own system or by Google translate. Below, we present the similarity measures that we use for the selection. BLEU+1 (B1). BLEU+1 (Lin and Och, 2004) is a smoothed version of BLEU (Papineni et al., 2002) used to address sparseness problems with n-gram matches when comparing sentences. BLEU+1 BP smooth (B1-BP). The BLEU+1 approximation of BLEU smooths the n-gram counts but not the brevity penalty, thus destroying the balance between the two; it also assigns a non-zero precision to cases with zero matches. Thus, we experiment with a version of BLEU+1 from (Nakov et al., 2012) that smooths the brevity penalty and also uses a “grounding” factor. BLEU+1 Sigmoid LP (B1-SG). Note that the brevity penalty of BLEU/BLEU+1 penalizes shorter but not l"
R13-1066,D12-1037,0,0.0156699,"inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the available resources for training, tuning and evaluation. These resources are hard to ob"
R13-1066,D11-1033,0,0.021347,"given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus crit"
R13-1066,D09-1074,0,0.0212759,"s of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the avai"
R13-1066,W09-0439,0,0.0212717,"-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.991 30.03 0.983 30.14 0.986 AVG, no self BLEU len 30.30 1.020 30.18 0.993 30.14 0.990 29.36 0.981 29.32 0.982 Table 2: Tuning and testing on MT04. We tune on the English input in the first column, then we translate all MT04x inputs. We report BLEU and hyp/ref length ratios averaged over (a) all MT04 datasets, and (b) all but the one used for tuning. Table 1 shows the results when tuning on MT04 and testing on MT05. There are several interesting observations we can make. First, the choice of test dataset has a"
R13-1066,D10-1044,0,0.0220994,"on, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SM"
R13-1066,E06-1005,0,0.0303122,"ields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions"
R13-1066,P10-2041,0,0.0333882,"multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical m"
R13-1066,W11-2123,0,0.0332866,"we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.991 30.03 0.983 30.14 0.986 AVG, no self BLEU len 30.30 1.020 30.18 0.993 30.14 0.990 29.36 0.981 29.32 0.982 Table 2: Tuning and testing"
R13-1066,C12-1121,1,0.90772,"Missing"
R13-1066,2005.eamt-1.19,1,0.791718,"nd Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the available resources for training, tuning and evalua"
R13-1066,2001.mtsummit-papers.46,0,0.0699721,"puts: (a) select one of the datasets, (b) select the best input for each sentence, and (c) synthesize an input for each sentence by fusing the available inputs. Surprisingly, we find out that it is best to tune on the hardest available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et"
R13-1066,W12-5611,0,0.0298896,"Missing"
R13-1066,P02-1040,0,0.092353,"side of the tuning and the evaluation dataset, but not for the source language, e.g., Arabic, Chinese. 1 One could hire translators, but this would be costly. 504 Proceedings of Recent Advances in Natural Language Processing, pages 504–510, Hissar, Bulgaria, 7-13 September 2013. 3 3.1 Method We will do the selection with respect to some English reference, e.g., backtranslation of the Arabic reference generated by our own system or by Google translate. Below, we present the similarity measures that we use for the selection. BLEU+1 (B1). BLEU+1 (Lin and Och, 2004) is a smoothed version of BLEU (Papineni et al., 2002) used to address sparseness problems with n-gram matches when comparing sentences. BLEU+1 BP smooth (B1-BP). The BLEU+1 approximation of BLEU smooths the n-gram counts but not the brevity penalty, thus destroying the balance between the two; it also assigns a non-zero precision to cases with zero matches. Thus, we experiment with a version of BLEU+1 from (Nakov et al., 2012) that smooths the brevity penalty and also uses a “grounding” factor. BLEU+1 Sigmoid LP (B1-SG). Note that the brevity penalty of BLEU/BLEU+1 penalizes shorter but not longer sentences. Thus, we also experiment with a versi"
R13-1066,N03-1017,0,0.023922,".001 0.51 MT054 BLEU len 35.46 0.988 35.31 0.972 35.12 0.970 34.81 0.960 34.82 0.965 35.15 0.973 0.65 AVERAGE BLEU len 34.24 0.989 34.12 0.972 33.95 0.970 33.74 0.960 33.67 0.964 34.03 0.974 0.57 Table 1: Tuning on MT04 and testing on MT05. Shown are BLEU scores and hypothesis/reference length ratios. The best and the worst BLEU scores for each test MT05 dataset are in bold and stroke out, respectively; the last row shows the absolute difference between them. 4 Experiments and Evaluation 4.1 4.2 Experimental Setup TEST ⇒ TUNE ⇓ MT040 MT041 MT042 MT043 MT044 We used the phrase-based SMT model (Koehn et al., 2003), as implemented in the Moses toolkit (Koehn et al., 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arab"
R13-1066,P08-2030,0,0.123937,", 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring"
R13-1066,2005.iwslt-1.8,0,0.156577,"tem (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.9"
R13-1066,E09-1082,0,0.0210994,"t available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at t"
R13-1066,P07-2045,0,0.00956357,"70 34.81 0.960 34.82 0.965 35.15 0.973 0.65 AVERAGE BLEU len 34.24 0.989 34.12 0.972 33.95 0.970 33.74 0.960 33.67 0.964 34.03 0.974 0.57 Table 1: Tuning on MT04 and testing on MT05. Shown are BLEU scores and hypothesis/reference length ratios. The best and the worst BLEU scores for each test MT05 dataset are in bold and stroke out, respectively; the last row shows the absolute difference between them. 4 Experiments and Evaluation 4.1 4.2 Experimental Setup TEST ⇒ TUNE ⇓ MT040 MT041 MT042 MT043 MT044 We used the phrase-based SMT model (Koehn et al., 2003), as implemented in the Moses toolkit (Koehn et al., 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et"
R13-1066,2006.amta-papers.25,0,0.0233586,"e easiest). We believe that this finding has implications on how we should pick good translators and how we should select useful data for parameter optimization. On the other hand, it might also indicate a problem with BLEU as an evaluation measure. In future work, we plan to test our methods on other Arabic-English datasets that have multiple English references. We further plan experiments with other language pairs, e.g., ChineseEnglish, which are available from NIST and IWSLT. We also want to study the effect of the tuning dataset selection on evaluation measures other than BLEU, e.g., TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). Looking at tuning dataset selection that takes the test data into account is another promising direction for future work. Features from quality estimation (Specia et al., 2010) might be also helpful to determine the best input to tune on. Another related, but different, research direction is about how to best evaluate (as opposed to tune, which we have explored above) an SMT system in case multiple possible versions of the input sentences are available. Choosing the hardest dataset A closer look at the strategies for backtranslate and X-vs-all-but-X rev"
S16-1137,S16-1130,1,0.829487,"uld work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar Foundation. It is part of the Interactive sYstems for Answer Search (Iyas) project, which is developed in collaboration with M"
S16-1137,P15-2113,1,0.3401,"Missing"
S16-1137,2012.eamt-1.60,0,0.0222671,"of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty. We will refer to the set of these features as BLEU COMP. 4.3 Task-specific features QL VEC (in MTE-NN-improved only). Similarly to the G OOGLE VEC, but on task-specific data, we train word vectors using WORD 2 VEC on all available cQA training data (Qatar Living) and use them as input to the NN. QL+IWSLT VEC (in MTE-NN-{primary, contrastive1/2} only). We also use trained word vectors on the concatenation of the cQA training data and the English portion of the IWSLT data, which consists of TED talks (Cettolo et al., 2012) and is thus informal and somewhat similar to cQA data. TASK FEAT. We further extract various taskspecific skip-arc features, most of them proposed for the 2015 edition of the task (Nakov et al., 2015). This includes some comment-specific features: • number of URLs/images/emails/phone numbers; • number of occurrences of the string thank;3 • number of tokens/sentences; • average number of tokens; • type/token ratio; • number of nouns/verbs/adjectives/adverbs/pronouns; • number of positive/negative smileys; • number of single/double/triple exclamation/interrogation symbols; • number of interroga"
S16-1137,P15-2114,0,0.102811,"we used our system for subtask A to solve subtask C, which asks to find good answers to a new question that was not asked before in the forum by reranking the answers to related questions. For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google). Without any subtask C specific addition, we achieved the fourth best result in the task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al."
S16-1137,P03-1003,0,0.0981864,"asks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who app"
S16-1137,P15-1078,1,0.448296,"Missing"
S16-1137,P16-2075,1,0.558646,"Missing"
S16-1137,D15-1068,1,0.265429,"Missing"
S16-1137,N16-1084,1,0.557796,"Missing"
S16-1137,P11-1143,0,0.386837,"al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a differ"
S16-1137,N12-1019,0,0.0191253,"2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreover, instead of using MTE metrics as features, we port an entire MTE framework to the cQA problem. 3 Neural Model for Answer Ranking The NN model we use for answer ranking is depicted in Figure 1. It is a direct adaptation of the feed-forward NN for MTE described in (Guzm´an et al., 2015). Technically, we have a binary classification task with input (q, c1 , c2 ), which should output 1 if c1 is a better answer to q than c2 , and 0 otherwise.2 The network computes a sigmoid"
S16-1137,P16-2065,1,0.435928,"we have adopted a pairwise neural network architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings of the input texts that are non-linearly combined in the hidden layer. 892 Our post-competition improvements have shown state-of-the-art performance (Guzm´an et al., 2016), with sizeable contribution from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions a"
S16-1137,S16-1136,1,0.763699,"we have adopted a pairwise neural network architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings of the input texts that are non-linearly combined in the hidden layer. 892 Our post-competition improvements have shown state-of-the-art performance (Guzm´an et al., 2016), with sizeable contribution from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions a"
S16-1137,K15-1032,1,0.168613,"from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HB"
S16-1137,R15-1058,1,0.13714,"from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HB"
S16-1137,N13-1090,0,0.0509505,"(q, c1 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). 4 Learning Features We experiment with three kinds of features: (i) input embeddings, (ii) features motivated by previous work on Machine Translation Evaluation (MTE) (Guzm´an et al., 2015) and (iii) task-specific features, mostly proposed by participants in the 2015 edition of the task (Nakov et al., 2015). 4.1 Embedding Features We use the following vector-based embeddings of (q, c1 , c2 ) as input to the NN: • G OOGLE VEC: We use the pre-trained, 300dimensional embedding vectors, which Tomas Mikolov trained on 100 billion words from Google News (Mikolov et al., 2013). • S YNTAX VEC: We parse the entire question/comment text using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. 889 Moreover, we use the above vectors to calculate pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evalu"
S16-1137,S15-2047,1,0.355744,"Missing"
S16-1137,S15-2036,1,0.557554,"Missing"
S16-1137,P02-1040,0,0.113293,"late pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candidate answer: • B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). • NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). • TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). • M ETEOR: A measure that matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). • P RECISION: measure, originating in information retrieval. • R ECALL: another measure coming from information retrieval. BLEU COMP. Following (Guzm´an et al., 2015), we further use as features various components that are"
S16-1137,P07-1059,0,0.170705,"s et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. Howeve"
S16-1137,2006.amta-papers.25,0,0.118347,"only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candidate answer: • B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). • NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). • TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). • M ETEOR: A measure that matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). • P RECISION: measure, originating in information retrieval. • R ECALL: another measure coming from information retrieval. BLEU COMP. Following (Guzm´an et al., 2015), we further use as features various components that are involved in the computation of B LEU: n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), lengths of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty. We will refer to the set of these f"
S16-1137,P13-1045,0,0.067615,"nput embeddings, (ii) features motivated by previous work on Machine Translation Evaluation (MTE) (Guzm´an et al., 2015) and (iii) task-specific features, mostly proposed by participants in the 2015 edition of the task (Nakov et al., 2015). 4.1 Embedding Features We use the following vector-based embeddings of (q, c1 , c2 ) as input to the NN: • G OOGLE VEC: We use the pre-trained, 300dimensional embedding vectors, which Tomas Mikolov trained on 100 billion words from Google News (Mikolov et al., 2013). • S YNTAX VEC: We parse the entire question/comment text using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. 889 Moreover, we use the above vectors to calculate pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candid"
S16-1137,J11-2003,0,0.0693038,"election (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreo"
S16-1137,S15-2038,0,0.178473,"oschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreover, instead of usin"
S16-1137,P15-2116,0,0.0572579,"Missing"
S16-1137,P15-1025,0,0.0622365,"the network. Finally, we used our system for subtask A to solve subtask C, which asks to find good answers to a new question that was not asked before in the forum by reranking the answers to related questions. For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google). Without any subtask C specific addition, we achieved the fourth best result in the task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Br"
W12-3136,J93-2003,0,0.0307065,"nted with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order becomes first @-@ order; • Maximum sentence length: 100 tokens; • Truecasing: convert sentence-initial words to their most frequent case in the training dataset; • Word alignments: directed IBM model 4 (Brown et al., 1993) alignments in both directions, then grow-diag-final-and heuristics; • Maximum phrase length: 7 tokens; 1 The WMT12 organizers invited systems translating between English and four other European languages, in both directions: French, Spanish, German, and Czech. However, we only participated in Spanish→English and German→English. • Phrase table scores: forward & reverse phrase translation probabilities, forward & reverse lexical translation probabilities, phrase penalty; 298 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298–303, c Montr´eal, Canada, June 7-8, 2012. 2"
W12-3136,N03-1017,0,0.0392872,"g Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline sy"
W12-3136,P07-2045,0,0.0110187,"2008): when a phrase pair appeared in both tables, they only kept the entry from the first table, while we keep the entries from both tables. 299 30.94 31.36 Table 2: Phrase table merging. 2.3 Language Models We built the language models (LM) for our systems using a probabilistic 5-gram model with KneserNey (KN) smoothing. We experimented with LMs trained on different training datasets. We used the SRILM toolkit (Stolcke, 2002) for training the language models, and the KenLM toolkit (Heafield and Lavie, 2010) for binarizing the resulting ARPA models for faster loading with the Moses decoder (Koehn et al., 2007). 2.3.1 Using WMT12 Corpora Only We trained 5-gram LMs on datasets provided by the task organizers. The results are presented in Table 3. The first line reports the baseline BLEU scores using a language model trained on the target side of the News Commentary + Europarl training bi-texts. The second line shows the results when using an interpolation (minimizing the perplexity on the news2010 tuning dataset) of different language models, trained on the following corpora: • the monolingual News Commentary corpus plus the English sides of all training News Commentary v.7 bi-texts (for French-Engli"
W12-3136,N04-1022,0,0.0436485,"t. This means that our selected source-side sentences tended to be shorter than in the baseline. Moreover, the standard deviation of the sentence lengths was smaller for our samples as well, which means that there were fewer long sentences; this is good since long sentences can take very long to translate. As a result, we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets. Decoding and Hypothesis Reranking We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmann and Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). The results are shown in Table 7. We can see that both yield improvements in BLEU, even if small. 2.6 Baseline (es:#2,de:#3) +MP 29.83 29.98 21.72 22.03 Baseline (es:#4,de:#5) +MBR 30.16 30.31 22.30 22.48 Table 7: Decoding parameters. Experiments with monotone at punctuation (MP) reordering, and minimum Bayes risk (MBR) decoding. The results for the actual news2012 testset are shown in Table 8: the system combination results are our primary submission. We can see that system combination yielded 0.4 BLEU points of improvement for Spanish-English and 0.2-0.3 BLEU points for German-English. 3 T"
W12-3136,W08-0320,1,0.751604,"g MERT.3 Table 2 shows that this improves by +0.42 BLEU points. 2 In theory, we should also re-normalize the conditional probabilities (forward/reverse phrase translation probability, and forward/reverse lexicalized phrase translation probability) since they may not sum to one anymore. In practice, this is not that important since the log-linear phrase-based SMT model does not require that the phrase table features be probabilities (e.g., F1 , F2 , F3 , and the phrase penalty are not probabilities); moreover, we have extra features whose impact is bigger. 3 This is similar but different from (Nakov, 2008): when a phrase pair appeared in both tables, they only kept the entry from the first table, while we keep the entries from both tables. 299 30.94 31.36 Table 2: Phrase table merging. 2.3 Language Models We built the language models (LM) for our systems using a probabilistic 5-gram model with KneserNey (KN) smoothing. We experimented with LMs trained on different training datasets. We used the SRILM toolkit (Stolcke, 2002) for training the language models, and the KenLM toolkit (Heafield and Lavie, 2010) for binarizing the resulting ARPA models for faster loading with the Moses decoder (Koehn"
W12-3136,P02-1040,0,0.0922053,"le combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order become"
W12-3136,2006.amta-papers.25,0,0.0155937,"esults show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order becomes first @-@ order; • Maximum s"
W12-3136,J03-1005,0,0.0353527,"smaller than in our baseline, the news2011 development dataset. This means that our selected source-side sentences tended to be shorter than in the baseline. Moreover, the standard deviation of the sentence lengths was smaller for our samples as well, which means that there were fewer long sentences; this is good since long sentences can take very long to translate. As a result, we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets. Decoding and Hypothesis Reranking We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmann and Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). The results are shown in Table 7. We can see that both yield improvements in BLEU, even if small. 2.6 Baseline (es:#2,de:#3) +MP 29.83 29.98 21.72 22.03 Baseline (es:#4,de:#5) +MBR 30.16 30.31 22.30 22.48 Table 7: Decoding parameters. Experiments with monotone at punctuation (MP) reordering, and minimum Bayes risk (MBR) decoding. The results for the actual news2012 testset are shown in Table 8: the system combination results are our primary submission. We can see that system combination yielded 0.4 BLEU points of improvement for Sp"
W14-3352,E06-1032,0,0.104635,"translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to the combination other metrics fr"
W14-3352,W07-0734,0,0.111099,"Missing"
W14-3352,D08-1024,0,0.0117943,"se years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous"
W14-3352,W14-3336,0,0.123967,"Missing"
W14-3352,2003.mtsummit-papers.9,0,0.0555949,"part raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to th"
W14-3352,P07-1098,0,0.0527917,"words in an Elementary Discourse Unit (EDU) are grouped under a predefined tag EDU, to which the nuclearity status of the EDU is attached: nucleus vs. satellite. Coherence relations, such as Attribution, Elaboration, and Enablement, between adjacent text spans constitute the internal nodes of the tree. Like the EDUs, the nuclearity statuses of the larger discourse units are attached to the relation labels. Notice that with this representation the tree kernel can easily be extended to find subtree matches at the word level, i.e., by including an additional layer of dummy leaves as was done in (Moschitti et al., 2007). We applied the same solution in our representations. Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolutio"
W14-3352,2003.mtsummit-papers.10,0,0.311329,"trics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Ne"
W14-3352,2012.amta-papers.6,0,0.0421525,"@qf.org.qa Abstract Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design bette"
W14-3352,P03-1021,0,0.00737295,"orms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Associatio"
W14-3352,P02-1040,0,0.100492,"nally, we add other metrics from the A SIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mea"
W14-3352,2006.amta-papers.25,0,0.163958,"ent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that are based on new representations of the DTs. In the remainder of this section, we will focus on the individual DT representations that"
W14-3352,P14-1065,1,0.494914,"Missing"
W14-3352,D07-1080,0,0.0139657,"hat participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, w"
W14-3352,D11-1125,0,0.0213356,"segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that"
W14-3352,D12-1083,1,\N,Missing
W15-3059,W11-2101,0,0.0448672,"Missing"
W15-3059,W07-0718,0,0.744642,"ls with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there difference"
W15-3059,W12-3102,0,0.110664,"Missing"
W15-3059,P14-1065,1,0.885127,"Missing"
W15-3059,2013.mtsummit-wptp.5,0,0.0518021,"Missing"
W15-3059,2006.amta-papers.25,0,0.201332,"Missing"
W15-3059,stymne-etal-2012-eye,0,0.215144,"information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there differences of behavior between bilinguals (i.e. evaluators fluent in both source and target languages) and monolinguals (i.e. evaluators fluent only in the target language)? Which group is more consistent? 457 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 457–466, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. Stymne et al. (2012) applied eye-tracking to machine translation error analysis. They found that longer gaze time and and a higher number of fixations correlate with high number of errors in the MT output. Doherty and O’Brien (2014) used eyetracking to evaluate the quality of raw machine translation output in terms of its usability by an end user. They concluded that eye-tracking correlates well with the other measures which they used for their study. In this work, we use eye-tracking to observe which sources of information evaluators use while performing an MT evaluation task and how this impacts the task comple"
W15-3059,2003.mtsummit-papers.51,0,0.22915,"Missing"
W15-3059,W07-0713,0,0.0259127,"er to use monolinguals with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the eval"
W15-3059,H93-1040,0,0.574996,"Missing"
W15-3059,1994.amta-1.25,0,0.487622,"Missing"
W15-3059,1993.mtsummit-1.24,0,\N,Missing
W15-3059,W14-3352,1,\N,Missing
W15-3059,W12-4906,0,\N,Missing
W19-5404,D11-1033,0,0.45229,"irs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine trans"
W19-5404,W18-6477,0,0.0322741,"nce pair. Their method has shown promising results in filtering WMT Paracrawl data and has achieved state-of-the-art performance on the BUCC corpus mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machine translation outputs and scores, some participants used word and sentence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and predicts a matching score. Some participants made a distinction between unsupervised methods that did not use exis"
W19-5404,W16-2347,1,0.85374,"hn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpus filtering was organized (Koehn et al., 2018). Acquiring parallel corpora from the web typically goes through the stages of identifying web sites with parallel text, downloading the pages of the web site, aligning document pairs, and aligning sentence pairs. A final stage of the processing pipeline filters out non parallel sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Cor"
W19-5404,W18-6478,0,0.0401905,"y consider sentence length, number of real words vs. other tokens, matching names, numbers, dates, email addresses, or URLs, too similar sentences (copied content), and language identification (Pinnis, 2018; Lu et al., 2018; Ash et al., 2018). Scoring functions. Sentence pairs that pass the pre-filtering stage are assessed with scoring functions which provide scores that hopefully correlate with quality of sentence pairs. Participants used a variety of such scoring functions, including language models, neural translation models and lexical translation probabilities, e.g., IBM Model 1 scores. (Junczys-Dowmunt, 2018; Rossenbach et al., 2018; Lo et al., 2018). 3 Low-Resource Corpus Filtering Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. 56 Corpus Specifically, we provided a very noisy 5060 million word (English token count) Nepali– English and Sinhala–English corpora crawled from the web using the Paracrawl processing pipeline (see Section 4.4 for details). We asked participants to generate sentence-level quality scores that allow sel"
W19-5404,W17-3209,0,0.038209,"://www.paracrawl.eu/ 54 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 54–72 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Related Work sifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-ba"
W19-5404,W18-2709,1,0.842012,"h web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a clas2.3 Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study about how noise that occurs in crawled parallel text impacts statistical and neural machine translation. 2 http://opus.nlpl.eu NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ 3 55 Learning weights for scoring functions. Given a large number of scoring functions, simply averaging their resulting scores may be inadequate. Learning weights to optimize machine translation system quality is computationally intractable due to the high cost of training these syste"
W19-5404,W19-5435,1,0.728844,"Missing"
W19-5404,2005.mtsummit-papers.11,1,0.234451,"ve for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and"
W19-5404,P13-2061,0,0.0514257,"ngual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl1 ) is an important option. 1 This paper gives an overview of the task, presents the results for the participating systems and provides analysis on additional subset sizes and the average sentence length of sub-selected data. http://www.paracrawl.eu/ 54 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 54–72 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Related Work sifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in paralle"
W19-5404,P07-2045,1,0.0139382,"ecifically, we provided a very noisy 5060 million word (English token count) Nepali– English and Sinhala–English corpora crawled from the web using the Paracrawl processing pipeline (see Section 4.4 for details). We asked participants to generate sentence-level quality scores that allow selecting subsets of sentence pairs that amount to (a) 1 million words, and (b) 5 million words, counted on the English side. These values were chosen as an approximation to the conditions on the WMT 2018 task. The resulting subsets were scored by building a statistical phrase-based machine translation system (Koehn et al., 2007) and a neural machine translation system (Ott et al., 2019) trained on this data, and then measuring their BLEU score on the flores Wikipedia test sets (Guzm´an et al., 2019). Participants in the shared task submitted a file with quality scores, one per line, corresponding to the sentence pairs. Scores are only required to have the property that higher scores indicate better quality. The scores were uploaded to a Google Drive folder which remains publicly accessible.4 For development purposes, we released configuration files and scripts that mirror the official testing procedure with a develop"
W19-5404,W19-5436,0,0.0439724,"Missing"
W19-5404,W18-6453,1,0.468977,"European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpus filtering was organized (Koehn et al., 2018). Acquiring parallel corpora from the web typically goes through the stages of identifying web sites with parallel text, downloading the pages of the web site, aligning document pairs, and aligning sentence pairs. A final stage of the processing pipeline filters out non parallel sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering3 was organized, albeit in t"
W19-5404,W19-5437,0,0.044404,"Missing"
W19-5404,W17-3204,1,0.941154,"ural machine translation systems on these subsets, and measured their quality with the BLEU score on a test set of multi-domain Wikipedia content (Guzm´an et al., 2019). Introduction Machine Translation (MT) has experienced significant advances in recent years thanks to improvements in modeling, and in particular neural models (Bahdanau et al., 2015; Gehring et al., 2016; Vaswani et al., 2017). Unfortunately, today’s neural machine translation models, perform poorly on low-resource language pairs, for which clean, parallel training data is high-quality training data is lacking, by definition (Koehn and Knowles, 2017). Improving performance on low resource language pairs is very impactful considering that these languages are spoken by a large fraction of the world population. This is a particular challenge for industrial machine translation systems that need to support hundreds of languages in order to provide adequate services to their multilingual user base. In face of the scarcity of clean parallel data, learning to translate from any multilingual noisy data such as web-crawls (e.g. from Wikipedia, Paracrawl1 ) is an important option. 1 This paper gives an overview of the task, presents the results for"
W19-5404,W18-6317,0,0.0989903,"g high-quality sentence pairs or by using the raw crawled data (S´anchez-Cartagena et al., 2018). Neural machine translation model training may combine data selection and model training, taking advantage of the increasing quality of the model to better detect noisy data or to increasingly focus on cleaner parts of the data (Wang et al., 2018; Kumar et al., 2019). 2.4 Sentence Embeddings Bouamor and Sajjad (2018) learned sentence embeddings for the source and target languages and selected the nearest translation from a list of candidate sentences for a given source sentence using a classifier. Guo et al. (2018) leveraged hard negatives to correctly identify translation pairs. Artetxe and Schwenk (2018) use multilingual sentence embeddings to compute cosine similarity between the source and the target sentence. They further normalize the score by the average cosine similarity of the nearest neighbors for the given sentence pair. Their method has shown promising results in filtering WMT Paracrawl data and has achieved state-of-the-art performance on the BUCC corpus mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machin"
W19-5404,N19-1208,0,0.0556731,"Missing"
W19-5404,L18-1548,0,0.0289109,"he same Devanagari script as Hindi and the languages are closely related. Neural machine translation models for low-resource language pairs have particularly benefited from training data in other language pairs, so parallel Hindi– English data and monolingual Hindi data may be beneficial to train models for our shared task. As shown in Table 4, we provide a relatively large 20 million word parallel corpus and almost 2 billion words of monolingual Hindi. This data was created from a variety of public domain sources and corpora developed at the Center for Indian Language Technology, IIT Bombay (Kunchukuttan et al., 2018). 4.4 English Words 58,537,167 60,999,374 not translations of each other, bad language (incoherent mix of words and non-words), incomplete or bad translations, etc. We used the processing pipeline of the Paracrawl project to create the data, using the clean parallel data to train underlying models such as the dictionary used by Hunalign (Varga et al., 2007) and a statistical translation model used by the document aligner. One modification was necessary to run the pipeline for Nepali due to the end-of-sentence symbol of the script that was previously not recognized by the sentence splitter. The"
W19-5404,W19-5438,0,0.0423381,"otal of 21 different submissions for Nepali and 23 different submissions for Sinhala that we scored. 5.2 A novel method that was central to the bestperforming submission was the use of crosslingual sentence embeddings that were directly trained from parallel sentence pairs (Chaudhary et al., 2019). Other submissions used monolingual word embeddings. These were first trained monolingually for each language from monolingual data. The resulting embedding spaces were mapped either in an unsupervised fashion (Soares and Costa-juss`a, 2019) or based on a dictionary ¨ learned from the parallel data (Kurfalı and Ostling, 2019). Bernier-Colborne and Lo (2019) use both monolingually trained word embeddings aligned in an unsupervised fashion and bilingually trained word embeddings. Methods used by Participants Almost all submissions used basic filtering rules as a first filtering step. These rules typically involve language identification and length considAnother approach is to first train a translation 59 Acronym AFRL DiDi Facebook Helsinki IITP Webinterpret NRC Stockholm SUNY Buffalo Sciling TALP-UPC Participant and System Description Citation Air Force Research Lab, USA (Erdmann and Gwinnup, 2019) DiDi, USA (Axelro"
W19-5404,W18-6486,0,0.0231679,"of submissions, many different approaches were explored for this task. However, most participants used a system using three components: (1) pre-filtering rules, (2) scoring functions for sentence pairs, and (3) a classifier that learned weights for feature functions. Pre-filtering rules. Some of the training data can be discarded based on simple deterministic filtering rules. These may include rules may consider sentence length, number of real words vs. other tokens, matching names, numbers, dates, email addresses, or URLs, too similar sentences (copied content), and language identification (Pinnis, 2018; Lu et al., 2018; Ash et al., 2018). Scoring functions. Sentence pairs that pass the pre-filtering stage are assessed with scoring functions which provide scores that hopefully correlate with quality of sentence pairs. Participants used a variety of such scoring functions, including language models, neural translation models and lexical translation probabilities, e.g., IBM Model 1 scores. (Junczys-Dowmunt, 2018; Rossenbach et al., 2018; Lo et al., 2018). 3 Low-Resource Corpus Filtering Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawl"
W19-5404,W18-6480,0,0.0190114,"ntence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and predicts a matching score. Some participants made a distinction between unsupervised methods that did not use existing parallel corpora to train parts of the system, and supervise methods that did. Unsupervised methods have the advantage that they can be readily deployed for language pairs for which no seed parallel corpora exist. Findings of the 2018 Shared Task The WMT 2018 Shared Task on Parallel Corpus Filtering (Koehn et al., 2018) attracted 18 submissions"
W19-5404,W18-6319,0,0.0276321,"with no additional data, and decoder beam size of 5,000 hypotheses. NMT For neural machine translation, we used fairseq (Ott et al., 2019) transformer model with the parameter settings shown in Figure 1. Preprocessing was done with sentence piece for a 5000 subword vocabulary on tokenized text using the Moses tokenizer (but no truecasing was used). Decoding was done with beam size 5 and length normalization 1.2. Training a system for the 1 million, and 5 million subsets took about 3, and 13 hours, respectively, on a single GTX 1080ti GPU. Scores on the test sets were computed with Sacrebleu (Post, 2018). We report case-insensitive scores. 9 https://github.com/facebookresearch/ flores#train-a-baseline-transformer-model 62 Nepali Submission AFRL 50k AFRL 150k Facebook main Facebook contrastive Helsinki Helsinki contrastive IITP IITP geom NRC ensemble NRC xlm NRC yisi-2-sup NRC yisi-2-unsup Stockholm Stockholm ngram SUNY Buffalo Sciling TALP-UPC primary TALP-UPC secondary Webinterpret primary Webinterpret cov Webinterpret prob 1 million SMT NMT test devt test devt 4.0 3.8 2.7 2.5 1.5 3.6 2.3 2.4 4.2 4.0 6.8 6.9 4.2 4.0 6.9 6.6 3.2 3.1 0.9 0.9 1.3 1.2 0.1 0.1 3.8 3.6 5.5 5.9 3.9 3.6 5.3 5.6 4.1"
W19-5404,W18-6481,0,0.165057,"mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machine translation outputs and scores, some participants used word and sentence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and predicts a matching score. Some participants made a distinction between unsupervised methods that did not use existing parallel corpora to train parts of the system, and supervise methods that did. Unsupervised methods have the advantage that they can b"
W19-5404,2009.mtsummit-posters.15,0,0.268115,"This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpu"
W19-5404,2011.mtsummit-papers.48,0,0.0642035,"ood sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine trans"
W19-5404,W18-6482,0,0.0212432,"s, many different approaches were explored for this task. However, most participants used a system using three components: (1) pre-filtering rules, (2) scoring functions for sentence pairs, and (3) a classifier that learned weights for feature functions. Pre-filtering rules. Some of the training data can be discarded based on simple deterministic filtering rules. These may include rules may consider sentence length, number of real words vs. other tokens, matching names, numbers, dates, email addresses, or URLs, too similar sentences (copied content), and language identification (Pinnis, 2018; Lu et al., 2018; Ash et al., 2018). Scoring functions. Sentence pairs that pass the pre-filtering stage are assessed with scoring functions which provide scores that hopefully correlate with quality of sentence pairs. Participants used a variety of such scoring functions, including language models, neural translation models and lexical translation probabilities, e.g., IBM Model 1 scores. (Junczys-Dowmunt, 2018; Rossenbach et al., 2018; Lo et al., 2018). 3 Low-Resource Corpus Filtering Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web),"
W19-5404,P99-1068,0,0.360125,"l data relevant for a task-specific machine translation system (Axelrod et al., 2011). van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs that are just not relevant for the targeted domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the C"
W19-5404,N19-4009,0,0.190586,"sh token count) Nepali– English and Sinhala–English corpora crawled from the web using the Paracrawl processing pipeline (see Section 4.4 for details). We asked participants to generate sentence-level quality scores that allow selecting subsets of sentence pairs that amount to (a) 1 million words, and (b) 5 million words, counted on the English side. These values were chosen as an approximation to the conditions on the WMT 2018 task. The resulting subsets were scored by building a statistical phrase-based machine translation system (Koehn et al., 2007) and a neural machine translation system (Ott et al., 2019) trained on this data, and then measuring their BLEU score on the flores Wikipedia test sets (Guzm´an et al., 2019). Participants in the shared task submitted a file with quality scores, one per line, corresponding to the sentence pairs. Scores are only required to have the property that higher scores indicate better quality. The scores were uploaded to a Google Drive folder which remains publicly accessible.4 For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of: Bible (two"
W19-5404,W18-6487,0,0.0336372,"Missing"
W19-5404,W18-6483,0,0.0192377,"her normalize the score by the average cosine similarity of the nearest neighbors for the given sentence pair. Their method has shown promising results in filtering WMT Paracrawl data and has achieved state-of-the-art performance on the BUCC corpus mining task. 2.5 Use of embeddings. While the participant’s methods were dominated by non-neural components, sometimes using neural machine translation outputs and scores, some participants used word and sentence embeddings. Given crosslingual word embeddings, sentence match scores based on the difference between the average of the word embeddings (Paetzold, 2018), or, for each word in the sentence, the closest match in the corresponding sentence (Hangya and Fraser, 2018). Matching of word embeddings may also be done monolingually, after machine translating the foreign sentence into English (Lo et al., 2018). Cross-lingual word embeddings were obtained using uses monolingual word embedding spaces which were aligned with an unsupervised method, or using pre-trained cross-lingual word embeddings. Littell et al. (2018) used monolingual sentence embedding spaces to discount outliers. Pham et al. (2018) use a neural model that takes a sentence pair and pred"
W19-5404,W18-6488,0,0.0961965,"Missing"
W19-5404,W19-5439,0,0.0372012,"Missing"
W19-5404,W19-5440,0,0.0387972,"Missing"
W19-5404,D17-1319,1,0.853992,"ther because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering3 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a clas2.3 Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study about how noise that occurs in crawled parallel text impacts st"
W19-5404,2011.eamt-1.25,0,0.259112,"Missing"
W19-5404,2011.mtsummit-papers.47,0,0.140127,"stage of the processing pipeline filters out non parallel sentence pairs. These exist either because the original web site did not have any actual parallel data (garbage in, garbage out), only partial parallel data, or due to failures of earlier processing steps. 2.2 Filtering Noisy Parallel Corpora In 2016, a shared task on sentence pair filtering3 was organized, albeit in the context of cleaning translation memories which tend to be cleaner than the data at the end of a pipeline that starts with web crawls. There is a robust body of work on filtering out noise in parallel data. For example: Taghipour et al. (2011) use an outlier detection algorithm to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a clas2.3 Impact of Noise on Neural Machine Translation Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) examine noisy training data and focus on types of noise occurring in web-crawled corpora. They"
W19-5404,tiedemann-2012-parallel,0,0.494303,"domain. Our task is focused on data quality that is relevant for all domains. Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic community on extraction of parallel corpora from the web has so far mostly focused on large stashes of multilingual content in homogeneous form, such as the Canadian Hansards, Europarl (Koehn, 2005), the United Nations (Rafalovitch and Dale, 2009; Ziemski et al., 2015), or European Patents (T¨ager, 2011). A nice collection of the products of these efforts is the OPUS web site2 (Tiedemann, 2012). 2.1 Parallel Corpus Acquisition The Paracrawl project is currently engaged in a large-scale effort to crawl text from the web. That work is funded by the European Union via the Connecting Europe Facility. The Paracrawl infrastructure was used to generate the noisy parallel data for this shared task. In previous years, as part of the Paracrawl effort, a shared task on document alignment (Buck and Koehn, 2016) and a shared task on parallel corpus filtering was organized (Koehn et al., 2018). Acquiring parallel corpora from the web typically goes through the stages of identifying web sites with"
W19-5404,W19-5441,0,0.0427315,"Missing"
W19-5404,D11-1126,0,0.0528814,"slation probabilities to bias towards more trustworthy ones. Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrate that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction, with a negligible loss of quality. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. It is notable that none of the participants in our shared task have tried to detect machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod e"
W19-5404,W18-6314,0,0.0587621,"ticipants used instead a classifier that learns how to distinguish between high-quality and low-quality sentence pairs. High-quality sentence pairs are selected from existing high-quality parallel corpora, while low-quality sentence pairs are either synthesized by scrambling high-quality sentence pairs or by using the raw crawled data (S´anchez-Cartagena et al., 2018). Neural machine translation model training may combine data selection and model training, taking advantage of the increasing quality of the model to better detect noisy data or to increasingly focus on cleaner parts of the data (Wang et al., 2018; Kumar et al., 2019). 2.4 Sentence Embeddings Bouamor and Sajjad (2018) learned sentence embeddings for the source and target languages and selected the nearest translation from a list of candidate sentences for a given source sentence using a classifier. Guo et al. (2018) leveraged hard negatives to correctly identify translation pairs. Artetxe and Schwenk (2018) use multilingual sentence embeddings to compute cosine similarity between the source and the target sentence. They further normalize the score by the average cosine similarity of the nearest neighbors for the given sentence pair. Th"
W19-5404,D17-1147,0,0.070816,"Missing"
W19-5435,D18-1549,0,0.141799,"e trained using the same setting as the public LASER encoder which involves normalizing texts and tokenization with Moses tools (falling back to the English mode). We first learn a joint 50k BPE vocabulary on the concatenated training data using fastBPE11 . The encoder sees Sinhala, Nepali, Hindi and English sentences at the input, without having any information about the current language. This input is always translated into English.12 We experimented with various techniques to add noise to the English input sentences, similar to what is used in unsupervised neural machine translation, e.g. (Artetxe et al., 2018; Lample et al., 2018), but this did not improve the results. The encoder is a five-layer BLSTM with 512 dimensional layers. The LSTM decoder has one hidden layer of size 2048, trained with the Adam optimizer. For development, we calculate similarity error on the concatenation of the flores dev sets for Sinhala–English and Nepali–English. Our models were trained for seven epochs for about 2.5 hours on 8 Nvidia GPUs. We experimented with various methods using a setup that closely mirrors the official scoring of the shared task. All methods are trained on the provided clean parallel data (see Ta"
W19-5435,D19-1632,1,0.878038,"Missing"
W19-5435,N19-4009,0,0.0476785,", we are provided with a very noisy 40.6 million-word (English token count) Nepali– English corpus and a 59.6 million-word Sinhala– English corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demonstrate that the proposed approach outperforms other existing approaches"
W19-5435,P13-2121,1,0.88624,"Missing"
W19-5435,W18-6319,0,0.0232643,"glish corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demonstrate that the proposed approach outperforms other existing approaches. Moreover we make use of an ensemble of multiple scoring functions to further boost the filtering performance. In this paper, w"
W19-5435,W18-6478,0,0.23575,"orks better than the global one. In that setting, LASER is on average 0.71 BLEU above the best non-LASER system. These gaps are higher for the 1M condition (0.94 BLEU). (iii) The best ensemble configuration provides small improvements over the best LASER configuration. For Sinhala–English the best configuration includes every other scoring method (ALL). For Nepali–English the best configuration is an ensemble of LASER scores. (iv) Dual cross entropy shows mixed results. For Sinhala–English, it only works once the language id filtering is enabled which is consistent with previous observations (Junczys-Dowmunt, 2018). For Nepali– English, it provides scores well below the rest of the scoring methods. Note that we did not perform an architecture exploration. LASER Encoder Training For our experiments and the official submission, we trained a multilingual sentence encoder using the permitted resources in Table 1. We trained a single encoder using all the parallel data for Sinhala–English, Nepali–English and HindiEnglish. Since Hindi and Nepali share the same script, we concatenated their corpora into a single parallel corpus. To account for the difference in size of the parallel training data, we over-sampl"
W19-5435,W18-6488,0,0.112392,"Missing"
W19-5435,W18-2709,1,0.554005,"ods and obtain additional gains. Our submission achieved the best overall performance for both the Nepali–English and Sinhala–English 1M tasks by a margin of 1.3 and 1.4 BLEU respectively, as compared to the second best systems. Moreover, our experiments show that this technique is promising for low and even no-resource scenarios. 1 Introduction The availability of high-quality parallel training data is critical for obtaining good translation performance, as neural machine translation (NMT) systems are less robust against noisy parallel data than statistical machine translation (SMT) systems (Khayrallah and Koehn, 2018). Recently, there is an increased interest in the filtering of noisy parallel corpora (such as Paracrawl1 ) to increase the amount of data that can be used to train translation systems (Koehn et al., 2018). While the state-of-the-art methods that use NMT models have proven effective in mining 1 2 http://www.paracrawl.eu/ https://github.com/facebookresearch/LASER 261 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 261–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Methodology 2.1 LASER Multi"
W19-5435,P18-2037,1,0.903256,"tion systems (Koehn et al., 2018). While the state-of-the-art methods that use NMT models have proven effective in mining 1 2 http://www.paracrawl.eu/ https://github.com/facebookresearch/LASER 261 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 261–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Methodology 2.1 LASER Multilingual Representations The underlying idea is to use the distances between two multilingual representations as a notion of parallelism between the two embedded sentences (Schwenk, 2018). To do this, we first train an encoder that learns to produce a multilingual, fixed-size sentence representation; and then compute a distance between two sentences in the learned embedding space. In addition, we use a margin criterion, which uses a k nearest neighbors approach to normalize the similarity scores given that cosine similarity is not globally consistent (Artetxe and Schwenk, 2018a). The WMT 2018 shared task for parallel corpus filtering (Koehn et al., 2018)3 introduced several methods to tackle a high-resource GermanEnglish data condition.While many of these methods were successf"
W19-5435,W18-6479,1,0.802465,"econd a local method, in which we only scored the noisy data using the noisy neighborhood, or the clean data using the clean neighborhood.5 4 We explored the absolute, distance and ratio margin criteria, but the latter worked best 5 this last part was only done for training an ensemble 3 http://statmt.org/wmt18/ parallel-corpus-filtering.html 262 2.2 Other Similarity Methods Forward and backward cross entropy scores, HF (y|x) and HB (x|y) respectively, are then averaged with an additional penalty on a large difference between the two scores |HF (y|x) − HB (x|y)|. Zipporah (Xu and Koehn, 2017; Khayrallah et al., 2018), which is often used as a baseline comparison, uses language model and word translation scores, with weights optimized to separate clean and synthetic noise data. In our setup, we trained Zipporah models for both language pairs Sinhala–English and Nepali–English. We used the open source release6 of the Zipporah tool without modifications. All components of the Zipporah model (probabilistic translation dictionaries and language models) were trained on the provided clean data (excluding the dictionaries). Language models were trained using KenLM (Heafield et al., 2013) over the clean parallel d"
W19-5435,D17-1319,1,0.834528,"th the clean data. Second a local method, in which we only scored the noisy data using the noisy neighborhood, or the clean data using the clean neighborhood.5 4 We explored the absolute, distance and ratio margin criteria, but the latter worked best 5 this last part was only done for training an ensemble 3 http://statmt.org/wmt18/ parallel-corpus-filtering.html 262 2.2 Other Similarity Methods Forward and backward cross entropy scores, HF (y|x) and HB (x|y) respectively, are then averaged with an additional penalty on a large difference between the two scores |HF (y|x) − HB (x|y)|. Zipporah (Xu and Koehn, 2017; Khayrallah et al., 2018), which is often used as a baseline comparison, uses language model and word translation scores, with weights optimized to separate clean and synthetic noise data. In our setup, we trained Zipporah models for both language pairs Sinhala–English and Nepali–English. We used the open source release6 of the Zipporah tool without modifications. All components of the Zipporah model (probabilistic translation dictionaries and language models) were trained on the provided clean data (excluding the dictionaries). Language models were trained using KenLM (Heafield et al., 2013)"
W19-5435,W19-5404,1,0.727968,"Missing"
W19-5435,P07-2045,1,0.0160857,"t known yet. For the task of low-resource filtering (Koehn et al., 2019), we are provided with a very noisy 40.6 million-word (English token count) Nepali– English corpus and a 59.6 million-word Sinhala– English corpus crawled from the web as part of the Paracrawl project. The challenge consists of providing scores for each sentence pair in both noisy parallel sets. The scores will be used to subsample sentence pairs that amount to 1 million and 5 million English words. The quality of the resulting subsets is determined by the quality of a statistical machine translation (Moses, phrase-based (Koehn et al., 2007)) and the neural machine translation system fairseq (Ott et al., 2019) trained on this data. The quality of the machine translation system will be measured by BLEU score using SacreBLEU (Post, 2018) on a held-out test set of Wikipedia translations for Sinhala–English and Nepali–English from the flores dataset (Guzm´an et al., 2019). In our submission for this shared task, we use of multilingual sentence embeddings obtained from LASER2 which uses an encoder-decoder architecture to train a multilingual sentence representation model using a relatively small parallel corpus. Our experiments demons"
W19-5435,W18-6453,1,\N,Missing
