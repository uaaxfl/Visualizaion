2021.nlp4posimpact-1.1,Restatement and Question Generation for Counsellor Chatbot,2021,-1,-1,1,1,2821,john lee,Proceedings of the 1st Workshop on NLP for Positive Impact,0,"Amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. In order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. It is thus important for the counsellor chatbot to encourage the user to open up and talk. One way to sustain the conversation flow is to acknowledge the counsellee{'}s key points by restating them, or probing them further with questions. This paper applies models from two closely related NLP tasks {---} summarization and question generation {---} to restatement and question generation in the counselling context. We conducted experiments on a manually annotated dataset of Cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. We obtained the best performance in both restatement and question generation by fine-tuning BertSum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset."
2021.latechclfl-1.10,Unsupervised Adverbial Identification in {M}odern {C}hinese Literature,2021,-1,-1,2,0,5494,wenxiu xie,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"In many languages, adverbials can be derived from words of various parts-of-speech. In Chinese, the derivation may be marked either with the standard adverbial marker DI, or the non-standard marker DE. Since DE also serves double duty as the attributive marker, accurate identification of adverbials requires disambiguation of its syntactic role. As parsers are trained predominantly on texts using the standard adverbial marker DI, they often fail to recognize adverbials suffixed with the non-standard DE. This paper addresses this problem with an unsupervised, rule-based approach for adverbial identification that utilizes dependency tree patterns. Experiment results show that this approach outperforms a masked language model baseline. We apply this approach to analyze standard and non-standard adverbial marker usage in modern Chinese literature."
2021.emnlp-main.632,Paraphrasing Compound Nominalizations,2021,-1,-1,1,1,2821,john lee,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"A nominalization uses a deverbal noun to describe an event associated with its underlying verb. Commonly found in academic and formal texts, nominalizations can be difficult to interpret because of ambiguous semantic relations between the deverbal noun and its arguments. Our goal is to interpret nominalizations by generating clausal paraphrases. We address compound nominalizations with both nominal and adjectival modifiers, as well as prepositional phrases. In evaluations on a number of unsupervised methods, we obtained the strongest performance by using a pre-trained contextualized language model to re-rank paraphrase candidates identified by a textual entailment model."
2021.bea-1.6,Character Set Construction for {C}hinese Language Learning,2021,-1,-1,2,1,12220,chak yeung,Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications,0,"To promote efficient learning of Chinese characters, pedagogical materials may present not only a single character, but a set of characters that are related in meaning and in written form. This paper investigates automatic construction of these character sets. The proposed model represents a character as averaged word vectors of common words containing the character. It then identifies sets of characters with high semantic similarity through clustering. Human evaluation shows that this representation outperforms direct use of character embeddings, and that the resulting character sets capture distinct semantic ranges."
2020.sltu-1.50,A Counselling Corpus in {C}antonese,2020,-1,-1,1,1,2821,john lee,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),0,"Virtual agents are increasingly used for delivering health information in general, and mental health assistance in particular. This paper presents a corpus designed for training a virtual counsellor in Cantonese, a variety of Chinese. The corpus consists of a domain-independent subcorpus that supports small talk for rapport building with users, and a domain-specific subcorpus that provides material for a particular area of counselling. The former consists of ELIZA style responses, chitchat expressions, and a dataset of general dialog, all of which are reusable across counselling domains. The latter consists of example user inputs and appropriate chatbot replies relevant to the specific domain. In a case study, we created a chatbot with a domain-specific subcorpus that addressed 25 issues in test anxiety, with 436 inputs solicited from native speakers of Cantonese and 150 chatbot replies harvested from mental health websites. Preliminary evaluations show that Word Mover{'}s Distance achieved 56{\%} accuracy in identifying the issue in user input, outperforming a number of baselines."
2020.paclic-1.68,"Bilingual Multi-word Expressions, Multiple-correspondence, and their cultivation from parallel patents: The {C}hinese-{E}nglish case",2020,-1,-1,3,0,15873,benjamin tsou,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.lrec-1.41,A Dataset for Investigating the Impact of Feedback on Student Revision Outcome,2020,-1,-1,2,0,13178,ildiko pilan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present an annotation scheme and a dataset of teacher feedback provided for texts written by non-native speakers of English. The dataset consists of student-written sentences in their original and revised versions with teacher feedback provided for the errors. Feedback appears both in the form of open-ended comments and error category tags. We focus on a specific error type, namely linking adverbial (e.g. however, moreover) errors. The dataset has been annotated for two aspects: (i) revision outcome establishing whether the re-written student sentence was correct and (ii) directness, indicating whether teachers provided explicitly the correction in their feedback. This dataset allows for studies around the characteristics of teacher feedback and how these influence students{'} revision outcome. We describe the data preparation process and we present initial statistical investigations regarding the effect of different feedback characteristics on revision outcome. These show that open-ended comments and mitigating expressions appear in a higher proportion of successful revisions than unsuccessful ones, while directness and metalinguistic terms have no effect. Given that the use of this type of data is relatively unexplored in natural language processing (NLP) applications, we also report some observations and challenges when working with feedback data."
2020.lrec-1.722,Automatic Compilation of Resources for Academic Writing and Evaluating with Informal Word Identification and Paraphrasing System,2020,38,0,3,0,282,seid yimam,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present the first approach to automatically building resources for academic writing. The aim is to build a writing aid system that automatically edits a text so that it better adheres to the academic style of writing. On top of existing academic resources, such as the Corpus of Contemporary American English (COCA) academic Word List, the New Academic Word List, and the Academic Collocation List, we also explore how to dynamically build such resources that would be used to automatically identify informal or non-academic words or phrases. The resources are compiled using different generic approaches that can be extended for different domains and languages. We describe the evaluation of resources with a system implementation. The system consists of an informal word identification (IWI), academic candidate paraphrase generation, and paraphrase ranking components. To generate candidates and rank them in context, we have used the PPDB and WordNet paraphrase resources. We use the Concepts in Context (CoInCO) {``}All-Words{''} lexical substitution dataset both for the informal word identification and paraphrase generation experiments. Our informal word identification component achieves an F-1 score of 82{\%}, significantly outperforming a stratified classifier baseline. The main contribution of this work is a domain-independent methodology to build targeted resources for writing aids."
2020.framenet-1.8,Using Verb Frames for Text Difficulty Assessment,2020,-1,-1,1,1,2821,john lee,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet",0,"This paper presents the first investigation on using semantic frames to assess text difficulty. Based on Mandarin VerbNet, a verbal semantic database that adopts a frame-based approach, we examine usage patterns of ten verbs in a corpus of graded Chinese texts. We identify a number of characteristics in texts at advanced grades: more frequent use of non-core frame elements; more frequent omission of some core frame elements; increased preference for noun phrases rather than clauses as verb arguments; and more frequent metaphoric usage. These characteristics can potentially be useful for automatic prediction of text readability."
2020.coling-main.196,Automatic Assistance for Academic Word Usage,2020,-1,-1,2,1,21291,dariush saberi,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper describes a writing assistance system that helps students improve their academic writing. Given an input text, the system suggests lexical substitutions that aim to incorporate more academic vocabulary. The substitution candidates are drawn from an academic word list and ranked by a masked language model. Experimental results show that lexical formality analysis can improve the quality of the suggestions, in comparison to a baseline that relies on the masked language model only."
2020.coling-main.309,Using Bilingual Patents for Translation Training,2020,-1,-1,1,1,2821,john lee,Proceedings of the 28th International Conference on Computational Linguistics,0,"While bilingual corpora have been instrumental for machine translation, their utility for training translators has been less explored. We investigate the use of bilingual corpora as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that concordancing with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus."
W19-8634,Personalized Substitution Ranking for Lexical Simplification,2019,0,0,1,1,2821,john lee,Proceedings of the 12th International Conference on Natural Language Generation,0,"A lexical simplification (LS) system substitutes difficult words in a text with simpler ones to make it easier for the user to understand. In the typical LS pipeline, the Substitution Ranking step determines the best substitution out of a set of candidates. Most current systems do not consider the user{'}s vocabulary proficiency, and always aim for the simplest candidate. This approach may overlook less-simple candidates that the user can understand, and that are semantically closer to the original word. We propose a personalized approach for Substitution Ranking to identify the candidate that is the closest synonym and is non-complex for the user. In experiments on learners of English at different proficiency levels, we show that this approach enhances the semantic faithfulness of the output, at the cost of a relatively small increase in the number of complex words."
U19-1021,Difficulty-aware Distractor Generation for Gap-Fill Items,2019,0,0,2,1,12220,chak yeung,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,
2019.ccnlg-1.6,Noun Generation for Nominalization in Academic Writing,2019,-1,-1,2,1,21291,dariush saberi,Proceedings of the 4th Workshop on Computational Creativity in Language Generation,0,None
W18-6706,Assisted Nominalization for Academic {E}nglish Writing,2018,-1,-1,1,1,2821,john lee,Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2{IS}{\\&}{NLG}),0,None
W18-1809,Register-sensitive Translation: a Case Study of {M}andarin and {C}antonese (Non-archival Extended Abstract),2018,0,1,2,1,28524,taksum wong,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 1: Research Track),0,None
L18-1647,{L}1-{L}2 Parallel Treebank of Learner {C}hinese: Overused and Underused Syntactic Structures,2018,0,0,2,0,30226,keying li,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1019,Personalizing Lexical Simplification,2018,0,3,1,1,2821,john lee,Proceedings of the 27th International Conference on Computational Linguistics,0,"A lexical simplification (LS) system aims to substitute complex words with simple words in a text, while preserving its meaning and grammaticality. Despite individual users{'} differences in vocabulary knowledge, current systems do not consider these variations; rather, they are trained to find one optimal substitution or ranked list of substitutions for all users. We evaluate the performance of a state-of-the-art LS system on individual learners of English at different proficiency levels, and measure the benefits of using complex word identification (CWI) models to personalize the system. Experimental results show that even a simple personalized CWI model, based on graded vocabulary lists, can help the system avoid some unnecessary simplifications and produce more readable output."
C18-1292,Personalized Text Retrieval for Learners of {C}hinese as a Foreign Language,2018,0,3,2,1,12220,chak yeung,Proceedings of the 27th International Conference on Computational Linguistics,0,"This paper describes a personalized text retrieval algorithm that helps language learners select the most suitable reading material in terms of vocabulary complexity. The user first rates their knowledge of a small set of words, chosen by a graph-based active learning model. The system trains a complex word identification model on this set, and then applies the model to find texts that contain the desired proportion of new, challenging, and familiar vocabulary. In an evaluation on learners of Chinese as a foreign language, we show that this algorithm is effective in identifying simpler texts for low-proficiency learners, and more challenging ones for high-proficiency learners."
W17-6530,Quantitative Comparative Syntax on the {C}antonese-{M}andarin Parallel Dependency Treebank,2017,11,0,4,1,28524,taksum wong,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
W17-6306,{L}1-{L}2 Parallel Dependency Treebank as Learner Corpus,2017,0,1,1,1,2821,john lee,Proceedings of the 15th International Conference on Parsing Technologies,0,"This opinion paper proposes the use of parallel treebank as learner corpus. We show how an L1-L2 parallel treebank {---} i.e., parse trees of non-native sentences, aligned to the parse trees of their target hypotheses {---} can facilitate retrieval of sentences with specific learner errors. We argue for its benefits, in terms of corpus re-use and interoperability, over a conventional learner corpus annotated with error tags. As a proof of concept, we conduct a case study on word-order errors made by learners of Chinese as a foreign language. We report precision and recall in retrieving a range of word-order error categories from L1-L2 tree pairs annotated in the Universal Dependency framework."
W17-6307,Splitting Complex {E}nglish Sentences,2017,0,0,1,1,2821,john lee,Proceedings of the 15th International Conference on Parsing Technologies,0,"This paper applies parsing technology to the task of syntactic simplification of English sentences, focusing on the identification of text spans that can be removed from a complex sentence. We report the most comprehensive evaluation to-date on this task, using a dataset of sentences that exhibit simplification based on coordination, subordination, punctuation/parataxis, adjectival clauses, participial phrases, and appositive phrases. We train a decision tree with features derived from text span length, POS tags and dependency relations, and show that it significantly outperforms a parser-only baseline."
W17-5903,{C}arrier Sentence Selection for Fill-in-the-blank Items,2017,0,0,2,0,9360,shu jiang,Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017),0,"Fill-in-the-blank items are a common form of exercise in computer-assisted language learning systems. To automatically generate an effective item, the system must be able to select a high-quality carrier sentence that illustrates the usage of the target word. Previous approaches for carrier sentence selection have considered sentence length, vocabulary difficulty, the position of the target word and the presence of finite verbs. This paper investigates the utility of word co-occurrence statistics and lexical similarity as selection criteria. In an evaluation on generating fill-in-the-blank items for learning Chinese as a foreign language, we show that these two criteria can improve carrier sentence quality."
W17-5015,Distractor Generation for {C}hinese Fill-in-the-blank Items,2017,0,4,2,0,9360,shu jiang,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper reports the first study on automatic generation of distractors for fill-in-the-blank items for learning Chinese vocabulary. We investigate the quality of distractors generated by a number of criteria, including part-of-speech, difficulty level, spelling, word co-occurrence and semantic similarity. Evaluations show that a semantic similarity measure, based on the word2vec model, yields distractors that are significantly more plausible than those generated by baseline methods."
W17-0408,Towards {U}niversal {D}ependencies for Learner {C}hinese,2017,0,0,1,1,2821,john lee,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),0,None
I17-3012,Automatic Difficulty Assessment for {C}hinese Texts,2017,0,0,1,1,2821,john lee,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"We present a web-based interface that automatically assesses reading difficulty of Chinese texts. The system performs word segmentation, part-of-speech tagging and dependency parsing on the input text, and then determines the difficulty levels of the vocabulary items and grammatical constructions in the text. Furthermore, the system highlights the words and phrases that must be simplified or re-written in order to conform to the user-specified target difficulty level. Evaluation results show that the system accurately identifies the vocabulary level of 89.9{\%} of the words, and detects grammar points at 0.79 precision and 0.83 recall."
I17-2055,Identifying Speakers and Listeners of Quoted Speech in Literary Works,2017,9,3,2,1,12220,chak yeung,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present the first study that evaluates both speaker and listener identification for direct speech in literary texts. Our approach consists of two steps: identification of speakers and listeners near the quotes, and dialogue chain segmentation. Evaluation results show that this approach outperforms a rule-based approach that is state-of-the-art on a corpus of literary texts."
I17-2073,Lexical Simplification with the Deep Structured Similarity Model,2017,0,0,3,0,1228,lis pereira,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We explore the application of a Deep Structured Similarity Model (DSSM) to ranking in lexical simplification. Our results show that the DSSM can effectively capture fine-grained features to perform semantic matching when ranking substitution candidates, outperforming the state-of-the-art on two standard datasets used for the task."
W16-5403,Developing {U}niversal {D}ependencies for {M}andarin {C}hinese,2016,13,0,6,1,31423,herman leung,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"This article proposes a Universal Dependency Annotation Scheme for Mandarin Chinese, including POS tags and dependency analysis. We identify cases of idiosyncrasy of Mandarin Chinese that are difficult to fit into the current schema which has mainly been based on the descriptions of various Indo-European languages. We discuss differences between our scheme and those of the Stanford Chinese Dependencies and the Chinese Dependency Treebank."
P16-4020,Personalized Exercises for Preposition Learning,2016,24,3,1,1,2821,john lee,Proceedings of {ACL}-2016 System Demonstrations,0,"We present a computer-assisted language learning (CALL) system that generates fill-in-the-blank items for preposition usage. The system takes a set of carrier sentences as input, chooses a preposition in each sentence as the key, and then automatically generates distractors. It personalizes item selection for the user in two ways. First, it logs items to which the user previously gave incorrect answers, and offers similar items in a future session as review. Second, it progresses from easier to harder sentences, to minimize any hindrance on preposition learning that might be posed by difficult vocabulary."
P16-1093,A {CALL} System for Learning Preposition Usage,2016,17,2,1,1,2821,john lee,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
L16-1168,An Annotated Corpus of Direct Speech,2016,6,3,1,1,2821,john lee,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We propose a scheme for annotating direct speech in literary texts, based on the Text Encoding Initiative (TEI) and the coreference annotation guidelines from the Message Understanding Conference (MUC). The scheme encodes the speakers and listeners of utterances in a text, as well as the quotative verbs that reports the utterances. We measure inter-annotator agreement on this annotation task. We then present statistics on a manually annotated corpus that consists of books from the New Testament. Finally, we visualize the corpus as a conversational network."
L16-1265,A Dependency Treebank of the {C}hinese Buddhist Canon,2016,14,1,2,1,28524,taksum wong,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a dependency treebank of the Chinese Buddhist Canon, which contains 1,514 texts with about 50 million Chinese characters. The treebank was created by an automatic parser trained on a smaller treebank, containing four manually annotated sutras (Lee and Kong, 2014). We report results on word segmentation, part-of-speech tagging and dependency parsing, and discuss challenges posed by the processing of medieval Chinese. In a case study, we exploit the treebank to examine verbs frequently associated with Buddha, and to analyze usage patterns of quotative verbs in direct speech. Our results suggest that certain quotative verbs imply status differences between the speaker and the listener."
C16-2003,A Reading Environment for Learners of {C}hinese as a Foreign Language,2016,7,0,1,1,2821,john lee,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"We present a mobile app that provides a reading environment for learners of Chinese as a foreign language. The app includes a text database that offers over 500K articles from Chinese Wikipedia. These articles have been word-segmented; each word is linked to its entry in a Chinese-English dictionary, and to automatically-generated review exercises. The app estimates the reading proficiency of the user based on a {``}to-learn{''} list of vocabulary items. It automatically constructs and maintains this list by tracking the user{'}s dictionary lookup behavior and performance in review exercises. When a user searches for articles to read, search results are filtered such that the proportion of unknown words does not exceed a user-specified threshold."
C16-2020,A Customizable Editor for Text Simplification,2016,0,0,1,1,2821,john lee,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"We present a browser-based editor for simplifying English text. Given an input sentence, the editor performs both syntactic and lexical simplification. It splits a complex sentence into shorter ones, and suggests word substitutions in drop-down lists. The user can choose the best substitution from the list, undo any inappropriate splitting, and further edit the sentence as necessary. A significant novelty is that the system accepts a customized vocabulary list for a target reader population. It identifies all words in the text that do not belong to the list, and attempts to substitute them with words from the list, thus producing a text tailored for the targeted readers."
W15-5949,Translation Quality and Effort: Options versus Post-editing,2015,19,0,2,0,34490,donald sturgeon,Proceedings of the 12th International Conference on Natural Language Processing,0,None
P15-2099,Automatic Detection of Sentence Fragments,2015,21,1,2,1,12220,chak yeung,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We present and evaluate a method for automatically detecting sentence fragments in English texts written by non-native speakers. Our method combines syntactic parse tree patterns and parts-of-speech information produced by a tagger to detect this phenomenon. When evaluated on a corpus of authentic learner texts, our best model achieved a precision of 0.84 and a recall of 0.62, a statistically significant improvement over baselines using non-parse features, as well as a popular grammar checker."
Y14-1063,Automatic Detection of Comma Splices,2014,31,2,1,1,2821,john lee,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"In English text, independent clauses should be demarcated with full-stops (periods), or linked together with conjunctions. Non-native speakers are often prone to linking them improperly with commas instead of conjunctions, producing comma splices. This paper describes a method to detect comma splices using Conditional Random Fields (CRF), with features derived from parse tree patterns. In experiments, our model achieved an average of 0.91 precision and 0.28 recall in detecting comma splices, significantly outperforming both a baseline model using only local features and a widely used commercial grammar checker."
W13-3409,Treebanking for Data-driven Research in the Classroom,2013,10,2,1,1,2821,john lee,Proceedings of the Fourth Workshop on Teaching {NLP} and {CL},0,"Data-driven research in linguistics typically involves the processes of data annotation, data visualization and identification of relevant patterns. We describe our experience in incorporating these processes at an undergraduate course on language information technology. Students collectively annotated the syntactic structures of a set of Classical Chinese poems; the resulting treebank was put on a platform for corpus search and visualization; finally, using this platform, students investigated research questions about the text of the treebank."
Y12-1022,Extracting Networks of People and Places from Literary Texts,2012,24,8,1,1,2821,john lee,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"We describe a method to automatically extract social networks from literary texts. Similar to those in prior research, nodes represent characters found in the texts; edges connect them to other characters with whom they interact, and also display sentences describing their interactions. Furthermore, other nodes encode places and are connected to characters who were active there. Thus, these networks present an overview of the xe2x80x9cwhoxe2x80x9d, xe2x80x9cwhatxe2x80x9d, and xe2x80x9cwherexe2x80x9d in large text corpora, visualizing associations between people and places."
W12-1011,A Classical {C}hinese Corpus with Nested Part-of-Speech Tags,2012,16,8,1,1,2821,john lee,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We introduce a corpus of classical Chinese poems that has been word segmented and tagged with parts-of-speech (POS). Due to the ill-defined concept of a 'word' in Chinese, previous Chinese corpora suffer from a lack of standardization in word segmentation, resulting in inconsistencies in POS tags, therefore hindering interoperability among corpora. We address this problem with nested POS tags, which accommodates different theories of wordhood and facilitates research objectives requiring annotations of the 'word' at different levels of granularity."
P12-2049,A Corpus of Textual Revisions in Second Language Writing,2012,21,6,1,1,2821,john lee,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper describes the creation of the first large-scale corpus containing drafts and final versions of essays written by non-native speakers, with the sentences aligned across different versions. Furthermore, the sentences in the drafts are annotated with comments from teachers. The corpus is intended to support research on textual revision by language learners, and how it is influenced by feedback. This corpus has been converted into an XML format conforming to the standards of the Text Encoding Initiative (TEI)."
N12-1020,A Dependency Treebank of Classical {C}hinese Poems,2012,14,21,1,1,2821,john lee,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"As interest grows in the use of linguistically annotated corpora in research and teaching of foreign languages and literature, treebanks of various historical texts have been developed. We introduce the first large-scale dependency treebank for Classical Chinese literature. Derived from the Stanford dependency types, it consists of over 32K characters drawn from a collection of poems written in the 8th century CE. We report on the design of new dependency relations, discuss aspects of the annotation process and evaluation, and illustrate its use in a study of parallelism in Classical Chinese poetry."
C12-2061,Glimpses of {A}ncient {C}hina from Classical {C}hinese Poems,2012,-1,-1,1,1,2821,john lee,Proceedings of {COLING} 2012: Posters,0,None
P11-1089,A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing,2011,19,24,1,1,2821,john lee,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the pipeline approach, assuming that morphological information has been separately obtained.n n However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection."
I11-1174,Toward a Parallel Corpus of Spoken {C}antonese and Written {C}hinese,2011,13,12,1,1,2821,john lee,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We introduce a parallel corpus of spoken Cantonese and written Chinese. This sentencealigned corpus consists of transcriptions of Cantonese spoken in television programs in Hong Kong, and their corresponding Chinese (Mandarin) subtitles. Preliminary evaluation shows that the corpus reflects known syntactic differences between Cantonese and Mandarin, facilitates quantitative analyses on these differences, and already reveals some phenomena not yet discussed in the literature."
lee-haug-2010-porting,Porting an {A}ncient {G}reek and {L}atin Treebank,2010,13,4,1,1,2821,john lee,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We have recently converted a dependency treebank, consisting of ancient Greek and Latin texts, from one annotation scheme to another that was independently designed. This paper makes two observations about this conversion process. First, we show that, despite significant surface differences between the two treebanks, a number of straightforward transformation rules yield a substantial level of compatibility between them, giving evidence for their sound design and high quality of annotation. Second, we analyze some linguistic annotations that require further disambiguation, proposing some simple yet effective machine learning methods."
W09-3010,Human Evaluation of Article and Noun Number Usage: Influences of Context and Construction Variability,2009,11,9,1,1,2821,john lee,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,Evaluating systems that correct errors in non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number.
W08-2117,A Nearest-Neighbor Approach to the Automatic Analysis of {A}ncient {G}reek Morphology,2008,14,5,1,1,2821,john lee,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"We propose a data-driven method for automatically analyzing the morphology of ancient Greek. This method improves on existing ancient Greek analyzers in two ways. First, through the use of a nearest-neighbor machine learning framework, the analyzer requires no hand-crafted rules. Second, it is able to predict novel roots, and to rerank its predictions by exploiting a large, unlabelled corpus of ancient Greek."
P08-1021,Correcting Misuse of Verb Forms,2008,11,48,1,1,2821,john lee,Proceedings of ACL-08: HLT,1,"This paper proposes a method to correct English verb form errors made by non-native speakers. A basic approach is template matching on parse trees. The proposed method improves on this approach in two ways. To improve recall, irregularities in parse trees caused by verb form errors are taken into account; to improve precision, n-gram counts are utilized to filter proposed corrections. Evaluation on non-native corpora, representing two genres and mother tongues, shows promising results."
P07-1011,Detecting Erroneous Sentences using Automatically Mined Sequential Patterns,2007,22,53,6,0,49181,guihua sun,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"This paper studies the problem of identifying erroneous/correct sentences. The problem has important applications, e.g., providing feedback for writers of English as a Second Language, controlling the quality of parallel bilingual sentences mined from the Web, and evaluating machine translation results. In this paper, we propose a new approach to detecting erroneous sentences by integrating pattern discovery with supervised learning models. Experimental results show that our techniques are promising."
P07-1060,A Computational Model of Text Reuse in Ancient Literary Texts,2007,12,23,1,1,2821,john lee,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We propose a computational model of text reuse tailored for ancient literary texts, available to us often only in small and noisy samples. The model takes into account source alternation patterns, so as to be able to align even sentences with low surface similarity. We demonstrate its ability to characterize text reuse in the Greek New Testament."
N07-2024,Detection of Non-Native Sentences Using Machine-Translated Training Data,2007,14,10,1,1,2821,john lee,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"Training statistical models to detect non-native sentences requires a large corpus of non-native writing samples, which is often not readily available. This paper examines the extent to which machine-translated (MT) sentences can substitute as training data.n n Two tasks are examined. For the native vs non-native classification task, non-native training data yields better performance; for the ranking task, however, models trained with a large, publicly available set of MT data perform as well as those trained with non-native data."
P06-2113,Combining Statistical and Knowledge-Based Spoken Language Understanding in Conditional Models,2006,12,28,4,0,37670,yeyi wang,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance. The traditional knowledge-based approach to this problem is very expensive --- it requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain. On the other hand, a statistical learning approach needs a large amount of annotated data for model training, which is seldom available in practical applications outside of large research labs. A generative HMM/CFG composite model, which integrates easy-to-obtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement. The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework. We also study and compare conditional random fields (CRFs) with perceptron learning for SLU. Experimental results show that the conditional models achieve more than 20% relative reduction in slot error rate over the HMM/CFG model, which had already achieved an SLU accuracy at the same level as the best results reported on the ATIS data."
2006.amta-papers.24,Combining Linguistic and Statistical Methods for Bi-directional {E}nglish {C}hinese Translation in the Flight Domain,2006,15,12,3,0,36748,stephanie seneff,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"In this paper, we discuss techniques to combine an interlingua translation framework with phrase-based statistical methods, for translation from Chinese into English. Our goal is to achieve high-quality translation, suitable for use in language tutoring applications. We explore these ideas in the context of a flight domain, for which we have a large corpus of English queries, obtained from users interacting with a dialogue system. Our techniques exploit a pre-existing English-to-Chinese translation system to automatically produce a synthetic bilingual corpus. Several experiments were conducted combining linguistic and statistical methods, and manual evaluation was conducted for a set of 460 Chinese sentences. The best performance achieved an {``}adequate{''} or better analysis (3 or above rating) on nearly 94{\%} of the 409 parsable subset. Using a Rover scheme to combine four systems resulted in an {``}adequate or better{''} rating for 88{\%} of all the utterances."
N04-2006,Automatic Article Restoration,2004,5,25,1,1,2821,john lee,Proceedings of the Student Research Workshop at {HLT}-{NAACL} 2004,0,"One common mistake made by non-native speakers of English is to drop the articles a, an, or the. We apply the log-linear model to automatically restore missing articles based on features of the noun phrase. We first show that the model yields competitive results in article generation. Further, we describe methods to adjust the model with respect to the initial quality of the sentence. Our best results are 20.5% article error rate (insertions, deletions and substitutions) for sentences where 30% of the articles have been dropped, and 38.5% for those where 70% of the articles have been dropped."
W97-1411,Referring to Displays in Multimodal Interfaces,1997,8,1,3,0,4414,daqing he,Referring Phenomena in a Multimedia Context and their Computational Treatment,0,"A system which displays information graphically, and also allows natural language queries, should allow these queries to interrogate the displayed (visual) information. Ideally this would use some uniform method for processing queries both about the display and about the world model. Such a system would have to cope with ambiguities introduced by these two sources of information. These ambiguities, and a preliminary proposal for a system to deal with it, are the main topics of this paper."
