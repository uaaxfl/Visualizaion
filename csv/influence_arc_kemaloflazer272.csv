1995.iwpt-1.24,P95-1004,0,0.0612064,"Missing"
1995.iwpt-1.24,E93-1057,0,0.0607632,"Missing"
1995.iwpt-1.24,C94-1066,0,0.0196382,"tions in the language in a uniform manner. They can be abstracted as finite state transducers over an alphabet of lexical and surface symbol pairs 1 : s, where either 1 or s (but not .both) may be the null symbol o. It is possible to apply error-tolerant recognition to languages whose word formations employ productive compounding and/or agglutination, and in fact to any language whose morphology is described completely as one (very large) finite state transducer. _ Full scale descriptions using this approach already exist for a number of languages like English, French, German, Turkish, Korean [8]. -Application of error-tolerant recognition to morphological analysis proceeds as described earlier. After a successful match with a surface symbol, the corresponding lexical symbol is appended to the output gloss string. During backtracking the candidate surface string and the gloss string are again shortened in tandem. The basic algorithm for this case is given in Figure 4.4 The actual algorithm is a slightly optimized version of this where transitions with null surface symbols are treated as special during forward and backtracking traversals to avoid unnecessary computations of the cut-off"
1995.iwpt-1.24,C92-1025,0,0.0645629,"Missing"
1995.iwpt-1.24,J96-1003,1,0.157113,"Missing"
1995.iwpt-1.24,A94-1037,1,0.883386,"Missing"
1995.iwpt-1.24,A94-1024,1,0.885645,"Missing"
1995.iwpt-1.24,C88-2146,0,0.0424569,"Missing"
1995.iwpt-1.24,E93-1046,0,0.0229277,"Missing"
2007.mtsummit-papers.61,J00-4006,0,\N,Missing
2007.mtsummit-papers.61,A00-1002,0,\N,Missing
2007.mtsummit-papers.61,E87-1021,0,\N,Missing
2007.mtsummit-papers.61,P02-1040,0,\N,Missing
2007.mtsummit-papers.61,W04-0409,1,\N,Missing
2007.mtsummit-papers.61,2005.eamt-1.12,0,\N,Missing
2021.gem-1.3,P19-1213,0,0.0335262,"Missing"
2021.gem-1.3,S16-1081,0,0.0623767,"Missing"
2021.gem-1.3,D15-1075,0,0.289457,"mantic Textual Similarity benchmark (STSb) dataset (Cer et al., 2017) into Turkish and presented the first semantic textual similarity dataset for Turkish as well. STSb dataset is a selection of data from English STS shared tasks between 2012 and 2017. These datasets have been widely used for sentence level similarity and semantic representations research (Cer et al., 2017). We also leveraged the NLI-TR dataset that has been presented recently for Turkish natural language inference task (Budur et al., 2020). The NLITR dataset combines the translated Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiGenre Natural Language Inference (MultiNLI) (Williams et al., 2018) datasets. Introduction Automatic document summarization aims to produce a summary that conveys the salient information in the given text(s). Automatic summarizers provide reduction in the size of the text, as well as, combine and cluster different sources of information, while preserving the informational content. There are two approaches to summarization: extractive and abstractive. Extractive summarization yields a summary by extracting important phrases or sentences from the document. In contrast, abstractive summ"
2021.gem-1.3,2020.emnlp-main.750,0,0.0143861,"ences and word pairs (Lin, 2004). Recently, there has been a range of studies focusing on the evaluation of factual correctness in the generated summaries. Falke et al. (2019) has studied whether textual entailment can be used to detect factual errors in generated summaries based on the idea that the source document should entail the information in a summary. The authors investigated whether factual errors can be reduced by reranking the alternative summaries using models trained on NLI datasets. They found that out-of-the-box NLI models do not perform well on the task of factual correctness. Kryscinski et al. (2020) proposed a model-based approach on the document-sentence level for verifying factual consistency in generated summaries. Zhao et al. (2020) addressed the problem of unsupported information in the generated summaries known as factual hallucination. Durmus et al. (2020) and Wang et al. (2020) suggested question answering based methods to evaluate the faithfullness of the generated summaries. In addition to the studies focusing on summarization evaluation, there are some recently proposed metrics to evaluate generated text with the gold standard. Zhang et al. (2019) proposed BERTScore that uses"
2021.gem-1.3,S17-2001,0,0.167098,"at takes an object in accusative case, ”telefon etmek” is a compound verb in Turkish and the equivalent of the accusative object in the first sentence is realized with a noun in dative case (as highlighted with underlines). Although, these sentences are semantically equivalent, ROUGE-1, ROUGE-2 and ROUGE-3 scores of these sentences are 0.25, 0, and 0.25 respectively. In this paper, we present a semantic similarity model which can be applied to abstractive summarization as a semantic evaluation metric. To this end, we translated the English Semantic Textual Similarity benchmark (STSb) dataset (Cer et al., 2017) into Turkish and presented the first semantic textual similarity dataset for Turkish as well. STSb dataset is a selection of data from English STS shared tasks between 2012 and 2017. These datasets have been widely used for sentence level similarity and semantic representations research (Cer et al., 2017). We also leveraged the NLI-TR dataset that has been presented recently for Turkish natural language inference task (Budur et al., 2020). The NLITR dataset combines the translated Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiGenre Natural Language Inference (Multi"
2021.gem-1.3,W19-5358,0,0.0238166,"tion 3, we explain natural language inference and semantic textual similarity. We present our STSb Turkish dataset and translation quality. In section 4, we present our experiments for semantic textual similarity. In section 5, we present the experiments for summarization. We applied our best performing four semantic similarity models as evaluation metrics to the summarization results. In section 6, we present our results both qualitatively and quantitatively by comparing the semantic similarity and ROUGE scores with human judgments in Pearson and Spearman correlations. 2 et al., 2020), YiSi (Lo, 2019), Prism (Thompson and Post, 2020)). 3 Methodology 3.1 Natural Language Inference Natural language inference is the study of determining whether there is an entailment, a contradiction or a neutral relationship between a hypothesis and a given premise. There are two major corpora in literature for natural language inference in English. These are Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiGenre Natural Language Inference (MultiNLI) (Williams et al., 2018) datasets. The SNLI corpus is about 570k sentence pairs while the MultiNLI corpus is about 433k sentence pairs."
2021.gem-1.3,2020.acl-main.747,0,0.0257935,"Missing"
2021.gem-1.3,N19-1423,0,0.0659313,"sed a model-based approach on the document-sentence level for verifying factual consistency in generated summaries. Zhao et al. (2020) addressed the problem of unsupported information in the generated summaries known as factual hallucination. Durmus et al. (2020) and Wang et al. (2020) suggested question answering based methods to evaluate the faithfullness of the generated summaries. In addition to the studies focusing on summarization evaluation, there are some recently proposed metrics to evaluate generated text with the gold standard. Zhang et al. (2019) proposed BERTScore that uses BERT (Devlin et al., 2019) to compute a similarity score between the generated and reference text. Several recent works proposed new evaluaiton metrics for machine translation (BLEURT (Sellam et al., 2020), COMET (Rei 3.2 Semantic Textual Similarity Semantic textual similarity aims to determine how similar two pieces of texts are. There are many application areas such as machine translation, summarization, text generation, question answering, dialogue and speech systems. It has become a remarkable area with the competitions organized by SemEval since 2012. Semantic textual similarity studies are very common in English,"
2021.gem-1.3,P02-1040,0,0.109623,"insan restoran masasında oturuyor. In this paper, we assumed that such translation errors will not cause a major problem in our similarity models. In order to verify our assumption, we tested the quality of translations by selecting 50 sentence pairs (100 sentences) randomly, considering the percentage of the categories in the dataset. So, 6, 19 and 25 pairs chosen from forum, caption and news categories respectively. These sentences were translated by three native Turkish speakers who are fluent in English. We evaluated quality of the system translations with the three references using BLEU (Papineni et al., 2002) score. We used the SacreBLEU4 tool (Post, 2018) version 1.5.1 and found BLEU score as 60.21 which shows that our system translations can be considered as very high quality translations (Google). Therefore, no changes have been made to the translations. Translation Quality It is possible to encounter some translation errors in the translated texts. The most striking mistakes are related to expressions that are not used in Turkish. For instance, the sentence in S1 is translated as T1; however, a more appropriate translation would be C1, as ”sitting” is translated differently for inanimate subje"
2021.gem-1.3,P19-1493,0,0.0219368,"erimented with state-of-theart sentence representation models that are applicable to Turkish (language-specific and multilingual models) and BERT cross-encoders. In sentence representation models, we obtained the semantic similarity scores using cosine similarity. All models were tested on the STSb-TR test dataset. 4.1 BERT Bidirectional Encoder Representations from Transformers (BERT) is designed to pretrain deep bi-directional representations from unlabeled text by conditioning together in both left and right context on all layers (Devlin et al., 2019). In this study, BERTurk9 and M-BERT10 (Pires et al., 2019) models were used. Sentence embeddings were obtained by averaging the BERT embeddings.11 In addition, the models were integrated into the Siamese network that we explained in section 4.1. Sentence Representation Models We experimented with LASER, LaBSE, MUSE, BERT, XLM-R and Sentence-BERT models as explained below. XLM-R RoBERTa Transformer model12 has been trained on a large multilingual data using a multilingual masked language modeling goal (Conneau et al., 2020). In this study, we used the model to compute sentence embeddings similar to BERT models. We also integrated it into the Siamese n"
2021.gem-1.3,2020.acl-main.454,0,0.0217318,"s based on the idea that the source document should entail the information in a summary. The authors investigated whether factual errors can be reduced by reranking the alternative summaries using models trained on NLI datasets. They found that out-of-the-box NLI models do not perform well on the task of factual correctness. Kryscinski et al. (2020) proposed a model-based approach on the document-sentence level for verifying factual consistency in generated summaries. Zhao et al. (2020) addressed the problem of unsupported information in the generated summaries known as factual hallucination. Durmus et al. (2020) and Wang et al. (2020) suggested question answering based methods to evaluate the faithfullness of the generated summaries. In addition to the studies focusing on summarization evaluation, there are some recently proposed metrics to evaluate generated text with the gold standard. Zhang et al. (2019) proposed BERTScore that uses BERT (Devlin et al., 2019) to compute a similarity score between the generated and reference text. Several recent works proposed new evaluaiton metrics for machine translation (BLEURT (Sellam et al., 2020), COMET (Rei 3.2 Semantic Textual Similarity Semantic textual si"
2021.gem-1.3,W18-6319,0,0.0214274,"d that such translation errors will not cause a major problem in our similarity models. In order to verify our assumption, we tested the quality of translations by selecting 50 sentence pairs (100 sentences) randomly, considering the percentage of the categories in the dataset. So, 6, 19 and 25 pairs chosen from forum, caption and news categories respectively. These sentences were translated by three native Turkish speakers who are fluent in English. We evaluated quality of the system translations with the three references using BLEU (Papineni et al., 2002) score. We used the SacreBLEU4 tool (Post, 2018) version 1.5.1 and found BLEU score as 60.21 which shows that our system translations can be considered as very high quality translations (Google). Therefore, no changes have been made to the translations. Translation Quality It is possible to encounter some translation errors in the translated texts. The most striking mistakes are related to expressions that are not used in Turkish. For instance, the sentence in S1 is translated as T1; however, a more appropriate translation would be C1, as ”sitting” is translated differently for inanimate subjects. S1: Old green bottle sitting on a table. Ta"
2021.gem-1.3,2020.acl-demos.12,0,0.0169425,"9 8,628 trained on multilingual data for translation language modeling. The model produces languageindependent sentence embeddings for 109 languages, including Turkish (Feng et al., 2020). Similar to the LASER model, Turkish sentence embeddings were computed using a pre-trained LaBSE model. Table 3: STSb dataset statistics in terms of number of sentence pairs. 4 MUSE Multilingual Universal Sentence Encoder (MUSE) model is a sentence embedding model trained on multiple languages at the same time. The model creates a common semantic embedding area for a total of 16 languages, including Turkish (Yang et al., 2020). In this study, CNN7 and Transformer8 models that are shared publicly in TensorFlow Hub are used. Experiments for Semantic Textual Similarity In order to assess the semantic similarity between a pair of texts, there are two main model structures: 1) Sentence representation models that try to map a sentence to a fixed-sized real-value vectors called sentence embeddings. 2) Cross-encoders that directly compute the semantic similarity score of a sentence pair. In this paper, we experimented with state-of-theart sentence representation models that are applicable to Turkish (language-specific and"
2021.gem-1.3,2020.emnlp-main.213,0,0.0476143,"Missing"
2021.gem-1.3,D19-1410,0,0.0886852,"//github.com/facebookresearch/LASER 27 Model Not trained for STS Avg. BERTurk embeddings Avg. M-BERT embeddings Avg. XLM-R embeddings LASER LaBSE MUSE-CNN MUSE-Transformer Trained on STS BERTurk + STS M-BERT + STS XLM-R + STS S-BERTurk + STS S-M-BERT + STS S-XLM-R + STS Trained on NLI + STS BERTurk + NLI + STS M-BERT + NLI + STS XLM-R + NLI + STS S-BERTurk + NLI + STS S-M-BERT + NLI + STS S-XLM-R + NLI + STS Sentence-BERT Sentence-BERT (SBERT) (also called Bi-Encoder BERT) is a modification of pre-trained BERT network (or other transformer models) using Siamese and ternary network structures (Reimers and Gurevych, 2019). The model derives close fixed-size sentence embedding in vector space for semantically similar sentences. The training loss function differs depending on the dataset the model was trained on. During the training on the NLI dataset, the classification objective function was used; whereas during the training on the STSb dataset, the regression objective function was used (Reimers and Gurevych, 2019). The classification objective function concatenates the sentence embeddings by element-wise difference and multiplies by a trainable weight. The model optimizes the cross entropy loss: where n is t"
2021.gem-1.3,2020.findings-emnlp.203,0,0.0287056,"Missing"
2021.gem-1.3,2020.emnlp-main.647,0,0.0160491,"verage Pearson Spearman 36.79 37.51 32.76 38.02 35.15 35.11 37.88 38.07 44.26 45.86 48.80 51.85 51.32 48.59 53.54 52.10 Table 6: Pearson and Spearman correlations of ROUGE, BERTScore and proposed evaluation metrics with human judgments. 5.1 Lead-3 We selected the first three sentences of the source text as a summary, based on the observation that the leading three sentences are a strong baseline for summarization (Nallapati et al., 2017; Sharma et al., 2019). Dataset MLSUM is the first large-scale MultiLingual SUMmarization dataset which contains 1.5M+ article/summary pairs including Turkish (Scialom et al., 2020). The authors compiled the dataset following the same methodology of CNN/DailyMail dataset. They considered news articles as the text input and their paired highlights/description as the summary. Turkish dataset was created from Internet Haber15 by crawling archived articles between 2010 and 2019. All the articles shorter than 50 words or summaries shorter than 10 words were discarded. The data was split into train, validation and test sets, with respect to the publication dates. The data from 2010 to 2018 was used for training; data between January-April 2019 was used for validation; and data"
2021.gem-1.3,2020.acl-main.704,0,0.0163478,"tion in the generated summaries known as factual hallucination. Durmus et al. (2020) and Wang et al. (2020) suggested question answering based methods to evaluate the faithfullness of the generated summaries. In addition to the studies focusing on summarization evaluation, there are some recently proposed metrics to evaluate generated text with the gold standard. Zhang et al. (2019) proposed BERTScore that uses BERT (Devlin et al., 2019) to compute a similarity score between the generated and reference text. Several recent works proposed new evaluaiton metrics for machine translation (BLEURT (Sellam et al., 2020), COMET (Rei 3.2 Semantic Textual Similarity Semantic textual similarity aims to determine how similar two pieces of texts are. There are many application areas such as machine translation, summarization, text generation, question answering, dialogue and speech systems. It has become a remarkable area with the competitions organized by SemEval since 2012. Semantic textual similarity studies are very common in English, and are based on datasets that are annotated and given similarity scores by human annotators. However, annotation is costly and time consuming. Recently, with the increase of suc"
2021.gem-1.3,P19-1212,0,0.0252802,"32.67 38.02 32.46 39.95 34.24 Fluency Pearson Spearman 21.40 20.30 16.43 20.83 20.17 18.63 24.74 19.85 25.63 26.70 31.24 30.17 34.10 27.88 34.62 29.31 Human Average Pearson Spearman 36.79 37.51 32.76 38.02 35.15 35.11 37.88 38.07 44.26 45.86 48.80 51.85 51.32 48.59 53.54 52.10 Table 6: Pearson and Spearman correlations of ROUGE, BERTScore and proposed evaluation metrics with human judgments. 5.1 Lead-3 We selected the first three sentences of the source text as a summary, based on the observation that the leading three sentences are a strong baseline for summarization (Nallapati et al., 2017; Sharma et al., 2019). Dataset MLSUM is the first large-scale MultiLingual SUMmarization dataset which contains 1.5M+ article/summary pairs including Turkish (Scialom et al., 2020). The authors compiled the dataset following the same methodology of CNN/DailyMail dataset. They considered news articles as the text input and their paired highlights/description as the summary. Turkish dataset was created from Internet Haber15 by crawling archived articles between 2010 and 2019. All the articles shorter than 50 words or summaries shorter than 10 words were discarded. The data was split into train, validation and test s"
2021.gem-1.3,2020.emnlp-main.8,0,0.017312,"natural language inference and semantic textual similarity. We present our STSb Turkish dataset and translation quality. In section 4, we present our experiments for semantic textual similarity. In section 5, we present the experiments for summarization. We applied our best performing four semantic similarity models as evaluation metrics to the summarization results. In section 6, we present our results both qualitatively and quantitatively by comparing the semantic similarity and ROUGE scores with human judgments in Pearson and Spearman correlations. 2 et al., 2020), YiSi (Lo, 2019), Prism (Thompson and Post, 2020)). 3 Methodology 3.1 Natural Language Inference Natural language inference is the study of determining whether there is an entailment, a contradiction or a neutral relationship between a hypothesis and a given premise. There are two major corpora in literature for natural language inference in English. These are Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiGenre Natural Language Inference (MultiNLI) (Williams et al., 2018) datasets. The SNLI corpus is about 570k sentence pairs while the MultiNLI corpus is about 433k sentence pairs. The MultiNLI corpus is in the sam"
2021.gem-1.3,2020.acl-main.450,0,0.0163894,"the source document should entail the information in a summary. The authors investigated whether factual errors can be reduced by reranking the alternative summaries using models trained on NLI datasets. They found that out-of-the-box NLI models do not perform well on the task of factual correctness. Kryscinski et al. (2020) proposed a model-based approach on the document-sentence level for verifying factual consistency in generated summaries. Zhao et al. (2020) addressed the problem of unsupported information in the generated summaries known as factual hallucination. Durmus et al. (2020) and Wang et al. (2020) suggested question answering based methods to evaluate the faithfullness of the generated summaries. In addition to the studies focusing on summarization evaluation, there are some recently proposed metrics to evaluate generated text with the gold standard. Zhang et al. (2019) proposed BERTScore that uses BERT (Devlin et al., 2019) to compute a similarity score between the generated and reference text. Several recent works proposed new evaluaiton metrics for machine translation (BLEURT (Sellam et al., 2020), COMET (Rei 3.2 Semantic Textual Similarity Semantic textual similarity aims to determ"
2021.gem-1.3,N18-1101,0,0.205727,"Turkish and presented the first semantic textual similarity dataset for Turkish as well. STSb dataset is a selection of data from English STS shared tasks between 2012 and 2017. These datasets have been widely used for sentence level similarity and semantic representations research (Cer et al., 2017). We also leveraged the NLI-TR dataset that has been presented recently for Turkish natural language inference task (Budur et al., 2020). The NLITR dataset combines the translated Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiGenre Natural Language Inference (MultiNLI) (Williams et al., 2018) datasets. Introduction Automatic document summarization aims to produce a summary that conveys the salient information in the given text(s). Automatic summarizers provide reduction in the size of the text, as well as, combine and cluster different sources of information, while preserving the informational content. There are two approaches to summarization: extractive and abstractive. Extractive summarization yields a summary by extracting important phrases or sentences from the document. In contrast, abstractive summarization provides a much more human-like summary by capturing the internal s"
A94-1024,H92-1022,0,0.0241165,"ves the parser from dealing with them at the syntactic level. Furthermore, it is also possible to recognize various proper nouns with this functionality. Such help from a tagging functionality would simplify the development of parsers for Turkish (Demir, 1993; Giing6rdii, 1993). Researchers have used a number of different approaches for building text taggers. Karlsson (Karlsson, 1990) has used a rule-based approach where the central idea is to maximize the use of morphological information. Local constraints expressed as rules basically discard many alternative parses whenever possible. Brill (Brill, 1992) has designed a rule-based tagger for English. The tagger works by automatically recognizing rules and remedying its weaknesses, thereby incrementally improving its performance. More recently, there has been a ruleAn example We can describe the process of tagging by showing the analysis for the sentence: iflen d6ner dfnmez evimizin yak~nmda bulunan derin gb&apos;lde yiizerek gev~emek en biiyiik zevkimdi. (Relaxing by swimming the deep lake near our house, as soon as I return from work was my greatest pleasure.) which we assume has been processed by the morphological analyzer with the following outp"
A94-1024,A88-1019,0,0.0193999,"-POSS door of the house using genitive-possessive agreement constraints. As a more complex case we can give the following: allnml§ 1 5 ADJ(al)+2SG-POSS+NtoV0+NARR+3SG 2 (V) (it) was your red (one) ADJ(al)+GEN+NtoV0+NARR+3SG (V) (it) belongs to the red (one) N(ahn) +NtoV0 +NARR+3SG (V) (it) was a forehead V(al) +PASS+VtoAdj (mis) (ADJ) (a) taken (object) V(al)+PASS+NARR+3SG 6 V(ahn)+VtoAdj(mis) 2 a 4 (v) based approach implemented with finite-state machines (Koskenniemi et al., 1992; Voutilainen and Tapanainen, 1993). A completely different approach to tagging uses statistical methods, (e.g., (Church, 1988; Cutting et al., 1993)). These systems essentially train a statistical model using a previously hand-tagged corpus and provide the capability of resolving ambiguity on the basis of most likely interpretation. The models that have been widely used assume that the part-ofspeech of a word depends on the categories of the two preceding words. However, the applicability of such approaches to word-order free languages remains to be seen. 2.1 (it) was taken (ADJ) (an) offended (person) V(ahn)+NARR+3SG (V) (s/he) was offended It is in general rather hard to select one of these interpretations without"
A94-1024,E93-1066,1,0.870954,"Missing"
A94-1024,E93-1046,0,0.145747,"Missing"
A94-1024,C94-1081,1,0.871847,"Missing"
A94-1024,C90-3030,0,0.0128818,"e very productive multi-word constructs, like ko~-a ko~-a run+OPT+3SG run+OPT+3SG 7 yap-ar yap-ma-z do+AOR+3SG do+NEG+AOR+3SG where both components are verbal but the compound construct is a manner or temporal adverb. This relieves the parser from dealing with them at the syntactic level. Furthermore, it is also possible to recognize various proper nouns with this functionality. Such help from a tagging functionality would simplify the development of parsers for Turkish (Demir, 1993; Giing6rdii, 1993). Researchers have used a number of different approaches for building text taggers. Karlsson (Karlsson, 1990) has used a rule-based approach where the central idea is to maximize the use of morphological information. Local constraints expressed as rules basically discard many alternative parses whenever possible. Brill (Brill, 1992) has designed a rule-based tagger for English. The tagger works by automatically recognizing rules and remedying its weaknesses, thereby incrementally improving its performance. More recently, there has been a ruleAn example We can describe the process of tagging by showing the analysis for the sentence: iflen d6ner dfnmez evimizin yak~nmda bulunan derin gb&apos;lde yiizerek ge"
A94-1024,C92-1027,0,\N,Missing
A94-1024,A92-1018,0,\N,Missing
A94-1024,E93-1069,0,\N,Missing
A94-1037,E93-1066,1,0.78989,"Missing"
bouamor-etal-2014-multidialectal,N12-1006,0,\N,Missing
bouamor-etal-2014-multidialectal,2006.amta-papers.21,0,\N,Missing
bouamor-etal-2014-multidialectal,E06-1047,1,\N,Missing
bouamor-etal-2014-multidialectal,P13-2001,0,\N,Missing
bouamor-etal-2014-multidialectal,J14-1006,0,\N,Missing
bouamor-etal-2014-multidialectal,P11-1122,0,\N,Missing
bouamor-etal-2014-multidialectal,N13-1036,1,\N,Missing
bouamor-etal-2014-multidialectal,habash-etal-2012-conventional,1,\N,Missing
bouamor-etal-2014-multidialectal,N13-1044,1,\N,Missing
bouamor-etal-2014-multidialectal,pasha-etal-2014-madamira,1,\N,Missing
bouamor-etal-2014-multidialectal,I13-1048,0,\N,Missing
bouamor-etal-2014-multidialectal,zribi-etal-2014-conventional,1,\N,Missing
bouamor-etal-2014-multidialectal,P13-2081,0,\N,Missing
bouamor-etal-2014-multidialectal,al-sabbagh-girju-2010-mining,0,\N,Missing
bouamor-etal-2014-multidialectal,W12-2301,1,\N,Missing
bouamor-etal-2014-multidialectal,N07-5003,1,\N,Missing
C00-1042,J95-4004,0,0.0114959,"dies in tagging and morphological disambiguation using various techniques. POS tagging systems have used either a statistical or a rule-based approach. In the statistical approach, a large corpus has been used to train a probabilistic model which then has been used to tag new text, assigning the most likely tag for a given word in a given context (e.g., Church (1988), Cutting et al. (1992)). In the rule-based approach, a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context (Karlsson et al., 1995). Brill (1995a) has presented a transformation-based learning approach, which induces disambiguation rules from tagged corpora. Morphological disambiguation in in ecting or agglutinative languages with complex morphology involves more than determining the major or minor parts-of-speech of the lexical items. Typically, morphology marks a number of in ectional or derivational features and this involves ambiguity. For instance, a given word may be chopped up in di erent ways into morphemes, a given morpheme may mark di erent features depending on the morphotactics, or lexicalized variants of derived words may"
C00-1042,W95-0101,0,0.0279339,"dies in tagging and morphological disambiguation using various techniques. POS tagging systems have used either a statistical or a rule-based approach. In the statistical approach, a large corpus has been used to train a probabilistic model which then has been used to tag new text, assigning the most likely tag for a given word in a given context (e.g., Church (1988), Cutting et al. (1992)). In the rule-based approach, a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context (Karlsson et al., 1995). Brill (1995a) has presented a transformation-based learning approach, which induces disambiguation rules from tagged corpora. Morphological disambiguation in in ecting or agglutinative languages with complex morphology involves more than determining the major or minor parts-of-speech of the lexical items. Typically, morphology marks a number of in ectional or derivational features and this involves ambiguity. For instance, a given word may be chopped up in di erent ways into morphemes, a given morpheme may mark di erent features depending on the morphotactics, or lexicalized variants of derived words may"
C00-1042,A88-1019,0,0.00939913,"summarize relevant aspects of Turkish and present details of various statistical models for morphological disambiguation for Turkish. We then present results and analyses from our experiments. 2 Related Work There has been a large number of studies in tagging and morphological disambiguation using various techniques. POS tagging systems have used either a statistical or a rule-based approach. In the statistical approach, a large corpus has been used to train a probabilistic model which then has been used to tag new text, assigning the most likely tag for a given word in a given context (e.g., Church (1988), Cutting et al. (1992)). In the rule-based approach, a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context (Karlsson et al., 1995). Brill (1995a) has presented a transformation-based learning approach, which induces disambiguation rules from tagged corpora. Morphological disambiguation in in ecting or agglutinative languages with complex morphology involves more than determining the major or minor parts-of-speech of the lexical items. Typically, morphology marks a number of in ectional or derivat"
C00-1042,A92-1018,0,0.0216773,"ant aspects of Turkish and present details of various statistical models for morphological disambiguation for Turkish. We then present results and analyses from our experiments. 2 Related Work There has been a large number of studies in tagging and morphological disambiguation using various techniques. POS tagging systems have used either a statistical or a rule-based approach. In the statistical approach, a large corpus has been used to train a probabilistic model which then has been used to tag new text, assigning the most likely tag for a given word in a given context (e.g., Church (1988), Cutting et al. (1992)). In the rule-based approach, a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context (Karlsson et al., 1995). Brill (1995a) has presented a transformation-based learning approach, which induces disambiguation rules from tagged corpora. Morphological disambiguation in in ecting or agglutinative languages with complex morphology involves more than determining the major or minor parts-of-speech of the lexical items. Typically, morphology marks a number of in ectional or derivational features and this"
C00-1042,P98-1062,0,0.0240296,"for the di erent kinds of morphological ambiguities in Turkish.) We assume that all syntactically relevant features of word forms have to be determined correctly for morphological disambiguation. In this context, there have been some interesting previous studies for di erent languages. Levinger et al. (1995) have reported on an approach that learns morpholexical probabilities from an untagged corpus and have used the resulting information in morphological disambiguation in Hebrew. Hajic and Hladka (1998) have used maximum entropy modeling approach for morphological disambiguation in Czech. Ezeiza et al. (1998) have combined stochastic and rule-based disambiguation methods for Basque. Megyesi (1999) has adapted Brill&apos;s POS tagger with extended lexical templates to Hungarian. Previous approaches to morphological disambiguation of Turkish text had employed a constraintbased approach (O azer and Kuruoz, 1994; Oflazer and Tur, 1996; O azer and Tur, 1997). Although results obtained earlier in these approaches were reasonable, the fact that the constraint rules were hand crafted posed a rather serious impediment to the generality and improvement of these systems. 3 Turkish Turkish is a free constituent"
C00-1042,P98-1080,0,0.1122,"Missing"
C00-1042,J95-3004,0,0.105323,"nal features and this involves ambiguity. For instance, a given word may be chopped up in di erent ways into morphemes, a given morpheme may mark di erent features depending on the morphotactics, or lexicalized variants of derived words may interact with productively derived versions (see O azer and Tur (1997) for the di erent kinds of morphological ambiguities in Turkish.) We assume that all syntactically relevant features of word forms have to be determined correctly for morphological disambiguation. In this context, there have been some interesting previous studies for di erent languages. Levinger et al. (1995) have reported on an approach that learns morpholexical probabilities from an untagged corpus and have used the resulting information in morphological disambiguation in Hebrew. Hajic and Hladka (1998) have used maximum entropy modeling approach for morphological disambiguation in Czech. Ezeiza et al. (1998) have combined stochastic and rule-based disambiguation methods for Basque. Megyesi (1999) has adapted Brill&apos;s POS tagger with extended lexical templates to Hungarian. Previous approaches to morphological disambiguation of Turkish text had employed a constraintbased approach (O azer and Ku"
C00-1042,W99-0633,0,0.0180002,"ly relevant features of word forms have to be determined correctly for morphological disambiguation. In this context, there have been some interesting previous studies for di erent languages. Levinger et al. (1995) have reported on an approach that learns morpholexical probabilities from an untagged corpus and have used the resulting information in morphological disambiguation in Hebrew. Hajic and Hladka (1998) have used maximum entropy modeling approach for morphological disambiguation in Czech. Ezeiza et al. (1998) have combined stochastic and rule-based disambiguation methods for Basque. Megyesi (1999) has adapted Brill&apos;s POS tagger with extended lexical templates to Hungarian. Previous approaches to morphological disambiguation of Turkish text had employed a constraintbased approach (O azer and Kuruoz, 1994; Oflazer and Tur, 1996; O azer and Tur, 1997). Although results obtained earlier in these approaches were reasonable, the fact that the constraint rules were hand crafted posed a rather serious impediment to the generality and improvement of these systems. 3 Turkish Turkish is a free constituent order language. The order of the constituents may change freely according to the discours"
C00-1042,A94-1024,1,0.89016,"Missing"
C00-1042,W96-0207,1,0.871567,"Missing"
C00-1042,P97-1029,1,0.865525,"Missing"
C00-1042,P99-1033,1,0.898211,"sever any relationships with an adverbial modi er modifying the root. Thus instead of a simple POS tag, we use the full morphological analyses of the words, represented as a combination of features (including any derivational markers) as their morphosyntactic tags. For instance in the example above, we would use everything including the root form as the morphosyntactic tag. In order to alleviate the data sparseness problem we break down the full tags. We represent each word as a sequence of in ectional groups (IGs hereafter), separated by ^DBs denoting derivation boundaries, as described by O azer (1999). Thus a morphological parse would be represented in the following general form: 1 The morphological features other than the POSs are: +Become: become verb, +Caus: causative verb, +Pos: Positive polarity, +Inf: marker that derives an in nitive form from a verb, +A3sg: 3sg number-person agreement, +Pnon: No possessive agreement, and +Nom: Nominative case. ^DB&apos;s mark derivational boundaries. Full Tags (No roots) In ectional Groups Possible Observed 1 9,129 10,531 2,194 Table 3: Numbers of Tags and IGs in our case P (wi jtn1 ) = P (wi jti ) = 1, we can write: ( P W jT 1. 2. 3. 4. Adj Verb+Become"
C00-1042,J95-2001,0,\N,Missing
C00-1042,W96-0102,0,\N,Missing
C00-1042,J88-1003,0,\N,Missing
C00-1042,W96-0213,0,\N,Missing
C00-1042,A00-2013,0,\N,Missing
C00-1042,W98-1303,0,\N,Missing
C00-1042,C98-1077,0,\N,Missing
C00-1042,A00-1031,0,\N,Missing
C00-1042,P98-1063,0,\N,Missing
C00-1042,C98-1060,0,\N,Missing
C92-1010,C73-2026,0,\N,Missing
C94-1081,J86-2006,0,0.0593165,"Missing"
C94-1081,E93-1066,1,0.774186,"Missing"
C94-1081,A94-1024,1,\N,Missing
C94-1081,E93-1024,0,\N,Missing
C94-1081,J87-1004,0,\N,Missing
C96-2144,J85-2001,0,0.0191813,"ccur in order resolve a sense. 4. Lexieal constraints that indicate any specific constraints on the heads of the arguments in order to convey a certain sense, and usually constrain the stem of the head noun to be a certain lexical form, or one of a small s e t &apos; o f lexical forms. 5. Semantic Constraints that indicate semantic selectional restriction constraints that may resolved using a companion ontological database (again implemented in TFS) in which we model the world by defining semantic categories, such as human, thing, nonliving object, living object, etc., along the lines described by Nagao et al. (1985). Figure 2 illustrates the simplified form of the constraint-sense mapping of the verb yc (eal). a.a Valeney Changing Transtbrmations As we have already stated, we encode senses of verbs in active voice unless a verb has an idiomatic usage with obligatory passive, causative and/or reflexive voices. 2 In order to handle these valency changing transfor-mations, we dellne lexical rules as shown in Figure 3. VERB: Refl~.xivi/~,ql(-iN: ] Rellexive:[IlittleI ~I IN: < t, Rellexive:~+[&quot;~ c,,.,~,.~.,,,~,,,/IN:~- ~ INPUT ~ CASEI,RAME c, ~a ve i P~ssbi*ali~,rl ( IN:&quot;~ ~ AII,GS: [DHbOILI: [~] |AG-N-O]]J:"
C96-2145,J96-1003,1,0.800757,"m - 1], Y [ n - 1]) dist(X[O],Z[n]) dist(X[m],Y[O]) = : Boundary Conditions ,~. S m.,5&apos; in the trie Our concern in this work is not the exact m a t c h of trees but rather a p p r o x i m a t e match. Given the vertex list sequence for a query tree, exact m a t c h over the trie can be performed using the standard t;ech niques by fbllowing the edge labeled with next vertex list until a loft in the trie is reached, ~-md the query vertex label sequence is exhausted. For a p p r o x i m a t e tree matching, we use the errortolerant a p p r o x i m a t e tinite-state recognition alg o r i t h m (Oflazer, 1996), which tinds all strings within a giwm error threshold of some string in the regular set accepted by the underlying finitestate acceptor. An a d a p t a t i o n of this algorithm will be briefly summarized here. hh:ror-tolerant m a t c h i n g of vertex list sequences requires an errol: inetric for measuring how rnuch two such sequences deviate from each other. The distance between two sequences measures the mini m u m n u m b e r of insertions, deletions and leaf label changes t h a t are necessary to convert one tree into another. It should be noted t h a t this is different fl:om the error"
C96-2145,C90-3044,0,0.0416161,"Missing"
C98-2203,H92-1022,0,0.0534733,"ith 4(}0 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 97.89~ on the training corpus and an average accuracy of 97.50~o on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision. 1 impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993). Brill (1992; 1994; 1995) has presented a transformationbased learning approach. This paper extends a novel approach to constraint-based tagging first applied for Turkish (Oflazer and Tiir, 1997), which relieves the rule developer from worrying about contlicting rule ordering requirements and constraints. The approach depends on assigning votes to constraints via statistical a n d / o r manual means, and then letting constraints vote on matching sequences on tokens, as depicted in Figure 1. This approach does not reflect the outcome of matching constraints to the set of morphological parses immediately as"
C98-2203,J95-4004,0,0.571872,"Missing"
C98-2203,A88-1019,0,0.696133,"are applied to a sentence, tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independe,d of the order of rule applications. Introduction Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information. Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g., Church (1988), Cutting et al. (1992), DeR,ose (1988)), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate 1277 W1 W2 W3 W4 tl tl .tl tl R1 R3 R2 .-. Wn ... Rm Tokens tl voting Rules Figure 1: Voting Constraint Rules (can,MD) © (can,MD) (can,MD) (the,DT}~ (I,PRP} ~ Figure 2: Representing sentences with a directed acyclic graph 2 Tagging by Path Voting Constraints We assume that sentences are delineated and that each token is assigned all possible tags by a lexicon or by a morphological analyzer. We represent each sentence as a standa"
C98-2203,A92-1018,0,0.0624836,"a sentence, tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independe,d of the order of rule applications. Introduction Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information. Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g., Church (1988), Cutting et al. (1992), DeR,ose (1988)), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate 1277 W1 W2 W3 W4 tl tl .tl tl R1 R3 R2 .-. Wn ... Rm Tokens tl voting Rules Figure 1: Voting Constraint Rules (can,MD) © (can,MD) (can,MD) (the,DT}~ (I,PRP} ~ Figure 2: Representing sentences with a directed acyclic graph 2 Tagging by Path Voting Constraints We assume that sentences are delineated and that each token is assigned all possible tags by a lexicon or by a morphological analyzer. We represent each sentence as a standard chart using a direct"
C98-2203,E93-1046,0,0.093945,"reet Journal Corpus indicate that with 4(}0 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 97.89~ on the training corpus and an average accuracy of 97.50~o on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision. 1 impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993). Brill (1992; 1994; 1995) has presented a transformationbased learning approach. This paper extends a novel approach to constraint-based tagging first applied for Turkish (Oflazer and Tiir, 1997), which relieves the rule developer from worrying about contlicting rule ordering requirements and constraints. The approach depends on assigning votes to constraints via statistical a n d / o r manual means, and then letting constraints vote on matching sequences on tokens, as depicted in Figure 1. This approach does not reflect the outcome of matching constraints to the set of morphological parses i"
C98-2203,J93-2006,0,0.101686,"Missing"
C98-2203,J88-1003,0,\N,Missing
C98-2203,E95-1022,0,\N,Missing
C98-2203,E93-1069,0,\N,Missing
C98-2203,P97-1029,1,\N,Missing
D14-1026,E06-1032,0,0.372467,"Missing"
D14-1026,W11-2103,0,0.0501888,"-scale gold-standard data. 3 NIST MEDAR WIKI # of Documents 100 4 7 # of Sentences 1056 509 327 Table 1: Statistics on the datasets. We use six state-of-the-art English-to-Arabic MT systems. These include four research-oriented phrase-based systems with various morphological and syntactic features and different Arabic tokenization schemes and also two commercial offthe-shelf systems. 3.2 In order conduct a manual evaluation of the six MT systems, we formulated it as a ranking problem. We adapt the framework used in the WMT 2011 shared task for evaluating MT metrics on European language pairs (Callison-Burch et al., 2011) for Arabic MT. We gather human ranking judgments by asking ten annotators (each native speaker of Arabic with English as a second language) to assess the quality of the English-Arabic systems, by ranking sentences relative to each other, from the best to the worst (ties are allowed). We use the Appraise toolkit (Federmann, 2012) designed for manual MT evaluation. The tool displays to the annotator, the source sentence and translations produced by various MT systems. The annotators received initial training on the tool and the task with ten sentences. They were presented with a brief guideline"
D14-1026,W11-2105,0,0.151955,"nslations. We only use the first reference in this study. (3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluation to topics beyond the commonly-used news topics. This sub-corpus consists of our in-house Arabic translations of seven English Wikipedia articles. The articles are: Earl Francis Lloyd, Western Europe, Citizenship, Marcus Garvey, Middle Age translation, Acadian, NBA. The English articles which do not exist in the Arabic Wikipedia were manually translated by a bilingual linguist. Table 1 gives an overview of these sub-corpora characteristics. tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation"
D14-1026,W05-0904,0,0.0825879,"e 300 remaining sentences (100 from each corpus) are kept for testing. The development and test sets are composed of equal portions of sentences from the three sub-corpora (NIST, MEDAR, WIKI). As baselines, we measured the correlation of BLEU and METEOR with human judgments collected for each sentence. We did not observe a strong correlation with the Arabic-tuned METEOR. We conducted our experiments on the standard METEOR which was a stronger baseline than its Arabic version. In order to avoid the zero ngram counts and artificially low BLEU scores, we use a smoothed version of BLEU. We follow Liu and Gildea (2005) to add a small value to both the matched n-grams and the total number of n-grams (epsilon value of 10−3 ). In order to reach an optimal ordering of partial matches, we conducted a set of experiments in which we compared different orders between the morphological and lexical matchings to settle with the final order which was presented in Figure 1. Table 4 shows a comparison of the average correlation with human judgments for BLEU, ME6 Conclusion We presented AL-BLEU, our adaptation of BLEU for the evaluation of machine translation into Arabic. The metric uses morphological, syntactic and lexic"
D14-1026,W10-1754,0,0.0836725,"beyond the commonly-used news topics. This sub-corpus consists of our in-house Arabic translations of seven English Wikipedia articles. The articles are: Earl Francis Lloyd, Western Europe, Citizenship, Marcus Garvey, Middle Age translation, Acadian, NBA. The English articles which do not exist in the Arabic Wikipedia were manually translated by a bilingual linguist. Table 1 gives an overview of these sub-corpora characteristics. tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to"
D14-1026,W11-2106,0,0.128828,"Arabic Wikipedia were manually translated by a bilingual linguist. Table 1 gives an overview of these sub-corpora characteristics. tion, Chen and Kuhn (2011) proposed AMBER, a modified version of BLEU incorporating recall, extra penalties, and light linguistic knowledge about English morphology. Liu et al. (2010) propose TESLA-M, a variant of a metric based on n-gram matching that utilizes light-weight linguistic analysis including lemmatization, POS tagging, and WordNet synonym relations. This metric was then extended to TESLA-B to model phrase synonyms by exploiting bilingual phrase tables (Dahlmeier et al., 2011). Tantug et al. (2008) presented BLEU+, a tool that implements various extension to BLEU computation to allow for a better evaluation of the translation performance for Turkish. To the best of our knowledge the only human judgment dataset for Arabic MT is the small corpus which was used to tune parameters of the METEOR metric for Arabic (Denkowski and Lavie, 2011). Due to the shortage of Arabic human judgment dataset, studies on the performance of evaluation metrics have been constrained and limited. A relevant effort in this area is the upper-bound estimation of BLEU and METEOR scores for Ara"
D14-1026,W13-2202,0,0.013344,"linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direcEvaluation of Machine Translation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´acˇ ek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too"
D14-1026,W11-2107,0,0.256144,"he research community. 1 2 Introduction Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direcEvaluation of Machine Translation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European targ"
D14-1026,maegaard-etal-2010-cooperation,0,0.0712016,"Missing"
D14-1026,I05-2014,0,0.0558301,"luate BLEU, METEOR and AL-BLEU on our human judgments corpus and show that AL-BLEU has the highest correlation with human judgments. We are releasing the dataset and software to the research community. 1 2 Introduction Related Work Several studies on MT evaluation have pointed out the inadequacy of the standard n-gram based evaluation metrics for various languages (CallisonBurch et al., 2006). For morphologically complex languages and those without word delimiters, several studies have attempted to improve upon them and suggest more reliable metrics that correlate better with human judgments (Denoual and Lepage, 2005; Homola et al., 2009). A common approach to the problem of morphologically complex words is to integrate some linguistic knowledge in the metric. METEOR (Denkowski and Lavie, 2011), TERPlus (Snover et al., 2010) incorporate limited linguistic resources. Popovi´c and Ney (2009) showed that n-gram based evaluation metrics calculated on POS sequences correlate well with human judgments, and recently designed and evaluated MPF, a BLEU-style metric based on morphemes and POS tags (Popovi´c, 2011). In the same direcEvaluation of Machine Translation (MT) continues to be a challenging research proble"
D14-1026,P02-1040,0,0.0891493,"ation (MT) continues to be a challenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´acˇ ek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate. In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language 1 The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ("
D14-1026,2011.mtsummit-papers.24,0,0.0251778,"Missing"
D14-1026,W09-0402,0,0.0482224,"Missing"
D14-1026,W11-2110,0,0.0353723,"Missing"
D14-1026,2006.amta-papers.25,0,0.0922648,"allenging research problem. There is an ongoing effort in finding simple and scalable metrics with rich linguistic analysis. A wide range of metrics have been proposed and evaluated mostly for European target languages (CallisonBurch et al., 2011; Mach´acˇ ek and Bojar, 2013). These metrics are usually evaluated based on their correlation with human judgments on a set of MT output. While there has been growing interest in building systems for translating into Arabic, the evaluation of Arabic MT is still an under-studied problem. Standard MT metrics such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) have been widely used for evaluating Arabic MT (El Kholy and Habash, 2012). These metrics use strict word and phrase matching between the MT output and reference translations. For morphologically rich target languages such as Arabic, such criteria are too simplistic and inadequate. In this paper, we present: (a) the first human judgment dataset for Arabic MT (b) the Arabic Language 1 The dataset and the software are available at: http://nlp.qatar.cmu.edu/resources/ AL-BLEU 207 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 207–213, c Octo"
D14-1026,W09-0403,0,0.0456348,"Missing"
D14-1026,P04-1078,0,0.266026,"Missing"
D14-1026,tantug-etal-2008-bleu,1,\N,Missing
E06-1012,A00-2018,0,0.0205897,"ited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels a"
E06-1012,P99-1065,0,0.114991,"thers. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meaning of the sentence. Collins et al. (1999) applied the parser of Collins (1997) developed for English, to Czech, and found that the performance was substantially lower when compared to the results for English. 2 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer, 1994). At the syntax level, the unmarked constituent order is SOV, but constituent order may vary freely as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text however,"
E06-1012,P96-1025,0,0.85012,"l exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotat"
E06-1012,P97-1003,0,0.133319,"ucture of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meaning of the sentence. Collins et al. (1999) applied the parser of Collins (1997) developed for English, to Czech, and found that the performance was substantially lower when compared to the results for English. 2 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer, 1994). At the syntax level, the unmarked constituent order is SOV, but constituent order may vary freely as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text however, the unmarked order is dominant at bo"
E06-1012,P03-1054,0,0.0585069,"e described below, the relevant statistical parameters needed have been estimated from the Turkish treebank (Oflazer et al., 2003). Since this treebank is relatively smaller than the available treebanks for other languages (e.g., Penn Treebank), we have 91 opted to model the bigram linkage probabilities in an unlexicalized manner (that is, by just taking certain morphosyntactic properties into account), to avoid, to the extent possible, the data sparseness problem which is especially acute for Turkish. We have also been encouraged by the success of the unlexicalized parsers reported recently (Klein and Manning, 2003; Chung and Rim, 2004). For parsing, we use a version of the Backward Beam Search Algorithm (Sekine et al., 2000) developed for Japanese dependency analysis adapted to our representations of the morphological structure of the words. This algorithm parses a sentence by starting from the end and analyzing it towards the beginning. By making the projectivity assumption that the relations do not cross, this algorithm considerably facilitates the analysis. 4 Details of the Parsing Models In this section we detail three models that we have experimented with for Turkish. All three models are unlexica"
E06-1012,W00-1303,0,0.0343035,"ults indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for fr"
E06-1012,W02-2016,0,0.378762,"Missing"
E06-1012,P05-1013,0,0.0463877,"onal groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentence without modifying the general meanin"
E06-1012,W04-2407,0,0.0611715,"is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages where constituents can easily change their position in the sentenc"
E06-1012,W03-3017,0,0.0390029,"Missing"
E06-1012,J03-4001,1,0.824103,"y, the fifth IG indicates a derivation into a relativizer adjective. A sentence would then be represented as a sequence of the IGs making up the words. When a word is considered as a sequence of IGs, linguistically, the last IG of a word determines its role as a dependent, so, syntactic relation links only emanate from the last IG of a (dependent) word, and land on one of the IGs of a (head) word on the right (with minor exceptions), as exemplified in Figure 2. And again with minor exceptions, the dependency links between the IGs, when drawn above the IG sequence, do not cross.3 Figure 3 from Oflazer (2003) shows a dependency tree for a Turkish sentence laid on top of the words segmented along IG boundaries. With this view in mind, the dependency relations that are to be extracted by a parser should be relations between certain inflectional groups and 1 Literally, “(the thing existing) at the time we caused (something) to become strong”. 2 The morphological features other than the obvious partof-speech features are: +Become: become verb, +Caus: causative verb, +PastPart: Derived past participle, +P3sg: 3sg possessive agreement, +A3sg: 3sg numberperson agreement, +Loc: Locative case, +Pos: Positi"
E06-1012,C00-2109,0,0.758787,"acy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have been found to be less effective for free-constituentorder languages"
E06-1012,W03-3023,0,0.0853659,"ht non-crossing dependency links. Our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing, and when contexts around the dependent are employed. 1 Introduction The availability of treebanks of various sorts have fostered the development of statistical parsers trained with the structural data in these treebanks. With the emergence of the important role of word-to-word relations in parsing (Charniak, 2000; Collins, 1996), dependency grammars have gained a certain popularity; e.g., Yamada and Matsumoto (2003) for English, Kudo and Matsumoto (2000; 2002), Sekine et al. (2000) for Japanese, Chung and Rim (2004) for Korean, Nivre et al. (2004) for Swedish, Nivre and Nilsson (2005) for Czech, among others. Dependency grammars represent the structure of the sentences by positing binary dependency relations between words. For instance, Figure 1 Figure 1: Dependency Relations for a Turkish and an English sentence shows the dependency graph of a Turkish and an English sentence where dependency labels are shown annotating the arcs which extend from dependents to heads. Parsers employing CFG-backbones have"
E12-1017,W10-2417,0,0.263206,"development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 feaTraining words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new text domain like Wikipedia.10 A descriptive list of our features is available in the supplementary document. We use a first-order structured perceptron; none of our features consider more than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram hO, Ii is disallowed). Training this model on ACE and ANER data achie"
E12-1017,attia-etal-2010-automatically,0,0.305231,"Missing"
E12-1017,W03-2201,0,0.00816761,"but not entirely, consistent with each other in their creation of custom categories. Further, almost all of our article-specific categories correspond to classes in the extended NE taxonomy of (Sekine et al., 2002), which speaks to the reasonableness of both sets of categories—and by extension, our open-ended annotation process. Our annotation of named entities outside of the traditional POL classes creates a useful resource for entity detection and recognition in new domains. Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (Toral et al., 2005; Babych and Hartley, 2003). Possible avenues for future work include annotating and projecting non-canonical 5 When it came to tagging NEs, one of the two annotators was assigned to each article. Custom categories only suggested by the other annotator were ignored. 164 NEs from English articles to their Arabic counterparts (Hassan et al., 2007), automatically clustering non-canonical types of entities into articlespecific or cross-article classes (cf. Frietag, 2004), or using non-canonical classes to improve the (author-specified) article categories in Wikipedia. Hereafter, we merge all article-specific categories with"
E12-1017,W09-3302,0,0.174637,"hard Stallman Amman were reserved Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System for training annotators, Claudio Filippone (PER) àñJ.ÊJ ¯ ñK XñÊ¿; Linux (SOFTWARE) ºJJ Ë; Spanish Gulf War for esti KðQK.; nuclear and League (CHAMPIONSHIPS) úGAJ.B@ ø PðYË@; proton (PARTICLE) àñ mating inter-annotator   agreement.  ; Real Zaragoza (ORG) é¢¯Qå ÈAK P radiation (GENERIC - MISC) ø ðñJË@ ¨AªB@ History Science Sports Technology appropriate entity classes will vary widely by domain; occurrence rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Se"
E12-1017,D08-1030,0,0.302426,"Missing"
E12-1017,W03-1022,0,0.0325535,"elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to pro"
E12-1017,W03-0407,0,0.0451401,"recall errors (yi 6= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) simila"
E12-1017,W02-1001,0,0.00590103,"test data. A larger set of Arabic Wikipedia articles, selected on the basis of quality heuristics, serves as unlabeled data for semisupervised learning. Our out-of-domain labeled NE data is drawn from the ANER (Benajiba et al., 2007) and ACE-2005 (Walker et al., 2006) newswire corpora. Entity types in this data are POL categories (PER, ORG, LOC) and MIS. Portions of the ACE corpus were held out as development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 feaTraining words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal"
E12-1017,D07-1074,0,0.00392108,"ance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain da"
E12-1017,P07-1033,0,0.00539308,"Missing"
E12-1017,farber-etal-2008-improving,0,0.399022,"the choice of this value (figure 1 shows how we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial dive"
E12-1017,D10-1033,0,0.0133877,"mains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them"
E12-1017,W04-3234,0,0.0317638,"Missing"
E12-1017,N10-1112,1,0.0770113,"2) Input: labeled data hhx(n) , y (n) iiN n=1 ; unlabeled ¯ (j) iJj=1 ; supervised learner L; data hx number of iterations T 0 Output: w w ← L(hhx(n) , y (n) iiN n=1 ) for t = 1 to T 0 do for j = 1 to J do ¯ (j) , y) yˆ(j) ← arg maxy w&gt; g(x ¯ (j) , yˆ(j) iiJj=1 ) w ← L(hhx(n) , y (n) iiN n=1 ∪ hhx y0 which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between different types of errors made during training. For a tag sequence y = hy1 , y2 , . . . , yM i, Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O ∧ yˆi 6= O for the ith word), recall errors (yi 6= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-t"
E12-1017,W11-0411,0,0.0136423,"ews text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (bu"
E12-1017,P05-1071,0,0.00777494,"ature weights (model parameters) w, the structured hinge loss is `hinge (x, y, w) =   &gt; 0 0 max w g(x, y ) + c(y, y ) − w&gt; g(x, y) 0 y 6 Additional details appear in the supplement. We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will b"
E12-1017,N06-2015,0,0.019601,"a well-established discriminative NER model and feature set. Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text. 2 Arabic Wikipedia NE Annotation Most of the effort in NER has been focused around a small set of domains and general-purpose entity classes relevant to those domains—especially the categories PER ( SON ), ORG ( ANIZATION ), and LOC ( ATION ) (POL), which are highly prominent in news text. Arabic is no exception: the publicly available NER corpora—ACE (Walker et al., 2006), ANER (Benajiba et al., 2008), and OntoNotes (Hovy et al., 2006)—all are in the news domain.2 However, 2 OntoNotes contains news-related text. ACE includes some text from blogs. In addition to the POL classes, both corpora include additional NE classes such as facility, event, product, vehicle, etc. These entities are infrequent and may not be comprehensive enough to cover the larger set of pos162 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics Table 1: Translated titles of Arabic Wikipedia arti´"
E12-1017,P11-1147,0,0.0170901,"ng this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (includin"
E12-1017,N06-1010,0,0.0244745,"t entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who r"
E12-1017,D07-1073,0,0.00752362,"t only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedi"
E12-1017,P11-2016,0,0.0185145,"English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a small corpus of Arabic Wikipedia articles v"
E12-1017,W04-3250,0,0.00512011,"sed model as the initial labeler for recall-oriented self-training. • ROP/ROP (the “double ROP” condition): recalloriented supervised model as the initial labeler for recall-oriented self-training. Note that the two ROPs can use different cost parameters. For evaluating our models we consider the named entity detection task, i.e., recognizing which spans of words constitute entities. This is measured by per-entity precision, recall, and F1 .13 To measure statistical significance of differences between models we use Gimpel and Smith’s (2010) implementation of the paired bootstrap resampler of (Koehn, 2004), taking 10,000 samples for each comparison. 5.1 Baseline Our baseline is the perceptron, trained on the POL entity boundaries in the ACE+ANER corpus (reg/none).14 Development data was used to select the number of iterations (10). We performed 3-fold cross-validation on the ACE data and found wide variance in the in-domain entity detection performance of this model: fold 1 fold 2 fold 3 average P 70.43 87.48 65.09 74.33 R 63.08 81.13 51.13 65.11 F1 66.55 84.18 57.27 69.33 (Fold 1 corresponds to the ACE test set described in table 4.) We also trained the model to perform POL detection and class"
E12-1017,N06-1020,0,0.0445779,"and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tr"
E12-1017,W04-2405,0,0.0438164,"= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the"
E12-1017,N06-2024,0,0.386018,"earning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tradeoff in NER. Their technique was to directly tune the weight of a single feature—the feature marking O (nonentity tokens); a lower weight for this feature will incur a greater penalty for predicting O. Below we demonstrate that our method, which is less coarse, is more successful in our setting.12 In our experiments we will show that injecting “arrogance” into the learner via the recall-oriented loss function substantially improves recall, especially for non-POL entities (§5.3). 4.2 Self-Training and Semisupervised Learning As we will show exper"
E12-1017,nezda-etal-2006-world,0,0.220858,"e—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (including temporal and numeric entities); this corpus has not been released publicly. testing examples, but not as training data. In §4 we will discuss our semisupervised approach to learning, which leverages ACE and ANER data as an annotated training corpus. 2.1 Annotation Strategy We conducted a small annotation project on Arabic Wikipedia articles. Two college-educated native Arabic speakers annotated about 3,000 sentences from 31 articles. We identified four topical areas of interest—history, technology, scien"
E12-1017,E09-1070,0,0.00793652,"glish and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The d"
E12-1017,D10-1069,0,0.00615818,"stinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled"
E12-1017,W09-1119,0,0.00861532,"9/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step:   yˆ ← arg max w&gt; g(x, y 0 ) + c(y, y 0 ) , (2) Input: labeled data hhx(n) , y (n) iiN n=1 ; unlabeled ¯ (j) iJj=1 ; supervised learner L; data hx number of it"
E12-1017,P08-2030,0,0.0266617,"meters) w, the structured hinge loss is `hinge (x, y, w) =   &gt; 0 0 max w g(x, y ) + c(y, y ) − w&gt; g(x, y) 0 y 6 Additional details appear in the supplement. We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorpo"
E12-1017,sekine-etal-2002-extended,0,0.045577,"nce rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it"
E12-1017,W04-1221,0,0.0307168,"9). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also"
E12-1017,W11-0902,0,0.0289153,"Missing"
E12-1017,D09-1158,0,0.0127085,"ic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—"
E12-1017,W03-1708,0,0.0109477,"ndantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annota"
E12-1017,N04-1001,0,\N,Missing
E93-1066,C92-1010,1,\N,Missing
E93-1066,P84-1038,0,\N,Missing
E93-1066,W83-0114,0,\N,Missing
hakkani-etal-1998-english,E93-1022,0,\N,Missing
hakkani-etal-1998-english,A97-1047,0,\N,Missing
hakkani-etal-1998-english,C92-3168,1,\N,Missing
hakkani-etal-1998-english,E93-1066,1,\N,Missing
hakkani-etal-1998-english,C96-1094,0,\N,Missing
hakkani-etal-1998-english,C92-4202,1,\N,Missing
hakkani-etal-1998-english,C94-1013,1,\N,Missing
hakkani-etal-1998-english,C94-1008,0,\N,Missing
I13-1031,radev-etal-2004-mead,0,0.0423336,"indicator of its grammaticality. In our experiments, we used the SRILM toolkit (Stolcke, 2002) to build 5-gram language model using the LDC Arabic Gigaword corpus. We then, apply this model to obtain log-likelihood and perplexity scores for each sentence. 5.1 MT-based scores: We extract a set of features from the generated MT output. These include the absolute number and the ratios of out of vocabulary terms and the ratio of Arabic detokenization that is performed on the Arabic MT output. The MEAD Summarization system In our experiments, the summary for each document is generated using MEAD (Radev et al., 2004), a state-of-the-art single- and multidocument summarization system. MEAD has been widely used both as a platform for developing summarization systems and as a baseline system for testing novel summarizers. It is a Morphosyntactic features: We use features to model the difference of sequences of POS tags 273 6.2 centroid-based extractive summarizer which selects the most important sentences from a sequence of sentences based on a linear combination of three parameters: the sentence length, the centroid score and the position score (Radev et al., 2001). MEAD also employs a cosine reranker to el"
I13-1031,2006.amta-papers.25,0,0.0239053,"uality estimation system (b) predicts if a translated sentence has high or low translation quality and assigns a quality score to each sentence. 3. We summarize the English document using our MT-aware summarization system (c), which incorporates the translation quality score (output of (b)) in its sentence selection process. 4.1 In order to train the binary classifier, we need gold standard data with English source sentences labeled as having high or low translation quality when translated into Arabic. For this labeling, we estimate translation quality by the Translation Edit Rate TER metric (Snover et al., 2006).2 We deliberately use two different metrics for gold standard labeling (TER) and the final MT evaluation (using BLEU (Papineni et al., 2002)) to reduce the bias that a metric can introduce to the framework. In this task, we use a parallel corpus that is composed of a set of documents. We automatically translate each document and label its sentences based on the following procedure: 4. We produce the final Arabic translation summary by matching the English summarized sentences with the corresponding Arabic translations (d). 5. We automatically evaluate the quality of our MT-aware summarization"
I13-1031,P10-1063,0,0.0173967,"source language text for an MT system can degrade the translation quality dramatically. Thus, extractive summarization like our framework is more suitable for MT summarization. In retrospect our annotated Arabic-English summaries is a unique bilingual resource as most other Arabic-English summarization corpora (e.g. DUC) are abstractive summaries. There has been a body of recent work on the reference-free prediction of translation quality both as confidence estimation metrics and also direct prediction of human judgment scores (Bojar et al., 2013; Specia, 2012) or the range of the BLEU score (Soricut and Echihabi, 2010; Mohit and Hwa, 2007). These works mostly use supervised learning frameworks with a rich set of source and target language features. Our binary classification of MT quality is closer to the classification system of Mohit and Hwa (2007) to estimate translation difficulty of phrases. However, there are several modifications such as the method of labeling, the focus on sentence level prediction and finally the use of a different metric for both the labeling and final evaluation (which reduces the metric bias). For learning features, we cumulatively explore and optimize most of the reported featu"
I13-1031,W06-3114,0,0.0132807,"h a quality superior to translating the entire document. Figure 1 illustrates an overview of our framework composed of the following major components: (a) a standard 271 (a) MT System Arabic Sentences English Sentences English Document (b) Quality Estimation SentEN1, SentAR1 : Score1 SentEN2, SentAR2 : Score2 SentEN3, SentAR3 : Score3 . . . . SentENn, SentARn : Scoren English Summary (c) MT-aware Summarizer Arabic Summary (d) Sentence Matcher English Sentences Figure 1: An overview of our MT-aware summarization system been developed in evaluation programs like NIST, and workshops such as WMT (Koehn and Monz, 2006). Since such human judgments do not exist for English to Arabic translations, we adapt the framework of Mohit and Hwa (2007) for predicting the translation quality. This framework uses only reference translations and the automatic MT evaluation scores to create labeled data for training a classifier. The binary classifier reads in a source language sentence, with its automatically obtained translation and predicts if the target sentence has high or low translation quality. We describe details of this framework in the following section. SMT system; (b) our reference-free MT quality estimation s"
I13-1031,P13-4014,0,0.0321452,"Missing"
I13-1031,W07-0734,0,0.0292012,"experimented with different configurations of the MT and the summarization system with the goal of achieving a balanced performance in both dimensions. We reached the sweet spot of performance in both dimensions in our MT-aware summarization system in which we achieved major (over 4 points BLEU score) improvements while maintaining an acceptable summarization quality. In the following we discuss the performance of the MT and summarization systems. 7.1 BLEU MT evaluation Table 1 presents MT quality for the baseline system and different summarization frameworks measured by BLEU, TER and METEOR (Lavie and Agarwal, 2007) scores.5 The remaining MT experiments are conducted on summarized documents. These include summaries provided by: (a) a length-based baseline system that simply chooses the subset of sentences with the shortest length (Length); (b) the state of the art MEAD summarizer (MEAD); (c) our MT quality estimation classifier (Classifier); (d) a linear interpolation of informativeness and MT quality scores in the spirit of Wan et al. (2010) (Interpol)6 ; (e) our MT-aware summarizer 5 Our English to Arabic baseline system shows a performance in the ballpark of the reported score for the state of the art"
I13-1031,N03-1020,0,0.122062,"the perfor7.3 Model-free summarization evaluation In addition to the reference-based summarization evaluation described above, we conducted modelfree experiments evaluating the summary quality for both languages. Recently, Louis and Nenkova (2013) proposed SIMetrix, a framework that does not require gold standard summaries for measuring the summarization quality. The framework is based on the idea that higher similarity with the source document would be indicative of high quality summary. SIMetrix is a suite of model-free similarity metrics for comparing a generated sum7 A study conducted by Lin and Hovy (2003) shows that automatic evaluation using unigram and bigram cooccurrences between summary pairs have the highest correlation with human evaluations and have high recall and precision in significance test with manual evaluation results. 8 http://duc.nist.gov/duc2007/tasks. html. 276 8 mary with the source document for which it was produced. That includes cosine similarity, distributional similarity and also use of topic signature words. SIMetrix is shown to produce summary scores that correlate accurately with human assessments.9 We used SIMetrix to evaluate the quality of the summaries generated"
I13-1031,P10-1094,0,0.273718,"the underlying SMT system and also its training data. There are many overlaps between the features used in confidence estimation and the MT quality prediction. However, the two frameworks use different learning methods. Confidence estimation systems usually do not have gold standard data and are mostly a linear interpolation of a large group of scores. In contrast, MT quality predictors such as our framework usually use supervised learning and rely on gold standard data. Text summarization has been successfully paired with different NLP applications such as MT in cross-language summarization. Wan et al. (2010) and Boudin et al. (2011) proposed crosslanguage summarization frameworks in which for each sentence, in a source language text, an MT quality and informativeness scores are combined to produce summary in a target language (Chinese and French, respectively). In the latter, sentences are first translated, ranked and then summaries are generated. Differently, in Wan et al. (2010), each sentence of the source document is ranked based on an a posteriori combination of both scores. The selected summarized sentences are then translated to the target language using Google Translate. In contrast, we g"
I13-1031,W04-1013,0,0.0355672,"Missing"
I13-1031,W06-3110,0,0.0219471,"rized to make a more cohesive document. Thus, for tasks in which complete translation is not mandatory, MT can be effective if the system can provide a 1 The bilingually summarized corpora could be found at: http://nlp.qatar.cmu.edu/resources/SuMT 270 International Joint Conference on Natural Language Processing, pages 270–278, Nagoya, Japan, 14-18 October 2013. ments of its own performance. The confidence measure is a score for N-grams (substrings of the hypothesis) which are generated by an MT system. Confidence estimation is performed at the word level (Blatz et al., 2003) or phrase level (Zens and Ney, 2006). The measure is based on feature values extracted from the underlying SMT system and also its training data. There are many overlaps between the features used in confidence estimation and the MT quality prediction. However, the two frameworks use different learning methods. Confidence estimation systems usually do not have gold standard data and are mostly a linear interpolation of a large group of scores. In contrast, MT quality predictors such as our framework usually use supervised learning and rely on gold standard data. Text summarization has been successfully paired with different NLP a"
I13-1031,J13-2002,0,0.0156541,"nt with five summarizers: Length, MEAD, Classifier, Interpol, SuMT. As expected, the MEAD summarizer shows the best summarization performance. Also, the length-based baseline system generates poor quality summaries (about 22 score ROUGE-1 reduction from MEAD). This is not surprising since the baseline only uses the length of the sentence regardless its content. Furthermore, the perfor7.3 Model-free summarization evaluation In addition to the reference-based summarization evaluation described above, we conducted modelfree experiments evaluating the summary quality for both languages. Recently, Louis and Nenkova (2013) proposed SIMetrix, a framework that does not require gold standard summaries for measuring the summarization quality. The framework is based on the idea that higher similarity with the source document would be indicative of high quality summary. SIMetrix is a suite of model-free similarity metrics for comparing a generated sum7 A study conducted by Lin and Hovy (2003) shows that automatic evaluation using unigram and bigram cooccurrences between summary pairs have the highest correlation with human evaluations and have high recall and precision in significance test with manual evaluation resu"
I13-1031,W07-0737,1,0.92321,"MT system can degrade the translation quality dramatically. Thus, extractive summarization like our framework is more suitable for MT summarization. In retrospect our annotated Arabic-English summaries is a unique bilingual resource as most other Arabic-English summarization corpora (e.g. DUC) are abstractive summaries. There has been a body of recent work on the reference-free prediction of translation quality both as confidence estimation metrics and also direct prediction of human judgment scores (Bojar et al., 2013; Specia, 2012) or the range of the BLEU score (Soricut and Echihabi, 2010; Mohit and Hwa, 2007). These works mostly use supervised learning frameworks with a rich set of source and target language features. Our binary classification of MT quality is closer to the classification system of Mohit and Hwa (2007) to estimate translation difficulty of phrases. However, there are several modifications such as the method of labeling, the focus on sentence level prediction and finally the use of a different metric for both the labeling and final evaluation (which reduces the metric bias). For learning features, we cumulatively explore and optimize most of the reported features, and add document-"
I13-1031,P03-1021,0,0.0173447,"Missing"
I13-1031,P02-1040,0,0.0867087,"ence. 3. We summarize the English document using our MT-aware summarization system (c), which incorporates the translation quality score (output of (b)) in its sentence selection process. 4.1 In order to train the binary classifier, we need gold standard data with English source sentences labeled as having high or low translation quality when translated into Arabic. For this labeling, we estimate translation quality by the Translation Edit Rate TER metric (Snover et al., 2006).2 We deliberately use two different metrics for gold standard labeling (TER) and the final MT evaluation (using BLEU (Papineni et al., 2002)) to reduce the bias that a metric can introduce to the framework. In this task, we use a parallel corpus that is composed of a set of documents. We automatically translate each document and label its sentences based on the following procedure: 4. We produce the final Arabic translation summary by matching the English summarized sentences with the corresponding Arabic translations (d). 5. We automatically evaluate the quality of our MT-aware summarization system using MT and summarization metrics. Our contributions are mainly related to the second and third components which will be discussed i"
I13-1031,C04-1046,0,\N,Missing
I13-1031,W13-2201,0,\N,Missing
I13-2001,font-llitjos-carbonell-2004-translation,0,0.361837,"Missing"
I13-2001,P11-4010,0,0.0809888,", we show how our framework employs automatic annotators to correct basic Arabic spelling mistakes to speed up the annotation process. Our framework consists of two major interfaces: (a) an Admin interface, which enables the lead annotator to create, assign, monitor, evaluate and export annotation tasks in large scale; and (b) 1 2 Related Work Traditionally, manual text correction is performed under the context of post-editing machine translation (MT) output. The goal of post-editing is to evaluate MT systems rather than building corpora of edits. Tools like PET (Aziz et al., 2012) and BLAST (Stymne, 2011) provide annotators a text-editorlike interface to identify, record, and correct errors. Text-editor-like interfaces are very flexible and allow all forms of corrections to be performed, but, they are not capable of accurately tracking token alignment, if at all. Frameworks such as EXMARaLDA (Schmidt, 2010) and GATE (Cunningham et al., 2011) facilitate multi-layer and multi-round annotations. An example of such approach is the work of Dickinson and Ledbetter (2012) who annotated errors in Hungarian students essays using multiple annotation layers from phonology to syntax in different stages. T"
I13-2001,dickinson-ledbetter-2012-annotating,0,\N,Missing
I13-2001,aziz-etal-2012-pet,0,\N,Missing
I13-2001,2012.eamt-1.31,0,\N,Missing
I13-2001,2012.tc-1.5,0,\N,Missing
I13-2002,W12-2038,0,0.126099,"eader does not have access to aids that would enable her to get over them including the problem of unknown words interpretation, unrecognized and forgotten names, difficult and hard-to-understand sentences, and lack of or forgetting the prior context in a former session of reading. There are many NLPbased tools, that offer various kinds of aids, to non-native English readers to help them in understanding a document. Many tools focus on assisting the reader in understanding of a specific word which may lead to better comprehension and vocabulary acquisition such as (Nerbonne et al., 1997) and (Eom et al., 2012). Some other tools focus on assisting the reader and second language learner with highlighting different patterns in the documents and providing the learner a visually enhanced version of the document (Meurers et al., ). SmartReader is an implementation of a NLPpowered tool to aid in reading texts in English by 2 System Overview Our system is based on client-server architecture as shown in Figure 1. The server is responsible for annotating plain text with NLP-related annotations and retrieving them based on the user’s interactions. The client is a standard web browser running on PCs or touch t"
I13-2002,P05-1045,0,0.0035597,"validate and normalize its orthography. Using Stanford CoreNLP tools, we segment the text into sentences, and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the wo"
I13-2002,W11-1902,0,0.0318399,"ext into sentences, and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the word overlapping with that position. These are which are then interpreted by the que"
I13-2002,J13-4004,0,0.0225131,", we segment the text into sentences, and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the word overlapping with that position. These are which are then inte"
I13-2002,A97-1020,0,0.780538,"blems, especially when the reader does not have access to aids that would enable her to get over them including the problem of unknown words interpretation, unrecognized and forgotten names, difficult and hard-to-understand sentences, and lack of or forgetting the prior context in a former session of reading. There are many NLPbased tools, that offer various kinds of aids, to non-native English readers to help them in understanding a document. Many tools focus on assisting the reader in understanding of a specific word which may lead to better comprehension and vocabulary acquisition such as (Nerbonne et al., 1997) and (Eom et al., 2012). Some other tools focus on assisting the reader and second language learner with highlighting different patterns in the documents and providing the learner a visually enhanced version of the document (Meurers et al., ). SmartReader is an implementation of a NLPpowered tool to aid in reading texts in English by 2 System Overview Our system is based on client-server architecture as shown in Figure 1. The server is responsible for annotating plain text with NLP-related annotations and retrieving them based on the user’s interactions. The client is a standard web browser ru"
I13-2002,D10-1048,0,0.0234154,", and then tokenize and perform POS tagging.1 We then use the following NLP components to annotate the text: • The Stanford Dependency Parser (De Marneffe et al., 2006), provides grammatical relation annotations for each word within the sentence. Once annotated, a document is loaded into the library. Figure 2, shows the annotation components that each document goes through. Figure 3 shows a logical view of a subset of the annotations for one sentence. • The Stanford Named Entity Recognizer (Finkel et al., 2005) and then the Stanford Co-reference Resolution (Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010) are used to determine the entities in the text and the relationships between them. Query Processing: All queries from the user client application are translated into a character offset in the text. Thus when this character offset is passed to UIMA, it returns efficiently all the 1 http://nlp.stanford.edu/software/ corenlp.shtml 6 Figure 2: Document Annotation (modules with dotted lines are under development) Figure 3: UIMA annotations for one sentence annotations associated with the word overlapping with that position. These are which are then interpreted by the query processing unit as descr"
I13-2002,de-marneffe-etal-2006-generating,0,\N,Missing
I13-2002,R13-1006,1,\N,Missing
I13-2002,W10-1002,0,\N,Missing
J00-1001,J94-3001,0,0.0574545,"Missing"
J00-1001,C92-1025,1,0.827362,"Missing"
J00-1001,J97-2003,0,0.110271,"Missing"
J01-1003,C96-1017,0,0.156148,"Missing"
J01-1003,J95-4004,0,0.822553,"eal&quot; suffixes will appear frequently in the corpus of inflected forms. Once common suffixes and prefixes are identified, the segmentation for an inflected word can be determined by choosing the segmentation with the most frequently occurring affix segments; the remainder is then considered the root. While this procedure seems to 1 For instance, the CoNLL(Computational Natural Language Learning) Workshops, recent special issues of Machine LearningJournal (Vol.34 Issue 1/3, Feb. 1999) and AIMagazine (Vol. 18, No. 4, 1997). 2 Not in the sense in which it is used in transformation-based learning (Brill 1995). 60 Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers be reasonable for a small root word list, the potential for &quot;noisy&quot; or incorrect alignments is quite high when the corpus of inflected forms is large and the procedure is not given any prior knowledge of possible segmentations. As a result, automatically selecting the &quot;correct&quot; segmentation becomes nontrivial. An additional complication is that allomorphs show up as distinct affixes and their counts in segmentations are not accumulated, which might lead to actual segmentations being missed due to fragmentation. The rule"
J01-1003,J94-3001,0,0.06617,"ads. The basic architecture of the morphological analyzer is depicted in Figure 2. The analyzer consists of the union of transducers, each of which implements the morphological analysis process for one paradigm. Each transducer is the composition of a n u m b e r of components. These components (from bottom to top) are described below: . The bottom component is an ordered sequence of morphographemic rules that are learned via transformation-based learning from the sample inflectional paradigms provided by the h u m a n informant. These rules are then composed into one finite-state transducer (Kaplan and Kay 1994). . The contains the citation forms and the affixes. We currently assume that all affixation is concatenative and that the lexicon is described by a regular expression of the sort [ P r e f i x e s ]* [ C i t a t i o n F o r m s ] [ S u f f i x e s ].7 citationformand affixlexicon 7 We currently assume that we have at most one prefix and at most one suffix,but this is not a fundamental limitation. The elicitation of morphotactics for an agglutinating language like Turkish or Finnish requires a significantlymore sophisticated elicitationmachinery. 63 Computational Linguistics Volume 27, Number"
J01-1003,C94-1066,0,0.0457807,"roved analyzer. 1. Introduction The Expedition project at NMSU Computing Research Laboratory is devoted to the fast &quot;ramp-up&quot; of machine translation systems from less studied, so-called low-density languages, into English. One of the components that must be acquired and built during this process is a morphological analyzer for the source language. Since language informants are not expected or required to be well-versed in computational linguistics in general, or in recent approaches to building morphological analyzers (e.g., Koskenniemi 1983; Antworth 1990; Karttunen, Kaplan, and Zaenen 1992; Karttunen 1994) and the operation of state-of-the-art finite-state tools (e.g., Karttunen 1993; Karttunen and Beesley 1992; Karttunen et al. 1996; Mohri, Pereira, and Riley 1998; van Noord 1999; van Noord and Gerdemann 1999) in particular, the generation of the morphological analyzer component has to be accomplished semiautomatically. The informant will be guided through a knowledge elicitation procedure using the elicitation component of Expedition, the Boas system. As this task is not easy, we expect that the development of the morphological analyzer will be an iterative process, whereby the human informan"
J01-1003,C92-1025,0,0.0432418,"Missing"
J01-1003,J00-1006,0,0.0327403,"Missing"
J01-1003,P98-2160,0,0.0115814,"uently are assumed to be roots, and observed surface forms are then either generated by the concatenative affixation of suffixes or by rewrite rules. 3 Since the system has no notion of what the roots and their part-of-speech values really are, and what morphological information is encoded by the affixes, this information needs to be retrofitted manually by a human, who has to weed through a large number of noisy rules. We feel that this approach, while quite novel, can be used to build real-world morphological analyzers only after substantial modifications are made. 3. The BOAS Project Boas (Nirenburg 1998; Nirenburg and Raskin 1998) is a semiautomatic knowledge elicitation system that guides a team of two people (a language informant and a programmer) through the process of developing the static knowledge sources required to produce a moderate-quality, broad-coverage MT system from any &quot;low-density&quot; language into English. Boas contains knowledge about human language phenomena and various realizations of these phenomena in a number of specific languages, as well as extensive pedagogical support, making the system a kind of &quot;linguist in a box,&quot; intended to help nonprofessional users with the tas"
J01-1003,J96-1003,1,0.787249,"ll the analyzers for each paradigm, one can do a more comprehensive test against a test corpus to see what surface forms in the test corpus are not recognized by the generated analyzer. Apart from revealing obvious deficiencies in coverage (e.g., missing citation forms in the lexicon), such testing provides feedback about minor human errors--the failure to cover certain morphographemic phenomena, or the incorrect assignment of citation forms to paradigms, for example. Our approach is as follows: we use the resulting morphological analyzer with an error-tolerant finite-state recognizer engine (Oflazer 1996). Using this engine, we try to find words recognized by the analyzer that are (very) close to a rejected (correct) word in the test corpus, essentially performing a reverse spelling correction. If the rejection is due to a small number of errors (1 or 2), the erroneous words recognized by the recognizer are aligned with the corresponding correct words from the test corpus. These aligned pairs can then be analyzed to see what the problems may be. 5.4 Applicability to Infixing, Circumfixing, and Agglutinating Languages The machine learning procedure for inducing rewrite rules is not language dep"
J01-1003,P97-1057,0,0.0129898,"this feedback is limited to identifying problems in handling morphographemic processes (such as for instance the change of word-final -y to -i when the suffix -est is added). The box in Figure 1 labeled Morphological Analyzer Generation is the main component, which takes in the elicited information and generates a series of regular expressions for describing the morphological lexicon and morphographemic rules. The morphographemic rules describing changes in spelling as a result of affixation operations are induced from the examples provided by using transformation-based learning (Brill 1995; Satta and Henderson 1997). The result is an ordered set of contextual replace or rewrite rules, much like those used in phonology. 4 We u s e the t e r m citation form to refer to the w o r d form that is u s e d to look u p a g i v e n inflected f o r m in a dictionary. It m a y be the root or s t e m form that affixation is applied to, or it m a y h a v e additional morphological m a r k e r s to indicate its citation form status. 5 We currently u s e XRCE finite-state tools as o u r target e n v i r o n m e n t (Karttunen et al. 1996). 6 The test c o r p u s is either elicited from the h u m a n i n f o r m a n t o"
J01-1003,A97-1016,0,0.123037,"xed number of contexts in which they appear; so if there is enough data, phonological rewrite rules can be generated to account for the data. Rules are ordered by some notion of &quot;surfaciness&quot;, and at each stage the most surfacy rule--the rule with the most transparent context-is selected. Golding and Thompson (1985) describe an approach for inducing rules of English word formation from a corpus of root forms and the corresponding inflected forms. The procedure described there generates a sequence of transformation rules, 2 each specifying how to perform a particular inflection. More recently, Theron and Cloete (1997) have presented a scheme for obtaining two-level morphology rules from a set of aligned segmented and surface pairs. They use the notion of string edit sequences, assuming that only insertions and deletions are applied to a root form to get the inflected form. They determine the root form associated with an inflected form (and consequently the suffixes and prefixes) by exhaustively matching the inflected form against all root words. The motivation is that &quot;real&quot; suffixes will appear frequently in the corpus of inflected forms. Once common suffixes and prefixes are identified, the segmentation"
J01-1003,W98-1308,0,\N,Missing
J01-1003,J01-2001,0,\N,Missing
J01-1003,C98-2155,0,\N,Missing
J01-1003,P84-1070,0,\N,Missing
J03-4001,A97-1012,0,0.0145514,"hological analysis, and parsing. Recent advances in the development of sophisticated tools for building finite-state systems (e.g., XRCE Finite State Tools [Karttunen et al. 1996], AT&T Tools [Mohri, Pereira, and Riley 1998], and Finite State Automata Utilities [van Noord 1997]) have fostered the development of quite complex finite-state systems for natural language processing. In the last several years, there have been a number of studies on developing finite-state parsing systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefenstette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et al. 1997). Another stream of work in using finite-state methods in parsing is based on approximating context-free grammars with finite-state grammars, which are then processed by efficient methods for such grammars (Black 1989; Pereira and Wright 1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been a number of approaches to natural language parsing using extended finite-state approaches in which a finite-state engine is applied multiple times to the input, or various derivatives thereof, until some termination condition is reached (Abney 1996; Roche 199"
J03-4001,W89-0229,0,0.11812,"ilities [van Noord 1997]) have fostered the development of quite complex finite-state systems for natural language processing. In the last several years, there have been a number of studies on developing finite-state parsing systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefenstette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et al. 1997). Another stream of work in using finite-state methods in parsing is based on approximating context-free grammars with finite-state grammars, which are then processed by efficient methods for such grammars (Black 1989; Pereira and Wright 1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been a number of approaches to natural language parsing using extended finite-state approaches in which a finite-state engine is applied multiple times to the input, or various derivatives thereof, until some termination condition is reached (Abney 1996; Roche 1997). This article presents an approach to dependency parsing using a finite-state approach. The approach is similar to those of Roche and Abney in that all three use an extended finite-state scheme to parse the input sentences. Our contri"
J03-4001,P96-1025,0,0.379839,"he correct parse was among the dependency trees with the smallest total link length. • Our approach can employ violable constraints for robust parsing so that when the parser fails to link all dependents to a head, one can use lenient filtering to allow parses with a small number of unlinked dependents to be output. • The rules for linking dependents to heads can specify constraints on the intervening material between them, so that, for instance, certain links may be prevented from crossing barriers such as punctuation or lexical items with certain parts of speech or morphological properties (Collins 1996; Giguet and Vergne 1997; Tapanainen and J¨arvinen 1997). We summarize in Figure 1 the basic idea of our approach. This figure presents in a rather high-level fashion, for a Turkish and an English sentence, the input and output representation for the approach to be presented. For the purposes of this summary, we assume that none of the words in the sentences have any morphological ambiguity and that their morphological properties are essentially obvious from the glosses. We represent the input to the parser as a string of symbols encoding the words with some additional delimiter markers. Panel"
J03-4001,P99-1065,0,0.164185,"Missing"
J03-4001,C96-1058,0,0.0294986,".g., a workshop dedicated to computational approaches to dependency grammars was held at COLING/ACL’98). J¨arvinen and Tapanainen (1998; Tapanainen and J¨arvinen 1997) have demonstrated an efficient wide-coverage dependency parser for English. The work of Sleator and Temperley (1991) on link grammar, essentially a lexicalized variant of dependency grammar, has also proved to be interesting in regard to a number of aspects. Dependency-based statistical language modeling and parsing have also become quite popular in statistical natural language processing (Lafferty, Sleator, and Temperley 1992; Eisner 1996; Chelba et al. 1997; Collins 1996; Collins et al. 1999). Robinson (1970) gives four axioms for well-formed dependency structures that have been assumed in almost all computational approaches. These state that, in a dependency structure of a sentence, 1. one and only one word is independent, that is, not linked to some other word; 2. all others depend directly on some word; 3. no word depends on more than one other; and 4. if a word A depends directly on word B, and some word C intervenes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. T"
J03-4001,2000.iwpt-1.33,0,0.459714,"are implemented. We briefly provide a scheme for a robust-parsing extension of our approach using the lenient composition operation. We then provide results from a prototype implementation of the parser and its application to dependency parsing of Turkish. We close with remarks and conclusions. 2. Overview of Related Work Although finite-state methods have been applied to parsing by many researchers, extended finite-state techniques were initially used only by Roche (1997), Abney (1996), and the FASTUS group (Hobbs et al. 1997). In the context of dependency parsing with finite-state machines, Elworthy (2000) has recently proposed a finite-state parser that produces a dependency output. Roche (1997) presents a top-down approach for parsing context-free grammars implemented with finite-state transducers. The transducers are based on a syntactic dictionary comprising patterns of lexical and nonlexical items. The input is initially bracketed with sentence markers at both ends and then fed into a transducer for bracketing according to bracketing rules for each of the patterns in the dictionary. The output of the transducer is fed back to the input, and the constituent structure is iteratively refined."
J03-4001,1997.iwpt-1.12,0,0.0145748,"se was among the dependency trees with the smallest total link length. • Our approach can employ violable constraints for robust parsing so that when the parser fails to link all dependents to a head, one can use lenient filtering to allow parses with a small number of unlinked dependents to be output. • The rules for linking dependents to heads can specify constraints on the intervening material between them, so that, for instance, certain links may be prevented from crossing barriers such as punctuation or lexical items with certain parts of speech or morphological properties (Collins 1996; Giguet and Vergne 1997; Tapanainen and J¨arvinen 1997). We summarize in Figure 1 the basic idea of our approach. This figure presents in a rather high-level fashion, for a Turkish and an English sentence, the input and output representation for the approach to be presented. For the purposes of this summary, we assume that none of the words in the sentences have any morphological ambiguity and that their morphological properties are essentially obvious from the glosses. We represent the input to the parser as a string of symbols encoding the words with some additional delimiter markers. Panel (a) of Figure 1 shows t"
J03-4001,P97-1058,0,0.0234995,"ed the development of quite complex finite-state systems for natural language processing. In the last several years, there have been a number of studies on developing finite-state parsing systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefenstette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et al. 1997). Another stream of work in using finite-state methods in parsing is based on approximating context-free grammars with finite-state grammars, which are then processed by efficient methods for such grammars (Black 1989; Pereira and Wright 1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been a number of approaches to natural language parsing using extended finite-state approaches in which a finite-state engine is applied multiple times to the input, or various derivatives thereof, until some termination condition is reached (Abney 1996; Roche 1997). This article presents an approach to dependency parsing using a finite-state approach. The approach is similar to those of Roche and Abney in that all three use an extended finite-state scheme to parse the input sentences. Our contributions can be summarized as follows: • Our a"
J03-4001,W98-0501,0,0.0989981,"Missing"
J03-4001,P98-1101,0,0.0684504,"f quite complex finite-state systems for natural language processing. In the last several years, there have been a number of studies on developing finite-state parsing systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefenstette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et al. 1997). Another stream of work in using finite-state methods in parsing is based on approximating context-free grammars with finite-state grammars, which are then processed by efficient methods for such grammars (Black 1989; Pereira and Wright 1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been a number of approaches to natural language parsing using extended finite-state approaches in which a finite-state engine is applied multiple times to the input, or various derivatives thereof, until some termination condition is reached (Abney 1996; Roche 1997). This article presents an approach to dependency parsing using a finite-state approach. The approach is similar to those of Roche and Abney in that all three use an extended finite-state scheme to parse the input sentences. Our contributions can be summarized as follows: • Our approach differ"
J03-4001,J94-3001,0,0.0155642,"an 1979]) are quite restricted and low level, developers of finite-state transducer manipulation systems have augmented the notational capabilities with operations at a much higher level of abstraction, much closer to the operations used by the computational linguistics application (see, e.g., Karttunen et al., [1996]; see also http://www.xrce.xerox.com/competencies/contentanalysis/fsCompiler/fssyntax.html, and also van Noord, [1997]). Finite-state transducers are closed under union, but in contrast to finite-state recognizers, they are not closed under difference and intersection operations (Kaplan and Kay 1994). On the other hand, finite-state transducers are closed under the operation of composition, which is very much an analog of function composition in algebra. Let T1 be a transducer that maps between regular languages U1 and L1 , and let T2 be a transducer that maps between regular languages U2 and L2 . The composition T of T1 and T2 , denoted by T1 ◦ T2 , is the transducer that maps between U = T1−1 (L1 ∩ U2 ) and L = T2 (L1 ∩ U2 ).5 That is, the resulting mapping is defined only for the respective images, in T1−1 and T2 , of the intersection L1 ∩ U2 . A pair of strings (x, y) ∈ T1 ◦ T2 if and"
J03-4001,E99-1035,0,0.0793315,"or the phrases recognized. The output of a stage goes to the next stage in the cascade, which further brackets the input using yet other rules. Each cascade typically corresponds to a level in a standard X-bar grammar. After a certain (fixed) number of cascades, the input is fully bracketed, with the structures being indicated by labels on the brackets. Iterations in Roche’s approach roughly correspond to cascades in Abney’s approach. The grammar, however, determines the number of levels or cascades in Abney’s approach: that is, structure is fixed. A work along the lines of Abney’s is that of Kokkinakis and Kokkinakis (1999) for parsing Swedish. Elworthy (2000) presents a finite-state parser that can produce a dependency structure from the input. The parser utilizes standard phrase structure rules that are annotated with “instructions” that associate the components of the phrases recognized with dependency grammar–motivated relations. A head is annotated with variables associating it with its dependents. These variables are filled in by the instructions associated with the rules. These variables are copied or percolated “up” the rules according to special instructions. The approach resembles a unification-based g"
J03-4001,C90-2040,0,0.134826,"roduction Finite-state machines have been used for many tasks in language processing, such as tokenization, morphological analysis, and parsing. Recent advances in the development of sophisticated tools for building finite-state systems (e.g., XRCE Finite State Tools [Karttunen et al. 1996], AT&T Tools [Mohri, Pereira, and Riley 1998], and Finite State Automata Utilities [van Noord 1997]) have fostered the development of quite complex finite-state systems for natural language processing. In the last several years, there have been a number of studies on developing finite-state parsing systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefenstette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et al. 1997). Another stream of work in using finite-state methods in parsing is based on approximating context-free grammars with finite-state grammars, which are then processed by efficient methods for such grammars (Black 1989; Pereira and Wright 1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been a number of approaches to natural language parsing using extended finite-state approaches in which a finite-state engine is applied mult"
J03-4001,C92-1027,0,0.0998541,"Missing"
J03-4001,C96-2123,0,0.0230573,"th@ A3pl Pass PastPart Pos P3sg Pnon Pos A3sg Past Loc@ Gen@ P3sg A3sg@ Acc@ Figure 11 Dependency tree for the second parse. 8. Discussion and Conclusions We have presented the architecture and implementation of a dependency parser using an extended finite state. Although the emphasis has been on the description of the approach, we have developed a dependency grammar for Turkish and have used it to experiment with a small sample of 200 Turkish sentences. We have also employed a scheme for ranking dependency parses using the total link length of the dependency trees, as originally suggested by Lin (1996), with quite promising results. It is possible to use algorithms for extracting k shortest paths to extract parses from the transducer, which compactly encodes all dependency parses, and further to rank a much smaller set of parses using lexical and statistical information whenever available. Another interesting point that we have noted, especially during the development of the grammar, is that the grammar rules do not have to pay any real attention to the sequence of the IGs that do not have anything to do with the current rule (with a very few exceptions in some special cases in which the ru"
J03-4001,W98-1302,0,0.0182914,"x finite-state systems for natural language processing. In the last several years, there have been a number of studies on developing finite-state parsing systems (Koskenniemi 1990; Koskenniemi, Tapanainen, and Voutilainen 1992; Grefenstette 1996; Chanod and Tapanainen 1996; Ait-Mokhtar and Chanod 1997; Hobbs et al. 1997). Another stream of work in using finite-state methods in parsing is based on approximating context-free grammars with finite-state grammars, which are then processed by efficient methods for such grammars (Black 1989; Pereira and Wright 1997; Grimley-Evans 1997; Johnson 1998; Nederhof 1998, 2000). There have also been a number of approaches to natural language parsing using extended finite-state approaches in which a finite-state engine is applied multiple times to the input, or various derivatives thereof, until some termination condition is reached (Abney 1996; Roche 1997). This article presents an approach to dependency parsing using a finite-state approach. The approach is similar to those of Roche and Abney in that all three use an extended finite-state scheme to parse the input sentences. Our contributions can be summarized as follows: • Our approach differs from Roche’s"
J03-4001,J00-1003,0,0.0373469,"Missing"
J03-4001,E93-1066,1,0.766188,"nes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. This last condition of projectivity (or various extensions of it; see, e.g., Lai and Huang [1994]) is usually assumed by most computational approaches to dependency grammars as a constraint for filtering configurations and has also been used as a simplifying condition in statistical approaches for inducing dependencies from corpora (e.g., Yuret ¨ 1998).1 4. Turkish Turkish is an agglutinative language in which a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer 1993). At the syntax level, the unmarked constituent order is Subject-Object-Verb, but constituent order may vary as demanded by the discourse context. Essentially all constituent orders are possible, especially at the main sentence level, with very minimal formal constraints. In written text, however, the unmarked order is dominant at both the main-sentence and embedded-clause level. Turkish morphophonology is characterized by a number of processes such as vowel harmony (vowels in suffixes, with very minor exceptions, agree with previous 1 See section 6 for how projectivity is checked and section"
J03-4001,A97-1011,0,0.147147,"Missing"
J03-4001,C98-1098,0,\N,Missing
J08-3003,P05-1038,0,0.0440134,"and Oﬂazer 2006). In this article, we units of syntactic structure (Eryigit corroborate this claim showing that it holds in both approaches we explore. We also study the impact of different morphological feature representations on parsing accuracy. The second set of issues concerns lexicalization, a topic that has been very prominent in the parsing literature lately. Whereas the best performing parsers for English all make use of lexical information, the real beneﬁts of lexicalization for English as well as other languages remains controversial (Dubey and Keller, 2003; Klein and Manning 2003; Arun and Keller 2005). The third set concerns the basic parsing methodology, including both parsing algorithms and learning algorithms. We ﬁrst introduce a statistical parser using a conditional probabilistic model which is very sensitive to the selected representational features and thus clearly exposes the ones ˘ Nivre, and Oﬂazer Eryigit, Dependency Parsing of Turkish with crucial importance for parsing Turkish. We then implement our models on a deterministic classiﬁer-based parser using discriminative learning, which is one of the best performing dependency parsers evaluated on a wide range of different langua"
J08-3003,W06-2922,0,0.0105271,"Missing"
J08-3003,W06-2923,0,0.0538957,"Missing"
J08-3003,W04-3224,0,0.00686561,"rtainly improves parsing accuracy for Turkish, only the lexicalization of conjunctions and nouns together has an impact on accuracy. Similarly to the experiments on inﬂectional features, we again see that the classiﬁer-based parser has no sparse data problem even if we use a totally lexicalized model. Although the effect of lexicalization has been discussed in several studies recently (Dubey and Keller 2003; Klein and Manning 2003; Arun and Keller 2005), it is often investigated as an all-or-nothing affair, except for a few studies that analyze the distributions of lexical items, for example, Bikel (2004) and Gildea (2001). The results for 25 IGs with a noun part-of-speech tag other than common nouns are marked with an additional minor part of speech that indicates whether the nominal is a proper noun or a derived form—one of future participle, past participle, inﬁnitive, or a form involving a zero-morpheme derivation. These latter four do not contain any root information. 377 Computational Linguistics Volume 34, Number 3 Figure 8 Unlabeled and labeled attachment scores for incrementally extended lexicalization for the classiﬁer-based parser. Turkish clearly show that the effect of lexicalizat"
J08-3003,W00-1201,0,0.0282273,"Linguistics Computational Linguistics Volume 34, Number 3 An important issue in this context is to what extent our models and algorithms are tailored to properties of speciﬁc languages or language groups. This issue is especially pertinent for data-driven approaches, where one of the claimed advantages is portability to new languages. The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser (Collins 1997, 1999), is applied to a new language, which often leads to a signiﬁcant decrease in the measured accuracy (Collins et al. 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and Manning 2003; Corazza et al. 2004). However, it is often quite difﬁcult to tease apart the inﬂuence of different features of the parsing methodology in the observed degradation of performance. A related issue concerns the suitability of different kinds of syntactic representation for different types of languages. Whereas most of the work on English has been based on constituency-based representations, partly inﬂuenced by the availability of data resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it has been argued that free consti"
J08-3003,H92-1026,0,0.0499667,"Missing"
J08-3003,J02-2002,0,0.0452358,"Missing"
J08-3003,W06-2920,0,0.34395,"been based on constituency-based representations, partly inﬂuenced by the availability of data resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it has been argued that free constituent order languages can be analyzed more adequately using dependency-based representations, which is also the kind of annotation found, for example, in the Prague Dependency Treebank of Czech (Hajiˇc et al. 2001). Recently, dependency-based parsing has been applied to 13 different languages in the shared task of the 2006 Conference on Computational Natural Language Learning (CoNLL) (Buchholz and Marsi 2006). In this article, we focus on dependency-based parsing of Turkish, a language that is characterized by rich agglutinative morphology, free constituent order, and predominantly head-ﬁnal syntactic constructions. Thus, Turkish can be viewed as the representative of a class of languages that are very different from English and most other languages that have been studied in the parsing literature. Using data from the recently released Turkish Treebank (Oﬂazer et al. 2003), we investigate the impact of different design choices in developing data-driven parsers. There are essentially three sets of"
J08-3003,W06-2925,0,0.0251147,"Missing"
J08-3003,W06-2926,0,0.0329291,"Missing"
J08-3003,W06-2927,0,0.0366282,"Missing"
J08-3003,P96-1025,0,0.197624,"the relatively naive simpler baseline parsers which cannot recover headinitial dependencies. 4. Probabilistic Dependency Parser A well-studied approach to dependency parsing is a statistical approach where the parser takes a morphologically tagged and disambiguated sentence as input, and outputs the most probable dependency tree by using probabilities induced from the training data. Such an approach comprises three components: 1. A parsing algorithm for building the dependency analyses (Eisner 1996; Sekine, Uchimoto, and Isahara 2000) 2. A conditional probability model to score the analyses (Collins 1996) Table 1 Unlabeled attachment scores and unlabeled word-to-word scores for the baseline parsers. Parsing Model ASU WWU Attach-to-next (ﬁrst IG) Attach-to-next (last IG) Rule-based 56.0 54.1 70.5 63.3 63.3 79.3 363 Computational Linguistics 3. Volume 34, Number 3 Maximum likelihood estimation to make inferences about the underlying probability models (Collins 1996; Chung and Rim 2004) 4.1 Methodology The aim of our probabilistic model is to assign a probability to each candidate dependency link by using the frequencies of similar dependencies computed from a training set. The aim of the parsing"
J08-3003,P97-1003,0,0.0192592,"eived: 5 October 2006; revised submission received: 3 April 2007; accepted for publication: 16 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 An important issue in this context is to what extent our models and algorithms are tailored to properties of speciﬁc languages or language groups. This issue is especially pertinent for data-driven approaches, where one of the claimed advantages is portability to new languages. The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser (Collins 1997, 1999), is applied to a new language, which often leads to a signiﬁcant decrease in the measured accuracy (Collins et al. 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and Manning 2003; Corazza et al. 2004). However, it is often quite difﬁcult to tease apart the inﬂuence of different features of the parsing methodology in the observed degradation of performance. A related issue concerns the suitability of different kinds of syntactic representation for different types of languages. Whereas most of the work on English has been based on constituency-based representations, partly inﬂu"
J08-3003,P99-1065,0,0.0474825,"ion for Computational Linguistics Computational Linguistics Volume 34, Number 3 An important issue in this context is to what extent our models and algorithms are tailored to properties of speciﬁc languages or language groups. This issue is especially pertinent for data-driven approaches, where one of the claimed advantages is portability to new languages. The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser (Collins 1997, 1999), is applied to a new language, which often leads to a signiﬁcant decrease in the measured accuracy (Collins et al. 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and Manning 2003; Corazza et al. 2004). However, it is often quite difﬁcult to tease apart the inﬂuence of different features of the parsing methodology in the observed degradation of performance. A related issue concerns the suitability of different kinds of syntactic representation for different types of languages. Whereas most of the work on English has been based on constituency-based representations, partly inﬂuenced by the availability of data resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it has been"
J08-3003,W06-2928,0,0.0384986,"Missing"
J08-3003,W06-2929,0,0.0193811,"Missing"
J08-3003,P03-1013,0,0.066789,"nal Linguistics Volume 34, Number 3 An important issue in this context is to what extent our models and algorithms are tailored to properties of speciﬁc languages or language groups. This issue is especially pertinent for data-driven approaches, where one of the claimed advantages is portability to new languages. The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser (Collins 1997, 1999), is applied to a new language, which often leads to a signiﬁcant decrease in the measured accuracy (Collins et al. 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and Manning 2003; Corazza et al. 2004). However, it is often quite difﬁcult to tease apart the inﬂuence of different features of the parsing methodology in the observed degradation of performance. A related issue concerns the suitability of different kinds of syntactic representation for different types of languages. Whereas most of the work on English has been based on constituency-based representations, partly inﬂuenced by the availability of data resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it has been argued that free constituent order languages c"
J08-3003,C96-1058,0,0.0606541,"Missing"
J08-3003,E06-1012,1,0.909481,"Missing"
J08-3003,W01-0521,0,0.0119816,"parsing accuracy for Turkish, only the lexicalization of conjunctions and nouns together has an impact on accuracy. Similarly to the experiments on inﬂectional features, we again see that the classiﬁer-based parser has no sparse data problem even if we use a totally lexicalized model. Although the effect of lexicalization has been discussed in several studies recently (Dubey and Keller 2003; Klein and Manning 2003; Arun and Keller 2005), it is often investigated as an all-or-nothing affair, except for a few studies that analyze the distributions of lexical items, for example, Bikel (2004) and Gildea (2001). The results for 25 IGs with a noun part-of-speech tag other than common nouns are marked with an additional minor part of speech that indicates whether the nominal is a proper noun or a derived form—one of future participle, past participle, inﬁnitive, or a form involving a zero-morpheme derivation. These latter four do not contain any root information. 377 Computational Linguistics Volume 34, Number 3 Figure 8 Unlabeled and labeled attachment scores for incrementally extended lexicalization for the classiﬁer-based parser. Turkish clearly show that the effect of lexicalization is not uniform"
J08-3003,W94-0314,0,0.0808522,"nclusions from our study. 2. Turkish: Morphology and Dependency Relations Turkish displays rather different characteristics compared to the more well-studied languages in the parsing literature. Most of these characteristics are also found in many agglutinative languages such as Basque, Estonian, Finnish, Hungarian, Japanese, and Korean.1 Turkish is a ﬂexible constituent order language. Even though in written texts the constituent order predominantly conforms to the SOV order, constituents may freely change their position depending on the requirements of the discourse context (Erguvanlı 1979; Hoffman 1994). However, from a dependency structure point of view, Turkish is predominantly (but not exclusively) head ﬁnal. Turkish has a very rich agglutinative morphological structure. Nouns can give rise to about 100 inﬂected forms and verbs to many more. Furthermore, Turkish words may be formed through very productive derivations, increasing substantially the number of possible word forms that can be generated from a root word. It is not uncommon to ﬁnd up to four or ﬁve derivations in a single word. Previous work on Turkish (Hakkani¨ Oﬂazer, and Tur ¨ 2002; Oﬂazer 2003; Oﬂazer et al. 2003; Eryigit ˘"
J08-3003,W06-2930,0,0.0358618,"Missing"
J08-3003,P03-1054,0,0.0188546,"rd forms as the basic ˘ and Oﬂazer 2006). In this article, we units of syntactic structure (Eryigit corroborate this claim showing that it holds in both approaches we explore. We also study the impact of different morphological feature representations on parsing accuracy. The second set of issues concerns lexicalization, a topic that has been very prominent in the parsing literature lately. Whereas the best performing parsers for English all make use of lexical information, the real beneﬁts of lexicalization for English as well as other languages remains controversial (Dubey and Keller, 2003; Klein and Manning 2003; Arun and Keller 2005). The third set concerns the basic parsing methodology, including both parsing algorithms and learning algorithms. We ﬁrst introduce a statistical parser using a conditional probabilistic model which is very sensitive to the selected representational features and thus clearly exposes the ones ˘ Nivre, and Oﬂazer Eryigit, Dependency Parsing of Turkish with crucial importance for parsing Turkish. We then implement our models on a deterministic classiﬁer-based parser using discriminative learning, which is one of the best performing dependency parsers evaluated on a wide ra"
J08-3003,W02-2016,0,0.0945693,".1±0.3 77.1±0.7 77.6±0.5 79.0±0.7 results than using root information (#5). Also, dynamic selection of tags seems to help performance (#3) but using all available inﬂectional information performs signiﬁcantly worse possibly due to data sparseness. 5. Classiﬁer-Based Dependency Parser Our second data-driven parser is based on a parsing strategy that has achieved a high parsing accuracy across a variety of different languages (Nivre et al. 2006, 2007). This strategy consists of the combination of the following three techniques: 1. Deterministic parsing algorithms for building dependency graphs (Kudo and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003) Table 3 Unlabeled attachment scores for different choices for morphological features. Model ASU IG-based model # (Dl =1, Dr =1, Hl =0, Hr =1) 72.1±0.3 1 Using major part of speech instead of minor part of speech 71.2±0.2 2 Using only minor part of speech and no other inﬂectional features 68.3±0.2 3 Using minor part of speech for all types of IGs together with case and possessive markers for nominals and possessive marker for adjectives (but no dynamic selection) 71.0±0.3 4 Using all inﬂectional features in addition to minor part of speech 46.5±0.4 5 Add"
J08-3003,P03-1056,0,0.0147149,"34, Number 3 An important issue in this context is to what extent our models and algorithms are tailored to properties of speciﬁc languages or language groups. This issue is especially pertinent for data-driven approaches, where one of the claimed advantages is portability to new languages. The results so far mainly come from studies where a parser originally developed for English, such as the Collins parser (Collins 1997, 1999), is applied to a new language, which often leads to a signiﬁcant decrease in the measured accuracy (Collins et al. 1999; Bikel and Chiang 2000; Dubey and Keller 2003; Levy and Manning 2003; Corazza et al. 2004). However, it is often quite difﬁcult to tease apart the inﬂuence of different features of the parsing methodology in the observed degradation of performance. A related issue concerns the suitability of different kinds of syntactic representation for different types of languages. Whereas most of the work on English has been based on constituency-based representations, partly inﬂuenced by the availability of data resources such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), it has been argued that free constituent order languages can be analyzed more ade"
J08-3003,W06-2931,0,0.0266142,"Missing"
J08-3003,P95-1037,0,0.0314318,"Missing"
J08-3003,J93-2004,0,0.0418254,"Missing"
J08-3003,W06-2932,0,0.10732,"Missing"
J08-3003,W03-3017,1,0.658285,"e an analysis for an input sentence, and efﬁcient, typically deriving this analysis in time that is linear in the length of the sentence. In the following sections, we will ﬁrst present the parsing methodology and then results that show that the IG-based model again outperforms the word-based model. We will then explore how we can further improve the accuracy by exploiting the advantages of this parser. All experiments are performed using the freely available implementation MaltParser.18 5.1 Methodology For the experiments in this article, we use a variant of the parsing algorithm proposed by Nivre (2003, 2006), a linear-time algorithm that derives a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and a list to store remaining input tokens. However, in contrast to the original arc-eager parsing strategy, we use an arc-standard bottom-up algorithm, as described in Nivre (2004). Like many algorithms used for dependency parsing, this algorithm is restricted to projective dependency graphs. The parser uses two elementary data structures, a stack σ of partially analyzed tokens and an input list τ of remaining input tokens. The pa"
J08-3003,W04-0308,1,0.769162,"ove the accuracy by exploiting the advantages of this parser. All experiments are performed using the freely available implementation MaltParser.18 5.1 Methodology For the experiments in this article, we use a variant of the parsing algorithm proposed by Nivre (2003, 2006), a linear-time algorithm that derives a labeled dependency graph in one left-to-right pass over the input, using a stack to store partially processed tokens and a list to store remaining input tokens. However, in contrast to the original arc-eager parsing strategy, we use an arc-standard bottom-up algorithm, as described in Nivre (2004). Like many algorithms used for dependency parsing, this algorithm is restricted to projective dependency graphs. The parser uses two elementary data structures, a stack σ of partially analyzed tokens and an input list τ of remaining input tokens. The parser is initialized with an empty stack and with all the tokens of a sentence in the input list; it terminates as soon as the input list is empty. In the following, we use subscripted indices, starting from 0, to refer to particular tokens in σ and τ. Thus, σ0 is the token on top of the stack σ (the top token) and τ0 is the ﬁrst token in the in"
J08-3003,W04-2407,1,0.787512,"Missing"
J08-3003,W06-2933,1,0.422301,"Missing"
J08-3003,P05-1013,1,0.627689,"siﬁer-based parser not only builds dependency structures but also assigns dependency labels, we give ASL scores as well as ASU scores. 19 Experiments have also been performed using memory-based learning (Daelemans and Bosch 2005). They were found to give lower parsing accuracy. 20 Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by adding an extra dependency label, we did not observe any improvement when applying the pseudo-projective processing of Nivre and Nilsson (2005), which is reported to improve accuracy for other languages. 372 ˘ Nivre, and Oﬂazer Eryigit, Dependency Parsing of Turkish 5.2 Experimental Results In this section, our ﬁrst aim is to conﬁrm the claim that using IGs as the units in parsing improves performance. For this purpose, we start by using models similar to those described in the previous section. We use an unlexicalized feature model where the parser uses only the minor POS and the DEP of tokens and compare the results with the probabilistic parser. We then show in the second part how we can improve accuracy by exploiting the morpholo"
J08-3003,nivre-etal-2006-talbanken05,1,0.930024,"Word-based model #1 (Dl =1, Dr =1, Hl =1, Hr =1) Word-based model #2 (Dl =1, Dr =1, Hl =1, Hr =1) IG-based model (Dl =1, Dr =1, Hl =0, Hr =1) ASU WWU 68.1±0.4 68.3±0.3 72.1±0.3 77.1±0.7 77.6±0.5 79.0±0.7 results than using root information (#5). Also, dynamic selection of tags seems to help performance (#3) but using all available inﬂectional information performs signiﬁcantly worse possibly due to data sparseness. 5. Classiﬁer-Based Dependency Parser Our second data-driven parser is based on a parsing strategy that has achieved a high parsing accuracy across a variety of different languages (Nivre et al. 2006, 2007). This strategy consists of the combination of the following three techniques: 1. Deterministic parsing algorithms for building dependency graphs (Kudo and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003) Table 3 Unlabeled attachment scores for different choices for morphological features. Model ASU IG-based model # (Dl =1, Dr =1, Hl =0, Hr =1) 72.1±0.3 1 Using major part of speech instead of minor part of speech 71.2±0.2 2 Using only minor part of speech and no other inﬂectional features 68.3±0.2 3 Using minor part of speech for all types of IGs together with case and possessive"
J08-3003,J03-4001,1,0.916091,"Missing"
J08-3003,W97-0301,0,0.106603,"Missing"
J08-3003,W06-2934,0,0.0278293,"Missing"
J08-3003,W05-1513,0,0.0207132,"Missing"
J08-3003,W06-2935,0,0.019617,"Missing"
J08-3003,C00-2109,0,0.0590108,"Missing"
J08-3003,W06-2936,0,0.0257373,"Missing"
J08-3003,W06-2937,0,0.0258319,"Missing"
J08-3003,W03-3023,0,0.46816,"ults than using root information (#5). Also, dynamic selection of tags seems to help performance (#3) but using all available inﬂectional information performs signiﬁcantly worse possibly due to data sparseness. 5. Classiﬁer-Based Dependency Parser Our second data-driven parser is based on a parsing strategy that has achieved a high parsing accuracy across a variety of different languages (Nivre et al. 2006, 2007). This strategy consists of the combination of the following three techniques: 1. Deterministic parsing algorithms for building dependency graphs (Kudo and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003) Table 3 Unlabeled attachment scores for different choices for morphological features. Model ASU IG-based model # (Dl =1, Dr =1, Hl =0, Hr =1) 72.1±0.3 1 Using major part of speech instead of minor part of speech 71.2±0.2 2 Using only minor part of speech and no other inﬂectional features 68.3±0.2 3 Using minor part of speech for all types of IGs together with case and possessive markers for nominals and possessive marker for adjectives (but no dynamic selection) 71.0±0.3 4 Using all inﬂectional features in addition to minor part of speech 46.5±0.4 5 Adding root information to the best perform"
J08-3003,W06-2938,0,0.0546519,"Missing"
J08-3003,megyesi-etal-2008-swedish,1,\N,Missing
J08-3003,E06-1011,0,\N,Missing
J08-3003,N06-1042,0,\N,Missing
J08-3003,J03-4003,0,\N,Missing
J08-3003,P05-1012,0,\N,Missing
J08-3003,C04-1010,1,\N,Missing
J08-3003,P09-1039,0,\N,Missing
J08-3003,P83-1017,0,\N,Missing
J08-3003,W06-2924,0,\N,Missing
J08-3003,W07-2420,0,\N,Missing
J08-3003,P93-1005,0,\N,Missing
J08-4010,W04-2407,1,\N,Missing
J08-4010,megyesi-etal-2008-swedish,1,\N,Missing
J08-4010,nivre-etal-2006-talbanken05,1,\N,Missing
J08-4010,J93-2004,0,\N,Missing
J08-4010,W03-3023,0,\N,Missing
J08-4010,E06-1011,0,\N,Missing
J08-4010,W06-2926,0,\N,Missing
J08-4010,W06-2937,0,\N,Missing
J08-4010,P97-1003,0,\N,Missing
J08-4010,W06-2935,0,\N,Missing
J08-4010,N06-1042,0,\N,Missing
J08-4010,W01-0521,0,\N,Missing
J08-4010,W06-2927,0,\N,Missing
J08-4010,W00-1201,0,\N,Missing
J08-4010,W02-2016,0,\N,Missing
J08-4010,W06-2933,1,\N,Missing
J08-4010,W06-2920,0,\N,Missing
J08-4010,J03-4001,1,\N,Missing
J08-4010,W04-0308,1,\N,Missing
J08-4010,W05-1513,0,\N,Missing
J08-4010,H92-1026,0,\N,Missing
J08-4010,W06-2936,0,\N,Missing
J08-4010,W06-2923,0,\N,Missing
J08-4010,C96-1058,0,\N,Missing
J08-4010,W06-2934,0,\N,Missing
J08-4010,W94-0314,0,\N,Missing
J08-4010,W06-2929,0,\N,Missing
J08-4010,W06-2925,0,\N,Missing
J08-4010,W06-2931,0,\N,Missing
J08-4010,E06-1012,1,\N,Missing
J08-4010,J03-4003,0,\N,Missing
J08-4010,P03-1054,0,\N,Missing
J08-4010,P05-1038,0,\N,Missing
J08-4010,P96-1025,0,\N,Missing
J08-4010,P05-1012,0,\N,Missing
J08-4010,W06-2930,0,\N,Missing
J08-4010,C04-1010,1,\N,Missing
J08-4010,P09-1039,0,\N,Missing
J08-4010,W06-2932,0,\N,Missing
J08-4010,P03-1056,0,\N,Missing
J08-4010,P83-1017,0,\N,Missing
J08-4010,W06-2928,0,\N,Missing
J08-4010,W06-2924,0,\N,Missing
J08-4010,P95-1037,0,\N,Missing
J08-4010,J02-2002,0,\N,Missing
J08-4010,W07-2420,0,\N,Missing
J08-4010,W06-2922,0,\N,Missing
J08-4010,P05-1013,1,\N,Missing
J08-4010,P99-1065,0,\N,Missing
J08-4010,W06-2938,0,\N,Missing
J08-4010,P93-1005,0,\N,Missing
J08-4010,P03-1013,0,\N,Missing
J08-4010,W03-3017,1,\N,Missing
J08-4010,W04-3224,0,\N,Missing
J08-4010,C00-2109,0,\N,Missing
J96-1003,P95-1004,0,0.065281,"Missing"
J96-1003,C94-1066,0,0.011155,"raph for matching ababa with threshold 1 Figure 5 Recognizer for (aba + bab)* a n d search g r a p h for ababa. 1 or s (but not both) may be the null symbol 0. It is possible to apply error-tolerant recognition to languages whose word formations employ productive compounding, or agglutination, or both. In fact, error-tolerant recognition can be applied to any language whose morphology has been described completely as one (very large) finitestate transducer. Full-scale descriptions using this approach already exist for a number of languages such as English, French, German, Turkish, and Korean (Karttunen 1994). Application of error-tolerant recognition to morphological analysis proceeds as described earlier. After a successful match with a surface symbol the corresponding lexical symbol is appended to the output gloss string. During backtracking the candidate surface string and the gloss string are again shortened in tandem. The basic algorithm for this case is given in Figure 6. 3 The actual algorithm is a slightly optimized version of this, in which transitions with null surface symbols are treated as special during forward and backtracking traversals to avoid unnecessary computations of the cut-"
J96-1003,C92-1025,0,0.0110703,"Missing"
J96-1003,E93-1066,1,0.72563,"ages is fundamentally different from w o r d structure in languages like English. Our morphological analyzer for Turkish is based on a lexicon of about 28,000 root 4 This is a manner adverb meaning roughly '(behaving) as if you were one of those whom we might not be able to civilize.' 5 Glosses in parentheses indicate derivations not explicitly indicated by a morpheme. • 80 Kemal Oflazer Error-tolerant Finite-state Recognition words and is a re-implementation, using Xerox two-level transducer technology (Karttunen and Beesley 1992), of an earlier version of the same description by the author (Oflazer 1993) (using the PC-KIMMO environment [Antworth 1990]). This description of Turkish morphology has 31 two-level rules that implement the morphographemic phenomena, such as vowel harmony and consonant changes across morpheme boundaries, and about 150 additional rules, again based on the two-level formalism, that fine-tune the morphotactics by enforcing long-distance feature sequencing and cooccurrence constraints. They also enforce constraints imposed by standard alternation linkage among various lexicons to implement the paradigms. Turkish morphotactics is circular, due to the presence of a relativ"
J96-1003,A94-1037,1,0.811461,"ve used morphological analysis techniques. Veronis (1988) presents a method for handling quite complex combinations of typographical and phonographic errors (phonographic errors are the kind usually made by language learners using computer-aided instruction). This method takes into account phonetic similarity, in addition to standard errors. Aduriz et al. (1993) present a two-level morphology approach to spelling correction in Basque. They use twolevel rules to describe common insertion and deletion errors, in addition to the twolevel rules for the morphographemic component. Oflazer and G6zey (1994) present a two-level morphology approach to spelling correction in agglutinative languages using a coarser morpheme-based morphotactic description rather than the finer lexi8 Ranking is dependent on the language, the application, and the error model. It is an important component of the spelling correction problem, but is not addressed in this paper. 83 Computational Linguistics Recognizer for the word list abacus, abacuses, abalone, abandone, Volume 22, Number 1 abandoned, abandoning access. Figure 7 A finite-state recognizer for the word list: abacus, abacuses, abalone, abandone, abandoned, a"
J96-1003,A94-1024,1,0.563022,"Missing"
J96-1003,J95-2004,0,0.0309018,"Missing"
J96-1003,C88-2146,0,0.0720668,"in a word list. For example, Finnish nouns have about 2,000 distinct forms, while Finnish verbs have about 12,000 forms (Gazdar and Mellish 1989, 59--60). Turkish is similar: nouns, for instance, may have about 170 different forms, not counting the forms for adverbs, verbs, adjectives, or other nominal forms, generated (sometimes circularly) by derivational suffixes. Hankamer (1989) gives much higher figures (in the millions) for Turkish; presumably he took derivations into account in his calculations. Some recent approaches to spelling correction have used morphological analysis techniques. Veronis (1988) presents a method for handling quite complex combinations of typographical and phonographic errors (phonographic errors are the kind usually made by language learners using computer-aided instruction). This method takes into account phonetic similarity, in addition to standard errors. Aduriz et al. (1993) present a two-level morphology approach to spelling correction in Basque. They use twolevel rules to describe common insertion and deletion errors, in addition to the twolevel rules for the morphographemic component. Oflazer and G6zey (1994) present a two-level morphology approach to spellin"
J96-1003,E93-1046,0,0.0115027,"erated (sometimes circularly) by derivational suffixes. Hankamer (1989) gives much higher figures (in the millions) for Turkish; presumably he took derivations into account in his calculations. Some recent approaches to spelling correction have used morphological analysis techniques. Veronis (1988) presents a method for handling quite complex combinations of typographical and phonographic errors (phonographic errors are the kind usually made by language learners using computer-aided instruction). This method takes into account phonetic similarity, in addition to standard errors. Aduriz et al. (1993) present a two-level morphology approach to spelling correction in Basque. They use twolevel rules to describe common insertion and deletion errors, in addition to the twolevel rules for the morphographemic component. Oflazer and G6zey (1994) present a two-level morphology approach to spelling correction in agglutinative languages using a coarser morpheme-based morphotactic description rather than the finer lexi8 Ranking is dependent on the language, the application, and the error model. It is an important component of the spelling correction problem, but is not addressed in this paper. 83 Com"
J96-1003,E93-1057,0,\N,Missing
J96-1003,E93-1069,0,\N,Missing
L16-1295,avramidis-etal-2014-taraxu,0,0.0610864,"A portion of the corpus contains an analysis of the type of errors made by the MT system. Elming (2006) created a 265K-word English-Danish MT manually corrected corpus by a human professional translator. The full corpus covers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The corpus is a collection of blocks composed of the source language texts, the machine translation output of a rule-based MT system and the final post-edited version done by a human translator. Moreover, Avramidis et al. (2014) built a corpus of human-annotated machine translations which was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their c"
L16-1295,D14-1026,1,0.764023,"was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22K rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014b; Zaghouani et al., 1869 2015). The project goal was to create an error corrected 2M-words corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. The 100K-word machine translation portion was selected from various"
L16-1295,W12-5611,0,0.0270103,"Missing"
L16-1295,2006.eamt-1.27,0,0.166044,"pair. Keywords: Post-Editing, Guidelines, Annotation 1. 2. Introduction In recent years, machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. Successful automatic post-editing approaches using manually corrected MT output were used by Elming (2006) and Simard et al. (2007). The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no manually post-edited MT corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this paper, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we c"
L16-1295,fishel-etal-2012-terra,0,0.104056,"ers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The corpus is a collection of blocks composed of the source language texts, the machine translation output of a rule-based MT system and the final post-edited version done by a human translator. Moreover, Avramidis et al. (2014) built a corpus of human-annotated machine translations which was evaluated by professional human translators for the following three language pairs: GermanEnglish, English-German and Spanish-German. Fishel et al. (2012) created a corpus of automatically produced translations with detailed manual translation error analysis of 576 sentences for four language pairs: EnglishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sen"
L16-1295,W14-3605,1,0.85988,"one. 7. Conclusions We have presented in detail the methodology used to create a 100K-word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015). We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. In the future, we plan to increase the size of the corpus and also to add other corpus domains. Acknowledgements We would like to thank the anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotat"
L16-1295,I13-2001,1,0.511808,"e was closely monitored during the initial period, before allowing the annotator to join the official post-editing production phase. Moreover, a dedicated online discussion group was frequently used by the annotation team to keep track of the MT post-editing questions and issues raised during the annotation process. This mechanism, proved to help the annotators and the lead annotator to have a better communication. The annotation itself is done using the QAWI annotation tool, an in house built web annotation framework designed originally for the manual correction of errors in L1 and L2 texts (Obeid et al., 2013). This framework includes two major components: The annotation management interface which is used to assist the lead annotator in the general work-flow process, it allows the annotation manager easily upload and organize files and projects, manage users, assign files in a batch or individually, export annotation tasks and monitor the current annotation progress by processing real time annotation progress statistics. Moreover inter-annotator agreement (IAA), evaluation metrics such as the Word Error Rate (WER) are integrated with the management interface to allow the scores to be computed and t"
L16-1295,pasha-etal-2014-madamira,1,0.825321,"Word Edit: to correct/modify a word. 2. Word Move: to move words to the right location in the sentence. 3. Add Word: insert missing words in the text. 4. Delete: delete unnecessary words. 5. Merge and Split: to merge or split words. All post-editing action history previously mentioned are recorded in a database and can be exported to an XML file. Figure 2 shows an example of how the annotation actions are stored in the XML annotation export file. Finally, and in order to increase the post-editing speed and prior to the first human pass, an automatic post-editing pass is done through MADAMIRA (Pasha et al., 2014), a tool that automatically corrects common spelling errors using a prediction model based on the words in-context. MADAMIRA uses a morphological analyzer to produce, for each input word, a list of analyses specifying every possible morphological interpretation of that word, covering all morphological features of the word. Most of the errors automatically corrected are related to Ya/AlifMaqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are common spelling errors in Arabic.6 6. 6.1. Evaluation Inter-Annotator Agreement We use Word error Rate (WER) as a proxy of the Inter annotator agreement"
L16-1295,W15-3204,1,0.858567,"We have presented in detail the methodology used to create a 100K-word English to Arabic MT manually post-edited corpus, including the development of the guidelines as well as the annotation procedure and the quality control procedure using frequent inter-annotator measures. The created guidelines will be made publicly available and we look forward to distribute the post-edited corpus in a planned shared task on automatic error correction and getting feedback from the community on its usefulness as it was in the previous shared tasks we organized for the L1 and L2 corpus (Mohit et al., 2014; Rozovskaya et al., 2015). We believe that this corpus will be valuable to advance research efforts in the machine translation area since manually annotated data is often needed by the MT community. We believe that our methodology for guideline development and annotation consistency checking can be applied in other projects and other languages as well. In the future, we plan to increase the size of the corpus and also to add other corpus domains. Acknowledgements We would like to thank the anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fat"
L16-1295,N07-1064,0,0.122569,"st-Editing, Guidelines, Annotation 1. 2. Introduction In recent years, machine translation (MT) became widely used by translation companies to reduce their costs and improve their speed. Therefore, the demand for quick and accurate machine translations is growing. Machine translation (MT) systems often produce incorrect output with many grammatical and lexical choice errors. Correcting machine-produced translation errors, or MT Post-Editing (PE) can be done automatically or manually. Successful automatic post-editing approaches using manually corrected MT output were used by Elming (2006) and Simard et al. (2007). The availability of annotated resources is required for such approaches. When it comes to the Arabic language, to the best of our knowledge, there is no manually post-edited MT corpora available to build such systems. Therefore, there is a clear need to build such valuable resources for the Arabic language. In this paper, we present our guidelines and annotation procedure to create a human corrected MT corpus for the Modern Standard Arabic (MSA). The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created comprehensive and"
L16-1295,2006.amta-papers.25,0,0.111195,"Missing"
L16-1295,wisniewski-etal-2014-corpus,0,0.0305494,"ed to check the annotation quality. To the best of our knowledge, this is the first published machine translation manual post-editing annotation effort for Arabic of this scale. In the next sections, we review related work (Section 2), describe our corpus and the development of the guidelines (Sections 3-4), and present our annotation procedure (Section 5), than we present the annotation evaluation in Section 6, finally we conclude our work in Section 7. Related Work Large scale manually corrected MT corpora are not yet widely available due to the high cost related to building such resources. Wisniewski et al. (2014) created a corpus of machine translation errors extracted from several translation students taking part in a master program in specialized translations. The texts are translated from English to French. A portion of the corpus contains an analysis of the type of errors made by the MT system. Elming (2006) created a 265K-word English-Danish MT manually corrected corpus by a human professional translator. The full corpus covers the chemical patents domain. Simard et al. (2007) created a 500K-word corpus of manually edited FrenchEnglish and English-French MT from the Canadian Job Bank website. The"
L16-1295,zaghouani-etal-2014-large,1,0.889004,"glishCzech;French-German;German-English;English-Serbian. Popescu-belis et al. (2002) produced a small corpus of 50 texts translated by students and corrected by their professors and all translation errors are annotated with their corrections in this corpus. For Arabic, we cite the effort of Bouamor et al. (2014) who created a medium scale human judgment corpus of Arabic machine translation using the output of six MT systems and a total of 1892 sentences and 22K rankings. Our corpus is a part of the Qatar Arabic Language Bank (QALB) project, a large scale manually annotated annotation project (Zaghouani et al., 2014b; Zaghouani et al., 1869 2015). The project goal was to create an error corrected 2M-words corpus for online user comments on news websites, native speaker essays, non-native speaker essays and machine translation output. The 100K-word machine translation portion was selected from various Wikinews English articles translated to Arabic automatically using the Google Translate tool.1 3. Corpus Description We collected a 100K-word corpus of English news articles taken from the collaborative journalism Wikinews website.2 Since Wikinews is a free-content news source, we avoided any copyrights comp"
L16-1295,W15-1614,1,0.623101,"Missing"
L16-1577,D15-1274,0,0.145302,"entence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y«ð Y«ð Y«ð  ð Y« ð Y« Y « ð Y « ð English he p"
L16-1577,W15-3209,1,0.859315,"th the diacritics for a variety of Arabic texts covering more than 10 genres. The target size of the annotated corpus is 2 million words. The present work was mainly motivated by the lack of equivalent multi-genres large scale annotated corpus. The creation of manually annotated corpus usually presents many challenges and issues. In order to address those challenges, we created comprehensive and simplified annotation guidelines that were used by a team consisting of five annotators and an annotation manager. The guidelines were defined after an initial pilot annotation experiment described in Bouamor et al. (2015). In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and a regular inter-annotator agreement (IAA) measures were performed to check annotation quality. To the best of our knowledge, this is the first Arabic diacritized multi-genres corpus. The remainder of this paper is organized as follows. We review related work in section 2. Afterwards, we discuss the challenges posed by the complexity of the Arabic diacritization process in section 3. Then, we describe our corpus and the development of the guidelines in sections 4 and 5. 3637 We pres"
L16-1577,2007.mtsummit-papers.20,1,0.926265,"nced, except the last letter diacritization, and b) case and mood diacritization, which exists above or below the last letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter translite"
L16-1577,palmer-etal-2008-pilot,1,0.881222,"utomatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another re"
L16-1577,dukes-habash-2010-morphological,0,0.0333421,"corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to fin"
L16-1577,maamouri-etal-2008-enhancing,0,0.0233017,"about the text, the author and the source. In order to use the CCA corpus, a normalization effort was done to produce a consistent XML mark-up format to be used by our annotation tool. 5. Development of the Guidelines We provided the annotators with detailed guidelines, describing our diacritization scheme and specifying when and where to add the diacritics. We describe the annotation procedure and explained how to deal with borderline cases. We also include several annotated examples to illustrate the specified rules. Our guidelines are mostly inspired from the LDC POS annotation guidelines (Maamouri et al., 2008). Since, the LDC guidelines are mainly designed for the POS annotation and not specifically for the diacritization per se, we created a simplified version and added some specific diacritization rules to make the annotation process consistent. Below we provide some examples of diacritization exceptions and specific rules. The Shadda: The shadda mark should be added in all cases specified in the guidelines except the following in the definite artilce, where it should not be added to the letter ÊË@ /Allymwn/ ’lemon’ È /l/ of the definite article (e.g. àñÒJ  ÊË@ /A˜llymwn/). Moreover, the shadda"
L16-1577,maamouri-etal-2010-speech,1,0.874215,"and present our future work in section 8. 2. Related Work Since, our paper is mainly about the creation and evaluation of a large annotated corpus, we will focus mostly on this aspect in the previous works. There have been numerous approaches to build an automatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacrit"
L16-1577,W14-3605,1,0.918057,"Missing"
L16-1577,pasha-etal-2014-madamira,1,0.898534,"Missing"
L16-1577,W15-3204,1,0.903933,"Missing"
L16-1577,D15-1152,0,0.0742449,"indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y"
L16-1577,2006.amta-papers.25,0,0.0982236,"Missing"
L16-1577,W10-1836,1,0.806692,"tion system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to"
L16-1577,W12-2511,1,0.785024,"genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and"
L16-1577,zaghouani-etal-2014-large,1,0.532947,"roposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w"
L16-1577,W15-1614,1,0.736149,"ing the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants."
L16-1577,L16-1295,1,0.768521,"diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants. Diacritics on the other h"
L16-1577,P06-1073,0,0.603177,"t letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter"
L18-1415,W15-3206,0,0.310868,"tactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the distribution of tasks to tens of annotators, including managing inter-annotator agreement (IAA) tasks. Our interface borrows ideas from three other existing annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have influenced the design of our system. DIWAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It suppo"
L18-1415,L16-1646,0,0.0160278,"al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readability assessments project in Arabic. In the COLABA initiative (Diab et al., 2010), the authors built tools and resources to process Arabic social media data such as blogs, discussion forums, and chats. Javed et al. (2018) presented an online interface for joint syntactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the"
L18-1415,aziz-etal-2012-pet,0,0.0188875,"Missing"
L18-1415,dickinson-ledbetter-2012-annotating,0,0.0264408,"of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readabi"
L18-1415,L18-1574,1,0.916519,"the word i.J⌦ mÃ '@AÎÒK. AK⌦  wyAbwhAAlxlyj1 involves two spelling errors2 (a word merge and character replacement) which can be corrected as i.J⌦ mÃ '@ AÎÒK. Ag.  wjAbwhA Alxlyj ‘and they brought it to the Gulf’. Furthermore, the first of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows:"
L18-1415,L18-1345,1,0.571093,"on such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online collaborative annotation tool for readability assessments project in Arabic. In the COLABA initiative (Diab et al., 2010), the authors built tools and resources to process Arabic social media data such as blogs, discussion forums, and chats. Javed et al. (2018) presented an online interface for joint syntactic annotation and morphological tokenization for Arabic. In general, many of these existing tools are not designed to handle the peculiarities of dialectal Arabic. They neither provide facilities for managing thousands of documents nor permit the distribution of tasks to tens of annotators, including managing inter-annotator agreement (IAA) tasks. Our interface borrows ideas from three other existing annotation tools: DIWAN, QAWI, and MANDIAC. Here we describe each of these tools and how they have influenced the design of our system. DIWAN is an"
L18-1415,L18-1607,1,0.939775,"al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-end. MADARi utilizes this design to provide the same utility. 3. MADARi Design The MADARi interface is designed to be used by human annotators to create a morphologically annotated corpus of Arabic text. The text we work with comes from social media and is highly dialectal (Bouamor et al., 2018; Khalifa et al., 2018) and has numerous spelling errors. The annotators will carefully correct the spelling of the words and also annotate their morphology. The in-context morphology annotation includes tokenization, POS tagging, lemmatization and English glossing. 3.1. Desiderata In order to manage and process the annotation of the large scale dialectal Arabic corpus, we needed to create a tool to streamline the annotation process. The desiderata for developing the MADARi annotation tool include the following: • The tool must have very minimal requirements on the annotators. • The tool must allow off-site data man"
L18-1415,font-llitjos-carbonell-2004-translation,0,0.0249819,"iscuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD, a general-purpose online colla"
L18-1415,maamouri-etal-2014-developing,1,0.844143,"mÃ '@ AÎÒK. Ag.  wjAbwhA Alxlyj ‘and they brought it to the Gulf’. Furthermore, the first of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows: we present work related to this effort in Section 2. In Section 3., we discuss the design and architecture of our annotation framework. In Section 4."
L18-1415,I13-2001,1,0.898118,"st of the two corrected words includes two clitics that when segmented produce the form: AÎ+ @ÒK. Ag. +  w+ jAbwA +hA ‘and+ they-brought +it’. 1 Transliterations are in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007). 2 Since Arabic dialects do not have a standard orthography, spelling correction here means to conventionalize as per the CODA standard (Habash et al., 2018). Previous work on Arabic morphology annotation interfaces focused either on the problem of manual annotations for POS tagging (Maamouri et al., 2014), or diacritization (Obeid et al., 2016), or spelling correction (Obeid et al., 2013). In this paper we present a tool that allows doing all of these tasks together, eliminating the possibility of error propagation from one annotation level to another. Our tool is named MADARi3 after the project under which it was created: Multi-Arabic Dialect Annotations and Resources (Bouamor et al., 2018). The remainder of this paper is structured as follows: we present work related to this effort in Section 2. In Section 3., we discuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We"
L18-1415,pasha-etal-2014-madamira,1,0.866692,"al., 2016). In particular, we utilized the client-server architecture, as well as the flexible hybrid SQL/JSON storage system used by MANDIAC. This allows us to easily extend our annotation interface with minor changes, if any, to the back-end. Our system stores documents one sentence per row, unlike MANDIAC which stores one document per row. This modification allows the annotation interface to handle larger file sizes without affecting its performance by only overwriting the JSON of the modified sentences and not that of the entire document. Like, DIWAN and MANDIAC, we also utilize MADAMIRA (Pasha et al., 2014), a morphological analyzer and disambiguator for Arabic to pre-compute analyses. 4. Annotation Interface The annotation interface (illustrated in Figures 1 to 4) is where annotators perform the annotation tasks assigned to them. Here we describe the different components and utilities provided this interface. 4.1. Task Overview When starting an annotation session, annotators are first shown the “Task Overview” screen (Figure 1). Here annotators can see information on the size of the current task and their progress so far (Figure 1a). The sentence list can be filtered to contain sentences matchi"
L18-1415,E12-2021,0,0.168509,"Missing"
L18-1415,P11-4010,0,0.0125903,"ection 3., we discuss the design and architecture of our annotation framework. In Section 4. and Section 5., we discuss the annotation and management interfaces, respectively. We finally describe a user study of working with MADARi in Section 6. 2. Related Work Several annotation tools and interfaces were proposed for many languages and to achieve various annotation tasks. Some are general purpose annotation tools, such as BRAT (Stenetorp et al., 2012) and WebAnno (Yimam et al., 2013). Task-specific annotation tools for post-editing and error correction include the work of Aziz et al. (2012), Stymne (2011), Llitjós and Carbonell (2004), and Dickinson and Ledbetter (2012). For Arabic, there are several existing annotation tools, however, they are designed to handle specific NLP tasks and are not easy to adapt to our project. Examples include tools for semantic annotation such as the work of Saleh and Al-Khalifa (2009) and El-ghobashy et al. (2014), 2616 3 ¯⌦ P@Y” madAriy means ‘my orbit’ in Arabic. and the work on dialect annotation by Benajiba and Diab (2010) and Diab et al. (2010). Attia et al. (2009) built a morphological annotation tool. Recently, Al-Twairesh et al. (2016) introduced MADAD,"
L18-1415,P13-4001,0,0.0513893,"Missing"
L18-1415,zaghouani-etal-2014-large,1,0.850041,"WAN is an annotation tool for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modi"
L18-1415,W15-1614,1,0.860721,"l for Arabic dialectal texts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-en"
L18-1415,L16-1295,1,0.845639,"exts (Al-Shargi and Rambow, 2015). It provides annotators with a set of tools for reducing duplicate effort including the use of morphological analyzers to pre-compute analyses, and the ability to apply analyses to multiple occurrences simultaneously. However, it requires installation on a Windows machine and the user interface is not very friendly to newcomers. QAWI (the QALB Annotation Web Interface) was used for token-based text editing to create raw and and text corrected parallel data for automatic text correction tasks (Obeid et al., 2013; Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016). It supported the exact recording of all modifications performed by the annotator which previous tools did not. We utilize this token-based editing system for minor text corrections that transform text of a given dialect into the appropriate CODA orthography (Habash et al., 2018). MANDIAC utilized the token-based editor used in QAWI to perform text diacritization tasks (Obeid et al., 2016). More importantly, it introduced a flexible hybrid data storage system that allows for adding new features to the annotation front-end with little to no modifications to the back-end. MADARi utilizes this d"
L18-1535,W14-1604,1,0.907171,"t. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/ma"
L18-1535,L16-1207,1,0.824624,"lectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not prod"
L18-1535,bouamor-etal-2014-multidialectal,1,0.953924,"cities has 12,000 sentences that are five-way parallel translations, and that could be used to build several Dialectal Arabic NLP applications such as machine translation. An example of a 28-way parallel sentences extracted from C ORPUS -25 is given in Figure 1.5 Translators, identified from each of the 25 cities specifically, were asked to read a set of sentences provided in English or French, and translate them into their dialects. The translators are all native speakers of the dialects of the cities they hail from. We did not choose MSA as a starting point to avoid biasing the translation (Bouamor et al., 2014). 6 4 The English, French and MSA versions we use are those provided in the IWSLT evaluation campaign (Eck and Hori, 2005). 5 The MADAR Corpus is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 6 The translation was handled by Ramitechs (http://www. ramitechs.com/), a company that creates and annotates several types of corpora and lexicons using expert linguists. 3388 English French MSA Beirut Cairo Doha Rabat Tunis Aleppo Alexandria Algiers Amman Aswan Baghdad Basra Benghazi Damascus Fes Jeddah Jerusalem Khartoum Mosul Muscat This room is too small. Cette chambre est trop pe"
L18-1535,cotterell-callison-burch-2014-multi,0,0.122726,"ri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions tha"
L18-1535,W14-3629,0,0.0380058,"these differences. Phonology An example of phonological differences is in the pronunciation of dialectal words whose MSA cog nate has the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (AS"
L18-1535,diab-etal-2014-tharwa,1,0.957896,"itten language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045"
L18-1535,habash-etal-2012-conventional,1,0.901479,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,W12-2301,1,0.897253,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,L18-1574,1,0.896503,"I) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic dialects suggest that the syntactic differences between the dialects are relat"
L18-1535,W14-3603,1,0.936447,"Missing"
L18-1535,W14-3627,1,0.896251,"and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but"
L18-1535,L16-1679,1,0.93096,"Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic di"
L18-1535,Y15-1004,0,0.20582,"h of their coverage and the fine location granularity. The focus on cities, as opposed to regions in studying Arabic dialects, opens new avenues to many areas of research from dialectology to dialect identification and machine translation. Keywords: Arabic Dialects, Parallel Corpus, Lexicon 1. Introduction 2. Dialectal Arabic (DA) is emerging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified fra"
L18-1535,pasha-etal-2014-madamira,1,0.875164,"les from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Extraction Our trilingual concept extraction approach focuses on collecting frequently used triplets."
L18-1535,W15-3208,1,0.95203,"hreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Synt"
L18-1535,P13-2001,0,0.0207968,"nformal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045 entries in each city’s"
L18-1535,salama-etal-2014-youdacc,1,0.898896,"developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted i"
L18-1535,W15-3205,0,0.305387,"ponsible for. • Delete all entries that are NOT relevant to the cities he/she is responsible for. • Apply the necessary changes for some entries that may need some minor fixes. • Add new words that are not on the AUTO list. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell"
L18-1535,N03-1033,0,0.0111141,"t key identification relies on an automatic process that extracts (English, French, Arabic) related tuples from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Ex"
L18-1535,L18-1111,1,0.771723,"ons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not produce naturally. The same conce"
L18-1535,P11-2007,0,0.621852,"erging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-wa"
L18-1535,N12-1006,0,0.0438526,"(Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the"
L18-1535,zribi-etal-2014-conventional,1,0.960805,"ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK"
mohamed-etal-2012-annotating,P05-1071,0,\N,Missing
N13-1004,J93-2003,0,0.0759257,"nal Linguistics Turkish-English pair both on hand-aligned data and by running end-to-end machine translation experiments. To evaluate our results, we created gold word alignments for 75 Turkish-English sentences. We obtain significant improvement of AER and BLEU scores over IBM Model 4. Section 2.1 introduces the concept of morpheme alignment in terms of its relation to word alignment. Section 2.2 presents the derivation of the EM algorithm and Section 3 presents the results of our experiments. 2 Two-level Alignment Model (TAM) 2.1 Morpheme Alignment Following the standard alignment models of Brown et al. (1993), we assume one-to-many alignment for both words and morphemes. A word alignment aw (or only a) is a function mapping a set of word positions in a source language sentence to a set of word positions in a target language sentence. A morpheme alignment am is a function mapping a set of morpheme positions in a source language sentence to a set of morpheme positions in a target language sentence. A morpheme position is a pair of integers (j, k), which defines a word position j and a relative morpheme position k in the word at position j. The alignments below are depicted in Figures 1 and 2. aw (1)"
N13-1004,D09-1075,1,0.906515,"account the morpheme, the smallest unit of syntax, beyond merely splitting words. Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspon"
N13-1004,E03-1004,0,0.650851,"Missing"
N13-1004,H05-1085,0,0.40777,"smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the transla"
N13-1004,D07-1031,0,0.0816608,"miss+VB–VBG+JJ person+NN–NNS search+NN unit+NN within+IN the+DT minister+NN– y+N|N. of+IN the+DT interior+NN .+. (e) In+IN November+NNP 1996+CD the+DT Turkish+JJ authorities+NNS set+VBD up+RP a+DT missing+JJ persons+NNS search+NN unit+NN within+IN the+DT Ministry+NNP of+IN the+DT Interior+NNP .+. Figure 3: Turkish-English data examples amounts to a small change to the M step of the original EM algorithm. We introduce Dirichlet priors α to perform an inexact normalization by applying the function f (v) = exp(ψ(v)) to the expected counts collected in the E step, where ψ is the digamma function (Johnson, 2007). θx|y = f (E[c(x|y)] + α) P f ( j E[c(xj |y)] + α) We set α to 10−20 , a very low value, to have the effect of anti-smoothing, as low values of α cause the algorithm to favor words which co-occur frequently and to penalize words that co-occur rarely. 3 Experimental Setup 3.1 Data We trained our model on a Turkish-English parallel corpus of approximately 50K sentences, which have a maximum of 80 morphemes. Our parallel data consists mainly of documents in international relations and legal documents from sources such as the Turkish Ministry of Foreign Affairs, EU, etc. We followed a heavily sup"
N13-1004,N03-1017,0,0.0315582,"assed GIZA++ in the Moses toolkit (Och and Ney, 2003). We also ran GIZA++ (IBM Model 1–4) on the data. We translated 1000 sentence test sets. 4 Results and Discussion We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Second, we used the word Viterbi alignments of our algorithm to obtain BLEU scores. Table 2 shows the AER (Och and Ney, 2003) of the word alignments of the Turkish-English pair and the translation performance of the word alignments learned by our models. We report the grow-diagfinal (Koehn et al., 2003) of the Viterbi alignments. In Table 2, results obtained with different versions of the English data are represented as follows: ‘Der’ stands for derivational morphology, ‘Inf’ for inflectional morphology, and ‘POS’ for part-of-speech 38 tags. ‘Der+Inf’ corresponds to the example sentence in line (d) in Figure 3, and ‘POS’ to line (e). ‘DIR’ stands for models with Dirichlet priors, and ‘NO DIR’ stands for models without Dirichlet priors. All reported results are of the HMM extension of respective models. Table 2 shows that using Dirichlet priors hurts the AER performance of the word-and-morphe"
N13-1004,W04-3250,0,0.068395,"stands for models with Dirichlet priors, and ‘NO DIR’ stands for models without Dirichlet priors. All reported results are of the HMM extension of respective models. Table 2 shows that using Dirichlet priors hurts the AER performance of the word-and-morpheme model in all experiment settings, and benefits the morpheme-only model in the POS tagged experiment settings. In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). Table 2 visualizes the partition of the BLEU scores into statistical significance groups. If two scores within the same column have the same background color, or the border between their cells is removed, then the difference between their scores is not statistically significant. For example, the best BLEU scores, which are in bold, have white background. All scores in a given experiment setting without white background are significantly worse than the best score in that experiment setting, unless there is no border separating them from the best score. In all experiment settings, the TAM Mode"
N13-1004,N04-4015,0,0.12299,"arch on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, an"
N13-1004,P04-1066,0,0.0443788,"Missing"
N13-1004,P11-1090,0,0.018095,"allest unit of syntax, beyond merely splitting words. Since morphology has not been addressed explicitly in word alignment models, researchers have resorted to tweaking SMT systems by manipulating the content and the form of what should be the so-called “word”. Since the word is the smallest unit of translation from the standpoint of word alignment models, the central focus of research on translating morphologically rich languages has been decomposition of morphologically complex words into tokens of the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternat"
N13-1004,C00-2162,0,0.282407,"f the right granularity and representation for machine translation. Chung and Gildea (2009) and Naradowsky and Toutanova (2011) use unsupervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes in line with the linguistic conception of these terms. Our model avoids t"
N13-1004,J03-1002,0,0.160536,"heme alignments, weighted by their probability. The morpheme count function below collects evidence from a word pair (e, f ) in a sentence pair (e, f ) as follows: For all words ej of the sentence e and for all word alignments aw (j), for all morphemes ekj of the word ej and for all morpheme alignments am (j, k), we collect counts for a particular input morpheme g and an output morpheme h iff ej = e and faw (j) = f and h = ekj and g = fam (j,k) . cm (h|g; e, f , aw , am ) = X X T (e|f ) |f | P 2lf − 1 words for the HMM model, the positions > lf stand for null positions between the words of f (Och and Ney, 2003). We do not allow null to null jumps. In sum, we enforce the following constraints: t(h|g) P (i + lf + 1|i0 ) = p(null|i0 ) |f | P P (i + lf + 1|i0 + lf + 1) = 0 1≤j≤|e| 1≤k≤|e| T (e|fi ) t(h|f i ) s.t. s.t. k i=0 i=1 e=ej h=ej f =faw (j) g=f am (j,k) P (i|i0 + lf + 1) = p(i|i0 ) The left part of the morpheme count function is the same as the word-counts in Eqn. 5. Since it does not contain h or g, it needs to be computed only once for each word. The right part of the equation is familiar from the IBM Model 1 counts. In the HMM extension of TAM, we perform forward-backward training using the w"
N13-1004,C96-2141,0,0.876455,"P (i|i0 + lf + 1) = p(i|i0 ) The left part of the morpheme count function is the same as the word-counts in Eqn. 5. Since it does not contain h or g, it needs to be computed only once for each word. The right part of the equation is familiar from the IBM Model 1 counts. In the HMM extension of TAM, we perform forward-backward training using the word counts in Eqn. 5 as the emission probabilities. We calculate the posterior word translation probabilities for each ej and fi such that 1 ≤ j ≤ le and 1 ≤ i ≤ 2lf − 1 as follows: 2.3 HMM Extension γj (i) = We implemented TAM with the HMM extension (Vogel et al., 1996) at the word level. We redefine p(e|f ) as follows: |e| R(e, f ) XY p(s(j ) |C (faw (j −1 ) )) t(ej |faw (j) ) aw j=1 R(ej , faw (j) ) |ej | XY ! t(ekj |fam (j,k) ) αj (i)βj (i) 2lP f −1 where α is the forward and β is the backward probabilities of the HMM. The HMM word counts, in turn, are the posterior word translation probabilities obtained from the forward-backward training: X cw (e|f ; e, f , aw ) = γj (aw (j)) 1≤j≤|e| s.t. e=ej f =faw (j) am k=1 where the distortion probability depends on the relative jump width s(j) = aw (j − 1) − aw (j), as opposed to absolute positions. The distortion"
N13-1004,P10-1047,1,0.87694,"supervised methods to find Kemal Oflazer Computer Science Carnegie Mellon University PO Box 24866, Doha, Qatar word segmentations that create a one-to-one mapping of words in both languages. Al-Onaizan et al. ˇ (1999), Cmejrek et al. (2003), and Goldwater and McClosky (2005) manipulate morphologically rich languages by selective lemmatization. Lee (2004) attempts to learn the probability of deleting or merging Arabic morphemes for Arabic to English translation. Niessen and Ney (2000) split German compound nouns, and merge German phrases that correspond to a single English word. Alternatively, Yeniterzi and Oflazer (2010) manipulate words of the morphologically poor side of a language pair to mimic having a morphological structure similar to the richer side via exploiting syntactic structure, in order to improve the similarity of words on both sides of the translation. We present an alignment model that assumes internal structure for words, and we can legitimately talk about words and their morphemes in line with the linguistic conception of these terms. Our model avoids the problem of collapsing words and morphemes into one single category. We adopt a twolevel representation of alignment: the first level invo"
N13-1046,D07-1074,0,0.105208,"Missing"
N13-1046,N06-1060,0,0.0229809,"sawa, 2007) and have been already used in different works for NEs recognition (Nothman et al., 2013) and disambiguation (Cucerzan, 2007). We improve the Arabic-English Wikipedia title lexicon of Mohit et al. (2012) and build a Wikipedia exclusive lexicon with 4K bilingual entities. In order to test the domain effects, our lexicon includes only NEs which are not present in the parallel corpus. The statistics given in Table 1 demonstrate different nature of the labeled datasets. The two datasets were labeled semi-automatically using the transliteration similarity measure (Frscore ) proposed by Freeman et al. (2006), a variant of edit distance measuring the similarity between an English word and its Arabic transliteration. In our experiments, English tokens having an Frscore &gt; 0.6 are considered as transliteration, others having Frscore &lt; 0.5 as translation. These thresholds were determined after tuning with a held out development set. For tokens having Frscore between 0.5 and 0.6, the decision is not obvious. To label these instances (around 5K unique tokens), we manually transliterate them using Microsoft Maren tool.3 We again compute the Frscore between the obtained transliteration, in its Buckwalter"
N13-1046,W98-1005,0,0.223308,"nditions is expected to reduce the domain gap and improve the translation quality. English Input: The British comedy troupe Monty Python. Baseline MT: Q.Ë@ éK YJ ÓñºË@ é¯Q ®Ë@ .ù ª¯@ UNK éJ KA¢ Alfrqp Alkwmydyp AlbryTAnyp UNK AfEY MT+Classifier: JK AK. úæKñÓ éJ KA¢ Q.Ë@ éK YJ ÓñºË@ é¯Q ®Ë@ . àñ Alfrqp Alkwmydyp AlbryTAnyp mwnty bAyvwn. 4 Related work A number of efforts have been made to undertake the NE translation problem for different language pairs. Among them some use sequence of phonetic-based probabilistic models to convert names written in Arabic into the English script (Glover-Stalls and Knight, 1998) for transliteration of names and technical terms that occurs in Arabic texts and originate in English. Others rely on spellingbased model that directly maps an English letter sequence into an Arabic one (Al-Onaizan and Knight, 2002a). In a related work, Al-Onaizan and Knight (2002b) describe a combination of a phonetic-based model and a spellingbased one to build a transliteration model to generate Arabic to English name translations. In the same direction, Hassan et al. (2007) extracted NE translation pairs from both comparable and parallel corpora and evaluate their quality in a NE translat"
N13-1046,P08-1045,0,0.178336,"Missing"
N13-1046,P07-2045,0,0.00674443,"e offer the transliterated form as an option to the decoder aiming to improve the decoding process. For that a human annotator selected the transliterations from the suggested list that is provided by the automatic transliterator (Maren) without any knowledge of the reference transliterations. Table 4 shows the impact of adding the classifier to the SMT pipeline with a modest improvement. Moreover, a bilingual annotator examined the automatically tagged NEs in the MT test set and labeled them with the translation vs. transliteration 4 The baseline MT system is the M OSES phrase-based decoder (Koehn et al., 2007) trained on a standard English-Arabic parallel corpus. The 18 million parallel corpus consists of the non-UN parts of the NIST corpus distributed by the Linguistic Data Consortium. We perform the standard preprocessing and tokenization on the English side. We also use MADA+TOKAN (Habash et al., 2009) to preprocess and tokenize the Arabic side of the corpus. We use the standard setting of G IZA ++ and the grow-diagonal-final heuristic of M OSES to get the word alignments. We use a set of 500 sentences to tune the decoder parameters using the MERT (Och, 2003). We use El Kholy and Habash (2010) d"
N13-1046,2011.iwslt-papers.3,0,0.164606,"es and technical terms that occurs in Arabic texts and originate in English. Others rely on spellingbased model that directly maps an English letter sequence into an Arabic one (Al-Onaizan and Knight, 2002a). In a related work, Al-Onaizan and Knight (2002b) describe a combination of a phonetic-based model and a spellingbased one to build a transliteration model to generate Arabic to English name translations. In the same direction, Hassan et al. (2007) extracted NE translation pairs from both comparable and parallel corpora and evaluate their quality in a NE translation system. More recently, Ling et al. (2011) propose a Web-based method that translates Chinese NEs into English. Our work is similar in its general objectives and framework to the work presented by Hermjakob et al. (2008), which describes an approach for identifying NEs that should be transliterated from Arabic into English during translation. Their method seeks to find a corresponding English word for each Arabic word in a parallel corpus, and tag the Arabic words as either NEs or non-NEs based on a matching algorithm. In contrast, we tackle this problem in the reverse direction (translating/transliterating English NEs into Arabic). W"
N13-1046,maegaard-etal-2010-cooperation,0,0.0664051,"Missing"
N13-1046,E12-1017,1,0.8978,"Missing"
N13-1046,P03-1021,0,0.0165298,"Missing"
N13-1046,P02-1040,0,0.0849224,"Missing"
N13-1046,W09-1119,0,0.0420229,"of the news related NEs and a smaller set of diverse-topic NEs extracted from Wikipedia titles. We evaluate the two classifiers in both news and the diverse domains to observe the effects of noise and domain change. 2.1 Preparing the labeled data Our classifier requires a set of NEs with token-level gold labels. We compile such data from two resources: We heuristically extract and label parallel NEs from a large word aligned parallel corpus and we use a lexicon of bilingual NEs collected from Arabic and Wikipedia titles. Starting with a word aligned parallel corpus, we use the UIUC NE tagger (Ratinov and Roth, 2009) to tag the English sentences with four classes of NEs: Person (PER), Location (LOC), Organization (ORG) and Miscellaneous (MISC). Furthermore, we use the word alignments to project and collect the span of the associated Arabic named-entities. To reduce the noisy nature of word alignments, we designed a procedure to clean up the noisy Arabic NE spans by POS verification, and heuristically filtering impossible items (e.g. verbs). This results in a bilingual lexicon of about 57K named-entity pairs. The distribution of NEs categories is reported in Table 1. To train and evaluate the Cdiverse clas"
N13-1046,P02-1051,0,\N,Missing
N13-1076,N10-1083,0,0.0254529,"ected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupe"
N13-1076,D08-1092,0,0.0201754,"s starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This"
N13-1076,W10-2906,0,0.0157732,"5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not dep"
N13-1076,J07-2003,0,0.0858748,"Missing"
N13-1076,W06-1670,0,0.725634,"Missing"
N13-1076,W03-1022,0,0.189249,"applications communication ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. Introduction A taxonomic view of lexical semantics groups word senses/usages into categories of varying granularities. WordNet supersense tags denote coarse semantic classes, including person and artifact (for nouns) and motion and weather (for verbs); these categories can be taken as the top level of a taxonomy. Nominal supersense tagging (Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annota"
N13-1076,P11-1061,0,0.0261324,"e morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel"
N13-1076,P10-4002,1,0.808987,"Missing"
N13-1076,elkateb-etal-2006-building,0,0.0311891,"(Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective"
N13-1076,2005.eamt-1.15,0,0.0305449,"bout topical domain performance we were able to find that holds across annotators and systems, in contrast with the stark topical effects observed by Mohit et al. (2012) for NER. 665 noun coverage gains track overall improvements. If QCRI produces better translations, why is cdec more useful for supersense tagging? As noted in §3.3, cdec gives word-level alignments (even when the decoder uses large phrasal units for translation), allowing for more precise projections.11 We suspect this is especially important in light of findings that larger phrase sizes are indicative of better translations (Gamon et al., 2005), so these are exactly the cases where we expect the translations to be valuable. 5 Conclusion To our knowledge, this is the first study of automatic Arabic supersense tagging. We have shown empirically that an MT-in-the-middle technique is most effective of several approaches that do not require labeled training data. Analysis sheds light on several challenges that would need to be overcome for better Arabic lexical semantic tagging. Acknowledgments We thank Wajdi Zaghouani for the translation of the Arabic Wikipedia MT set, Francisco Guzman and Preslav Nakov for the output of QCRI’s MT syste"
N13-1076,P05-1071,0,0.0961947,"Missing"
N13-1076,N06-2015,0,0.0206598,"hunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English,"
N13-1076,P12-1073,0,0.0128156,"is; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the train"
N13-1076,N03-1017,0,0.00971947,"Missing"
N13-1076,P07-2045,1,0.0132485,"Missing"
N13-1076,J94-2001,0,0.08432,"nstructs the Arabic-to-English mapping {1→person11 , 4→location43 , {5, 6}→artifact76 }, resulting in the tagging shown in the bottom row. weighted to favor earlier senses (presumed by lexicographers to be more frequent) and then the supersense with the greatest aggregate weight is selected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with oth"
N13-1076,P07-1123,0,0.019554,"Missing"
N13-1076,H93-1061,0,0.0657483,"Missing"
N13-1076,E12-1017,1,0.866478,"Missing"
N13-1076,P02-1040,0,0.0859619,"Missing"
N13-1076,N12-1090,0,0.0532349,"ords that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpu"
N13-1076,P08-2030,0,0.0741844,"Missing"
N13-1076,P12-2050,1,0.801661,"toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and co"
N13-1076,W04-3207,1,0.770277,"ts with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in"
N13-1076,2006.amta-papers.25,0,0.0975624,"Missing"
N13-1076,N01-1026,0,0.0399772,"t sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely du"
N13-1076,H01-1035,0,0.0177244,"(for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch),"
N13-1076,D08-1063,0,0.11663,"0 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et"
N13-1076,W05-0909,0,\N,Missing
P06-1020,J05-3003,0,0.0378506,"Missing"
P06-1020,J02-2002,0,0.028177,"Missing"
P06-1020,P05-2013,0,0.163224,"Missing"
P06-1020,J03-4001,1,0.884463,"Missing"
P06-1020,E06-1012,1,0.876639,"Missing"
P07-2048,2005.eamt-1.12,0,0.0702639,"Missing"
P07-2048,2003.mtsummit-papers.21,0,0.0251571,"ut minor divergences at morphological and syntactic levels make the translation problem rather non-trivial. Our approach is based on essentially morphological processing, and direct lexical and morphological transfer, augmented with substantial multi-word processing on the source language side and statistical processing on the target side where data for statistical language modelling is more readily available. 189 2 Related Work Studies on machine translation between close languages are generally concentrated around certain Slavic languages (e.g., Czech→Slovak, Czech→Polish, Czech→Lithuanian (Hajic et al., 2003)) and languages spoken in the Iberian Peninsula (e.g., Spanish↔Catalan (Canals et al., 2000), Spanish↔Galician (Corbi-Bellot et al., 2003) and Spanish↔Portugese (Garrido-Alenda et al., 2003). Most of these implementations use similar modules: a morphological analyzer, a part-of-speech tagger, a bilingual transfer dictionary and a morphological generator. Except for the Czech→Lithuanian system which uses a shallow parser, syntactic parsing is not necessary in most cases because of the similarities in word orders. Also, the lexical semantic ambiguity is usually preserved so, none of these system"
P07-2048,P02-1040,0,0.077488,"Missing"
P07-2048,2007.mtsummit-papers.61,1,\N,Missing
P07-2048,tantug-etal-2008-bleu,1,\N,Missing
P07-2048,A00-1002,0,\N,Missing
P07-2048,E87-1021,0,\N,Missing
P07-2048,W04-0409,1,\N,Missing
P07-2048,2008.eamt-1.12,0,\N,Missing
P10-1047,P07-1017,0,0.0842955,"essen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with additional morphological information extracted from a syntactic tree, and have used this in translation to Greek and Czech. Recently, Bisazza and Federico (2009) have applied morphological segmentation in Turkish-to-English statistical machine translation and found that it provides nontrivial BLEU • Object reordering (ObjR), in which the objects and their dependents are moved in front of the verb. • Adverbial phrase reordering (AdvR), which involve"
P10-1047,P08-1087,0,0.22684,"sed phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with additional morphological information extracted from a syntactic tree, and have used this in translation to Greek and Czech. Recently, Bisazza and Federico (2009) have applied morphological segmentation in Turkish-to-English statistical machine translation and found that it provides nontrivial BLEU • Object reordering (ObjR), in which the objects and their dependents are moved in front of the verb. • Adverbial phrase reordering (AdvR), which involve moving post-verbal adverbial phrases in front of the verb. • Passive sentence agent reordering (PassAgR), in which any"
P10-1047,J04-2003,0,0.0188993,"de, the decoder needs to generate both the right sequence of constituents and the right sequence of morphemes for each word. Furthermore, since for such languages one can generate tens of hundreds of inflected variants, standard word-based alignment approaches suffer from sparseness issues. Koehn (2005) applied standard phrase-based SMT to Finnish using the Europarl corpus and reported that translation to Finnish had the worst BLEU scores. Using morphology in statistical machine translation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007)"
P10-1047,2009.iwslt-papers.1,0,0.0259038,"glish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with additional morphological information extracted from a syntactic tree, and have used this in translation to Greek and Czech. Recently, Bisazza and Federico (2009) have applied morphological segmentation in Turkish-to-English statistical machine translation and found that it provides nontrivial BLEU • Object reordering (ObjR), in which the objects and their dependents are moved in front of the verb. • Adverbial phrase reordering (AdvR), which involve moving post-verbal adverbial phrases in front of the verb. • Passive sentence agent reordering (PassAgR), in which any post-verbal agents marked by by, are moved in front of the verb. • Subordinate clause reordering (SubCR) which involve moving postnominal relative clauses or prepositional phrase modifers i"
P10-1047,W07-0704,1,0.816977,"Missing"
P10-1047,W09-0420,0,0.0208214,"(<X>+IN PMOD <Z>+NN<TAG>) then { APPEND <X>+IN TO <Z>+NN<TAG> REMOVE <X>+IN } Here <X>, <Y> and <Z> can be considered as Prolog like-variables that bind to patterns (mostly root words), and the conditions check for specified dependency relations (e.g., PMOD) between the left and the right sides. When the condition is satisfied, then the part matching the function word is removed and its syntactic information is appended to form the complex tag on the noun (<TAG> would either match null string or any previously appended function word markers.)5 • Prepositions attach to the head-word of their 4 Fraser (2009) uses the first four letters of German words after morphological stripping and compound decomposition to help with alignment in German to English and reverse translation. 5 We outline two additional rules later when we see a more complex example in Figure 2. 456 There are several other rules that handle more mundane cases of date and time constructions (for which, the part of the date construct which the parser attaches a preposition, is usually different than the part on the Turkish side that gets inflected with case markers, and these have to be reconciled by overriding the parser output.) T"
P10-1047,H05-1085,0,0.144961,"hology in statistical machine translation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with additional morphological information extracted from a syntactic tree, and have used this in translation to Greek and Czech. Recently, Bisazza and Federico (2009) have applied morphological segmentation in Turkish-to-English statistical machine translation and found that it provides nontrivial BLEU • Object reorde"
P10-1047,P07-1037,0,0.033841,"Missing"
P10-1047,2001.mtsummit-papers.68,0,0.0113552,"the former to the complex tag on the latter and then deletes the former token. 9 The tune set was not used in this work but reserved for future work so that meaningful comparisons could be made. 10 It is possible that the 10 test sets are not mutually exclusive. 8 6 For example, the morphological analyzer outputs +A3sg to mark a singular noun, if there is no explicit plural morpheme. Such markers are removed. 457 Figure 2: An English-Turkish sentence pair with multiple transformations applied 0.1.11 We did not use MERT to further optimize our model.12 For evaluation, we used the BLEU metric (Papineni et al., 2001). Each experiment was repeated over the 10 data sets. Wherever meaningful, we report the average BLEU scores over 10 data sets along with the maximum and minimum values and the standard deviation. 3.2.1 The Baseline Systems As a baseline system, we built a standard phrasebased system, using the surface forms of the words without any transformations, and with a 3-gram LM in the decoder. We also built a second baseline system with a factored model. Instead of using just the surface form of the word, we included the root, part-of-speech and morphological tag information into the corpus as additio"
P10-1047,popovic-ney-2004-towards,0,0.0267706,"tandard phrase-based SMT to Finnish using the Europarl corpus and reported that translation to Finnish had the worst BLEU scores. Using morphology in statistical machine translation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with additional morphological information extracted from a syntactic tree, and have used this in translation to Greek and Czech. Recently, Bisazza and Federico (2009) have"
P10-1047,D07-1091,0,0.15245,"Missing"
P10-1047,N03-1017,0,0.00650663,"Missing"
P10-1047,P07-2045,0,0.00518969,"emove any morphological features that are not explicitly marked by an overt morpheme.6 So for both sides we get, 3.2 Experiments We evaluated the impact of the transformations in factored phrase-based SMT with an EnglishTurkish data set which consists of 52712 parallel sentences. In order to have more confidence in the impact of our transformations, we randomly generated 10 training, test and tune set combinations. For each combination, the latter two were 1000 sentences each and the remaining 50712 sentences were used as training sets.9,10 We performed our experiments with the Moses toolkit (Koehn et al., 2007). In order to encourage long distance reordering in the decoder, we used a distortion limit of -1 and a distortion weight of E: if+IN a+DT request+NN is+VBZ made+VBN orally+RB the+DT authority+NN must+MD make+VB a+DT record+NN of+IN it+PRP T: istek+Noun s¨ ozl¨ u+Adj olarak+Verb+ByDoingSo yap+Verb+Pass+Narr+Cond yetkili+Adj makam+Noun bu+Pron+Acc kaydet+Verb+Neces+Cop Finally we parse the English sentences using MaltParser (Nivre et al., 2007), which gives us labeled dependency parses. On the output of the parser, we make one more transformation. We replace each word with its root, and possibl"
P10-1047,N03-1033,0,0.00553875,"Results Data Preparation We worked on an English-Turkish parallel corpus which consists of approximately 50K sentences with an average of 23 words in English sentences and 18 words in Turkish sentences. This is the same parallel data that has been used in earlier SMT work on Turkish (Durgar-El-Kahlout and Oflazer, 2010). Let’s assume we have the following pair of parallel sentences: E: if a request is made orally the authority must make a record of it T: istek s¨ ozl¨ u olarak yapılmıs ¸sa yetkili makam bunu kaydetmelidir On the English side of the data, we use the Stanford Log-Linear Tagger (Toutanova et al., 2003), to tag the text with Penn Treebank Tagset. On the Turkish side, we perform a full morphological analysis, (Oflazer, 1994), and morphological disambiguation (Yuret and T¨ure, 2006) to select the contextually salient interpretation of words. We then remove any morphological features that are not explicitly marked by an overt morpheme.6 So for both sides we get, 3.2 Experiments We evaluated the impact of the transformations in factored phrase-based SMT with an EnglishTurkish data set which consists of 52712 parallel sentences. In order to have more confidence in the impact of our transformation"
P10-1047,2005.mtsummit-papers.11,0,0.0180245,"21.73 21.88 21.88 STD 0.72 0.71 0.50 0.73 0.61 Max. 22.91 23.12 22.44 23.03 22.77 Min. 20.67 20.56 20.69 20.51 20.92 Table 4: BLEU scores of after reordering transformations 5 Related Work Statistical Machine Translation into a morphologically rich language is a challenging problem in that, on the target side, the decoder needs to generate both the right sequence of constituents and the right sequence of morphemes for each word. Furthermore, since for such languages one can generate tens of hundreds of inflected variants, standard word-based alignment approaches suffer from sparseness issues. Koehn (2005) applied standard phrase-based SMT to Finnish using the Europarl corpus and reported that translation to Finnish had the worst BLEU scores. Using morphology in statistical machine translation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEng"
P10-1047,N09-1028,0,0.0320525,"Missing"
P10-1047,N04-4015,0,0.0247026,"word-based alignment approaches suffer from sparseness issues. Koehn (2005) applied standard phrase-based SMT to Finnish using the Europarl corpus and reported that translation to Finnish had the worst BLEU scores. Using morphology in statistical machine translation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation quality. Avramidis and Koehn (2008) have annotated English with additional morphological information extracted from a syntactic tree,"
P10-1047,E06-1006,0,0.0235957,"right sequence of morphemes for each word. Furthermore, since for such languages one can generate tens of hundreds of inflected variants, standard word-based alignment approaches suffer from sparseness issues. Koehn (2005) applied standard phrase-based SMT to Finnish using the Europarl corpus and reported that translation to Finnish had the worst BLEU scores. Using morphology in statistical machine translation has been addressed by many researchers for translation from or into morphologically rich(er) languages. Niessen and Ney (2004) used morphological decomposition to get better alignments. Yang and Kirchhoff (2006) have used phrasebased backoff models to translate unknown words by morphologically decomposing the unknown source words. Lee (2004) and Zolmann et al. (2006) have exploited morphology in ArabicEnglish SMT. Popovic and Ney (2004) investigated improving translation quality from inflected languages by using stems, suffixes and part-ofspeech tags. Goldwater and McClosky (2005) use morphological analysis on the Czech side to get improvements in Czech-to-English statistical machine translation. Minkov et al. (2007) have used morphological postprocessing on the target side, to improve translation qu"
P10-1047,N06-1042,0,0.0711585,"Missing"
P10-1047,N06-2051,0,0.0445838,"Missing"
P10-1047,P02-1040,0,\N,Missing
P10-1047,W07-0702,0,\N,Missing
P12-2035,W96-0102,0,0.00992123,"ut any word segmentation. We use the Columbia Arabic Treebank 6-tag tag set: PRT (Particle), NOM (Nouns, Adjectives, and Adverbs), PROP (Proper Nouns), VRB (Verb), VRB-pass (Passive Verb), and PNX (Punctuation) (Habash and Roth, 2009). For example, the word wHnktblhm (and we will write to them, )وحىكتبلهم receives the tag PRT+PRT+VRB+PRT+NOM. This results in 58 composite tags, 9 of which occur 5 times or less in the converted ECA training set. We converted two sections of the Arabic Treebank (ATB): p2v3 and p3v2. For all the POS tagging experiments, we use the memory-based POS tagger (MBT) (Daelemans et al., 1996) The best results, tuned on a dev set, were obtained, in nonexhaustive search, with the Modified Value Difference Metric as a distance metric and with k (the number of nearest neighbors) = 25. For known words, we use the IGTree algorithm and 2 words to the left, their POS tags, the focus word and its list of possible tags, 1 right context word and its list of possible tags as features. For unknown words, we use the IB1 algorithm and the word itself, its first 5 and last 3 characters, 1 left context word and its POS tag, and 1 right context word and its list of possible tags as features. 3.1. D"
P12-2035,W05-0708,0,0.0507993,"elated Work To the best of our knowledge, ours is the first work that generates CEA automatically from morphologically disambiguated MSA, but Habash et al. (2005) discussed root and pattern morphological analysis and generation of Arabic dialects within the MAGED morphological analyzer. MAGED incorporates the morphology, phonology, and orthography of several Arabic dialects. Diab et al. (2010) worked on the annotation of dialectal Arabic through the COLABA project, and they used the (manually) annotated resources to facilitate the incorporation of the dialects in Arabic information retrieval. Duh and Kirchhoff (2005) successfully designed a POS tagger for CEA that used an MSA morphological analyzer and information gleaned from the intersection of several Arabic dialects. This is different from our approach for which POS tagging is only an application. Our focus is to use any existing MSA data to generate colloquial Arabic resources that can be used in virtually any NLP task. 179 At a higher level, our work resembles that of Kundu and Roth (2011), in which they chose to adapt the text rather than the model. While they adapted the test set, we do so at the training set level. 5. Conclusions and Future Work"
P12-2035,W05-0703,0,0.238206,"Missing"
P12-2035,W11-0327,0,\N,Missing
P12-2035,N10-1105,1,\N,Missing
P12-2035,P09-2056,0,\N,Missing
P12-2050,attardi-etal-2010-resource,0,0.557965,"Missing"
P12-2050,P98-1013,0,0.0338496,"r require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (o"
P12-2050,W06-1670,0,0.786017,"Missing"
P12-2050,W03-1022,0,0.0596354,"(Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 N"
P12-2050,P05-1004,0,0.00884402,"al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 Note that work"
P12-2050,elkateb-etal-2006-building,0,0.102714,"Missing"
P12-2050,N06-2015,0,0.015385,"in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johns"
P12-2050,kingsbury-palmer-2002-treebank,0,0.0369554,"upersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and"
P12-2050,C02-1150,0,0.0134343,"SONs. This lumping property might be expected to give too much latitude to annotators; yet we find that in practice, it is possible to elicit reasonable inter-annotator agreement, even for a language other than English. We encapsulate our interpretation of the tags in a set of brief guidelines that aims to be usable by anyone who can read and understand a text in the target language; our annotators had no prior expertise in linguistics or linguistic annotation. Finally, we note that ad hoc categorization schemes not unlike SSTs have been developed for purposes ranging from question answering (Li and Roth, 2002) to animacy hierarchy representation for corpus linguistics (Zaenen et al., 2004). We believe the interpretation of the SSTs adopted here can serve as a single starting point for diverse resource engineering efforts and applications, especially when fine-grained sense annotation is not feasible. 2 Tagging Conventions WordNet’s definitions of the supersenses are terse, and we could find little explicit discussion of the specific rationales behind each category. Thus, we have crafted more specific explanations, summarized for nouns in figure 2. English examples are given, but the guidelines are"
P12-2050,H93-1061,0,0.614672,"applications, especially when fine-grained sense annotation is not feasible. 2 Tagging Conventions WordNet’s definitions of the supersenses are terse, and we could find little explicit discussion of the specific rationales behind each category. Thus, we have crafted more specific explanations, summarized for nouns in figure 2. English examples are given, but the guidelines are intended to be language-neutral. A more systematic breakdown, formulated as a 43-rule decision list, is included with the corpus.5 In developing these guidelines we consulted English WordNet (Fellbaum, 1998) and SemCor (Miller et al., 1993) for examples and synset definitions, occasionally making simplifying decisions where we found distinctions that seemed esoteric or internally inconsistent. Special cases (e.g., multiword expressions, anaphora, figurative 5 For example, one rule states that all man-made structures (buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs. 254 language) are addressed with additional rules. 3 Arabic Wikipedia Annotation The annotation in this work was on top of a small corpus of Arabic Wikipedia articles that had already been annotated for named entities (Mohit et al., 2012). Here we use t"
P12-2050,E12-1017,1,0.890745,"Missing"
P12-2050,passonneau-etal-2010-word,0,0.021609,"o a full-fledged lexicon/ontology, which may insufficiently cover the language/domain of interest or require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for d"
P12-2050,picca-etal-2008-supersense,0,0.610731,"Missing"
P12-2050,W09-3531,0,0.371255,"two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 Note that work in supersense tagging used text with finegrained sense annotations that were then coarsened to SSTs. 4 The noun/verb d"
P12-2050,sekine-etal-2002-extended,0,0.0260826,"s paper describes coarse lexical semantic annotation of Arabic Wikipedia articles subject to these constraints. Traditional lexical semantic representations are either narrow in scope, like named entities,1 or make reference to a full-fledged lexicon/ontology, which may insufficiently cover the language/domain of interest or require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr"
P12-2050,W04-0216,0,0.0213529,"Missing"
P12-2050,C98-1013,0,\N,Missing
P13-2126,J96-1002,0,0.0178359,"e last word in a sentence, quote or parentheses, and • between a punctuation mark following a word or between two consecutive punctuation marks. It posits breaks (i) before a word following a punctuation, and (ii) before prepositions, auxiliary verbs, coordinating conjunctions, subordinate conjunctions, relative pronouns, relative adverbs, conjunctive adverbs, and correlative conjunctions. 1 We expect to make our annotated data available upon the publication of the paper. 721 Maximum Entropy Classifier We used the CRF++ Tool2 but with the option to run it only as a maximum entropy classifier (Berger et al., 1996), to train a classifier. We used a large set of about 100 features grouped into the following categories: Precision Recall F1 Baseline 77.9 80.4 79.1 ME-All 87.3 90.2 88.8 ME-GA 89.2 90.2 89.7 Table 1: Results from Baseline and Maximum Entropy break classifiers • Lexical features: These features include the token and the POS tag for the previous, current and the next word. We also encode whether the word is part of a compound noun or a verb, or is an adjective that subcategorizes a specific preposition in WordNet, (e.g., familiar with). Maximum Entropy Classifier with GA Feature Selection We u"
P13-2126,de-marneffe-etal-2006-generating,0,0.0580096,"Missing"
P13-2126,P03-1054,0,0.0241447,"reen. Figure 1 (b) displays an example of a small fragment of text typeset using the output of our best break classifier. One can immediately note that this typesetting has more raggedness overall, but avoids the bad breaks in (a). We are currently in the process of designing a series of experiments for extrinsic evaluation to determine if such typeset text helps comprehension for secondary language learners. We use the following tools to process the sentences to extract some of these features: • Stanford constituency and dependency parsers, (De Marneffe et al., 2006; Klein and Manning, 2002; Klein and Manning, 2003), • lemmatization tool in NLTK (Bird, 2006), • WordNet for compound nouns and verbs (Fellbaum, 1998). 2 Available at http://crfpp.googlecode.com/ svn/trunk/doc/index.html. 722 4.1 Error Analysis breaking before the object NP, could be OK, if not doing so causes an inordinate amount of raggedness. Then the final typesetting stage can optimize a combination of raggedness and the total “badness” of all the breaks it posits. An analysis of the errors our best classifier makes (which may or may not be translated into an actual error in the final typesetting) shows that the majority of the errors ba"
P13-2126,P06-4018,0,\N,Missing
P97-1029,E93-1046,0,0.0758847,"native nature of the language usually helps resolution of such ambiguities due to the restrictions on morphotactics of subsequent morphemes. On the Introduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al., 1992; DeRose, 1988), constraint-based techniques (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and T i l l 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements. The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given 1Voutilainen, Private communication. 222 other hand, this very nature introduces another kind of ambiguity, where a l"
P97-1029,J95-4004,0,0.732687,"ntroduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al., 1992; DeRose, 1988), constraint-based techniques (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and T i l l 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements. The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given 1Voutilainen, Private communication. 222 other hand, this very nature introduces another kind of ambiguity, where a lexical form can be morphologically interpreted in many ways not usually predictable in advance. Furthermore, Turkish allows very"
P97-1029,A88-1019,0,0.652025,"guages like English. In Turkish, there are ambiguities of the sort typically found in languages like English (e.g., book/noun vs book/verb type). However, the agglutinative nature of the language usually helps resolution of such ambiguities due to the restrictions on morphotactics of subsequent morphemes. On the Introduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al., 1992; DeRose, 1988), constraint-based techniques (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and T i l l 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements. The approach depends on assigning votes to constraints according to their complexity and specificity, a"
P97-1029,A92-1018,0,0.629509,"glish. In Turkish, there are ambiguities of the sort typically found in languages like English (e.g., book/noun vs book/verb type). However, the agglutinative nature of the language usually helps resolution of such ambiguities due to the restrictions on morphotactics of subsequent morphemes. On the Introduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al., 1992; DeRose, 1988), constraint-based techniques (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and T i l l 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements. The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constr"
P97-1029,J88-1003,0,0.19806,"re are ambiguities of the sort typically found in languages like English (e.g., book/noun vs book/verb type). However, the agglutinative nature of the language usually helps resolution of such ambiguities due to the restrictions on morphotactics of subsequent morphemes. On the Introduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al., 1992; DeRose, 1988), constraint-based techniques (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and T i l l 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements. The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast vote"
P97-1029,J94-3001,0,0.313878,"lly are considering just the top level 226 inflectional information of the parses. This is very similar to Brill&apos;s use of contexts to induce transformation rules for his tagger (Brill, 1992; Brill, 1995), but instead of generating transformation rules from a training text, we gather statistics and apply them to parses in the text being disambiguated. 5 Efficient Implementation Techniques and Extensions The current implementation of the voting approach is meant to be a proof of concept implementation and is rather inefficient. However, the use of regular relations and finite state transducers (Kaplan and Kay, 1994) provide a very efficient implementation method. For this, we view the parses of the tokens making up a sentence as making up a acyclic a finite state recognizer with the states marking word boundaries and the ambiguous interpretations of the tokens as the state transitions between states, the rightmost node denoting the final state, as depicted in Figure 1 for a sentence with 5 tokens. In Figure 1, the transition labels are triples of the sort (wi, pj, O) for the jth parse of token i, with the 0 indicating the initial vote of the parse. The rules imposing constraints can also be represented a"
P97-1029,A94-1024,1,0.937004,"lly helps resolution of such ambiguities due to the restrictions on morphotactics of subsequent morphemes. On the Introduction Automatic morphological disambiguation is an important component in higher level analysis of natural language text corpora. There has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques, e.g., (Church, 1988; Cutting et al., 1992; DeRose, 1988), constraint-based techniques (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkil/i, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and KuruSz, 1994; Oflazer and T i l l 1996) and transformation-based techniques (Brilt, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requirements. The approach depends on assigning votes to constraints according to their complexity and specificity, and then letting constraints cast votes on matching parses of a given 1Voutilainen, Private communication. 222 other hand, this very nature introduces another kind of ambiguity, where a lexical form can be morphol"
P97-1029,W96-0207,1,0.855943,"Missing"
P97-1029,E95-1022,0,\N,Missing
P97-1029,E93-1069,0,\N,Missing
P97-1029,P98-2208,1,\N,Missing
P97-1029,C98-2203,1,\N,Missing
P98-2208,H92-1022,0,0.0550906,"Missing"
P98-2208,J95-4004,0,0.584383,"Missing"
P98-2208,A88-1019,0,0.679359,"are applied to a sentence, tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independent of the order of rule applications. Introduction Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information. Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g., Church (1988), Cutting et al. (1992), DeRose (1988)), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate 1277 W1 R1 W2 R3 W3 W4 R2 --&quot; Wn Rm Tokens voting Rules Figure 1: Voting Constraint Rules (can, HD) © (I.PRP) ~ ~ (cartoHD| (can, l,~) - _ (the,DT) Figure 2: Representing sentences with a directed acyclic graph 2 Tagging by Path Constraints Voting We assume t h a t sentences are delineated and that each token is assigned all possible tags by a lexicon or by a morphological analyzer. We represent each sentence as a standard chart"
P98-2208,A92-1018,0,0.0611882,"a sentence, tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independent of the order of rule applications. Introduction Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information. Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g., Church (1988), Cutting et al. (1992), DeRose (1988)), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate 1277 W1 R1 W2 R3 W3 W4 R2 --&quot; Wn Rm Tokens voting Rules Figure 1: Voting Constraint Rules (can, HD) © (I.PRP) ~ ~ (cartoHD| (can, l,~) - _ (the,DT) Figure 2: Representing sentences with a directed acyclic graph 2 Tagging by Path Constraints Voting We assume t h a t sentences are delineated and that each token is assigned all possible tags by a lexicon or by a morphological analyzer. We represent each sentence as a standard chart using a directed acycli"
P98-2208,J88-1003,0,0.115801,"disambiguated in parallel. Thus, the outcome of the rule applications is independent of the order of rule applications. Introduction Part-of-speech tagging is one of the preliminary steps in many natural language processing systems in which the proper part-of-speech tag of the tokens comprising the sentences are disambiguated using either statistical or symbolic local contextual information. Tagging systems have used either a statistical approach where a large corpora is employed to train a probabilistic model which then is used to tag unseen text, (e.g., Church (1988), Cutting et al. (1992), DeRose (1988)), or a constraint-based approach which employs a large number of hand-crafted linguistic constraints that are used to eliminate 1277 W1 R1 W2 R3 W3 W4 R2 --&quot; Wn Rm Tokens voting Rules Figure 1: Voting Constraint Rules (can, HD) © (I.PRP) ~ ~ (cartoHD| (can, l,~) - _ (the,DT) Figure 2: Representing sentences with a directed acyclic graph 2 Tagging by Path Constraints Voting We assume t h a t sentences are delineated and that each token is assigned all possible tags by a lexicon or by a morphological analyzer. We represent each sentence as a standard chart using a directed acyclic graph where n"
P98-2208,P97-1029,1,0.707921,"erage accuracy of g7.50~ on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision. 1 impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993). BriU (1992; 1994; 1995) has presented a transformationbased learning approach. This paper extends a novel approach to constraint-based tagging first applied for Turkish (Oflazer and Tiir, 1997), which relieves the rule developer from worrying about conflicting rule ordering requirements and constraints. The approach depends on assigning votes to constraints via statistical and/or manual means, and then letting constraints vote on matching sequences on tokens, as depicted in Figure 1. This approach does not reflect the outcome of matching constraints to the set of morphological parses immediately as usually done in constraint-based systems. Only after all applicable rules are applied to a sentence, tokens are disambiguated in parallel. Thus, the outcome of the rule applications is in"
P98-2208,E93-1046,0,0.0931471,"Street Journal Corpus indicate that with 400 statistically derived constraint rules and about 800 hand-crafted constraint rules, we can attain an average accuracy of 9Z89~ on the training corpus and an average accuracy of g7.50~ on the testing corpus. We can also relax the single tag per token limitation and allow ambiguous tagging which lets us trade recall and precision. 1 impossible sequences or morphological parses for a given word in a given context, recently most prominently exemplified by the Constraint Grammar work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993). BriU (1992; 1994; 1995) has presented a transformationbased learning approach. This paper extends a novel approach to constraint-based tagging first applied for Turkish (Oflazer and Tiir, 1997), which relieves the rule developer from worrying about conflicting rule ordering requirements and constraints. The approach depends on assigning votes to constraints via statistical and/or manual means, and then letting constraints vote on matching sequences on tokens, as depicted in Figure 1. This approach does not reflect the outcome of matching constraints to the set of morphological parses immedia"
P98-2208,J93-2006,0,0.0935429,"Missing"
P98-2208,E95-1022,0,\N,Missing
P98-2208,E93-1069,0,\N,Missing
P99-1033,A97-1012,0,0.0563904,"Missing"
P99-1033,C96-1058,0,0.0236386,"dicated to computational approaches to dependency grammars has been held at COLING/ACL'98 Conference). J~irvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and J~irvinen, 1997; J£rvinen and Tapanainen, 1998). The work of Sleator and Temperley(1991) on link grammar, an essentially lexicalized variant of dependency grammar, has also proved to be interesting in a number of aspects. Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing (Lafferty et al., 1992; Eisner, 1996; Chelba and et al., 1997). Robinson(1970) gives four axioms for well-formed dependency structures, which have been assumed in almost all computational approaches. In a dependency structure of a sentence (i) one and only one word is independent, i.e., not linked to some other word, (ii) all others depend directly on some word, (iii) no word depends on more than one other, and, (iv) if a word A depends directly on B, and some word C intervenes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. This last condition of projectivity (or various"
P99-1033,W98-0501,0,0.0733689,"Missing"
P99-1033,J94-3001,0,0.0139602,"h a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal constituents. This paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of Roche and Abney. The parser produces outputs that encode a labeled dependency tree representation of the syntactic relations between the words in the sentence. We assume that the reader is familiar with the basic concepts of finite state transducers (FST hereafter), finite state devices that map between two regular languages U and L (Kaplan and Kay, 1994). 254 2 Dependency Syntax Dependency approaches to syntactic representation use the notion of syntactic relation to associate surface lexical items. The book by Mel~uk (1988) presents a comprehensive exposition of dependency syntax. Computational approaches to dependency syntax have recently become quite popular (e.g., a workshop dedicated to computational approaches to dependency grammars has been held at COLING/ACL'98 Conference). J~irvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and J~irvinen, 1997; J£rvinen and Tapanainen, 1998"
P99-1033,C92-1027,0,0.0252317,"h as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), ATgzT Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer. Abney(1996) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols"
P99-1033,C90-2040,0,0.0358375,"representations such as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), ATgzT Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer. Abney(1996) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the inpu"
P99-1033,C96-2123,0,0.0575165,"Missing"
P99-1033,E93-1066,1,0.875606,"Missing"
P99-1033,A97-1011,0,0.145901,"Missing"
P99-1033,P97-1058,0,\N,Missing
P99-1033,E99-1035,0,\N,Missing
P99-1033,P96-1025,0,\N,Missing
P99-1033,P98-1101,0,\N,Missing
P99-1033,C98-1098,0,\N,Missing
P99-1033,W98-1302,0,\N,Missing
P99-1033,P99-1065,0,\N,Missing
P99-1033,J00-1003,0,\N,Missing
P99-1033,W89-0229,0,\N,Missing
R13-1006,W11-1902,0,0.0823116,"Missing"
R13-1006,J13-4004,0,0.042183,"Missing"
R13-1006,I13-2002,1,0.643072,"09)) along with super-sense taggers Ciaramita and Altun (2006), to build a system combination that can hopefully do a better job than the baseline, at least on our intrinsic test sets. Compound Annotator identifies the phrasal verbs and the compound nouns in the text and adds additional annotation to words of a compound. In-text Question Answering Annotator assigns the questions to the related named entities, and ranks them. The questions are generated using Heilman’s question generator tool (Heilman and Smith, 2010). For more details on the use of UIMA and the server architecture, please see Azab et al. (2013). 5 Lexical simplification : Text simplification can be defined as any process that reduces the syntactic or lexical complexity of a text while attempting to preserve its meaning and information content. The aim of text simplification is to make text easier to comprehend for a human user, or process by a program (Siddharthan, 2004). Text simplification has been studied for both human text readers and programs that process text. We are specifically concerned with students who try to acquire English as a second language (Petersen, 2007). Approaches for this target audience use simplification tec"
R13-1006,W06-1670,0,0.0181843,"d documents (iii) handling user-interactions, and (iv) sending queries to the server. The presentation layer is designed to be light and fast, with all the heavy processing to be done on the server side. 1 http://nlp.stanford.edu/software/ corenlp.shtml 45 Word Sense Annotator currently assigns the most frequent WordNet senses to content words by filtering the senses by just using the POS tag. able to significantly exceed the most-frequent sense heuristic. Our current plan is to incorporate multiple word-sense disambiguators (e.g., Pedersen and Kolhatkar (2009)) along with super-sense taggers Ciaramita and Altun (2006), to build a system combination that can hopefully do a better job than the baseline, at least on our intrinsic test sets. Compound Annotator identifies the phrasal verbs and the compound nouns in the text and adds additional annotation to words of a compound. In-text Question Answering Annotator assigns the questions to the related named entities, and ranks them. The questions are generated using Heilman’s question generator tool (Heilman and Smith, 2010). For more details on the use of UIMA and the server architecture, please see Azab et al. (2013). 5 Lexical simplification : Text simplifica"
R13-1006,de-marneffe-etal-2006-generating,0,0.0220657,"Missing"
R13-1006,W12-2015,0,0.017691,"adability levels. REAP chooses documents that contain cer41 Proceedings of Recent Advances in Natural Language Processing, pages 41–48, Hissar, Bulgaria, 7-13 September 2013. presents the word meaning as the default response. tain target vocabulary words that a student needs to learn. It also presents the documents within a web browser-based application along with a dictionary to provide word meanings and a set of automatically generated set of closed questions as an exercise. Recently, Eom et al. (2012) presented a system that incorporates word sense disambiguation for vocabulary assistance. Maamouri et al. (2012) presents, ARET (Arabic Reading Enhancement Tool) that aids the readers of Arabic as a second language. It provides the user with the morphological analyses, the meanings of the words and a text-to-speech module to pronounce the word. ARET also has an assessment tool that asks the user several kinds of questions to evaluate reading comprehension. Our system currently targets English and offers a wider set of functionalities to users, in addition to a software architecture which can be extended very easily with more annotation components complying with UIMA interfaces. However, our system archi"
R13-1006,W12-2038,0,0.18689,"), aimed at selecting individualized practice reading documents from the web using lexical, syntactic and readability levels. REAP chooses documents that contain cer41 Proceedings of Recent Advances in Natural Language Processing, pages 41–48, Hissar, Bulgaria, 7-13 September 2013. presents the word meaning as the default response. tain target vocabulary words that a student needs to learn. It also presents the documents within a web browser-based application along with a dictionary to provide word meanings and a set of automatically generated set of closed questions as an exercise. Recently, Eom et al. (2012) presented a system that incorporates word sense disambiguation for vocabulary assistance. Maamouri et al. (2012) presents, ARET (Arabic Reading Enhancement Tool) that aids the readers of Arabic as a second language. It provides the user with the morphological analyses, the meanings of the words and a text-to-speech module to pronounce the word. ARET also has an assessment tool that asks the user several kinds of questions to evaluate reading comprehension. Our system currently targets English and offers a wider set of functionalities to users, in addition to a software architecture which can"
R13-1006,A97-1020,0,0.82085,"rmation Management Architecture) based server (Ferrucci and Lally, 2004). These annotated documents are then accessed via browser-based clients which essentially look like traditional e-book reading environments but with a much richer set of user accessible functionality. Thus our system can also be seen as a showcase application for demonstrating 2 Using NLP in Reading Aids Recently, Computer Assisted Language Learning (CALL) systems have started making use of advanced language technology to build intelligent systems to aid and assess reading comprehension. An early project, GLOSSER Project (Nerbonne et al., 1997) developed a system that aids readers of foreign language text, by providing access to a dictionary, exploiting morphological analysis and part-of-speech disambiguation. The FreeText Project (Hamel and Girard, 2000), developed a NLP-based CALL system for intermediate to advanced learners of French. The LISTEN project at CMU on the other hand, has aimed to tutor elementary school students in reading English text by using speech technology (Mostow and Aist, 2001). The REAP (Reader Specific Lexical Practice) project (Heilman et al., 2006), aimed at selecting individualized practice reading docume"
R13-1006,N09-5005,0,0.032081,"ser status and the opened documents, (ii) displaying the opened documents (iii) handling user-interactions, and (iv) sending queries to the server. The presentation layer is designed to be light and fast, with all the heavy processing to be done on the server side. 1 http://nlp.stanford.edu/software/ corenlp.shtml 45 Word Sense Annotator currently assigns the most frequent WordNet senses to content words by filtering the senses by just using the POS tag. able to significantly exceed the most-frequent sense heuristic. Our current plan is to incorporate multiple word-sense disambiguators (e.g., Pedersen and Kolhatkar (2009)) along with super-sense taggers Ciaramita and Altun (2006), to build a system combination that can hopefully do a better job than the baseline, at least on our intrinsic test sets. Compound Annotator identifies the phrasal verbs and the compound nouns in the text and adds additional annotation to words of a compound. In-text Question Answering Annotator assigns the questions to the related named entities, and ranks them. The questions are generated using Heilman’s question generator tool (Heilman and Smith, 2010). For more details on the use of UIMA and the server architecture, please see Aza"
R13-1006,radev-etal-2004-mead,0,0.0460008,"Missing"
R13-1006,D10-1048,0,0.0454088,"Missing"
salama-etal-2014-youdacc,N12-1006,0,\N,Missing
salama-etal-2014-youdacc,P11-2007,0,\N,Missing
salama-etal-2014-youdacc,P13-2001,0,\N,Missing
salama-etal-2014-youdacc,P11-1122,0,\N,Missing
salama-etal-2014-youdacc,N13-1036,0,\N,Missing
salama-etal-2014-youdacc,N13-1044,0,\N,Missing
salama-etal-2014-youdacc,C12-1114,0,\N,Missing
tantug-etal-2008-bleu,E06-1032,0,\N,Missing
tantug-etal-2008-bleu,W05-0909,0,\N,Missing
W04-0409,W02-2001,0,0.0440599,"Missing"
W04-0409,W03-1807,0,0.0433635,"Missing"
W06-3102,1992.tmi-1.8,0,0.147328,"Missing"
W06-3102,J93-2003,0,0.00547388,"itial Explorations in English to Turkish Statistical Machine Translation ˙ Ilknur Durgar El-Kahlout Faculty of Enginering and Natural Sciences Sabancı University Istanbul, 34956, Turkey Kemal Oflazer Faculty of Engineering and Natural Sciences Sabancı University Istanbul, 34956, Turkey ilknurdurgar@su.sabanciuniv.edu oflazer@sabanciuniv.edu Abstract 1 Introduction The availability of large amounts of so-called parallel texts has motivated the application of statistical techniques to the problem of machine translation starting with the seminal work at IBM in the early 90’s (Brown et al., 1992; Brown et al., 1993). Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input “signal” e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Oc"
W06-3102,P05-1033,0,0.0264573,"uous sequence of functional words and tags in English texts, and that some morphemes should be aligned differently depending on the other morphemes in their context, we attempted a morpheme grouping. For example the morpheme sequence +DHr +mA marks infinitive form of a causative verb which in Turkish inflects like a noun; or the lexical morpheme sequence +yAcAk +DHr usually maps to “it/he/she will”. To find such groups of morphemes and functional words, we applied a sequence of morpheme groupings by extracting frequently occuring n-grams of morphemes as follows (much like the grouping used by Chiang (2005): in a series of iterations, we obtained high-frequency bigrams from the morphemic representation of parallel texts, of either morphemes, or of previously such identified morpheme groups and neighboring morphemes until up to four morphemes or one root 3 morpheme could be combined. During this process we ignored those combinations that contain punctuation or a morpheme preceding a root word. A similar grouping was done on the English side grouping function words and morphemes before and after root words. 11 The aim of this process was two-fold: it let frequent morphemes to behave as a single to"
W06-3102,N03-1017,0,0.0279648,"hine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input “signal” e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translation scores for Finnish are the lowest on average, even with the large number of This paper presents s"
W06-3102,2005.mtsummit-papers.11,0,0.0807394,"tactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translation scores for Finnish are the lowest on average, even with the large number of This paper presents some very preliminary results for and problems in developing a statistical machine translation system from English to Turkish. Starting with a baseline word model trained from about 20K aligned sentences, we explore various ways of exploiting morphological structure to improve upon the baseline system. As Turkish is a language with complex agglutinative word structures, we experiment with morphologically segmented and disambiguated versions of the parallel te"
W06-3102,N04-4015,0,0.261193,"Missing"
W06-3102,W02-1018,0,0.0149394,"1992; Brown et al., 1993). Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input “signal” e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translation scores for Finnish are the lowest on average, even with t"
W06-3102,J04-2003,0,0.161512,"with the seminal work at IBM in the early 90’s (Brown et al., 1992; Brown et al., 1993). Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input “signal” e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translat"
W06-3102,J04-4002,0,0.00909292,"3). Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input “signal” e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translation scores for Finnish are the lowest on average, even with the large number of T"
W06-3102,P02-1040,0,0.0944425,"any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translation scores for Finnish are the lowest on average, even with the large number of This paper presents some very preliminary results for and problems in developing a statistical machine translation system from English to Turkish. Starting with a baseline word model trained from about 20K aligned sentences, we explore various ways of exploiting morphological structure to improve upon the ba"
W06-3102,P01-1067,0,0.0335269,"early 90’s (Brown et al., 1992; Brown et al., 1993). Statistical machine translation views the translation process as a noisy-channel signal recovery process in which one tries to recover the input “signal” e, from the observed output signal f.1 Early statistical machine translation systems used a purely word-based approach without taking into account any of the morphological or syntactic properties of the languages (Brown et al., 1993). Limitations of basic word-based models prompted researchers to exploit morphological and/or syntactic/phrasal structure (Niessen and Ney, (2004), Lee,(2004), Yamada and Knight (2001), Marcu and Wong (2002), Och and Ney (2004),Koehn et al. (2003), among others.) In the context of the agglutinative languages similar to Turkish (in at least morphological aspects) , there has been some recent work on translating from and to Finnish with the significant amount of data in the Europarl corpus. Although the BLEU (Papineni et al., 2002) score from Finnish to English is 21.8, the score in the reverse direction is reported as 13.0 which is one of the lowest scores in 11 European languages scores (Koehn, 2005). Also, reported from and to translation scores for Finnish are the lowest"
W07-0704,N04-4015,0,0.285293,"Missing"
W07-0704,P07-1017,0,0.113139,"Missing"
W07-0704,J04-2003,0,0.070923,"Missing"
W07-0704,P02-1040,0,0.0977201,"will be monitor7 +vvn in the framework6 of the association4 agreement5 . Note that when the morphemes/tags (starting with a +) are concatenated, we get the “word-based” version of the corpus, since surface words are directly recoverable from the concatenated representation. We use this word-based representation also for word-based language models used for rescoring. We employ the phrase-based SMT framework (Koehn et al., 2003), and use the Moses toolkit (Koehn et al., 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al., 2002), using a single reference translation. 5 The training set in the first row of 1 was limited to sentences on the Turkish side which had at most 90 tokens (roots and bound morphemes) in total in order to comply with requirements of the GIZA++ alignment tool. However when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction when morphemes/function word are removed. Moses Dec. Parms. Default dl = -1, -weight-d = 0.1 BLEU 16.29 20.16 BLEU-c 16.13 19.77 Table 2: BLEU results for baseline experiments. BLEU is for th"
W07-0704,popovic-ney-2004-towards,0,0.175722,"Missing"
W07-0704,N07-1064,0,0.0173136,"rtaklıˇgı olus¸turulacaktır. Lit.: as a key feature of this strategy an accession partnership based on earlier eu summit resolutions will be formed . along with the literal paraphrases of the translation and the reference versions. The first two are quite accurate and acceptable translations while the third clearly has missing and incorrect parts. Model Iteration We have also experimented with an iterative approach to use multiple models to see if further improvements are possible. This is akin to post-editing (though definitely not akin to the much more sophisticated approach in described in Simard et al. (2007)). We proceeded as follows: We used the selective segmentation based model above and decoded our English training data ET rain and English test data ET est to obtain T1T rain and T1T est re30 BLEU 24.61 24.77 25.08 Table 6: BLEU results for two model iterations spectively. We then trained the next model using T1T rain and TT rain , to build a model that hopefully will improve upon the output of the previous model, T1T est , to bring it closer to TT est . This model when applied to T1T rain and T1T est produce T2T rain and T2T est respectively. We have not included the content word corpus in th"
W07-0704,W05-0909,0,0.0198442,"29.86, [60.0/34.9/23.3/16.] and 30.48 [63.3/35.6/23.4/16.4] respectively. Obviously, these are upper-bound oracle scores, as subsequent candidate generation and lattice rescoring could make er14 This was suggested by one of the reviewers. It would however perhaps be much better if the decoder could be augmented with a filter that could be invoked at much earlier stages of sentence generation to check if certain generated segments violate hard-constraints (such as morphotactic constraints) regardless of what the statistics say. 15 31 Note that using stems and their synonyms as used in METEOR (Banerjee and Lavie, 2005) could also be considered for word similarity. Again using the BLEU+ tool and a slightly different formulation of token similarity in BLEU computation, we find that using morphological similarity our best score above, 25.08 BLEU increases to 25.14 BLEU, while using only root word synonymy and very close hypernymy from Wordnet, gives us 25.45 BLEU. The combination of rules and Wordnet match gives 25.46 BLEU. Note that these increases are much less than what can (potentially) be gained from solving the word-repair problem above. 7 Conclusions We have presented results from our investigation into"
W07-0704,corston-oliver-gamon-2004-normalizing,0,0.0695561,"Missing"
W07-0704,W06-3102,1,0.839684,"d morphology and rather fixed SVO constituent order, Turkish is an agglutinative language with a very rich and productive derivational and inflectional morphology, and a very flexible (but SOV dominant) constituent order. Another issue of practical significance is the lack of large scale parallel text resources, with no substantial improvement expected in the near future. In this paper, we investigate different representational granularities for sub-lexical representation of parallel data for English-to-Turkish phrase-based Turkish and SMT Our previous experience with SMT into Turkish (Durgar El-Kahlout and Oflazer, 2006) hinted that exploiting sub-lexical structure would be a fruitful avenue to pursue. This was based on the observation that a Turkish word would have to align with a complete phrase on the English side, and that sometimes these phrases on the English side could be discontinuous. Figure 1 shows a pair of English and Turkish sentences that are aligned at the word (top) and morpheme (bottom) levels. At the morpheme level, we have split the Turkish words into their lexical morphemes while English words with overt morphemes have been stemmed, and such morphemes have been marked with a tag. The produ"
W07-0704,H05-1085,0,0.196047,"Missing"
W07-0704,N03-1017,0,0.00409785,"ma] [ortaklık2 +sh +nhn] [uygula3 +hn +ma +sh] [,] [ortaklık4 ] [anlas ¸ma5 +sh] [c ¸erc ¸eve6 +sh +nda] [izle7 +hn +yacak +dhr] [.] • E: the implementation3 of the accession1 partnership2 will be monitor7 +vvn in the framework6 of the association4 agreement5 . Note that when the morphemes/tags (starting with a +) are concatenated, we get the “word-based” version of the corpus, since surface words are directly recoverable from the concatenated representation. We use this word-based representation also for word-based language models used for rescoring. We employ the phrase-based SMT framework (Koehn et al., 2003), and use the Moses toolkit (Koehn et al., 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al., 2002), using a single reference translation. 5 The training set in the first row of 1 was limited to sentences on the Turkish side which had at most 90 tokens (roots and bound morphemes) in total in order to comply with requirements of the GIZA++ alignment tool. However when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction"
W07-0704,2005.mtsummit-papers.11,0,0.00603093,"Missing"
W07-0704,P06-1122,0,0.0220938,"+ndA vs. masa+sH+ndA. We should however note that although employing a morpheme based representations dramatically reduces the vocabulary size on the Turkish side, it also runs the risk of overloading distortion mechanisms to account for both word-internal morpheme sequencing and sentence level word ordering. The segmentation of a word in general is not unique. We first generate a representation that contains both the lexical segments and the morphological features encoded for all possible segmenta1 This is in a sense very similar to the more general problem of lexical redundancy addressed by Talbot and Osborne (2006) but our approach does not require the more sophisticated solution there. 26 tions and interpretations of the word. For the word emeli for instance, our morphological analyzer generates the following with lexical morphemes bracketed with (..): (em)em+Verb+Pos(+yAlH)ˆDB+Adverb+Since since (someone) sucked (something) (emel)emel+Noun+A3sg(+sH)+P3sg+Nom his/her ambition (emel)emel+Noun+A3sg+Pnon(+yH)+Acc ambition (as object of a transitive verb) These analyses are then disambiguated with a statistical disambiguator (Y¨uret and T¨ure, 2006) which operates on the morphological features.2 Finally, t"
W07-0704,E06-1006,0,0.0602295,"Missing"
W07-0704,N06-1042,0,0.046162,"Missing"
W07-0704,W06-3110,0,0.012613,"90 pairs of sentences (about 20 times the data that we have!) gives a BLEU score of 13.00. However, in this study, nothing specific for Finnish was employed, and one can certainly employ techniques similar to presented here to improve upon this. 6.1 rors, but nevertheless they are very close to the root word BLEU scores above. Another path to pursue in repairing words is to identify morphologically correct words which are either OOVs in the language model or for which the language model has low confidence. One can perhaps identify these using posterior probabilities (e.g., using techniques in Zens and Ney (2006)) and generate additional morphologically valid words that are “close” and construct a lattice that can be rescored. 6.2 Some Thoughts on BLEU BLEU is particularly harsh for Turkish and the morpheme based-approach, because of the all-or-none nature of token comparison, as discussed above. There are also cases where words with different morphemes have very close morphosemantics, convey the relevant meaning and are almost interchangeable: • gel+hyor (geliyor - he is coming) vs. gel+makta (gelmekte - he is (in a state of) coming) are essentially the same. On a scale of 0 to 1, one could rate thes"
W07-0704,N06-2051,0,0.0238368,"Missing"
W07-0704,tantug-etal-2008-bleu,1,\N,Missing
W07-0704,W07-0728,0,\N,Missing
W07-0704,P07-2045,0,\N,Missing
W13-2262,D07-1031,0,0.0342055,"e and f . In other words, the sum of the posterior probabilities in each column of the small trellis is the same. Therefore, we collect word translation counts only from the last morphemes of the words in e. Variational Bayes In order to prevent overfitting, we use the Variational Bayes extension of the EM algorithm (Beal, 2003). This amounts to a small change to the M step of the original EM algorithm. We introduce Dirichlet priors α to perform an inexact normalization by applying the function f (v) = exp(ψ(v)) to the expected counts collected in the E step, where ψ is the digamma function (Johnson, 2007). The M-step update for a multinomial parameter θx|y becomes: θx|y = 499 f (E[c(x|y)] + α) P f ( j E[c(xj |y)] + α) TAM-HMM Multi-rate WordMorph HMM Morph only BLEU WORD IBM 4 Baseline TR to EN 30.82 29.48 29.98 29.13 27.91 EN to TR 23.09 22.55 22.54 21.95 21.82 0.254 0.255 0.256 0.375 0.370 AER Table 1: AER and BLEU Scores We set α to 10−20 , a very low value, to have the effect of anti-smoothing, as low values of α cause the algorithm to favor words which co-occur frequently and to penalize words that co-occur rarely. We used Dirichlet priors on morpheme translation probabilities. 4 4.1 of T"
W13-2262,N03-1017,0,0.00817684,"ly marked by an overt morpheme. For English, we use partof-speech tagged data. The number of English words is 1,033,726 and the size of the English vocabulary is 28,647. The number of Turkish words is 812,374, the size of the Turkish vocabulary is 57,249. The number of Turkish morphemes is 1,484,673 and the size of the morpheme vocabulary is 16,713. 4.2 We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different backgr"
W13-2262,P07-2045,0,0.0088783,"number of English words is 1,033,726 and the size of the English vocabulary is 28,647. The number of Turkish words is 812,374, the size of the Turkish vocabulary is 57,249. The number of Turkish morphemes is 1,484,673 and the size of the morpheme vocabulary is 16,713. 4.2 We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant"
W13-2262,W04-3250,0,0.00986126,"ish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant. The best scores in the leftmost column were obtained from multi-rate HMMs with Dirichlet priors only during the TAM 1 training. On the contrary, the best scores for TAM-HMM and the baseline-HMM were obtained with Dirichlet priors both during the TAM 1 and the TAM-HMM Experiments We initialized our implementation of the single level ‘word-only’ model, which we call ‘baseline’ in Table 1,"
W13-2262,N04-4015,0,0.0753661,"Missing"
W13-2262,J93-2003,0,0.0535186,"Missing"
W13-2262,D09-1075,1,0.832868,"Missing"
W13-2262,C00-2162,0,0.105832,"Missing"
W13-2262,E03-1004,0,0.0408877,"Missing"
W13-2262,J03-1002,0,0.0724201,"MM differs from multi-rate HMM only by the lack of morpheme-level sequence modeling, and has complexity O(m2 n3 ). For the HMM to work correctly, we must handle jumping to and jumping from null positions. We learn the probabilities of jumping to a null position from the data. To compute the transition probability from a null position, we keep track of the nearest previous source word (or morpheme) that does not align to null, and use the position of the previous non-null word to calculate the jump width. In order to keep track of the previous nonnull word, we insert a null word between words (Och and Ney, 2003). Similarly, we insert a null morpheme after every non-null morpheme. M(p, q) = Class(fpq ) Fourth, as the arrow from faw (0) to fam (0,0) in Figure 4 shows, there is a conditional dependence on the word class that the morpheme is in: W(r) = Class(fr ) Putting together these components, the morpheme transitions are formulated as follows: p(am (j, k) = (r, s) |am (prev(j, k)) = (p, q)) ∝  p J (p, q, r, s)|M(p, q), W(r) δ(p, q, r, s) (2) The block diagonal matrix Ab consists of morpheme transition probabilities. 3.1.2 Word transitions In the multi-rate HMM, word transition probabilities have tw"
W13-2262,P02-1040,0,0.0868218,"urkish vocabulary is 57,249. The number of Turkish morphemes is 1,484,673 and the size of the morpheme vocabulary is 16,713. 4.2 We evaluated the performance of our model in two different ways. First, we evaluated against gold word alignments for 75 Turkish-English sentences. Table 1 shows the AER (Och and Ney, 2003) of the word alignments; we report the growdiag-final (Koehn et al., 2003) of the Viterbi alignments. Second, we used the Moses toolkit (Koehn et al., 2007) to train machine translation systems from the Viterbi alignments of our various models, and evaluated the results with BLEU (Papineni et al., 2002). In order to reduce the effect of nondeterminism, we run Moses three times per experiment setting, and report the highest BLEU scores obtained. Since the BLEU scores we obtained are close, we did a significance test on the scores (Koehn, 2004). In Table 1, the colors partition the table into equivalence classes: If two scores within the same row have different background colors, then the difference between their scores is statistically significant. The best scores in the leftmost column were obtained from multi-rate HMMs with Dirichlet priors only during the TAM 1 training. On the contrary, t"
W13-2262,C96-2141,0,0.371781,"orithm. TAMs can align rarely occurring words through their frequently occurring morphemes. In other words, they use morpheme probabilities to smooth rare word probabilities. Eyig¨oz et al. (2013) introduced TAM 1, which is analogous to IBM Model 1, in that the first level is a bag of words in a pair of sentences, and the second level is a bag of morphemes. By introducing distortion probabilities at the word level, Eyig¨oz et al. (2013) defined the HMM extension of TAM 1, the TAM-HMM. TAM-HMM was shown to be superior to its single-level counterpart, i.e., the HMM-based word alignment model of Vogel et al. (1996). The alignment example in Figure 1 shows a Turkish word aligned to an English phrase. The morphemes of the Turkish word are aligned to the English words. As the example shows, morphologically rich languages exhibit complex reordering phenomena at the morpheme level, which is left unutilized in TAM-HMMs. In this paper, we add morpheme sequence modeling to TAMs to capture morpheme level distortions. The example also shows that the Turkish morpheme orWe apply multi-rate HMMs, a tree structured HMM model, to the word-alignment problem. Multi-rate HMMs allow us to model reordering at both the morp"
W13-2262,P10-1047,1,0.869521,"Missing"
W13-2262,J08-4010,1,\N,Missing
W13-2262,J08-3003,1,\N,Missing
W13-2262,H05-1085,0,\N,Missing
W13-2262,N13-1004,1,\N,Missing
W14-3618,D12-1052,0,0.0398906,"ata and selected the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012a) built specialized decoders for English grammatical error correction. More recently, (Pasha et al., 2014) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system especially with Hamza spelling correction. In contrast to the approaches described above, we use a machine translation (MT) based method to train an error correction system. To the best of our knowledge, this is the first error correction system for Arabic using an MT approach. 3 omission errors, which makes a good base for other"
W14-3618,P10-4002,0,0.0297961,"Missing"
W14-3618,P08-2015,0,0.0602574,"ts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Proceedings of t"
W14-3618,I08-2131,0,0.42987,"curring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Pr"
W14-3618,P13-2121,0,0.0348173,"Missing"
W14-3618,P07-2045,0,0.00793191,"correct the spelling errors and 99K tokens were inserted (mostly punctuation marks). Furthermore, there is a total of 6,7K non necessary tokens deleted and 10.6K attached tokens split and 18.2 tokens merged. Finally, there are only 427 tokens moved in the sentence and 1563 multiple correction action. We experiment with different configurations and reach the sweet spot of performance when combining the different modules. Table 2: Clitics handled by the rule-based module. instance of that character (e.g. !!!!!!! would be replaced with !). Statistical Phrase-based Model We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above. We treat this last step as a translation problem, where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for te"
W14-3618,C12-2011,0,0.527137,"Missing"
W14-3618,W14-3605,1,0.794478,"Missing"
W14-3618,J03-1002,0,0.0057042,"where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 4.1 Results To evaluate the performance of"
W14-3618,J96-1003,1,0.552787,"evelop and evaluate spelling correction systems for Arabic trained either on naturally occurring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and"
W14-3618,P02-1040,0,0.10657,"de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 4.1 Results To evaluate the performance of our system on the development data, we compare its output to the reference (gold annotation). We then compute the usual measures of precision, recall and f-measure. R"
W14-3618,pasha-etal-2014-madamira,0,0.169771,"Missing"
W14-3618,N12-1067,0,\N,Missing
W14-3618,shaalan-etal-2012-arabic,0,\N,Missing
W14-3618,zaghouani-etal-2014-large,1,\N,Missing
W14-3627,P06-1086,1,0.865853,"ia industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect. DA is now emerging as the language of informal communication online. DA differs phonologically, lexically, morphologically, and syntactically from MSA. And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script. Thus, MSA tools cannot effectively model DA; for instance, over one-third of Levantine verbs cannot be analyzed using an MSA morphological analyzer (Habash and Rambow, 2006). These differences make the direct use of MSA NLP tools and applications for handling dialects impractical. In this paper, we present a statistical machine translation system for English to Dialectal Arabic (DA), using Modern Standard Arabic (MSA) as a pivot. We create a core system to translate from English to MSA using a large bilingual parallel corpus. Then, we design two separate pathways for translation from MSA into DA: a two-step domain and dialect adaptation system and a one-step simultaneous domain and dialect adaptation system. Both variants of the adaptation systems are trained on"
W14-3627,W12-2301,1,0.940968,"for Machine Translation into Egyptian Arabic Serena Jeblee1 , Weston Feely1 , Houda Bouamor2 Alon Lavie1 , Nizar Habash3 and Kemal Oflazer2 1 Carnegie Mellon University {sjeblee, wfeely, alavie}@cs.cmu.edu 2 Carnegie Mellon University in Qatar hbouamor@qatar.cmu.edu, ko@cs.cmu.edu 3 New York University Abu Dhabi nizar.habash@nyu.edu Abstract chine translation (Zbib et al., 2012; Salloum and Habash, 2013; Salloum et al., 2014; Al-Mannai et al., 2014) and in terms of data collection (Cotterell and Callison-Burch, 2014; Bouamor et al., 2014; Salama et al., 2014) and basic enabling technologies (Habash et al., 2012; Pasha et al., 2014). However, the focus is on a small number of iconic dialects, (e.g., Egyptian). The Egyptian media industry has traditionally played a dominant role in the Arab world, making the Egyptian dialect the most widely understood and used dialect. DA is now emerging as the language of informal communication online. DA differs phonologically, lexically, morphologically, and syntactically from MSA. And while MSA has an established standard orthography, the dialects do not: people write words reflecting their phonology and sometimes use roman script. Thus, MSA tools cannot effective"
W14-3627,N13-1044,1,0.929805,"ey may not always be able to pinpoint exact linguistic differences. In the context of natural language processing (NLP), some Arabic dialects have started receiving increasing attention, particularly in the context of maEgyptian Arabic is much closer to MSA than it is to English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to tran"
W14-3627,W14-3628,0,0.190206,"Missing"
W14-3627,bouamor-etal-2014-multidialectal,1,0.847169,"r experimental setup and the results obtained. Then, we give an analysis of our system output in Section 7. Finally, we conclude and describe our future work in Section 8. 2 Related work Machine translation (MT) for dialectal Arabic (DA) is quite challenging given the limited resources to build rule-based models or train statistical models for MT. While there has been a considerable amount of work in the context of standard Arabic NLP (Habash, 2010), DA is impoverished in terms of available tools and resources compared to MSA, e.g., there are few parallel DAEnglish corpora (Zbib et al., 2012; Bouamor et al., 2014). The majority of DA resources are for speech recognition, although more and more resources for machine translation and enabling tech3 Using Phrase-Based MT as an Adaptation System For commercial use, MT output is usually postedited by a human translator in order to fix the errors generated by the MT system. This is often faster and cheaper than having a human translate 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and mak"
W14-3627,W14-5311,0,0.0124183,"y translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that differences in genre betwe"
W14-3627,I13-1048,0,0.0168271,"ent research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that di"
W14-3627,P13-2121,0,0.0484788,"Missing"
W14-3627,P11-2031,1,0.921571,"m, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from n"
W14-3627,W11-2123,0,0.0346794,"e one-step adaptation system, where no reordering produced the best result. We also tested two different heuristics for symmetrizing the word alignments: grow-diag and grow-diag-final-and (Och and Ney, 2003). We found that using grow-diag as our symmetrization heuristic produced slightly better scores on the 100k datasets. For the baseline and adaptation systems we built 5-gram language models with KenLM (Heafield et al., 2013) using the target side of the training set, and for the core system we used the large MSA language model described in section 4. We use KenLM because it has been shown (Heafield, 2011) to be faster and use less memory than SRILM (Stolcke, 2002) and IRSTLM (Federico et al., 2008). below each table. The difference in scores between the different reordering window sizes (7, 4, and 0) we tried for the adaptation systems was not large (between 0 and 0.7 BLEU). In the following tables we present the best results for each adaptation system, which is a reordering window size of 7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4),"
W14-3627,cotterell-callison-burch-2014-multi,0,0.147926,"Missing"
W14-3627,2007.mtsummit-papers.34,0,0.0414651,"te 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and make it more closely resemble the references. Simard et al. (2007) used a phrase-based MT system as an automatic posteditor for the output of a commercial rule-based MT system, showing that it produced better results than both the rule-based system alone and a single pass phrase-based MT system. This technique is also useful for adapting to a specific domain or dataset. Isabelle et al. (2007) used a statistical MT system to automatically post-edit the output of a generic rule-based MT system, to avoid manually customizing a system dictionary and to reduce the amount of manual post-editing required. For our adaptation systems, we build a core phrase-based MT system with a large amount of out-of-domain data, which allows us to have better coverage of the target language. For an adaptation system, we then build a second phrase-based MT system by translating the in-domain train, tune, and test sets through the core translation system, then using that data to build the second system. T"
W14-3627,P07-2045,0,0.00633831,"stem below. 1 198 http://arz.wikipedia.org/ System Design Baseline MT System 100K sent. English Translation Egyptian Arabic One-Step Adaptation MT System 5M sent. English Translation 100K sent. Domain & MSA Dialect Adaptation Egyptian Arabic Two-Step Adaptation MT System English 5M sent. 100K sent. Translation Domain Adaptation MSA 100K sent. In-domain MSA Dialect Adaptation Egyptian Arabic Figure 1: An overview of the different system architectures. Baseline System Two-Step Adaptation System Our baseline system is a single phrase-based English to Egyptian Arabic MT system, built using Moses (Koehn et al., 2007) on the 100k corpus described in Section 4. This system does not include any MSA data, nor does it have an adaptation system; it is a typical, one-pass MT system that translates English directly into Egyptian Arabic. We will show that using adaptation systems improves the results significantly. We also build a two-step adaptation system that consists of two adaptation steps: one to adapt the MSA output of the core system to the domain of the MSA in the 100k corpus, and a second system to translate the MSA output of the domain adaptation system into Egyptian Arabic. We use the first adaptation"
W14-3627,W11-2107,1,0.838244,"he phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.2 B"
W14-3627,P12-2035,1,0.927262,"ools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel"
W14-3627,P05-1071,1,0.686452,"and also to adapt to the Egyptian dialect. What we refer to as the “one-step” system is a core system plus one adaptation system, whereas the “two-step” system consists of the core plus two subsequent adaptation systems. We describe the systems in more detail in Section 5. sentences from NIST MT09 (NIST Multimodal Information Group, 2010b). We use a 5-gram MSA language model built using the SRILM toolkit (Stolcke, 2002) on 260 million words of MSA from the Arabic Gigaword (Parker et al., 2011). All our MSA parallel data and monolingual MSA language modeling data were tokenized with MADA v3.1 (Habash and Rambow, 2005) using the ATB (Arabic Treebank) tokenization scheme. For the adaptation systems, we build a 100k tri-parallel corpus Egyptian-MSA-English corpus. The MSA and English parts are extracted from the NIST corpus distributed by the Linguistic Data Consortium. The Egyptian sentences are obtained automatically by extending Mohamed et al. (2012) method for generating Egyptian Arabic from morphologically disambiguated MSA sentences. This rule-based method relies on 103 transformation rules covering essentially nouns, verbs and pronouns as well as certain lexical items. For each MSA sentence, this metho"
W14-3627,N07-1064,0,0.0331669,"ch recognition, although more and more resources for machine translation and enabling tech3 Using Phrase-Based MT as an Adaptation System For commercial use, MT output is usually postedited by a human translator in order to fix the errors generated by the MT system. This is often faster and cheaper than having a human translate 197 the document from scratch. However, we can apply statistical phrase-based MT to create an automatic machine post-editor (what we refer to in this paper as an adaptation system) to improve the output of an MT system, and make it more closely resemble the references. Simard et al. (2007) used a phrase-based MT system as an automatic posteditor for the output of a commercial rule-based MT system, showing that it produced better results than both the rule-based system alone and a single pass phrase-based MT system. This technique is also useful for adapting to a specific domain or dataset. Isabelle et al. (2007) used a statistical MT system to automatically post-edit the output of a generic rule-based MT system, to avoid manually customizing a system dictionary and to reduce the amount of manual post-editing required. For our adaptation systems, we build a core phrase-based MT"
W14-3627,J03-1002,0,0.00421305,"nts Since MSA and Egyptian are more similar to each other than they are to English, we tried several different reordering window sizes to find the optimal reordering distance for adapting MSA to Egyptian Arabic, including the typical reordering window of length 7, a smaller window of length 4, and no reordering at all. We found a reordering window 199 size of 7 to work best for all our systems, except for the one-step adaptation system, where no reordering produced the best result. We also tested two different heuristics for symmetrizing the word alignments: grow-diag and grow-diag-final-and (Och and Ney, 2003). We found that using grow-diag as our symmetrization heuristic produced slightly better scores on the 100k datasets. For the baseline and adaptation systems we built 5-gram language models with KenLM (Heafield et al., 2013) using the target side of the training set, and for the core system we used the large MSA language model described in section 4. We use KenLM because it has been shown (Heafield, 2011) to be faster and use less memory than SRILM (Stolcke, 2002) and IRSTLM (Federico et al., 2008). below each table. The difference in scores between the different reordering window sizes (7, 4,"
W14-3627,P02-1040,0,0.10335,"7 for all systems, except for the phrase-based onestep domain and dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is gener"
W14-3627,2006.amta-papers.25,0,0.0165285,"d dialect adaptation system, which performs better with no reordering (0.2 BLEU better than a window of 7, 0.6 BLEU better than a window of 4), but these small differences in BLEU scores are within noise. The greatest difference in scores from the reordering windows was in the two-step systems domain adaptation step (MSA to MSA) on top of the phrase-based core, where a reordering window of 7 was 0.7 BLEU better than a window of 0. 6 6.1 Evaluation and Results For evaluation we use multeval (Clark et al., 2011) to calculate BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), TER (Snover et al., 2006), and length of the test set for each system. We evaluate the core and adaptation systems on the MSA and Egyptian sides of the test set drawn from the 100k corpus, which we refer to as the 100k sets. The data used for evaluation is a genuine Egyptian Arabic generated from MSA, just like the data the systems were trained on. It is not practical to evaluate on naturallygenerated Egyptian Arabic in this case because the domain of our datasets is very formal, since most of the text comes from news sources, and dialectal Arabic is generally used in informal situations.2 Below we report BLEU scores"
W14-3627,pasha-etal-2014-madamira,1,0.871023,"Missing"
W14-3627,P11-2007,0,0.118831,"thod relies on 103 transformation rules covering essentially nouns, verbs and pronouns as well as certain lexical items. For each MSA sentence, this method provides more than one possible candidate, in its original version, the Egyptian sentence kept was chosen randomly. We extend the selection algorithm by scoring the different sentences using a language model. For this, we use SRILM with modified Kneser-Ney smoothing to build a 5-gram language model. The model is trained on a corpus including articles extracted from the Egyptian version of Wikipedia1 and the Egyptian side of the AOC corpus (Zaidan and Callison-Burch, 2011). We chose to include Egyptian Wikipedia for the formal level of sentences in it different from the regular DA written in blogs or microblogging websites (e.g., Twitter) and closer to the ones generated by our system. We split this data into train, tune, and test sets of 98,027, 960, and 961 sentences respectively, after removing duplicates across sets. The MSA corpus was tokenized using MADA and the Egyptian Arabic data was tokenized with MADA-ARZ v0.4 (Habash et al., 2013), both using the ATB tokenization scheme, with alif/ya normalization. 4 5 Data For the core English to MSA system, we use"
W14-3627,P13-2001,0,0.101077,"ble for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sen"
W14-3627,N12-1006,0,0.393994,"ts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation between Tunisian Arabic and MSA. These works are limited to a dictionary or rules that are not available for all dialects. Zbib et al. (2012) used crowdsourcing to translate sentences from Egyptian and Levantine into English, and thus built two bilingual corpora. The dialectal sentences were selected from a large corpus of Arabic web text. Then, they explored several methods for dialect/English MT. Their best Egyptian/English system was trained on dialect/English parallel data. They argued that differences in genre between MSA and DA make bridging through MSA of limited value. For this reason, while pivoting through MSA, it is important to consider the domain and add an additional step: domain adaptation. The majority of previous e"
W14-3627,salama-etal-2014-youdacc,1,0.710505,"Missing"
W14-3627,N13-1036,1,0.844451,"English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowledge about the relation betwe"
W14-3627,P14-2125,1,0.860146,"Missing"
W14-3627,2010.amta-papers.5,0,0.287107,"than it is to English, so one can get a system bet196 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 196–206, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics nologies such as morphological analyzers are becoming available for specific dialects (Habash et al., 2012; Habash et al., 2013). For Arabic and its dialects, several researchers have explored the idea of exploiting existing MSA rich resources to build tools for DA NLP. Different research work successfully translated DA to MSA as a bridge to translate to English (Sawaf, 2010; Salloum and Habash, 2013), or to enhance the performance of Arabic-based information retrieval systems (Shatnawi et al., 2012). Among the efforts on translation from DA to MSA, Abo Bakr et al. (2008) introduced a hybrid approach to transfer a sentence from Egyptian Arabic to MSA. Sajjad et al. (2013) used a dictionary of Egyptian/MSA words to transform Egyptian to MSA and showed improvement in the quality of machine translation. A similar but rule-based work was done by Mohamed et al. (2012). Boujelbane et al. (2013) and Hamdi et al. (2014) built a bilingual dictionary using explicit knowled"
W15-1614,abuhakema-etal-2008-annotating,0,0.311605,"access to detailed error statistics. This can provide learners with a very useful feedback and help them improve their proficiency level. 129 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 129–139, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics These errors may take place in words, phrases, language structures, and the ways words or expressions are used (Granger, 2003). For Arabic, there are few projects that aim at developing Arabic learner corpora and annotating them but most of them are not freely available for users or researchers (Abuhakema et al., 2008; Hassan and Daud, 2011). In this paper, we present our annotation method and our efforts for extending an L1 large scale Arabic language corpus and its manually edited corrections to include annotated non-native Arabic learner text (L2). This work is part of the Qatar Arabic Language Bank (QALB) project (Zaghouani et al., 2014b), a large-scale error annotation effort that aims to create a manually corrected corpus of errors for a variety of Arabic texts (the target size is 2 million words).1 Our overarching goal is to use our annotated corpus to develop components for automatic detection and"
W15-1614,W13-1703,0,0.0240313,"ar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K"
W15-1614,dickinson-ledbetter-2012-annotating,0,0.136697,"ines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annotated a small corpus of 9K words of Arabic written materials produced by native speakers of English in the US who learned Ara"
W15-1614,N13-1066,1,0.853751,"ly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing vario"
W15-1614,I08-1059,0,0.0165036,"such as POS, lemma, gender, number or person. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refer"
W15-1614,habash-etal-2012-conventional,1,0.127188,"text uses one of multiple widely acceptable transliterations, the annotators should not modify the word. Punctuation Errors: Punctuation errors should be corrected according to the commonly accepted Arabic punctuation rules. 132 Dialectal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic"
W15-1614,N13-1044,1,0.82105,"tal Usage Errors: In comparison to Standard Arabic, where there are clear spelling standards and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently oc"
W15-1614,W10-1802,0,0.0226722,"non-native speakers of other languages such as English (Leacock et al., 2010; Rozovskaya and Roth, 2010). Lexical Correction: Finally, if it is impossible to fully correct the word using the previous four steps, there is a clear case of word choice errors and the annotator may have to replace the word used. This can be employed to especially correct inadequate lexical choices or unknown words. In the example given in 5 3. Correct derivation errors; but keep root intact. 4 The minimum edits approach in error correction have already been used in the Error-tagged Learner Corpus of Czech project (Hana et al., 2010) 133 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym ˇ ¯  ˆ ð', yˆ Zø', ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 6 A clitic is a linguistic unit that is pronounced and written like an affix but is grammatically independent. Inflection Error Correction Original ˇ knt qd bdÂnA fy AlςAm AlmADy rHl¯h Alaý mk¯h. Correction ˇ knt qd bdÂt fy AlςAm AlmADy rHl¯h Alaý mk¯h. English Original Correction English Original Correction English Original Correction English  úÍ@ éÊgP  ú"
W15-1614,W14-3603,1,0.49852,"and conventions, Arabic dialects do not have official orthographic standards partly since they were not commonly written until recently. Today, Arabic dialects are often seen in social media, but also in published novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotat"
W15-1614,W14-3605,1,0.412475,"us. The results obtained in the evaluation suggest that the annotators produced consistently similar results under the proposed guidelines. We believe that publishing this corpus will give researchers a common development and test set for developing related natural language processing applications. A subset of our L2 corpus will be used as part of the Second QALB Shared Task on Automatic Arabic Error Correction in conjunction with the ACL-2015 Workshop on Arabic NLP.9 This shared task follows the success of the First QALB Shared Task held in conjunction with EMNLP-2014 Workshop on Arabic NLP (Mohit et al., 2014). In the future, we will extend our annotation guidelines to address machine translation output correction (i.e., manual post-editing). We also plan to extend our systems for automatic correction of Arabic language errors (Jeblee et al., 2014; Rozovskaya et al., 2014) to handle L2 data, using the corpus discussed here for training and test purposes. 9 http://www.arabic-nlp.net/wanlp 137 Acknowledgements We thank anonymous reviewers for their valuable comments and suggestions. We also thank all our dedicated annotators: Noor Alzeer, Hoda Fathy, Hoda Ibrahim, Anissa Jrad, Samah Lakhal, Jihene Wa"
W15-1614,P06-1031,0,0.0280632,"erson. The robust design of MADAMIRA allows it to consider different possible spellings of words, especially relating to Ya/Alif-Maqsura, Ha/Ta-Marbuta and Hamzated Alif forms, which are very common error sources. MADAMIRA selects the correct form in context, thus correcting for these errors which are often connected to lemma choice or morphology. 7 7.1 Evaluation Inter-Annotator Agreement Our annotation effort consists of a single annotation pass as commonly done in many annotation projects due to time and budget constraints (Rozovskaya and Roth, 2010; Gamon et al., 2008; Izumi et al., 2004; Nagata et al., 2006). In order to evaluate the quality of our correction annotations, we frequently measure the inter-annotator agreement (IAA) to ensure that the annotators are following the guidelines provided consistently. A high level of agreement between the annotators indicates that the annotation is reliable and the guidelines are useful in producing homogeneous and consistent data. We measure the IAA by averaging WER (Word Error Rate) over all pairs of annotations to compute the AWER (Average 135 Word Error Rate).7 For the purpose of this evaluation, the WER refers to an annotation error and it is measure"
W15-1614,I13-2001,1,0.525343,"eyeglasses to read the book.’   @Q¯ @ ú Ë è @QÖÏ @ © A  P A¢ JË@ © A ú Ë H@ Table 1: Examples of the different parts of the correction priority order   ¯ h ‘mirror’ was replaced Table 1, the word è @QÖÏ @ AlmrA¯  P A¢ JË@ AlnDArAt ˇ by the word H@ ‘eyeglasses’. alignments starting from document tokenization to after human annotation. 5 6 The Annotation Tool In order to ensure the speed and efficiency of the annotation process, as well as better management, we provide the annotators with a web-based annotation framework, originally developed to manually correct errors in L1 texts (Obeid et al., 2013). The annotation interface allows annotators to perform different actions corresponding to the following types of corrections: (a) edit misspelled words; (b) move words that are not in the right location; (c) add missing words; (d) delete extraneous words; (e) merge words that have been split erroneously; and (f) split words that have been merged erroneously. In our final corpus output format, we record for each annotated file the list of actions taken by the annotator. These actions operate on one or two tokens depending on the action. We also supply token 134 The Annotation Pipeline The anno"
W15-1614,P02-1040,0,0.105018,"ear that ALC is less challenging than ALWC as shown in the IAA of the first round and second rounds. Overall, the high-level of agreement obtained in the second round shows that the annotators produced consistently similar results under the proposed guidelines; and their differences are all within acceptable variation. This of course makes the evaluation of automatic correction harder.8 7 The annotation manager is excluded from this evaluation. This problem might be solved by considering multiple references in the evaluation process similarly to what is done in machine translation evaluation (Papineni et al., 2002). Unfortu8 Original  ®Ö Ï @ úæîDKA ÐA« ú¯ éËA à@ ø ñK@ B@ . ZAKCJË@ ÉJ.¯ àA Anwy An sAnthy AlmqAl¯h fy ςAm AlAnsAn qbl AlθlAθA’. ‘I plan I will be-done the article in the year of humanity before Tuesday.’ Annotator 1 @ ÐA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the year of humanity before Tuesday.’ Annotator 2 @ ÕËA« á«  ®Ö Ï @ úæîE @ à @ ø ñK @ B éËA . ZAKCJË@ ÉJ.¯ àA ˇ Ânwy Ân Ânhy AlmqAl¯h ςn ςAlm AlAnsAn qbl AlθlAθA’. ‘I plan to finish-off the article about the"
W15-1614,pasha-etal-2014-madamira,1,0.792787,"Missing"
W15-1614,W10-1004,1,0.231979,"able at http://reports-archive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. A"
W15-1614,W14-3622,1,0.888722,"Missing"
W15-1614,N13-1036,1,0.851271,"shed novels (and there is even an Egyptian Arabic Wikipedia). Habash et al. (2012) proposed a Conventional Orthography for Dialectal Arabic (or CODA) targeting Egyptian Arabic for computational modeling purposes and demonstrated how to map to it in (Eskander et al., 2013) and (Pasha et al., 2014; Habash et al., 2013). CODAs for other dialects have also been proposed (Zribi et al., 2014; Jarrar et al., 2014). In our current annotation task we neither address dialectal Arabic spelling normalization (Eskander et al., 2013), nor do we systematically translate dialectal words into Standard Arabic (Salloum and Habash, 2013). We recognize that the Arabic language is in a diglossic situation and borrowing is frequent. Most of the texts provided for annotation are in Standard Arabic, but dialectal words are sometimes mistakenly used. We are interested in reducing various spelling inconsistencies that frequently occur. So, as was done in the L1 annotation effort (Zaghouani et al., 2014b), we asked annotators to flag the highly dialectal cases to be reviewed later by the annotation manager. The guidelines classify dialectal word issues into five categories inspired by Habash et al. (2008): dialectal lexical choice, p"
W15-1614,W08-1205,0,0.00798721,"etailed description of ALC is given at: http://www.arabiclearnercorpus.com/ 131 and structures, with some items overused and others significantly underused. They also contain varying degrees of grammatical, orthographic and lexical errors. Moreover, sentences written by Arabic L2 speaker have often a different structure and are not as fluent as sentences produced by a native speaker even when no clear mistakes can be found. Therefore, the correction task is complicated by the fact that the acceptability level of a given sentence differs widely within the native speaker annotators as stated by Tetreault and Chodorow (2008). These issues can be related to linguistic factors such as inter-language (L1 interference), the student’s teaching and learning methodology, and to the translation effect (conscious interference). Thus, correcting the Arabic L2 essays can be a very challenging task that requires a lot of interpretation efforts by the annotators. This will likely lead to lower inter-annotator agreement as there is often many possible ways to correct the L2 errors. In order to annotate the L2 corpus, we use our annotation guidelines designed for L1 (Zaghouani et al., 2014b) and add specific L2 annotation rules"
W15-1614,P11-1019,0,0.0242372,"ive.adm.cs.cmu.edu/ anon/qatar/CMU-CS-QTR-124.pdf 2 130 Section 2; then we describe the corpus and the annotation guidelines in Sections 3 and 4. Afterwards, we present our annotation tool and pipeline in Sections 5 and 6. Finally, we present an evaluation of the annotation quality and discuss the L2 annotation challenges in Section 7. 2 Related Work Currently available manually corrected learner corpora are generally limited when it comes to the language, size and the genre of data. Several corpora of learners of English annotated for errors are publicly available (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), ranging in size between 60K words and more than one million words. Dickinson and Ledbetter (2012) annotated errors in student essays written by learners of Hungarian at three proficiency levels at Indiana University. The annotation was performed using EXMARaLDA, a freely available tool that allows multiple and concurrent annotations (Schmidt, 2010). Student errors were marked according to various categories of phonological, spelling, agreement and derivation errors. For Arabic, very few learner corpora annotation project have been built. Abuhakema et al. (2008) annot"
W15-1614,zaghouani-etal-2014-large,1,0.588046,"Missing"
W15-1614,zribi-etal-2014-conventional,1,0.33632,"Missing"
W15-1614,W14-3618,1,\N,Missing
W15-3209,2007.mtsummit-papers.20,1,0.567467,"stances of a word type are observed in a corpus, and (2) ambiguity where a word has multiple readings or interpretations. Undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology. The lack of diacritics usually leads to considerable lexical ambiguity, as shown in the example in Table 1, a reason for which diacritization, aka vowel/diacritic restoration, has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications. In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available diacritized MSA corpora are generally limited to the newswire genres (as distributed by the LDC) or religion related texts such as the Quran or the Tashkeela corpus.2 In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from fiv"
W15-3209,P06-1073,0,0.583986,"lts show that readers benefited from the disambiguating diacritics. This study was a MIN scheme exploration focused on heterophonic-homographic target verbs that have different pronunciations in active and Related Work The task of diacritization is about adding diacritics to the canonical underspecified written form. This task has been discussed in several research works in various NLP areas addressing various applications. Automatic Arabic Diacritization Much work has been done on recovery of diacritics over the past two decades by developing automatic methods yielding acceptable accuracies. Zitouni et al. (2006) built a diacritization framework based on such as number, gender, aspect, voice, etc. Whereas a lemma is a conventionalized citation form. 82 ATB News ATB BN ATB WebLog Tashkeela Wikipedia Total Size in words 2,478 3,093 3,177 5,172 2,850 16,770 GOLD annotation Yes Yes Yes Yes No - Table 2: The size of the data for annotation per corpus genre passive. classical Arabic books). This corpus contains over 6 million words fully diacritized. For our study we include a subset of 5k words from this corpus. In this work we are interested in two components: annotating large amounts of varied genres typ"
W15-3209,P05-1071,0,0.0659168,"n between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More"
W15-3209,N07-2014,0,0.648924,"acritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a"
W15-3209,maamouri-etal-2008-enhancing,0,0.396831,"very text genre, two annotators were asked to annotate independently a sample of 100 words. We measured the IAA between two annotators by averaging WER (Word Error Rate) over all pairs of words. The higher the WER between two annotations, the lower their agreement. The results given in Table 5, show clearly that the Advanced mode is the best strategy to adopt for this diacritization task. It is the less confusing method on all text genres (with WER between 1.56 and 5.58). We note that Wiki annotations in Advanced mode garner the highest IAA with a very low WER. We extended the LDC guidelines (Maamouri et al., 2008) by adding some diacritization rules: The shadda mark should not be added to the definite article (e.g., àñÒJ ÊË@/’lemon’ and not àñÒJ ÊË@); The sukuun sign should not be indicated at the end /’from’); The letters folof silent words (e.g., áÓ lowed by a long Alif, should not be diacritized  as it is a deterministic diacritization ( Y« @ñ ®Ë@/’the rules’); Abbreviations are not diacritized ( Õ»/’km’, /’kg’). Ñª» We also added an appendix that sumWe measure the reliability of the annotations by comparing them against gold standard annotations. In order to build the gold Wiki annotations, we hi"
W15-3209,pasha-etal-2014-madamira,1,0.916558,"Missing"
W15-3209,P08-2030,1,0.710351,"II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More recently, Abandah et"
W15-3209,W04-1612,0,0.67965,"kAtib/’writer’ and I.KA¿/kAtab/’to correspond’ distinguishes between the meanings of the word (lexical disambiguation) rather than their inflections. Any of diacritics may be used to mark lexical variation. A common example with the shadda (gemination) diacritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorize"
W15-3217,W08-0509,0,0.0366323,"Missing"
W15-3217,I08-2131,0,0.527985,"uction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB: We trained and evaluated our system using the data provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012). These datasets are extracted from the QALB corpus of human-edited Arabic text produced by native speakers, non-native speakers and machines (Zaghouani et al., 2014). The corpus contains a large 144 Proceed"
W15-3217,E09-2008,0,0.0206763,"r for a regular set over a, b described by the regular expression (aba + bab)*, and we want to recognize the inputs that are slightly corrupted, for example, abaaaba may be matched to abaaba (correcting for a spurious a), or babbb may be matched to babbab (correcting for a deletion), or ababba may be matched to either abaaba (correcting a b to an a) or to ababab (correcting the reversal of the last two symbols). This method is perfect for handling mainly transposition errors resulting from swapping two letters , or typing errors of neighboring letters in the keyboard. We use the Foma library (Hulden, 2009) to build the finite-state tranducer using the Arabic Word-list as a dictionary.4 For each word, our system checks if the word is analyzed and recognized by the finite-state transducer. It then generates a list of correction candidates for the nonrecognized ones. The candidates are words having an edit distance lower than a certain threshold. We score the different candidates using a LM and consider the best one as the possible correction for each word. 4 ous system configurations on the L2 dev and test 2014 sets are given in Table 3. The results clearly show different modules are complementry"
W15-3217,C12-2011,0,0.494671,"Missing"
W15-3217,W14-3618,1,0.86265,"Missing"
W15-3217,N12-1067,0,0.163355,"Missing"
W15-3217,W14-3620,0,0.536639,"Missing"
W15-3217,P07-2045,0,0.00732275,"Missing"
W15-3217,W14-3309,1,0.929547,"f the statistics reported in Table 1 is taken from Diab et al. (2014) 2 The list is freely available at: http: //sourceforge.net/projects/ arabic-wordlist/ 145 Original Target English Characters à@ ñë ... à @ ñë ‘... Source Source Target H. ñKñJ Ë @ H. ñKñJ Ë@ ú ¯ ú ¯ éKYëA éKYëA ø YË @ ø YË@ which I have seen in Youtube is that è H X è @  # ø X È @ ... # à @# ð è# H. ð H ð ø È @# ø ¬#    ... # à @# ð è# H. ð H ð ø È @# ø ¬# è H X è @ # ø X È @ Table 2: Preparing the training and tuning and test corpus for alignment 3.2 Rule-based Corrector (Rules) verse (Sajjad et al., 2013a; Durrani et al., 2014a). The conversion of Arabic dialects to MSA at character-level can be seen as a spelling correction task where small character-level changes are made to convert a dialectal word into an MSA word. We also formulate our correction problem as a character-level machine translation problem, where the pre-processed incorrect Arabic text is considered as the source, and our target is the correct Arabic text provided by the Shared task organizers. The goal is to learn correspondences between errors and their corrections. All the train data is used to train our the phrase-based model. We treat sentenc"
W15-3217,W14-3605,0,0.0929078,"Missing"
W15-3217,E14-4029,1,0.927876,"f the statistics reported in Table 1 is taken from Diab et al. (2014) 2 The list is freely available at: http: //sourceforge.net/projects/ arabic-wordlist/ 145 Original Target English Characters à@ ñë ... à @ ñë ‘... Source Source Target H. ñKñJ Ë @ H. ñKñJ Ë@ ú ¯ ú ¯ éKYëA éKYëA ø YË @ ø YË@ which I have seen in Youtube is that è H X è @  # ø X È @ ... # à @# ð è# H. ð H ð ø È @# ø ¬#    ... # à @# ð è# H. ð H ð ø È @# ø ¬# è H X è @ # ø X È @ Table 2: Preparing the training and tuning and test corpus for alignment 3.2 Rule-based Corrector (Rules) verse (Sajjad et al., 2013a; Durrani et al., 2014a). The conversion of Arabic dialects to MSA at character-level can be seen as a spelling correction task where small character-level changes are made to convert a dialectal word into an MSA word. We also formulate our correction problem as a character-level machine translation problem, where the pre-processed incorrect Arabic text is considered as the source, and our target is the correct Arabic text provided by the Shared task organizers. The goal is to learn correspondences between errors and their corrections. All the train data is used to train our the phrase-based model. We treat sentenc"
W15-3217,J03-1002,0,0.00661281,"Missing"
W15-3217,J96-1003,1,0.68369,"and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask. 1 Introduction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB: We trained and evaluated our system using the data provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012). These dataset"
W15-3217,pasha-etal-2014-madamira,0,0.131692,"Missing"
W15-3217,W13-3602,0,0.0694364,"We trained and tested our spelling corrector using the dataset provided by the shared task organizers. Our system outperforms the baseline system and yeilds better correction quality with an F-score of 68.12 on L1test-2015 testset and 38.90 on the L2-test2015. This ranks us 2nd in the L2 subtask and 5th in the L1 subtask. 1 Introduction With the increased usage of computers in the processing of various languages comes the need for correcting errors introduced at different stages. Hence, the topic of text correction has seen a lot of interest in the past several years (Haddad and Yaseen, 2007; Rozovskaya et al., 2013). Numerous approaches have been explored to correct spelling errors in texts using NLP tools and resources (Kukich, 1992; Oflazer, 1996). The spelling correction for Arabic is an understudied problem in comparison to English, although small amount of research has been done previously (Shaalan et al., 2003; Hassan et al., 2008). The reason for this is the complexity of Arabic language and unavailability of language resources. For example, the Arabic spell checker in Microsoft Word gives incorrect suggests for even simple errors. First shared task on automatic Arabic text 2 Data Resources QALB:"
W15-3217,W14-3622,0,0.351338,"Missing"
W15-3217,P13-2001,1,0.907081,"Missing"
W15-3217,2013.iwslt-evaluation.8,1,0.897486,"Missing"
W15-3217,zaghouani-etal-2014-large,1,\N,Missing
W15-3217,W11-2123,0,\N,Missing
W16-4115,J11-4004,0,0.0319415,"ding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition to the quality of manual annotation and also allow the identification of many factors causing lower inter-annotator agreements. For example, Bayerl and Paul (2011) showed that there is a correlation between the inter-annotator agreement and the complexity of the annotation task; for instance, the larger the number of categories is, the lower the inter-annotator agreement is. Moreover, the categories prone to confusions are generally limited. This brings out two complexity issues related to the number of categories and to the existence of ambiguity between some the categories as explained in (Popescu-Belis, 2007). Furthermore, there are some annotation tasks for which the choice of a label is entirely left to the annotator, which can lead to even more co"
W16-4115,D15-1274,0,0.0235161,"ng ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The o"
W16-4115,W15-3209,1,0.748269,"5; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The overreaching goal of our project is to manually create a large-scale annotated corpus with the diacritics for a variety of Arabic texts. The creation of manually annotated corpora presents many challenges and issues related to the linguistic complexity of the Arabic language. In order to streamline the annotation process, we designed various annotation experimental conditions in order to answer the following questions: Can we automatically detect linguistic difficulties such as linguistic ambiguity? To what extent is there agreement between machines and human annotators when it comes to"
W16-4115,2007.mtsummit-papers.20,1,0.914733,"owels and diacritics rendering a mostly consonantal orthography (Schulz, 2004). Arabic diacritization is an orthographic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents"
W16-4115,palmer-etal-2008-pilot,1,0.782763,"raining of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2."
W16-4115,2012.eamt-1.18,0,0.135091,"e analyses contain more than one possibility, the word is marked as ambiguous; otherwise, it is believed to be not ambiguous. Words that have no analysis generated using MADAMIRA are also considered ambiguous. For each sentence, we count the number of words that are marked as ambiguous using our approach, and then calculate the percentage of ambiguity. We sort the sentences according to their ambiguity percentages in descending order so that we give annotators ranked sentences for annotation. Because we are concerned with MSA dataset only, we further filter out dialectal sentences using AIDA (Elfardy and Diab, 2012), a tool that classifies words and sentences as MSA (formal Arabic) or DA (Dialectal Arabic). 5 Evaluation For the evaluation, we used a sample of 10K-Words from the CCA corpus representing 4 domains with approximately 2.5K-words per domain (children stories, economics,sports and politics). We have three experimental conditions for three evaluations carried over a period of six weeks. 1. The first condition (COND1): In the first experimental condition (COND1), four annotators were given raw undiacritized sentences and were asked to add the missing diacritics as per the guidelines. They either"
W16-4115,W07-1522,0,0.0406205,"l reasons that may cause disagreement in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally as"
W16-4115,W11-0405,0,0.0272132,"in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous ca"
W16-4115,maamouri-etal-2010-speech,1,0.827854,"ral ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition"
W16-4115,W12-2015,1,0.900794,"Missing"
W16-4115,W14-3605,1,0.871251,"Missing"
W16-4115,P06-1031,0,0.094128,"Missing"
W16-4115,I13-2001,1,0.828094,"or: the Shadda gemination mark, the Soukoun (absence of a vowel) and the Nunation marks at the end of a word. Moreover, in some cases, the letters followed by a long Alif  letter @, should not be diacritized as it is considered a deterministic diacritization as in AJJ Ó /miyvAqu/  ’Treaty’ and not AJJ Ó /miyvaAqu/.2 A summary of the most common Arabic diacritization rules is also added as a reference in the guidelines. 3.2 Annotation Tool We designed and implemented MANDIAC, a web-based annotation tool and a work-flow management interface (Obeid et al., 2016), the tool is based on QAWI (Obeid et al., 2013) a token-based editor, used to annotate and correct spelling errors in Arabic text for the Qatar Arabic Language Bank (QALB) project.3 The basic interface of the annotation tool is shown in Figure 1, apart from the surface controls, the interface allows annotators to select from an automatically generated diacritized words list and/or edit words manually as shown. The annotation interface allows users to undo/redo actions, and the history is kept over multiple sessions. The interface includes a timer to keep track of how long each sentence annotation has taken. We used the timer feature to mea"
W16-4115,pasha-etal-2014-madamira,1,0.873559,"Missing"
W16-4115,W10-1004,0,0.0710577,"Missing"
W16-4115,W15-3204,1,0.861846,"Missing"
W16-4115,D15-1152,0,0.0130569,"abic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zagh"
W16-4115,W10-1836,1,0.835215,"tators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity"
W16-4115,W12-2511,1,0.86205,"t rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies th"
W16-4115,L16-1577,1,0.81745,"Missing"
W16-4115,L16-1295,1,0.886376,"Missing"
W16-4115,P06-1073,0,0.0303502,"hic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation"
W18-4702,P98-1013,0,0.594043,"rarily within the respective domains. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionable whether a collection of the existing heterogenous annotation schemes for closed-domain events adequately contribute to interoperable and consistent annotation of events across domains. On the other hand, prior work on open-domain events have some limitations with respect to coverage of events and their relations. Lexical databases such as WordNet (Miller et al., 1990), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) can be viewed as a superset of event lexicon, and their subtaxonomies seem to provide an extensional definition of events. However, these databases have a narrow coverage of events because they generally do not cover current terminology and proper nouns due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank cover the proper noun ‘Hurricane Katrina’. In addition, they do not provide any principles or guidelines about how to annotate events in text by themselves due to their different focus. TimeML (Pustejovsky et al., 2003) focus"
W18-4702,cybulska-vossen-2014-using,0,0.0184551,"-level questions for reading comprehension, such as the one that requires learners to infer answers over multiple sentences (Araki et al., 2016). Our contribution is twofold. First, we annotate a wide coverage of events, comprising verbs, nouns, adjectives, and phrases which are continuous or discontinuous (see Section 3). Despite this relatively wide and flexible annotation of events on text in 10 different domains, we show that our annotation achieved high inter-annotator agreement. Second, unlike previous methodologies which generally focus on deal only with event coreference such as ECB+ (Cybulska and Vossen, 2014), we present methodologies to annotate five event relations in unrestricted domains (see Section 4). 2 Data and Annotation Procedures In this section, we describe our data and annotation procedures. Our annotation target is not restricted in any specific domains. Thus, ideally speaking, our annotation should include all kinds of events in a domain-agnostic manner. However, annotating all kinds of events manually in unrestricted domains would be unrealistic due to annotation cost. Therefore, in order to make the corpus creation manageable while retaining the domain diversity, we select 100 arti"
W18-4702,doddington-etal-2004-automatic,0,0.156065,"nts have received relatively little attention in the literature. From the perspective of information extraction, much previous work on events pays attention to domainspecific clause-level argument structure (e.g., attackers kill victims, plaintiffs sue defendants, etc), putting less emphasis on what semantically constitutes events. The formalization focusing on domain-specific clause-level argument structure often involves its own definition of events based on instantiation of event ontology for a particular domain, aimed at automatic extraction of closed-domain events, as illustrated in ACE (Doddington et al., 2004), TAC KBP (Mitamura et al., 2017), PASBio (Wattarujeekrit et al., 2004), and BioNLP (Kim et al., 2009). For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. The closed-domain formalization might be of practical use in some domain-specific scenarios. However, it designs event definitions and annotation schemes arbitrarily within the respective domains. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionab"
W18-4702,W17-0812,0,0.0123804,"to be a cause-and-effect relation, in which we can explain the causation between two event nuggets X and Y, saying “X causes Y”. One example of causality is “The tsunami was caused by the earthquake.” Causality also adds another distinctive characteristic to annotation. Causality inherently entails an event sequence. For example, if we say “The tsunami was caused by the earthquake”, it means that the tsunami happened after the earthquake. To distinguish causality from event sequences and other relations such as preconditions (Palmer et al., 2016), we perform causality tests, largely based on (Dunietz et al., 2017): 1. The “why” test: After reading the sentence, can an annotator answer “why” questions about the potential effect argument? If not, it is not causal. 2. The temporal order test: Is the cause asserted to precede the effect? If not, it is not causal. 3. The counterfactuality test: Would the effect have been just as probable to occur or not occur had the cause not happened? If so, it is not causal. 4. The ontological asymmetry test: Could you just as easily claim the cause and effect are reversed? If so, it is not causal. 5. The linguistic test: Can the sentence be rephrased as It is because (o"
W18-4702,W13-1203,1,0.901977,"fer to the same event. For two event nuggets to corefer, they should be semantically identical, have the same participants (e.g., agent, patient) or attribute (e.g., location, time), and have the same polarity. For instance, ‘Great Fire of London’ and ‘fire’ are coreferential in (11). (11) The Great Fire of London happened in 1666. The fire lasted for three days. When considering event identity for event coreference, we use the notion of event hopper from Rich ERE (Song et al., 2015), which is a more inclusive, less strict notion than the event coreference defined in ACE. Subevent. Following (Hovy et al., 2013), we define subevent relations as follows. Event A is a subevent of event B if B represents a stereotypical sequence of events, or a script (Schank and Abelson, 1977), and A is a part of that script. For example, ‘affected’, ‘flooded’ and ‘broke’ are three subevents of ‘Hurricane Katrina’ in (12). We refer to ‘Hurricane Katrina’ as a parent (event) of the three subevents. (12) On August 29, 2005, New Orleans was affected by Hurricane Katrina which flooded most of the city when city levees broke. Causality. We define causality to be a cause-and-effect relation, in which we can explain the causa"
W18-4702,W09-1401,0,0.0521808,"on, much previous work on events pays attention to domainspecific clause-level argument structure (e.g., attackers kill victims, plaintiffs sue defendants, etc), putting less emphasis on what semantically constitutes events. The formalization focusing on domain-specific clause-level argument structure often involves its own definition of events based on instantiation of event ontology for a particular domain, aimed at automatic extraction of closed-domain events, as illustrated in ACE (Doddington et al., 2004), TAC KBP (Mitamura et al., 2017), PASBio (Wattarujeekrit et al., 2004), and BioNLP (Kim et al., 2009). For clarification, we use the term ‘domain’ to refer to a specific genre of text, such as biology, finance, and so forth. The closed-domain formalization might be of practical use in some domain-specific scenarios. However, it designs event definitions and annotation schemes arbitrarily within the respective domains. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionable whether a collection of the existing heterogenous annotation schemes for closed-domain events adequ"
W18-4702,H05-1004,0,0.0574364,"63 21 Dise 67 94 105 30 10 Eco 51 48 37 73 15 Edu 61 101 28 62 17 Geo 52 91 87 58 8 Hist 42 119 68 117 21 Poli 46 147 36 66 24 Tran 51 93 60 88 11 Total 512 946 586 643 146 Table 2: Statistics of event coreference clusters and cluster-level event relations in SW100. For brevity, we use a prefix with 3 or 4 characters to refer to each domain. 4.4 Inter-annotator Agreement on Annotation of Event Relations One way to compute inter-annotator agreement on event coreference is to use evaluation metrics developed by prior work, such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and BLANC (Recasens and Hovy, 2011). However, these metrics are appropriate specifically for (event) coreference and cannot be consistently applied to other event relations such as subevents. Instead, a single consistent metric is ideal for comparing inter-annotator agreement. Since we have three annotators, we use Fleiss’ Kappa (Fleiss, 1971) to compute inter-annotator agreement on the five event relations annotated by the three annotators. Specifically, we consider all pairwise relations between events and propagate event relations via event coreference, following (Mitamura et al., 2017)."
W18-4702,W15-0809,1,0.9275,"n (RED) (Palmer et al., 2016) defines events and their relations in a general manner, but its annotation was performed only in the clinical domain (O’Gorman et al., 2016). In this work, we present our methodologies for annotating events and relations between events in unrestricted domains. The goal of our annotation project is to provide human-annotated data to build a generation generation application to enhance reading comprehension for English as second language (ESL) students, as a continuous effort of (Araki et al., 2016). Using the notion of eventualities (Bach, 1986) and event nuggets (Mitamura et al., 2015), our event annotation scheme defines events and annotates event spans of text while not assigning any specific event types to them. In that sense, our event annotation is span-oriented, as compared to the traditional argument-oriented annotation of events. As for relations between events, we choose to annotate five relations from the perspective of the goal: event coreference, subevent, causality, event sequence (‘after’ relations), and simultaneity. To our knowledge, this is the first work that performs human annotation of events and the five relations in unrestricted domains. We believe tha"
W18-4702,J88-2003,0,0.829549,"annotation. 6. The adjudicator finalizes event relation annotation. 3 Annotation of Events This section describes our definition of events and principles for annotation of events. 3.1 Definition of Events: Eventualities As with TimeML (Pustejovsky et al., 2003) and ISO-TimeML (ISO, 2012), our definition of events uses eventualities (Bach, 1986), which are a broader notion of events, including states, processes, and events. This definition is inclusive in the sense that it includes states in addition to events and processes. We define the three classes on the basis of durativity and telicity (Moens and Steedman, 1988; Pulman, 1997): • states: notions that remain unchanged until their change or are brought as a result of an event, e.g., He owns a car. Tom was happy when he received a present; • processes: notions that involve a change of state without an explicit goal or completion, e.g., it was raining yesterday; • events4 : notions that involve a change of state with an explicit goal or completion, e.g., walked to Boston, buy a book. We recognize that annotating states is generally more difficult than annotating processes and actions because states are often confused with attributes which are not eventiv"
W18-4702,W16-5706,0,0.0416448,"Missing"
W18-4702,J05-1004,0,0.195177,"ns. For example, ACE considers resulting states (resultatives)1 as events, but others might exclude them or include a broader notion of states as events. Therefore, it is questionable whether a collection of the existing heterogenous annotation schemes for closed-domain events adequately contribute to interoperable and consistent annotation of events across domains. On the other hand, prior work on open-domain events have some limitations with respect to coverage of events and their relations. Lexical databases such as WordNet (Miller et al., 1990), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) can be viewed as a superset of event lexicon, and their subtaxonomies seem to provide an extensional definition of events. However, these databases have a narrow coverage of events because they generally do not cover current terminology and proper nouns due to their dictionary nature. For instance, none of WordNet, FrameNet and PropBank cover the proper noun ‘Hurricane Katrina’. In addition, they do not provide any principles or guidelines about how to annotate events in text by themselves due to their different focus. TimeML (Pustejovsky et al., 2003) focuses on temporal aspects of events an"
W18-4702,W15-0812,0,0.0173505,"nt sequence, and simultaneity. Event coreference. We define event coreference as a linguistic phenomenon that two event nuggets refer to the same event. For two event nuggets to corefer, they should be semantically identical, have the same participants (e.g., agent, patient) or attribute (e.g., location, time), and have the same polarity. For instance, ‘Great Fire of London’ and ‘fire’ are coreferential in (11). (11) The Great Fire of London happened in 1666. The fire lasted for three days. When considering event identity for event coreference, we use the notion of event hopper from Rich ERE (Song et al., 2015), which is a more inclusive, less strict notion than the event coreference defined in ACE. Subevent. Following (Hovy et al., 2013), we define subevent relations as follows. Event A is a subevent of event B if B represents a stereotypical sequence of events, or a script (Schank and Abelson, 1977), and A is a part of that script. For example, ‘affected’, ‘flooded’ and ‘broke’ are three subevents of ‘Hurricane Katrina’ in (12). We refer to ‘Hurricane Katrina’ as a parent (event) of the three subevents. (12) On August 29, 2005, New Orleans was affected by Hurricane Katrina which flooded most of th"
W18-4702,E12-2021,0,0.0984759,"Missing"
W18-4702,M95-1005,0,0.122093,"ations Arch 32 74 31 48 8 Chem 61 81 63 38 11 Disa 49 98 71 63 21 Dise 67 94 105 30 10 Eco 51 48 37 73 15 Edu 61 101 28 62 17 Geo 52 91 87 58 8 Hist 42 119 68 117 21 Poli 46 147 36 66 24 Tran 51 93 60 88 11 Total 512 946 586 643 146 Table 2: Statistics of event coreference clusters and cluster-level event relations in SW100. For brevity, we use a prefix with 3 or 4 characters to refer to each domain. 4.4 Inter-annotator Agreement on Annotation of Event Relations One way to compute inter-annotator agreement on event coreference is to use evaluation metrics developed by prior work, such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), and BLANC (Recasens and Hovy, 2011). However, these metrics are appropriate specifically for (event) coreference and cannot be consistently applied to other event relations such as subevents. Instead, a single consistent metric is ideal for comparing inter-annotator agreement. Since we have three annotators, we use Fleiss’ Kappa (Fleiss, 1971) to compute inter-annotator agreement on the five event relations annotated by the three annotators. Specifically, we consider all pairwise relations between events and propagate event relations via event"
W96-0207,H92-1022,0,0.0381266,"Missing"
W96-0207,A88-1019,0,0.289316,"Missing"
W96-0207,A92-1018,0,0.319889,"Missing"
W96-0207,J88-1003,0,0.364191,"Missing"
W96-0207,A94-1024,1,0.83069,"Missing"
W96-0207,J95-3004,0,0.142177,"Missing"
W96-0207,E93-1066,1,0.805019,"wum] [AGR 3SG] [POSS 2SG] [CASE NOM]] 1SG LOC [CAT VERB DIK 5. [[CAT NOUN] [ROOT talksho~u] [AGR 3SG] [POSS 1SG] [CASE GENII 6. [[CAT NOUN] [ROOT talkshow] [AGR 3SG] [POSS ISG] [CASE GEN]] which are then processed just like any other during disambiguation.S This however is not a sufficient solution for some very obscure situations where for the foreign word is written using its, say, English orthography, while suffixation goes on according to its English pronunciation, which may make some constraints like vowel 3.2 Unknown Words Although the coverage of our morphological analyzer for Turkish (Oflazer, 1993), with about 30,000 root words and about 35,000 proper names, is very satisfactory, it is inevitable that there will be forms in the corpora being processed that are not recognized by the morphological analyzer. These are almost always foreign proper names, words adapted into the language and not in the lexicon, or very obscure technical words. These are nevertheless inflected (using Turkish word formation paradigms) 8Incidentally, the correct analysis is the 6 th, meaning o.[ my talk show. The 5th one has the same morphological features except for the root. 73 harmony inapplicable on the grap"
W96-0207,E93-1046,0,0.238392,"r of studies in tagging and morphological disambiguation using various techniques. Part-of-speech tagging systems have used either a statistical approach where a large corpora has been used to train a probabilistic model which then has been used to tag new text, assigning the most likely tag for a given word in a given context (e.g., Church (1988), Cutting et al. (1992), DeRose (1988)). Another approach is the rule-based or constraint-based approach, recently most prominently exemplified by the Constraint G r a m m a r work (Karlsson et al., 1995; Voutilainen, 1995b; Voutilainen et al., 1992; Voutilainen and Tapanainen, 1993), where a large number of hand-crafted linguistic constraints are used to eliminate impossible tags or morphological parses for a given word in a given context. Brill (1992; 1994; 1995a) has presented a transformation-based learning approach, which induces rules from tagged corpora. Recently he has extended this work so that learning can proceed in an unsupervised manner using an untagged corpus (Brill, 1995b). Levinger et al. (1995) have recently reported on an approach that learns morpholexical probabilities from untagged corpus and have the used the resulting information in morphological di"
W96-0207,E93-1069,0,\N,Missing
W96-0207,J95-4004,0,\N,Missing
W96-0409,E95-1034,0,0.0306495,"Missing"
W96-0409,P92-1044,0,0.0726774,"Missing"
W96-0409,E93-1066,1,0.897355,"Missing"
W98-1309,J95-4004,0,0.0468722,"ive investigation. 4 Implementing Voting Constraints with Finite State Transducers The approach described above can also be implemented by finite state transducers. For this, we view the parses of the tokens making up a sentence as acyclic a finite state recognizer (or an identity transducer [4]). The states mark word boundaries, transitions are labeled with labels are of the sort L = (wi, t i j , vij), and the rightmost node denotes the finalstate. This approach is very different from that of Roche and Schabes [9] who use transducers to implement Brill's transformation-based tagging approach [1]. It shares certain concepts with Tz0ukermann and Radev's use of weighted finite state tra~nsdueers for tagging [13] in that both approaches combine statistical and hand-crafted linguistic information, but employ finite state devices in very different ways. The basic idea behind using finite state transducers is that the voting constraint rules can be represented as transducers which increment the votes of the matching input sequence segments Thus the ambiguities of the tokens were limited to the ones found in the training corpus. 94 Constraint Set Train. Set Test S e t Accuracy Accuracy 1 95."
W98-1309,A88-1019,0,0.171761,"es, the votes of all parses in that sequences are incremented by V. Once all constraints are applied to all possible sequences in all paths, we select the path(s) with the maximum total tallied vote for the parses on it. If there are multiple paths with the same maximum vote, the tokens whose parses are different in these paths are assumed to be left ambiguous. Given that in English each token has on the average about more than one tag, the procedurai description above is, in general, very inefficient. A quite efficient procedure for imphmenting this operation based on Church's windowing idea [2] has been described by Tiir and Oflazer [12]. Also, Oflaser and Tiir [8] presents an application of essentially the same approach (augmented with some additional statistical help) to morphological disambiguation of Turkish. 3 Preliminary Results from Tagging English We have experimented with this approach using the Wail Street Journal Corpus from the Penn 2~reebank CD. We used two classes of constraints: one class derived from the training corpus (a set of 5000 sentences (about 109,000 tokens in total) from the WSJ Corpus) and a second set of hand-crafted constraints mainly incorporating negat"
W98-1309,J94-3001,0,0.0844092,"ints were negative constraints (with large negative votes) to rule out certain tag sequences. Table 2 presents a set of tagging result from this experimentation. Although the results are quite preliminary, we feel that the results in the last row of Table 2 are quite satisfactory and warrant further extensive investigation. 4 Implementing Voting Constraints with Finite State Transducers The approach described above can also be implemented by finite state transducers. For this, we view the parses of the tokens making up a sentence as acyclic a finite state recognizer (or an identity transducer [4]). The states mark word boundaries, transitions are labeled with labels are of the sort L = (wi, t i j , vij), and the rightmost node denotes the finalstate. This approach is very different from that of Roche and Schabes [9] who use transducers to implement Brill's transformation-based tagging approach [1]. It shares certain concepts with Tz0ukermann and Radev's use of weighted finite state tra~nsdueers for tagging [13] in that both approaches combine statistical and hand-crafted linguistic information, but employ finite state devices in very different ways. The basic idea behind using finite"
W98-1309,P96-1015,0,0.0264241,"VOTES "")""2. [5 ""( .... ~ I "" WORD VOTES "") ....("" ""VSI"" WORD [ ,,(,, TAGSWORDVOTES "")"" ] [ ""{"" C ""("" TAGS WORD [ ADDIOO ] ""("" TAGS WORD [ &DDIOO ] II l,~. II l I II II II II II II e-> ""("" ... ""}"" ] (2) (a) I I VOTES "")"" .O. r II (I) *Oo I "")"" "")"" 3 2 ], .0. (4) ""(""-> D, "")""-> D}; This transducer is the composition of four transducers (separated by 'the composition operator . o.). The top transducer (1) constrains the input to valid triples, s The second transducer brackets with ( and ), any sequence of such triplets matching the given rule constraints, using the longest match bracket operator [5] .6 Thus any sequence of two triplets in the input sequence where the first has a tag MDand the second has a tag VB are bracketed by this transducer. The 4 Please note that this is a slightly different order than described earlier. In practice, this order was found to generate smaller transducers duz~ng compositions. 5 Here W0BDdenotes a regular expression wh/ch describes an arbitrary sequence of English characters. TAGSdenotes a regular expression which is the union of all (possibly mslti-chazacter) tag symbols. VOTESdenotes a regular expression of the sort ""<"" ['+"" I""-""3 DIGITS+ "">"" with DIG"
W98-1309,C90-2040,0,0.0706852,"y make up the the highest voted combination. The approach depends on assigning votes to constraints via statistical and/or manual means, and then letting constraint rules cast votes on matching parses of a given lexical item. This approach does not reflect the outcome of matching constraint rules to the set of morphological parses immediately. Only after all applicable rules are applied to a sentence, all tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independent of the order of rule applications. Constraint-based morphological disambiguation systems (e.g. [6, 7, 15]) typically look at a context of several sequential tokens each annotated with their possible morphological interpretations (or tags), and in a reductionistic way, remove parses that are considered to be impossible in the given context. Since constraint rule application is ordered, parses removed by one rule may not be used or referred to in subsequent rule applications. Addition of a new rule requires that its place in the sequence be carefully determined to avoid any undesirable interactions. Automata intersection based approaches run the risk of deleting all parses of a sentence, and have a"
W98-1309,A94-1024,1,0.89364,"y make up the the highest voted combination. The approach depends on assigning votes to constraints via statistical and/or manual means, and then letting constraint rules cast votes on matching parses of a given lexical item. This approach does not reflect the outcome of matching constraint rules to the set of morphological parses immediately. Only after all applicable rules are applied to a sentence, all tokens are disambiguated in parallel. Thus, the outcome of the rule applications is independent of the order of rule applications. Constraint-based morphological disambiguation systems (e.g. [6, 7, 15]) typically look at a context of several sequential tokens each annotated with their possible morphological interpretations (or tags), and in a reductionistic way, remove parses that are considered to be impossible in the given context. Since constraint rule application is ordered, parses removed by one rule may not be used or referred to in subsequent rule applications. Addition of a new rule requires that its place in the sequence be carefully determined to avoid any undesirable interactions. Automata intersection based approaches run the risk of deleting all parses of a sentence, and have a"
W98-1309,P97-1029,1,0.802658,"all constraints are applied to all possible sequences in all paths, we select the path(s) with the maximum total tallied vote for the parses on it. If there are multiple paths with the same maximum vote, the tokens whose parses are different in these paths are assumed to be left ambiguous. Given that in English each token has on the average about more than one tag, the procedurai description above is, in general, very inefficient. A quite efficient procedure for imphmenting this operation based on Church's windowing idea [2] has been described by Tiir and Oflazer [12]. Also, Oflaser and Tiir [8] presents an application of essentially the same approach (augmented with some additional statistical help) to morphological disambiguation of Turkish. 3 Preliminary Results from Tagging English We have experimented with this approach using the Wail Street Journal Corpus from the Penn 2~reebank CD. We used two classes of constraints: one class derived from the training corpus (a set of 5000 sentences (about 109,000 tokens in total) from the WSJ Corpus) and a second set of hand-crafted constraints mainly incorporating negative constraints (demoting impossible or unlikely situations) or lexicali"
W98-1309,J95-2004,0,0.0486924,"esults in the last row of Table 2 are quite satisfactory and warrant further extensive investigation. 4 Implementing Voting Constraints with Finite State Transducers The approach described above can also be implemented by finite state transducers. For this, we view the parses of the tokens making up a sentence as acyclic a finite state recognizer (or an identity transducer [4]). The states mark word boundaries, transitions are labeled with labels are of the sort L = (wi, t i j , vij), and the rightmost node denotes the finalstate. This approach is very different from that of Roche and Schabes [9] who use transducers to implement Brill's transformation-based tagging approach [1]. It shares certain concepts with Tz0ukermann and Radev's use of weighted finite state tra~nsdueers for tagging [13] in that both approaches combine statistical and hand-crafted linguistic information, but employ finite state devices in very different ways. The basic idea behind using finite state transducers is that the voting constraint rules can be represented as transducers which increment the votes of the matching input sequence segments Thus the ambiguities of the tokens were limited to the ones found in t"
W98-1309,P98-2208,1,0.839462,"es are incremented by V. Once all constraints are applied to all possible sequences in all paths, we select the path(s) with the maximum total tallied vote for the parses on it. If there are multiple paths with the same maximum vote, the tokens whose parses are different in these paths are assumed to be left ambiguous. Given that in English each token has on the average about more than one tag, the procedurai description above is, in general, very inefficient. A quite efficient procedure for imphmenting this operation based on Church's windowing idea [2] has been described by Tiir and Oflazer [12]. Also, Oflaser and Tiir [8] presents an application of essentially the same approach (augmented with some additional statistical help) to morphological disambiguation of Turkish. 3 Preliminary Results from Tagging English We have experimented with this approach using the Wail Street Journal Corpus from the Penn 2~reebank CD. We used two classes of constraints: one class derived from the training corpus (a set of 5000 sentences (about 109,000 tokens in total) from the WSJ Corpus) and a second set of hand-crafted constraints mainly incorporating negative constraints (demoting impossible or unli"
W98-1309,C98-2203,1,\N,Missing
W99-0703,J95-4004,0,0.07504,"Missing"
W99-0703,P84-1070,0,0.184046,"Missing"
W99-0703,J96-1003,1,0.854985,"Missing"
W99-0703,J94-3001,0,0.0205025,"Missing"
W99-0703,W98-1308,0,0.0212845,"Missing"
W99-0703,P97-1057,0,0.028827,"Missing"
W99-0703,C92-1025,0,0.0243393,"Missing"
W99-0703,A97-1016,0,0.140673,"Missing"
W99-0703,C94-1066,0,0.0321245,"Missing"
W99-0703,P98-2160,1,0.855506,"Missing"
W99-0703,C98-2155,1,\N,Missing
zaghouani-etal-2014-large,W11-2602,1,\N,Missing
zaghouani-etal-2014-large,W10-1004,1,\N,Missing
zaghouani-etal-2014-large,P05-1071,1,\N,Missing
zaghouani-etal-2014-large,P11-1019,0,\N,Missing
zaghouani-etal-2014-large,P08-2015,1,\N,Missing
zaghouani-etal-2014-large,N13-1036,1,\N,Missing
zaghouani-etal-2014-large,W13-1703,0,\N,Missing
zaghouani-etal-2014-large,shaalan-etal-2012-arabic,0,\N,Missing
zaghouani-etal-2014-large,pasha-etal-2014-madamira,1,\N,Missing
zaghouani-etal-2014-large,dickinson-ledbetter-2012-annotating,0,\N,Missing
zaghouani-etal-2014-large,I13-2001,1,\N,Missing
zaghouani-etal-2014-large,abuhakema-etal-2008-annotating,0,\N,Missing
zaghouani-etal-2014-large,N13-1066,1,\N,Missing
