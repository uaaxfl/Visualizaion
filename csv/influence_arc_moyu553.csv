2020.emnlp-main.624,N19-1358,0,0.38351,"their modeling of the action values is less accurate and less data-efficient. The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observation is often not a sufficient summary of the interaction history and may not provide enough information to determine the long-term effects of actions. Previous approaches address this problem by building a representation over past observations (e.g., building a graph of objects, positions, and spatial relations) (Ammanabrolu and Riedl, 2019; Ammanabrolu and Hausknecht, 2020). These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant. We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension (MPRC) and harness MPRC techniques to solve the huge action space and partial observability challenges. The graphical illustration is shown in Figure 2. First, the ac"
2020.emnlp-main.624,P17-1171,0,0.019741,"eading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with historical observations and predict actions from multiple observation paragraphs. A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The para"
2020.emnlp-main.624,W14-4012,0,0.0216932,"Missing"
2020.emnlp-main.624,P16-1153,0,0.476122,"hallenges on the NLU techniques. The first challenge is the difficulty of exploration in the huge natural language action space. To make RL agents learn efficiently without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others. To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently (Narasimhan et al., 2015; Hausknecht et al., 2019a); or embed each valid action as another vector and predict action value based on the vector-space similarities (He et al., 2016). These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient. The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observation is often not a sufficient summary of the interaction history and may not provide enough information to determine the long-term effects of actions. Prev"
2020.emnlp-main.624,P17-1147,0,0.156873,"ns between objects. KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space. Reading Comprehension Models for Question Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting. Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been 7757 r"
2020.emnlp-main.624,Q19-1026,0,0.0198992,"KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space. Reading Comprehension Models for Question Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting. Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been 7757 recently applied to enhance"
2020.emnlp-main.624,P18-1161,0,0.01666,"presentation with historical observations and predict actions from multiple observation paragraphs. A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal w"
2020.emnlp-main.624,N19-1423,0,0.0102308,"s extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting. Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been 7757 recently applied to enhance the encoding and attention mechanisms of the aforementioned reader models. They give performance boost but are more resource-demanding and do not suit the IF game playing task very well. Reading Comprehension over Multiple Paragraphs. Multi-paragraph reading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with histori"
2020.emnlp-main.624,D19-1284,0,0.0108733,"where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with historical observations and predict actions from multiple observation paragraphs. A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval"
2020.emnlp-main.624,P19-1259,0,0.0127385,"t paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal with evidence from Wikipedia, news collections, and, recently, books (Mou et al., 2020). We are the first to extend these ideas to IF games. 3 3.1 Multi-Paragraph RC for IF Games Problem Formulation Each"
2020.emnlp-main.624,2020.nuse-1.13,1,0.761209,"es from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal with evidence from Wikipedia, news collections, and, recently, books (Mou et al., 2020). We are the first to extend these ideas to IF games. 3 3.1 Multi-Paragraph RC for IF Games Problem Formulation Each IF game can be defined as a Partially Observable Markov Decision Process (POMDP), namely a 7-tuple of h S, A, T , O, ⌦, R, i, representing the hidden game state set, the action set, the state transition function, the set of textual observations composed from vocabulary words, the textual observation function, the reward function, and the Encoder Block + Arg0 GRU Embedding Arg1 GRU Embedding Forward Layer Q(o, a) Self-Attention Encoder Block BiDAF Encoder Block Encoder Block Enco"
2020.emnlp-main.624,D15-1001,0,0.754624,"those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning (RL), poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in the huge natural language action space. To make RL agents learn efficiently without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others. To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently (Narasimhan et al., 2015; Hausknecht et al., 2019a); or embed each valid action as another vector and predict action value based on the vector-space similarities (He et al., 2016). These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient. The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observat"
2020.emnlp-main.624,D14-1162,0,0.0833343,"., vk ) component of the template actions as a query. Then a verb-aware observation representation is derived via a RC reader model with Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and self-attention. The observation representation responding to the arg0 and arg1 words are pooled and projected to a scalar value estimate for Q(o, a=hverb, arg0 , arg1 i; ✓). A high-level model architecture of our model is illustrated in Figure 3. Observation and verb Representation. We tokenize the observation and the verb phrase into words, then embed these words using pre-trained GloVe embeddings (Pennington et al., 2014). A shared encoder block that consists of LayerNorm (Ba et al., 2016) and Bidirectional GRU (Cho et al., 2014) processes the observation and verb word embeddings to obtain the separate observation and verb representation. Observation-verb Interaction Layers. Given the separate observation and verb representation, we apply two attention mechanisms to compute a verb-contextualized observation representation. We first apply BiDAF with observation as the context input and verb as the query input. Specifically, we denote the processed embeddings for observation word i and template word j as oi and"
2020.emnlp-main.624,N18-1202,0,0.0109396,"tandard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting. Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been 7757 recently applied to enhance the encoding and attention mechanisms of the aforementioned reader models. They give performance boost but are more resource-demanding and do not suit the IF game playing task very well. Reading Comprehension over Multiple Paragraphs. Multi-paragraph reading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state repres"
2020.emnlp-main.624,D16-1264,0,0.0483491,"rules to extract relations between objects. KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space. Reading Comprehension Models for Question Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting. Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 20"
2020.findings-emnlp.351,D18-1386,0,0.0233612,"Missing"
2020.findings-emnlp.351,N18-1165,0,0.0592915,"Gs) represent knowledge of the world as relationships between entities, i.e., triples with the form (subject, predicate, object) (Bollacker et al., 2008; Suchanek et al., 2007; Vrandeˇci´c and Kr¨otzsch, 2014; Auer et al., 2007; Carlson et al., 2010). Such knowledge resource provides clean and structured evidence for many downstream applications such as question answering. KGs are usually constructed by human experts, which is time-consuming and leads to highly incomplete graphs (Min et al., 2013). Therefore automatic KG completion (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2014; Chen et al., 2018; Socher et al., 2013; Lao et al., 2011) is proposed to infer a missing link of relationship r between a head entity h and a tail entity t. Existing KG completion work mainly makes use of two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations (Bordes et al., 2013; Dettmers et al., 2017; Lin et al., 2015; Neelakantan et al., 2015; Shi and Weninger, 2017; Trouillon et al., 2016; Wang et al.,"
2020.findings-emnlp.351,D11-1049,0,0.0535932,"relationships between entities, i.e., triples with the form (subject, predicate, object) (Bollacker et al., 2008; Suchanek et al., 2007; Vrandeˇci´c and Kr¨otzsch, 2014; Auer et al., 2007; Carlson et al., 2010). Such knowledge resource provides clean and structured evidence for many downstream applications such as question answering. KGs are usually constructed by human experts, which is time-consuming and leads to highly incomplete graphs (Min et al., 2013). Therefore automatic KG completion (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2014; Chen et al., 2018; Socher et al., 2013; Lao et al., 2011) is proposed to infer a missing link of relationship r between a head entity h and a tail entity t. Existing KG completion work mainly makes use of two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations (Bordes et al., 2013; Dettmers et al., 2017; Lin et al., 2015; Neelakantan et al., 2015; Shi and Weninger, 2017; Trouillon et al., 2016; Wang et al., 2014; Xie et al., 2016; Yang et al., 20"
2020.findings-emnlp.351,D16-1011,0,0.0370258,"Missing"
2020.findings-emnlp.351,D18-1362,0,0.0114754,"learns a multi-hop chain as a rule to deduce the target r. An example of such a chain is given in 3948 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3948–3954 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1a to infer whether an athlete plays in an location. Multi-hop reasoning approaches can usually utilize richer evidence and self-justifiable in terms of reasoning path rules used in the predictions, making the prediction of missing relations more interpretable. Despite advantages and success of the multi-hop reasoning approach (Lin et al., 2018; Xiong et al., 2017; Das et al., 2017; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017), a target relationship may not be perfectly inferred from a single relation chain. There could exist multiple weak relation chains that correlate with the target relation. Figure 1 gives examples of such cases. These multiple chains could be leveraged in following ways: (1) the reasoning process naturally relies on the logic conjunction of multiple chains (Figure 1b); (2) more commonly, there are instances for which none of the chains is accurate, but aggregating multiple pieces of evidence improv"
2020.findings-emnlp.351,N13-1095,0,0.0374002,"multiple chains cannot sufficiently infer the target but improves its confidence. Introduction Knowledge graphs (KGs) represent knowledge of the world as relationships between entities, i.e., triples with the form (subject, predicate, object) (Bollacker et al., 2008; Suchanek et al., 2007; Vrandeˇci´c and Kr¨otzsch, 2014; Auer et al., 2007; Carlson et al., 2010). Such knowledge resource provides clean and structured evidence for many downstream applications such as question answering. KGs are usually constructed by human experts, which is time-consuming and leads to highly incomplete graphs (Min et al., 2013). Therefore automatic KG completion (Nickel et al., 2011; Bordes et al., 2013; Yang et al., 2014; Chen et al., 2018; Socher et al., 2013; Lao et al., 2011) is proposed to infer a missing link of relationship r between a head entity h and a tail entity t. Existing KG completion work mainly makes use of two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations (Bordes et al., 2013; Dettmers et"
2020.findings-emnlp.351,P15-1016,0,0.0540057,"Missing"
2020.findings-emnlp.351,D15-1174,0,0.0233004,"generator. To have bounded rewards, we use the predictors’ accuracy instead of the loss values Lp and Lc . The generator is also modeled with a MLP that is of the same architecture as the predictor. The output is a |Ri |×2 vector which represents the probabilities that each chain would be selected into Si and Sic . Rule selection during inference During inference, to have a fixed number (d) of selection, for each instance, we select the top-d chains according to the probability predicted by the generator. 4 Empirical Evaluation We evaluate our model with MCMH rules on two datasets, FB15K-237 (Toutanova et al., 2015) and NELL-995 (Xiong et al., 2017). We follow the existing setting of treating each target relationship as a separate task and training and evaluating relationship-specific reasoning models, and use the standard data splits (Xiong et al., 2017). Table 1 summarizes statistics of two datasets. For each target relation in the datasets, we extract candidate chain set R following Section 2. Table 2 shows the number of extracted chains for each relation. We compare with previous works in the same setting, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2017). They both are single-chain method"
2020.findings-emnlp.351,D17-1060,0,0.222884,"two types of information: 1) co-occurrence of entities and relations and 2) deducible reasoning paths of tuples. KG embeddings encode entities and relations, the first type of information, together into continuous vector space with low-rank tensor approximations (Bordes et al., 2013; Dettmers et al., 2017; Lin et al., 2015; Neelakantan et al., 2015; Shi and Weninger, 2017; Trouillon et al., 2016; Wang et al., 2014; Xie et al., 2016; Yang et al., 2014). Ours approach utilizes the second type of information, reasoning path of tuples that can be deduced to the target tuple (Lao and Cohen, 2010; Xiong et al., 2017; Das et al., 2016, 2017). Here a reasoning path starts with the head entity h and r rN r ends with the tail entity t: h →1 e1 →k ek → t, where r1 ∧ ... ∧ rN forms a relation chain that infers the existence of r. Therefore these methods are also referred as multi-hop reasoning over KGs, which learns a multi-hop chain as a rule to deduce the target r. An example of such a chain is given in 3948 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3948–3954 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1a to infer whether an athlete plays"
2020.findings-emnlp.351,D19-1420,1,0.82201,"ulti-chain multi-hop rule set. Here, instead of treating each single multihop chain as a rule, we learn rules consisting of a small set of multi-hop chains. Therefore the inference of target relationships becomes a joint scoring of such a set of chains. We treat each set of chains as one rule and, since different query pairs can follow different rules, together we have a set of rules to reason each relation. Learning the generalized multi-hop rule set is a combinatorial search problem. We address this challenge with a game-theoretic approach inspired by (Lei et al., 2016; Carton et al., 2018; Yu et al., 2019). Our approach consists of two steps: (1) selecting a generalized multi-hop rule set by employing a Multi-Layer Perceptron (MLP) over the candidate chains; (2) reasoning with the generalized rule set, which uses another MLP to model the conditional probability of the target relationship given the selected relation chains. The nonlinearity of MLP as reasoner provides the potential to model the logic conjunction among the selected chains in the rule set. We demonstrate the advantage of our method on KG completion tasks in FB15K-237 and NELL995. Our method outperforms existing single-chain approa"
2020.nuse-1.13,N19-1423,0,0.361724,"er investigate how state-ofthe-art open-domain QA approaches can help BookQA. Besides achieving state-of-the-art on the NarrativeQA benchmark, our study also reveals the difficulty of evidence retrieval in books with a wealth of experiments and analysis - which necessitates future effort on novel solutions for evidence retrieval in BookQA. 1 Bingsheng Yao Rensselaer Polytechnic Institute Troy, NY 12180 yaob@rpi.edu Introduction The task of question answering has benefited largely from the advancements in deep learning, especially from the pre-trained language models(LM) (Radford et al., 2019; Devlin et al., 2019). While question answering over single passage (reading comprehension datasets) and over the large-scale open-domain corpora (open-domain QA) have largely benefited from these, the performance of QA over book stories (BookQA) lags behind. For example, the most representative benchmark in this direction, the NarrativeQA (Koˇcisk`y et al., 2018) which was released three years ago - the current state-of-the-art methods only show marginal improvement over the first baselines. There are several challenges in NarrativeQA which slow down the research progress. First, the narrative stories lead to a n"
2020.nuse-1.13,D19-5823,0,0.0632979,"ary setting and the full-story setting. Our BookQA task corresponds to the full-story setting that finds answers from books or movie scripts. Note that the NarrativeQA is a generative QA task. The answers are not guaranteed to appear in the books. 2 1 To be more accurate, the question should be denoted as QB but we use Q for simplicity. 3 109 Settings https://huggingface.co/transformers/model doc/gpt2.html In practice, we set the hyperparameters 0.7 and 0.4 System Attention Sum (Koˇcisk`y et al., 2018) BiDAF (Koˇcisk`y et al., 2018) IAL-CPG (Tay et al., 2019) R3 (Wang et al., 2017) BERT-heur (Frermann, 2019) Our generative/extractive systems w/ trained ranker w/ pre-trained LM w/ extra training data X X X X X X Table 1: Summary of the characteristics of the compared systems. Red/blue color refers to generative/extraction QA systems. In addition to the standard techniques, (Wang et al., 2017) uses reinforcement learning to train the ranker; and (Tay et al., 2019) uses curriculum to train the reader to overcome the divergence of evidence retrieval qualities between training and testing. We preprocess the raw data with SpaCy4 tokenization. Then following (Koˇcisk`y et al., 2018), we cut the books in"
2020.nuse-1.13,2020.emnlp-main.550,0,0.0302039,"Missing"
2020.nuse-1.13,Q18-1023,0,0.0985804,"Missing"
2020.nuse-1.13,P19-1612,0,0.0287139,"e annotation make BookQA task similar to open-domain QA. In this paper, we first study whether the ideas used in state-of-the-art open-domain QA systems can be extended to improve BookQA including: (1) the neural ranker-reader pipeline (Wang et al., 2018), where a neural ranker is used to select related passages (evidence) given a question from a large candidate sets; (2) the usage of pre-trained LMs as reader and ranker, such as GPT (Radford et al., 2019), BERT (Devlin et al., 2019) and their followup work; (3) the distantly supervised and unsupervised training techniques (Wang et al., 2018; Lee et al., 2019; Min et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) that help rankers learn more from noisy gold data. By training a ranker-reader framework on BookQA, we successfully achieve a new state-ofthe-art on NarrativeQA using both generative and extractive readers. Based on these results and our 108 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 108–113 c July 9, 2020. 2020 Association for Computational Linguistics analysis, we observe the followings: • Using the pre-trained LMs as the reader model, such as BERT and GPT, improves the Narrativ"
2020.nuse-1.13,W04-1013,0,0.0207629,"k summaries. However, the summary is not considered available by design (Koˇcisk`y et al., 2018) in the general full-story scenario where questions should be answered solely from books.5 Although not the focus of the paper, our reader performance in the summary setting is also reported (Section 3.2), to show the properties of the readers. Metrics Because of the generative nature of the task, following previous works (Koˇcisk`y et al., 2018; Tay et al., 2019; Frermann, 2019), we evaluate the QA performance with Bleu-1, Bleu-4 (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), Rouge-L (Lin, 2004).6 We also report the Exact Match(EM) and F1 scores7 that are commonly used in open-domain QA evaluation. We convert both hypothesis and reference to lowercase and remove the punctuation before evaluation. Model Selection We select the best models on the development set according to its average score 4 https://spacy.io/ In NarrativeQA, the summary has a good coverage of the answers due to the data collection procedures; also, summaries can be viewed as humans’ comprehension of the books. 6 We used an open-source evaluation library (Sharma et al., 2017): https://github.com/Maluuba/nlg-eval. 7 T"
2020.nuse-1.13,D19-1284,0,0.0517182,"BookQA task similar to open-domain QA. In this paper, we first study whether the ideas used in state-of-the-art open-domain QA systems can be extended to improve BookQA including: (1) the neural ranker-reader pipeline (Wang et al., 2018), where a neural ranker is used to select related passages (evidence) given a question from a large candidate sets; (2) the usage of pre-trained LMs as reader and ranker, such as GPT (Radford et al., 2019), BERT (Devlin et al., 2019) and their followup work; (3) the distantly supervised and unsupervised training techniques (Wang et al., 2018; Lee et al., 2019; Min et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) that help rankers learn more from noisy gold data. By training a ranker-reader framework on BookQA, we successfully achieve a new state-ofthe-art on NarrativeQA using both generative and extractive readers. Based on these results and our 108 Proceedings of the 1st Joint Workshop on Narrative Understanding, Storylines, and Events, pages 108–113 c July 9, 2020. 2020 Association for Computational Linguistics analysis, we observe the followings: • Using the pre-trained LMs as the reader model, such as BERT and GPT, improves the NarrativeQA performance. W"
2020.nuse-1.13,P19-1220,0,0.029331,"Missing"
2020.nuse-1.13,P02-1040,0,0.108802,", which constructed evidence-level supervision with the usage of book summaries. However, the summary is not considered available by design (Koˇcisk`y et al., 2018) in the general full-story scenario where questions should be answered solely from books.5 Although not the focus of the paper, our reader performance in the summary setting is also reported (Section 3.2), to show the properties of the readers. Metrics Because of the generative nature of the task, following previous works (Koˇcisk`y et al., 2018; Tay et al., 2019; Frermann, 2019), we evaluate the QA performance with Bleu-1, Bleu-4 (Papineni et al., 2002), Meteor (Banerjee and Lavie, 2005), Rouge-L (Lin, 2004).6 We also report the Exact Match(EM) and F1 scores7 that are commonly used in open-domain QA evaluation. We convert both hypothesis and reference to lowercase and remove the punctuation before evaluation. Model Selection We select the best models on the development set according to its average score 4 https://spacy.io/ In NarrativeQA, the summary has a good coverage of the answers due to the data collection procedures; also, summaries can be viewed as humans’ comprehension of the books. 6 We used an open-source evaluation library (Sharma"
2020.nuse-1.13,D19-1244,0,0.0458111,"Missing"
2020.nuse-1.13,P19-1486,0,0.161756,"arrativeQA provides two different settings, the summary setting and the full-story setting. Our BookQA task corresponds to the full-story setting that finds answers from books or movie scripts. Note that the NarrativeQA is a generative QA task. The answers are not guaranteed to appear in the books. 2 1 To be more accurate, the question should be denoted as QB but we use Q for simplicity. 3 109 Settings https://huggingface.co/transformers/model doc/gpt2.html In practice, we set the hyperparameters 0.7 and 0.4 System Attention Sum (Koˇcisk`y et al., 2018) BiDAF (Koˇcisk`y et al., 2018) IAL-CPG (Tay et al., 2019) R3 (Wang et al., 2017) BERT-heur (Frermann, 2019) Our generative/extractive systems w/ trained ranker w/ pre-trained LM w/ extra training data X X X X X X Table 1: Summary of the characteristics of the compared systems. Red/blue color refers to generative/extraction QA systems. In addition to the standard techniques, (Wang et al., 2017) uses reinforcement learning to train the ranker; and (Tay et al., 2019) uses curriculum to train the reader to overcome the divergence of evidence retrieval qualities between training and testing. We preprocess the raw data with SpaCy4 tokenization. Then follo"
2020.nuse-1.13,N16-1170,0,0.0413933,"Missing"
2021.eacl-main.234,P19-1416,0,0.0176475,"ng evidence to the top.1 For example in Figure 1 (top), P1 and P2 are two candidate evidence passages that are closely related to the question but only cover the same unilateral fact required by the question, therefore leading us to the wrong answer Newton. This paper formulates a new problem of complementary evidence identification for answering complex questions. The key idea is to consider the problem as measuring the properties of the selected passages, more than the individual relevance. Specifically, we hope the selected passages can serve as a set of spanning bases that supports the 1 (Min et al., 2019) pointed out the shortcut problem in multi-hop QA. However, as some works (Wang et al., 2019) show that even a better designed multi-hop model can still benefit from full evidence in such situation. 2720 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2720–2726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics question. The selected passage set thus should satisfy the properties of (1)relevancy, i.e., they should be closely related to the question; (2) diversity, i.e., they should cover diverse information gi"
2021.eacl-main.234,P18-1160,0,0.0282789,"plex question. To this end, we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain. 1 Introduction In recent years, significant progress has been made in the field of open-domain question answering (Chen et al., 2017; Wang et al., 2017, 2018; Clark and Gardner, 2018; Min et al., 2018; Asai et al., 2019). Very recently, some works turn to deal with a more challenging task of asking complex questions (Welbl et al., 2018; Clark et al., 2018; Yang et al., 2018) from the open-domain text corpus. In the open-domain scenario, one critical challenge raised by complex questions is that each question may require multiple pieces of evidence to get the right answer, while the evidence usually scatters in different passages. Examples in Figure 1 shows two types of questions that require evidence from multiple passages. To deal with the challenging multi-evidence questions, an open-dom"
2021.eacl-main.234,D19-5813,1,0.778884,"passages that are closely related to the question but only cover the same unilateral fact required by the question, therefore leading us to the wrong answer Newton. This paper formulates a new problem of complementary evidence identification for answering complex questions. The key idea is to consider the problem as measuring the properties of the selected passages, more than the individual relevance. Specifically, we hope the selected passages can serve as a set of spanning bases that supports the 1 (Min et al., 2019) pointed out the shortcut problem in multi-hop QA. However, as some works (Wang et al., 2019) show that even a better designed multi-hop model can still benefit from full evidence in such situation. 2720 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2720–2726 April 19 - 23, 2021. ©2021 Association for Computational Linguistics question. The selected passage set thus should satisfy the properties of (1)relevancy, i.e., they should be closely related to the question; (2) diversity, i.e., they should cover diverse information given the coverage property is satisfied; (3) compactness, i.e., the number of passages to sati"
2021.eacl-main.234,Q18-1021,0,0.0311086,"y within the selected set, in addition to the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain. 1 Introduction In recent years, significant progress has been made in the field of open-domain question answering (Chen et al., 2017; Wang et al., 2017, 2018; Clark and Gardner, 2018; Min et al., 2018; Asai et al., 2019). Very recently, some works turn to deal with a more challenging task of asking complex questions (Welbl et al., 2018; Clark et al., 2018; Yang et al., 2018) from the open-domain text corpus. In the open-domain scenario, one critical challenge raised by complex questions is that each question may require multiple pieces of evidence to get the right answer, while the evidence usually scatters in different passages. Examples in Figure 1 shows two types of questions that require evidence from multiple passages. To deal with the challenging multi-evidence questions, an open-domain QA system should be able to (1) efficiently retrieve a small number of passages that cover the full evidence; and (2) accurately extr"
2021.eacl-main.234,N18-1101,0,0.0390016,"Missing"
2021.eacl-main.234,D18-1259,0,0.354317,"o the relevance between the question and passages. Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain. 1 Introduction In recent years, significant progress has been made in the field of open-domain question answering (Chen et al., 2017; Wang et al., 2017, 2018; Clark and Gardner, 2018; Min et al., 2018; Asai et al., 2019). Very recently, some works turn to deal with a more challenging task of asking complex questions (Welbl et al., 2018; Clark et al., 2018; Yang et al., 2018) from the open-domain text corpus. In the open-domain scenario, one critical challenge raised by complex questions is that each question may require multiple pieces of evidence to get the right answer, while the evidence usually scatters in different passages. Examples in Figure 1 shows two types of questions that require evidence from multiple passages. To deal with the challenging multi-evidence questions, an open-domain QA system should be able to (1) efficiently retrieve a small number of passages that cover the full evidence; and (2) accurately extract the answer by jointly considering th"
2021.eacl-main.234,D19-1258,0,0.0517723,"Missing"
2021.eacl-main.234,N18-1202,0,0.0181826,"and practically needs to be a small value, whereas it is not the case in SRD. To achieve the above goals, a straightforward approach is to train a model that evaluates each subset of the candidate passages, e.g., by concatenating passages in any subsets. However, this approach is highly inefficient since it requires to encode O(K L ) passage subsets, where K is the total number of candidates and L is the maximum size of subsets. Thus, a practical complementary evidence identification method needs to be computationally efficient. This is especially critical when we use heavy models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), where passage encoding is time and memory consuming. To this end, we propose an efficient method to select a set of spanning passages that is sufficient and diverse. The core idea is to represent questions and passages in a vector space and define the measures of our criterion in the vector space. For example, in the vector space, sufficiency can be defined as a similarity between the question vector and the sum of selected passage vectors, measured by a cosine function with a higher score indicating a closer similarity; and diversity can be defined as `1 dista"
2021.emnlp-main.519,D19-5816,1,0.892822,"Missing"
2021.emnlp-main.519,P99-1071,1,0.465211,"is on the size of input event graph. 4 Related Work etc. However, they ignore relations between event arguments, or only use hierarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework."
2021.emnlp-main.519,D19-5602,0,0.0273129,"Missing"
2021.emnlp-main.519,N19-1240,0,0.0414547,"Missing"
2021.emnlp-main.519,N19-1423,0,0.177587,"ences measured by A time-aware optimal transport distance is the inter-similarity of these articles. In these meththen introduced for learning the compression ods, the document representations are limited to model in an unsupervised manner. We show local text features, ignoring the global context of that our approach significantly improves the the news collection. The applications of neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major"
2021.emnlp-main.519,P19-1259,0,0.0422027,"Missing"
2021.emnlp-main.519,P19-1102,0,0.084656,"e of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Processing (NLP) techniques: (1) In contrast to multi-document summarization (MDS) we construct a single event graph from the dealing with tens of documents (Fabbri et al., 2019), input documents, with co-referential entities (e.g., house, mansion in Figure 1) and co1 The programs, data and resources are publicly available referential events (e.g., die, collapsed) merged for research purpose in https://github.com/limanling/ event-graph-summarization. across documents. Our comprehensive event 6443 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6443–6456 c November 7–11, 2021. 2021 Association for Computational Linguistics Input Documents 2009-06-27 There was no sign of foul play in the death of Michael Jackson......A recor"
2021.emnlp-main.519,W04-1017,0,0.0911918,"ptimal transport (OT): We propose a new for- minimal distance with only m events to be kept, a global decision is learned to select salient but mulation of timeline summarization, by selecting also diverse events. The summary graphs are genevent nodes from the input graph to form a smaller summary graph. Under a certain summary size con- erated using a differentiable compression model according to a hyperparameter of compression rate, straint, a summary graph with high coverage has a instead of using annotated timelines. Thus, our small information loss, compared to the one with low coverage (Filatova and Hatzivassiloglou, 2004). objective allows model training in an end-to-end unsupervised way. We constrain the total number of event nodes to be kept in the summary, and optimize the summary (3) Time-aware Gromov-Wasserstein distance: graph to be close to the original graph using opti- The distance between two graphs should capture 6444 the following criteria: i) Semantic relevance: each node first has its initial local context encoded via a pre-trained BERT model and node type embeddings. For example, S TART P OSITION event is not closely related to the T RANSPORT event in Figure 1 though they have temporal dependenc"
2021.emnlp-main.519,C10-1039,0,0.0590717,"erarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timel"
2021.emnlp-main.519,N09-1041,0,0.0241803,"Work etc. However, they ignore relations between event arguments, or only use hierarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracp"
2021.emnlp-main.519,2020.acl-main.457,1,0.840441,"is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timeline Summarization. Due to the lack of to hierarchically generate the timeline. training data, timeline summarization focuses on extractive methods with heuristics (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Ac"
2021.emnlp-main.519,2021.naacl-main.274,1,0.693645,"ion from a node to its mentions v An event node in an event graph e An entity node in an event graph hvi , vl i A temporal ordering edge (vl happens after vi ) hvi , a, ej i An argument edge (the entity ej plays argument role a in event vi ) hej , r, ek i An entity relation edge between ej and ek , and r is the relation type Table 1: List of symbols. We apply OneIE (Lin et al., 2020), a state-ofthe-art Information Extraction (IE) system, to extract entities, relations and events; then perform 2 Method cross-document entity and event coreference res2.1 Overview olution (Pan et al., 2015, 2017; Lai et al., 2021) Our approach aims at finding the graph that has over the document cluster of each timeline topic. minimal distance from the input graph (Filatova We apply (Ning et al., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the be"
2021.emnlp-main.519,2020.acl-main.230,1,0.885492,"Missing"
2021.emnlp-main.519,2020.emnlp-main.50,1,0.76894,"ral attribute accuracy if there is a tie. The events with temporal attributes extracted directly from the context are of highest priority, followed by events having temporal attributes propagated from neighbor events in §2.2, and then the ones using document publication date. 4 https://wwconw.voanews.com 5 https://www.reuters.com following §2.2. 6 We use the ACE event ontology7 , with 7 entity types, 6 relation types, 33 event types, and 22 argument roles. For the (unsupervised) training of our event graph compression model, we use event graphs constructed from VoA news between 2011 and 2017 (Li et al., 2020a). The statistics are shown in Table 2. Dataset Split #Doc #Event #Entity #Rel Timeline17 Input Timeline 4,650 19 74,320 115,585 136,509 974 1,936 1,134 Crisis Input 20,463 325,695 551,228 610,410 Timeline 22 736 1,184 1,309 Timeline100 Input 10,379 178,581 301,132 306,975 Timeline 100 3,296 8,901 23,732 Unlabeled Input 72,576 913,679 381,735 1,046,066 (for OT) Timeline - Table 2: Data statistics, including the number of documents, events, entities, and temporal relations. Evaluation Metrics. We use the conventional metrics for timeline summarization (Martschat and Markert, 2018) to evaluate"
2021.emnlp-main.519,2020.acl-main.713,1,0.86566,"2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Processing (NLP) techniques: (1) In contrast to multi-document summarization (MDS) we construct a single event graph from the dealing with tens of documents (Fabbri et al., 2019), input documents, with co-referential entities (e.g., house, mansion in Figure 1) and co1 The programs, data and resources are publicly available referential events (e.g., die, collapsed) merged for research purpose in https://github.com/limanling/ event-graph-summarization. across documents. Our comprehensive event 6443 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process"
2021.emnlp-main.519,P19-1500,0,0.0578843,"Missing"
2021.emnlp-main.519,P14-5010,0,0.00254608,"l., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the best transport plan that has a minimal distance event, We extract and normalize time expressions between two graphs. To apply optimal transport using publication date (Manning et al., 2014), and to timeline summarization, the key is to design the then apply (Wen et al., 2021) to extract the event distance to evaluate the information loss, and thus temporal attributes from the context. If the tem6445 poral attributes can not be decided according to the context, we propagate the temporal attributes from neighbor events based on their shared arguments (?). After that, we use the document publication date to populate the remaining missing dates. For example, in Figure 1, the date 2009-0625 of the collapse (D IE) event is extracted from context last Thursday, and the date of the unco"
2021.emnlp-main.519,K18-1023,0,0.0660956,"orld datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Pr"
2021.emnlp-main.519,C14-1114,0,0.0646958,"neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction"
2021.emnlp-main.519,D19-1642,0,0.0443163,"Missing"
2021.emnlp-main.519,N15-1119,1,0.81622,"s type w A mapping function from a node to its mentions v An event node in an event graph e An entity node in an event graph hvi , vl i A temporal ordering edge (vl happens after vi ) hvi , a, ej i An argument edge (the entity ej plays argument role a in event vi ) hej , r, ek i An entity relation edge between ej and ek , and r is the relation type Table 1: List of symbols. We apply OneIE (Lin et al., 2020), a state-ofthe-art Information Extraction (IE) system, to extract entities, relations and events; then perform 2 Method cross-document entity and event coreference res2.1 Overview olution (Pan et al., 2015, 2017; Lai et al., 2021) Our approach aims at finding the graph that has over the document cluster of each timeline topic. minimal distance from the input graph (Filatova We apply (Ning et al., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtai"
2021.emnlp-main.519,P17-1178,1,0.766016,"Missing"
2021.emnlp-main.519,2021.textgraphs-1.4,0,0.0817398,"Missing"
2021.emnlp-main.519,D19-5403,0,0.210679,"hey pendency across key stories, which, compared to determine the key dates of events. These standard MDS, poses additional challenges in remethods overlook the events’ intra-structures (arguments) and inter-structures (event-event constructing temporal order. (3) Manual labeling connections). Following a different route, of timeline summaries is costly; thus the labeled we propose to represent the news articles data for model training is very limited. as an event-graph, thus the summarization As a result, previous studies (Martschat and task becomes compressing the whole graph Markert, 2018; Steen and Markert, 2019) usually to its salient sub-graph. The key hypothesis take an unsupervised approach. Specifically, these is that the events connected through shared arguments and temporal order depict the skelemethods first identify the key dates from the pubton of a timeline, containing events that are lication time distribution. Then for each key date semantically related, structurally salient, and and its associated news articles, a summary is gentemporally coherent in the global event graph. erated based on the salient sentences measured by A time-aware optimal transport distance is the inter-similarity o"
2021.emnlp-main.519,2020.acl-main.553,0,0.0178075,"ument. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timeline Summarization. Due to the lack of to hierarchically generate the timeline. training data, timeline summarization focuses on extractive methods with heuristics (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh"
2021.emnlp-main.519,N16-1008,0,0.0607266,"Missing"
2021.emnlp-main.519,2021.naacl-main.6,1,0.500178,"r events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the best transport plan that has a minimal distance event, We extract and normalize time expressions between two graphs. To apply optimal transport using publication date (Manning et al., 2014), and to timeline summarization, the key is to design the then apply (Wen et al., 2021) to extract the event distance to evaluate the information loss, and thus temporal attributes from the context. If the tem6445 poral attributes can not be decided according to the context, we propagate the temporal attributes from neighbor events based on their shared arguments (?). After that, we use the document publication date to populate the remaining missing dates. For example, in Figure 1, the date 2009-0625 of the collapse (D IE) event is extracted from context last Thursday, and the date of the unconscious (I NJURE) event is propagated along with their shared argument Michael Jackson."
2021.emnlp-main.519,J81-4005,0,0.679601,"Missing"
2021.emnlp-main.519,D11-1040,0,0.138735,"ised manner. We show local text features, ignoring the global context of that our approach significantly improves the the news collection. The applications of neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline s"
2021.emnlp-main.519,K17-1045,0,0.0478793,"Missing"
2021.emnlp-main.519,2021.textgraphs-1.5,1,0.837732,"Missing"
2021.emnlp-main.519,P19-1628,0,0.0232982,"e summaries of all selected dates; (2) agree F1 to compute ROUGE only between the summaries which have the same dates; (3) align F1 to first align summaries in the output with those in the reference based on similarity and the distance between their dates, then compute the ROUGE score between aligned summaries. Distant alignments are punished. Baselines. We compare with: (1) (Chieu and Lee, 2004), a typical extractive model based on sentence similarity; and (2) (Martschat and Markert, 2018), the state-of-the-art extractive timeline sumarization model based on submodular functions. (3) PacSum (Zheng and Lapata, 2019), the state-of-the-art unsupervised graph-based ranking summarization baseline, which utilizes BERT to encode sentences for sentence centrality ranking in a sentence graph. We use the publication date of the selected sentence as key dates. (4) SummPip (Zhao et al., 2020), the state-of-the-art unsupervised multi-document summarization baseline, which constructs a sentence graph and performs spectral clustering. After that, a summary is generated for each sentence cluster 6 The preprocessed event graphs are released together with the dataset. 7 https://www.ldc.upenn.edu/collaborations/ past-proj"
2021.findings-acl.339,2020.findings-emnlp.89,1,0.729463,"d OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i. The edge set EG consists"
2021.findings-acl.339,W13-2322,0,0.0612546,"Missing"
2021.findings-acl.339,D13-1160,0,0.0607099,"f the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts"
2021.findings-acl.339,P13-2131,0,0.0801353,"Missing"
2021.findings-acl.339,dorr-etal-1998-thematic,0,0.326306,"Missing"
2021.findings-acl.339,kingsbury-palmer-2002-treebank,0,0.0480491,"(AMR) graph; (ii) transforms the AMR graph to a set of candidate KB-aligned logical queries, via a novel but simple graph transformation approach; (iii) uses a Logical Neural Network (LNN) (Riegel et al., 2020) to reason over KB facts and produce answers to KB-aligned logical queries. We describe each of these modules in the following sections. 2.1 AMR Parsing NSQA utilizes AMR parsing to reduce the complexity and noise of natural language questions. An AMR parse is a rooted, directed, acyclic graph. AMR nodes represent concepts, which may include normalized surface symbols, Propbank frames (Kingsbury and Palmer, 2002) as well as other AMR-specific constructs to handle named entities, quantities, dates and other phenomena. Edges in an AMR graph represent the relations between concepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate vari"
2021.findings-acl.339,2020.findings-emnlp.288,1,0.824748,"Missing"
2021.findings-acl.339,P19-1451,1,0.842549,"cepts such as standard OntoNotes roles but also AMR specific relations such as polarity or mode. As shown in Figure 1, AMR provides a representation that is fairly close to the KB representation. A special amr-unknown node, indicates the missing concept that represents the answer to the given question. In the example of Figure 1, amr-unknown is a person, who is the subject of act-01. Furthermore, AMR helps identify intermediate variables 3885 that behave as secondary unknowns. In this case, a movie produced by Benicio del Toro in Spain. NSQA utilizes a stack-Transformer transitionbased model (Naseem et al., 2019; Astudillo et al., 2020) for AMR parsing. An advantage of transition-based systems is that they provide explicit question text to AMR node alignments. This allows encoding closely integrated text and AMR input to multiple modules (Entity Linking and Relation Linking) that can benefit from this joint input. 2.2 AMR to KG Logic The core contribution of this work is our next step where the AMR of the question is transformed to a query graph aligned with the underlying knowledge graph. We formalize the two graphs as follows: AMR graph G is a rooted edge-labeled directed acyclic graph hVG , EG i."
2021.findings-acl.339,N19-1243,0,0.0615569,"Missing"
2021.findings-acl.339,D07-1071,0,0.057132,"l start [sic] the final match of the football world cup 2018? Table 4: Question types supported by NSQA , with examples from QALD Falcon NMD+BLINK Dataset QALD-9 QALD-9 P 0.81 0.82 R 0.83 0.90 F1 0.82 0.85 Falcon NMD+BLINK LC-QuAD 1.0 LC-QuAD 1.0 0.56 0.87 0.69 0.86 0.62 0.86 Table 5: Performance of Entity Linking modules compared to SOTA Falcon on our dev sets we intend to explore more uses of such reasoners for KBQA in the future. 4 Related Work Early work in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to re"
2021.findings-acl.339,N19-1301,0,0.0169546,"proaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgrap"
2021.findings-acl.339,D18-1110,1,0.837164,"ually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions have also been developed. Some have primarily focused on generating SPARQL sketches (Maheshwari et al., 2019; Chen et al., 2020) where they evaluate these sketches (2-hop) by providing gold entities and ignoring the evaluation of selecting target variables or other aggregation functions like sorting and counting. (Zheng and Zhang, 2019) generates the question subgraph via filling the entity and relationship slots of 12 predefined question template. Their performance on th"
2021.findings-acl.339,P14-1090,0,0.0858204,"Missing"
2021.findings-acl.339,P17-1053,1,0.839108,"rk in KBQA focused mainly on designing parsing algorithms and (synchronous) grammars to semantically parse input questions into KB queries (Zettlemoyer and Collins, 2007; Berant et al., 2013), with a few exceptions from the information extraction perspective that directly rely on relation detection (Yao and Van Durme, 2014; Bast and Haussmann, 2015). All the above approaches train statistical machine learning models based on human-crafted features and the performance is usually limited. Deep Learning Models. The renaissance of neural models significantly improved the accuracy of KBQA systems (Yu et al., 2017; Wu et al., 2019a). Recently, the trend favors translating the question to its corresponding subgraph in the KG in an end-to-end learnable fashion, to reduce the human efforts and feature engineering. This includes two most commonly adopted directions: (1) embedding-based approaches to make the pipeline end-to-end differentiable (Bordes et al., 2015; Xu et al., 2019); (2) hard-decision approaches that generate a sequence of actions that forms the subgraph (Xu et al., 2018; Bhutani et al., 2019). On domains with complex questions, like QALD and LC-QuAD, end-to-end approaches with harddecisions"
2021.naacl-industry.38,2020.insights-1.16,0,0.133432,"ned language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ. 50 Average accuracy on HINT3 collection Figure 1: Accuracy of commercial solutions on the HINT3 collection of datasets. Results are averaged across the Full versions of the three datasets and their Subset versions. The in-scope accuracy is reported on a threshold of 0.1. Watson Assistant achieves the best results on average. Results for all methods except Watson Assistant are obtained from Arora et al. (2020). Intent detection and entity recognition form the basis of the Natural Language Understanding (NLU) components of a task-oriented dialog system. The intents and entities identified in a given user utterance help trigger the appropriate conditions defined in a dialog tree which guides the user through a predetermined dialog-flow. These task-oriented dialog systems have gained popularity for designing applications around customer support, personal assistants, and opinion mining, etc. The Conversational AI market is expected to grow to an estimated USD 13.9 billion by 2025 as Equal contribution"
2021.naacl-industry.38,W17-5522,0,0.0295916,"e also explore few-shot and robustness settings, and compare the resource efficiency and training times of different commercial solutions as well as pretrained LMs. Among these solutions, Watson Assistant’s new intent detection algorithm performs better than other commercial solutions (Figure 1), and achieves comparable accuracy when compared to large-scale pretrained LMs (Figure 2) while being much more efficient. 2 Related Work Several datasets have been released to test the performance of intent detection for task-oriented dialog systems such as Web Apps, Ask Ubuntu and Chatbot corpus from Braun et al. (2017); ATIS dataset (Price, 1990) and SNIPS (Coucke et al., 2018). The ATIS and SNIPS datasets have been created with focus on voice interactive chatbots. Voice modality has some specific characters, i.e., it does not contain typos and it is less noisy than text-based communication. Thus, these datasets are oversimplified version of the text-based intent detection task ""in the wild"" due to well-constructed dataset and limited number of classes. Recently, CLINC150 (Larson et al., 2019), BANKING77 (Casanueva et al., 2020), and HWU64 (Liu et al., 2019b) have been used to benchmark the performance of i"
2021.naacl-industry.38,2020.nlp4convai-1.5,0,0.175008,"omes an important consideration for real-world conversational AI solutions. In this work, taking the aforementioned three realistic challenges into consideration, we evaluate multiple intent detection models and focus on their accuracy, data efficiency, robustness, and computational efficiency. We compare the performance of various commercial intent detection models on three datasets in the HINT3 collection (Arora et al., 2020). We also evaluate pretrained Language Models (LM) on three commonly used public datasets for benchmarking intent detection - CLINC150 (Larson et al., 2019), BANKING77 (Casanueva et al., 2020), and HUW64 (Liu et al., 2019b). In addition, we create few-shot learning settings from these datasets, to better match real world low-resource scenarios. Furthermore, we measure the ""in the wild"" robustness of the systems via creating difficult test subsets from existing test sets. Finally, we evaluate the classification accuracy and training time of these models because it directly affects the usability and development lifecycle of an conversational AI solution. We build upon the existing study in Arora et al. (2020) which benchmarked commercial solutions aside from IBM Watson Assistant (i.e"
2021.naacl-industry.38,D19-1131,0,0.0295574,"Missing"
2021.naacl-industry.38,2021.ccl-1.108,0,0.0402986,"Missing"
2021.naacl-industry.38,N18-1202,0,0.0875574,"uest where users can input non-standard paraphrases until we receive the status that the model is trained of the queries, we propose to create more difficult 5 subsets of the provided test sets to mimic the real- and available for serving. world setting. 3 We release the difficult subsets at https://github. Following Arora et al. (2020), we create a sub- com/haodeqi/BenchmarkingIntentDetection to facilitate repeatability and future research. set of each test set with semantically dissimilar 4 https://www.ibm.com/cloud/ sentences from the training set. Instead of using watson-assistant 5 ELMo (Peters et al., 2018) and entailment scores, Note that the training times may vary depending on the we use TF/IDF cosine distance to pick the most load on the web API. 306 CLINC150 CLINC150 HWU64 BANKING77 Average WA classic WA enhanced 93.9 95.7 88.8 90.5 90.6 92.6 91.1 92.9 RASA Distilbert-base BERT-base BERT-large USE-base RoBERTa-base 89.4 96.3 96.8 97.1 94.7 97.0 84.9 91.7 91.6 91.9 88.9 92.1 89.9 92.1 93.3 93.7 89.9 94.1 88.1 93.4 93.9 94.2 91.2 94.4 Table 1: Accuracy on CLINC150, HWU64 and BANKING77 for Watson Assistant (WA), RASA and pretrained LMs. Training is performed on the full train sets and evaluati"
2021.naacl-industry.38,H90-1020,0,0.462915,"ess settings, and compare the resource efficiency and training times of different commercial solutions as well as pretrained LMs. Among these solutions, Watson Assistant’s new intent detection algorithm performs better than other commercial solutions (Figure 1), and achieves comparable accuracy when compared to large-scale pretrained LMs (Figure 2) while being much more efficient. 2 Related Work Several datasets have been released to test the performance of intent detection for task-oriented dialog systems such as Web Apps, Ask Ubuntu and Chatbot corpus from Braun et al. (2017); ATIS dataset (Price, 1990) and SNIPS (Coucke et al., 2018). The ATIS and SNIPS datasets have been created with focus on voice interactive chatbots. Voice modality has some specific characters, i.e., it does not contain typos and it is less noisy than text-based communication. Thus, these datasets are oversimplified version of the text-based intent detection task ""in the wild"" due to well-constructed dataset and limited number of classes. Recently, CLINC150 (Larson et al., 2019), BANKING77 (Casanueva et al., 2020), and HWU64 (Liu et al., 2019b) have been used to benchmark the performance of intent detection systems. The"
2021.naacl-industry.38,N19-1423,0,0.0172596,"complexity of doing finegrained classification. Arora et al. (2020) proposed a new collection of datasets called HINT3, containing a noisy and diverse set of intents and examples across three domains sourced from domain experts and real users. Prior work from Arora et al. (2020), Braun et al. (2017), and Liu et al. (2019a) study the performance of different conversational AI services using the datasets mentioned above. Casanueva et al. (2020), Larson et al. (2019), Arora et al. (2020), Bunk et al. (2020) and others have benchmarked several state-of-the-art (SOTA) pretrained LMs such as BERT (Devlin et al., 2019) on the aforementioned datasets. We aim to standardize the benchmarking tests that need to be run while developing an industry scale intent detection system. The tests should cover a variety of real-world datasets, settings such as few-shot scenarios and testing on semantically dissimilar test examples. Additionally, the tests should also cover resource efficiency and training time - since they affect the overall deployment costs 2 of a virtual assistant cloud service. A carefully choInference time is usually dependent on service-level agreements between the provider and the user which determi"
2021.naacl-main.20,2020.acl-main.747,0,0.193157,"tAlign can lead to error propagation to the rest of the pipeline; (2) FastAlign mainly creates the alignments with word-level translation and usually overlooks the contextual semantic compositions. As a result, the tuned mBERT is biased to shallow cross-lingual correspondence. Importantly, both approaches only involve word-level alignment tasks. Building on the success of monolingual pretrained language models (LM) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), their multilingual counterparts mBERT (Devlin et al., 2019) In this work, we focus on self-supervised, and XLM-R (Conneau et al., 2020) are trained alignment-oriented training tasks using minimum using the same objectives—Masked Language parallel data to improve mBERT’s cross-lingual Modeling (MLM) and in the case of mBERT, Next transferability. We propose a Post-Pretraining Sentence Prediction (NSP). MLM is applied to Alignment (PPA) method consisting of both wordmonolingual text that covers over 100 languages. level and sentence-level alignment, as well as a Despite the absence of parallel data and explicit finetuning technique on downstream tasks that take alignment signals, these models transfer surpris- pairs of text as"
2021.naacl-main.20,D18-1269,0,0.0489453,"Missing"
2021.naacl-main.20,D19-1252,0,0.0391928,"Missing"
2021.naacl-main.20,L18-1548,0,0.0381153,"performance in our experiments. 212 ensure the two augmented pairs appear in the same batch. 3 We focus on XLT in this work. For zero-shot crosslingual transfer, we train on the English SQuAD v1.1 (Rajpurkar et al., 2016) training set. For translate-train, we train on translation data provided in Hu et al. (2020) 5 Experimental Settings 3.1 Parallel Data for Post-Pretraining Parallel Data All parallel data we use involve English as the source language. Specifically, we collect en-fr, en-es, en-de parallel pairs from Europarl, en-ar, en-zh from MultiUN (Ziemski et al., 2016), en-hi from IITB (Kunchukuttan et al., 2018), and en-bg from both Europarl and EUbookshop. All datasets were downloaded from the OPUS3 website (Tiedemann, 2012). In our experiments, we vary the number of parallel sentence pairs for PPA. For each language, we take the first 250k, 600k, and 2M English-translation parallel sentence pairs except for those too short (where either sentence has less than 10 WordPiece tokens), or too long (where both sentences concatenated together have more than 128 WordPiece tokens). Table 1 shows the actual number of parallel pairs in each of our 250k, 600k, and 2M settings. 3.2 Evaluation Benchmarks XNLI is"
2021.naacl-main.20,N19-1423,0,0.0361924,"words and those of the corresponding words in other languages. Such post-hoc approach suffers from the limitations of word-alignment toolkits: (1) the noises from FastAlign can lead to error propagation to the rest of the pipeline; (2) FastAlign mainly creates the alignments with word-level translation and usually overlooks the contextual semantic compositions. As a result, the tuned mBERT is biased to shallow cross-lingual correspondence. Importantly, both approaches only involve word-level alignment tasks. Building on the success of monolingual pretrained language models (LM) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), their multilingual counterparts mBERT (Devlin et al., 2019) In this work, we focus on self-supervised, and XLM-R (Conneau et al., 2020) are trained alignment-oriented training tasks using minimum using the same objectives—Masked Language parallel data to improve mBERT’s cross-lingual Modeling (MLM) and in the case of mBERT, Next transferability. We propose a Post-Pretraining Sentence Prediction (NSP). MLM is applied to Alignment (PPA) method consisting of both wordmonolingual text that covers over 100 languages. level and sentence-level alignment, as well as a"
2021.naacl-main.20,2020.acl-main.653,0,0.0692157,"Missing"
2021.naacl-main.20,2021.ccl-1.108,0,0.0755651,"Missing"
2021.naacl-main.20,P19-1493,0,0.0443413,"Missing"
2021.naacl-main.20,D16-1264,0,0.0150857,"nd context ces pair can be augmented to two question-context pairs (qes , cen ) and (qen , ces ) with code-switching, resulting in 2x training data 2 . The same goes for XNLI with premises and hypotheses. The code-switching is always between English, and a target language. During training, we 2 The original question-context pair (qes , ces ) is not used for training as it did not help improve model performance in our experiments. 212 ensure the two augmented pairs appear in the same batch. 3 We focus on XLT in this work. For zero-shot crosslingual transfer, we train on the English SQuAD v1.1 (Rajpurkar et al., 2016) training set. For translate-train, we train on translation data provided in Hu et al. (2020) 5 Experimental Settings 3.1 Parallel Data for Post-Pretraining Parallel Data All parallel data we use involve English as the source language. Specifically, we collect en-fr, en-es, en-de parallel pairs from Europarl, en-ar, en-zh from MultiUN (Ziemski et al., 2016), en-hi from IITB (Kunchukuttan et al., 2018), and en-bg from both Europarl and EUbookshop. All datasets were downloaded from the OPUS3 website (Tiedemann, 2012). In our experiments, we vary the number of parallel sentence pairs for PPA. For"
2021.naacl-main.20,L16-1561,0,0.0320216,"raining as it did not help improve model performance in our experiments. 212 ensure the two augmented pairs appear in the same batch. 3 We focus on XLT in this work. For zero-shot crosslingual transfer, we train on the English SQuAD v1.1 (Rajpurkar et al., 2016) training set. For translate-train, we train on translation data provided in Hu et al. (2020) 5 Experimental Settings 3.1 Parallel Data for Post-Pretraining Parallel Data All parallel data we use involve English as the source language. Specifically, we collect en-fr, en-es, en-de parallel pairs from Europarl, en-ar, en-zh from MultiUN (Ziemski et al., 2016), en-hi from IITB (Kunchukuttan et al., 2018), and en-bg from both Europarl and EUbookshop. All datasets were downloaded from the OPUS3 website (Tiedemann, 2012). In our experiments, we vary the number of parallel sentence pairs for PPA. For each language, we take the first 250k, 600k, and 2M English-translation parallel sentence pairs except for those too short (where either sentence has less than 10 WordPiece tokens), or too long (where both sentences concatenated together have more than 128 WordPiece tokens). Table 1 shows the actual number of parallel pairs in each of our 250k, 600k, and 2"
2021.naacl-main.20,tiedemann-2012-parallel,0,0.0123076,"For zero-shot crosslingual transfer, we train on the English SQuAD v1.1 (Rajpurkar et al., 2016) training set. For translate-train, we train on translation data provided in Hu et al. (2020) 5 Experimental Settings 3.1 Parallel Data for Post-Pretraining Parallel Data All parallel data we use involve English as the source language. Specifically, we collect en-fr, en-es, en-de parallel pairs from Europarl, en-ar, en-zh from MultiUN (Ziemski et al., 2016), en-hi from IITB (Kunchukuttan et al., 2018), and en-bg from both Europarl and EUbookshop. All datasets were downloaded from the OPUS3 website (Tiedemann, 2012). In our experiments, we vary the number of parallel sentence pairs for PPA. For each language, we take the first 250k, 600k, and 2M English-translation parallel sentence pairs except for those too short (where either sentence has less than 10 WordPiece tokens), or too long (where both sentences concatenated together have more than 128 WordPiece tokens). Table 1 shows the actual number of parallel pairs in each of our 250k, 600k, and 2M settings. 3.2 Evaluation Benchmarks XNLI is an evaluation dataset for cross-lingual NLI that covers 15 languages. The dataset is human-translated from the deve"
2021.naacl-main.20,D19-1575,0,0.0395307,"Missing"
2021.naacl-main.20,N18-1101,0,0.0144323,"ntence pairs for PPA. For each language, we take the first 250k, 600k, and 2M English-translation parallel sentence pairs except for those too short (where either sentence has less than 10 WordPiece tokens), or too long (where both sentences concatenated together have more than 128 WordPiece tokens). Table 1 shows the actual number of parallel pairs in each of our 250k, 600k, and 2M settings. 3.2 Evaluation Benchmarks XNLI is an evaluation dataset for cross-lingual NLI that covers 15 languages. The dataset is human-translated from the development and test sets of the English MultiNLI dataset (Williams et al., 2018). Given a sentence pair of premise and hypothesis, the task is to classify their relationship as entailment, contradiction, and neutral. For zero-shot cross-lingual transfer, we train on the English MultiNLI training set, and apply the model to the test sets of the other languages. For translatetrain, we train on translation data that come with the dataset 4 . MLQA is an evaluation dataset for QA that covers seven languages. The dataset is derived from a three step process. (1) Parallel sentence mining from Wikipedia of the languages. (2) English question annotation and answer span annotation"
2021.naacl-main.20,D19-1077,0,0.0330427,"Missing"
C16-1164,D13-1160,0,0.766093,"entities as subject are then the fact search space for this question. CharCNN and word-CNN decompose each question-fact match into an entity-mention surface-form match and a predicate-pattern semantic match. Our approach has a simple architecture, but it outperforms the state-of-the-art, a system that has a much more complicated structure. 2 Related Work As mentioned in Section 1, factoid QA against Freebase can be categorized into single-relation QA and multi-relation QA. Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013). Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing th"
C16-1164,D14-1067,0,0.48883,"work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in"
C16-1164,P16-1076,0,0.692821,"m by an embedding-based QA system developed under the framework of Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015). The setting of the SimpleQA corresponds to the elementary operation of performing a single lookup in the memory. They investigate the performance of training on the combination of SimpleQuestions, WebQuestions and Reverb training sets. Golub and He (2016) propose a character-level attention-based encoder-decoder framework to encode the question and subsequently decode into (subject, predicate) tuple. Our model in this work is much simpler than these prior systems. Dai et al. (2016) combine a unified conditional probabilistic framework with deep recurrent neural networks and neural embeddings to get state-of-the-art performance. Treating SimpleQA as fact selection is inspired by work on answer selection (e.g., Yu et al. (2014), Yin et al. (2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given question. The answer candidates in those tasks are raw text, not structured information as facts in Freebase are. We are also inspired by work that generates natural language questions given knowledge graph facts (Seyler et al., 2015; Se"
C16-1164,P15-1026,0,0.370013,"form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word"
C16-1164,D11-1142,0,0.0153623,"h the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word is first preprocessed into a count vector of character-trigram vocabulary, then forwarded into the CNN as input. We treat entities and mentions as character sequences. Our char-CNN for entity-mention match is more end-to-end without data preprocessing. (ii)"
C16-1164,P13-1158,0,0.0602297,"eebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a question, such as “what’s the hometown of Obama?”, asks a single and direct topic of an entity. In this example, the entity is “Obama” and the topic is hometown. So our task is reduced to finding one fact (subject, predicate, object) in Freebase that answers the question, which roughly means the subject and predicate are the best matches for the topical entity “Obama” and for the topic description “what’s the hometown of”, respectively. Thus, we aim to des"
C16-1164,D16-1166,0,0.655535,"system, given a question, is asked to choose the best answer from a list of candidates. In this work, we formulate the SimpleQA task as a fact selection problem and the key issue lies in the system design for how to match a fact candidate to the question. The first obstacle is that Freebase has an overwhelming number of facts. A common and effective way is to first conduct entity linking of a question over Freebase, so that only a small subset of facts remain as candidates. Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). Then, facts whose subject entities match those n-grams are kept. Our first contribution in this work is to present a simple while effective entity linker ∗ 1 This work was conducted during the first author’s internship at IBM Watson Group. We release our entity linking results at: https://github.com/Gorov/SimpleQuestions-EntityLinking This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1746 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers"
C16-1164,P14-1090,0,0.301761,"Missing"
C16-1164,W14-2416,0,0.0221327,"Missing"
C16-1164,N15-3014,0,0.0599456,"c parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashin"
C16-1164,P14-2105,0,0.095584,"e based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a"
C16-1164,P15-1128,0,0.592067,"y linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a que"
C16-1164,W16-0103,1,0.856224,"Missing"
C16-1164,Q16-1019,1,0.0271245,"Missing"
C16-1164,N03-1033,0,\N,Missing
C16-1164,Q14-1002,1,\N,Missing
D12-1037,P08-1024,0,0.14643,"mprovements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due"
D12-1037,D08-1024,0,0.623546,"introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the d"
D12-1037,P05-1033,0,0.0879558,"r experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times using MERT, and then se406 Methods Global method Local method Steps Decoding Retrieval Local training Seconds 2.0 +0.6 +0.3 NIST02 NIST05 27.07 27.75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set used to tune λ for MBUU and EBUU, and NIST06 and NIST08 are test"
D12-1037,D11-1004,0,0.149234,"e global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are"
D12-1037,P10-1064,0,0.0295431,"Missing"
D12-1037,2005.eamt-1.19,0,0.0784023,"arns a weight for each sentence. Further, our translation framework integrates the training and testing into one unit, instead of treating them separately. One of the advantages is that it can adapt the weights for each of the test sentences. Our method resorts to some translation examples, which is similar as example-based translation or translation memory (Watanabe and Sumita, 2003; He et al., 2010; Ma et al., 2011). Instead of using translation examples to construct translation rules for enlarging the decoding space, we employed them 409 to discriminatively learn local weights. Similar to (Hildebrand et al., 2005; L¨u et al., 2007), our method also employes IR methods to retrieve examples for a given test set. Their methods utilize the retrieved examples to acquire translation model and can be seen as the adaptation of translation model. However, ours uses the retrieved examples to tune the weights and thus can be considered as the adaptation of tuning. Furthermore, since ours does not change the translation model which needs to run GIZA++ and it incrementally trains local weights, our method can be applied for online translation service. 7 Conclusion and Future Work This paper proposes a novel local"
D12-1037,D11-1125,0,0.403742,"or statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of th"
D12-1037,N03-1017,0,0.114511,"Missing"
D12-1037,P07-2045,0,0.014159,".75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EBUU) and a global method (MERT). NIST05 is the set used to tune λ for MBUU and EBUU, and NIST06 and NIST08 are test sets. + means the local method is significantly better than MERT with p &lt; 0.05. lect the best-performing one as our baseline for the following experiments. As Table 1 indicates, our baseline In-Hiero is comparable to the phrase-based MT (Moses) and the hierarchical phrase-based MT (Moses hier) implemented in Moses, an open source MT toolkit2 (Koehn et al., 2007). Both of these systems are with default setting. All three systems are trained by MERT with 100 best candidates. To compare the local training method in Algorithm 2, we use a standard global training method, MERT, as the baseline training method. We do not compare with Algorithm 1, in which retraining is performed for each input sentence, since retraining for the whole test set is impractical given that each sentence-wise retraining may take some hours or even days. Therefore, we just compare Algorithm 2 with MERT. 5.2 Runtime Results To run the Algorithm 2, we tune the baseline weight Wb on"
D12-1037,W04-3250,0,0.0660269,"h modified Kneser-Ney smoothing (Chen and Goodrj . Due to the existence of L2 norm in objective man, 1998). In our experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times using MERT, and then se406 Methods Global method Local method Steps Decoding Retrieval Local training Seconds 2.0 +0.6 +0.3 NIST02 NIST05 27.07 27.75+ 27.85+ NIST06 26.32 27.88+ 27.99+ NIST08 19.03 20.84+ 21.08+ Table 3: The performance comparison of local training methods (MBUU and EB"
D12-1037,C10-1075,0,0.091706,"009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipeline. Firstly, on the document level, the performance of these methods is dependent on the choice of a development set, which may potentially lead to an unstable translation performance for testing. As referred in our experiment, the BLEU points on NIST08 are 402 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 402–411, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics  1, 0  h ( f 1 , e11 )  h ( f 1 , e12 )  2,0  h("
D12-1037,D07-1036,0,0.0396223,"Missing"
D12-1037,P11-1124,0,0.197989,"Missing"
D12-1037,P00-1056,0,0.0922694,"Missing"
D12-1037,P02-1038,0,0.282749,"method to address these two problems. Unlike a global training method, such as MERT, in which a single weight is learned and used for all the input sentences, we perform training and testing in one step by learning a sentencewise weight for each input sentence. We propose efficient incremental training methods to put the local training into practice. In NIST Chinese-to-English translation tasks, our local training method significantly outperforms MERT with the maximal improvements up to 2.0 BLEU points, meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007;"
D12-1037,P03-1021,0,0.66567,"meanwhile its efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and un"
D12-1037,P02-1040,0,0.0913028,"e eˆ(fj ; W ) is defined in Equation (1), and tence pair. We train a 4-gram language model on Error(rj , e) is the sentence-wise minus BLEU (Pa- the Xinhua portion of the English Gigaword corpineni et al., 2002) of a candidate e with respect to pus using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodrj . Due to the existence of L2 norm in objective man, 1998). In our experiments the translation perfunction (5), the optimization algorithm MERT can formances are measured by case-insensitive BLEU4 not be applied for this question since the exact line metric (Papineni et al., 2002) and we use mtevalsearch routine does not hold here. Motivated by v13a.pl as the evaluation tool. The significance test(Och, 2003; Smith and Eisner, 2006), we approxi- ing is performed by paired bootstrap re-sampling mate the Error in (5) by the expected loss, and then (Koehn, 2004). We use an in-house developed hierarchical derive the following function: phrase-based translation (Chiang, 2005) as our baseK λ XX 1 2 kW −Wb k + Error(rj ; e)Pα (e|fj ; W ), line system, and we denote it as In-Hiero. To ob2 K tain satisfactory baseline performance, we tune Inj=1 e (6) Hiero system for 5 times usi"
D12-1037,D09-1147,0,0.080957,"arable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li"
D12-1037,2003.mtsummit-papers.54,1,0.859025,"nd the incremental training in line 5 of Algorithm 2. 3 Acquiring Training Examples In line 4 of Algorithm 2, to retrieve training examples for the sentence ti , we first need a metric to retrieve similar translation examples. We assume that the metric satisfy the property: more similar the test sentence and translation examples are, the better translation result one obtains when decoding the test sentence with the weight trained on the translation examples. The metric we consider here is derived from an example-based machine translation. To retrieve translation examples for a test sentence, (Watanabe and Sumita, 2003) defined a metric based on the combination of edit distance and TF-IDF (Manning and Sch¨utze, 1999) as follows: dist(f1 , f2 ) = θ × edit-dist(f1 , f2 )+ (1 − θ) × tf-idf(f1 , f2 ), (2) where θ(0 ≤ θ ≤ 1) is an interpolation weight, fi (i = 1, 2) is a word sequence and can be also considered as a document. In this paper, we extract similar examples from training data. Like examplebased translation in which similar source sentences have similar translations, we assume that the optimal translation weights of the similar source sentences are closer. 4 Incremental Training Based on Ultraconservati"
D12-1037,D07-1080,1,0.964911,"tion Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of source sentences(Li et al., 2010), there are some shortcomings in this pipel"
D12-1037,N09-2006,0,0.20149,"ts efficiency is comparable to that of the global method. 1 (1) e Introduction Och and Ney (2002) introduced the log-linear model for statistical machine translation (SMT), in which translation is considered as the following optimization problem: where f and e (e0 ) are source and target sentences, respectively. h is a feature vector which is scaled by a weight W . Parameter estimation is one of the most important components in SMT, and various training methods have been proposed to tune W . Some methods are based on likelihood (Och and Ney, 2002; Blunsom et al., 2008), error rate (Och, 2003; Zhao and Chen, 2009; Pauls et al., 2009; Galley and Quirk, 2011), margin (Watanabe et al., 2007; Chiang et al., 2008) and ranking (Hopkins and May, 2011), and among which minimum error rate training (MERT) (Och, 2003) is the most popular one. All these training methods follow the same pipeline: they train only a single weight on a given development set, and then use it to translate all the sentences in a test set. We call them a global training method. One of its advantages is that it allows us to train a single weight offline and thereby it is efficient. However, due to the diversity and uneven distribution of"
D12-1037,C08-1074,0,\N,Missing
D15-1205,Q14-1043,0,0.0267622,"ieve such specialization in a more general fashion: 1. Enhancing Compositional Models with Features. A recent trend enhances compositional models with annotation features. Such an approach has been shown to significantly improve over pure compositional models. For example, Hermann et al. (2014) and Nguyen and Grishman (2014) gave different weights to words with different syntactic context types or to entity head words with different argument IDs. Zeng et al. (2014) use concatenations of embeddings as features in a CNN model, according to their positions relative to the target entity mentions. Belinkov et al. (2014) enrich embeddings with linguistic features before feeding them forward to a RNN model. Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced RNN models by refining the transformation matrices with phrase types and CCG super tags. 2. Engineering of Embedding Features. A different approach to combining traditional linguistic features and embeddings is hand-engineering features with word embeddings and adding them to log-linear models. Such approaches have achieved state-of-the-art results in many tasks including NER, chunking, dependency parsing, semantic role labeling, and relation ext"
D15-1205,W06-1670,0,0.020732,"Missing"
D15-1205,P15-1061,0,0.0537127,"our FCM can easily utilize these features without changing the model structures. In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015), which share similar motivations to the usage of On-path features in our work. Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification. 9 Conclusion We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word embeddings. Compared to existing compositional models, FCM can easily handle arbitrary types of input and handle global information for composition, while remaining easy to implement. We have demonstrated that FCM alone attains near state-of-the-art performances on several relation extraction tasks, and in combination with traditional feature based loglinear models it obtains state-of-the-art results"
D15-1205,K15-1027,0,0.267326,"2005 (Walker et al., 2006) and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al., 2010). Contributions This paper makes several contributions, including: 1. We introduce the FCM, a new compositional embedding model for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the"
D15-1205,W09-2415,0,0.223175,"Missing"
D15-1205,P13-1088,0,0.0209533,"al models with annotation features. Such an approach has been shown to significantly improve over pure compositional models. For example, Hermann et al. (2014) and Nguyen and Grishman (2014) gave different weights to words with different syntactic context types or to entity head words with different argument IDs. Zeng et al. (2014) use concatenations of embeddings as features in a CNN model, according to their positions relative to the target entity mentions. Belinkov et al. (2014) enrich embeddings with linguistic features before feeding them forward to a RNN model. Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced RNN models by refining the transformation matrices with phrase types and CCG super tags. 2. Engineering of Embedding Features. A different approach to combining traditional linguistic features and embeddings is hand-engineering features with word embeddings and adding them to log-linear models. Such approaches have achieved state-of-the-art results in many tasks including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013)."
D15-1205,P14-1136,0,0.0677707,"we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, we begin with a precise construction of compositional embeddings using word embeddings in conjunction with unlexicalized features. Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) are cap2 Such embeddings have a long history in NLP, including term-document frequency matrices and their lowdimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF), Brown clusters, random projections and vector space models. Recently, neural networks / deep learning have provided several popular methods for obtaining such embeddings. 1774 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1774–1784, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Class (1) AR"
D15-1205,P08-1068,0,0.0321661,".com/mgormley/pacaya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structur"
D15-1205,P14-1038,0,0.154946,"Missing"
D15-1205,P15-2047,0,0.0353483,"l., 2011; Plank and Moschitti, 2013). Roth and Woodsend (2014) considered features similar to ours for semantic role labeling. However, in prior work both of above approaches are only able to utilize limited information, usually one property for each word. Yet there may be different useful properties of a word which can contribute to the performances of the task. By contrast, our FCM can easily utilize these features without changing the model structures. In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015), which share similar motivations to the usage of On-path features in our work. Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification. 9 Conclusion We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word embeddings. Compared"
D15-1205,P15-2029,0,0.0226376,"d, 2014; Sun et al., 2011; Plank and Moschitti, 2013). Roth and Woodsend (2014) considered features similar to ours for semantic role labeling. However, in prior work both of above approaches are only able to utilize limited information, usually one property for each word. Yet there may be different useful properties of a word which can contribute to the performances of the task. By contrast, our FCM can easily utilize these features without changing the model structures. In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015), which share similar motivations to the usage of On-path features in our work. Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification. 9 Conclusion We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word em"
D15-1205,P14-5010,0,0.0084969,"Missing"
D15-1205,N04-1043,0,0.0670003,"and Yu contributed equally. https://github.com/mgormley/pacaya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embedd"
D15-1205,P14-2012,0,0.110196,"guistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, we begin with a precise const"
D15-1205,W15-1506,0,0.235351,") and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al., 2010). Contributions This paper makes several contributions, including: 1. We introduce the FCM, a new compositional embedding model for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the goal of identifying, for all"
D15-1205,P15-1062,0,0.0617909,"is artificially reduced. Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014). For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015). Other work has as1775 sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015). The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions. We are the first to apply neural language model embeddings to this task. Motivation and Examples Whether a word is indicative of a relation depends on multiple properties, which may relate to its context within the sentence. F"
D15-1205,P10-1040,0,0.791123,"ually. https://github.com/mgormley/pacaya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary l"
D15-1205,P13-1147,0,0.178163,"ith aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, w"
D15-1205,Q15-1017,1,0.853446,"ompositional embedding model for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the goal of identifying, for all pairs of entity mentions, what relation exists between them, if any. For each pair of entity mentions in a sentence S, we construct an instance (y, x), where x = (M1 , M2 , S, A). S ="
D15-1205,S10-1057,0,0.357117,"Missing"
D15-1205,D14-1045,0,0.293506,"ya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand c"
D15-1205,D12-1110,0,0.882678,"M1 ) are different relations. Table 1 shows ACE 2005 relations, and has a strong label bias towards negative examples. We also consider the task of relation classification (SemEval), where the number of negative examples is artificially reduced. Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014). For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015). Other work has as1775 sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015). The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions. We"
D15-1205,N15-1155,1,0.49156,"del for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the goal of identifying, for all pairs of entity mentions, what relation exists between them, if any. For each pair of entity mentions in a sentence S, we construct an instance (y, x), where x = (M1 , M2 , S, A). S = {w1 , w2 , ..., wn }"
D15-1205,C14-1220,0,0.307454,"elations. Table 1 shows ACE 2005 relations, and has a strong label bias towards negative examples. We also consider the task of relation classification (SemEval), where the number of negative examples is artificially reduced. Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014). For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015). Other work has as1775 sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015). The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions. We are the first to app"
D15-1205,P05-1053,0,0.367173,"several methods. (1) FCM these embeddings from an neural language model in isolation without fine-tuning. (2) FCM in isolaand then fine-tune them for our supervised task. tion with fine-tuning (i.e. trained as a log-bilinear The training process for the hybrid model (§ 4) 7 is also easily done by backpropagation since each Obtained from the constituency parse using the CONLL sub-model has separate parameters. 2000 chunking converter (Perl script). 1778 model). (3) A log-linear model with a rich binary feature set from Sun et al. (2011) (Baseline)— this consists of all the baseline features of Zhou et al. (2005) plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research. We exclude the Country gazetteer and WordNet features from Zhou et al. (2005). The two remaining methods are hybrid models that integrate FCM as a submodel within the log-linear model (§ 4). We consider two combinations. (4) The feature set of Nguyen and Grishman (2014) obtained by using the embeddings of heads of two entity mentions (+HeadOnly). (5) Our full FCM model (+FCM). All models use L2 regularization tuned on dev data. 6.1 Datasets and Evaluation ACE"
D15-1205,P13-1045,0,0.188701,"et al. (2015). 8 Related Work Compositional Models for Sentences In order to build a representation (embedding) for a sentence based on its component word embeddings and structural information, recent work on compositional models (stemming from the deep learning community) has designed model structures that mimic the structure of the input. For example, these models could take into account the order of the words (as in Convolutional Neural Networks (CNNs)) (Collobert et al., 2011) or build off of an input tree (as in Recursive Neural Networks (RNNs) or the Semantic Matching Energy Function) (Socher et al., 2013b; Bordes et al., 2012). While these models work well on sentence-level representations, the nature of their designs also limits them to fixed types of substructures from the annotated sentence, such as chains for CNNs and trees for RNNs. Such models cannot capture arbitrary combinations of linguistic annotations available for a given task, such as word order, dependency tree, and named entities used for relation extraction. Moreover, these approaches ignore the differences in functions between words appearing in different roles. This does not suit more general substructure labeling tasks in N"
D15-1205,D13-1170,0,0.0209519,"et al. (2015). 8 Related Work Compositional Models for Sentences In order to build a representation (embedding) for a sentence based on its component word embeddings and structural information, recent work on compositional models (stemming from the deep learning community) has designed model structures that mimic the structure of the input. For example, these models could take into account the order of the words (as in Convolutional Neural Networks (CNNs)) (Collobert et al., 2011) or build off of an input tree (as in Recursive Neural Networks (RNNs) or the Semantic Matching Energy Function) (Socher et al., 2013b; Bordes et al., 2012). While these models work well on sentence-level representations, the nature of their designs also limits them to fixed types of substructures from the annotated sentence, such as chains for CNNs and trees for RNNs. Such models cannot capture arbitrary combinations of linguistic annotations available for a given task, such as word order, dependency tree, and named entities used for relation extraction. Moreover, these approaches ignore the differences in functions between words appearing in different roles. This does not suit more general substructure labeling tasks in N"
D15-1205,P11-1053,0,0.494673,"ogical features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. I"
D15-1205,D12-1042,0,0.300334,"Missing"
D15-1205,P10-1030,0,\N,Missing
D15-1205,P14-1130,0,\N,Missing
D15-1205,P14-1063,0,\N,Missing
D16-1223,H94-1010,0,0.121505,"tead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and evaluation data of"
D16-1223,K16-1028,1,0.595295,"Missing"
D16-1223,H90-1020,0,0.139898,"smaller. Instead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and"
D16-1223,P06-2113,0,0.0276978,"ed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and evaluation data of 893 sentences. The"
D16-1223,P15-1109,0,0.0100616,"ummarization (Nallapati et al., 2016) and so on. The difference is that the proposed encoder-labeler LSTM accepts the same input sequence twice while the usual encoder-decoder LSTM accepts the input sequence once in the encoder. Note that the LSTMs for encoding and labeling are different in the encoder-labeler LSTM, but the same word embedding matrix is used both for the encoder and labeler since the same input sequence is fed twice. 2.4 Related Work on Considering Sentence-level Information Bi-directional RNN/LSTM have been proposed to capture sentence-level information (Mesnil et al., 2015; Zhou and Xu, 2015; Vu et al., 2016). While the bi-directional RNN/LSTM model the preceding and succeeding contexts at a specific word and O O O O O O B-ToCity I need a ticket to Seattle O &lt;B&gt; O I (a) Labeler LSTM(W). O O need O O O a B-ToCity O ticket O to Seattle to ticket a need I O O &lt;B&gt; O Encoder (backward) LSTM Seattle (b) Labeler LSTM(W+L). O I need O O O B-ToCity to ticket a need I Encoder LSTM (backward) a ticket to Seattle Labeler LSTM(W) to ticket a need I O &lt;B&gt; B-ToCity O O O O O O I Encoder LSTM (backward) (d) Encoder-labeler LSTM(W). O (c) Encoder-decoder LSTM. Seattle Seattle O Decoder LSTM O O O"
D18-1083,P02-1040,0,0.108335,"Missing"
D18-1083,W05-0909,0,0.359463,"Missing"
D18-1083,P82-1020,0,0.783379,"Missing"
D18-1083,W04-1013,0,0.0305857,"Missing"
D18-1110,P11-1060,0,0.107614,"Missing"
D18-1110,P14-5010,0,0.00409275,", and tuned on the development set for ATIS. The learning rate is set to 0.001. The decoder has 1 layer, and its hidden state size is 300. The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting. We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300. For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized. We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees. Results and Discussion. Table 1 summarizes the results of our model and existing semantic parsers on three datasets. Our model achieves competitive performance on Jobs640, ATIS and Geo880. Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in Dong and Lapata (2016), which only uses limited syntactic information. Comparison with Baseline. To better demonstrate that our work is an effective way to utilize both multiple trees and the word sequence for semantic"
D18-1110,D14-1162,0,0.0832142,"standard train/development/test split as previous works, and the logical form accuracy as our evaluation metric. The model is trained using the Adam optimizer (Kingma and Ba, 2014), with mini-batch size 30. Our hyper-parameters are cross-validated on the training set for Jobs640 and Geo880, and tuned on the development set for ATIS. The learning rate is set to 0.001. The decoder has 1 layer, and its hidden state size is 300. The dropout strategy (Srivastava et al., 2014) with the ratio of 0.5 is applied at the decoder layer to avoid overfitting. We is initialized using GloVe word vectors from Pennington et al. (2014) and the dimension of word embedding is 300. For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized. We use the Stanford CoreNLP tool (Manning et al., 2014) to generate the dependency and constituent trees. Results and Discussion. Table 1 summarizes the results of our model and existing semantic parsers on three datasets. Our model achieves competitive performance on Jobs640, ATIS and Geo880. Our work is the first to use both multiple trees and the word sequence fo"
D18-1110,P16-1004,0,0.418197,"form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information. 1 Introduction The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse. However, the syntactic features capture important"
D18-1110,J08-2005,0,0.0433929,"der + Dep + Cons Accori 84.8 83.5 82.9 84.0 85.2 84.9 86.0 Accpara 78.7 80.1 77.3 80.7 82.3 79.9 83.5 Related Work Existing works of generating text representation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su"
D18-1110,P17-1105,0,0.045337,"Missing"
D18-1110,D15-1205,1,0.917794,"Missing"
D18-1110,P18-1124,0,0.0396135,"s, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018). 6 Diff. -6.1 -3.4 -5.6 -3.3 -2.9 -5.0 -2.5 Conclusions Existing neural semantic parsers mainly leverage word order features while neglecting other valuable syntactic information. To address this, we propose to build a syntactic graph which represents three types of syntactic information, and further apply a novel graph-to-sequence model to map the syntactic graph to a logical form. Experimental results show that the robustness of our model is improved due to the incorporating more aspects of syntactic information, and our model outperforms previous semantic parsing systems. Table 3: Evaluati"
D18-1110,D17-1009,0,0.058631,"Missing"
D18-1110,P16-1002,0,0.0205339,"and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information. 1 Introduction The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse. However, the syntactic feat"
D18-1110,D17-1127,0,0.0233625,"Missing"
D18-1110,D17-1160,0,0.0118697,"ults on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information. 1 Introduction The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural semantic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs (Jia and Liang, 2016; Dong and Lapata, 2016; Krishnamurthy et al., 2017). All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as sequence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse. However, the syntactic features capture important structural information of the"
D18-1110,P15-1150,0,0.0594178,"d sequence for semantic parsing, we compare with an addiExperiments We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings; 920 could correctly predict these logical forms while the model that only uses word order features may fail. tional straightforward baseline method (referred as BASELINE in Table 1). To deal with the graph input, the BASELINE decomposes the graph embedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embeddings, Wseq ; (2) two TreeLSTMs (Tai et al., 2015) to extract the dependency tree and constituency features while taking Wseq as initial word embeddings. The resulted word embeddings and nonterminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder. Complicated Query & Predicted Logical Forms Jobs Q: what are the jobs for programmer that has salary 50000 that uses c++ and not related with AI Pred: answer(J,(job(J),-((area(J,R),const(R,’ai’))), language(J,L),const(L,’c++’), title(J,P), const(P,’Programmer’),salary greater than(J, 50000,year)))). Geo Q: which is the density of the state that the largest river in the united s"
D18-1110,D13-1161,0,0.0445237,"Missing"
D18-1110,D14-1135,0,0.0276615,"Missing"
D18-1110,D11-1140,0,0.0608927,"Missing"
D18-1110,D18-1482,1,0.874088,"Missing"
D18-1110,P16-1220,1,0.856177,"epresentation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and paraphrasing for semantic parsing (Su and Yan, 2017; Gur et al., 2018). 6 Diff. -6.1 -3.4 -5.6 -3.3 -2.9 -5.0 -2.5 Conclusions Existing neural semantic parsers mainly leverage wor"
D18-1110,D07-1071,0,0.124847,"Missing"
D18-1110,P18-1030,0,0.101787,"t experiment, we still train the model on Devori and evaluate it on the newly created dataset. Feature Word Order Dep Cons Dep + Cons Word Order + Dep Word Order + Cons Word Order + Dep + Cons Accori 84.8 83.5 82.9 84.0 85.2 84.9 86.0 Accpara 78.7 80.1 77.3 80.7 82.3 79.9 83.5 Related Work Existing works of generating text representation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embeddings of word sequences (Wu et al., 2018a; Arora et al., 2017), or building Bi-directional LSTMs over the text (Zhang et al., 2018). These methods neglect other syntactic information, which, however, has been proved to be useful in shallow semantic parsing, e.g., semantic role labeling (Punyakanok et al., 2008). To address this, recent works attempt to incorporate these syntactic information into the text representation. For example, Xu et al. (2016) builds separated neural networks for different types of syntactic annotation. Gormley et al. (2015); Wu et al. (2018b) decompose a graph to simpler sub-graphs and embed these subgraphs independently. Our approach, compared to the above methods, provided a unified solution to"
D18-1110,N15-1162,0,0.0287305,"Missing"
D18-1216,Q17-1010,0,0.0348146,"the ability to transfer, as their encoders are both initialized from enc, which has been trained on source data and unlabeled target data. Oracle We also report the performance of an O RACLE which shares the same architecture as ours but is supervised by the oracle attention. The oracle attention is derived from a held-out dataset with large-scale annotations for the target task (see Appendix 3 for details). This helps us analyze the contribution of our R2A approach in isolation of the inherent limitations of the target tasks. 4.3 Implementation details We use pre-trained fastText embeddings (Bojanowski et al., 2017), a 200-dimension bidirectional LSTM (Hochreiter and Schmidhuber, 1997) for the language encoder, and a 50dimension bi-directional LSTM for the R2A encoder. Dropout (Srivastava et al., 2014) is applied with drop rate 0.1 on the word embeddings and the last hidden layers of the classifiers. All 1908 Source Target S VM R A -S VM‡ R A -C NN‡ T RANS† R A -T RANS‡† O URS‡† O RACLE† Beer aroma+palate Beer look 74.41 74.83 74.94 72.75 76.41 79.53 80.29 Beer look+palate Beer aroma 68.57 69.23 67.55 69.92 76.45 77.94 78.11 Beer look+aroma Beer palate 63.88 67.82 65.72 74.66 73.40 75.24 75.50 Table 3: A"
D18-1216,P17-1171,0,0.0298985,"ve analyses confirm that our R2A model is capable of generating high-quality attention for target tasks. 2 In this paper, we consider a more general setting where one domain contains multiple tasks. Also, we assume having one source domain. However, our proposed method is a general framework and can be easily adapted to problems with multiple source domains. 2 Related Work Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contra"
D18-1216,D17-1070,0,0.0329439,"guide the sentence-level attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we discern the intrinsic difference between human rationales and machine attention. Moreover, we learn a model to map human rationales into high-quality attention so as to provide a richer supervision for low-resource models. Transfer learning When labeled data on the target task is available, existing approaches typically transfer the knowledge by either fine-tuning an encoder trained on the source tasks(s) (Conneau et al., 2017; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zh"
D18-1216,I17-2002,0,0.0288353,"., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications. Rationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale words are removed from the inputs, and validate this idea for linear models. Recent work (Zhang et al., 2016) explores the potential of integrating rationales with more complex neural classifiers. In th"
D18-1216,D14-1181,0,0.00806323,"Missing"
D18-1216,D16-1011,1,0.909188,"Missing"
D18-1216,C16-1291,0,0.0176167,"to problems with multiple source domains. 2 Related Work Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications. Rationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale"
D18-1216,P17-1164,0,0.0877057,"Missing"
D18-1216,D15-1166,0,0.158164,"Missing"
D18-1216,N18-1202,0,0.0217746,"el attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we discern the intrinsic difference between human rationales and machine attention. Moreover, we learn a model to map human rationales into high-quality attention so as to provide a richer supervision for low-resource models. Transfer learning When labeled data on the target task is available, existing approaches typically transfer the knowledge by either fine-tuning an encoder trained on the source tasks(s) (Conneau et al., 2017; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin"
D18-1216,D15-1044,0,0.118907,"Missing"
D18-1216,N16-1174,0,0.0741901,"tion, both qualitative and quantitative analyses confirm that our R2A model is capable of generating high-quality attention for target tasks. 2 In this paper, we consider a more general setting where one domain contains multiple tasks. Also, we assume having one source domain. However, our proposed method is a general framework and can be easily adapted to problems with multiple source domains. 2 Related Work Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data (Bahdanau et al., 2014; Luong et al., 2015; Rush et al., 2015; Yang et al., 2016; Lin et al., 2017; Chen et al., 2017; Vaswani et al., 2017). In this setting, typically no additional supervision is required for learning the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailore"
D18-1216,N07-1033,0,0.810389,"ning attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation (Liu et al., 2016), employing argument words to supervise attention in event detection (Liu et al., 2017), utilizing linguisticallymotivated annotations to guide attention in constituency parsing (Kamigaito et al., 2017). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications. Rationale-based models Zaidan et al. (2007) was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale words are removed from the inputs, and validate this idea for linear models. Recent work (Zhang et al., 2016) explores the potential of integrating rationales with more complex neural classifiers. In their model, human rationales are directly used to guide the sentence-level attention for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we disc"
D18-1216,D16-1076,0,0.0761291,"ne distance: (2) where λTatt controls the importance of LTatt . For better transfer, we initialize the encoder in the target classifier as enc from the trained R2A model. 4 r2a i) exp(h˜ uti , qatt α ˆ it = P t r2a i) , uj , qatt j exp(h˜ Pipeline Datasets We evaluate our approach on two transfer settings: transfer among aspects within the same domain and transfer among different domains. Aspect transfer We first consider the transfer problem between multiple aspects of one domain: beer review. We use a subset of the BeerAdvocate3 review dataset (McAuley et al., 2012) introduced by Lei et al. (2016). This dataset contains reviews with ratings (in the scale of [0, 1]) from three aspects of the beer: look, aroma and palate. We treat d(a, b) , max(0, 1 − cos(a, b) − 0.1), 3 1907 https://www.beeradvocate.com Beer Aspects Source Train Source Dev Target Train‡ Target Dev Target Test Look Aroma Palate 43,351 39,825 30,041 10,170 8,772 7,152 200 200 200 200 200 200 4,014 4,212 3,804 Basic classifier We train a linear S VM using bag-of-ngrams representation on the labeled target data. We combine uni-gram, bi-grams, and trigrams as features and use tf-idf weighting. Table 1: Statistics of the beer"
D18-1216,Q17-1036,1,0.823822,"learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin et al., 2016; Zhang et al., 2017). In contrast, our model adapts the mapping from rationales to attention; thus after training, it can be applied to different target tasks. labeled source data with rationales R2A unlabeled target data Step 2: R2A inference labeled target data with rationales R2A labeled target data with R2A-generated attention Step 3: Training target classifier 3 Method Problem formulation We assume that we have N source tasks {Si }N i=1 , where each of them has sufficient amounts of labeled examples. Using existing methods (Lei et al., 2016), we can generate rationales for each source example automatically ("
D18-1216,P16-1031,0,0.0273339,"17; Peters et al., 2018) or multi-task learning on all tasks with a shared encoder (Collobert et al., 2011). In this paper, we explore the transferability of the task-specific attention through human rationales. We believe this will further assist learning in low-resource scenarios. Our work is also related to unsupervised domain 1904 Step 1: Training R2A adaptation, as the R2A model has never seen any target annotations during training. Existing methods commonly adapt the classifier by aligning the representations between the source and target domains (Glorot et al., 2011; Chen et al., 2012; Zhou et al., 2016; Ganin et al., 2016; Zhang et al., 2017). In contrast, our model adapts the mapping from rationales to attention; thus after training, it can be applied to different target tasks. labeled source data with rationales R2A unlabeled target data Step 2: R2A inference labeled target data with rationales R2A labeled target data with R2A-generated attention Step 3: Training target classifier 3 Method Problem formulation We assume that we have N source tasks {Si }N i=1 , where each of them has sufficient amounts of labeled examples. Using existing methods (Lei et al., 2016), we can generate rationale"
D18-1223,N18-1165,1,0.856653,"redicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as shown in Figure 1, a large portion of KG relations are ac"
D18-1223,D15-1082,0,0.0368976,"Missing"
D18-1223,N13-1095,0,0.0328599,"ation frequencies in Wikidata. There are a large portion of relations that only have a few triples. Introduction Large-scale knowledge graphs (Suchanek et al., 2007; Vrandeˇci´c and Kr¨otzsch, 2014; Bollacker et al., 2008; Auer et al., 2007; Carlson et al., 2010) represent every piece of information as binary relationships between entities, usually in the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new f"
D18-1223,P15-1016,0,0.0904123,"etween entities, usually in the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as"
D18-1223,P18-1150,0,0.0606118,"Missing"
D18-1223,D17-1060,1,0.894664,"n the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as shown in Figure 1,"
D18-1223,N18-1109,1,0.901118,"ot pay attention to those sparse symbols. More recently, several models (Shi and Weninger, 2017; Xie et al., 2016) have been proposed to handle unseen entities by leveraging text descriptions. In contrast to these approaches, our model deals with long-tail or newly added relations and focuses on one-shot relational learning without any external information, such as text descriptions of entities or relations. Few-Shot Learning Recent deep learning based few-shot learning approaches fall into two main categories: (1) metric based approaches (Koch, 2015; Vinyals et al., 2016; Snell et al., 2017; Yu et al., 2018), which try to learn generalizable metrics and the corresponding matching functions from a set of training tasks. Most methods in this class adopt the general matching framework proposed in deep siamese network (Koch, 2015). One example is the Matching Networks (Vinyals et al., 2016), which make predictions by comparing the input example with a small labeled support set; (2) meta-learner based approaches (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017; Li et al., 2017), which aim to learn the optimization of model parameters (by either outputting the parameter updates or"
D18-1223,D15-1174,0,0.345211,"entity t given the head entity and the query relation: (h, r, ?). As our purpose is to infer unseen facts for newly added or existing long-tail relations, we focus on the latter case. In contrast to previous work that usually assumes enough triples for the query relation are available for training, this work studies the case where only one training triple is available. To be more specific, the goal is to rank the true tail entity ttrue higher than other candidate entities t ∈ Ch,r , given only an example triple (h0 , r, t0 ). The candidates set is constructed using the entity type constraint (Toutanova et al., 2015). It is also worth noting that when we predict new facts of the relation r, we only consider a closed set of entities, i.e. no unseen entities during testing. For open-world settings where new entities might appear during testing, external information such as text descriptions about these entities are usually required and we leave this to future work. 3.2 One-Shot Learning Settings This section describes the settings for the training and evaluation of our one-shot learning model. The goal of our work is to learn a metric that could be used to predict new facts with oneshot examples. Following"
D19-1020,D17-1209,0,0.142049,"endency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary informati"
D19-1020,P18-1026,0,0.0260887,"nce. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative i"
D19-1020,H05-1091,0,0.867277,"the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (a) nmod comp comp nmod comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (b) Figure 1: (a) 1-best dependency tree and (b) dependency forest for a medical-domain sentence, where edge label “comp” represents “compound”. Associated mentions are in different colors. Some irrelevant words and edges are omitted for simplicity. Previous work has shown that dependency syntax is important for guiding relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Liu et al., 2015; Gormley et al., 2015; Xu et al., 2015a,b; Miwa and Bansal, 2016; Zhang et al., 2018b), especially in biological and medical domains (Quirk and Poon, 2017; Peng et al., 2017; Song et al., 2018b). Compared with sequential surface-level structures, such as POS tags, dependency trees help to model word-toword relations more easily by drawing direct connections between distant words that are syntactically correlated. Take the phrase “effect on the medicine” for example; “effect” and “medicine” are directly connected in a dependency tree, regardless of how many modifiers are adde"
D19-1020,W11-2905,0,0.0727527,"Missing"
D19-1020,W11-0216,0,0.0751446,"Missing"
D19-1020,C96-1058,0,0.0517257,"w recall given an imperfect parser. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the fi"
D19-1020,D15-1205,1,0.779495,"Missing"
D19-1020,P19-1024,0,0.154261,"Missing"
D19-1020,W09-2415,0,0.0207045,"Missing"
D19-1020,W05-1506,0,0.102938,"r. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the final K-bests by combining identical dependen"
D19-1020,D15-1137,0,0.031279,"2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semant"
D19-1020,I05-1006,0,0.184,"Missing"
D19-1020,Q17-1029,1,0.895317,"Missing"
D19-1020,P15-2047,0,0.103796,"Missing"
D19-1020,D11-1149,0,0.0191059,"ms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine"
D19-1020,P18-1249,0,0.0333378,"Missing"
D19-1020,D17-1159,0,0.500712,"iment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of"
D19-1020,J93-2004,0,0.064101,"able 1: Statistics on forests generated with various (upper half) and K (lower half) on the development set. • D EP T REE: Our baseline using 1-best dependency trees, as shown in Section 4. 7.4 • E DGEWISE PS and E DGEWISE: Our models using the forests generated by our E DGEWISE algorithm with or without parser scores. • KB EST E ISNER PS and KB EST E ISNER: Our model using the forests generated by our KB EST E ISNER algorithm with or without parser scores, respectively. 7.3 Settings We take a state-of-the-art deep biaffine parser (Dozat and Manning, 2017), trained on the Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) converted to Universal Dependency, to obtain 1-best trees and full search spaces for generating forests. Using standard PTB data split (02–21 for training, 22 for development and 23 for testing), it gives UAS and LAS scores of 95.7 and 94.6, respectively. For the other hyper-parameters, word embeddings are initialized with the 200-dimensional BioASQ vectors5 , pretrained on 10M abstracts of biomedical articles, and are fixed during training. The dimension of hidden vectors in Bi-LSTM is set to 200, and the number of message passing steps T is set to 2 based on Zhang et al. (2018b). We use Ada"
D19-1020,P08-2026,0,0.0861827,"Missing"
D19-1020,W16-3009,0,0.0314234,"Missing"
D19-1020,C10-1123,0,0.0261185,"ature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering"
D19-1020,P08-1023,0,0.0537829,"t al., 2017) and a recent dataset focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees f"
D19-1020,N18-1080,0,0.246962,"present an edge for simplicity, and p✏ is the parser probability for edge ✏. The edge probabilities are not adjusted during end-task training. 6 Training Relation loss Given a set of training instances, each containing a sentence s with two target mentions ⇠ and ⇣, and a dependency structure D (tree or forest), we train our models with a crossentropy loss between the gold-standard relations r and model distribution: lR = log p(r|s, ⇠, ⇣, D; ✓), (13) where ✓ represents the model parameters. Using additional NER loss For training on BioCreative VI CPR, we follow previous work (Liu et al., 2017; Verga et al., 2018) to take NER loss as additional supervision, though the mention boundaries are known during testing. lN ER = N 1 X log p(tn |s, D; ✓), N (14) n=1 where tn is the gold NE tag of wn with the “BIO” scheme. Both losses are conditionally independent given the deep features produced by our Experiments We conduct experiments on two medical benchmarks to test the usefulness of dependency forest. 7.1 Data BioCreative VI CPR (Krallinger et al., 2017) This task2 focuses on the relations between chemical compounds (such as drugs) and proteins (such as genes). The full corpus contains 1020, 612 and 800 ext"
D19-1020,P16-1105,0,0.0839479,"Missing"
D19-1020,Q17-1008,0,0.357248,"on. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two."
D19-1020,D15-1062,0,0.231787,"Missing"
D19-1020,E17-1110,0,0.0869635,"wed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two. The output is a relation from a predefined relation set R = (r1 , . . . , rM , None), where “None” means that no relation holds for the entities. Two steps are taken for predicting the correct relation given an input sentence. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode bot"
D19-1020,W08-0504,0,0.0520198,"Missing"
D19-1020,C10-2133,0,0.0320316,"from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature. 1 Introduction The sheer amount of medical articles and their rapid growth prevent researchers from receiving comprehensive literature knowledge by direct reading. This can hamper both medical research and clinical diagnosis. NLP techniques have been used for automating the knowledge extraction process from the medical literature (Friedman et al., 2001; Yu and Agichtein, 2003; Hirschman et al., 2005; Xu et al., 2010; Sondhi et al., 2010; Abacha and Zweigenbaum, 2011). Along this line of work, a long-standing task is relation extraction, which mines factual knowledge from free text by labeling relations between entity mentions. As shown in Figure 1, the sub-clause “previously observed cytochrome P450 3A4 ( CYP3A4 ) interaction of the dual orexin receptor antagonist almorexant” contains two entities, namely “orexin receptor” and “almorexant”. There is an “adversary” relation between these two entities, denoted as“CPR:6”. ⇤ Yue Zhang is the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin rec"
D19-1020,Q19-1002,1,0.928194,"e usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated"
D19-1020,P18-1150,1,0.94662,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,D18-1110,1,0.810915,"neration (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi repres"
D19-1020,D15-1206,0,0.201939,"Missing"
D19-1020,C18-1120,0,0.0200441,"focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labe"
D19-1020,D18-1246,1,0.942065,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,P18-1030,1,0.867698,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1020,N19-1152,0,0.0439742,"Missing"
D19-1020,D18-1244,0,0.376206,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1364,D18-1514,0,0.0722876,"Missing"
D19-1364,D18-1077,0,0.103613,"Missing"
D19-1364,D17-1314,0,0.429563,"u can help me with. Watson, I need your help Can you book a cleaning with my dentist for me? Can you schedule my dentist’s appointment? You can end the meeting now Meeting is over ··· OOD utterances My birthday is coming! Schedule Appointment End Meeting blah blah... Table 1: A few-shot ID training set for a conversation service for teleconference management, with OOD testing examples. Introduction Text classification tasks in real-world applications often consists of 2 components- In-Doman (ID) classification and Out-of-Domain (OOD) detection components (Liao et al., 2018; Kim and Kim, 2018; Shu et al., 2017; Shamekhi et al., 2018). ID classification refers to classifying a user’s input with a label that exists in the training data, and OOD detection refers to designate a special OOD tag to the input when it does not belong to any of the labels in the ID training dataset (Dai et al., 2007). Recent state-of-the-art deep learning (DL) approaches for OOD detection and ID classification task often require massive amounts of ID or OOD labeled data (Kim and Kim, 2018). In reality, many applications have very limited ID labeled data (i.e., few-shot learning) and no OOD labeled data (i.e., zero-shot lear"
D19-1364,N19-1084,1,0.881731,"Missing"
D19-1364,D18-1223,1,0.910736,"Missing"
D19-1364,N18-1109,1,0.894161,"19 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3566–3572, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics the meta-learning strategy (Vinyals et al., 2016) has been proposed to deal with the problem of limited training examples for each label (few-shot learning). In this line of work, Prototypical Networks (Snell et al., 2017), which was originally introduced for few-shot image classification, has proven to be promising for few-shot ID text classification (Yu et al., 2018). However the usage of prototypical network for OOD detection is unexplored in this regard. To the best of our knowledge, this work is the first one to adopt a meta-learning strategy to train an OOD-Resistant Prototypical Network for simultaneously detecting OOD examples and classifying ID examples. The contributions of this work are two-fold: 1) Unified solution using a prototypical network model which can detect OOD instances and classify ID instances in a real-world low-resource scenario. 2) Experiments and analysis on two datasets prove that the proposed model outperforms previous work on"
D19-1420,W09-2415,0,0.041273,"Missing"
D19-1420,N16-1181,0,0.0157953,"a other than mere performance. For example, medical (Yala et al., 2019) and other high-value decision applications require some means of verifying reasons for the predicted outcomes. This area of selfexplaining models in the context of NLP applications has primarily evolved along two parallel tracks. On one hand, we can design neural architectures that expose more intricate mechanisms ⇤ Authors contributed equally to this work. The code and data for our method is publicly available at https://github.com/Gorov/three_player_ for_emnlp. 1 tommi@csail.mit.edu of reasoning such as module networks (Andreas et al., 2016a,b; Johnson et al., 2017). While important, such approaches may still require adopting specialized designs and architectural choices that do not yet reach accuracies comparable to blackbox approaches. On the other hand, we can impose limited architectural constraints in the form of selective rationalization (Lei et al., 2016; Li et al., 2016b; Chen et al., 2018a,b) where the goal is to only expose the portion of the text relevant for prediction. The selection is done by a separately trained model called rationale generator. The resulting text selection can be subsequently used as an input to"
D19-1420,D16-1011,1,0.902682,"Missing"
D19-1420,N19-1264,0,0.0161889,"terpretability is widely studied in the general machine learning field. For example, evaluating feature importance with gradient information (Simonyan et al., 2013; Li et al., 2016a; Sundararajan et al., 2017) or local perturbations (Kononenko et al., 2010; Lundberg and Lee, 2017); and interpreting deep networks by locally fitting interpretable models (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018). Besides selective rationalization, the cooperative game has been studied in the latter direction above (Lee et al., 2018). It has also been applied to a relevant problem on summarization (Arumae and Liu, 2019), where the selected summary should be sufficient for answering questions related to the document. In this problem, the sum4101 mary is a special type of rationale of a document. Another related concurrent work (Bastings et al., 2019) proposes differentiable solution to optimize the cooperative rationalization method. Game-theoretical methods Though not having been explored much for self-explaining models, the minimax game setup has been widely used in many machine learning problems, such as selfplaying for chess games (Silver et al., 2017), generative models (Goodfellow et al., 2014) and many"
D19-1420,N16-1082,0,0.0948721,"that expose more intricate mechanisms ⇤ Authors contributed equally to this work. The code and data for our method is publicly available at https://github.com/Gorov/three_player_ for_emnlp. 1 tommi@csail.mit.edu of reasoning such as module networks (Andreas et al., 2016a,b; Johnson et al., 2017). While important, such approaches may still require adopting specialized designs and architectural choices that do not yet reach accuracies comparable to blackbox approaches. On the other hand, we can impose limited architectural constraints in the form of selective rationalization (Lei et al., 2016; Li et al., 2016b; Chen et al., 2018a,b) where the goal is to only expose the portion of the text relevant for prediction. The selection is done by a separately trained model called rationale generator. The resulting text selection can be subsequently used as an input to an unconstrained, complex predictor, i.e., architectures used in the absence of any rationalization.2 The main challenge of this track is how to properly coordinating the rationale generator with the powerful predictor operating on the selected information during training. In this paper, we build on and extend selective rationalization. The s"
D19-1420,P19-1284,0,0.0396236,"(Kononenko et al., 2010; Lundberg and Lee, 2017); and interpreting deep networks by locally fitting interpretable models (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018). Besides selective rationalization, the cooperative game has been studied in the latter direction above (Lee et al., 2018). It has also been applied to a relevant problem on summarization (Arumae and Liu, 2019), where the selected summary should be sufficient for answering questions related to the document. In this problem, the sum4101 mary is a special type of rationale of a document. Another related concurrent work (Bastings et al., 2019) proposes differentiable solution to optimize the cooperative rationalization method. Game-theoretical methods Though not having been explored much for self-explaining models, the minimax game setup has been widely used in many machine learning problems, such as selfplaying for chess games (Silver et al., 2017), generative models (Goodfellow et al., 2014) and many tasks that can be formulated as multi-agent reinforcement learning (Busoniu et al., 2006). Our three-player game framework also shares a similar idea with (Zhang et al., 2017; Zhao et al., 2017), which aim to learn domain-invariant r"
D19-1420,Q17-1036,1,0.835222,"ionale of a document. Another related concurrent work (Bastings et al., 2019) proposes differentiable solution to optimize the cooperative rationalization method. Game-theoretical methods Though not having been explored much for self-explaining models, the minimax game setup has been widely used in many machine learning problems, such as selfplaying for chess games (Silver et al., 2017), generative models (Goodfellow et al., 2014) and many tasks that can be formulated as multi-agent reinforcement learning (Busoniu et al., 2006). Our three-player game framework also shares a similar idea with (Zhang et al., 2017; Zhao et al., 2017), which aim to learn domain-invariant representations with both cooperative and minimax games. 7 Conclusion We proposed a novel framework for improving the predictive accuracy and comprehensiveness of the selective rationalization methods. This framework (1) addresses the degeneration problem in previous cooperative frameworks by regularizing the unselected words via a three-player game; and (2) augments the conventional generator with introspection, which can better maintain the performance for down-stream tasks. Experiments with both automatic evaluation and subjective st"
D19-1420,N16-3020,0,0.750734,"ities may still correlate with the labels. In the case, our model will pick these words as a part of the rationale. 6 Related Work Model interpretability Besides the two major categories of self-explaining models discussed in Section 1, model interpretability is widely studied in the general machine learning field. For example, evaluating feature importance with gradient information (Simonyan et al., 2013; Li et al., 2016a; Sundararajan et al., 2017) or local perturbations (Kononenko et al., 2010; Lundberg and Lee, 2017); and interpreting deep networks by locally fitting interpretable models (Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2018). Besides selective rationalization, the cooperative game has been studied in the latter direction above (Lee et al., 2018). It has also been applied to a relevant problem on summarization (Arumae and Liu, 2019), where the selected summary should be sufficient for answering questions related to the document. In this problem, the sum4101 mary is a special type of rationale of a document. Another related concurrent work (Bastings et al., 2019) proposes differentiable solution to optimize the cooperative rationalization method. Game-theoretical methods Though no"
D19-1682,P08-1095,0,0.852991,"reated per Slack channel with 355 messages, when they discuss group projects. Prior work on conversation thread disentanglement is often based on pairwise message compar⇤ Equal contributions from the corresponding authors: mingtan@us.ibm.com, dakuowang@ibm.com, yupeng.gao@ibm.com. 1 A instant messaging platform: https://slack.com ison. Some solutions use unsupervised clustering methods with hand-engineered features (Wang and Oard, 2009; Shen et al., 2006), while others use supervised approaches with statistical (Du et al., 2017) or linguistic features (Wang et al., 2008; Wang and Ros´e, 2010; Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012). Recent work by (Jiang et al., 2018; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps). However, linguistic theories (Biber and Conrad, 2019) differentiate the following three concepts: register, genre and style, to describe the text varieties. Register refers to the linguistic features such as the choice of words in content. Genre and Style refer to the conversational structure such as the sentence sequence and dis"
D19-1682,J10-3004,0,0.423572,"Missing"
D19-1682,P11-1118,0,0.662802,"Missing"
D19-1682,N18-1164,0,0.259625,"rior work on conversation thread disentanglement is often based on pairwise message compar⇤ Equal contributions from the corresponding authors: mingtan@us.ibm.com, dakuowang@ibm.com, yupeng.gao@ibm.com. 1 A instant messaging platform: https://slack.com ison. Some solutions use unsupervised clustering methods with hand-engineered features (Wang and Oard, 2009; Shen et al., 2006), while others use supervised approaches with statistical (Du et al., 2017) or linguistic features (Wang et al., 2008; Wang and Ros´e, 2010; Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012). Recent work by (Jiang et al., 2018; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps). However, linguistic theories (Biber and Conrad, 2019) differentiate the following three concepts: register, genre and style, to describe the text varieties. Register refers to the linguistic features such as the choice of words in content. Genre and Style refer to the conversational structure such as the sentence sequence and distribution. All aforementioned thread disentanglement methods fail to tak"
D19-1682,W12-1607,0,0.0515923,"Missing"
D19-1682,N09-1023,0,0.368831,"users to manually organize messages in threads. A recent study (Wang et al., 2019) found that users most likely do not manually create threads. On average, only 15.3 threads were created per Slack channel with 355 messages, when they discuss group projects. Prior work on conversation thread disentanglement is often based on pairwise message compar⇤ Equal contributions from the corresponding authors: mingtan@us.ibm.com, dakuowang@ibm.com, yupeng.gao@ibm.com. 1 A instant messaging platform: https://slack.com ison. Some solutions use unsupervised clustering methods with hand-engineered features (Wang and Oard, 2009; Shen et al., 2006), while others use supervised approaches with statistical (Du et al., 2017) or linguistic features (Wang et al., 2008; Wang and Ros´e, 2010; Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012). Recent work by (Jiang et al., 2018; Mayfield et al., 2012) adopt deep learning approaches to compute message pair similarity, using a combination of message content and simple contextual features (e.g., authorship and timestamps). However, linguistic theories (Biber and Conrad, 2019) differentiate the following three concepts: register, genre and style, to describe the text"
D19-1682,N10-1097,0,0.0609763,"Missing"
D19-1682,P16-1044,1,0.836979,"perform thread classification. CATD-MATCH runs LSTM on Til 1 and mi separately, performs attention to obtain the context embeddings and then gets the matching vector elmatch for thread classification. For a newly-generated thread, we use a parameter u as the only input for its LSTM. a thread Til 1 semantically closest to mi . Thus we independently encode each thread and mi with the parameter-shared LSTM (Only one LSTM step for mi ). In order to dynamically point to more related messages in the thread history, we use an one-way attention, which has been successfully adopted in many NLP tasks (Tan et al., 2016; Hermann et al., 2015). Specifically, given the sequence outputs of each thread’s LSTM, {hl (l,k) }K k=1 , we perform a weighted mean pooling to get a context embedding elcxt , attended by the one-step LSTMˆi . encoded mi , denoted as h ↵kl = elcxt = ˆ i )k=[1,K] sof tmax(hl (l,k) · h X ↵kl hl (l,k) (3) k = ˆi ) N (elcxt ) ⌦ N (h (4) CATD-COMBINE: Finally, we propose the dynamically-gated combination of FLOW and MATCH model, such that the elflow in Eq. 1 is replaced by a combination of the two models, elcombine = gl = g l )elmatch + g l elflow (5) ˆ i ) · w0 (6) sigmoid N (elcxt ) N (h (1 Tra"
D19-5806,P17-1171,0,0.0353296,"ctive Bridge Reasoning for Open-Domain Multi-Hop Question Answering Wenhan Xiong† , Mo Yu‡ , Xiaoxiao Guo‡ , Hong Wang† , Shiyu Chang‡ , Murray Campbell‡ , William Yang Wang† † University of California, Santa Barbara ‡ IBM Research {xwhan, william}@cs.ucsb.edu, {yum,mcam}@us.ibm.com, {shiyu.chang, xiaoxiao.guo}@ibm.com Abstract search. In this paper, we focus on the practical open-domain HotpotQA benchmark where the questions are asked upon natural language passages instead of knowledge bases and the supporting passages are not known beforehand. The typical pipeline of open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Htut et al., 2018) is to first use an IR system to retrieve a compact set of paragraphs and then run a machine reading model over the concatenated or reranked paragraphs. While IR works reasonably well for simple questions1 , it often fails to retrieve the correct answer paragraph for multi-hop questions. This is due to the fact that the question often cannot fully cover the information for the second or further hops. Consider the question “What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?” from the HotpotQA (Yang et al."
D19-5806,N18-1059,0,0.0348029,"Missing"
D19-5806,D14-1179,0,0.0387723,"Missing"
D19-5806,P18-1078,0,0.0151082,"ssages into a standard passage reader to predict the final answer to the multi-hop question. 3.1 The Base Span Prediction Model Both the bridge reasoner and the passage reader use a model that predicts a relevant span given a 2 66.07 F1 and 49.43 EM with full support access versus 64.77 F1 and 50.96 EM with only answer passage access. 49 extract the correct answer span. We run the target passage reader on the top 10 answer passage candidates predicted by the bridge reasoner. question. We use the same model architecture for both tasks and the architecture is base on the document QA model from (Clark and Gardner, 2018), which is used by Yang et al. (2018) as the baseline for HotpotQA. The model uses a shared bidirectional GRU (Cho et al., 2014) to encode the question and the passages. The encoded questions and passages are then passed to a bidirectional attention layer (Seo et al., 2017) to get the questionaware passage states. The state vectors are enhanced by a self-attention layer (Wang et al., 2017) and are finally fed into linear layers to predict the start and end span scores at every word position. 3.2 Training Passages from Cross-validation As we are using the same set of training questions for trai"
D19-5806,P17-1018,0,0.0174812,"reader on the top 10 answer passage candidates predicted by the bridge reasoner. question. We use the same model architecture for both tasks and the architecture is base on the document QA model from (Clark and Gardner, 2018), which is used by Yang et al. (2018) as the baseline for HotpotQA. The model uses a shared bidirectional GRU (Cho et al., 2014) to encode the question and the passages. The encoded questions and passages are then passed to a bidirectional attention layer (Seo et al., 2017) to get the questionaware passage states. The state vectors are enhanced by a self-attention layer (Wang et al., 2017) and are finally fed into linear layers to predict the start and end span scores at every word position. 3.2 Training Passages from Cross-validation As we are using the same set of training questions for training the bridge reasoner and the target passage reader, there will be a discrepancy between the training and evaluation of QA: at evaluation time, the reader sees the passages predicted by the bridge reasoner, while at training time, the groundtruth answer passage is known. On the other hand, we also cannot use the predicted passages for training the reader, as the bridge reasoner itself i"
D19-5806,P19-1259,0,0.0808276,"Missing"
D19-5806,Q18-1021,0,0.100685,"Missing"
D19-5806,N18-4017,0,0.0116842,"Multi-Hop Question Answering Wenhan Xiong† , Mo Yu‡ , Xiaoxiao Guo‡ , Hong Wang† , Shiyu Chang‡ , Murray Campbell‡ , William Yang Wang† † University of California, Santa Barbara ‡ IBM Research {xwhan, william}@cs.ucsb.edu, {yum,mcam}@us.ibm.com, {shiyu.chang, xiaoxiao.guo}@ibm.com Abstract search. In this paper, we focus on the practical open-domain HotpotQA benchmark where the questions are asked upon natural language passages instead of knowledge bases and the supporting passages are not known beforehand. The typical pipeline of open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Htut et al., 2018) is to first use an IR system to retrieve a compact set of paragraphs and then run a machine reading model over the concatenated or reranked paragraphs. While IR works reasonably well for simple questions1 , it often fails to retrieve the correct answer paragraph for multi-hop questions. This is due to the fact that the question often cannot fully cover the information for the second or further hops. Consider the question “What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?” from the HotpotQA (Yang et al., 2018) dataset. Since the name of the"
D19-5806,D18-1259,0,0.291902,"t al., 2017; Wang et al., 2018; Htut et al., 2018) is to first use an IR system to retrieve a compact set of paragraphs and then run a machine reading model over the concatenated or reranked paragraphs. While IR works reasonably well for simple questions1 , it often fails to retrieve the correct answer paragraph for multi-hop questions. This is due to the fact that the question often cannot fully cover the information for the second or further hops. Consider the question “What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?” from the HotpotQA (Yang et al., 2018) dataset. Since the name of the person (Shirley Temple) is not directly mentioned in the question and the answer is about another aspect of the person other than film acting, traditional IR heuristics based on n-gram matching might fail to retrieve the answer passage. In fact, the correct answer passage of Shirley Temple never appears in the top passages ranked by the default IR method of HotpotQA. Instead of predicting the answer passage with text matching between passages and questions, we claim that the answer passage can be better inferred based on the context-level information. Noticing t"
D19-5806,D14-1162,0,0.0869645,"reasoning to future work. Question: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell ? Figure 1: The overview of our QA system. The bridge reasoner reads the start passages retrieved by an IR system and predicts a set of candidate bridges (anchor links) that lead to the answer passages, which is further processed by the passage reader to return the answer. swer coverage of the top-ranked passages and thus increase the final QA performance by 14 point F1. Despite that our bridge reasoner and passage reader only learn above GloVe embeddings (Pennington et al., 2014), we achieve competitive performance with methods that use BERT (Devlin et al., 2018) in multiple modules. 2 Problem Definition and Motivation An open-domain multi-hop QA system aims to answer complex questions by retrieving evidence from a large open-domain passage corpus, such as Wikipedia. Usually the evidence scatters in a distributed set of supporting passages p1 , p2 , ..., pn that forms an ordered chain. Each pi provides evidence that partially fulfills the information required to answer the question, as well as provides clues (usually concepts or entities) that lead to the next support"
D19-5806,D16-1264,0,0.189657,"Missing"
D19-5813,N19-1240,0,0.0472223,"Missing"
D19-5813,P19-1416,0,0.0558822,"Missing"
D19-5813,P18-2118,1,0.795786,"Oracle setting. BERT was able to get a small improvement from its inner cross passage attention which introduces some weak reasoning. Surprisingly, overall the context passage in the reasoning path does not inherently contribute to the performance of these methods, which indicates that the models are not learning much multi-hop reasoning as previously thought. ¯ q = H q Gq H ¯q Settings We trained and evaluated each model for comparison for each setting separately. Following previous work (Yang et al., 2018), we report the exactmatch and F1 score for the answer prediction task. The work from (Wang et al., 2018) proposed a comatching mechanism which is used to jointly encode the question and answer with the context passage. We extend the idea to conduct the multi-hop reasoning in our setup. Specifically, we integrate the co-matching to the baseline readers by firstly applying bi-attention described in Equation 2 on (H q , H p2 ), and (H p1 , H p2 ) using the same set of parameters. Gq = Sof tM ax((W g H q + bg ⊗ ep2 )T H p2 ) ¯ p1 = H p1 Gp1 H Experiments Model Single-Oracle EM F1 Ordered-Oracle EM F1 HotpotReader Bert 55.07 64.08 55.17 65.03 70.00 77.86 70.75 79.15 Table 1: Baseline results for Hotp"
D19-5813,Q18-1021,0,0.0943292,"Missing"
D19-5813,D18-1259,0,0.428102,"o Multi-hop Readers Dream of Reasoning Chains? Haoyu Wang ∗† Mo Yu ∗† Xiaoxiao Guo ∗† Rajarshi Das ∗‡ ∗§ ∗† Wenhan Xiong Tian Gao † ‡ § IBM Research Umass Amherst UC Santa Barbara Abstract would be unsolvable by using only similarities between the question and answer. Recent multihop QA datasets, such as WikiHop (Welbl et al., 2018), ComplexWebQuestions (Talmor and Berant, 2018), and HotpotQA (Yang et al., 2018), have accelerated the rapid progress of QA models for multi-hop reasoning problems. There have been several reading comprehension models proposed to address the problem. Some methods (Yang et al., 2018; Zhong et al., 2019) rely on cross-attention among the question and evidence passages. BERT (Devlin et al., 2018) is one successful model of such an approach. Moreover, a substantial amount of query reformulation approaches (Weston et al., 2014; Wu et al., 2016; Shen et al., 2017; Das et al., 2019) have been proposed. Most of these methods adopt a soft version of reformulation, i.e. modifying the question embeddings based on the attention computed from each reasoning step. Similarly, some hard query reformulation approaches (Buck et al., 2018) propose to rewrite the question in the original l"
D19-5816,D07-1074,0,0.0439162,"linker that finds an introductory Wikipedia paragraph describing the entity, corresponding to each entity mention. Several IR approaches (Xiong et al., 2016; Raviv et al., 2016) use an off-the-shelf entity linker. However, most entity linking systems (Ganea and Hofmann, 2017; Raiman and Raiman, 2018) have been trained on Wikipedia data and hence using an off-the-shelf linker would be unfair, since there exists a possibility of test-time leakage. To ensure strictness, we developed our own simple linking strategy. Following the standard approach of using mention text and hyper-link information (Cucerzan, 2007; Ji and Grishman, 2011), we create a mapping (alias table) between them. The alias table stores mappings between a mention string (e.g. “Bill”) and various entities it can refer to (e.g. Bill Clinton, Billy Joel, etc). The top-40 documents returned by the BM25 retriever on the dev and test queries are also ignored while building the alias table. At test time, our reranker considers all the candidate entity paragraphs that a mention is linked to via the alias table. Although simple, we find this strategy to work well for our task and we plan to use a learned entity linker for future work. 2 Co"
D19-5816,D17-1277,0,0.0264935,"and returns an initial set of evidence. For our experiments, we use the popular BM25 retriever, but this component can be replaced by any IR model. We assume that all spans of entity mentions have been identified in the paragraph text by a one-time preprocessing, with an entity tagger.3 Entity Linking The next component of our model is an entity linker that finds an introductory Wikipedia paragraph describing the entity, corresponding to each entity mention. Several IR approaches (Xiong et al., 2016; Raviv et al., 2016) use an off-the-shelf entity linker. However, most entity linking systems (Ganea and Hofmann, 2017; Raiman and Raiman, 2018) have been trained on Wikipedia data and hence using an off-the-shelf linker would be unfair, since there exists a possibility of test-time leakage. To ensure strictness, we developed our own simple linking strategy. Following the standard approach of using mention text and hyper-link information (Cucerzan, 2007; Ji and Grishman, 2011), we create a mapping (alias table) between them. The alias table stores mappings between a mention string (e.g. “Bill”) and various entities it can refer to (e.g. Bill Clinton, Billy Joel, etc). The top-40 documents returned by the BM25"
D19-5816,P11-1115,0,0.0340076,"s an introductory Wikipedia paragraph describing the entity, corresponding to each entity mention. Several IR approaches (Xiong et al., 2016; Raviv et al., 2016) use an off-the-shelf entity linker. However, most entity linking systems (Ganea and Hofmann, 2017; Raiman and Raiman, 2018) have been trained on Wikipedia data and hence using an off-the-shelf linker would be unfair, since there exists a possibility of test-time leakage. To ensure strictness, we developed our own simple linking strategy. Following the standard approach of using mention text and hyper-link information (Cucerzan, 2007; Ji and Grishman, 2011), we create a mapping (alias table) between them. The alias table stores mappings between a mention string (e.g. “Bill”) and various entities it can refer to (e.g. Bill Clinton, Billy Joel, etc). The top-40 documents returned by the BM25 retriever on the dev and test queries are also ignored while building the alias table. At test time, our reranker considers all the candidate entity paragraphs that a mention is linked to via the alias table. Although simple, we find this strategy to work well for our task and we plan to use a learned entity linker for future work. 2 Code, pre-trained models a"
D19-5816,W17-2609,0,0.0690815,"Missing"
D19-5816,P18-1223,0,0.0539173,"Missing"
D19-5816,D17-1061,0,0.167894,"oth the initial paragraph and the representation of all the entities within it to determine which evidence to gather next. Essentially, our method introduces a new way of multi-step retrieval that uses information about intermediate entities. A standard way of doing multistep retrieval is via pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001) in which relevant terms from initial retrieved documents are used to reformulate the initial question. A few recent works learn to reformulate the query using task specific reward such as document recall or performance on a QA task (Nogueira and Cho, 2017; Buck et al., 2018; Das et al., 2019). However, these methods do not necessarily use the information about entities present in the evidence as they might not be the more frequent/salient terms in it. 113 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 113–118 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Which US city did Ron Teachworth complete his BA in? Rochester Hills (formerly Avon Township) is a city in northeast Oakland County in the U.S. state of Michigan…... Ronald S. Teachworth is an American artist, actor, w"
D19-5816,N18-1202,0,0.0190123,"rward. The training set had on an avg. 6.35 positive chains per example suggesting a multi-instance multi-label learning training setup (Surdeanu et al., 2012). However, for this work, we treat each chain independently. We use a simple binary cross-entropy loss to train the network. 3 Experiments For all our experiment, unless specified otherwise, we use the open domain corpus4 released by Yang et al. (2018) which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018).5 For the B ERT encoder, we use the B ERT- BASE - UNCASED model.6 We use the implementation of widely-used BM25 retrieval 4 https://hotpotqa.github.io/wiki-readme.html 5 https://allennlp.org/models 6 https://github.com/google-research/bert Model @2 ACCURACY @5 @10 @20 MAP BM25 P RF - TFIDF P RF - RM P RF - TASK 0.093 0.088 0.083 0.097 0.191 0.157 0.175 0.198 0.259 0.204 0.242 0.267 0.324 0.258 0.296 0.330 0.412 0.317 0.406 0.420 B ERT-re-ranker Q UERY +E- DOC 0.146 0.101 0.271 0.223 0.347 0.301 0.409 0.367 0.470 0.568 Our Model 0.230 0.482 0.612 0.674 0.654 Table 1: Retrieval performance of m"
D19-5816,D12-1042,0,0.0424696,"D and E and as we show empirically, not considering both leads to decrease in performance. During training, we mark a chain of paragraphs as a positive example, if the last paragraph of the chain is present in the supporting facts, since that is a chain of reasoning that led to a relevant paragraph. All other paragraph chains are treated as negative examples. In our experiments, we consider chains of length 2, although extending to longer chains is straightforward. The training set had on an avg. 6.35 positive chains per example suggesting a multi-instance multi-label learning training setup (Surdeanu et al., 2012). However, for this work, we treat each chain independently. We use a simple binary cross-entropy loss to train the network. 3 Experiments For all our experiment, unless specified otherwise, we use the open domain corpus4 released by Yang et al. (2018) which contains over 5.23 million Wikipedia abstracts (introductory paragraphs). To identify spans of entities, we use the implementation of the state-of-the-art entity tagger presented in Peters et al. (2018).5 For the B ERT encoder, we use the B ERT- BASE - UNCASED model.6 We use the implementation of widely-used BM25 retrieval 4 https://hotpot"
D19-5816,N18-1059,0,0.0645134,"Missing"
D19-5816,Q18-1021,0,0.0340131,"o the reader model. We achieve a 10.59 absolute increase in F1 score than the baseline. It should be noted that we use the simple baseline reader model and we are confident that we can achieve better scores by using more sophisticated reader architectures, e.g. using B ERT based architectures. Our results show that retrieval is an important component of an opendomain system and equal importance should be given to both the retriever and reader component. 3.3 Zero-shot experiment on Wikihop We experiment if our model trained on H OTPOT QA can generalize to another multi-hop dataset – W IK IHOP (Welbl et al., 2018), without any training. In the W IKIHOP dataset, a set of candidate introductory Wikipedia paragraphs are given per question. Hence, we do not need to use our initial BM25 retriever. We assign the first entity mention occurring in a 116 Model BM25 B ERT-re-ranker (zs) Our Model (zs) acc@2 acc@5 0.06 0.08 0.10 0.30 0.27 0.41 et al., 2017; Zamani et al., 2018, inter-alia). Bagof-words and contextual embedding models, such as word2vec and BERT, have also been explored extensively for various IR tasks, from document to sentence-level retrieval (Padigela et al., 2019; Zamani and Croft, 2016, 2017)."
D19-5816,D18-1259,0,0.335373,"eira and Cho (2019). However, unlike us, they do not model the chains of evidence paragraphs required for a multi-hop question. Secondly, they also do not have a entity linking component to identify the relevant paragraphs. Our model out-performs them for multi-hop QA. To summarize, this paper presents an entitycentric IR approach that jointly performs entity linking and effectively finds relevant evidence required for questions that need multi-hop reasoning from a large corpus containing millions of paragraphs. When the retrieved paragraphs are supplied to the baseline QA model introduced in Yang et al. (2018), it improved the QA performance on the hidden test set by 10.59 F1 points.2 2 Methodology Our approach is summarized in Figure 2. The first component of our model is a standard IR system that takes in a natural language query ‘Q’ and returns an initial set of evidence. For our experiments, we use the popular BM25 retriever, but this component can be replaced by any IR model. We assume that all spans of entity mentions have been identified in the paragraph text by a one-time preprocessing, with an entity tagger.3 Entity Linking The next component of our model is an entity linker that finds an"
D19-5816,N19-1423,0,\N,Missing
N13-1063,P09-1056,0,0.0156231,"nce it is impossible to obtain sufficient labeled data for all NLP tasks. In these situations semisupervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; Täckström et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform co"
N13-1063,P08-1068,0,0.500648,"data sparsity, since it is impossible to obtain sufficient labeled data for all NLP tasks. In these situations semisupervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representations, which are feature vectors of lexical items, from unlabeled data and then plug them into a supervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-ofthe-art supervised systems on a variety of tasks (Koo et al., 2008; Huang and Yates, 2009; Täckström et al., 2012). With the development of neural language models (NLM) (Bengio et al., 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embeddings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help t"
N13-1063,N12-1052,0,0.0123376,"Missing"
N13-1063,P10-1040,0,0.886975,"s) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and semantic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al., 2011a, 2011b) and help these systems perform comparably with the state-of-the-art models based on handcrafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al. 2010). Although the direct usage of continuous embeddings has been proved to be an effective method for enhancing the state-of-the-art supervised models, it has some disadvantages, which made them be out-performed by simpler Brown cluster features (Turian et al, 2010) and made them computationally complicated. Firstly, embeddings of rare words are insufficiently trained since they are only updated few times and are close to their random initial values. As shown in (Turian et al, 2010), this is the main reason that models with embedding features made more errors than those with Brown cluster feature"
N13-1063,D07-1096,0,\N,Missing
N13-1063,D11-1014,0,\N,Missing
N13-1063,P05-1045,0,\N,Missing
N15-1155,D14-1067,0,0.0267419,"Missing"
N15-1155,D14-1082,0,0.369288,"a large number of training examples. For smaller training sets, the variance of their estimator will be high resulting in increased ~ ( to0)use ✓~ ⇡ 0 We seek generalization error on test data. -.5 .3 .8 .7 1 0 1 0 0 1 many more features (based on rich annotations such as syntactic parsing and NER) and larger label sets, which further exacerbates the problem of overfitting. We propose a new method of learning interactions between engineered features and word embeddings by combining the idea of the outer product in FCM (Yu et al., 2014) with learning feature embeddings (Collobert et al., 2011; Chen and Manning, 2014).2 Our model jointly learns feature embeddings and a tensor-based classifier which relies on the outer product between features embeddings and word embeddings. Therefore, the number of parameters are dramatically reduced since features are only represented as low-dimensional embeddings, which alleviates problems with overfitting. The resulting model benefits from both approaches: conjunctions between feature and word embeddings allow model 2 2 Collobert et al. (2011) and Chen and Manning (2014) also capture interactions between word embeddings and features by using deep convolutional networks"
N15-1155,W06-1670,0,0.0304999,"uding the “social” relation. The reason is that the asymmetric subtype, “social.role”, dominates the class: 679 of 834 total “social” relations. Setup We randomly initialize the feature embeddings Wf and pre-train 200-dimensional word embeddings on the NYT portion of Gigaword 5.0 (Parker et al., 2011) with word2vec (default setting of the toolkit) (Mikolov et al., 2013). Dependency parses are obtained from the Stanford Parser (De Marneffe et al., 2006). We use the same feature templates as Yu et al. (2014). When gold entity types are unavailable, we replace them with WordNet tags annotated by Ciaramita and Altun (2006). Learning rates, weights of L2-regularizations, the number of iterations and the size of the feature embeddings d are tuned on dev sets. We selected d from {12, 15, 20, 25, 30, 40}. We used d=30 for feature embeddings for fine-grained ACE without gold types, and d=20 otherwise. For ERE, we have d=15. The weights of L2 λ was selected from {1e3, 5e-4, 1e-4}. As in prior work (Yu et al., 2014), regularization did not significantly help FCM. However for LRFCM, λ=1e-4 slightly helps. We use a learning rate of 0.05. We compare to two baselines. First, we use the features of Sun et al. (2011), who b"
N15-1155,de-marneffe-etal-2006-generating,0,0.0575853,"Missing"
N15-1155,P14-1129,0,0.0788578,"Missing"
N15-1155,P13-1088,0,0.0368571,"ure 1, “driving” is a strong indicator of the “ART” (ACE) relation because it appears on the dependency path between a person and a vehicle. Yet such conjunctions of different syntactic/semantic annotations (dependency and NER) are typically not available in compositional models. In contrast, hand-crafted features can easily capture this information, e.g. feature fi3 (Figure 1). Therefore, engineered features should be combined with learned representations in compositional models. One approach is to use the features to select specific transformations for a sub-structure (Socher et al., 2013a; Hermann and Blunsom, 2013; Hermann et al., 2014; Roth and Woodsend, 2014), which can conjoin features and word embeddings, but is impractical as the numbers of transformations will exponentially increase with additional features. Typically, less than 10 features are used. A solution 1374 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1374–1379, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics A A A0 of B 0 -.5 A f ⌦e FCT M2=taxicab T f1 … f fi [f : e] w1=“A” .8 A y=ART(M1,M2) M1=man .3 B0 of f i fi 1 0 1 0 0 1 B .7 ewi B"
N15-1155,P14-1136,0,0.250951,"& Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a co"
N15-1155,P08-1068,0,0.0380643,"s while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlev"
N15-1155,N04-1043,0,0.0547495,"es and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim"
N15-1155,P14-2012,0,0.470285,"g feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlevel structure representations, composition of embeddings alone may"
N15-1155,P13-1147,0,0.471663,"L , which yields  T ∂`/∂s = (I[y = y 0 ] − P (y 0 |x; T, Wf ))1≤y0 ≤L , where I[x] is the indicator function equal to 1 if x is true and 0 otherwise. Then we have the following stochastic gradients, where ◦ is the tensor product: (y,x)∈D n ∂` X ∂` = ⊗ gi ⊗ ewi , (4) ∂T ∂s i=1  n n  X X ∂` ∂` ∂gi ∂` = = T◦ ◦ ewi ⊗ fi . ∂Wf ∂gi ∂Wf ∂s i=1 i=1 4 Experiments Datasets We consider two relation extraction datasets: ACE2005 and ERE, both of which contain two sets of relations: coarse relation types and fine relation (sub-)types. Prior work on English ACE 2005 has focused only on coarse relations (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Li and Ji, 2014); to the best of our knowledge, this paper establishes the first baselines for the other datasets. Since the fine-grained relations require a large number of parameters, they will test the ability Model PM’13 (S) FCM (S) LRFCM (S) BASELINE (ST) FCM (ST) LRFCM (ST) ACE-bc (|L|=11) P R F1 55.3 43.1 48.5 62.3 45.1 52.3 58.5 46.8 52.0 72.2 52.0 60.5 66.2 54.2 59.6 65.1 54.7 59.4 ACE-bc (|L|=32) P R F1 59.7 41.6 49.0 57.4 46.2 51.2 60.2 51.2 55.3 62.9 49.6 55.4 63.5 51.1 56.6 ERE (|L|=9) P R F1 68.3 52.6 59.4 65.1 56.1 60.3 76.2 64.0 69.5 73.0 65.4 69.0"
N15-1155,D14-1045,0,0.0637846,"y with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlevel structure representations, composition of embeddings alone may not capture important synt"
N15-1155,D12-1110,0,0.16476,"n Language Technology Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature e"
N15-1155,P13-1045,0,0.432027,"Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly wit"
N15-1155,D13-1170,0,0.0395996,"Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly wit"
N15-1155,P11-1053,0,0.304691,"ameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlevel structure representations, compositi"
N15-1155,P10-1040,0,0.374507,"R. Gormley, Mark Dredze Mo Yu ∗ Machine Intelligence Human Language Technology Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while mai"
N15-1155,P05-1053,0,0.300494,"ng rates, weights of L2-regularizations, the number of iterations and the size of the feature embeddings d are tuned on dev sets. We selected d from {12, 15, 20, 25, 30, 40}. We used d=30 for feature embeddings for fine-grained ACE without gold types, and d=20 otherwise. For ERE, we have d=15. The weights of L2 λ was selected from {1e3, 5e-4, 1e-4}. As in prior work (Yu et al., 2014), regularization did not significantly help FCM. However for LRFCM, λ=1e-4 slightly helps. We use a learning rate of 0.05. We compare to two baselines. First, we use the features of Sun et al. (2011), who build on Zhou et al. (2005) with additional highly tuned features for ACE-style relation extraction from years of research. We implement these in a logistic regression model BASELINE, excluding country gazetteer and WordNet features. This baseline includes gold entity types and represents a high quality feature rich model. Second, we include results from Plank and Moschitti (2013) (PM’13), who obtained improveERE (|L|=18) Correct FCM Incorrect LRFCM Correct 423 57 Incorrect 34 246 Table 2: Confusion Matrix between the results of FCM and LRFCM on the test set of ERE fine relation task. Each item in the table shows the nu"
N15-1155,P14-1038,0,\N,Missing
N15-3018,W14-2907,1,0.881586,"Missing"
N15-3018,P14-1073,1,0.741206,"on-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quicklime (Section 3.7). an example visualization. 3.6 Cross Document Coreference Resolution Cross document coreference resolution is performed via the phylogenetic entity clustering model of Andrews et al. (2014).5 Since the method is fully unsupervised we did not require a Chinese specific model. We use this system to cluster EntityMentions and store the clustering in top level C ONCRETE Clustering objects. 3.7 Creating Manual Annotations Quicklime6 is a browser-based tool for viewing and editing NLP annotations stored in a C ONCRETE document. Quicklime supports a wide array of analytics, including parse trees, token taggings, entities, mentions, and “situations” (e.g. relations.) Quicklime uses the visualization layer of BRAT (Stenetorp et al., 2012) to display some annotations but does not use the"
N15-3018,W08-0336,0,0.0139229,"tions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 ,"
N15-3018,P05-1045,0,0.00391797,"cy parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit which is a CRF model based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gaz"
N15-3018,P03-1054,0,0.0257485,"Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit wh"
N15-3018,P14-5010,0,0.0125522,"ata sets include annotations for entities and a variety of relations (Aguilar et al., 2014). The labeled entities and relations are represented by C ONCRETE EntityMentions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on"
N15-3018,W12-3018,1,0.837337,"Missing"
N15-3018,P11-1053,0,0.0126349,"otations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quic"
N15-3018,N03-1033,0,0.0354521,"Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, a"
N15-3018,P05-1053,0,0.027847,"l based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path betw"
N16-1117,Q14-1043,0,0.305337,"elopment set. 7 Experimental Settings We evaluate LRFR on three tasks: relation extraction, PP attachment and preposition disambiguation (see Table 1 for a task summary). We include detailed feature templates in Table 2. PP-attachment and relation extraction are two fundamental NLP tasks, and we test our models on the largest English data sets. The preposition disambiguation task was designed for compositional semantics, which is an important application of deep learning and distributed representations. On all these tasks, we compare to the state-of-the-art. We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015). 1024 Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain. To highlight the impact of training data size we evaluate with all 43,518 relations (entity mention pairs) and a reduced training set of the first 10,000 relations. We report precision, recall, and F1. W"
N16-1117,P14-1063,0,0.024792,", 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (§3) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. 10 Conclusion We have presented LRFR, a feature representation model that exp"
N16-1117,D14-1082,0,0.0440731,"ositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (§3) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. 10 Conclusion We have presented LRFR, a feature representation model that exploits the inner structure of complex lexica"
N16-1117,D15-1205,1,0.749854,"Missing"
N16-1117,P13-1088,0,0.0204306,"lion features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to ex"
N16-1117,P14-1136,0,0.0652837,"re 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open question. To address the above problems,1 we prop"
N16-1117,P10-1030,0,0.0172187,"r than the one-hot word encodings. We denote the m-dimensional word embeddings by ew ; so the transformation matrices Wi for the lexical parts are of size ri × m where m  |V |. We note that when sufficiently large labeled data is available, our model allows for fine-tuning the pre-trained word embeddings to improve the expressive strength of the model, as is common with deep network models. Remarks Our LRFRs introduce embeddings for non-lexical properties and labels, making them better suit the common setting in NLP: rich linguistic properties; and large label sets such as open-domain tasks (Hoffmann et al., 2010). The LRFR - CP better suits n-gram features, since when n increases 1, the only new parameters are the corresponding Wi . It is also very efficient during prediction (O(nr)), since the cost of transformations can be ignored with the help of look-up tables and pre-computing. 5 Learning Representations for n-gram Lexical Features of Mixed Lengths For features with n lexical parts, we can train an LRFR n model to obtain their representations. However, we often have features of varying n (e.g. both unigrams (n=1) and bigrams (n=2) as in Figure 1). 1023 We require representations for features with"
N16-1117,P08-1068,0,0.621823,"a word type is conjoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in re"
N16-1117,P14-1130,0,0.494653,"applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths. Our model makes the following contributions when contrasted with prior work: Lei et al. (2014) applied CP to combine different views of features. Compared to their work, our usage of CP-decomposition is different in the application to feature learning: (1) We focus on dimensionality reduction of existing, well-verified features, while Lei et al. (2014) generates new features (usually different from ours) by combining some “atom” features. Thus their work may ignore some useful features; it relies on binary features as supplementary but our model needs not. (2) Lei et al. (2014)’s factorization relies on views with explicit meanings, e.g. head/modifier/arc in dependency parsing, making"
N16-1117,N15-1121,0,0.0535943,"Missing"
N16-1117,P15-2047,0,0.0198918,"eved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and"
N16-1117,P15-2029,0,0.0264122,"these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views o"
N16-1117,N06-1020,0,0.0858249,"Missing"
N16-1117,N04-1043,0,0.0487803,"semantic similarity, a word type is conjoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these"
N16-1117,P14-2012,0,0.0481492,"mensional features, including PCA, alternating structural optimization (Ando and Zhang, 2005), denoising autoencoders (Vincent et al., 2008), and feature embeddings (Yang and Eisenstein, 2015). These methods treat features as atomic elements and ignore the inner structure of features, so they learn separate embedding for each feature without shared parameters. As a result, they still suffer from large parameter spaces when the feature space is very huge.5 Another line of research studies the inner structures of lexical features: e.g. Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composit"
N16-1117,D14-1045,0,0.122582,"to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open question. To address the a"
N16-1117,D12-1110,0,0.0573295,"(2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a spe"
N16-1117,P13-1045,0,0.0424596,". (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguis"
N16-1117,D13-1170,0,0.00328851,". (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguis"
N16-1117,P11-1053,0,0.573266,"tion in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open q"
N16-1117,N15-1163,0,0.0208344,"nored with the help of look-up tables and pre-computing. 5 Learning Representations for n-gram Lexical Features of Mixed Lengths For features with n lexical parts, we can train an LRFR n model to obtain their representations. However, we often have features of varying n (e.g. both unigrams (n=1) and bigrams (n=2) as in Figure 1). 1023 We require representations for features with arbitrary different n simultaneously. We propose two solutions. The first is a straightforward solution based on our framework, which handles each n with a (n+2)-way tensor. This strategy is commonly used in NLP, e.g. Taub-Tabib et al. (2015) have different kernel functions for different order of dependency features. The second is an approximation method which aims to use a single tensor to handle all ns. Multiple Low-Rank Tensors Suppose that we can divide the feature set S(x, y) into subsets S1 (x, y), S2 (x, y), . . . , Sn (x, y) which correspond to features with one lexical part (unigram features), two lexical parts (bigram features), . . . and n lexical parts (n-gram features), respectively. To handle these types of features, we modify the training objective as follows: X minimize `(x, y; T1 , T2 , . . . , ...Tn ), (8) T1 ,T2"
N16-1117,P10-1040,0,0.211356,"njoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning"
N16-1117,N15-1069,0,0.128161,"∧ y is the conjunction of the four parts. Figure (d) is the one-hot representation of φ, which is equivalent to the outer product (i.e. a 4-way tensor) among the four one-hot vectors. v(x) = 1 means the vector v has a single non-zero element in the x position. eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths. Our model makes the following contributions when contrasted with prior work: Lei et al. (2014) applied CP to combine different views of features. Compared to their work, our usage of CP-decomposition is different in the application to feature learning: (1) We focus on dimensionality reduction of existing, well-verified feat"
N16-1117,Q15-1017,1,0.803277,"guistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (§3) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. 10 Conclusion We have presented LRFR, a feature representation model that exploits the inner structure of complex lexical features and applies"
N16-1117,N15-1155,1,0.519215,"andidate head. Figure (c) shows what the fifth feature (φ) is @R like, when the candidate is “see”. As is common in multi-class classification tasks, each template generates a different feature for each label y. Thus a feature φ = wg ∧ wc ∧ u ∧ y is the conjunction of the four parts. Figure (d) is the one-hot representation of φ, which is equivalent to the outer product (i.e. a 4-way tensor) among the four one-hot vectors. v(x) = 1 means the vector v has a single non-zero element in the x position. eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths. Our model makes the following contributions when contrasted with prior w"
N16-1117,P14-2107,0,0.0158469,"ion also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Soch"
N16-1117,P05-1053,0,0.157917,"al. (2015). 1024 Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain. To highlight the impact of training data size we evaluate with all 43,518 relations (entity mention pairs) and a reduced training set of the first 10,000 relations. We report precision, recall, and F1. We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al. (2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al. (2015), which uses rich linguistic features for relation extraction. We use the same feature templates and evaluate on finegrained relations (sub-types, 32 labels) (Yu et al., 2015). This will evaluate how LRFR can utilize nonlexical linguistic features. PP-attachment We consider the prepositional phrase (PP) attachment task of Belinkov et al. (2014),3 where for each PP the correct head (verbs or nouns) must be selected from content words before the PP (within a 10-word window). We formulate the task a"
N18-1109,P07-1056,0,0.373365,"raining for future work. 4 Tasks and Data Sets We test our methods by conducting experiments on two text classification data sets. We used NLTK toolkit3 for tokenization. The task are divided into meta-training tasks and meta-testing tasks (target tasks), where the meta-training tasks are used for clustering and cluster-encoder training. The metatesting tasks are few-shot tasks, which are used for evaluating the method in Eq. (6). 4.1 Amazon Review Sentiment Classification First, following Barzilai and Crammer (2015), we construct multiple tasks with the multi-domain sentiment classification (Blitzer et al., 2007) data set. The dataset consists of Amazon product reviews for 23 types of products (see Appendix D for the details). For each product domain, we construct three binary classification tasks with different thresholds on the ratings: the tasks consider a review as positive if it belongs to one of the following buckets = 5 stars, &gt;= 4 stars or &gt;= 2 stars.4 These buckets then form the basis of the task-setup, giving us 23 ⇥ 3=69 tasks in total. For each domain we distribute the reviews uniformly 3 http://www.nltk.org/ Data downloaded from http://www.cs.jhu.edu/ ˜mdredze/datasets/sentiment/, in whic"
N18-1109,D14-1181,0,0.00578282,"test instance x ˆ and the support set S: y = P (.|ˆ x, S) = |S| X ↵(ˆ x, xi ; ✓)yi , M different from Eq. (1): (1) y = P (.|ˆ x, S) = i=1 B,S⇠D ↵(ˆ x, Si ; ✓)yi . (3) i=1 where we defined ↵(., .) to be a softmax distribution given ⇤(ˆ x, xi ), where xi is a supporting instance, i.e., ↵(ˆ x, xi ; ✓) = T exp(f (ˆ x)T f (xi ))/P|S |exp(f (ˆ x) f (xj )), where ✓ j=1 are the parameters of the encoder f . Thus, y is a valid distribution over the supporting set’s labels |S| {yi }i=1 . To adapt the MNet to text classification, we choose encoder f to be a convolutional neural network (CNN) following (Kim, 2014; Johnson and Zhang, 2016). Figure 1 shows the MNet with the CNN architecture. Following (Collobert et al., 2011; Kim, 2014), the model consists of a convolution layer and a max-pooling operation over the entire sentence. To train the MNets, we first sample the training dataset D for task T from all tasks T , with notation simplified as D ⇠ T . For each class in the sampled dataset D, we sample k random instances in that class to construct a support set S, and sample a batch of training instances B as training examples, i.e., B, S ⇠ D. The training objective is to minimize the prediction error"
N18-1109,D14-1162,0,0.0810149,": a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 3; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above singletask encoders with Eq. (6). This baseline can be viewed as a variation of our method without task clustering. We initialize all models with pretrained 100-dim Glove embeddings (trained on 6B corpus) (Pennington et al., 2014). Hyper-Parameter Tuning In all experiments, we set both p1 and p2 parameters in (4) to 0.5. This strikes a balance between obtaining enough observed entries in Y, and ensuring that most of the retained similarities are consistent with the cluster membership. The window/hidden-layer sizes of CNN and the initialization of embeddings (random or pre-trained) are tuned during the clusterencoder training phase, with the validation sets of meta-training tasks. We have the CNN with window size of 5 and 200 hidden units. The singlemetric FSL baselines have 400 hidden units in the CNN encoders. On sent"
N19-1084,P16-2022,0,0.0280537,"mentions are actually pronouns, such as “he” or “it”, this kind of mentions alone provide only limited clues about general entity types (e.g., “he” is a “person”) but little information about fine-grained types. In this case, directly appending representation of pronouns does not provide extra useful information for making fine-grained predictions. Thus, instead of using the concatenation operator, we propose to construct a stronger interaction between the mention and context with an attentionbased matching module, which has shown its effectiveness in recent natural language inference models (Mou et al., 2016; Chen et al., 2017). Formally consider the mention representation M ∈ Rhm and context’s hidden feature Ch ∈ 4.2 Methodology In this section, we first briefly introduce the neural architecture to encode raw text inputs. Then we describe the matching module we use to enhance the interaction between the mention span and the context sentence. Finally, we move to the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. Figure 1 provides a graphical overview of our model, with 1a) illustrat"
N19-1084,P17-1152,0,0.0259247,"lly pronouns, such as “he” or “it”, this kind of mentions alone provide only limited clues about general entity types (e.g., “he” is a “person”) but little information about fine-grained types. In this case, directly appending representation of pronouns does not provide extra useful information for making fine-grained predictions. Thus, instead of using the concatenation operator, we propose to construct a stronger interaction between the mention and context with an attentionbased matching module, which has shown its effectiveness in recent natural language inference models (Mou et al., 2016; Chen et al., 2017). Formally consider the mention representation M ∈ Rhm and context’s hidden feature Ch ∈ 4.2 Methodology In this section, we first briefly introduce the neural architecture to encode raw text inputs. Then we describe the matching module we use to enhance the interaction between the mention span and the context sentence. Finally, we move to the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. Figure 1 provides a graphical overview of our model, with 1a) illustrating both the text en"
N19-1084,D14-1162,0,0.0859281,"the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. Figure 1 provides a graphical overview of our model, with 1a) illustrating both the text encoders and the matching module, and 1b) showing an example of graph propagation. 4.1 Representation Model Our base model to encode the context and the mention span follows existing neural approaches (Shimaoka et al., 2016; Xu and Barbosa, 2018; Choi et al., 2018). To encode the context, we first apply a standard Bi-LSTM, which takes GloVe (Pennington et al., 2014) embeddings and position embeddings (three vectors representing positions before, inside or after the mention span) as inputs and outputs the hidden states at each time step t ∈ [1, lc ]. With the derived hidden states Mention-Context Interaction 2 Please refer to (Shimaoka et al., 2017) and (Choi et al., 2018) for more detailed descriptions. 775 Rlc ×hc , where lc indicates the number of tokens in the context sentence and hm , hc denote feature dimensions. We first project the mention feature M into the same dimension space as Ch with a linear layer (W1 ∈ Rhm ×hc ) and a tanh function3 : mpro"
N19-1084,P18-1009,0,0.199154,"d in entity typing datasets. For benchmarks annotated with the knowledge base (KB) guided distant supervision, this assumption is often valid since all types are from KB ontologies and naturally follow tree-like structures. However, since knowledge bases are inherently incomplete (Min et al., 2013), existing KBs only include a limited set of entity types. Thus, models trained on these datasets fail to generalize to lots of unseen types. In this work, we investigate entity typing in a more open scenario where the type set is not restricted by KB schema and includes over 10,000 free-form types (Choi et al., 2018). As most of the types do not follow any predefined structures, methods that explicitly incorporate type hierarchies cannot be straightforwardly applied here. To effectively capture the underlying label correlations without access to known type structures, we propose a novel label-relational inductive bias, represented by a graph propagation layer that operates in the latent label space. Specifically, this layer learns to incorporate a label affinity matrix derived from global type co-occurrence statistics and word-level type similarities. It can be seamlessly coupled with existing models and"
N19-1084,P17-2052,0,0.160724,"imple cross-entropy loss. Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET (Ren et al., 2016a) used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbosa, 2018) proposed hierarchical loss normalization which alleviated the noise of too specific types. Our work differs from these works in that we do not rely on known label structures and aim to learn the underlying correlations from data. Rabinovich and Klein (2017) recently proposed a structure-prediction approach which used type correlation features. The inference on their learned factor graph is approximated by a greedy decoding algorithm, which outperformed unstructured methods on their own dataset. Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding. • We impose an effective label-relational bias on entity typing models with an easy-toimplement graph propagation layer, which allows the model to implicitly capture type dependencies; • We augment our gr"
N19-1084,Q14-1037,0,0.0487843,"ierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 2016) and question answering (Lee et al., 2006; Yavuz et al., 2016). 1 https://github.com/xwhan/ Extremely-Fine-Grained-Entity-Typing 773 Proceedings of NAACL-HLT 2019, pages 773–784 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics • Empirically, our model is able to offer significant improvements over previous models on the Ultra-Fine dataset and also reduces the cases of inconsistent type predictions. tion that the underlying type structures are predefined in entity typing datasets. For benchmarks anno"
N19-1084,D16-1144,0,0.588196,"is identified as a “criminal”, then the entity must also be a “person”, but it is less likely for this entity to be a “police officer” at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in Table 1, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions. Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction ("
N19-1084,W16-1313,0,0.602463,"l., 2009) for entity typing and created one of the early large-scale datasets. Although DS provides an efficient way to annotate training data, later work (Gillick et al., 2014) pointed out that entity type labels induced by DS ignore entities’ local context and may have limited usage in contextaware applications. Most of the following research has since focused on testing in context-dependent scenarios. While early methods (Gillick et al., 2014; Yogatama et al., 2015) on this task rely on well-designed loss functions and a suite of handcraft features that represent both context and entities, Shimaoka et al. (2016) proposed the first attentive neural model which outperformed featurebased methods with a simple cross-entropy loss. Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET (Ren et al., 2016a) used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbosa, 2018) proposed hierarchical loss normalization which alleviated the noise of too specific types. Our work differs from these works in that we do"
N19-1084,E17-1119,0,0.698182,"t it is less likely for this entity to be a “police officer” at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in Table 1, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions. Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 2016) and question answering (Lee et al., 2006; Yavuz et al."
N19-1084,N18-1002,0,0.709,"“criminal”, then the entity must also be a “person”, but it is less likely for this entity to be a “police officer” at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in Table 1, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions. Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 201"
N19-1084,D16-1015,0,0.134564,"t al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 2016) and question answering (Lee et al., 2006; Yavuz et al., 2016). 1 https://github.com/xwhan/ Extremely-Fine-Grained-Entity-Typing 773 Proceedings of NAACL-HLT 2019, pages 773–784 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics • Empirically, our model is able to offer significant improvements over previous models on the Ultra-Fine dataset and also reduces the cases of inconsistent type predictions. tion that the underlying type structures are predefined in entity typing datasets. For benchmarks annotated with the knowledge base (KB) guided distant supervision, this assumption is often valid since all types a"
N19-1084,P15-2048,0,0.101731,"grained entity typing was first thoroughly investigated in (Ling and Weld, 2012), which utilized Freebase-guided distant supervision (DS) (Mintz et al., 2009) for entity typing and created one of the early large-scale datasets. Although DS provides an efficient way to annotate training data, later work (Gillick et al., 2014) pointed out that entity type labels induced by DS ignore entities’ local context and may have limited usage in contextaware applications. Most of the following research has since focused on testing in context-dependent scenarios. While early methods (Gillick et al., 2014; Yogatama et al., 2015) on this task rely on well-designed loss functions and a suite of handcraft features that represent both context and entities, Shimaoka et al. (2016) proposed the first attentive neural model which outperformed featurebased methods with a simple cross-entropy loss. Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET (Ren et al., 2016a) used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbo"
N19-1084,P09-1113,0,\N,Missing
N19-1084,N13-1095,0,\N,Missing
N19-1086,D18-1514,0,0.421659,"from previous tasks as anchor points and minimizes the distortion of the anchor points in the embedding space in the lifelong relation extraction. The aligned embedding space is then utilized for relation extraction. Experiment results show that our method outperforms the state-of-the-art significantly in accuracy while remaining efficient. The main contributions of this work include: • We introcduce the lifelong relation detection problem and construct lifelong relation detection benchmarks from two datasets with large relation vocabularies: SimpleQuestions (Bordes et al., 2015) and FewRel (Han et al., 2018). • We propose a simple memory replay approach and find that current popular methods such as EWC and GEM underperform this method. • We propose an alignment model which aims to alleviate the catastrophic forgetting problem by slowing down the fast changes in the embedding space for lifelong learning. 2 To make f perform well on the previous tasks, during the lifelong learning process, we usually allow the learner to maintain and observe a memory M of samples from the previous tasks. Practically, with the growth of the number of tasks, it is difficult to store all the task data1 . Therefore, in"
N19-1086,P82-1020,0,0.719966,"Missing"
N19-1086,N15-2018,0,0.0711611,"Missing"
N19-1086,P15-2123,0,0.126278,"Missing"
N19-1086,D14-1162,0,0.110347,"recent works such as GEM ignore this step and randomly select samples from each task to store in the memory. ka(f (x)) − a(k−1) (f (x))k2 (k) (x,y)∈Dtrain + X Selective Storing Samples in Memory ka(f (x)) − a(k−1) (f (k−1) (x))k2 (k) (x,y)∈Dreplay Embedding Alignment on Relation Detection Model We introduce how to add embedding alignment to relation detection models. The basic model we use is a ranking model that is similar to HR-BiLSTM (Yu et al., 2017). Two BiLSTMs (Hochreiter and Schmidhuber, 1997) are used to encode the sentence and relation respectively given their GloVe word embedding (Pennington et al., 2014). Cosine similarity between the sentence and relation embedding is computed as the score. Relation with maximum score is predicted by the model for the sentence. Ranking loss is used to train the model6 . This base model is our model f , which is trained on a new task k at each step and results in an updated model f (k) . Our proposed approach (Figure 1) inserts an alignment model a to explicitly align to embedding space for stored instances and maintain the embedding space of the current task. Note that the label y (the relation here) also has embedding, so it needs to pass through the alignm"
N19-1086,P15-1128,0,0.0253822,"alignment model to mitigate the sentence embedding distortion of the learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches. 1 Introduction The task of relation detection/extraction aims to recognize entity pairs’ relationship from given contexts. As an essential component for structured information extraction, it has been widely used in downstream tasks such as automatic knowledgebased completion (Riedel et al., 2013) and question answering (Yih et al., 2015; Yu et al., 2017). Existing relation detection methods always assume a closed set of relations and perform once∗ Co-mentoring Code and dataset can be found in this repository: https://github.com/hongwang600/Lifelong_ Relation_Detection 796 Proceedings of NAACL-HLT 2019, pages 796–806 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics long learning is to learn a classification model f . At each step k, f observes the task T (k) , and optimizes the loss function on its training data with a loss function `(f (x), y). At the same time, we require the m"
N19-1086,N13-1008,0,0.0685422,"g space. Specifically, we utilize an explicit alignment model to mitigate the sentence embedding distortion of the learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches. 1 Introduction The task of relation detection/extraction aims to recognize entity pairs’ relationship from given contexts. As an essential component for structured information extraction, it has been widely used in downstream tasks such as automatic knowledgebased completion (Riedel et al., 2013) and question answering (Yih et al., 2015; Yu et al., 2017). Existing relation detection methods always assume a closed set of relations and perform once∗ Co-mentoring Code and dataset can be found in this repository: https://github.com/hongwang600/Lifelong_ Relation_Detection 796 Proceedings of NAACL-HLT 2019, pages 796–806 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics long learning is to learn a classification model f . At each step k, f observes the task T (k) , and optimizes the loss function on its training data with a loss function `(f (x"
N19-1086,D16-1022,0,0.167323,"Missing"
N19-1086,P17-2023,0,0.0985245,"Missing"
P11-1016,H05-1045,0,0.0392263,"Missing"
P11-1016,C10-2028,0,0.835958,"Missing"
P11-1016,W06-0301,0,0.119207,"Missing"
P11-1016,H05-1066,0,0.0223227,"Missing"
P11-1016,P04-1035,0,0.0542744,"sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned above, these approaches work in a target-independent way, and so need to be adapted for target-dependent sentiment classification. 3 Approach Overview The problem we address in this paper is targetdependent sentiment classification of tweets. So the input of our task is a collection of tweets containing the target and the output is labels assigned to each of the tweets. Inspired by (Barbosa and Feng, 2010; Pang and Lee, 2004), we design a three-step approach in this paper: 1. Subjectivity classification as the first step to decide if the tweet is subjective or neutral about the target; 2. Polarity classification as the second step to decide if the tweet is positive or negative about the target if it is classified as subjective in Step 1; 3. Graph-based optimization as the third step to further boost the performance by taking the related tweets into consideration. In each of the first two steps, a binary SVM classifier is built to perform the classification. To train the classifiers, we use SVM-Light 6 with a linea"
P11-1016,W02-1011,0,0.0397246,"timent target as a query, and search for tweets containing positive or negative sentiments towards the target. The problem needing to be addressed can be formally named as Target-dependent Sentiment Classification of Tweets; namely, given a query, classifying the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem, such as (Go et al., 2009 5 ; Barbosa and Feng, 2010), basically follow (Pang et al., 2002), who utilize machine learning based classifiers for the sentiment classification of texts. However, their classifiers actually work in a target-independent way: all the features used in the classifiers are independent of the target, so the sentiment is decided no matter what the target is. Since (Pang et al., 2002) (or later research on sentiment classification Abstract Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positiv"
P11-1016,J01-4004,0,0.0118864,"ependent sentiment classification. In addition to the noun phrases including the target, we further expand the extended target set with the following three methods: 1. Adding mentions co-referring to the target as new extended targets. It is common that people use definite or demonstrative noun phrases or pronouns referring to the target in a tweet and express sentiments directly on them. For instance, in “Oh, Jon Stewart. How I love you so.”, the author expresses a positive sentiment to “you” which actually refers to “Jon Stewart”. By using a simple co-reference resolution tool adapted from (Soon et al., 2001), we add all the mentions referring to the target into the extended target set. 2. Identifying the top K nouns and noun phrases which have the strongest association with the target. Here, we use Pointwise Mutual Information (PMI) to measure the association. PMI ( w, t )  log   Target-dependent Features Target-dependent sentiment classification needs to distinguish the expressions describing the target from other expressions. In this paper, we rely on the syntactic parse tree to satisfy this need. Specifically, for any word stem wi in a tweet which has one of the following relations with the"
P11-1016,P02-1053,0,0.026769,"e Lakers, we can confidently classify this tweet as positive. The remainder of this paper is structured as follows. In Section 2, we briefly summarize related work. Section 3 gives an overview of our approach. We explain the target-dependent and contextaware approaches in detail in Sections 4 and 5 respectively. Experimental results are reported in Section 6 and Section 7 concludes our work. 2 Related Work In recent years, sentiment analysis (SA) has become a hot topic in the NLP research community. A lot of papers have been published on this topic. 2.1 2.3 Target-independent SA Specifically, Turney (2002) proposes an unsupervised method for classifying product or movie reviews as positive or negative. In this method, sentimental phrases are first selected from the reviews according to predefined part-of-speech patterns. Then the semantic orientation score of each phrase is calculated according to the mutual information values between the phrase and two predefined seed words. Finally, a review is classified based on the average semantic orientation of the sentimental phrases in the review. In contrast, (Pang et al., 2002) treat the sentiment classification of movie reviews simply as a special c"
P11-1016,H05-1044,0,0.209257,"Missing"
P11-1016,D12-1110,0,\N,Missing
P11-1016,J11-2001,0,\N,Missing
P11-1016,P03-1054,0,\N,Missing
P11-1016,C10-2005,0,\N,Missing
P11-1016,N10-1120,0,\N,Missing
P11-1016,P11-2008,0,\N,Missing
P11-1016,P97-1023,0,\N,Missing
P13-2056,P11-1061,0,0.169857,"ai Yu2 1 School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China {yumo,tjzhao,ylbai}@mtlab.hit.edu.cn 2 Baidu Inc., Beijing, China {tianhao,yudianhai}@baidu.com Abstract that target language has to be similar to source language. Otherwise the performance will degrade especially when the orders of phrases between source and target languages differ a lot. Another common type of projection methods map labels from resource-rich language sentences to resource-scarce ones in a parallel corpus using word alignment information (Yarowsky et al., 2001; Hwa et al., 2005; Das and Petrov, 2011). We refer them as projection based on word alignments in this paper. Compared to other types of projection methods, this type of methods is more robust to syntactic differences between languages since it trained models on the target side thus following the topology of the target language. This paper aims to build an accurate projection method with strong generality to various pairs of languages, even when the languages are from different families and are typologically divergent. As far as we know, only a few works focused on this topic (Xia and Lewis 2007; T¨ackstr¨om et al., 2013). We adopte"
P13-2056,P10-1002,0,0.0616785,"Missing"
P13-2056,D11-1006,0,0.0535494,"languages. Unfortunately, it is impossible to build sufficient labeled data for all tasks in all languages. To address NLP tasks in resource-scarce languages, cross-lingual projection methods were proposed, which make use of existing resources in resource-rich language (also called source language) to help NLP tasks in resource-scarce language (also named as target language). There are several types of projection methods. One intuitive and effective method is to build a common feature space for all languages, so that the model trained on one language could be directly used on other languages (McDonald et al., 2011; T¨ackstr¨om et al., 2012). We call it direct projection, which becomes very popular recently. The main limitation of these methods is 312 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 312–317, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Firstly, we introduce Brown clusters of target language to make the projection models cover broader cases. Brown clustering is a kind of word representations, which assigns word with similar functions to the same cluster. They can be efficiently learned on large-scale unla"
P13-2056,N12-1052,0,0.0481418,"Missing"
P13-2056,N13-1126,0,0.0726683,"Missing"
P13-2056,I05-3027,0,0.0449224,"projection of POS tagging, we used the test set of CTB. Since English and Chinese have different annotation standards, labels in the two languages were converted to the universal POS tag set (Petrov et al., 2011; Das and Petrov, 2011) so that the labels between the source and target languages were consistent. The universal tag set made the task of POS tagging easier since the fine-grained types are no more cared. The Brown clusters were trained on Chinese Wikipedia. The bodies of all articles are retained to induce 1000 clusters using the algorithm in (Liang, 2005) . Stanford word segmentor (Tseng et al., 2005) was used for Chinese word segmentation. When English Brown clusters were in need, we trained the word clusters on the tokenized English Wikipedia. We chose LDC2003E14 as the parallel corpus, which contains about 200,000 sentences. GIZA++ (Och and Ney, 2000) was used to generate word alignments. It is easier to obtain similar amount of parallel sentences between English and minor languages, making the conclusions more general for problems of projection in real applications. 3.2 avg Prec 47.48 71.6 63.96 73.44 avg Rec 28.12 37.84 46.59 47.63 avg F1 33.91 47.66 53.75 56.60 Table 2: Performances"
P13-2056,N07-1057,0,0.0251259,"ky et al., 2001; Hwa et al., 2005; Das and Petrov, 2011). We refer them as projection based on word alignments in this paper. Compared to other types of projection methods, this type of methods is more robust to syntactic differences between languages since it trained models on the target side thus following the topology of the target language. This paper aims to build an accurate projection method with strong generality to various pairs of languages, even when the languages are from different families and are typologically divergent. As far as we know, only a few works focused on this topic (Xia and Lewis 2007; T¨ackstr¨om et al., 2013). We adopted the projection method based on word alignments since it is less affected by language differences. However, such methods also have some disadvantages. Firstly, the models trained on projected data could only cover words and cases appeared in the target side of parallel corpus, making it difficult to generalize to test data in broader domains. Secondly, the performances of these methods are limited by the accuracy of word alignments, especially when words between two languages are not one-one aligned. So the obtained labeled data contains a lot of noises,"
P13-2056,H01-1035,0,0.0665547,"Tiejun Zhao1 Yalong Bai1 Hao Tian2 Dianhai Yu2 1 School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China {yumo,tjzhao,ylbai}@mtlab.hit.edu.cn 2 Baidu Inc., Beijing, China {tianhao,yudianhai}@baidu.com Abstract that target language has to be similar to source language. Otherwise the performance will degrade especially when the orders of phrases between source and target languages differ a lot. Another common type of projection methods map labels from resource-rich language sentences to resource-scarce ones in a parallel corpus using word alignment information (Yarowsky et al., 2001; Hwa et al., 2005; Das and Petrov, 2011). We refer them as projection based on word alignments in this paper. Compared to other types of projection methods, this type of methods is more robust to syntactic differences between languages since it trained models on the target side thus following the topology of the target language. This paper aims to build an accurate projection method with strong generality to various pairs of languages, even when the languages are from different families and are typologically divergent. As far as we know, only a few works focused on this topic (Xia and Lewis 2"
P13-2056,J92-4003,0,\N,Missing
P13-2056,petrov-etal-2012-universal,0,\N,Missing
P14-2089,P05-1077,0,0.0423536,"., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms another. Controlling this behavior is challenging with an unsupervised objective. However, rich prior knowledge exist"
P14-2089,D13-1170,0,0.00486117,"not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improve"
P14-2089,P10-1040,0,0.177376,"ments on word pairs. Word embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for"
P14-2089,P10-2043,0,0.0244334,"Missing"
P14-2089,N13-1092,0,0.163013,"Missing"
P14-2089,P12-1092,0,0.502229,"semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of"
P14-2089,W13-3512,0,0.0760931,"We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms ano"
P15-2062,D12-1032,1,0.598705,"due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in determining coreference. In contrast, standalone name matching modules are context independent (Andrews et al., 2012; Green et al., 2012). In addition to showing name matching improvements on newly developed datasets of matched Chinese name pairs, we show improvements in a downstream Chinese entity clustering task by using our improved name matching system. We call our name matching tool Mingpipe, a Python package that can be used as a standalone tool or integrated within a larger system. We release Mingpipe as well as several datasets to support further work on this task.1 Methods for name matching, an important component to support downstream tasks such as entity linking and entity clustering, have focuse"
P15-2062,W04-3248,0,0.113265,"Missing"
P15-2062,N03-1003,0,0.170748,"clustering task. 1 Introduction A key technique in entity disambiguation is name matching: determining if two mention strings could refer to the same entity. The challenge of name matching lies in name variation, which can be attributed to many factors: nicknames, aliases, acronyms, and differences in transliteration, among others. In light of these issues, exact string match can lead to poor results. Numerous downstream tasks benefit from improved name matching: entity coreference (Strube et al., 2002), name transliteration (Knight and Graehl, 1998), identifying names for mining paraphrases (Barzilay and Lee, 2003), entity linking (Rao et al., 2013) and entity clustering (Green et al., 2012). As a result, there have been numerous proposed name matching methods (Cohen et al., 2003), with a focus on person names. Despite extensive exploration of this task, most work has focused on IndoEuropean languages in general and English in particular. These languages use alphabets as representations of written language. In contrast, other languages use logograms, which represent a word 2 Name Matching Methods Name matching originated as part of research into record linkage in databases. Initial work focused 1 The co"
P15-2062,D07-1020,0,0.0367092,"0.05) Features ALL - Jaccard similariy - Levenshtein - Simplified pairs - Pinyin pairs - Others Exact match Jaro-winkler Levenshtein Transducer SVM Precision 84.55 84.87 83.16 90.33 90.05 Dev Recall 57.46 58.35 61.13 74.92 63.90 F1 68.42 69.15 70.46 81.90 74.75 Precision 63.95 70.79 69.56 73.59 74.33 Test Recall 65.44 66.21 67.27 63.70 67.60 F1 64.69 68.42 68.40 68.29 70.81 Table 7: Results on Chinese entity clustering. (cross document coreference resolution), where the goal is identify co-referent named mentions across documents. Only a few studies have considered Chinese entity clustering (Chen and Martin, 2007), including the TAC KBP shared task, which has included clustering Chinese NIL mentions (Ji et al., 2011). We construct an entity clustering dataset from the TAC KBP entity linking data. All of the 2012 Chinese data is used as development, and the 2013 data as test. We use the system of Green et al. (2012), which allows for the inclusion of arbitrary name matching metrics. We follow their setup for training and evaluation (B3 ) and use TF-IDF context features. We tune the clustering cutoff for their hierarchical model, as well as the name matching threshold on the development data. For the tra"
P15-2062,W10-4152,0,0.186548,"chnology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 2 Machine Intelligence and Translation Lab Harbin Institute of Technology, Harbin, China npeng1@jhu.edu, gflfof@gmail.com, mdredze@cs.jhu.edu Abstract or morpheme, the most popular being Chinese which uses hanzi (汉字). This presents challenges for name matching: a small number of hanzi represent an entire name and there are tens of thousands of hanzi in use. Current methods remain largely untested in this setting, despite downstream tasks in Chinese that rely on name matching (Chen et al., 2010; Cassidy et al., 2011). Martschat et al. (2012) point out errors in coreference resolution due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in deter"
P15-2062,W12-4511,0,0.0715765,"Missing"
P15-2062,P14-2102,1,0.856086,"antage of trained models is that, with sufficient training data, they can be tuned for specific tasks. While many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010; Cassidy et al., 2011; Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998; Bouchard-Cˆot´e et al., 2008; Dreyer et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al., 2011a; McNamee et al.,"
P15-2062,D08-1113,0,0.0282315,"t al., 2003). The advantage of trained models is that, with sufficient training data, they can be tuned for specific tasks. While many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010; Cassidy et al., 2011; Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998; Bouchard-Cˆot´e et al., 2008; Dreyer et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al."
P15-2062,I11-1029,0,0.438392,"r et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al., 2011a; McNamee et al., 2011b) and cross lingual entity clustering (Green et al., 2012). However, little work has focused on logograms, with the exception of Cheng et al. (2011). As we will demonstrate in § 3, there are special challenges caused by the logogram nature of Chinese. We believe this is the first evaluation of Chinese name matching. 3 Notes simplified v.s. traditional Abbreviation and traditional v.s. simplified Transliteration of Addis Ababa in Mainland and Taiwan. Different hanzi, similar pronunciations. Transliteration of Florence in Mainland and Hong Kong. Different writing and dial"
P15-2062,J01-4004,0,0.395596,"ting, despite downstream tasks in Chinese that rely on name matching (Chen et al., 2010; Cassidy et al., 2011). Martschat et al. (2012) point out errors in coreference resolution due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in determining coreference. In contrast, standalone name matching modules are context independent (Andrews et al., 2012; Green et al., 2012). In addition to showing name matching improvements on newly developed datasets of matched Chinese name pairs, we show improvements in a downstream Chinese entity clustering task by using our improved name matching system. We call our name matching tool Mingpipe, a Python package that can be used as a standalone tool or integrated within a larger system. We release Mingpipe as well as several dat"
P15-2062,W02-1040,0,0.0355989,"Missing"
P15-2062,C10-1145,0,0.0334396,"atching. We consider two new pinyin representations. Since each Chinese character corresponds to a pinyin, we take each pinyin as a token corresponding to the Chinese character. We call this “character-pinyin”. Additionally, every Mandarin syllable (represented by a pinyin) can be spelled with a combination of an initial and a final segment. Therefore, we split each pinyin token further into the initial and final segment. We call this “segmented-pinyin”5 . Name Matching as Classification An alternate learning formulation considers name matching as a classification task (Mayfield et al., 2009; Zhang et al., 2010; Green et al., 2012). Each string pair is an instance: a positive classification means that two strings can refer to the same name. This allows for arbitrary and global features of the two strings. We use an SVM with a linear kernel. To learn possible edit rules for Chinese names we add features for pairs of n-grams. For each string, we extract all n-grams (n=1,2,3) and align n-grams between strings using the Hungarian algorithm.6 Features correspond to the aligned ngram pairs, as well as the unaligned n-grams. To reduce the number of parameters, we only include features which appear in posit"
P15-2062,N12-1007,1,\N,Missing
P15-2062,J98-4003,0,\N,Missing
P17-1053,C16-1236,0,0.0667671,"Missing"
P17-1053,D13-1160,0,0.384459,"ames via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field su"
P17-1053,P14-2012,0,0.113812,"extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of re"
P17-1053,D16-1244,0,0.0119806,"Missing"
P17-1053,S10-1057,0,0.0312402,"m to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2"
P17-1053,P16-1076,0,0.0424625,"embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE re"
P17-1053,D16-1054,0,0.0546844,"Missing"
P17-1053,P15-1061,1,0.455256,"rks on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wiki"
P17-1053,P11-1053,0,0.0156809,"art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et a"
P17-1053,P13-1158,0,0.0412673,"ation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571–581 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1053 Question: what episode was mike kelley the writer of Entity Linking Question: what tv show did grant show play on in 2008 Entity Link"
P17-1053,D16-1166,0,0.291728,"tion is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE research. While for data sets like SimpleQuestions and ParaLex"
P17-1053,N16-1065,0,0.032177,"Missing"
P17-1053,D15-1205,1,0.564694,"Missing"
P17-1053,P16-1123,0,0.048317,"Missing"
P17-1053,C16-1164,1,0.738237,"Missing"
P17-1053,N16-1170,0,0.0445311,"Missing"
P17-1053,N16-1117,1,0.8342,"ng capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of relations in KBQA. As a result, few work in this field focuses on dealing with large number of relations or unseen relations. Yu et al. (2016) proposed to use relation embeddings in a low-rank tensor method. However their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments. search assumes that the two argument entities are both available. Thus it usually benefits from features (Nguyen and Grishman, 2014; Gormley et al., 2015) or attention mechanisms (Wang et al., 2016) based on the entity information (e.g. entity types or entity embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the"
P17-1053,C14-1220,0,0.029909,"n this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it c"
P17-1053,P16-1220,0,0.44264,"entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55"
P17-1053,P15-1049,0,0.0158297,"sts of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6 . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. We tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({50, 100, 200, 400})9 ; (2) learning ra"
P17-1053,P05-1053,0,0.147839,"r simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Ta"
P17-1053,P16-2034,0,0.0088906,"t paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of rela"
P17-1053,W14-2416,0,0.0230301,"Missing"
P17-1053,P14-1090,0,0.128408,"Missing"
P17-1053,P15-1128,0,0.342367,"m that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Pro"
P17-1053,P14-2105,0,0.0490471,"nchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571–581 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1053 Question: what episode was mike kelley the writer of Entity Linking Question: what tv show did grant show play on in 2008 Entity Linking Relation Detec"
P17-1053,P16-2033,0,0.0532749,"ion KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6 . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. We tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({"
P18-2118,P16-1223,0,0.0515277,"sentence-level aggregation of the co-matching states. All these representations will be further integrated by another Bi-LSTM to get the final triplet matching representation. Hs = [hs1 ; hs2 ; . . . ; hsN ], ht = MaxPooling (Bi-LSTM (Hs )) , (5) 748 RACE-M RACE-H RACE Random Sliding Window Stanford AR GA ElimiNet HAF MUSIC 24.6 37.3 44.2 43.7 45.3 51.5 25.0 30.4 43.0 44.2 47.9 45.7 24.9 32.2 43.3 44.1 44.7 47.2 47.4 Hier-Co-Matching - Hier-Aggregation - Co-Matching 55.8∗ 54.2 50.7 48.2∗ 46.2 45.6 50.4∗ 48.5 46.4 Turkers Ceiling 85.1 95.4 69.4 94.2 73.3 94.5 • Stanford Attentive Reader (AR) (Chen et al., 2016) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer representation to get the answer probabilities. • GA (Dhingra et al., 2017) uses gated attention mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers. • ElimiNet (Soham et al., 2017) tries to first eliminate the most irrelevant choices and then select the best answer. • HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidat"
P18-2118,P17-1168,0,0.0804797,"sN ], ht = MaxPooling (Bi-LSTM (Hs )) , (5) 748 RACE-M RACE-H RACE Random Sliding Window Stanford AR GA ElimiNet HAF MUSIC 24.6 37.3 44.2 43.7 45.3 51.5 25.0 30.4 43.0 44.2 47.9 45.7 24.9 32.2 43.3 44.1 44.7 47.2 47.4 Hier-Co-Matching - Hier-Aggregation - Co-Matching 55.8∗ 54.2 50.7 48.2∗ 46.2 45.6 50.4∗ 48.5 46.4 Turkers Ceiling 85.1 95.4 69.4 94.2 73.3 94.5 • Stanford Attentive Reader (AR) (Chen et al., 2016) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer representation to get the answer probabilities. • GA (Dhingra et al., 2017) uses gated attention mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers. • ElimiNet (Soham et al., 2017) tries to first eliminate the most irrelevant choices and then select the best answer. • HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidate answer, but also the matching between the candidate answers. • MUSIC (Xu et al., 2017) integrates different sequence matching strategies into the model and also adds a unit of multi-step reasoning for sele"
P18-2118,P82-1020,0,0.828018,"Missing"
P18-2118,D17-1082,0,0.37953,"poses a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance. 1 Introduction Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016). Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in Figure 1. Thus, answering these questions is more chall"
P18-2118,D16-1264,0,0.0501101,"een a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance. 1 Introduction Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016). Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passages, as illustrated by the two example questions (Q1 & Q2) in Figu"
P18-2118,D13-1020,0,0.133603,"prehension is a challenging task, which involves the matching between a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem, which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance. 1 Introduction Enabling machines to understand natural language text is arguably the ultimate goal of natural language processing, and the task of machine reading comprehension is an intermediate step towards this ultimate goal (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016). Recently, Lai et al. (2017) released a new multi-choice machine comprehension dataset called RACE that was extracted from middle and high school English examinations in China. Figure 1 shows an example passage and two related questions from RACE. The key difference between RACE and previously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016)) is that the answers in RACE often cannot be directly extracted from the given passag"
P18-2118,P15-1150,0,0.0139012,"nces and we use P1 , P2 , . . . , PN to represent these sentences, where N is the number of sentences in the passage. For each triplet {Pn , Q, A}, n ∈ [1, N ], we can get the co-matching states Cn through Eqn. (1-3). Then we build a bi-directional LSTM followed by max pooling on top of the comatching states of each sentence as follows: where Wg ∈Rl×2l and bg ∈ Rl are the parame· ters to learn. is the column-wise concatenation · of two matrices, and · · and · ⊗ · are the elementwise subtraction and multiplication between two matrices, which are used to build better matching representations (Tai et al., 2015; Wang and Jiang, 2017). Mq ∈ Rl×P represents the matching between the hidden states of the passage and the corresponding attention-weighted representations of the question. Similarly, we match the passage with the candidate answer and represent the matching results using Ma ∈ Rl×P . Finally C ∈ R2l×P is the concatenation of Mq ∈ Rl×P hsn = MaxPooling (Bi-LSTM (Cn )) , (4) where the function MaxPooling(·) is the row-wise max pooling operation. hsn ∈ Rl , n ∈ [1, N ] is the sentence-level aggregation of the co-matching states. All these representations will be further integrated by another Bi-L"
P18-2118,D15-1167,0,0.261894,"r made his living by driving. b. The author’s wife supported to buy the island. c. Blue and red are the main colors of his national flag. d. People can travel around the island free of charge. Q2: How did the author get the island? a. It was a present from Blandy. b. The king sold it to him. c. He bought it from Blandy. d. He inherited from his father. Table 1: An example passage and two related multi-choice questions. The ground-truth answers are in bold. process. Finally in Section 2.3 we present the objective function. An overview of our co-matching model is shown in Figure 2. chical LSTM (Tang et al., 2015) over the sequence of co-matching states at different positions of the passage. Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level. In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage. Our model improves the state-of-the-art model by 3 percentage on the RACE dataset. Our code will be released under https://github. com/shuohangwang/comatch. 2 2.1 Co-matching The co-matching part of our model aims to match the passage with the question and the candidate answer at the"
P18-2118,P16-1041,0,0.0113818,"the passage. Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level. In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage. Our model improves the state-of-the-art model by 3 percentage on the RACE dataset. Our code will be released under https://github. com/shuohangwang/comatch. 2 2.1 Co-matching The co-matching part of our model aims to match the passage with the question and the candidate answer at the word-level. Inspired by some previous work (Wang and Jiang, 2016; Trischler et al., 2016), we first use bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) to pre-process the sequences as follows: Model Hp = Bi-LSTM(P), Hq = Bi-LSTM(Q), Ha = Bi-LSTM(A), For the task of multi-choice reading comprehension, the machine is given a passage, a question and a set of candidate answers. The goal is to select the correct answer from the candidates. Let us use P ∈ Rd×P , Q ∈ Rd×Q and A ∈ Rd×A to represent the passage, the question and a candidate answer, respectively, where each word in each sequence is represented by an embedding vector. d is the dimensionality of the embeddings, and P"
P18-2118,N16-1170,1,0.781208,"different positions of the passage. Information is aggregated from wordlevel to sentence-level and then from sentencelevel to document-level. In this way, our model can better deal with the questions that require evidence scattered in different sentences in the passage. Our model improves the state-of-the-art model by 3 percentage on the RACE dataset. Our code will be released under https://github. com/shuohangwang/comatch. 2 2.1 Co-matching The co-matching part of our model aims to match the passage with the question and the candidate answer at the word-level. Inspired by some previous work (Wang and Jiang, 2016; Trischler et al., 2016), we first use bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) to pre-process the sequences as follows: Model Hp = Bi-LSTM(P), Hq = Bi-LSTM(Q), Ha = Bi-LSTM(A), For the task of multi-choice reading comprehension, the machine is given a passage, a question and a set of candidate answers. The goal is to select the correct answer from the candidates. Let us use P ∈ Rd×P , Q ∈ Rd×Q and A ∈ Rd×A to represent the passage, the question and a candidate answer, respectively, where each word in each sequence is represented by an embedding vector. d is the dimensionality"
P18-2118,W16-0103,0,0.0296719,"Missing"
P19-1132,P18-2077,0,0.0234535,"ection describes the proposed one-pass encoding MRE solution. The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding, and an entity-aware self-attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states. The framework is illustrated in Figure 1. It is worth mentioning that our solution can easily use other transformer-based encoders besides BERT, e.g. (Radford et al., 2018). 3.1 to perform edge predictions over a graph with entities as nodes. Inspired by (Dozat and Manning, 2018; Ahmad et al., 2018), we propose that we can first encode the input paragraph using BERT. Thus, the representation for a pair of entity mentions (ei , ej ) can be denoted as oi and oj respectively. In the case of a mention ei consist of multiple hidden states (due to the byte pair encoding), oi is aggregated via average-pooling over the hidden states of the corresponding tokens in the last BERT layer. We then concatenate oi and oj denoted as [oi : oj ], and pass it to a linear classifier2 to predict the relation N X j=1 exp eij (hj WV + aVij ), PN exp e ik k=1 √ where eij = hi WQ (hj WK + aK"
P19-1132,D15-1204,0,0.0192228,"r et al., 2006) and ERE (Linguistic Data Consortium, 2013). In MRE, given as a text paragraph x = {x1 , . . . , xN } and M mentions e = {e1 , . . . , eM } as input, the goal is to predict the relation rij for each mention pair (ei , ej ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation. This paper uses “entity mention”, “mention” and “entity” interchangeably. Existing MRE approaches are based on either feature and model architecture selection techniques (Xu et al., 2015; Gormley et al., 2015; Nguyen and Grishman, 2015; F. Petroni and Gemulla, 2015; Sorokin and Gurevych, 2017; Song et al., 2018b), or domain adaptations approaches (Fu et al., 2017; Shi et al., 2018). But these approaches require multiple passes of encoding over the paragraph, as they treat a MRE task as multiple passes of a SRE task. 3 P (rij |x, ei , ej ) = softmax(WL [oi : oj ] + b), 3.2 Entity-Aware Self-Attention based on Relative Distance This section describes how we encode multiplerelations information into the model. The key concept is to use the relative distances between words and entities to encode the positional information for each entity. This information i"
P19-1132,I17-2072,0,0.418704,", xN } and M mentions e = {e1 , . . . , eM } as input, the goal is to predict the relation rij for each mention pair (ei , ej ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation. This paper uses “entity mention”, “mention” and “entity” interchangeably. Existing MRE approaches are based on either feature and model architecture selection techniques (Xu et al., 2015; Gormley et al., 2015; Nguyen and Grishman, 2015; F. Petroni and Gemulla, 2015; Sorokin and Gurevych, 2017; Song et al., 2018b), or domain adaptations approaches (Fu et al., 2017; Shi et al., 2018). But these approaches require multiple passes of encoding over the paragraph, as they treat a MRE task as multiple passes of a SRE task. 3 P (rij |x, ei , ej ) = softmax(WL [oi : oj ] + b), 3.2 Entity-Aware Self-Attention based on Relative Distance This section describes how we encode multiplerelations information into the model. The key concept is to use the relative distances between words and entities to encode the positional information for each entity. This information is propagated through different layers via attention computations. Following (Shaw et al., 2018), for"
P19-1132,S18-1111,0,0.068481,"Missing"
P19-1132,D15-1205,1,0.893886,"Missing"
P19-1132,W09-2415,0,0.149148,"Missing"
P19-1132,S18-1125,0,0.0949375,"Missing"
P19-1132,S18-1128,0,0.0431103,"Missing"
P19-1132,Q14-1013,0,0.0635708,"Missing"
P19-1132,N13-1008,0,0.0435499,"ets easily; which makes it more usable in real-world applications. 1 1 Linear Pool … … Entity-aware Self-attention+ Feed-forward ×12 … in south suburbs of Bagh #dad and Iraqi artillery fired … Figure 1: Model Architecture. Different pairs of entities, e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are predicted simultaneously. Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion (Surdeanu et al., 2012; Riedel et al., 2013; Verga et al., 2016), knowledge base question answering (Yih et al., 2015; Xu et al., 2016; Yu et al., 2017), and symbolic approaches for visual question answering (Mao et al., 2019; Hu et al., 2019), etc. One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. However, nearly all exi"
P19-1132,S18-1112,0,0.057016,"Missing"
P19-1132,N18-2074,0,0.037064,"oaches (Fu et al., 2017; Shi et al., 2018). But these approaches require multiple passes of encoding over the paragraph, as they treat a MRE task as multiple passes of a SRE task. 3 P (rij |x, ei , ej ) = softmax(WL [oi : oj ] + b), 3.2 Entity-Aware Self-Attention based on Relative Distance This section describes how we encode multiplerelations information into the model. The key concept is to use the relative distances between words and entities to encode the positional information for each entity. This information is propagated through different layers via attention computations. Following (Shaw et al., 2018), for each pair of word tokens (xi , xj ) with the input representations from the previous layer as hi and hj , we extend the computation of self-attention zi as: zi = Structured Prediction with BERT for MRE The BERT model has been successfully applied to various NLP tasks. However, the final prediction layers used in the original model is not applicable to MRE tasks. The MRE task essentially requires (1) where WL ∈ R2dz ×l . dz is the dimension of BERT embedding at each token position, and l is the number of relation labels. Proposed Approach This section describes the proposed one-pass encod"
P19-1132,D18-1125,0,0.0844262,"Missing"
P19-1132,D18-1246,0,0.0171216,"13). In MRE, given as a text paragraph x = {x1 , . . . , xN } and M mentions e = {e1 , . . . , eM } as input, the goal is to predict the relation rij for each mention pair (ei , ej ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation. This paper uses “entity mention”, “mention” and “entity” interchangeably. Existing MRE approaches are based on either feature and model architecture selection techniques (Xu et al., 2015; Gormley et al., 2015; Nguyen and Grishman, 2015; F. Petroni and Gemulla, 2015; Sorokin and Gurevych, 2017; Song et al., 2018b), or domain adaptations approaches (Fu et al., 2017; Shi et al., 2018). But these approaches require multiple passes of encoding over the paragraph, as they treat a MRE task as multiple passes of a SRE task. 3 P (rij |x, ei , ej ) = softmax(WL [oi : oj ] + b), 3.2 Entity-Aware Self-Attention based on Relative Distance This section describes how we encode multiplerelations information into the model. The key concept is to use the relative distances between words and entities to encode the positional information for each entity. This information is propagated through different layers via atten"
P19-1132,D17-1188,0,0.0210026,"nguistic Data Consortium, 2013). In MRE, given as a text paragraph x = {x1 , . . . , xN } and M mentions e = {e1 , . . . , eM } as input, the goal is to predict the relation rij for each mention pair (ei , ej ) either belongs to one class of a list of pre-defined relations R or falls into a special class NA indicating no relation. This paper uses “entity mention”, “mention” and “entity” interchangeably. Existing MRE approaches are based on either feature and model architecture selection techniques (Xu et al., 2015; Gormley et al., 2015; Nguyen and Grishman, 2015; F. Petroni and Gemulla, 2015; Sorokin and Gurevych, 2017; Song et al., 2018b), or domain adaptations approaches (Fu et al., 2017; Shi et al., 2018). But these approaches require multiple passes of encoding over the paragraph, as they treat a MRE task as multiple passes of a SRE task. 3 P (rij |x, ei , ej ) = softmax(WL [oi : oj ] + b), 3.2 Entity-Aware Self-Attention based on Relative Distance This section describes how we encode multiplerelations information into the model. The key concept is to use the relative distances between words and entities to encode the positional information for each entity. This information is propagated through differe"
P19-1132,P15-1128,0,0.024403,"Pool … … Entity-aware Self-attention+ Feed-forward ×12 … in south suburbs of Bagh #dad and Iraqi artillery fired … Figure 1: Model Architecture. Different pairs of entities, e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are predicted simultaneously. Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion (Surdeanu et al., 2012; Riedel et al., 2013; Verga et al., 2016), knowledge base question answering (Yih et al., 2015; Xu et al., 2016; Yu et al., 2017), and symbolic approaches for visual question answering (Mao et al., 2019; Hu et al., 2019), etc. One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. However, nearly all existing approaches for MRE tasks (Qu et al., ∗ Equal contributions from the"
P19-1132,P17-1053,1,0.871275,"on+ Feed-forward ×12 … in south suburbs of Bagh #dad and Iraqi artillery fired … Figure 1: Model Architecture. Different pairs of entities, e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are predicted simultaneously. Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion (Surdeanu et al., 2012; Riedel et al., 2013; Verga et al., 2016), knowledge base question answering (Yih et al., 2015; Xu et al., 2016; Yu et al., 2017), and symbolic approaches for visual question answering (Mao et al., 2019; Hu et al., 2019), etc. One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. However, nearly all existing approaches for MRE tasks (Qu et al., ∗ Equal contributions from the corresponding authors: {wanghaoy,mi"
P19-1132,D12-1042,0,0.0654174,"scales to larger datasets easily; which makes it more usable in real-world applications. 1 1 Linear Pool … … Entity-aware Self-attention+ Feed-forward ×12 … in south suburbs of Bagh #dad and Iraqi artillery fired … Figure 1: Model Architecture. Different pairs of entities, e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are predicted simultaneously. Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion (Surdeanu et al., 2012; Riedel et al., 2013; Verga et al., 2016), knowledge base question answering (Yih et al., 2015; Xu et al., 2016; Yu et al., 2017), and symbolic approaches for visual question answering (Mao et al., 2019; Hu et al., 2019), etc. One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. Ho"
P19-1132,N16-1103,0,0.0246612,"es it more usable in real-world applications. 1 1 Linear Pool … … Entity-aware Self-attention+ Feed-forward ×12 … in south suburbs of Bagh #dad and Iraqi artillery fired … Figure 1: Model Architecture. Different pairs of entities, e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are predicted simultaneously. Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion (Surdeanu et al., 2012; Riedel et al., 2013; Verga et al., 2016), knowledge base question answering (Yih et al., 2015; Xu et al., 2016; Yu et al., 2017), and symbolic approaches for visual question answering (Mao et al., 2019; Hu et al., 2019), etc. One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. However, nearly all existing approaches for"
P19-1132,N18-1080,0,0.0485434,"Pass BERTSP (our model in §3.1) 64.42 Entity-Aware BERTSP (our full model) 67.46 BERTSP w/ entity-indicator on input-layer 65.32 BERTSP w/ pos-emb on final att-layer 67.23 67.09 69.25 66.86 69.13 53.20 61.70 57.65 58.68 52.73 58.48 53.56 55.04 57.67 63.14 59.36 60.95 Single-Relation per Pass BERTSP (our model in §3.1) 65.13 66.95 Entity-Aware BERTSP (our full model) 68.90 68.52 BERTSP w/ entity-indicator on input-layer 67.12 69.76 55.43 63.71 58.05 54.39 57.20 56.27 58.92 63.14 61.36 Table 1: Main Results on ACE 2005. directly to each token’s word embedding3 . This method is an extension of (Verga et al., 2018) to the MRE scenario. Hyperparameters For our experiments, most model hyperparameters are the same as in pretraining. We tune the training epochs and the new hyperparameter k (in Eq. 4) on the development set of ACE 2005. Since the SemEval task has no development set, we use the best hyperparameters selected on ACE. For the number of training epochs, we make the model pass similar number of training instances as in ACE 2005. 4.2 Results on ACE 2005 Main Results Table 1 gives the overall results on ACE 2005. The first observation is that our model architecture achieves much better results compa"
P19-1132,P16-1123,0,0.210507,"Missing"
P19-1132,D15-1062,1,0.902481,"Missing"
P19-1132,P16-1220,1,0.872055,"ware Self-attention+ Feed-forward ×12 … in south suburbs of Bagh #dad and Iraqi artillery fired … Figure 1: Model Architecture. Different pairs of entities, e.g., (Iraqi and artillery), (southern suburbs, Baghdad) are predicted simultaneously. Relation extraction (RE) aims to find the semantic relation between a pair of entity mentions from an input paragraph. A solution to this task is essential for many downstream NLP applications such as automatic knowledge-base completion (Surdeanu et al., 2012; Riedel et al., 2013; Verga et al., 2016), knowledge base question answering (Yih et al., 2015; Xu et al., 2016; Yu et al., 2017), and symbolic approaches for visual question answering (Mao et al., 2019; Hu et al., 2019), etc. One particular type of the RE task is multiplerelations extraction (MRE) that aims to recognize relations of multiple pairs of entity mentions from an input paragraph. Because in real-world applications, whose input paragraphs dominantly contain multiple pairs of entities, an efficient and effective solution for MRE has more important and more practical implications. However, nearly all existing approaches for MRE tasks (Qu et al., ∗ Equal contributions from the corresponding aut"
P19-1214,W04-3252,0,0.0620575,"ces But the flight was cancelled due to the weather. But I lost my passport. The meeting was cancelled. The weather is good today. Figure 1: An example for the Mask pre-training task. A sentence is masked in the original paragraph, and the model is required to predicted the missing sentence from the candidate sentences. Introduction Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles. This paradigm has been proven effective by many previous systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015). In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document. However, previous works (Nallapati et al., 2017; Al-Sabahi et al., 2018; Zhou et al., 2018; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to le"
P19-1214,K16-1028,0,0.0886086,"Missing"
P19-1214,W17-5308,0,0.0545636,"Missing"
P19-1214,P07-1010,0,0.0362907,"o learn the contextualized sentence representation with self-supervision. Self-supervised learning (Raina et al., 2007; Doersch et al., 2015; Agrawal et al., 2015; Wang and Gupta, 2015) is a newly emerged paradigm, which aims to learn from the intrinsic structure of the raw data. The general framework is to construct training signals directly from the structured raw data, and use it to train the model. The structure information learned through the process can then be easily transformed and benefit other tasks. Thus self-supervised learning has been widely applied in structured data like text (Okanohara and Tsujii, 2007; Collobert and Weston, 2008; Peters et al., 2018; Devlin et al., 2018; Wu et al., 2019) and images (Doersch et al., 2015; Agrawal et al., 2015; Wang and Gupta, 2015; Lee et al., 2017). 2221 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2221–2227 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Since documents are well organized and structured, it is intuitive to employ the power of selfsupervised learning to learn the intrinsic structure of the document and model the document-level context for the summ"
P19-1214,N18-1049,0,0.0259131,"ystem can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 2014; Nie and Bansal, 2017; Lin et al., 2017; Peters et al., 2018; Devlin et al., 2018; Subramanian et al., 2018; Cer et al., 2018; Logeswaran and Lee, 2018; Pagliardini et al., 2018) have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision. Self-supervised learning (Raina et al., 2007; Doersch et al., 2015; Agrawal et al., 2015; Wang and Gupta, 2015) is a newly emerged paradigm, which aims to learn from the intrinsic structure of the raw data"
P19-1214,D14-1162,0,0.0949777,"8; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 2014; Nie and Bansal, 2017; Lin et al., 2017; Peters et al., 2018; Devlin et al., 2018; Subramanian et al., 2018; Cer et al., 2018; Logeswaran and Lee, 2018; Pagliardini et al., 2018) have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision. Self-supervised learning ("
P19-1214,N18-1202,0,0.0365077,"ing system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 2014; Nie and Bansal, 2017; Lin et al., 2017; Peters et al., 2018; Devlin et al., 2018; Subramanian et al., 2018; Cer et al., 2018; Logeswaran and Lee, 2018; Pagliardini et al., 2018) have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision. Self-supervised learning (Raina et al., 2007; Doersch et al., 2015; Agrawal et al., 201"
P19-1214,P19-1375,1,0.839455,"Missing"
P19-1214,D18-1088,0,0.056554,"troduction Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles. This paradigm has been proven effective by many previous systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015). In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document. However, previous works (Nallapati et al., 2017; Al-Sabahi et al., 2018; Zhou et al., 2018; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 201"
P19-1214,P18-1061,0,0.365384,"idate sentences. Introduction Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles. This paradigm has been proven effective by many previous systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015). In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document. However, previous works (Nallapati et al., 2017; Al-Sabahi et al., 2018; Zhou et al., 2018; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (P"
P19-1214,P17-1099,0,0.0573945,"ng these selected sentences, i.e., each selected sentence will be put in another position within the same document. Let Cs be the set of positions where the sentences are switched. Similarly, we use a linear layer fs to predict if a sentence is switched and minimize the MSE loss: `s = MSE(fs (Di ), yis ) where yis = 1 if i ∈ Cs , otherwise yis = 0. 3 Experiment To show the effectiveness of the pre-training method (Mask, Replace and Switch), we conduct experiments on the commonly used dataset CNN/DM (Hermann et al., 2015; Nallapati et al., 2016), and compare them with a popular baseline Lead3 (See et al., 2017), which selects first three sentences as the summary, and the state-of-theart extractive summarization method N EU S UM (Zhou et al., 2018), which jointly scores and selects sentences using pointer network. 3.1 On CNN/DM Dataset Model and training details We use the rulebased system from (Zhou et al., 2018) to label sentences in a document, e.g., sentences to be extracted will be labeled as 1. Rouge score3 (Lin, 2004) is used to evaluate the performance of the model, and we report Rouge-1, Rouge-2, and Rouge-L as in prior work. We use the pretrained glove embedding (Pennington et al., 2014) wi"
P19-1214,W04-1013,0,\N,Missing
P19-1214,D18-2029,0,\N,Missing
P19-1214,N19-1423,0,\N,Missing
P19-1304,D18-1246,1,0.897995,"ic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index. This aggregator 1 Lebron James is transl"
P19-1304,D18-1032,0,0.0812748,"ultilingual knowledge graphs (KGs), such as DBpedia (Auer et al., 2007) and Yago (Suchanek et al., 2007), represent human knowledge in the structured format and have been successfully used in many natural language processing applications. These KGs encode rich monolingual knowledge but lack the cross-lingual links to bridge the language gap. Therefore, the cross-lingual KG alignment task, which automatically matches entities in a multilingual KG, is proposed to address this problem. Most recently, several entity matching based approaches (Hao et al., 2016; Chen et al., 2016; Sun et al., 2017; Wang et al., 2018) have been proposed for this task. Generally, these approaches first project entities of each KG into lowdimensional vector spaces by encoding monolingual KG facts, and then learn a similarity score function to match entities based on their vector representations. However, since some entities in different languages may have different KG To address these drawbacks, we propose a topic entity graph to represent the KG context information of an entity. Unlike previous methods that utilize entity embeddings to match entities, we formulate this task as a graph matching problem between the topic enti"
P19-1304,Q17-1010,0,0.0352098,"8 37.29 74.49 83.45 91.56 84.71 92.35 EN-FR @1 @10 14.61 37.25 21.26 50.60 32.97 65.91 36.77 73.06 81.03 90.79 84.15 91.76 66.91 67.93 67.92 64.01 65.28 65.21 72.63 73.97 73.52 69.76 71.29 70.18 87.62 89.38 88.96 87.65 88.18 88.01 77.52 78.48 78.36 78.12 79.64 79.48 85.09 87.15 86.87 83.48 84.63 84.29 94.19 95.24 94.28 93.66 94.75 94.37 Table 1: Evaluation results on the datasets. non-linearity function σ is ReLU (Glorot et al., 2011) and the parameters of aggregators are randomly initialized. Since KGs are represented in different languages, we first retrieve monolingual fastText embeddings (Bojanowski et al., 2017) for each language, and apply the method proposed in Conneau et al. (2017) to align these word embeddings into a same vector space, namely, crosslingual word embeddings. We use these embeddings to initialize word representations in the first layer of GCN1 . Results and Discussion. Following previous works, we used Hits@1 and Hits@10 to evaluate our model, where Hits@k measures the proportion of correctly aligned entities ranked in the top k. We implemented a baseline (referred as BASELINE in Table 1) that selects k closest G2 entities to a given G1 entity in the cross-lingual embedding space,"
P19-1304,D18-1223,1,0.789993,"Missing"
P19-1304,D18-1110,1,0.821566,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,D18-1112,1,0.597172,"will discuss in §4. 3 Graph Matching Model Figure 2 gives an overview of our method for aligning Lebron James in the English and Chinese knowledge graph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to ag"
P19-1304,P18-1030,0,0.0133111,"raph1 . Specifically, we fist retrieve topic entity graphs of Lebron James from two KGs, namely G1 and G2 . Then, we propose a graph matching model to estimate the probability that G1 and G2 are describing the same entity. In particular, the matching model includes the following four layers: Input Representation Layer The goal of this layer is to learn embeddings for entities that occurred in topic entity graphs by using a GCN (henceforth GCN 1 ) (Xu et al., 2018a). Recently, GCN has been successfully applied in many NLP tasks, such as semantic parsing (Xu et al., 2018b), text representation (Zhang et al., 2018), relation extraction (Song et al., 2018) and text generation (Xu et al., 2018c). We use the following embedding generation of entity v as an example to explain the GCN algorithm: (1) We first employ a word-based LSTM to transform v’s entity name to its initial feature vector av ; (2) We categorize the neighbors of v into incoming neighbors N` (v) and outgoing neighbors Na (v) according to the edge direction. (3) We leverage an aggregator to aggregate the incoming representations of v’s incoming neighbors k {hk−1 u` , ∀u ∈ N` (v)} into a single vector, hN` (v) , where k is the iteration index."
P19-1417,P17-1171,0,0.483694,"the relevant text piece. Knowledge bases (KBs) are considered as an essential resource for answering factoid questions. However, accurately constructing KB with a welldesigned and complicated schema requires lots of human efforts, which inevitably limits the coverage of KBs (Min et al., 2013). As a matter of fact, KBs are often incomplete and insufficient to cover full evidence required by open-domain questions. On the other hand, the vast amount of unstructured text on the Internet can easily cover a wide range of evolving knowledge, which is commonly used for open-domain question answering (Chen et al., 2017; Wang et al., 2018). Therefore, to improve the coverage of KBs, it is straightforward to augment KB with text data. Recently, text-based QA models along (Seo et al., 2016; Xiong et al., 2017; Yu et al., 2018) have achieved remarkable performance when dealing with a single passage that is guaranteed to include the answer. However, they are still insufficient when multiple documents https://github.com/xwhan/Knowledge-Aware-Reader. Cameron Newton (born May 11, 1989) plays for the Carolina Panthers of the National Football League (NFL) … Answer: Carolina Panthers Introduction 1 Document Retrieval"
P19-1417,P18-1076,0,0.0234033,"ear gate function P as γ e = g(~e, (ei ,ri )∈Ne s˜(ri ,ei ) σ(We [~ ri ; e~i ]))3 , which controls how much information in the original entity representation should be retained.4 3.2 Knowledge-Aware Text Reader Knowledge-aware Passage Enhancement To encode the retrieved passages, we use a standard bi-LSTM, which takes several token-level features5 . With the entity linking annotations in passages, we fuse the entity knowledge with the token-level features in a similar fashion as the query reformulation process. However, instead of applying a standard gating mechanism (Yang and Mitchell, 2017; Mihaylov and Frank, 2018), we propose a new conditional gating function that explicitly conditions on the question q~0 . This simple modification allows the reader to dynamically select the inputs according to their relevance to the question. Considering a passage token wid with its token features f~wd i and its linked entity ewi 6 , we define the conditional gating function as: ~idw = γ d e~0 wi + (1 − γ d )f~wd , where i i γ d = sigmoid(Wgd [~ q · e~0 w ; ~ q · f~wd ]). i i e~0 wi denotes the entity embedding learned by our SGR EADER. With the learned KB embeddings, our model enhances text reading with KAR EADER. Br"
P19-1417,D16-1147,0,0.0398533,"3 47.7 49.2 21.3 34.3 33.5 46.7 66.7 66.5 38.6 62.4 58.0 KV-KB+T EXT GN-LF GN-EF SGR EADER + KAR EADER (Ours) 24.6 29.8 31.5 33.6 14.4 17.0 17.7 18.9 27.0 39.1 40.7 42.6 17.7 25.9 25.2 27.1 32.5 46.2 49.9 52.7 23.6 35.6 34.7 36.1 40.5 65.4 67.8 67.2 30.9 56.8 60.4 57.3 GN-LF+EF (ensemble) 33.3 19.3 42.5 26.7 52.3 37.4 68.7 62.3 Table 1: Comparisons with Key-Value Memory Networks and GRAFT-Nets under different KB settings. downsampled to different extents. For a fair comparison, the retrieved document set is the same as the previous work. Baselines and Evaluation Key-Value (KV) Memory Network (Miller et al., 2016) is a simple baseline that treats KB triples and documents as memory cells. Specifically, we consider its two variants, KV-KB and KV-KB+Text. The former is a KB-only model while the latter uses both KB and text. We also compare to the latest method GraftNet (GN) (Sun et al., 2018), which treats documents as a special genre of nodes in KBs and utilizes graph convolution (Kipf and Welling, 2016) to aggregate the information. Similar to the KV-based baselines, we denote GN-KB as the KB-only version. Further, both GN-LF (late fusion) and GN-EF (early fusion) consider both KB and text. The former o"
P19-1417,N13-1095,0,0.0354997,"tball player Textual evidence: Cam Newton plays for Panthers Figure 1: A real example from WebQSP. Here the answer cannot be directly found in the KB. But the knowledge provided by the KB, i.e., Cam Newton is a football player, indicates he signed with the team he plays for. This knowledge can be essential for recognizing the relevant text piece. Knowledge bases (KBs) are considered as an essential resource for answering factoid questions. However, accurately constructing KB with a welldesigned and complicated schema requires lots of human efforts, which inevitably limits the coverage of KBs (Min et al., 2013). As a matter of fact, KBs are often incomplete and insufficient to cover full evidence required by open-domain questions. On the other hand, the vast amount of unstructured text on the Internet can easily cover a wide range of evolving knowledge, which is commonly used for open-domain question answering (Chen et al., 2017; Wang et al., 2018). Therefore, to improve the coverage of KBs, it is straightforward to augment KB with text data. Recently, text-based QA models along (Seo et al., 2016; Xiong et al., 2017; Yu et al., 2018) have achieved remarkable performance when dealing with a single pa"
P19-1417,P18-1150,0,0.0594365,"Missing"
P19-1417,D18-1455,0,0.206151,"Missing"
P19-1417,P17-1132,0,0.0248211,"meter calculated by a linear gate function P as γ e = g(~e, (ei ,ri )∈Ne s˜(ri ,ei ) σ(We [~ ri ; e~i ]))3 , which controls how much information in the original entity representation should be retained.4 3.2 Knowledge-Aware Text Reader Knowledge-aware Passage Enhancement To encode the retrieved passages, we use a standard bi-LSTM, which takes several token-level features5 . With the entity linking annotations in passages, we fuse the entity knowledge with the token-level features in a similar fashion as the query reformulation process. However, instead of applying a standard gating mechanism (Yang and Mitchell, 2017; Mihaylov and Frank, 2018), we propose a new conditional gating function that explicitly conditions on the question q~0 . This simple modification allows the reader to dynamically select the inputs according to their relevance to the question. Considering a passage token wid with its token features f~wd i and its linked entity ewi 6 , we define the conditional gating function as: ~idw = γ d e~0 wi + (1 − γ d )f~wd , where i i γ d = sigmoid(Wgd [~ q · e~0 w ; ~ q · f~wd ]). i i e~0 wi denotes the entity embedding learned by our SGR EADER. With the learned KB embeddings, our model enhances text"
P19-1417,P14-2105,0,0.0349953,"ts of our model consist of a graph-attention based KB reader (§3.1) and a knowledge-aware text reader (§3.2). The interaction between the modules is shown in Figure 2. 3.1 SubGraph Reader This section describes the KB subgraph reader (SGR EADER), which employs graph-attention techniques to accumulate knowledge of each subgraph entity (e) from its linked neighbors (Ne ). The graph attention mechanism is particularly designed to take into account two important aspects: (1) whether the neighbor relation is relevant to the question; (2) whether the neighbor entity is a topic 2 Annotated by STAGG (Yih et al., 2014). entity mentioned by the question. After the propagation, the SGR EADER finally outputs a vectorized representation for each entity, encoding the knowledge indicated by its linked neighbors. Question-Relation Matching To match the question and KB relation in an isomorphic latent space, we apply a shared LSTM to encode the question {w1q , w2q , ..., wlqq } and the tokenized relation {w1r , w2r , ..., wlrr }. With the derived hidden states hq ∈ Rlq ×dh and hr ∈ Rlr ×dh for each word, we first compute the representation of relations with a self-attentive encoder: ~r = X αi~hri , αi ∝ exp(w~r · ~"
P19-1417,P16-2033,0,0.206707,"Missing"
P19-1417,P17-1053,1,\N,Missing
P19-1435,D15-1075,0,\N,Missing
P19-1435,C04-1051,0,\N,Missing
P19-1435,P02-1040,0,\N,Missing
P19-1435,D14-1162,0,\N,Missing
P19-1435,marelli-etal-2014-sick,0,\N,Missing
P19-1435,P18-1041,0,\N,Missing
P19-1435,D18-1453,0,\N,Missing
P19-1435,N18-1101,0,\N,Missing
P19-1496,W11-2107,0,0.0849864,"Missing"
P19-1496,P11-2008,0,0.0618779,"Missing"
P19-1496,P17-1044,0,0.0754159,"Missing"
P19-1496,D14-1108,0,0.0518288,"Penn Treebank (Marcus et al., 1993). In recent 3 https://developer.twitter.com/ years, with the increasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed. For example, Gimpel et al. (2011) build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets. Ritter et al. (2011) annotated 800 tweets, and performed an empirical study for partof-speech tagging and chunking on a new Twitter dataset. They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets. Kong et al. (2014) annotated 929 tweets, and built the first dependency parser for tweets, whereas Wang et al. (2014) built the Chinese counterpart based on 1,000 annotated Weibo posts. To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the f"
P19-1496,D17-1082,0,0.0442583,"ension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets. The early development of the RC datasets focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015) or quiz-style problems (Richardson et al., 2013; Lai et al., 2017). The former one aims to generate single-token answers from automatically constructed pseudo-questions while the latter requires choosing from multiple answer candidates. However, such unnatural settings make them fail to serve as the standard QA benchmarks. Instead, researchers started to ask human annotators to create questions and answers given passages in a crowdsourced way. Such efforts give the rise of large-scale human-annotated RC datasets, many of which are quite popular in the community such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), NewsQA (Trischler et al.,"
P19-1496,J93-2004,0,0.0681079,"Missing"
P19-1496,D16-1147,0,0.0464777,"s for core natural language understanding tasks involving syntactic and semantic analysis have been developed for noisy social media text (Gimpel et al., 2011; Ritter et al., 2011; Kong et al., 2014; Wang et al., 2014), there is little work on question answering or reading comprehension over social media, with the primary bottleneck being the lack of available datasets. We observe that recently proposed QA datasets usually focus on formal domains, e.g. CNN/DAILY M AIL (Hermann et al., 2015) and NewsQA (Trischler et al., 2016) on news articles; SQuAD (Rajpurkar et al., 2016) and W IKI M OVIES (Miller et al., 2016) that use Wikipedia. In this paper, we propose the first large-scale dataset for QA over social media data. Rather than naively obtaining tweets from Twitter using 5020 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5020–5031 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the Twitter API3 which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and re"
P19-1496,D18-1009,0,0.0434984,"On pretraining model Although BERT was demonstrated to be a powerful tool for reading comprehension, this is the first time a detailed analysis has been done on its reasoning skills. From the results, the huge improvement of BERT mainly comes from two types. The first is paraphrasing, which is not surprising because that a well pretrained language model is expected to be able to better encode sentences. Thus the derived embedding space could work better for sentence comparison. The second type is commonsense, which is consistent with the good performance of BERT (Devlin et al., 2018) on SWAG (Zellers et al., 2018). We believe that this provides further evidence about the connection between largescaled deep neural language model and certain kinds of commonsense. 6 Conclusion We present the first dataset for QA on social media data by leveraging news media and crowdsourcing. The proposed dataset informs us of the distinctiveness of social media from formal domains in the context of QA. Specifically, we find that QA on social media requires systems to comprehend social media specific linguistic patterns like informality, hashtags, usernames, and authorship. These distinguishing linguistic factors bring up"
P19-1496,P02-1040,0,0.103171,"Missing"
P19-1496,N18-1202,0,0.0112655,"is further modeled by an RNN layer to make the span predictions. Since our T WEET QA does not have labeled answer spans as in SQuAD, we need to use the human-written answers to retrieve the answerspan labels for training. To get the approximate answer spans, we consider the same matching approach as in the query matching baseline. But instead of using questions to do matching, we use the human-written answers to get the spans that achieve the best BLEU-1 scores. Fine-Tuning BERT This is another extractive RC model that benefits from the recent advance in pretrained general language encoders (Peters et al., 2018; Devlin et al., 2018). In our work, we select the BERT model (Devlin et al., 2018) which has achieved the best performance on SQuAD. In our experiments, we use the PyTorch reimple5026 5.2 Evaluation on Dev/Test Data Models BLEU-1 METEOR ROUGE-L H UMAN E XTRACT-UB 76.4|78.2 79.5|80.3 63.7|66.7 68.8|69.8 70.9|73.5 74.3|75.6 Query-Matching 30.3|29.4 12.0|12.1 17.0|17.4 BiDAF Generative BERT Neural Baselines 48.3|48.7 31.6|31.4 53.4|53.7 32.1|31.8 67.3|69.6 56.9|58.6 38.9|38.6 39.5|39.0 62.6|64.1 Reasoning Types Paraphrasing Sentence relations Authorship Oral/Tweet habits UserIDs&Hashtags Commons"
P19-1496,D16-1264,0,0.454066,"s-2017/ In recent years, while several tools for core natural language understanding tasks involving syntactic and semantic analysis have been developed for noisy social media text (Gimpel et al., 2011; Ritter et al., 2011; Kong et al., 2014; Wang et al., 2014), there is little work on question answering or reading comprehension over social media, with the primary bottleneck being the lack of available datasets. We observe that recently proposed QA datasets usually focus on formal domains, e.g. CNN/DAILY M AIL (Hermann et al., 2015) and NewsQA (Trischler et al., 2016) on news articles; SQuAD (Rajpurkar et al., 2016) and W IKI M OVIES (Miller et al., 2016) that use Wikipedia. In this paper, we propose the first large-scale dataset for QA over social media data. Rather than naively obtaining tweets from Twitter using 5020 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5020–5031 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the Twitter API3 which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implyin"
P19-1496,D13-1020,0,0.157474,"ering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets. The early development of the RC datasets focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015) or quiz-style problems (Richardson et al., 2013; Lai et al., 2017). The former one aims to generate single-token answers from automatically constructed pseudo-questions while the latter requires choosing from multiple answer candidates. However, such unnatural settings make them fail to serve as the standard QA benchmarks. Instead, researchers started to ask human annotators to create questions and answers given passages in a crowdsourced way. Such efforts give the rise of large-scale human-annotated RC datasets, many of which are quite popular in the community such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), NewsQA"
P19-1496,D11-1141,0,0.115207,"Missing"
P19-1496,D14-1122,1,0.853261,"reasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed. For example, Gimpel et al. (2011) build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets. Ritter et al. (2011) annotated 800 tweets, and performed an empirical study for partof-speech tagging and chunking on a new Twitter dataset. They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets. Kong et al. (2014) annotated 929 tweets, and built the first dependency parser for tweets, whereas Wang et al. (2014) built the Chinese counterpart based on 1,000 annotated Weibo posts. To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets. The early development of the"
Q15-1017,D10-1115,0,0.366509,"word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use supervised training with small"
Q15-1017,P14-1063,0,0.0883156,"q. (1) to predict a label y, the score of y given phrase p will be s(y, p) = PN T UyT ep = i Uy (λi ewi ) in log-linear models, where Uy is the parameter vector for y. This is equivalent to using a parameter tensor T to evaluate P T ×1 y ×2 f (wi , p) × the score with s0 (y, p) = N i ewi , while forcing the tensor to have a low-rank form as T ≈ U ⊗ α ⊗ ew . Here ×k indicates tensor multiplication of the kth view, and ⊗ indicates matrix outer product (Kolda and Bader, 2009). From this point of view, our work is closely related to the discriminative training methods for low-rank tensors in NLP (Cao and Khudanpur, 2014; Lei et al., 2014), while it can handle more complex ngram-to-ngram tasks, where the label y also has its embedding composed from basic word embeddings. Therefore our model can capture the above work as special cases. Moreover, we have a different method of decomposing the inputs, which results in views of lexical parts and non-lexical features. As we show in this paper, this input decomposition allows us to benefit from pre-trained word embeddings and feature weights. 8 Conclusion We have presented FCT, a new composition model for deriving phrase embeddings from word embed240 dings. Compared"
Q15-1017,P13-1039,0,0.0129296,"plying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and one of its context words; others have considered extensions to multiple words (Grefenstette et al., 2013; Dinu and Baroni, 2014). The primary drawback of these approaches is the high computational complexity, limiting their usefulness for semantics (Section 6.2.) A second approach draws on the concept of contextualization (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011), which sums embeddings of multiple words in a linear combination. For example, Cheung and Penn (2013) apply contextualization to word compositions in a generative event extraction model. However, this is an indirect way to capture interactions (the transformations are still unaware of interactions between components), and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with othe"
Q15-1017,P14-1059,0,0.0145947,"associated with one word and the embedding of the other, producing a new embedding for the phrase. Using one tensor (not word-specific) to compose two embedding vectors (has not been tested on phrase similarity tasks) (Bordes et al., 2014; Socher et al., 2013b) is a special case of this approach, where a “wordspecific transformation matrix” is derived by multiplying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and one of its context words; others have considered extensions to multiple words (Grefenstette et al., 2013; Dinu and Baroni, 2014). The primary drawback of these approaches is the high computational complexity, limiting their usefulness for semantics (Section 6.2.) A second approach draws on the concept of contextualization (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011), which sums embeddings of multiple words in a linear combination. For example, Cheung and Penn (2013) apply contextualization to word compositions in a generative event extraction model. However, this is an indirect way to capture interactions (the transformations are still unaware of interactions between components), and thus has not b"
Q15-1017,D10-1113,0,0.0240476,"Socher et al., 2013b) is a special case of this approach, where a “wordspecific transformation matrix” is derived by multiplying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and one of its context words; others have considered extensions to multiple words (Grefenstette et al., 2013; Dinu and Baroni, 2014). The primary drawback of these approaches is the high computational complexity, limiting their usefulness for semantics (Section 6.2.) A second approach draws on the concept of contextualization (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011), which sums embeddings of multiple words in a linear combination. For example, Cheung and Penn (2013) apply contextualization to word compositions in a generative event extraction model. However, this is an indirect way to capture interactions (the transformations are still unaware of interactions between components), and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer ling"
Q15-1017,D08-1094,0,0.0614568,"Missing"
Q15-1017,W13-0109,0,0.0131256,"se the input of the model is the concatenation of word representations, matrix transformations cannot capture interactions between a word and its contexts, or between component words. There are three ways to restore these interactions: The first is to use word-specific/tensor transformations to force the interactions between component words in a phrase. In these methods, wordspecific transformations, which are usually matrices, are learned for a subset of words according to their syntactic properties (e.g. POS tags) (Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013; Erk, 2013). Composition between a word in this subset and another word becomes the multiplication between the matrix associated with one word and the embedding of the other, producing a new embedding for the phrase. Using one tensor (not word-specific) to compose two embedding vectors (has not been tested on phrase similarity tasks) (Bordes et al., 2014; Socher et al., 2013b) is a special case of this approach, where a “wordspecific transformation matrix” is derived by multiplying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and"
Q15-1017,N13-1092,0,0.0338236,"Missing"
Q15-1017,W13-0112,0,0.0596645,"phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use supervised training with small task-specific datasets. An exception is the unsu"
Q15-1017,P13-1088,0,0.234528,"state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use s"
Q15-1017,P14-1136,0,0.0253992,"rning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on p"
Q15-1017,P12-1092,0,0.0873224,"ficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication,"
Q15-1017,S13-2007,0,0.0186772,"igram phrase, i.e., the input phrase or the swapped input (“word monosyllabic” for this example), and the five output candidates. The correct answer in this case should still be the pair of original input phrase and the original correct output candidate (in bold). (4) PPDB (ngram) is similar to PPDB, but in which both inputs and outputs becomes noun phrases with arbitrary lengths. and Dredze, 2014) to that between phrases. Data details appear in Table 2. Phrase Similarity Datasets We use a variety of human annotated datasets to evaluate phrase semantic similarity: the SemEval2013 shared task (Korkontzelos et al., 2013), and the noun-modifier problem (Turney2012) in Turney (2012). Both tasks provide evaluation data and training data. SemEval2013 Task 5(a) is a classification task to determine if a word phrase pair are semantically similar. Turney2012 is a task to select the closest matching candidate word for a given phrase from candidate words. The original task contained seven candidates, two of which are component words of the input phrase (seven-choice task). Followup work has since removed the components words from the candidates (five-choice task). Turney (2012) also propose a 10-choice task based on t"
Q15-1017,P14-1130,0,0.0281039,"y, the score of y given phrase p will be s(y, p) = PN T UyT ep = i Uy (λi ewi ) in log-linear models, where Uy is the parameter vector for y. This is equivalent to using a parameter tensor T to evaluate P T ×1 y ×2 f (wi , p) × the score with s0 (y, p) = N i ewi , while forcing the tensor to have a low-rank form as T ≈ U ⊗ α ⊗ ew . Here ×k indicates tensor multiplication of the kth view, and ⊗ indicates matrix outer product (Kolda and Bader, 2009). From this point of view, our work is closely related to the discriminative training methods for low-rank tensors in NLP (Cao and Khudanpur, 2014; Lei et al., 2014), while it can handle more complex ngram-to-ngram tasks, where the label y also has its embedding composed from basic word embeddings. Therefore our model can capture the above work as special cases. Moreover, we have a different method of decomposing the inputs, which results in views of lexical parts and non-lexical features. As we show in this paper, this input decomposition allows us to benefit from pre-trained word embeddings and feature weights. 8 Conclusion We have presented FCT, a new composition model for deriving phrase embeddings from word embed240 dings. Compared to existing phrase"
Q15-1017,P08-1028,0,0.0735639,"rian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts"
Q15-1017,W12-3018,0,0.0243723,"Missing"
Q15-1017,P14-2012,0,0.0363822,"actions between components), and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with other information (i.e. features) of their context words, and even the global features. Recent research has created novel features based on combining word embeddings and contextual information (Nguyen and Grishman, 2014; Roth and Woodsend, 2014; Kiros et al., 2014; Yu et al., 2014; Yu et al., 2015). Yu et al. (2015) further proposed converting the contextual features into a hidden layer called feature embeddings, which is similar to the α matrix in this paper. Examples of applications to phrase semantics include Socher et al. (2013a) and Hermann and Blunsom (2013), who enhanced RNNs by refining the transformation matrices with phrase types and CCG super tags. However, these models are only able to use limited information (usually one property for each compositional transformation), whereas FCT exploits multi"
Q15-1017,D14-1045,0,0.0245382,", and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with other information (i.e. features) of their context words, and even the global features. Recent research has created novel features based on combining word embeddings and contextual information (Nguyen and Grishman, 2014; Roth and Woodsend, 2014; Kiros et al., 2014; Yu et al., 2014; Yu et al., 2015). Yu et al. (2015) further proposed converting the contextual features into a hidden layer called feature embeddings, which is similar to the α matrix in this paper. Examples of applications to phrase semantics include Socher et al. (2013a) and Hermann and Blunsom (2013), who enhanced RNNs by refining the transformation matrices with phrase types and CCG super tags. However, these models are only able to use limited information (usually one property for each compositional transformation), whereas FCT exploits multiple features. Finally, ou"
Q15-1017,D11-1014,0,0.846936,"(2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tas"
Q15-1017,D12-1110,0,0.496571,"R) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use supervised training with small task-specific datase"
Q15-1017,P13-1045,0,0.183107,"and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition"
Q15-1017,D13-1170,0,0.20677,"and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition"
Q15-1017,I11-1127,0,0.0534138,"Missing"
Q15-1017,P10-1040,0,0.0701477,"t capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapa"
Q15-1017,P14-2089,1,0.58832,"gram noun phrases (obtained from the annotated data) as the input phrases for Eq. (3). A subset from January 1998 of NYT data is withheld for evaluation. We use “input embeddings” learned by word2vec. https://opennlp.apache.org/ 232 Results We evaluate the perplexity of language models that include lexical embeddings and our composed phrase embeddings from FCT using the LM objective. We use the perplexity computation method of Mikolov et al. (2013a) suitable for skip-gram models. The FCT models are trained by the HS strategy, which can output the exact probability efficiently and was shown by Yu and Dredze (2014) to obtain better performance on language modeling. Since in Section 6.1 we use FCT models trained by NCE, we also include the results of models trained by NCE. Note that scores obtained from a model trained with HS or NCE are not comparable. While the model trained by HS is efficient to evaluate perplexities, NCE training requires summation over all words in the vocabulary in the denominator of the softmax to compute perplexity, an impracticality for large vocabulary. Therefore, we report NCE loss with a fixed set of samples for NCE trained models. Model SUM (2 epochs) word2vec (2 epochs) FCT"
Q15-1017,N15-1155,1,0.853515,"third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with other information (i.e. features) of their context words, and even the global features. Recent research has created novel features based on combining word embeddings and contextual information (Nguyen and Grishman, 2014; Roth and Woodsend, 2014; Kiros et al., 2014; Yu et al., 2014; Yu et al., 2015). Yu et al. (2015) further proposed converting the contextual features into a hidden layer called feature embeddings, which is similar to the α matrix in this paper. Examples of applications to phrase semantics include Socher et al. (2013a) and Hermann and Blunsom (2013), who enhanced RNNs by refining the transformation matrices with phrase types and CCG super tags. However, these models are only able to use limited information (usually one property for each compositional transformation), whereas FCT exploits multiple features. Finally, our work is related to recent work on low-rank tensor app"
Q15-1017,C10-1142,0,0.0371531,"word representations. A traditional approach for composition is to form a point-wise combination of single word representations with compositional operators either pre-defined (e.g. elementwise sum/multiplication) or learned from data (Le and Mikolov, 2014). However, these approaches ignore the inner structure of phrases, e.g. the order of words in a phrase and its syntactic tree, and the point-wise operations are usually less expressive. One solution is to apply a matrix transformation (possibly followed by a non-linear transformation) to the concatenation of component word representations (Zanzotto et al., 2010). For longer phrases, 239 matrix multiplication can be applied recursively according to the associated syntactic trees (Socher et al., 2010). However, because the input of the model is the concatenation of word representations, matrix transformations cannot capture interactions between a word and its contexts, or between component words. There are three ways to restore these interactions: The first is to use word-specific/tensor transformations to force the interactions between component words in a phrase. In these methods, wordspecific transformations, which are usually matrices, are learned"
R19-1114,W15-4640,0,0.0682423,"Missing"
R19-1114,D07-1074,0,0.0286743,"parameters to learn. There is also the problem of not being able to learn good neural embeddings for individual NEs, as individual NEs (e.g., a particular phone number) generally occur only a few times in a dataset. Another previously proposed method is to encode all the NEs with random embeddings and keep them fixed throughout (Yin et al., 2015), but here we lose the meaning associated with the neural embeddings and risk interference and correlation with others in unexpected ways. A third method is to first recognize the NE-type with either NE taggers (Finkel et al., 2005) or entity linkers (Cucerzan, 2007; Guo et al., 2013), and then replace them with NE-type tags. For example, all location names could be replaced with the tag NE location. This prevents the explosion in vocabulary size; however, the system loses the ability to distinguish and reference different NEs of the same type. There is also the possibility of new NEs arising during the test time. In fact, many of the Out-Of-Vocabulary (OOV) words that arise during test time in many NLP tasks (e.g. Bordes and Weston (2016)) are NEs. Furthermore, in many scenarios it is easier and accurate to work with the actual exact values of NEs rathe"
R19-1114,P14-5010,0,0.0069829,"Missing"
R19-1114,P17-1045,0,0.0779459,"Missing"
R19-1114,P05-1045,0,0.0168703,"vocabulary size and hence the number of parameters to learn. There is also the problem of not being able to learn good neural embeddings for individual NEs, as individual NEs (e.g., a particular phone number) generally occur only a few times in a dataset. Another previously proposed method is to encode all the NEs with random embeddings and keep them fixed throughout (Yin et al., 2015), but here we lose the meaning associated with the neural embeddings and risk interference and correlation with others in unexpected ways. A third method is to first recognize the NE-type with either NE taggers (Finkel et al., 2005) or entity linkers (Cucerzan, 2007; Guo et al., 2013), and then replace them with NE-type tags. For example, all location names could be replaced with the tag NE location. This prevents the explosion in vocabulary size; however, the system loses the ability to distinguish and reference different NEs of the same type. There is also the possibility of new NEs arising during the test time. In fact, many of the Out-Of-Vocabulary (OOV) words that arise during test time in many NLP tasks (e.g. Bordes and Weston (2016)) are NEs. Furthermore, in many scenarios it is easier and accurate to work with th"
R19-1114,N18-1202,0,0.024348,"Missing"
R19-1114,P16-1154,0,0.0550956,"Missing"
R19-1114,P16-1014,0,0.0584909,"Missing"
R19-1114,P15-1152,0,0.0910481,"Missing"
R19-1114,N13-1122,0,0.0762531,"arn. There is also the problem of not being able to learn good neural embeddings for individual NEs, as individual NEs (e.g., a particular phone number) generally occur only a few times in a dataset. Another previously proposed method is to encode all the NEs with random embeddings and keep them fixed throughout (Yin et al., 2015), but here we lose the meaning associated with the neural embeddings and risk interference and correlation with others in unexpected ways. A third method is to first recognize the NE-type with either NE taggers (Finkel et al., 2005) or entity linkers (Cucerzan, 2007; Guo et al., 2013), and then replace them with NE-type tags. For example, all location names could be replaced with the tag NE location. This prevents the explosion in vocabulary size; however, the system loses the ability to distinguish and reference different NEs of the same type. There is also the possibility of new NEs arising during the test time. In fact, many of the Out-Of-Vocabulary (OOV) words that arise during test time in many NLP tasks (e.g. Bordes and Weston (2016)) are NEs. Furthermore, in many scenarios it is easier and accurate to work with the actual exact values of NEs rather than neural embed"
W19-4805,D16-1011,0,0.0782085,"Missing"
W19-4805,N18-1100,0,0.0839713,"their attention weights. However, previous work has several limitations. Lin et al. (2017), for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1, such as “bleeding after nasogastric tube insertion”, are larger than a single word. Another issue of Lin et al. (2017) is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting. Thus the explainability of the model is limited. Lei et al. (2016) introduce a regularizer over the selected (single-word) positions to encourage the model to extract larger phrases. However, their method can not tell how much a selected unit contributes to the model’s decision through a weight value. In this paper, we study what the meaningful units to highlight are. We define multi-granular ngrams as basic units, so that all"
W19-4805,D14-1162,0,0.0826462,"coder, all ngrams with the same order can be computed in parallel, and the model needs at most 7 iterative steps along the depth dimension for representing a given text of arbitrary length. 4 ACC #Param. 2.6 4.6 1.4 64.8 64.5 66.2 848,228 147,928 168,228 tokens, and one label out of five categories indicating which disease this document is about. We randomly split the dataset into train/dev/test sets by 8:1:1 for each category, and end up with 11,216/1,442/1,444 instances for each set. Hyperparameters We use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington et al., 2014), and set the hidden size as 100 for node embeddings. We apply dropout to every layer with a dropout ratio 0.2, and set the batch size as 50. We minimize the cross-entropy of the training set with the ADAM optimizer (Kingma and Ba, 2014), and set the learning rate is to 0.001. During training, the pre-trained word embeddings are not updated. (5) (6) Eval Time 57.0 92.1 30.3 Table 2: Efficiency evaluation. (4) c=i u+f h +f h Train Time 4.1 Properties of the multi-granular encoder Influence of the n-gram order: For CNN and our LeftForest encoder, we vary the order of ngrams from 1 to 9, and plot"
W19-4805,P15-1150,0,0.13561,"Missing"
