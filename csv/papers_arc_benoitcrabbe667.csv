2021.jeptalnrecital-taln.9,Analyse en d{\\'e}pendances du fran{\\c{c}}ais avec des plongements contextualis{\\'e}s ({F}rench dependency parsing with contextualized embeddings),2021,-1,-1,2,0,5600,loic grobol,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,Cet article pr{\'e}sente un analyseur syntaxique en d{\'e}pendances pour le fran{\c{c}}ais qui se compare favorablement {\`a} l{'}{\'e}tat de l{'}art sur la plupart des corpus de r{\'e}f{\'e}rence. L{'}analyseur s{'}appuie sur de riches repr{\'e}sentations lexicales issues notamment de BERT et de FASTTEXT. On remarque que les repr{\'e}sentations lexicales produites par FLAUBERT ont un caract{\`e}re auto-suffisant pour r{\'e}aliser la t{\^a}che d{'}analyse syntaxique de mani{\`e}re optimale.
2021.jeptalnrecital-taln.24,Un mod{\\`e}le Transformer G{\\'e}n{\\'e}ratif Pr{\\'e}-entrain{\\'e} pour le{\\_}{\\_}{\\_}{\\_}{\\_}{\\_} fran{\\c{c}}ais (Generative Pre-trained Transformer in{\\_}{\\_}{\\_}{\\_}{\\_}{\\_} ({F}rench) We introduce a {F}rench adaptation from the well-known {GPT} model),2021,-1,-1,2,0,5637,antoine simoulin,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"Nous proposons une adaptation en fran{\c{c}}ais du fameux mod{\`e}le Generative Pre-trained Transformer (GPT). Ce dernier appartient {\`a} la cat{\'e}gorie des architectures transformers qui ont significativement transform{\'e} les m{\'e}thodes de traitement automatique du langage. Ces architectures sont en particulier pr{\'e}-entra{\^\i}n{\'e}es sur des t{\^a}ches auto-supervis{\'e}es et sont ainsi sp{\'e}cifiques pour une langue donn{\'e}e. Si certaines sont disponibles en fran{\c{c}}ais, la plupart se d{\'e}clinent avant tout en anglais. GPT est particuli{\`e}rement efficace pour les t{\^a}ches de g{\'e}n{\'e}ration de texte. Par ailleurs, il est possible de l{'}appliquer {\`a} de nombreux cas d{'}usages. Ses propri{\'e}t{\'e}s g{\'e}n{\'e}ratives singuli{\`e}res permettent de l{'}utiliser dans des conditions originales comme l{'}apprentissage sans exemple qui ne suppose aucune mise {\`a} jour des poids du mod{\`e}le, ou modification de l{'}architecture."
2021.emnlp-main.377,Are {T}ransformers a Modern Version of {ELIZA}? {O}bservations on {F}rench Object Verb Agreement,2021,-1,-1,3,0,9479,bingzhi li,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Many recent works have demonstrated that unsupervised sentence representations of neural networks encode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in our assessment of neural networks{'} syntactic ability. Our fine-grained analyses of results on the long-range French object-verb agreement show that contrary to LSTMs, Transformers are able to capture a non-trivial amount of grammatical structure."
2021.eacl-srw.11,Contrasting distinct structured views to learn sentence embeddings,2021,-1,-1,2,0,5637,antoine simoulin,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"We propose a self-supervised method that builds sentence embeddings from the combination of diverse explicit syntactic structures of a sentence. We assume structure is crucial to building consistent representations as we expect sentence meaning to be a function of both syntax and semantic aspects. In this perspective, we hypothesize that some linguistic representations might be better adapted given the considered task or sentence. We, therefore, propose to learn individual representation functions for different syntactic frameworks jointly. Again, by hypothesis, all such functions should encode similar semantic information differently and consequently, be complementary for building better sentential semantic embeddings. To assess such hypothesis, we propose an original contrastive multi-view framework that induces an explicit interaction between models during the training phase. We make experiments combining various structures such as dependency, constituency, or sequential schemes. Our results outperform comparable methods on several tasks from standard sentence embedding benchmarks."
2021.acl-srw.23,How Many Layers and Why? {A}n Analysis of the Model Depth in Transformers,2021,-1,-1,2,0,5637,antoine simoulin,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"In this study, we investigate the role of the multiple layers in deep transformer models. We design a variant of Albert that dynamically adapts the number of layers for each token of the input. The key specificity of Albert is that weights are tied across layers. Therefore, the stack of encoder layers iteratively repeats the application of the same transformation function on the input. We interpret the repetition of this application as an iterative process where the token contextualized representations are progressively refined. We analyze this process at the token level during pre-training, fine-tuning, and inference. We show that tokens do not require the same amount of iterations and that difficult or crucial tokens for the task are subject to more iterations."
2020.lrec-1.302,{F}lau{BERT}: Unsupervised Language Model Pre-training for {F}rench,2020,-1,-1,8,0,5778,hang le,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP."
2020.lrec-1.724,{F}r{S}em{C}or: Annotating a {F}rench Corpus with Supersenses,2020,0,0,6,0,18073,lucie barque,Proceedings of the 12th Language Resources and Evaluation Conference,0,"French, as many languages, lacks semantically annotated corpus data. Our aim is to provide the linguistic and NLP research communities with a gold standard sense-annotated corpus of French, using WordNet Unique Beginners as semantic tags, thus allowing for interoperability. In this paper, we report on the first phase of the project, which focused on the annotation of common nouns. The resulting dataset consists of more than 12,000 French noun occurrences which were annotated in double blind and adjudicated according to a carefully redefined set of supersenses. The resource is released online under a Creative Commons Licence."
2020.jeptalnrecital-taln.26,{F}lau{BERT} : des mod{\\`e}les de langue contextualis{\\'e}s pr{\\'e}-entra{\\^\\i}n{\\'e}s pour le fran{\\c{c}}ais ({F}lau{BERT} : Unsupervised Language Model Pre-training for {F}rench),2020,-1,-1,8,0,5778,hang le,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Les mod{\`e}les de langue pr{\'e}-entra{\^\i}n{\'e}s sont d{\'e}sormais indispensables pour obtenir des r{\'e}sultats {\`a} l{'}{\'e}tat-de-l{'}art dans de nombreuses t{\^a}ches du TALN. Tirant avantage de l{'}{\'e}norme quantit{\'e} de textes bruts disponibles, ils permettent d{'}extraire des repr{\'e}sentations continues des mots, contextualis{\'e}es au niveau de la phrase. L{'}efficacit{\'e} de ces repr{\'e}sentations pour r{\'e}soudre plusieurs t{\^a}ches de TALN a {\'e}t{\'e} d{\'e}montr{\'e}e r{\'e}cemment pour l{'}anglais. Dans cet article, nous pr{\'e}sentons et partageons FlauBERT, un ensemble de mod{\`e}les appris sur un corpus fran{\c{c}}ais h{\'e}t{\'e}rog{\`e}ne et de taille importante. Des mod{\`e}les de complexit{\'e} diff{\'e}rente sont entra{\^\i}n{\'e}s {\`a} l{'}aide du nouveau supercalculateur Jean Zay du CNRS. Nous {\'e}valuons nos mod{\`e}les de langue sur diverses t{\^a}ches en fran{\c{c}}ais (classification de textes, paraphrase, inf{\'e}rence en langage naturel, analyse syntaxique, d{\'e}sambigu{\""\i}sation automatique) et montrons qu{'}ils surpassent souvent les autres approches sur le r{\'e}f{\'e}rentiel d{'}{\'e}valuation FLUE {\'e}galement pr{\'e}sent{\'e} ici."
W19-0422,Using {W}iktionary as a resource for {WSD} : the case of {F}rench verbs,2019,0,1,3,0,17270,vincent segonne,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"As opposed to word sense induction, word sense disambiguation (WSD) has the advantage of us-ing interpretable senses, but requires annotated data, which are quite rare for most languages except English (Miller et al. 1993; Fellbaum, 1998). In this paper, we investigate which strategy to adopt to achieve WSD for languages lacking data that was annotated specifically for the task, focusing on the particular case of verb disambiguation in French. We first study the usability of Eurosense (Bovi et al. 2017) , a multilingual corpus extracted from Europarl (Kohen, 2005) and automatically annotated with BabelNet (Navigli and Ponzetto, 2010) senses. Such a resource opened up the way to supervised and semi-supervised WSD for resourceless languages like French. While this perspective looked promising, our evaluation on French verbs was inconclusive and showed the annotated senses{'} quality was not sufficient for supervised WSD on French verbs. Instead, we propose to use Wiktionary, a collaboratively edited, multilingual online dictionary, as a resource for WSD. Wiktionary provides both sense inventory and manually sense tagged examples which can be used to train supervised and semi-supervised WSD systems. Yet, because senses{'} distribution differ in lexicographic examples found in Wiktionary with respect to natural text, we then focus on studying the impact on WSD of the training data size and senses{'} distribution. Using state-of-the art semi-supervised systems, we report experiments of Wiktionary-based WSD for French verbs, evaluated on FrenchSemEval (FSE), a new dataset of French verbs manually annotated with wiktionary senses."
Q19-1005,Unlexicalized Transition-based Discontinuous Constituency Parsing,2019,21,2,2,1,5608,maximin coavoux,Transactions of the Association for Computational Linguistics,0,"Lexicalized parsing models are based on the assumptions that (i) constituents are organized around a lexical head and (ii) bilexical statistics are crucial to solve ambiguities. In this paper, we introduce an unlexicalized transition-based parser for discontinuous constituency structures, based on a structure-label transition system and a bi-LSTM scoring system. We compare it with lexicalized parsing models in order to address the question of lexicalization in the context of discontinuous constituency parsing. Our experiments show that unlexicalized models systematically achieve higher results than lexicalized models, and provide additional empirical evidence that lexicalization is not necessary to achieve strong parsing results. Our best unlexicalized model sets a new state of the art on English and German discontinuous constituency treebanks. We further provide a per-phenomenon analysis of its errors on discontinuous constituents."
D19-1106,Variable beam search for generative neural parsing and its relevance for the analysis of neuro-imaging signal,2019,0,0,1,1,5601,benoit crabbe,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,This paper describes a method of variable beam size inference for Recurrent Neural Network Grammar (rnng) by drawing inspiration from sequential Monte-Carlo methods such as particle filtering. The paper studies the relevance of such methods for speeding up the computations of direct generative parsing for rnng. But it also studies the potential cognitive interpretation of the underlying representations built by the search method (beam activity) through analysis of neuro-imaging signal.
E17-2053,Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks,2017,6,2,2,1,5608,maximin coavoux,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We introduce a constituency parser based on a bi-LSTM encoder adapted from recent work (Cross and Huang, 2016b; Kiperwasser and Goldberg, 2016), which can incorporate a lower level character biLSTM (Ballesteros et al., 2015; Plank et al., 2016). We model two important interfaces of constituency parsing with auxiliary tasks supervised at the word level: (i) part-of-speech (POS) and morphological tagging, (ii) functional label prediction. On the SPMRL dataset, our parser obtains above state-of-the-art results on constituency parsing without requiring either predicted POS or morphological tags, and outputs labelled dependency trees."
E17-1118,Incremental Discontinuous Phrase Structure Parsing with the {GAP} Transition,2017,27,6,2,1,5608,maximin coavoux,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"This article introduces a novel transition system for discontinuous lexicalized constituent parsing called SR-GAP. It is an extension of the shift-reduce algorithm with an additional gap transition. Evaluation on two German treebanks shows that SR-GAP outperforms the previous best transition-based discontinuous parser (Maier, 2015) by a large margin (it is notably twice as accurate on the prediction of discontinuous constituents), and is competitive with the state of the art (Fern{\'a}ndez-Gonz{\'a}lez and Martins, 2015). As a side contribution, we adapt span features (Hall et al., 2014) to discontinuous parsing."
2017.jeptalnrecital-long.6,Repr{\\'e}sentation et analyse automatique des discontinuit{\\'e}s syntaxiques dans les corpus arbor{\\'e}s en constituants du fran{\\c{c}}ais (Representation and parsing of syntactic discontinuities in {F}rench constituent treebanks),2017,-1,-1,2,1,5608,maximin coavoux,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Nous pr{\'e}sentons de nouvelles instanciations de trois corpus arbor{\'e}s en constituants du fran{\c{c}}ais, o{\`u} certains ph{\'e}nom{\`e}nes syntaxiques {\`a} l{'}origine de d{\'e}pendances {\`a} longue distance sont repr{\'e}sent{\'e}s directement {\`a} l{'}aide de constituants discontinus. Les arbres obtenus rel{\`e}vent de formalismes grammaticaux l{\'e}g{\`e}rement sensibles au contexte (LCFRS). Nous montrons ensuite qu{'}il est possible d{'}analyser automatiquement de telles structures de mani{\`e}re efficace {\`a} condition de s{'}appuyer sur une m{\'e}thode d{'}inf{\'e}rence approximative. Pour cela, nous pr{\'e}sentons un analyseur syntaxique par transitions, qui r{\'e}alise {\'e}galement l{'}analyse morphologique et l{'}{\'e}tiquetage fonctionnel des mots de la phrase. Enfin, nos exp{\'e}riences montrent que la raret{\'e} des ph{\'e}nom{\`e}nes concern{\'e}s dans les donn{\'e}es fran{\c{c}}aises pose des difficult{\'e}s pour l{'}apprentissage et l{'}{\'e}valuation des structures discontinues."
P16-1017,Neural Greedy Constituent Parsing with Dynamic Oracles,2016,18,13,2,1,5608,maximin coavoux,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic oracle for transition-based constituent parsing. Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers."
C16-1001,Boosting for Efficient Model Selection for Syntactic Parsing,2016,1,0,2,0,7687,rachel bawden,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We present an efficient model selection method using boosting for transition-based constituency parsing. It is designed for exploring a high-dimensional search space, defined by a large set of feature templates, as for example is typically the case when parsing morphologically rich languages. Our method removes the need to manually define heuristic constraints, which are often imposed in current state-of-the-art selection methods. Our experiments for French show that the method is more efficient and is also capable of producing compact, state-of-the-art models."
P15-2078,Dependency length minimisation effects in short spans: a large-scale analysis of adjective placement in complex noun phrases,2015,16,9,3,0,22877,kristina gulordava,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"It has been extensively observed that languages minimise the distance between two related words. Dependency length minimisation effects are explained as a means to reduce memory load and for effective communication. In this paper, we ask whether they hold in typically short spans, such as noun phrases, which could be thought of being less subject to efficiency pressure. We demonstrate that minimisation does occur in short spans, but also that it is a complex effect: it is not only the length of the dependency that is at stake, but also the effect of the surrounding dependencies."
D15-1212,Multilingual discriminative lexicalized phrase structure parsing,2015,28,6,1,1,5601,benoit crabbe,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,We provide a generalization of discriminative lexicalized shift reduce parsing techniques for phrase structure grammar to a wide range of morphologically rich languages. The model is efficient and outperforms recent strong baselines on almost all languages considered. It takes advantage of a dependency based modelling of morphology and a shallow modelling of constituency boundaries.
2015.jeptalnrecital-long.25,Comparaison d{'}architectures neuronales pour l{'}analyse syntaxique en constituants,2015,-1,-1,2,1,5608,maximin coavoux,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}article traite de l{'}analyse syntaxique lexicalis{\'e}e pour les grammaires de constituants. On se place dans le cadre de l{'}analyse par transitions. Les mod{\`e}les statistiques g{\'e}n{\'e}ralement utilis{\'e}s pour cette t{\^a}che s{'}appuient sur une repr{\'e}sentation non structur{\'e}e du lexique. Les mots du vocabulaire sont repr{\'e}sent{\'e}s par des symboles discrets sans liens entre eux. {\`A} la place, nous proposons d{'}utiliser des repr{\'e}sentations denses du type plongements (embeddings) qui permettent de mod{\'e}liser la similarit{\'e} entre symboles, c{'}est-{\`a}-dire entre mots, entre parties du discours et entre cat{\'e}gories syntagmatiques. Nous proposons d{'}adapter le mod{\`e}le statistique sous-jacent {\`a} ces nouvelles repr{\'e}sentations. L{'}article propose une {\'e}tude de 3 architectures neuronales de complexit{\'e} croissante et montre que l{'}utilisation d{'}une couche cach{\'e}e non-lin{\'e}aire permet de tirer parti des informations donn{\'e}es par les plongements."
F14-1029,A discriminative parser of the {LR} family for phrase structure parsing (Un analyseur discriminant de la famille {LR} pour l{'}analyse en constituants) [in {F}rench],2014,-1,-1,1,1,5601,benoit crabbe,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
C14-1052,An {LR}-inspired generalized lexicalized phrase structure parser,2014,30,6,1,1,5601,benoit crabbe,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"The paper introduces an LR-based algorithm for efficient phrase structure parsing of morphologically rich languages. The algorithm generalizes lexicalized parsing (Collins, 2003) by allowing a structured representation of the lexical items. Together with a discriminative weighting component (Collins, 2002), we show that this representation allows us to achieve state of the art accurracy results on a morphologically rich language such as French while achieving more efficient parsing times than the state of the art parsers on the French data set. A comparison with English, a lexically poor language, is also provided."
J13-3005,{XMG}: e{X}tensible {M}eta{G}rammar,2013,59,28,1,1,5601,benoit crabbe,Computational Linguistics,0,"In this article, we introduce eXtensible MetaGrammar (xmg), a framework for specifying tree-based grammars such as Feature-Based Lexicalised Tree-Adjoining Grammars (FB-LTAG) and Interaction Grammars (IG). We argue that xmg displays three features which facilitate both grammar writing and a fast prototyping of tree-based grammars. Firstly, xmg is fully declarative. For instance, it permits a declarative treatment of diathesis that markedly departs from the procedural lexical rules often used to specify tree-based grammars. Secondly, the xmg language has a high notational expressivity in that it supports multiple linguistic dimensions, inheritance and a sophisticated treatment of identifiers. Thirdly, xmg is extensible in that its computational architecture facilitates the extension to other linguistic formalisms. We explain how this architecture naturally supports the design of three linguistic formalisms namely, FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show how it permits a straightforward integration of additional mechanisms such as linguistic and formal principles. To further illustrate the declarativity, notational expressivity and extensibility of xmg , we describe the methodology used to specify an FB-LTAG for French augmented with a unification-based compositional semantics. This illustrates both how xmg facilitates the modelling of the tree fragment hierarchies required to specify tree-based grammars and of a syntax/semantics interface between semantic representations and syntactic trees. Finally, we briefly report on several grammars for French, English and German that were implemented using xmg and compare xmg to other existing grammar specification frameworks for tree-based grammars."
F13-1013,Towards a treebank of spoken {F}rench (Vers un treebank du fran{\\c{c}}ais parl{\\'e}) [in {F}rench],2013,-1,-1,2,0,23421,anne abeille,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
seddah-etal-2012-ubiquitous,Ubiquitous Usage of a Broad Coverage {F}rench Corpus: Processing the {E}st {R}epublicain corpus,2012,0,1,3,0.330709,167,djame seddah,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we introduce a set of resources that we have derived from the EST R{\'E}PUBLICAIN CORPUS, a large, freely-available collection of regional newspaper articles in French, totaling 150 million words. Our resources are the result of a full NLP treatment of the EST R{\'E}PUBLICAIN CORPUS: handling of multi-word expressions, lemmatization, part-of-speech tagging, and syntactic parsing. Processing of the corpus is carried out using statistical machine-learning approaches - joint model of data driven lemmatization and part- of-speech tagging, PCFG-LA and dependency based models for parsing - that have been shown to achieve state-of-the-art performance when evaluated on the French Treebank. Our derived resources are made freely available, and released according to the original Creative Common license for the EST R{\'E}PUBLICAIN CORPUS. We additionally provide an overview of the use of these resources in various applications, in particular the use of generated word clusters from the corpus to alleviate lexical data sparseness for statistical parsing."
W11-0601,Testing the Robustness of Online Word Segmentation: Effects of Linguistic Diversity and Phonetic Variation,2011,20,10,3,0,43368,luc boruta,Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,0,"Models of the acquisition of word segmentation are typically evaluated using phonemically transcribed corpora. Accordingly, they implicitly assume that children know how to undo phonetic variation when they learn to extract words from speech. Moreover, whereas models of language acquisition should perform similarly across languages, evaluation is often limited to English samples. Using child-directed corpora of English, French and Japanese, we evaluate the performance of state-of-the-art statistical models given inputs where phonetic variation has not been reduced. To do so, we measure segmentation robustness across different levels of segmental variation, simulating systematic allophonic variation or errors in phoneme recognition. We show that these models do not resist an increase in such variations and do not generalize to typologically different languages. From the perspective of early language acquisition, the results strengthen the hypothesis according to which phonological knowledge is acquired in large part before the construction of a lexicon."
candito-etal-2010-statistical,Statistical {F}rench Dependency Parsing: Treebank Conversion and First Results,2010,23,85,2,1,16504,marie candito,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We first describe the automatic conversion of the French Treebank (Abeill{\'e} and Barrier, 2004), a constituency treebank, into typed projective dependency trees. In order to evaluate the overall quality of the resulting dependency treebank, and to quantify the cases where the projectivity constraint leads to wrong dependencies, we compare a subset of the converted treebank to manually validated dependency trees. We then compare the performance of two treebank-trained parsers that output typed dependency parses. The first parser is the MST parser (Mcdonald et al., 2006), which we directly train on dependency trees. The second parser is a combination of the Berkeley parser (Petrov et al., 2006) and a functional role labeler: trained on the original constituency treebank, the Berkeley parser first outputs constituency trees, which are then labeled with functional roles, and then converted into dependency trees. We found that used in combination with a high-accuracy French POS tagger, the MST parser performs a little better for unlabeled dependencies (UAS=90.3{\%} versus 89.6{\%}), and better for labeled dependencies (LAS=87.6{\%} versus 85.6{\%})."
2010.jeptalnrecital-long.8,Approche quantitative en syntaxe : l{'}exemple de l{'}alternance de position de l{'}adjectif {\\'e}pith{\\`e}te en fran{\\c{c}}ais,2010,-1,-1,3,0,43087,juliette thuilier,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une analyse statistique sur des donn{\'e}es de syntaxe qui a pour but d{'}aider {\`a} mieux cerner le ph{\'e}nom{\`e}ne d{'}alternance de position de l{'}adjectif {\'e}pith{\`e}te par rapport au nom en fran{\c{c}}ais. Nous montrons comment nous avons utilis{\'e} les corpus dont nous disposons (French Treebank et le corpus de l{'}Est-R{\'e}publicain) ainsi que les ressources issues du traitement automatique des langues, pour mener {\`a} bien notre {\'e}tude. La mod{\'e}lisation {\`a} partir de 13 variables relevant principalement des propri{\'e}t{\'e}s du syntagme adjectival, de celles de l{'}item adjectival, ainsi que de contraintes bas{\'e}es sur la fr{\'e}quence, permet de pr{\'e}dire {\`a} plus de 93{\%} la position de l{'}adjectif. Nous insistons sur l{'}importance de contraintes relevant de l{'}usage pour le choix de la position de l{'}adjectif, notamment {\`a} travers la fr{\'e}quence d{'}occurrence de l{'}adjectif, et la fr{\'e}quence de contextes dans lesquels il appara{\^\i}t."
W09-3821,Improving generative statistical parsing with semi-supervised word clustering,2009,12,68,2,1,16504,marie candito,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We present a semi-supervised method to improve statistical parsing performance. We focus on the well-known problem of lexical data sparseness and present experiments of word clustering prior to parsing. We use a combination of lexicon-aided morphological clustering that preserves tagging ambiguity, and unsupervised word clustering, trained on a large unannotated corpus. We apply these clusterings to the French Treebank, and we train a parser with the PCFG-LA unlexicalized algorithm of (Petrov et al., 2006). We find a gain in French parsing performance: from a baseline of F1=86.76% to F1=87.37% using morphological clustering, and up to F1=88.29% using further unsupervised clustering. This is the best known score for French probabilistic parsing. These preliminary results are encouraging for statistically parsing morphologically rich languages, and languages with small amount of annotated data."
W09-3824,Cross parser evaluation : a {F}rench Treebanks study,2009,13,8,3,0.330709,167,djame seddah,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,None
W09-1008,On Statistical Parsing of {F}rench with Supervised and Semi-Supervised Strategies,2009,19,19,2,1,16504,marie candito,Proceedings of the {EACL} 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference,0,"This paper reports results on grammatical induction for French. We investigate how to best train a parser on the French Treebank (Abeille et al., 2003), viewing the task as a trade-off between generaliz-ability and interpretability. We compare, for French, a supervised lexicalized parsing algorithm with a semi-supervised un-lexicalized algorithm (Petrov et al., 2006) along the lines of (Crabbe and Candito, 2008). We report the best results known to us on French statistical parsing, that we obtained with the semi-supervised learning algorithm. The reported experiments can give insights for the task of grammatical learning for a morphologically-rich language, with a relatively limited amount of training data, annotated with a rather flat structure."
2009.jeptalnrecital-long.4,Analyse syntaxique du fran{\\c{c}}ais : des constituants aux d{\\'e}pendances,2009,-1,-1,2,1,16504,marie candito,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une technique d{'}analyse syntaxique statistique {\`a} la fois en constituants et en d{\'e}pendances. L{'}analyse proc{\`e}de en ajoutant des {\'e}tiquettes fonctionnelles aux sorties d{'}un analyseur en constituants, entra{\^\i}n{\'e} sur le French Treebank, pour permettre l{'}extraction de d{\'e}pendances typ{\'e}es. D{'}une part, nous sp{\'e}cifions d{'}un point de vue formel et linguistique les structures de d{\'e}pendances {\`a} produire, ainsi que la proc{\'e}dure de conversion du corpus en constituants (le French Treebank) vers un corpus cible annot{\'e} en d{\'e}pendances, et partiellement valid{\'e}. D{'}autre part, nous d{\'e}crivons l{'}approche algorithmique qui permet de r{\'e}aliser automatiquement le typage des d{\'e}pendances. En particulier, nous nous focalisons sur les m{\'e}thodes d{'}apprentissage discriminantes d{'}{\'e}tiquetage en fonctions grammaticales."
2009.jeptalnrecital-court.1,Adaptation de parsers statistiques lexicalis{\\'e}s pour le fran{\\c{c}}ais : Une {\\'e}valuation compl{\\`e}te sur corpus arbor{\\'e}s,2009,-1,-1,3,0.330709,167,djame seddah,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article pr{\'e}sente les r{\'e}sultats d{'}une {\'e}valuation exhaustive des principaux analyseurs syntaxiques probabilistes dit {``}lexicalis{\'e}s{''} initialement con{\c{c}}us pour l{'}anglais, adapt{\'e}s pour le fran{\c{c}}ais et {\'e}valu{\'e}s sur le CORPUS ARBOR{\'E} DU FRAN{\c{C}}AIS (Abeill{\'e} et al., 2003) et le MODIFIED FRENCH TREEBANK (Schluter {\&} van Genabith, 2007). Confirmant les r{\'e}sultats de (Crabb{\'e} {\&} Candito, 2008), nous montrons que les mod{\`e}les lexicalis{\'e}s, {\`a} travers les mod{\`e}les de Charniak (Charniak, 2000), ceux de Collins (Collins, 1999) et le mod{\`e}le des TIG Stochastiques (Chiang, 2000), pr{\'e}sentent des performances moindres face {\`a} un analyseur PCFG {\`a} Annotation Latente (Petrov et al., 2006). De plus, nous montrons que le choix d{'}un jeu d{'}annotations issus de tel ou tel treebank oriente fortement les r{\'e}sultats d{'}{\'e}valuations tant en constituance qu{'}en d{\'e}pendance non typ{\'e}e. Compar{\'e}s {\`a} (Schluter {\&} van Genabith, 2008; Arun {\&} Keller, 2005), tous nos r{\'e}sultats sont state-of-the-art et infirment l{'}hypoth{\`e}se d{'}une difficult{\'e} particuli{\`e}re qu{'}aurait le fran{\c{c}}ais en terme d{'}analyse syntaxique probabiliste et de sources de donn{\'e}es."
2008.jeptalnrecital-long.17,Exp{\\'e}riences d{'}analyse syntaxique statistique du fran{\\c{c}}ais,2008,-1,-1,1,1,5601,benoit crabbe,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous montrons qu{'}il est possible d{'}obtenir une analyse syntaxique statistique satisfaisante pour le fran{\c{c}}ais sur du corpus journalistique, {\`a} partir des donn{\'e}es issues du French Treebank du laboratoire LLF, {\`a} l{'}aide d{'}un algorithme d{'}analyse non lexicalis{\'e}."
W06-3504,Increasing the coverage of a domain independent dialogue lexicon with {VERBNET},2006,18,3,1,1,5601,benoit crabbe,Proceedings of the Third Workshop on Scalable Natural Language Understanding,0,"This paper investigates how to extend coverage of a domain independent lexicon tailored for natural language understanding. We introduce two algorithms for adding lexical entries from VerbNet to the lexicon of the Trips spoken dialogue system. We report results on the efficiency of the method, discussing in particular precision versus coverage issues and implications for mapping to other lexical databases."
W06-1502,A Constraint Driven Metagrammar,2006,11,2,2,0,5824,joseph roux,Proceedings of the Eighth International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"We present an operational framework allowing to express a large scale Tree Adjoining Grammar (tag) by using higher level operational constraints on tree descriptions. These constraints first meant to guarantee the well formedness of the grammatical units may also be viewed as a way to put model theoretic syntax at work through an efficient offline grammatical compilation process. Our strategy preserves tag formal properties, hence ensures a reasonable processing efficiency."
E06-2005,{XMG} - An Expressive Formalism for Describing Tree-Based Grammars,2006,11,2,3,0,30986,yannick parmentier,Demonstrations,0,"In this paper we introduce eXtensible MetaGrammar, a system that facilitates the development of tree based grammars. This system includes both (1) a formal language adapted to the description of linguistic information and (2) a compiler for this language. It applies techniques of logic programming (e.g. Warren's Abstract Machine), thus providing an efficient and theoretically motivated framework for the processing of linguistic meta-descriptions."
2005.jeptalnrecital-court.8,Projection et monotonie dans un langage de repr{\\'e}sentation lexico-grammatical,2005,-1,-1,1,1,5601,benoit crabbe,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article apporte une m{\'e}thode de d{\'e}veloppement grammatical pour la r{\'e}alisation de grammaires d{'}arbres adjoints (TAG) de taille importante augment{\'e}es d{'}une dimension s{\'e}mantique. La m{\'e}thode que nous pr{\'e}sentons s{'}exprime dans un langage informatique de repr{\'e}sentation grammatical qui est d{\'e}claratif et monotone. Pour arriver au r{\'e}sultat, nous montrons comment tirer parti de la th{\'e}orie de la projection dans le langage de repr{\'e}sentation que nous utilisons. Par cons{\'e}quent cet article justifie l{'}utilisation d{'}un langage monotone pour la repr{\'e}sentation lexico-grammaticale."
2003.jeptalnrecital-long.6,Une plate-forme de conception et d{'}exploitation d{'}une grammaire d{'}arbres adjoints lexicalis{\\'e}s,2003,-1,-1,1,1,5601,benoit crabbe,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons un ensemble d{'}outils de conception et d{'}exploitation pour des grammaires d{'}arbres adjoints lexicalis{\'e}s. Ces outils s{'}appuient sur une repr{\'e}sentation XML des ressources (lexique et grammaire). Dans notre repr{\'e}sentation, {\`a} chaque arbre de la grammaire est associ{\'e} un hypertag d{\'e}crivant les ph{\'e}nom{\`e}nes linguistiques qu{'}il recouvre. De ce fait, la liaison avec le lexique se trouve plus compact{\'e}e et devient plus ais{\'e}e {\`a} maintenir. Enfin, un analyseur permet de valider les grammaires et les lexiques ainsi con{\c{c}}us aussi bien de fa{\c{c}}on interactive que diff{\'e}r{\'e}e sur des corpus."
W02-2233,A new metagrammar compiler,2002,4,15,2,0,46601,bertrand gaiffe,Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+6),0,This paper describes a compiler that allows to generate a wide set of TAG elementary trees associated with feature structures. The initial information is encoded in an inheritance network. This tool is an enhancement of M.-H. Candito's compiler (Candito 99).
