2020.acl-main.192,W13-2322,0,0.0330679,"ll-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), which is then adapted for other tasks (He et al., 2018; Luan et al., 2018, 2019; Dixit and Al-Onaizan, 2019; Zhang and Zhao, 2019). At the core of the model is a mo"
2020.acl-main.192,P19-1525,0,0.0182512,"f analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), which is then adapted for other tasks (He et al., 2018; Luan et al., 2018, 2019; Dixit and Al-Onaizan, 2019; Zhang and Zhao, 2019). At the core of the model is a module to represent each span as a fixed-length vector, which is used to predict labels for spans or span pairs. We first briefly describe the span representation used and proven to be effective in previous works, then highlight some details we introduce to make this model generalize to a wide variety of tasks. Span Representation Given a sentence x = [w1 , w2 , ..., wn ] of n tokens, a span si = [wbi , wbi +1 , ..., wei ] is represented by concatenating two components: a content representation zci calculated as the weighted average across"
2020.acl-main.192,J02-3001,0,0.346515,"Missing"
2020.acl-main.192,P19-1340,0,0.0535015,"Missing"
2020.acl-main.192,C16-1120,0,0.0380112,"Missing"
2020.acl-main.192,D17-1206,0,0.0271568,"provements (p-value < 0.05 with paired bootstrap re-sampling) with SpanBERT, a contextualized embedding pre-trained with span-based training objectives, while only one task degrades (ABSA), indicating its superiority in reconciling spans from different tasks. The GLAD benchmark provides a holistic testbed for evaluating natural language analysis capability. Task Relatedness Analysis To further investigate how different tasks interact with each other, we choose five source tasks (i.e., tasks used to improve other tasks, e.g., POS, NER, Consti., Dep., and SRL) that have been widely used in MTL (Hashimoto et al., 2017; Strubell et al., 2018) and six target tasks (i.e., tasks to be improved, e.g., OpenIE, NER, RE, ABSA, ORL, and SRL) to perform pairwise multi-task learning. We hypothesize that although language modeling pre-training is theoretically orthogonal to MTL (Swayamdipta et al., 2018), in practice their benefits tends to overlap. To analyze these two factors separately, we start with a weak representation GloVe to study task relatedness, then move to BERT to demonstrate how much we can still improve with MTL given strong and contextualized representations. As shown in Table 6 (GloVe), tasks are not"
2020.acl-main.192,P18-2058,0,0.0264805,"l., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), which is then adapted for other tasks (He et al., 2018; Luan et al., 2018, 2019; Dixit and Al-Onaizan, 2019; Zhang and Zhao, 2019). At the core of the model is a module to represent each span as a fixed-length vector, which is used to predict labels for spans or span pairs. We first briefly describe the span representation used and proven to be effective in previous works, then highlight some details we introduce to make this model generalize to a wide variety of tasks. Span Representation Given a sentence x = [w1 , w2 , ..., wn ] of n tokens, a span si = [wbi , wbi +1 , ..., wei ] is represented by concatenating two components: a content represe"
2020.acl-main.192,P17-1044,0,0.0608351,"Missing"
2020.acl-main.192,S10-1006,0,0.0973666,"Missing"
2020.acl-main.192,2020.tacl-1.5,0,0.0471454,"Missing"
2020.acl-main.192,W09-1401,0,0.0237655,"g et al., 2019b), and SuperGLUE (Wang et al., 2019a) have been proposed to facilitate fast and holistic evaluation of models’ understanding ability. They 2127 mainly focus on sentence-level tasks, such as natural language inference, while our GLAD benchmark focuses on token/phrase-level analysis tasks with diverse coverage of different linguistic structures. New tasks and datasets can be conveniently added to our benchmark as long as they are in the BRAT standoff format, which is one of the most commonly used data format in the NLP community, e.g., it has been used in the BioNLP shared tasks (Kim et al., 2009) and the Universal Dependency project (McDonald et al., 2013). 6 Conclusion We provide the simple insight that a large number of natural language analysis tasks can be represented in a single format consisting of spans and relations between spans. As a result, these tasks can be solved in a single modeling framework that first extracts spans and predicts their labels, then predicts relations between spans. We attempted 10 tasks with this SpanRel model and show that this generic task-independent model can achieve competitive performance as state-of-the-art methods tailored for each tasks. We me"
2020.acl-main.192,N18-2016,1,0.917521,"Missing"
2020.acl-main.192,N16-1030,0,0.0832492,"Missing"
2020.acl-main.192,C18-1328,1,0.819372,"rent word with the corresponding dependency type. Detailed explanations of all tasks can be found in Appendix A. While the tasks above represent a remarkably broad swath of NLP, it is worth mentioning what we have not covered, to properly scope this work. Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with span"
2020.acl-main.192,D17-1018,0,0.101382,"g., “born-in”). These labeled relations can form a tree or a graph structure, expressing the linguistic structure of sentences (e.g., dependency tree). We detail this BRAT format and how it can be used to represent a wide number of natural language analysis tasks in Section 2. The simple hypothesis behind our paper is: if humans can perform natural language analysis in a single unified format, then perhaps machines can as well. Fortunately, there already exist NLP models that perform span prediction and prediction of relations between pairs of spans, such as the endto-end coreference model of Lee et al. (2017). We extend this model with minor architectural modifications (which are not our core contributions) and pre-trained contextualized representations (e.g., 2120 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7"
2020.acl-main.192,P19-1441,0,0.137786,"nations of all tasks can be found in Appendix A. While the tasks above represent a remarkably broad swath of NLP, it is worth mentioning what we have not covered, to properly scope this work. Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of"
2020.acl-main.192,W15-4640,0,0.0268177,"o be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; Liu et al., 2019), and thus we mainly focus on tasks using phrase-like spans. In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014), dialog response generation (Lowe et al., 2015), and summarization (Nallapati et al., 2016). There are also a small number of analysis tasks such as semantic parsing to logical forms (Banarescu et al., 2013) where the outputs are not directly associated with spans in the input, and handling these tasks is beyond the scope of this work. 3 ARG1 Span-relation Model Now that it is clear that a very large number of analysis tasks can be formulated in a single format, we turn to devising a single model that can solve these tasks. We base our model on a span-based model first designed for end-to-end coreference resolution (Lee et al., 2017), whic"
2020.acl-main.192,D18-1360,0,0.235918,"n for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 3 3 3 3 7 7 7 3 7 3 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 3 7 3 Single Model for Different Tasks Guo et al. (2016) Swayamdipta et al. (2018) Strubell et al. (2018) Clark et al. (2018) Luan et al. (2018, 2019) Dixit and Al-Onaizan (2019) Marasovi´c and Frank (2018) Hashimoto et al. (2017) This Work 7 7 7 3 3 3 7 7 3 3 7 7 7 3 3 7 7 3 7 3 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 3 7 7 3 3 7 7 7 3 3 Table 1: A comparison of the tasks covered by previous work and our work. BERT; Devlin et al. (2019)1 ) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference resolution (Coref.), open information extraction (OpenIE), part-of-speech tagging (POS), dependency parsing (Dep.), constituency parsing (Const"
2020.acl-main.192,N19-1308,0,0.0909902,"Missing"
2020.acl-main.192,P16-1101,0,0.0378504,"option for span prediction is to formulate it as a sequence labeling task, as in previous works (Lample et al., 2016; He et al., 2017), where time complexity is O(n). Although slower than token-based labeling models, span-based models offer the advantages of being able to model overlapping spans and use span-level information for label prediction (Lee et al., 2017). 5 Related Work General Architectures for NLP There has been a rising interest in developing general architectures for different NLP tasks, with the most prominent examples being sequence labeling framework (Collobert et al., 2011; Ma and Hovy, 2016) used for tagging tasks and sequence-to-sequence framework (Sutskever et al., 2014) used for generation tasks. Moreover, researchers typically pick related tasks, motivated by either linguistic insights or empirical results, and create a general framework to perform MTL, several of which are summarized in Table 1. For example, Swayamdipta et al. (2018) and Strubell et al. (2018) use constituency and dependency parsing to improve SRL. Luan et al. (2018, 2019); Wadden et al. (2019) use a spanbased model to jointly solve three informationextraction-related tasks (NER, RE, and Coref.). Li et al. ("
2020.acl-main.192,N18-1054,0,0.051672,"Missing"
2020.acl-main.192,H94-1020,0,0.839919,"Missing"
2020.acl-main.192,P13-2017,0,0.0586846,"Missing"
2020.acl-main.192,K16-1028,0,0.0728938,"Missing"
2020.acl-main.192,C18-1326,0,0.0779707,"Missing"
2020.acl-main.192,D14-1162,0,0.08335,"Missing"
2020.acl-main.192,N18-1202,0,0.737179,"orm span prediction and prediction of relations between pairs of spans, such as the endto-end coreference model of Lee et al. (2017). We extend this model with minor architectural modifications (which are not our core contributions) and pre-trained contextualized representations (e.g., 2120 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 3 3 3 3 7 7 7 3 7 3 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 3 7 3 Single Model for Different Tasks Guo et al. (2016) Swayamdipta et al. (2018) Strubell et al. (2018) Clark et al. (2018) Luan et al. (2018, 2019) Dixit and Al-Onaizan (2019) Marasovi´c and Frank (2018) Hashimoto et al. (2017) This Work 7 7 7 3 3 3 7 7 3 3 7 7 7 3 3 7 7 3 7 3 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 3 7 7 3 3 7 7 7 3 3 Table 1: A comparison of the tasks covered by previous work and our"
2020.acl-main.192,S14-2004,0,0.109332,"Missing"
2020.acl-main.192,W13-3516,0,0.078957,"Missing"
2020.acl-main.192,W12-4501,0,0.28944,"sily express many tasks by defining L and R accordingly, as summarized in Table 2a and Table 2b. These tasks fall in two categories: span-oriented tasks, where the goal is to predict labeled spans (e.g., named entities in NER) and relation-oriented tasks, where the goal is to predict relations between two spans (e.g., relation between two entities in RE). For example, constituency parsing (Collins, 1997) is a span-oriented task aiming to produce a syntactic parse tree for a sentence, where each node of the tree is an individual span associated with a constituent label. Coreference resolution (Pradhan et al., 2012) is a relation-oriented task that links an expression to its mentions within or beyond a single sentence. Dependency parsing (K¨ubler et al., 2121 Task Spans annotated with labels Task Spans and relations annotated with labels NER Barack Obama was born in Hawaii. RE The burst has been caused by pressure. Coref. I voted for Tom because he is clever. cause-effect person Consti. location coref. And their suspicions of each other run deep . NP NP PP ARG1 ARG0 ADVP VP ARG2 SRL We OpenIE The four lawyers climbed out from under a table. brought you the tale of two cities. NP ARG0 S POS ABSA det What"
2020.acl-main.192,W96-0213,0,0.651256,"Missing"
2020.acl-main.192,W03-0419,0,0.430431,"Missing"
2020.acl-main.192,D16-1252,0,0.117228,"Missing"
2020.acl-main.192,N18-1081,0,0.0655639,"Missing"
2020.acl-main.192,E12-2021,0,0.0576819,"Missing"
2020.acl-main.192,P17-1076,0,0.0379359,"nd oj· is the corresponding scalar from oj· indicating how likely two spans are related. We use option 1 for all tasks except for coreference resolution which uses option 2. Note that the above loss functions only differ in how relation scores are normalized and the other parts of the model remain the same across different tasks. At test time, we follow previous inference methods to generate valid outputs. For coreference resolution, we link a span to the antecedent with highest score (Lee et al., 2017). For constituency parsing, we use greedy top-down decoding to generate a valid parse tree (Stern et al., 2017). For dependency parsing, each word is linked to exactly one parent with the highest relation probability. For other tasks, we predict relations for all span pairs and use those not predicted as NEG REL to construct outputs. Our core insight is that the above formulation is largely task-agnostic, meaning that a task can be modeled in this framework as long as it can be formulated as a span-relation prediction problem with properly defined span labels L and relation labels R. As shown in Table 1, this unified SpanRelation (SpanRel) model makes it simple to scale to a large number of language an"
2020.acl-main.192,D18-1548,0,0.0339302,"Missing"
2020.acl-main.192,D18-1412,0,0.210558,"(e.g., 2120 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120–2133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Information Extraction NER RE Coref. OpenIE POS Parsing Dep. Consti. SRL Sentiment ABSA ORL Different Models for Different Tasks ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) SpanBERT (Joshi et al., 2019) 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 7 3 3 7 7 7 3 3 7 3 7 7 7 7 7 7 3 3 3 3 7 7 7 3 7 3 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 3 7 3 Single Model for Different Tasks Guo et al. (2016) Swayamdipta et al. (2018) Strubell et al. (2018) Clark et al. (2018) Luan et al. (2018, 2019) Dixit and Al-Onaizan (2019) Marasovi´c and Frank (2018) Hashimoto et al. (2017) This Work 7 7 7 3 3 3 7 7 3 3 7 7 7 3 3 7 7 3 7 3 7 7 3 7 7 7 3 7 7 7 7 7 7 7 7 3 7 7 3 3 7 7 7 3 3 Table 1: A comparison of the tasks covered by previous work and our work. BERT; Devlin et al. (2019)1 ) then demonstrate the applicability and versatility of this single model on 10 tasks, including named entity recognition (NER), relation extraction (RE), coreference resolution (Coref.), open information extraction (OpenIE), part-of-speech tagging"
2020.acl-main.192,P19-1452,0,0.0384993,"Missing"
2020.acl-main.192,N03-1033,0,0.0909204,"Missing"
2020.acl-main.192,P19-3007,0,0.024746,", (1) for OpenIE and ORL, multi-task learning with SRL improves the performance significantly, while other tasks lead to less or no improvements. (2) Dependency parsing and SRL are generic source tasks that are beneficial to most of the target tasks. This unified SpanRel makes it easy to perform MTL and decide beneficial source tasks. Next, we demonstrate that our framework also provides a platform for analysis of similarities and differences between different tasks. Inspired by the intuition that the attention coefficients are somewhat indicative of a model’s internal focus (Li et al., 2016; Vig, 2019; Clark et al., 2019), we hypothesize that the similarity or difference between attention mechanisms may be correlated with similarity between tasks, or even the success or failure of MTL. To test this hypothesis, we extract the attention maps of two BERT-based SpanRel models (trained on a source t0 and a target task t separately) over sentences Xt from the target task, and compute 2125 Category IE Task Metric Dataset GloVe STL MTL +FT ELMo STL MTL +FT BERTbase STL MTL +FT SpanBERTbase STL MTL +FT NER F1 CoNLL03 WLP 88.4 86.2↓ 87.5↓ 77.6 71.5↓ 76.5↓ 91.9 91.6 91.6 79.2 77.4↓ 78.2↓ 91.0 88.6↓ 9"
2020.acl-main.192,D19-1585,0,0.0181614,"for different NLP tasks, with the most prominent examples being sequence labeling framework (Collobert et al., 2011; Ma and Hovy, 2016) used for tagging tasks and sequence-to-sequence framework (Sutskever et al., 2014) used for generation tasks. Moreover, researchers typically pick related tasks, motivated by either linguistic insights or empirical results, and create a general framework to perform MTL, several of which are summarized in Table 1. For example, Swayamdipta et al. (2018) and Strubell et al. (2018) use constituency and dependency parsing to improve SRL. Luan et al. (2018, 2019); Wadden et al. (2019) use a spanbased model to jointly solve three informationextraction-related tasks (NER, RE, and Coref.). Li et al. (2019) formulate both nested NER and flat NER as a machine reading comprehension task. Compared to existing works, we aim to create an output representation that can solve nearly every natural language analysis task in one fell swoop, allowing us to cover a far broader range of tasks with a single model. In addition, NLP has seen a recent burgeoning of contextualized representations pre-trained on large corpora (e.g., ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019)). The"
2020.acl-main.192,N19-1242,0,0.0530198,"Missing"
2020.acl-main.192,P13-1161,0,0.048908,"Missing"
2020.acl-main.443,P18-1009,0,0.0186997,"for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to ext"
2020.acl-main.443,W04-1213,0,0.0311238,"nnotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow"
2020.acl-main.443,P11-2008,0,0.191018,"Missing"
2020.acl-main.443,L18-1708,0,0.0384899,"rior programming knowledge can easily recognize that ‘list()’ is code, ‘list’ can be either code or a common English word, whereas ‘listing’ is more likely a non-code natural language token. We thus introduce a code recognition module to capture such prior probability of how 4 https://archive.org/details/ stackexchange likely a word can be a code token without considering any contextual information. It is worth noting that this standalone code recognition model is also useful for language-and-code research, such as retrieving code snippets based on natural language queries (Iyer et al., 2016; Giorgi and Bader, 2018; Yao et al., 2019) Our code recognition model (Code Recognizer) is a binary classifier. It utilizes language model features and spelling patterns to predict whether a word is a code entity. The input features include unigram word and 6-gram character probabilities from two language models (LMs) that are trained on the Gigaword corpus (Napoles et al., 2012) and all the code-snippets in the StackOverflow 10-year archive respectively. We also pre-trained FastText (Joulin et al., 2016) word embeddings using these code-snippets, where a word vector is represented as a sum of its character ngrams."
2020.acl-main.443,D18-1306,0,0.0181373,"from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity t"
2020.acl-main.443,P16-1195,0,0.0663524,"sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identif"
2020.acl-main.443,N18-2016,1,0.919977,"pecific auxiliary tasks. 4917 P Test set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) Dev set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) R F1 71.77 73.03 78.22 77.02 68.77 78.42 39.70 64.82 78.59 45.92 67.47 79.79 51.12 68.68 78.41 57.54 68.12 79.10 66.85 74.44 79.43 79.57 72.11 78.81 46.19 68.71 80.00 46.42 70.51 81.72 54.64 71.46 79.72 58.64 71.30 80.24 models in various domains (Lample et al., 2016; Kulkarni et al., 2018; Dai et al., 2019). • An Attentive BiLSTM-CRF model with in-domain ELMo embeddings as well as domain-specific embeddings from the code recognizer and the entity segmenter. This model combines these three word embeddings using an attention network and then utilizes a BiLSTM-CRF layer to predict the entity type of each input word (details in Appendix B). Table 2: Evaluation on the dev and test sets of the StackOverflow NER corpus. Our SoftNER model outperforms the existing approaches. 4.1 Data We train and evaluate our SoftNER model on the StackOverflow NER corpus of 9,352 train, 2,942 developm"
2020.acl-main.443,N16-1030,0,0.462484,"ings and two domain-specific auxiliary tasks. 4917 P Test set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) Dev set Feature-based CRF BiLSTM-CRF (ELMoVerflow) Attentive BiLSTM-CRF (ELMoVerflow) Fine-tuned BERT Fine-tuned BERTOverflow SoftNER (BERTOverflow) R F1 71.77 73.03 78.22 77.02 68.77 78.42 39.70 64.82 78.59 45.92 67.47 79.79 51.12 68.68 78.41 57.54 68.12 79.10 66.85 74.44 79.43 79.57 72.11 78.81 46.19 68.71 80.00 46.42 70.51 81.72 54.64 71.46 79.72 58.64 71.30 80.24 models in various domains (Lample et al., 2016; Kulkarni et al., 2018; Dai et al., 2019). • An Attentive BiLSTM-CRF model with in-domain ELMo embeddings as well as domain-specific embeddings from the code recognizer and the entity segmenter. This model combines these three word embeddings using an attention network and then utilizes a BiLSTM-CRF layer to predict the entity type of each input word (details in Appendix B). Table 2: Evaluation on the dev and test sets of the StackOverflow NER corpus. Our SoftNER model outperforms the existing approaches. 4.1 Data We train and evaluate our SoftNER model on the StackOverflow NER corpus of 9,35"
2020.acl-main.443,W16-3920,0,0.0149243,"s a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain"
2020.acl-main.443,D17-1086,0,0.0177935,"2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity"
2020.acl-main.443,P16-1101,0,0.0382171,"chieves sufficient performance to be useful for applications on GitHub.7 We leave investigation of semi-supervised learning and other domain adaptation approaches for future work. 7 As a reference, the state-of-the-art performance for 10class Twitter NER is 70.69 F1 (Zhang et al., 2018). 4920 6 Related Work The CoNLL 2003 dataset (Sang and De Meulder, 2003) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball"
2020.acl-main.443,D14-1162,0,0.0948039,"code recognition and entity segmentation. The input to the SoftNER model include 850-dimensional vectors extracted from both the code recognizer and the entity segmenter. We pre-trained BERTbase , ELMo and GloVe vectors on 152 million sentences from the StackOverflow, excluding sentences from the 1,237 posts in our annotated corpus. The pretraining of the 768-dimensional BERTbase model with 64,000 WordPiece vocabulary took 7 days on a Google TPU. The pre-training of 1024dimensional ELMo vectors took 46 days on 3 NVIDIA Titan X Pascal GPUs. The pre-training of 300-dimensional GloVe embeddings (Pennington et al., 2014) with a frequency cut-off of 5 took 8 hours on a server with 32 CPU cores and 386 GB memory. We train the SoftNER model and the two auxiliary models separately. Our segmentation model follows the simple BERT fine-tuning architecture except for the input, where BERT embeddings are concatenated with 100-dimensional code markdown and 10-dimensional word frequency features. We set the number of bins k to 10 for Gaussian vectorization. Our code recognition model is a feedforward network with two hidden layers and a single output node with sigmoid activation. 4 Evaluation In this section, we show th"
2020.acl-main.443,N18-1202,0,0.720905,"ey contributions are the following: 1 Our code and data are available at: https:// github.com/jeniyat/StackOverflowNER/ • A new StackOverflow NER corpus manually annotated with 20 types of named en4913 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4913–4926 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tities, including all in-line code within natural language sentences (§2). We demonstrate that NER in the software domain is an ideal benchmark task for testing effectiveness of contextual word representations, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), due to its inherent polysemy and salient reliance on context. • An in-domain trained neural SoftNER tagger for StackOveflow (§3) that can recognize 20 fine-grained named entity types related to software developing. We also tested its performance on GitHub data of readme files and issue reports. • A code token recognizer (§3.1) that utilizes StackOveflow code snippets to capture the spelling patterns of code-related tokens, and consistently improves the NER tagger. • In-domain pretrained ELMo and BERT representations (§3.3) on 152 million sentences from StackOve"
2020.acl-main.443,E14-1078,0,0.0202013,"LL 2003 dataset (Sang and De Meulder, 2003) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on nam"
2020.acl-main.443,P15-1085,0,0.0326894,"low) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and tec"
2020.acl-main.443,D18-1410,1,0.927645,"ilizes language model features and spelling patterns to predict whether a word is a code entity. The input features include unigram word and 6-gram character probabilities from two language models (LMs) that are trained on the Gigaword corpus (Napoles et al., 2012) and all the code-snippets in the StackOverflow 10-year archive respectively. We also pre-trained FastText (Joulin et al., 2016) word embeddings using these code-snippets, where a word vector is represented as a sum of its character ngrams. We first transform each ngram probability into a k-dimensional vector using Gaussian binning (Maddela and Xu, 2018), which has shown to improve the performance of neural models using numeric features (Sil et al., 2017; Liu et al., 2016; Maddela and Xu, 2018). We then feed the vectorized features into a linear layer, concatenate the output with FastText character-level embeddings, and pass them through another hidden layer with sigmoid activation. We predict the token as a codeentity if the output probability is greater than 0.5. This binary prediction is then converted into a vector and used as an input to the SoftNER model. 3.1.3 Entity Segmentation The segmentation task refers to identifying entity spans"
2020.acl-main.443,P14-5010,0,0.00242252,"readme files collected from these 143 repositories. The resulting GitHub NER dataset consists of 6,510 sentences and 10,963 entities of 20 types labeled by two inhouse annotators. The inter-annotator agreement of this dataset is 0.68, measured by span-level Cohen’s Kappa. 2.4 common features of web texts, including abbreviations, emoticons, URLs, ungrammatical sentences and spelling errors. We found that tokenization is non-trivial as many code-related tokens are mistakenly split by the existing web-text tokenizers, including the CMU Twokenizer (Gimpel et al., 2011), Stanford TweetTokenizer (Manning et al., 2014), and NLTK Twitter Tokenizer (Bird et al., 2009): txScope.Complete() std::condition variable math.h hspani a==b Therefore, we implemented a new tokenizer, using Twokenizer3 as the starting point and added additional regular expression rules to avoid splitting code-related tokens. 3 Named Entity Recognition Models The extraction of software-related named entities imposes significant challenges as it requires resolving a significant amount of unseen tokens, inherent polysemy, and salient reliance on context. Unlike news or biomedical data, spelling patterns and long-distance dependencies are mor"
2020.acl-main.443,D11-1141,1,0.492578,"Related Work The CoNLL 2003 dataset (Sang and De Meulder, 2003) is a widely used benchmark for named entity recognition, which contains annotated newswire text from the Reuters RCV1 corpus. State-of-the-art approaches on this dataset (Baevski et al., 2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively litt"
2020.acl-main.443,P15-1140,0,0.0316407,"tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identifying software-related named entities (e.g., variable names or application names) within natural language texts. In this paper, we present a comprehensive study that investigates the unique challenges of named entity recognition in the social computer programming domain. These named entities are often ambiguous and have implicit reliance on the accompanied code snippets. For example,"
2020.acl-main.443,W12-3018,0,0.0368501,"Missing"
2020.acl-main.443,N19-1250,0,0.0158917,"s social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to extract those finegrained entities. 7 long-dist"
2020.acl-main.443,W03-0419,0,0.151891,"Missing"
2020.acl-main.443,W18-5618,0,0.0385168,"Missing"
2020.acl-main.443,E12-2021,0,0.0329356,"Missing"
2020.acl-main.443,D18-1310,0,0.0139321,"s a feature, converting the scalar value into a k-dimensional vector by Gaussian binning (Maddela and Xu, 2018). • Code Markdown indicates whether the given token appears inside a hcodei markdown tag in the StackOverflow post. It is worth noting that hcodei tags are noisy as users do not always enclose inline code in a hcodei tag or sometimes use the tag to highlight non-code texts (details in §2.1). Nevertheless, we find it helpful to include the markdown information as a feature as it improves the performance of our segmentation model. The inclusion of hand-crafted features is influenced by Wu et al. (2018), where word-shapes and POS tags were shown to improve the performance of sequence tagging models. 3.2 Embedding-Level Attention For each input word wi in the input sentence, we have three embeddings: BERT (wi1 ), Code Recognizer (wi2 ), and Entity Segmenter (wi3 ). We introduce the embedding-level attention αit (t ∈ {1, 2, 3}), which captures each embedding’s contribution towards the meaning of the word, to combine them together. To compute αit , we pass the input embeddings through a bidirectional GRU and generate their corresponding hidden repre←−−→ sentations hit = GRU (wit ). These vector"
2020.acl-main.443,D18-1034,0,0.012837,"2019) use a bidirectional LSTM (Lample et al., 2016; Ma and Hovy, 2016) with conditional random field (Collobert et al., 2011) and contextualized word representations (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2019). Named entity recognition has been explored for new domains and languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standar"
2020.acl-main.443,C18-1183,0,0.0143257,"d languages, such as social media (Finin et al., 2010; Ritter et al., 2011; Plank et al., 2014; Derczynski et al., 2015; Limsopatham and Collier, 2016; Aguilar et al., 2017), biomedical texts (Collier and Kim, 2004; Greenberg et al., 2018; Kulkarni et al., 2018), multilingual texts (Benajiba et al., 2008; Xie et al., 2018) and codeswitched corpora (Aguilar et al., 2018; Ball and Garrette, 2018). Various methods have been investigated for handling rare entities, for example incorporating external context (Long et al., 2017) or approaches that make use of distant supervision (Choi et al., 2018; Yang et al., 2018; Onoe and Durrett, 2019). There has been relatively little prior work on named entity recognition in the software engineering domain. Ye et al. (2016) annotated 4,646 sentences from StackOverflow with five named entity types (Programming Language, Platform, API, Tool-Library-Framework and Software Standard). The authors used a traditional feature-based CRF to recognize these entities. In contrast, we present a much larger annotated corpus consisting of 15,372 sentences labeled with 20 fine-grained entity types. We also develop a novel attention based neural NER model to extract those finegrai"
2020.acl-main.443,N16-1174,0,0.156375,"Missing"
2020.acl-main.443,D18-2002,0,0.063399,"kOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT. We also present the SoftNER model which achieves an overall 79.10 F1 score for code and named entity recognition on StackOverflow data. Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERTbased tagging model.1 1 Figure 1: Examples of software-related named entities in a StackOverflow post. Introduction Recently there has been significant interest in modeling human language together with computer code (Quirk et al., 2015; Iyer et al., 2016; Yin and Neubig, 2018), as more data becomes available on websites such as StackOverflow and GitHub. This is an ambitious yet promising direction for scaling up language understanding to richer domains. Access to domain-specific NLP tools could help a wide range of downstream applications. For example, extracting software knowledge bases from text (Movshovitz-Attias and Cohen, 2015), developing better quality measurements of StackOverflow posts (Ravi et al., 2014), finding similar questions (Amirreza Shirani, 2019) and more. However, there is a lack of NLP resources and techniques for identifying software-related n"
2020.acl-main.443,D08-1030,0,\N,Missing
2020.acl-main.443,N03-1031,0,\N,Missing
2020.acl-main.443,W17-4419,0,\N,Missing
2020.acl-main.443,W18-3219,0,\N,Missing
2020.acl-main.443,D18-1347,0,\N,Missing
2020.acl-main.443,D19-1539,0,\N,Missing
2020.acl-main.443,N19-1423,0,\N,Missing
2020.acl-main.443,W10-0713,0,\N,Missing
2020.acl-main.709,C12-1034,0,0.0275277,"a, 2007; Siddharthan and Katsos, 2010), and people with language disorders (Rello et al., 2013). As a preprocessing step, text simplification can also improve 1 Code and data are available at: https://github. com/chaojiang06/wiki-auto. Newsela data need to be requested at: https://newsela.com/data/. the performance of many natural language processing (NLP) tasks, such as parsing (Chandrasekar et al., 1996), semantic role labelling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) , summarization (Vanderwende et al., 2007; Xu and Grishman, 2009), and machine translation ˇ (Chen et al., 2012; Stajner and Popovic, 2016). Automatic text simplification is primarily addressed by sequence-to-sequence (seq2seq) models whose success largely depends on the quality and quantity of the training corpus, which consists of pairs of complex-simple sentences. Two widely used corpora, N EWSELA (Xu et al., 2015) and W IK I L ARGE (Zhang and Lapata, 2017), were created by automatically aligning sentences between comparable articles. However, due to the lack of reliable annotated data,2 sentence pairs are often aligned using surface-level similarity metrics, such as Jaccard coefficient (Xu et al.,"
2020.acl-main.709,P17-1152,0,0.0331246,"hen sentences undergo dramatic rewriting. Paragraph Alignment Adding paragraph alignment (BERTf inetune + ParaAlign) improves the precision on Task 1 from 93.3 to 98.4 with a negligible decrease in recall when compared to not aligning paragraphs (BERTf inetune ). Moreover, paragraph alignments generated by our algorithm (Our Aligner) perform close to the gold alignments (Our Aligner + gold ParaAlign) with only 0.9 and 0.3 difference in F1 on Task 1 and 2, respectively. Semantic Similarity BERTf inetune performs better than other neural models, including Infersent (Conneau et al., 2017), ESIM (Chen et al., 2017), BERTScore (Zhang et al., 2020) and pretrained BERT embedding (Devlin et al., 2019). For BERTScore, we use idf weighting, and treat simple sentence as reference. 5 5.2 Comparison with existing datasets Existing datasets of complex-simple sentences, N EWSELA (Xu et al., 2015) and W IKI L ARGE (Zhang and Lapata, 2017), were aligned using lexical similarity metrics. N EWSELA dataset (Xu et al., Baselines and Simplification Models We compare the following seq2seq models trained using our new datasets versus the existing datasets: 1. A BERT-initialized Transformer, where the encoder and decoder fo"
2020.acl-main.709,C10-1152,0,0.526715,"between comparable articles. However, due to the lack of reliable annotated data,2 sentence pairs are often aligned using surface-level similarity metrics, such as Jaccard coefficient (Xu et al., 2015) or cosine distance of TF-IDF vectors (Paetzold et al., 2017), which fails to capture paraphrases and the context of surrounding sentences. A common drawback of text simplification models trained on such datasets is that they behave conservatively, performing mostly deletion, and rarely paraphrase (Alva-Manchego et al., 2017). Moreover, W IKI L ARGE is the concatenation of three early datasets (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011) that are extracted from Wikipedia dumps and are known to contain many errors (Xu et al., 2015). To address these problems, we create the first high-quality manually annotated sentence-aligned datasets: N EWSELA -M ANUAL with 50 article sets, and W IKI -M ANUAL with 500 article pairs. We design a novel neural CRF alignment model, which utilizes fine-tuned BERT to measure semantic similarity and leverages the similar order of content be2 Hwang et al. (2015) annotated 46 article pairs from Simple-Normal Wikipedia corpus; however, its annotati"
2020.acl-main.709,C10-1089,0,\N,Missing
2020.acl-main.709,Q15-1021,1,\N,Missing
2020.acl-main.709,W09-2809,1,\N,Missing
2020.acl-main.709,C96-2183,0,\N,Missing
2020.acl-main.709,W03-1004,0,\N,Missing
2020.acl-main.709,W07-1007,0,\N,Missing
2020.acl-main.709,P02-1040,0,\N,Missing
2020.acl-main.709,N10-1063,0,\N,Missing
2020.acl-main.709,D11-1038,0,\N,Missing
2020.acl-main.709,P08-1040,0,\N,Missing
2020.acl-main.709,P11-2117,0,\N,Missing
2020.acl-main.709,P12-1107,0,\N,Missing
2020.acl-main.709,P14-1041,0,\N,Missing
2020.acl-main.709,D14-1162,0,\N,Missing
2020.acl-main.709,R13-1091,0,\N,Missing
2020.acl-main.709,P15-2135,0,\N,Missing
2020.acl-main.709,W15-3014,0,\N,Missing
2020.acl-main.709,O13-1007,0,\N,Missing
2020.acl-main.709,N15-1022,0,\N,Missing
2020.acl-main.709,N16-1072,0,\N,Missing
2020.acl-main.709,Q16-1029,1,\N,Missing
2020.acl-main.709,D17-1062,0,\N,Missing
2020.acl-main.709,P17-2014,0,\N,Missing
2020.acl-main.709,W16-3411,0,\N,Missing
2020.acl-main.709,I17-3001,0,\N,Missing
2020.acl-main.709,I17-1030,0,\N,Missing
2020.acl-main.709,L18-1615,0,\N,Missing
2020.acl-main.709,C18-1116,0,\N,Missing
2020.acl-main.709,N19-1317,0,\N,Missing
2020.acl-main.709,P19-1331,0,\N,Missing
2020.acl-main.709,N19-1423,0,\N,Missing
2020.acl-main.709,D19-1136,0,\N,Missing
2020.acl-main.709,N10-1144,0,\N,Missing
2020.acl-main.709,W14-1210,0,\N,Missing
2020.emnlp-main.382,2020.acl-main.709,1,0.873797,"Missing"
2020.emnlp-main.382,kamholz-etal-2014-panlex,0,0.0191955,"Missing"
2020.emnlp-main.382,W19-5407,0,0.0284384,"ki, Oscar (+ code-switch) 10.4B/6.1B/4.3B WordPiece 50k/21k/26k no base 125M Table 1: Configuration comparisons for AraBERT (Antoun et al., 2020), mBERT (Devlin et al., 2019), XLMRoBERTa (Conneau et al., 2020a), and GigaBERT (this work). trained language model designed specifically for English-Arabic. K et al. (2020) pre-trained smallscale (e.g., 1GB data and 2M training steps) bilingual BERT for English-Hindi, English-Spanish, and English-Russian to study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives on crosslingual transfer. Kim et al. (2019) presented a bilingual BERT using multi-task learning for translation quality estimation with regards to English-Russian and English-German. Conneau et al. (2020b) focused on the bilingual XLM for English-French, English-Russian, and English-Chinese to analyze the cross-lingual transfer ability with domain similarity, anchor points, parameter sharing, and language similarity. 3 GigaBERT We present five versions of GigaBERT pre-trained using the Transformer encoder (Vaswani et al., 2017) with BERTbase configurations: 12 attention layers, each has 12 attention heads and 768 hidden dimensions, wh"
2020.emnlp-main.382,D19-1077,0,0.135851,"20) in both the supervised and zero-shot transfer settings. We have made our pre-trained models publicly available at https://github.com/ lanwuwei/GigaBERT. 1 Introduction Fine-tuning pre-trained Transformer models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) has recently achieved state-of-the-art results on a wide range of NLP tasks where supervised training data is available. When trained on multilingual corpora, BERT-based models have demonstrated the ability to learn multilingual representations that support zero-shot cross-lingual transfer learning surprisingly effectively (Wu and Dredze, 2019; Pires et al., 2019; Lample and Conneau, 2019). Without access to any parallel text or target language annotations, multilingual BERT (mBERT; Devlin et al., 2019) even supports cross-lingual transfer for language pairs that are written in different scripts, for example, English-to-Arabic. However, the transfer learning performance still lags far behind where supervised data is available in the target language. In this paper, we explore to what extent it is possible to improve performance in the zero-shot scenario by building a customized bilingual BERT for English and Arabic, a particularly c"
2020.wnut-1.33,C18-1139,0,0.0617004,"We provided the participants baseline model for both of the subtasks. The baseline model for named 261 entity recognition task utilized a feature-based CRF tagger developed using the CRF-Suite3 with a standard set of contextual, lexical and gazetteer features. The baseline relation extraction system employed a feature-based logistic regression model developed using the Scikit-Learn4 with a standard set of contextual, lexical and gazetteer features. DSC-IITISM (Gupta et al., 2020) developed a BiLSTM-CRF model that utilized a concatenation of CamemBERTbase (Martin et al., 2020), Flair(PubMed) (Akbik et al., 2018), and GloVe(en) (Pennington et al., 2014) word representations. 2.3 IBS (Sikdar et al., 2020) utilized an ensemble classifier with 4 feature based on CRF taggers. Fancy Man (Zeng et al., 2020) fine-tuned the BERTbase (Devlin et al., 2019) model with an additional linear layer. NER Systems Thirteen teams (Table 3) participated in the named entity recognition sub-task. A wide variety of approaches were taken to tackle this task. Table 2 summarizes the word representations, features and the machine learning approaches taken by each team. Majority of the teams (11 out of 13) utilized contextual wo"
2020.wnut-1.33,N19-1423,0,0.247952,"and gazetteer features. The baseline relation extraction system employed a feature-based logistic regression model developed using the Scikit-Learn4 with a standard set of contextual, lexical and gazetteer features. DSC-IITISM (Gupta et al., 2020) developed a BiLSTM-CRF model that utilized a concatenation of CamemBERTbase (Martin et al., 2020), Flair(PubMed) (Akbik et al., 2018), and GloVe(en) (Pennington et al., 2014) word representations. 2.3 IBS (Sikdar et al., 2020) utilized an ensemble classifier with 4 feature based on CRF taggers. Fancy Man (Zeng et al., 2020) fine-tuned the BERTbase (Devlin et al., 2019) model with an additional linear layer. NER Systems Thirteen teams (Table 3) participated in the named entity recognition sub-task. A wide variety of approaches were taken to tackle this task. Table 2 summarizes the word representations, features and the machine learning approaches taken by each team. Majority of the teams (11 out of 13) utilized contextual word representations. Four teams combined the contextual word representations with global word vectors. Only two teams did not use any type of word representations and relied entirely on hand-engineered features and a CRF taggers. The best"
2020.wnut-1.33,doddington-etal-2004-automatic,0,0.139539,"ticipant teams, evaluated on the Test-20 corpus. Both of the teams utilized the gold entities and then predict the relations among these entities by fine-tuning con848 16161 8000 6000 Baseline 3015 587 4558 Error Count Figure 2: Summary of incorrectly classified entity tokens by each submitted systems. 4000 2000 Figure 3: Summary of incorrectly predicted relations in each submitted systems. 4 Related Work The task of information extraction from wet lab protocols is closely related to the event trigger extraction task. The event trigger task has been studied extensively, mostly using ACE data (Doddington et al., 2004) and the BioNLP data (N´edellec et al., 2013). Broadly, there are two ways to classify various event trigger detection models: (1) Rule-based methods using pattern matching and regular expression to identify triggers (Vlachos et al., 2009) and (2) Machine Learning based methods focusing on generation of high-end hand-crafted features to be used in classification models like SVMs or maxent classifiers (Pyysalo et al., 2012). Kernel based learning methods have also been utilized with embedded features from the syntactic and semantic contexts to identify and extract the biomed264 ical event entit"
2020.wnut-1.33,2020.acl-main.740,0,0.0383653,"Missing"
2020.wnut-1.33,2020.acl-main.192,1,0.820751,"chnologies to extract structured representations of procedures from natural language protocols (Kuniyoshi et al., 2020; Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010). Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions. However, performance of state-of-the-art tools for extracting named entity and relations from wet lab protocols still lags behind well edited text genres (Jiang et al., 2020). This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. 1 https://autoprotocol.org/ 2 Wet Lab Protocols Wet lab protocols consist of the guidelines from different lab procedures which involve chemicals, drugs, or other materials in liquid solutions or volatile phases. The protocols contain a sequence of steps that are followed to perform a desired task. These protocols also include general guidelines or warnings about the materials being used. The publicly available archive of protocol.io contains such guidelines of wet lab expe"
2020.wnut-1.33,2020.wnut-1.34,0,0.0265821,"Missing"
2020.wnut-1.33,W04-1213,0,0.041156,"also been utilized with embedded features from the syntactic and semantic contexts to identify and extract the biomed264 ical event entities (Zhou et al., 2014). In order to counteract highly sparse representations, different neural models were proposed. These neural models utilized the dependency based word embeddings with feed forward neural networks (Wang et al., 2016b), CNNs (Wang et al., 2016a) and Bidirectional RNNs (Rahul et al., 2017). Previous work has experimented on datasets of well-edited biomedical publications with a small number of entity types. For example, the JNLPBA corpus (Kim et al., 2004) with 5 entity types (CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN) and the BC2GM corpus (Hirschman et al., 2005) with a single entity class for genes/proteins. In contrast, our dataset addresses the challenges of recognizing 18 finegrained named entities along with 15 types of relations from the user-created wet lab protocols. 5 Summary In this paper, we presented a shared task for consisting of two sub-tasks: named entity recognition and relation extraction from the wet lab protocols. We described the task setup and datasets details, and also outlined the approach taken by the participating s"
2020.wnut-1.33,2020.wnut-1.40,0,0.56796,"near CRF with hand-crafted features. mahab (Pour and Farinnia, 2020) fine-tuned the BERTbase (Devlin et al., 2019) sequence tagging model. mgsohrab (Sohrab et al., 2020) fine-tuned the SciBERT (Beltagy et al., 2019) model. PublishInCovid19 (Singh and Wadhawan, 2020) employed a structured ensemble classifier (Nguyen and Guo, 2007) consisting of 11 BiLSTMCRF taggers, that utilized the PubMedBERT (Gu et al., 2020) word representation. SudeshnaTCS (Jana, 2020) fine-tuned XLNet (Yang et al., 2019) model. IITKGP (Kaushal and Vaidhya, 2020) finetuned the Bio-BERT (Lee et al., 2020) model. 2.4 BiTeM (Knafou et al., 2020) developed a voting based ensemble classifier containing 14 transformer models, and utilized 7 different word representations including BERT (Devlin et al., 2019), ClinicalBERT (Huang et al., 2019), PubMedBERTbase (Gu et al., 2020), BioBERT (Lee et al., 2020), RoBERTa (Liu et al., 2019), BiomedRoBERTabase (Gururangan et al., 2020) and XLNet (Yang et al., 2019). 3 Kabir (Khan, 2020) employed an RNN-CRF model that utilized concatenation of Flair(PubMed) (Akbik et al., 2018) and ELMo(PubMed) (Peters et al., 2018) word representations. RE Systems Two teams (Table 3) participated in the relation ex"
2020.wnut-1.33,N18-2016,1,0.301656,"e, see Figure 1). While there have been efforts to develop domain-specific formal languages in order to support robotic automation1 of experimental procedures (Bates et al., 2017), the vast majority of knowledge about how to carry out biological experiments or chemical synthesis procedures is only documented in natural language texts, including in scientific papers, electronic lab notebooks, and so on. Recent research has begun to apply human language technologies to extract structured representations of procedures from natural language protocols (Kuniyoshi et al., 2020; Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010). Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions. However, performance of state-of-the-art tools for extracting named entity and relations from wet lab protocols still lags behind well edited text genres (Jiang et al., 2020). This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. 1 https://autoprotocol"
2020.wnut-1.33,2020.lrec-1.239,0,0.0116175,"hemistry or biology experiments (for an example, see Figure 1). While there have been efforts to develop domain-specific formal languages in order to support robotic automation1 of experimental procedures (Bates et al., 2017), the vast majority of knowledge about how to carry out biological experiments or chemical synthesis procedures is only documented in natural language texts, including in scientific papers, electronic lab notebooks, and so on. Recent research has begun to apply human language technologies to extract structured representations of procedures from natural language protocols (Kuniyoshi et al., 2020; Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010). Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions. However, performance of state-of-the-art tools for extracting named entity and relations from wet lab protocols still lags behind well edited text genres (Jiang et al., 2020). This motivates the need for continued research, in addition to new datasets and tools adapted to"
2020.wnut-1.33,2021.ccl-1.108,0,0.0224884,"Missing"
2020.wnut-1.33,N19-1308,0,0.0327409,"tence 24.32 318.77 255.25 171.90 13.11 10.49 7.07 Table 1: Statistics of the Wet Lab Protocol corpus. noisy style of user created protocols imposed crucial challenges for the entity and relation extraction systems. Hence, off-the-shelf named entity recognition and relation extraction tools, tuned for well edited texts, suffer a severe performance degradation when applied to noisy protocol texts (Kulkarni et al., 2018). To address these challenges, there has been an increasing body of work on adapting entity and relation extraction recognition tools for noisy wet lab texts (Jiang et al., 2020; Luan et al., 2019; Kulkarni et al., 2018). However, different research groups have used different evaluation setups (e.g., training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation, we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user generated wet-lab text genres. T YPE -L INK, C O - REFERENCE -L INK, M OD -L INK, C OUNT, M ERONYM, U SING, M EASURE, C OM MANDS, O F -T YPE, O R, P RODUCT, and ACTS - ON. The training and development dataset for our t"
2020.wnut-1.33,2020.wnut-1.36,0,0.0133602,"and XLNet (Yang et al., 2019). 3 Kabir (Khan, 2020) employed an RNN-CRF model that utilized concatenation of Flair(PubMed) (Akbik et al., 2018) and ELMo(PubMed) (Peters et al., 2018) word representations. RE Systems Two teams (Table 3) participated in the relation extraction sub-task. Both of the teams followed fine-tuning of contextual word representation and did not use any hand-crafted features. Table 5 summarizes the word representations and the machine learning approaches followed by each team. Below we provide a brief description of the model developed by taken by each team. Big Green (Miller and Vosoughi, 2020) considered the protocols as a knowledge graph, in which relationships between entities are edges in the knowledge graph. They trained a BERT (Devlin et al., 2019) based system to classify edge presence and type between two entities, given entity text, label, and local context. 262 Team BiTeM PublishInCovid19 Fancy Man mahab mgsohrab SudeshnaTCS IITKGP B-NLP BIO-BIO DSC-IITISM Kabir IBS KaushikAcharya Baseline Word Representation BERT, BioBERT, RoBERTa, XLNet PubMedBERT BERT BERT SciBERT XLNet BioBERT SciBERT, word2vec BioBERT GLoVe, CamemBERT, Flair GLoVe, ELMo, BERT, Flair - Features Lexical"
2020.wnut-1.33,W13-2001,0,0.086491,"Missing"
2020.wnut-1.33,D14-1162,0,0.0824004,"Missing"
2020.wnut-1.33,N18-1202,0,0.00904826,"ushal and Vaidhya, 2020) finetuned the Bio-BERT (Lee et al., 2020) model. 2.4 BiTeM (Knafou et al., 2020) developed a voting based ensemble classifier containing 14 transformer models, and utilized 7 different word representations including BERT (Devlin et al., 2019), ClinicalBERT (Huang et al., 2019), PubMedBERTbase (Gu et al., 2020), BioBERT (Lee et al., 2020), RoBERTa (Liu et al., 2019), BiomedRoBERTabase (Gururangan et al., 2020) and XLNet (Yang et al., 2019). 3 Kabir (Khan, 2020) employed an RNN-CRF model that utilized concatenation of Flair(PubMed) (Akbik et al., 2018) and ELMo(PubMed) (Peters et al., 2018) word representations. RE Systems Two teams (Table 3) participated in the relation extraction sub-task. Both of the teams followed fine-tuning of contextual word representation and did not use any hand-crafted features. Table 5 summarizes the word representations and the machine learning approaches followed by each team. Below we provide a brief description of the model developed by taken by each team. Big Green (Miller and Vosoughi, 2020) considered the protocols as a knowledge graph, in which relationships between entities are edges in the knowledge graph. They trained a BERT (Devlin et al.,"
2020.wnut-1.33,W17-2340,0,0.0124734,"generation of high-end hand-crafted features to be used in classification models like SVMs or maxent classifiers (Pyysalo et al., 2012). Kernel based learning methods have also been utilized with embedded features from the syntactic and semantic contexts to identify and extract the biomed264 ical event entities (Zhou et al., 2014). In order to counteract highly sparse representations, different neural models were proposed. These neural models utilized the dependency based word embeddings with feed forward neural networks (Wang et al., 2016b), CNNs (Wang et al., 2016a) and Bidirectional RNNs (Rahul et al., 2017). Previous work has experimented on datasets of well-edited biomedical publications with a small number of entity types. For example, the JNLPBA corpus (Kim et al., 2004) with 5 entity types (CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN) and the BC2GM corpus (Hirschman et al., 2005) with a single entity class for genes/proteins. In contrast, our dataset addresses the challenges of recognizing 18 finegrained named entities along with 15 types of relations from the user-created wet lab protocols. 5 Summary In this paper, we presented a shared task for consisting of two sub-tasks: named entity rec"
2020.wnut-1.33,2020.wnut-1.35,0,0.0682165,"Missing"
2020.wnut-1.33,2020.wnut-1.38,0,0.480082,"Missing"
2020.wnut-1.33,E12-2021,0,0.0592604,"Missing"
2020.wnut-1.33,W09-1405,0,0.0756553,"Missing"
2020.wnut-1.33,2020.wnut-1.39,0,0.0145229,"3 with a standard set of contextual, lexical and gazetteer features. The baseline relation extraction system employed a feature-based logistic regression model developed using the Scikit-Learn4 with a standard set of contextual, lexical and gazetteer features. DSC-IITISM (Gupta et al., 2020) developed a BiLSTM-CRF model that utilized a concatenation of CamemBERTbase (Martin et al., 2020), Flair(PubMed) (Akbik et al., 2018), and GloVe(en) (Pennington et al., 2014) word representations. 2.3 IBS (Sikdar et al., 2020) utilized an ensemble classifier with 4 feature based on CRF taggers. Fancy Man (Zeng et al., 2020) fine-tuned the BERTbase (Devlin et al., 2019) model with an additional linear layer. NER Systems Thirteen teams (Table 3) participated in the named entity recognition sub-task. A wide variety of approaches were taken to tackle this task. Table 2 summarizes the word representations, features and the machine learning approaches taken by each team. Majority of the teams (11 out of 13) utilized contextual word representations. Four teams combined the contextual word representations with global word vectors. Only two teams did not use any type of word representations and relied entirely on hand-en"
2021.acl-long.531,D18-1080,0,0.0244736,"ment model (Jiang et al., 2020) to align sentences between different versions of the papers and sample 200 nonidentical aligned sentence pairs for further annotation. The word alignment is annotated in a similar procedure to that of the MultiMWA-Wiki. MultiMWA-Wiki. Wikipedia has been widely used in text-to-text tasks, including text simpli5 More specifically, we sample from the exact test set used in Table 2 in Maddela et al. (2021). 6 This annotator has annotated MultiMWA-MTRef. 7 https://arxiv.org/ 8 https://github.com/pkubowicz/ opendetex fication (Jiang et al., 2020), sentence splitting (Botha et al., 2018), and neutralizing bias language (Pryzant et al., 2020). We follow the method in (Pryzant et al., 2020) to extract parallel sentences from Wikipedia revision history dump (dated 01/01/2021) and randomly sample 4,099 sentence pairs for further annotation. We first use an earlier version of our neural semi-CRF word aligner (§3) to automatically align words for the sentence pairs, then ask two in-house annotators to correct the aligner’s outputs. The interannotator agreement is 98.1 at token-level measured by F1 .9 We split the data into 2514/533/1052 sentence pairs for train/dev/test sets. 5 Exp"
2021.acl-long.531,D15-1075,0,0.0373731,"stion answering (WikiQA, TrecQA), and semantic textual similarity (STS-B, STS14) tasks. The datasets in this table are ordered by the size of their training set, as shown in the second row. 6.2 7 Sentence Pair Modeling We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) da"
2021.acl-long.531,S17-2001,0,0.023441,"tence Pair Modeling We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can"
2021.acl-long.531,N19-1423,0,0.0126344,"9.8 / 49.6 99.1 / 47.1 49.0 27.0 99.5 96.9 94.9 94.9 97.1 99.6 / 66.2 99.6 / 60.0 99.6 / 62.9 96.1 98.2 97.2 99.3 / 76.2 99.8 / 78.3 99.5 / 77.3 96.1 43.5 99.1 / 70.5 99.9 / 71.9 94.5 / 71.2 98.0 97.2 62.5 99.6 / 80.4 99.5 / 79.5 99.5 / 79.9 97.6 97.0 97.4 28.0 53.4 57.8 52.1 64.8 Table 3: Out-of-domain evaluation of different monolingual word alignment models on the MultiMWA benchmark. All the models in this table are trained on the MultiMWA-MTRefSure+P oss dataset. from every token in the source sentence to a span in the target sentence, which is then solved by finetuning multilingual BERT (Devlin et al., 2019) similarly as for SQuAD-style question answering task. Taking the sentence pair in Figure 1 as an example, the word to be aligned is marked by ¶ in the source sentence and concatenated with the entire target sentence to form the input as “With Canadian · · · ¶conduct¶ · · · his model. Lkoyd performed · · · his model. ”. A span prediction model based on fine-tuning multilingual BERT is then expected to extract performed from the target sentence. The predictions from both directions (source to target, target to source) are symmetrized to produce the final alignment, using a probability threshold"
2021.acl-long.531,I05-5002,0,0.183085,"n extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as effectively for small data size. Conclusion In this work, we present the first neural semiCRF"
2021.acl-long.531,2021.eacl-main.33,0,0.0334442,"Missing"
2021.acl-long.531,P19-1331,0,0.031939,"Missing"
2021.acl-long.531,N13-1073,0,0.0464415,"detailed ablation and error analysis to better understand the performance gains. Finally, we demonstrate the utility of monolingual word alignment in two downstream applications, namely automatic text simplification and sentence pair classification. 2 Related Work Word alignment has a long history and was first proposed for statistical machine translation. The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuri"
2021.acl-long.531,P11-1042,0,0.0149539,"nd error analysis to better understand the performance gains. Finally, we demonstrate the utility of monolingual word alignment in two downstream applications, namely automatic text simplification and sentence pair classification. 2 Related Work Word alignment has a long history and was first proposed for statistical machine translation. The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on"
2021.acl-long.531,W18-2501,0,0.0303013,"Missing"
2021.acl-long.531,2020.acl-main.709,1,0.748384,"and non-identical (e.g., conduct ↔ performed) alignments. Externel indicates whether the annotation relies on additional linguistic information. † ESPADA (train) has not been released at the time of writing; statistics are based on the SPADE (dev/test) dataset. ∗ Newsela data is free for academic research but license needs to be requested at: https://newsela.com/data. professional editors. It has been widely used in text simplification research (Xu et al., 2016; Zhang and Lapata, 2017; Zhong et al., 2020). We randomly select 500 complex-simple sentence pairs from the test set of Newsela-Auto (Jiang et al., 2020),5 which is the newest sentencealigned version of Newsela. 214 of these 500 pairs contain sentence splitting. An in-house annotator6 labels the word alignment by correcting the outputs from GIZA++ (Och and Ney, 2003). MultiMWA-arXiv. The arXiv7 is an openaccess platform that stores more than 1.7 million research papers with their historical versions. It has been used to study paraphrase generation (Dong et al., 2021) and statement strength (Tan and Lee, 2014). We first download the LATEX source code for 750 randomly sampled papers and their historical versions, then use OpenDetex8 package to e"
2021.acl-long.531,2020.tacl-1.5,0,0.0219508,"irst two compose negative log-likelihood loss: the span interaction function υ, which accounts the similarity between a source span and a target span; the Markov transition function τ , which models the transition of alignment labels between adjacent source spans; the cost is implemented with Hamming loss to encourage the predicted alignment sequence to be consistent with gold labels. Function υ and τ are implemented as two neural components which we describe below. Span Representation Layer. First, source and target sentences are concatenated together and encoded by the pre-trained SpanBERT (Joshi et al., 2020) model. The hidden representations in the last layer of the encoder are extracted for each WordPiece token, then averaged to form the word representations. Following previous work (Joshi et al., 2020), the span is represented by a selfattention vector computed over the representations of each word within the span, concatenated with the Transformer output states of two endpoints. Span Interaction Layer. The semantic similarity score between source span si and target span tj is calculated by a 2-layer feed-forward neural network FFsim with Parametric Relu (PReLU) (He et al., 2015),2 after applyi"
2021.acl-long.531,2021.naacl-main.277,1,0.751534,"ls (§6.1), such as for simplifying complex sentences for children to read. Introduction Monolingual word alignment aims to align words or phrases with similar meaning in two sentences that are written in the same language. It is useful for improving the interpretability in natural language understanding tasks, including semantic textual similarity (Li and Srikumar, 2016) and question answering (Yao, 2014). Monolingual word alignment can also support the analysis of human editing operations (Figure 1) and improve model performance for text-to-text generation tasks, such as text simplification (Maddela et al., 2021) and neutralizing biased language (Pryzant et al., 2020). It has also been shown to be helpful for data augmentation and label projection 1 Our code and data will be available at: https:// github.com/chaojiang06/neural-Jacana F Authors contributed equally. (Culkin et al., 2021) when combined with paraphrase generation. One major challenge for automatic alignment is the need to handle not only alignments between words and linguistic phrases (e.g., a dozen ↔ more than 10), but also non-linguistic phrases that are semantically related given the context (e.g., tensions ↔ relations being strained i"
2021.acl-long.531,marelli-etal-2014-sick,0,0.0379471,"ilarity (STS-B, STS14) tasks. The datasets in this table are ordered by the size of their training set, as shown in the second row. 6.2 7 Sentence Pair Modeling We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and P"
2021.acl-long.531,2020.emnlp-main.41,0,0.291426,"match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine translation and word alignment, while Zenkel et al. (2020) designed an alignment layer on top of Transformer for machine translation. Both can be trained without word alignment annotations but rely on millions of bilingual sentence pairs. As for supervised methods, Stengel-Eskin et al. (2019) extracted representations from the Transformerbased MT system, then used convolutional neural network to incorporate neighboring words for alignment. Nagata et al. (2020) proposed a span prediction method and formulated bilingual word alignment as a SQuAD-style question answering task, then solved it by fine-tuning multilingual BERT. We adapt their method to monolingual word alignment as a new state-of-the-art baseline (§5.1). Some monolingual neural models have different settings from this work. Ouyang and McKeown (2019) introduced pointer networks for long, sentence- or clause-level alignments. Arase and Tsujii (2017, 2020) utilized constituency parsers for compositional and non-compositional phrase alignments. Culkin et al. (2021) considered span alignment"
2021.acl-long.531,J03-1002,0,0.0886694,"od generalizability on three out-of-domain datasets. We present a detailed ablation and error analysis to better understand the performance gains. Finally, we demonstrate the utility of monolingual word alignment in two downstream applications, namely automatic text simplification and sentence pair classification. 2 Related Work Word alignment has a long history and was first proposed for statistical machine translation. The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al.,"
2021.acl-long.531,P19-1467,0,0.0177306,"y on millions of bilingual sentence pairs. As for supervised methods, Stengel-Eskin et al. (2019) extracted representations from the Transformerbased MT system, then used convolutional neural network to incorporate neighboring words for alignment. Nagata et al. (2020) proposed a span prediction method and formulated bilingual word alignment as a SQuAD-style question answering task, then solved it by fine-tuning multilingual BERT. We adapt their method to monolingual word alignment as a new state-of-the-art baseline (§5.1). Some monolingual neural models have different settings from this work. Ouyang and McKeown (2019) introduced pointer networks for long, sentence- or clause-level alignments. Arase and Tsujii (2017, 2020) utilized constituency parsers for compositional and non-compositional phrase alignments. Culkin et al. (2021) considered span alignment for FrameNet (Baker et al., 1998) annotations and treated each span pair as independent prediction. 3 Neural Semi-CRF Alignment Model In this section, we first describe the problem formulation for monolingual word alignment, then present the architecture of our neural semi-CRF word alignment model (Figure 2). 3.1 Problem Formulation We formulate word alig"
2021.acl-long.531,P09-5002,0,0.0288656,"ransformed into a real-value score by a 1-layer feed forward neural network. Training and Inference. During training, we minimizes the negative log-likelihood of the gold alignment a∗ , and the model is trained from both directions (source to target, target to source): P (s,t,a∗ ) −log p(a∗s2t |s, t) − log p(a∗t2s |t, s) (4) where a∗s2t and a∗t2s represent the gold alignment labels from both directions. During inference, we use the Viterbi algorithm to find the optimal alignment. There are different strategies to merge the outputs from two directions, including intersection, union, grow-diag (Koehn, 2009), bidi-avg (Nagata et al., 2020), etc. It can be seen as a hyper-parameter and decided based on the dev set. In this work, we use intersection in our semi-CRF model for all experiments. 3.3 Implementation Details We implement our model in PyTorch (Paszke et al., 2017). We use the Adam optimizer and set both the learning rate and weight decay as 1e-5. We set the maximum span size to 3 for our neural semi-CRF model, which can converge within 5 epochs. The neural semi-CRF model has ∼2 hour training time per epoch for MultiMWA-MTRef, measured on a single GeForce GTX 1080 Ti GPU. 4 A Multi-Genre Be"
2021.acl-long.531,D17-1126,1,0.854639,"it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as effectively for small data size. Conclusion In this work, we present the first neural semiCRF word alignment model whi"
2021.acl-long.531,C18-1328,1,0.854603,"8/83.0 86.2 75.0 78.7 84.4/89.6 90.8 84.8/83.1 BERT + Aligner 67.3 88.9 86.8/86.0 83.7 83.2/84.4 87.2 75.5 78.5 85.1/87.8 90.9 84.8/83.5 Models SNLI 549k Acc 90.5 90.4 Table 6: Downstream applications on natural language inference (RTE, SICK, MNLI, SNLI), paraphrase identification (MRPC, PIT, URL, QQP), question answering (WikiQA, TrecQA), and semantic textual similarity (STS-B, STS14) tasks. The datasets in this table are ordered by the size of their training set, as shown in the second row. 6.2 7 Sentence Pair Modeling We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA"
2021.acl-long.531,D16-1237,0,0.0220573,"tasks.1 1 Lloyd performed successful laboratory experiments of his model. Insertion Figure 1: An example that illustrates monolingual word alignment (shown as arrows) can support analysis of human editing process and training of text generation models (§6.1), such as for simplifying complex sentences for children to read. Introduction Monolingual word alignment aims to align words or phrases with similar meaning in two sentences that are written in the same language. It is useful for improving the interpretability in natural language understanding tasks, including semantic textual similarity (Li and Srikumar, 2016) and question answering (Yao, 2014). Monolingual word alignment can also support the analysis of human editing operations (Figure 1) and improve model performance for text-to-text generation tasks, such as text simplification (Maddela et al., 2021) and neutralizing biased language (Pryzant et al., 2020). It has also been shown to be helpful for data augmentation and label projection 1 Our code and data will be available at: https:// github.com/chaojiang06/neural-Jacana F Authors contributed equally. (Culkin et al., 2021) when combined with paraphrase generation. One major challenge for automat"
2021.acl-long.531,D08-1084,0,0.061937,"or statistical machine translation. The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2"
2021.acl-long.531,Q14-1018,0,0.0498708,"re works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine translation and word alignment, while Zenkel et al. (2020) designed an alignment layer on top of Transformer for machine translation. Both can be trained without word alignment annotatio"
2021.acl-long.531,P14-1138,0,0.0241983,"ncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine translation and word alignment, while Zenkel et al. (2020) designed an alignment layer on top of Transformer for machine translation. Both can be trained without word alignment annotations but rely on millions of bilingual sentence pairs. As for supervised methods, Stengel-Eskin et al. (2019) extracted representations from the Transformerbased MT system, then used convolutional neural network to incorporate neighboring words for ali"
2021.acl-long.531,P14-2066,0,0.0242832,"Missing"
2021.acl-long.531,C12-2120,0,0.178625,"ntative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine t"
2021.acl-long.531,P11-2044,0,0.0364035,"anslation. The most representative ones are the IBM models(Brown et al., 1993), which are a sequence of unsupervised models with increased complexity and implemented the GIZA++ toolkit (Och and Ney, 2003). Many more works followed, such as FastAlign (Dyer et al., 2013). Dyer et al. (2011) also used a globally normalized log-linear model for discriminative word alignment. Bansal et al. (2011) proposed a hidden semi-Markov model to handle both continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task"
2021.acl-long.531,W18-5446,0,0.0394285,"Missing"
2021.acl-long.531,D07-1003,0,0.0890539,"adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as effectively for small data size."
2021.acl-long.531,N18-1101,0,0.0152592,"cQA), and semantic textual similarity (STS-B, STS14) tasks. The datasets in this table are ordered by the size of their training set, as shown in the second row. 6.2 7 Sentence Pair Modeling We can utilize our neural aligner in sentence pair classification tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, ST"
2021.acl-long.531,P13-1017,0,0.0284393,"h continuous and noncontinuous phrase alignment. These statistical methods promoted the development of monolingual word alignment (MacCartney et al., 2008; Thadani and McKeown, 2011; Thadani et al., 2012). Yao et al. (2013a) proposed a CRF aligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine translation and word alignment, while Zenkel et al. (2020) designed an alignment layer on top of Transformer for machine translation. Both can be trained without word alignment annotations but rely on millions of bilingual sentence pairs. As for supervised methods, Stengel-Eskin et al. (2019) extracted representations from the Transformerbased MT system, then used convolutional neural network to incorporate nei"
2021.acl-long.531,D15-1237,0,0.0289479,"tion tasks (Lan and Xu, 2018), adding conditional alignment probability p(a|s, t) as an extra feature. We concatenate it with the [CLS] representation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as e"
2021.acl-long.531,P13-2123,0,0.0354982,"Missing"
2021.acl-long.531,D13-1056,0,0.0532014,"Missing"
2021.acl-long.531,2020.acl-main.146,0,0.018613,"ligner following (Blunsom and Cohn, 2006), then extended it to a semiCRF model for phrase-level alignments (Yao et al., 2013b). Sultan et al. (2014) designed a simple system with heuristic rules based on word similarity and contextual evidence. Neural methods have been explored in the past decade primarily for bilingual word alignment. Some early attempts (Yang et al., 2013; Tamura et al., 2014) did not match the performance of GIZA++, but recent Transformer-based models started to outperform. Garg et al. (2019) proposed a multi-task framework for machine translation and word alignment, while Zenkel et al. (2020) designed an alignment layer on top of Transformer for machine translation. Both can be trained without word alignment annotations but rely on millions of bilingual sentence pairs. As for supervised methods, Stengel-Eskin et al. (2019) extracted representations from the Transformerbased MT system, then used convolutional neural network to incorporate neighboring words for alignment. Nagata et al. (2020) proposed a span prediction method and formulated bilingual word alignment as a SQuAD-style question answering task, then solved it by fine-tuning multilingual BERT. We adapt their method to mon"
2021.acl-long.531,D17-1062,0,0.0636673,"Missing"
2021.acl-long.531,S15-2001,1,0.791846,"entation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as effectively for small data size. Conclusion In this work, we present the first neural semiCRF word alignment model which achieves competitiv"
2021.acl-long.531,Q15-1021,1,0.812744,"entation in fine-tuned BERT and apply the softmax layer for prediction. We experiment with on different datsets for various tasks, including: natural language inference on SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), SICK (Marelli et al., 2014), and RTE (Giampiccolo et al., 2007) from the GLUE benchmark (Wang et al., 2018); semantic textual similarity on STS-B (Cer et al., 2017) and STS14 (Agirre et al., 2014); question answering on WikiQA (Yang et al., 2015) and TrecQA (Wang et al., 2007); paraphrase identification on MRPC (Dolan and Brockett, 2005), URL (Lan et al., 2017), PIT (Xu et al., 2015a), and QQP (Iyer et al., 2017). We implement the fine-tuned BERTbase model using Huggingface’s library (Wolf et al., 2019). Table 6 shows performance improvement on small (2k-15k) datasets, which include SICK, STS-B, MRPC, RTE, WikiQA, and PIT, but little or no improvement on large (40k-550k) datasets, such as SNLI, MNLI, and QQP. We hypothesize that the Transformer model can potentially learn the latent word alignment through self-attentions, but not as effectively for small data size. Conclusion In this work, we present the first neural semiCRF word alignment model which achieves competitiv"
2021.emnlp-main.409,D19-1371,0,0.314419,"f model. 5 https://github.com/nlplab/brat The average time to annotate each sentence varies across datasets based on the complexity of the text, average length of sentences, etc. P ROCEDURE Corpus Collection. To pre-train our models, we create a novel collection of procedural texts from the same domains as the annotated data in §3, hereinafter referred to as the P ROCE DURE corpus. Specially trained classifiers were used to identify paragraphs describing experimental procedures. For PubMed, a classifier was used to identify paragraphs describing experimental procedures by fine-tuning SciBERT (Beltagy et al., 2019) on the SciSeg dataset (Dasigi et al., 2017), which is annotated with scientific discourse structure, to extract procedures from the Materials and Methods section of 680k articles. For the chemical synthesis domain, the chemical reaction extractor developed by Lowe (2012) was applied to the Description section of 303k patents (174k U.S. and 129k European) we collected from USPTO7 and EPO8 . More details of our data collection process can be found in Appendix B. Cooking recipes are also an important domain for research on procedural text understanding, therefore we include the text component of"
2021.emnlp-main.409,W06-1615,0,0.226306,"urrett (2019). The state of an ingredient in each cooking step is correct if it matches with the gold labels, as either present or absent. Results. Test set results of eight pre-trained language models on six procedural text datasets are presented in Table 7.20 ProcBERT, performs best in most tasks and even achieves the state-off-the-art performance on operational argument role labeling (""Core"" and ""Non-Core"") of XWLP, showing the effectiveness of in-domain pre-training. 7 Conclusion In this paper, we address a number of questions related to the costs of adapting an NLP model to a new domain (Blitzer et al., 2006; Han and Eisenstein, 2019), an important and well-studied problem in NLP. We frame domain adaptation under a constrained budget as a problem of consumer choice. Experiments are conducted using several pre-trained models in three procedural text domains to determine when it is economical to pre-train indomain transformers (Gururangan et al., 2020), and when it is better to spend available resources on annotation. Our results suggest that when a small number of NLP models need to be adapted to a new domain, pre-training, by itself, is not an economical solution. For C H EMU, gold arguments are"
2021.emnlp-main.409,2020.acl-main.194,0,0.0155624,"TPUs to pre-train learning and unsupervised domain adaptation is large in-domain language models. In this paper, we that labeled data is expensive; therefore, train- empirically study the best strategy for adapting to ing on a combination of labeled and unlabeled a new domain given a fixed budget. data is an economical approach to improve perWe view the NLP practitioner’s dilemma of how formance when adapting to a new domain (Blum to adapt to a new domain as a problem of consumer and Mitchell, 1998; Daume III and Marcu, 2006; choice, a classical problem in microeconomics Hoffman et al., 2018; Chen et al., 2020). Recent (Becker, 1965; Lancaster, 1966). As illustrated in work has shown that pre-training in-domain Trans- Figure 1, the NLP practitioner (consumer) can obformers is an effective method for unsupervised tain Xa annotated documents (by hiring annotators) adaptation (Han and Eisenstein, 2019; Wright and at a cost of Ca each, and Xp hours of pre-training Augenstein, 2020) and even boosts performance (by renting GPUs or TPUs) at a cost of Cp per hour. 1 Given a fixed budget B, the consumer may choose Our code and data are publicly available on Github: https://github.com/bflashcp3f/ProcBERT. any"
2021.emnlp-main.409,P07-1033,0,0.3889,"Missing"
2021.emnlp-main.409,N19-1423,0,0.540544,"producible scientific experiments, yet few annotated datasets currently exist in these domains. Furthermore, annotation of scientific procedures is not easily amenable to crowdsourcing, making this an ideal testbed for pretraining-based domain adaptation. We measure the cost of in-domain pre-training on a large collection of unlabeled procedural texts using Google’s Cloud TPUs.2 Model performance is then evaluated under varying budget constraints in six source and target domain combinations. Our analysis suggests that given current costs of pre-training large Transformer models, such as BERT (Devlin et al., 2019), and RoBERTa (Liu et al., 2019), in-domain data annotation should always be part of an economical strategy when adapting a single NLP system to a new domain. For small budgets (e.g. less than $800 USD), spending all funds on annotation is the best policy; however, as more funding becomes available, a combination of pre-training and annotation is the best choice. This paper addresses a specific question that is often faced by NLP practitioners working on applications: what is the most economical approach to adapt an NLP system to a new domain when no pre-trained models or task-annotated datase"
2021.emnlp-main.409,D19-1070,0,0.012125,"410.1 84.540.8 81.750.4 82.920.3 WLP NER RE BERTbase BERTlarge RoBERTabase RoBERTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including"
2021.emnlp-main.409,2020.acl-main.740,0,0.0420387,"Missing"
2021.emnlp-main.409,D19-1433,0,0.121017,"xed budget. data is an economical approach to improve perWe view the NLP practitioner’s dilemma of how formance when adapting to a new domain (Blum to adapt to a new domain as a problem of consumer and Mitchell, 1998; Daume III and Marcu, 2006; choice, a classical problem in microeconomics Hoffman et al., 2018; Chen et al., 2020). Recent (Becker, 1965; Lancaster, 1966). As illustrated in work has shown that pre-training in-domain Trans- Figure 1, the NLP practitioner (consumer) can obformers is an effective method for unsupervised tain Xa annotated documents (by hiring annotators) adaptation (Han and Eisenstein, 2019; Wright and at a cost of Ca each, and Xp hours of pre-training Augenstein, 2020) and even boosts performance (by renting GPUs or TPUs) at a cost of Cp per hour. 1 Given a fixed budget B, the consumer may choose Our code and data are publicly available on Github: https://github.com/bflashcp3f/ProcBERT. any combination that fits within the budget con5002 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5002–5015 c November 7–11, 2021. 2021 Association for Computational Linguistics straint Xa Ca + Xp CP ≤ B. The goal is to choose a combination that ma"
2021.emnlp-main.409,2021.emnlp-main.831,0,0.0327949,"IsnL0O7WnEvK9Xbi3KtnsdRBMfgBJwBF1yBGrgBTdACGDyCZ/AK3qwn68V6tz7mowUr3zkEf2R9/gAacpTB</latexit&gt; Number of Pretraining Hours (TPUs) Figure 1: We view domain adaptation as a consumer choice problem (Becker, 1965; Lancaster, 1966). The NLP practitioner (consumer) is faced with the problem of choosing an optimal combination of annotation and pre-training under a constrained budget. This figure is purely for illustration and is not based on experimental data. when large quantities of in-domain data are available (Gururangan et al., 2020). However, modern pre-training methods incur substantial costs (Izsak et al., 2021), and generate carbon emissions (Strubell et al., 2019; Schwartz et al., 2020; Bender et al., 2021). This raises an important question: given a fixed budget to improve a model’s performance, what steps should an NLP practitioner take? On one hand, they could hire annotators to label 1 Introduction in-domain task-specific data, while on the other, The conventional wisdom on semi-supervised they could buy or rent GPUs or TPUs to pre-train learning and unsupervised domain adaptation is large in-domain language models. In this paper, we that labeled data is expensive; therefore, train- empirically"
2021.emnlp-main.409,D16-1032,0,0.0151893,"an benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including trigger detection and argument role labeling. The XWLP corpus (Tamari et al., 2021) provides the Process Event Graphs (PEG) of 279 wet-lab biochemistry protocols. The PEG is a document-level graph-based representation specifying the involved experimental objects. The R ECIPE corpus (Kiddon et al., 2016) includes annotation of entity states for 866 cooking recipes. It supports Entity Tracking (ET) task which predicts whether or not a specific ingredient is involved in each step of the recipe. 6.2 Experiments on Ancillary Tasks chitecture of Gupta and Durrett (2019). The state of an ingredient in each cooking step is correct if it matches with the gold labels, as either present or absent. Results. Test set results of eight pre-trained language models on six procedural text datasets are presented in Table 7.20 ProcBERT, performs best in most tasks and even achieves the state-off-the-art perform"
2021.emnlp-main.409,C16-1038,0,0.0184099,"ts the target, and the third is domain-independent. These contextualized vectors are then concatenated and fed into a linear layer that is 3 times as large as the base model’s. When encoding data from a specific domain (e.g. C HEM S YN), the other domain’s representations are zeroed out (1/3 of the new representations will always be 0.0). This enables the domain-specific block of the linear layer to encode information specific to that domain, while the domain-independent parameters can learn to represent information that transfers across domains. This is similar to prior work using EasyAdapt (Kim et al., 2016) for LSTMs. As we have three procedural text datasets (§3) annotated with entities and relations, we can experiment with six source ⇒ target adaptation settings. For each domain pair, we compare five different 5.4 Experimental Results and Analysis pre-trained language models when adapted to the procedural text domain under varying budgets. We present the test set NER and RE results with five Based on the estimations of the annotation costs annotation and pre-training combination strategies Ca (§3) and pre-training costs Cp (§4), we con- under six domain adaptation settings in Table 4 and duct"
2021.emnlp-main.409,2020.wnut-1.40,0,0.0153524,"ER RE BERTbase BERTlarge RoBERTabase RoBERTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including trigger detection and argument ro"
2021.emnlp-main.409,P19-1355,0,0.124699,"qwn68V6tz7mowUr3zkEf2R9/gAacpTB</latexit&gt; Number of Pretraining Hours (TPUs) Figure 1: We view domain adaptation as a consumer choice problem (Becker, 1965; Lancaster, 1966). The NLP practitioner (consumer) is faced with the problem of choosing an optimal combination of annotation and pre-training under a constrained budget. This figure is purely for illustration and is not based on experimental data. when large quantities of in-domain data are available (Gururangan et al., 2020). However, modern pre-training methods incur substantial costs (Izsak et al., 2021), and generate carbon emissions (Strubell et al., 2019; Schwartz et al., 2020; Bender et al., 2021). This raises an important question: given a fixed budget to improve a model’s performance, what steps should an NLP practitioner take? On one hand, they could hire annotators to label 1 Introduction in-domain task-specific data, while on the other, The conventional wisdom on semi-supervised they could buy or rent GPUs or TPUs to pre-train learning and unsupervised domain adaptation is large in-domain language models. In this paper, we that labeled data is expensive; therefore, train- empirically study the best strategy for adapting to ing on a comb"
2021.emnlp-main.409,2020.wnut-1.33,1,0.80909,"Missing"
2021.emnlp-main.409,2021.eacl-main.187,1,0.774105,"75.040.8 73.771.6 75.480.7 74.890.6 95.70 80.620.7 81.530.5 83.410.1 84.540.8 81.750.4 82.920.3 WLP NER RE BERTbase BERTlarge RoBERTabase RoBERTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extrac"
2021.emnlp-main.409,2020.emnlp-main.639,0,0.04382,"Missing"
2021.emnlp-main.409,D19-1005,0,0.0220881,"Missing"
2021.emnlp-main.409,2020.wnut-1.38,0,0.0210938,"RTalarge SciBERT BioMed-RoBERTa SOTA 91.930.6 92.100.9 92.541.2 92.101.1 91.800.5 92.420.6 R ECIPE ET – 95.090.2 95.260.1 95.300.2 95.660.2 95.820.2 95.380.2 – 92.630.2 92.870.5 93.390.3 92.870.2 93.270.2 93.160.3 – Table 7: Test set F1 on six procedural text datasets. The best task performance is boldfaced, and the second-best performance is underlined. For the SOTA model of each dataset, we refer readers to the corresponding paper for further details: Tamari et al. (2021) for XWLP, Wang et al. (2020) for C H EMU, Gupta and Durrett (2019) for R ECIPE, Knafou et al. (2020) for NER on WLP, and Sohrab et al. (2020) for RE on WLP. 6.1 Ancillary Procedural NLP Tasks In addition to the procedural text datasets discussed in §5, we experiment with three ancillary procedural text corpora, to explore how in-domain pretraining can benefit other tasks. The C H EMU corpus (Nguyen et al., 2020) contains NER and event annotations for 1500 chemical reaction snippets collected from 170 English patents. Its NER task focuses on identifying chemical compounds, and its event extraction (EE) task aims at detecting chemical reaction events including trigger detection and argument role labeling. The XWLP corpus (Tamari et a"
2021.emnlp-main.500,P17-1099,0,0.0505589,"I S PLIT for evaluation, because this corpus was constructed explicitly to be used only as training data, as it contains inherent noise and biases. While B I SECT contains 928,440/9,079 train and dev pairs, W IKI S PLIT contains 989,944/5,000 train and dev pairs. Note that we constructed B I SECT test set by manually selecting 583 high-quality sentence splits from 1000 random source-target pairs from EMEA and JRCACQUIS corpora. We compare our approach with Copy512 (Aharoni and Goldberg, 2018), a state-of-the-art model consisting of an attention-based LSTM encoderdecoder with a copy mechanism (See et al., 2017). We use our base model trained on W IKI S PLIT (Rothe et al., 2020) as another state-of-the-art baseline. 5.2 Automatic Evaluation Existing automatic metrics, such as BLEU (Papineni et al., 2002) and SAMSA (Sulem et al., 2018), yˆi =(1 − δi )xi + δi yi are not optimal for the Split and Rephrase task as ( they rely on lexical overlap between the output and 0, if xi is copied δi = the target (or source) and underestimate the split1, otherwise ting capability of the models that rephrase often. where m is the number of training examples and We focus on BERTScore (Zhang et al., 2020b) and yˆ&lt;i rep"
2021.emnlp-main.500,D18-1081,0,0.028008,"m the classifier and the Transformer using the cross entropy loss and our custom split-focused loss. We provide model and training details in Appendix A. 5 Experiments and Results In this section, we compare different split and rephrase models trained on our new B I SECT corpus. We also conduct a carefully designed human evaluation as automatic metrics are not totally reliable. Our model trained on B I SECT establishes a new start-of-the-art for the task. 5.1 Data and Baselines We train the models on B I SECT and W IKI S PLIT corpora. For evaluation, we select the B I SECT and HS PLIT-W IKI (Sulem et al., 2018) test sets to represent splitting with a high degree and minimal of rephrasing respectively. HS PLIT-W IKI is a human annotated dataset with 359 complex sentences and 4 references for each complex sentence. Following previous work (Botha et al., 2018; Zhang et al., 2020a), we do not use W IKI S PLIT for evaluation, because this corpus was constructed explicitly to be used only as training data, as it contains inherent noise and biases. While B I SECT contains 928,440/9,079 train and dev pairs, W IKI S PLIT contains 989,944/5,000 train and dev pairs. Note that we constructed B I SECT test set b"
2021.emnlp-main.500,tiedemann-nygaard-2004-opus,0,0.14168,"lit and Rephrase data that is both meaning preserving and sufficient in size dent, 2014). However, the structural paraphrasing required to split a sentence makes for an interest- for training, we present the B I SECT corpus. ing problem in itself, with many downstream NLP 3.1 Corpus Creation Procedure applications. Thus, Narayan et al. (2017) proposed the Split and Rephrase task, and introduced the The construction of the B I SECT corpus relies W EB S PLIT corpus, created by aligning sentences on leveraging the sentence-level alignments from in WebNLG (Gardent et al., 2017). W EB S PLIT OPUS (Tiedemann and Nygaard, 2004), a publicly contains duplicate instances and phrasal repetitions available collection of bilingual parallel corpora (Aharoni and Goldberg, 2018; Botha et al., 2018), over many language pairs. While most of the transand most splitting operations can be trivially classi- lated sentences in OPUS are aligned 1-1, i.e., one fied (Zhang et al., 2020a), so subsequent Split and sentence in Language A is mapped to one sentence Rephrase corpora have been created to improve in Language B, there are many aligned pairs contraining (Botha et al., 2018) and evaluation (Sulem sisting of multiple sentences fr"
2021.emnlp-main.500,P18-1042,0,0.153258,"editors are not only trying into shorter sentences. This task is referred to as to split a sentence, but also often simultaneously Split and Rephrase (Narayan et al., 2017). modifying the sentence for other purposes, which Several past efforts have created Split and Rephrase training sets, which consist of long, com- results in changes of the initial meaning. In this paper, we introduce a novel methodology plex input sentences paired with multiple shorter for creating Split and Rephrase corpora via bilin∗ Equal contribution. 1 Our code and data are available at https://github. gual pivoting (Wieting and Gimpel, 2018; Hu et al., com/mounicam/BiSECT. 2019b). Figure 1 demonstrates the process. First, 6193 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6193–6209 c November 7–11, 2021. 2021 Association for Computational Linguistics we extract all 1-2 and 2-1 sentence-level alignments (Gale and Church, 1993) from bilingual parallel corpora, where a single sentence in one language aligns to two sentences in the other language. We then machine translate the foreign sentences into English. The result is our B I SECT corpus. Split and Rephrase corpora, including B I S"
2021.emnlp-main.500,Q15-1021,1,0.877802,"Missing"
2021.findings-acl.153,2020.emnlp-demos.22,0,0.0410419,"ould achieve higher scores on these metrics. We use the “Filtered” setting for all the evaluations, which filters out other true answers from the prediction results to get the final rank for each test case. 5.4 Hyperparameter Settings According to Ruffinelli et al. (2020), performances of KGE methods are sensitive to hyperparameters. Following them, we run 30 quasi-random trails for all models from predefined hyperparameter spaces. We list the hyperparameter spaces we use in Appendix A.5. We run all trails for 100 epochs. For all single-view KE methods, we use the implementations from LibKGE (Broscheit et al., 2020), which utilizes the Ax framework to perform quasi-random hyperparameter search. For AttH, we use the implementation from the authors1 . For JOIE, we use the implementation from the authors2 . We use TransE as the backend and adopt the suggested hyperparameter space from the paper. 6 Experimental Results In this section, we provide the experimental results and further propose several future directions. 6.1 Knowledge Abstraction The results of knowledge abstraction are shown in Table 4. From the results, we can see that AttH has 1 2 https://github.com/HazyResearch/KGEmb https://github.com/Junhe"
2021.findings-acl.153,2020.acl-main.617,0,0.0496769,"Missing"
2021.findings-acl.153,P15-1067,0,0.0372355,") and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations. Thirdly, some datasets provide the full concept graphs (Hao et al., 2019), but both the scale and the depth of the concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type informat"
2021.findings-acl.153,D18-1222,1,0.91582,"EC-KG is shown in Figure 1. During the last decade, there are massive works focusing on learning representations for KGs such as TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and TuckER (Balaˇzevi´c et al., 2019). Though they have achieved promising results on knowledge graph completion, most of them focus on a single graph, especially the entity graph. Beyond modeling a single graph of KGs, recent studies demonstrate that jointly modeling the two graphs in the EC-KG can improve the understanding of each one (Xie et al., 2016; Moon et al., 2017; Lv et al., 2018; Hao et al., 2019). They also propose 1751 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1751–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics several tasks on the EC-KG, such as link prediction and entity typing. These tasks focus on partial aspects of knowledge abstraction, concretization, and completion, which are essential abilities for humans to recognize the world and acquire knowledge. For example, in entity typing, a model may link the entity “Da Vinci” to the concept “painter” which reflects the model’s abstraction ability. Ho"
2021.findings-acl.153,N18-2053,0,0.0200075,"concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type information into KE methods to help the completion of entity graphs. ETE (Moon 1752 et al., 2017) further conducts entity typing, which can be seen as a simplified version of our knowledge abstraction task. Though types of entities can be"
2021.findings-acl.153,2020.emnlp-main.669,0,0.0266774,"on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations."
2021.findings-acl.153,D15-1174,0,0.541521,"archies perform better than general KGE models on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide"
2021.findings-acl.405,W06-0901,0,0.136354,"the event argument, plays a specific event role. Multiple event instances of the same event type populate an event table. Introduction Event extraction (EE) is a vital task that aims to harvest structured event instances from unstructured plain text. Such structured knowledge can benefit many downstream applications, such as question answering, language understanding, knowledge graph, etc. In general, an event instance is composed of a group of entities (person, organization, date, etc.) that jointly describes an incident. Each entity of the event instance, also referred to The early method (Ahn, 2006) formalized EE as the unification of many sub-tasks, including entity recognition, event detection, and argument extraction, etc. Later research improved EE from two aspects: the modeling to capture complicated semantic structures (Li et al., 2013; Yang and Mitchell, 2016; Nguyen and Nguyen, 2019) and the labeling to combat the lack of training data (Chen et al., 2017; Zeng et al., 2018). Recently, Zheng et al. (2019a) proposed the first end-to-end model, called Doc2EDAG, for document-level EE. Given a text document, Doc2EDAG can generate an entitybased directed acyclic graph (EDAG) to fill an"
2021.findings-acl.405,P17-1038,0,0.0690865,"tanding, knowledge graph, etc. In general, an event instance is composed of a group of entities (person, organization, date, etc.) that jointly describes an incident. Each entity of the event instance, also referred to The early method (Ahn, 2006) formalized EE as the unification of many sub-tasks, including entity recognition, event detection, and argument extraction, etc. Later research improved EE from two aspects: the modeling to capture complicated semantic structures (Li et al., 2013; Yang and Mitchell, 2016; Nguyen and Nguyen, 2019) and the labeling to combat the lack of training data (Chen et al., 2017; Zeng et al., 2018). Recently, Zheng et al. (2019a) proposed the first end-to-end model, called Doc2EDAG, for document-level EE. Given a text document, Doc2EDAG can generate an entitybased directed acyclic graph (EDAG) to fill an event table directly. Different from the modeling and labeling aspects, the evaluation of EE attracted very little re4609 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4609–4617 August 1–6, 2021. ©2021 Association for Computational Linguistics search attention. Notably, most existing research, merely presenting a separate evaluatio"
2021.findings-acl.405,D18-1158,0,0.0279785,"Missing"
2021.findings-acl.405,C16-1215,0,0.0169054,"elated Work Since EE is a very sophisticated task that requires the unification of many sub-tasks (Ahn, 2006), including entity recognition, event detection, and argument extraction, plenty of previous research put considerable efforts to the modeling aspect. Specifically, Nguyen and Grishman (2015); Liu et al. (2017); Chen et al. (2018); Wang et al. (2019); Liu et al. (2019) only considered event detection that is to detect trigger words and assign correct event types. Some advanced methods (Poon and Vanderwende, 2010; Riedel and McCallum, 2011; Li et al., 2013, 2014; Venugopal et al., 2014; Judea and Strube, 2016; Nguyen et al., 2016; Sha et al., 2018) tried to unify two sub-tasks, event detection and argument extraction, but all assumed that entity candidates were given in advance. A few studies attempted to fulfill all sub-tasks of EE jointly. Yang and Mitchell (2016) was the first work towards this goal but relied on handcrafted features. Later research (Nguyen and Nguyen, 2019) explored the joint modeling further by introducing neural networks but also retained many traditional lexical and syntactic features. Recently, Zheng et al. (2019a) formalized a new succinct task for EE without trigger word"
2021.findings-acl.405,P13-1008,0,0.0250119,"ain text. Such structured knowledge can benefit many downstream applications, such as question answering, language understanding, knowledge graph, etc. In general, an event instance is composed of a group of entities (person, organization, date, etc.) that jointly describes an incident. Each entity of the event instance, also referred to The early method (Ahn, 2006) formalized EE as the unification of many sub-tasks, including entity recognition, event detection, and argument extraction, etc. Later research improved EE from two aspects: the modeling to capture complicated semantic structures (Li et al., 2013; Yang and Mitchell, 2016; Nguyen and Nguyen, 2019) and the labeling to combat the lack of training data (Chen et al., 2017; Zeng et al., 2018). Recently, Zheng et al. (2019a) proposed the first end-to-end model, called Doc2EDAG, for document-level EE. Given a text document, Doc2EDAG can generate an entitybased directed acyclic graph (EDAG) to fill an event table directly. Different from the modeling and labeling aspects, the evaluation of EE attracted very little re4609 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4609–4617 August 1–6, 2021. ©2021 Associat"
2021.findings-acl.405,D14-1198,0,0.050385,"Missing"
2021.findings-acl.405,P17-1164,0,0.0278224,"Missing"
2021.findings-acl.405,N19-1080,0,0.0119262,"weakly labeled data by aligning event instances from knowledge bases to plain text and then assigning labels to matched samples. This strategy was originated from relation extraction (Mintz et al., 2009; Zheng et al., 2019b), but most existing EE models relied on trigger words to anchor an event mention. Accordingly, two types of research explorations emerged: one was to label trigger words with the help of extra linguistic resources (Chen et al., 2017) or predefined dictionaries (Yang et al., 2018), and the other was to remove the requirement of trigger words in modeling (Zeng et al., 2018; Liu et al., 2019; Zheng et al., 2019a). In this paper, we follow the no-triggerwords design because it can ease the labeling work of EE and thus generate large-scale data. Different from all the related work, this paper focuses on the evaluation aspect and attempts to extend the scope of EE evaluation methods to better support the event-as-a-whole scenarios. 3 Preliminaries In this section, we provide readers with necessary background to better understand our research. 3.1 Terminologies We follow Yang and Mitchell (2016) to use a general “entity” notion that covers persons, dates, numbers, and so on for brevi"
2021.findings-acl.405,P09-1113,0,0.164691,"ion of an EDAG and thus enabled the end-to-end modeling for EE. With the rapid development of modeling techniques, labeled data gradually became the main bottleneck that prevented from putting EE into practice. Most of the previous research was based on ACE 20051 , a benchmark dataset annotated by human experts. However, human annotation is both expensive and time-consuming. Recent research attempted to generate weakly labeled data by aligning event instances from knowledge bases to plain text and then assigning labels to matched samples. This strategy was originated from relation extraction (Mintz et al., 2009; Zheng et al., 2019b), but most existing EE models relied on trigger words to anchor an event mention. Accordingly, two types of research explorations emerged: one was to label trigger words with the help of extra linguistic resources (Chen et al., 2017) or predefined dictionaries (Yang et al., 2018), and the other was to remove the requirement of trigger words in modeling (Zeng et al., 2018; Liu et al., 2019; Zheng et al., 2019a). In this paper, we follow the no-triggerwords design because it can ease the labeling work of EE and thus generate large-scale data. Different from all the related"
2021.findings-acl.405,N16-1034,0,0.033199,"a very sophisticated task that requires the unification of many sub-tasks (Ahn, 2006), including entity recognition, event detection, and argument extraction, plenty of previous research put considerable efforts to the modeling aspect. Specifically, Nguyen and Grishman (2015); Liu et al. (2017); Chen et al. (2018); Wang et al. (2019); Liu et al. (2019) only considered event detection that is to detect trigger words and assign correct event types. Some advanced methods (Poon and Vanderwende, 2010; Riedel and McCallum, 2011; Li et al., 2013, 2014; Venugopal et al., 2014; Judea and Strube, 2016; Nguyen et al., 2016; Sha et al., 2018) tried to unify two sub-tasks, event detection and argument extraction, but all assumed that entity candidates were given in advance. A few studies attempted to fulfill all sub-tasks of EE jointly. Yang and Mitchell (2016) was the first work towards this goal but relied on handcrafted features. Later research (Nguyen and Nguyen, 2019) explored the joint modeling further by introducing neural networks but also retained many traditional lexical and syntactic features. Recently, Zheng et al. (2019a) formalized a new succinct task for EE without trigger words and proposed the fi"
2021.findings-acl.405,N10-1123,0,0.110107,"Missing"
2021.findings-acl.405,D11-1001,0,0.0990335,"Missing"
2021.findings-acl.405,D14-1090,0,0.0574451,"Missing"
2021.findings-acl.405,N19-1105,0,0.023976,"Missing"
2021.findings-acl.405,N16-1033,0,0.238623,"ructured knowledge can benefit many downstream applications, such as question answering, language understanding, knowledge graph, etc. In general, an event instance is composed of a group of entities (person, organization, date, etc.) that jointly describes an incident. Each entity of the event instance, also referred to The early method (Ahn, 2006) formalized EE as the unification of many sub-tasks, including entity recognition, event detection, and argument extraction, etc. Later research improved EE from two aspects: the modeling to capture complicated semantic structures (Li et al., 2013; Yang and Mitchell, 2016; Nguyen and Nguyen, 2019) and the labeling to combat the lack of training data (Chen et al., 2017; Zeng et al., 2018). Recently, Zheng et al. (2019a) proposed the first end-to-end model, called Doc2EDAG, for document-level EE. Given a text document, Doc2EDAG can generate an entitybased directed acyclic graph (EDAG) to fill an event table directly. Different from the modeling and labeling aspects, the evaluation of EE attracted very little re4609 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4609–4617 August 1–6, 2021. ©2021 Association for Computational Lin"
2021.findings-acl.405,P18-4009,0,0.0121543,"an experts. However, human annotation is both expensive and time-consuming. Recent research attempted to generate weakly labeled data by aligning event instances from knowledge bases to plain text and then assigning labels to matched samples. This strategy was originated from relation extraction (Mintz et al., 2009; Zheng et al., 2019b), but most existing EE models relied on trigger words to anchor an event mention. Accordingly, two types of research explorations emerged: one was to label trigger words with the help of extra linguistic resources (Chen et al., 2017) or predefined dictionaries (Yang et al., 2018), and the other was to remove the requirement of trigger words in modeling (Zeng et al., 2018; Liu et al., 2019; Zheng et al., 2019a). In this paper, we follow the no-triggerwords design because it can ease the labeling work of EE and thus generate large-scale data. Different from all the related work, this paper focuses on the evaluation aspect and attempts to extend the scope of EE evaluation methods to better support the event-as-a-whole scenarios. 3 Preliminaries In this section, we provide readers with necessary background to better understand our research. 3.1 Terminologies We follow Yan"
2021.findings-acl.405,D19-1032,1,0.122662,"nt instance is composed of a group of entities (person, organization, date, etc.) that jointly describes an incident. Each entity of the event instance, also referred to The early method (Ahn, 2006) formalized EE as the unification of many sub-tasks, including entity recognition, event detection, and argument extraction, etc. Later research improved EE from two aspects: the modeling to capture complicated semantic structures (Li et al., 2013; Yang and Mitchell, 2016; Nguyen and Nguyen, 2019) and the labeling to combat the lack of training data (Chen et al., 2017; Zeng et al., 2018). Recently, Zheng et al. (2019a) proposed the first end-to-end model, called Doc2EDAG, for document-level EE. Given a text document, Doc2EDAG can generate an entitybased directed acyclic graph (EDAG) to fill an event table directly. Different from the modeling and labeling aspects, the evaluation of EE attracted very little re4609 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4609–4617 August 1–6, 2021. ©2021 Association for Computational Linguistics search attention. Notably, most existing research, merely presenting a separate evaluation for each sub-task of EE, can only get the approx"
2021.findings-acl.405,P19-1137,1,0.127312,"nt instance is composed of a group of entities (person, organization, date, etc.) that jointly describes an incident. Each entity of the event instance, also referred to The early method (Ahn, 2006) formalized EE as the unification of many sub-tasks, including entity recognition, event detection, and argument extraction, etc. Later research improved EE from two aspects: the modeling to capture complicated semantic structures (Li et al., 2013; Yang and Mitchell, 2016; Nguyen and Nguyen, 2019) and the labeling to combat the lack of training data (Chen et al., 2017; Zeng et al., 2018). Recently, Zheng et al. (2019a) proposed the first end-to-end model, called Doc2EDAG, for document-level EE. Given a text document, Doc2EDAG can generate an entitybased directed acyclic graph (EDAG) to fill an event table directly. Different from the modeling and labeling aspects, the evaluation of EE attracted very little re4609 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4609–4617 August 1–6, 2021. ©2021 Association for Computational Linguistics search attention. Notably, most existing research, merely presenting a separate evaluation for each sub-task of EE, can only get the approx"
2021.findings-acl.405,P15-2060,0,0.0454949,"Missing"
2021.findings-emnlp.155,P14-1105,0,0.0582928,"Missing"
2021.findings-emnlp.155,C94-2174,0,0.196937,"e annotated spans are mostly single or short multi-word spans given the relative short context. In contrast, low agreements are obtained in the speech domain. Manual inspections reveal that our model tends to tag phrases including subjective pronouns such as “I” and “we”, which are informing signals in the Wikipedia domain for expressing subjective opinions, but under-perform in speech transcripts. 5 Related Work Detection of Subjective Bias. The study of detection of subjectivity can be dated back to 1990s, when pioneers start noticing the subjectivity genre on document level classification (Karlgren and Cutting, 1994; Kessler et al., 1997). Later, works like (Bruce and Wiebe, 1999; Hatzivassiloglou and Wiebe, 2000) bring people’s attention to the subjecHuman Evaluation. We sampled 50 sentences tivity on sentence level. There is a long line of reper corpus for human annotations. For each sen- search focusing on sentence classification utilizing tence, 3 qualified Turkers were asked to pick the methods based on linguistic features or handcrafted biased spans without length constraints. We con- rules (Riloff and Wiebe, 2003; Wiebe and Riloff, 1806 2005; Pang and Lee, 2004; Lin et al., 2011; Murray and Careni"
2021.findings-emnlp.155,P97-1005,0,0.386635,"single or short multi-word spans given the relative short context. In contrast, low agreements are obtained in the speech domain. Manual inspections reveal that our model tends to tag phrases including subjective pronouns such as “I” and “we”, which are informing signals in the Wikipedia domain for expressing subjective opinions, but under-perform in speech transcripts. 5 Related Work Detection of Subjective Bias. The study of detection of subjectivity can be dated back to 1990s, when pioneers start noticing the subjectivity genre on document level classification (Karlgren and Cutting, 1994; Kessler et al., 1997). Later, works like (Bruce and Wiebe, 1999; Hatzivassiloglou and Wiebe, 2000) bring people’s attention to the subjecHuman Evaluation. We sampled 50 sentences tivity on sentence level. There is a long line of reper corpus for human annotations. For each sen- search focusing on sentence classification utilizing tence, 3 qualified Turkers were asked to pick the methods based on linguistic features or handcrafted biased spans without length constraints. We con- rules (Riloff and Wiebe, 2003; Wiebe and Riloff, 1806 2005; Pang and Lee, 2004; Lin et al., 2011; Murray and Carenini, 2009; Yang et al.,"
2021.findings-emnlp.155,P18-1249,0,0.0578353,"Missing"
2021.findings-emnlp.155,P17-4012,0,0.0187342,"Missing"
2021.findings-emnlp.155,W04-3250,0,0.240253,"Missing"
2021.findings-emnlp.155,2020.acl-main.703,0,0.0519529,"Missing"
2021.findings-emnlp.155,N18-1169,0,0.0247776,"Missing"
2021.findings-emnlp.155,P02-1040,0,0.110798,"Missing"
2021.findings-emnlp.155,I11-1129,0,0.0347385,"ication (Karlgren and Cutting, 1994; Kessler et al., 1997). Later, works like (Bruce and Wiebe, 1999; Hatzivassiloglou and Wiebe, 2000) bring people’s attention to the subjecHuman Evaluation. We sampled 50 sentences tivity on sentence level. There is a long line of reper corpus for human annotations. For each sen- search focusing on sentence classification utilizing tence, 3 qualified Turkers were asked to pick the methods based on linguistic features or handcrafted biased spans without length constraints. We con- rules (Riloff and Wiebe, 2003; Wiebe and Riloff, 1806 2005; Pang and Lee, 2004; Lin et al., 2011; Murray and Carenini, 2009; Yang et al., 2017), then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al.,"
2021.findings-emnlp.155,N18-1012,0,0.020229,", then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al., 2017; Reddy and Knight, 2016; Xu et al., 2015; Rao and Tetreault, 2018). More recently, pipeline-based or stepwise approaches (Li et al., 2018; Leeftink and Spanakis, 2019; Madaan et al., 2020) focuses on first localizing the style to a fixed portion of the word, then generating replacement based on target style. Pryzant et al. (2020) adopts a similar approach by incorporating the localized style attribute into a jointembedding and enforces the text generation model to pay attention to the modifications. 6 Conclusion In this work, we contribute the first manually annotated parallel corpus of over 4,000 sentence pairs for the task of subjective bias detection. Thi"
2021.findings-emnlp.155,D15-1166,0,0.0234309,"Missing"
2021.findings-emnlp.155,P16-1101,0,0.0157453,"tion result on test set with different training data, reported on average of three runs. © means the model is further fine-tuned on Trainmanual . Model B ERT F INETUNED H IERARCHICAL macro-F1 class-level F1 F E D 33.9 39.0 41.0 56.3 22.0 24.2 62.1 20.5 35.2 61.0 26.5 35.8 Table 6: Macro and class-level F1 (Framing, Epistemological, and Demographic bias) on test set, averaged across three runs. learning inter-relations between the segment tagging and the sentence classification tasks. Biased Segment Tagging. We experiment with multiple baselines (Table 7), including (1) a BiLSTM-CNN-CRF model (Ma and Hovy, 2016), (2) a BERTAtten baseline which extracts words/phrases receiving high self-attention scores in the BERT encoder fine-tuned for the binary classification task (§3.1.1), (3) a DETECTOR model from (Pryzant et al., 2020) which labels the word with highest predicted probability, and (4) a finetune BERT tagging model in which we use the base size checkpoint as the encoder and a linear layer to predict token labels. Prior work (Recasens et al., 2013; Pryzant et al., 2020) demonstrated that linguistic features can assist in the detection of subjective bias. Thus, (5) we incorporate the linguistic fea"
2021.findings-emnlp.155,2020.acl-main.169,0,0.0127334,"ens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al., 2017; Reddy and Knight, 2016; Xu et al., 2015; Rao and Tetreault, 2018). More recently, pipeline-based or stepwise approaches (Li et al., 2018; Leeftink and Spanakis, 2019; Madaan et al., 2020) focuses on first localizing the style to a fixed portion of the word, then generating replacement based on target style. Pryzant et al. (2020) adopts a similar approach by incorporating the localized style attribute into a jointembedding and enforces the text generation model to pay attention to the modifications. 6 Conclusion In this work, we contribute the first manually annotated parallel corpus of over 4,000 sentence pairs for the task of subjective bias detection. This corpus covers multiple-word span annotations with fine-grained bias type on the source side and sentence level bias type"
2021.findings-emnlp.155,D09-1140,0,0.24116,"s work, we study how to detect and further mitigate biases in language. Specifically, we focus on a particular type of bias, “subjective bias”, in which the language is skewed towards an obvious feeling, with the presupposed or entailed proposition or considering opinions as truth. Contents with the subjective bias can make people be doubtful about the texts’ reliability and possibly trigger social unrest with offensive language. Prior research has used the lexical and grammatical cues like lexicon-syntactic patterns (Wiebe and Riloff, 2005; Riloff and Wiebe, 2003) or various n-gram features (Murray and Carenini, 2009; Wilson and Raaijmakers, 2008; Wiebe et al., 1999) to classify sentences as either subjective or objective. For instance, in the encyclopedia domain, Recasens et al. (2013) constructed an automatic parallel corpus from Wikipedia revisions that violate the Neutral Point of View (NPOV) policy,which advocates for “fairly presenting views with reliable sources and avoiding editor bias” and introduced the task of identifying the bias-induced word in a statement. They further uncovered two types of subjective bias through linguistic analysis, which includes framing bias such as praising or perspect"
2021.findings-emnlp.155,N19-4009,0,0.048689,"Missing"
2021.findings-emnlp.155,P04-1035,0,0.0838877,"cument level classification (Karlgren and Cutting, 1994; Kessler et al., 1997). Later, works like (Bruce and Wiebe, 1999; Hatzivassiloglou and Wiebe, 2000) bring people’s attention to the subjecHuman Evaluation. We sampled 50 sentences tivity on sentence level. There is a long line of reper corpus for human annotations. For each sen- search focusing on sentence classification utilizing tence, 3 qualified Turkers were asked to pick the methods based on linguistic features or handcrafted biased spans without length constraints. We con- rules (Riloff and Wiebe, 2003; Wiebe and Riloff, 1806 2005; Pang and Lee, 2004; Lin et al., 2011; Murray and Carenini, 2009; Yang et al., 2017), then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al.,"
2021.findings-emnlp.155,P13-1162,0,0.302267,"towards an obvious feeling, with the presupposed or entailed proposition or considering opinions as truth. Contents with the subjective bias can make people be doubtful about the texts’ reliability and possibly trigger social unrest with offensive language. Prior research has used the lexical and grammatical cues like lexicon-syntactic patterns (Wiebe and Riloff, 2005; Riloff and Wiebe, 2003) or various n-gram features (Murray and Carenini, 2009; Wilson and Raaijmakers, 2008; Wiebe et al., 1999) to classify sentences as either subjective or objective. For instance, in the encyclopedia domain, Recasens et al. (2013) constructed an automatic parallel corpus from Wikipedia revisions that violate the Neutral Point of View (NPOV) policy,which advocates for “fairly presenting views with reliable sources and avoiding editor bias” and introduced the task of identifying the bias-induced word in a statement. They further uncovered two types of subjective bias through linguistic analysis, which includes framing bias such as praising or perspective-specific words and epistemological bias related to presupposed/entailed propositions. Pryzant et al. (2020) extended such revision corpus and further proposed to transfo"
2021.findings-emnlp.155,W16-5603,0,0.0211733,"ay and Carenini, 2009; Yang et al., 2017), then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al., 2017; Reddy and Knight, 2016; Xu et al., 2015; Rao and Tetreault, 2018). More recently, pipeline-based or stepwise approaches (Li et al., 2018; Leeftink and Spanakis, 2019; Madaan et al., 2020) focuses on first localizing the style to a fixed portion of the word, then generating replacement based on target style. Pryzant et al. (2020) adopts a similar approach by incorporating the localized style attribute into a jointembedding and enforces the text generation model to pay attention to the modifications. 6 Conclusion In this work, we contribute the first manually annotated parallel corpus of over 4,000 sentence pairs for"
2021.findings-emnlp.155,W03-1014,0,0.307987,"s and keep the quality of the reference work. In this work, we study how to detect and further mitigate biases in language. Specifically, we focus on a particular type of bias, “subjective bias”, in which the language is skewed towards an obvious feeling, with the presupposed or entailed proposition or considering opinions as truth. Contents with the subjective bias can make people be doubtful about the texts’ reliability and possibly trigger social unrest with offensive language. Prior research has used the lexical and grammatical cues like lexicon-syntactic patterns (Wiebe and Riloff, 2005; Riloff and Wiebe, 2003) or various n-gram features (Murray and Carenini, 2009; Wilson and Raaijmakers, 2008; Wiebe et al., 1999) to classify sentences as either subjective or objective. For instance, in the encyclopedia domain, Recasens et al. (2013) constructed an automatic parallel corpus from Wikipedia revisions that violate the Neutral Point of View (NPOV) policy,which advocates for “fairly presenting views with reliable sources and avoiding editor bias” and introduced the task of identifying the bias-induced word in a statement. They further uncovered two types of subjective bias through linguistic analysis, wh"
2021.findings-emnlp.155,D13-1010,0,0.0783099,"Missing"
2021.findings-emnlp.155,W06-1639,0,0.168741,"te whether the phrase is supported or against the stance of the target (i.e., totally irresponsible − illustrates that the speaker uses this phrase to criticize the work of Republican Party). The extracted phrases from the speeches domain cover the signature words of the speaker without in-domain knowledge. “have a plan” is prevalent in 2020’s presidency debates and signature words “tremendous” and “very powerfully” of Donald Trump have also been captured. (3) The model can tight the connection between subjective bias with research over stance detection, especially in the formal text domains (Thomas et al., 2006; Walker et al., 2012; Chakrabarty et al., 2019; Lawrence and Reed, 2020). With our subjective bias tagger, complete verb phrases or noun phrases can be obtained, which naturally eases the extraction of topics and opinions, two necessary components for stance detection problem. For instance, “because Obamacare is no good” span can sufficiently illustrate the opinion of Trump that is against the prior healthcare policy. Meanwhile, “frustrated hypocrite” can indicate the left-wing media’s dislike of the Republican governor’s behavior. sider a span receiving more than one annotator vote the gold"
2021.findings-emnlp.155,walker-etal-2012-corpus,0,0.0396414,"is supported or against the stance of the target (i.e., totally irresponsible − illustrates that the speaker uses this phrase to criticize the work of Republican Party). The extracted phrases from the speeches domain cover the signature words of the speaker without in-domain knowledge. “have a plan” is prevalent in 2020’s presidency debates and signature words “tremendous” and “very powerfully” of Donald Trump have also been captured. (3) The model can tight the connection between subjective bias with research over stance detection, especially in the formal text domains (Thomas et al., 2006; Walker et al., 2012; Chakrabarty et al., 2019; Lawrence and Reed, 2020). With our subjective bias tagger, complete verb phrases or noun phrases can be obtained, which naturally eases the extraction of topics and opinions, two necessary components for stance detection problem. For instance, “because Obamacare is no good” span can sufficiently illustrate the opinion of Trump that is against the prior healthcare policy. Meanwhile, “frustrated hypocrite” can indicate the left-wing media’s dislike of the Republican governor’s behavior. sider a span receiving more than one annotator vote the gold label. The second col"
2021.findings-emnlp.155,P99-1032,0,0.729375,"Missing"
2021.findings-emnlp.155,J04-3002,0,0.211898,"Missing"
2021.findings-emnlp.155,H05-1044,0,0.0582223,"The state-ofthe-art baselines still struggle with multi-span deJoint Sentence Classification and Tagging. We tection, with significantly worse performance comdeploy a model to jointly learn sentence-level clas- paring to the estimated human upper bond. Thus, sification and token-level segmentation of bias. our corpus can serve as a useful research benchMore specifically, we utilize a BERT tagging model mark for future studies. Manual inspections on 6 tagging results suggest that models mainly failed I.e., lexicons of hedges (Thompson, 2005), factive verbs (Hooper, 1975), and subjective clues (Wilson et al., 2005). in detecting spans with content-dependent bias and 1804 preserving the completeness of phrases. The joint model achieves worse performance on the segment tagging task which is mainly attributed to the lower recall, while obtains a slight performance gain on the classification task. 3.3 Text Generation for Neutralizing Bias Bias neutralization can also be viewed as a text generation problem (Pryzant et al., 2020). In this section, we experiment with multiple generation baselines over W IKI B IAS, including Source Copy (directly copy input as output), LSTM and attention based seq2seq model (Lu"
2021.findings-emnlp.155,Q15-1021,1,0.782212,"ang et al., 2017), then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al., 2017; Reddy and Knight, 2016; Xu et al., 2015; Rao and Tetreault, 2018). More recently, pipeline-based or stepwise approaches (Li et al., 2018; Leeftink and Spanakis, 2019; Madaan et al., 2020) focuses on first localizing the style to a fixed portion of the word, then generating replacement based on target style. Pryzant et al. (2020) adopts a similar approach by incorporating the localized style attribute into a jointembedding and enforces the text generation model to pay attention to the modifications. 6 Conclusion In this work, we contribute the first manually annotated parallel corpus of over 4,000 sentence pairs for the task of subj"
2021.findings-emnlp.155,C12-1177,1,0.58767,"Lee, 2004; Lin et al., 2011; Murray and Carenini, 2009; Yang et al., 2017), then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al., 2017; Reddy and Knight, 2016; Xu et al., 2015; Rao and Tetreault, 2018). More recently, pipeline-based or stepwise approaches (Li et al., 2018; Leeftink and Spanakis, 2019; Madaan et al., 2020) focuses on first localizing the style to a fixed portion of the word, then generating replacement based on target style. Pryzant et al. (2020) adopts a similar approach by incorporating the localized style attribute into a jointembedding and enforces the text generation model to pay attention to the modifications. 6 Conclusion In this work, we contribute the first manually annotated paralle"
2021.findings-emnlp.155,D17-1213,1,0.805716,"et al., 1997). Later, works like (Bruce and Wiebe, 1999; Hatzivassiloglou and Wiebe, 2000) bring people’s attention to the subjecHuman Evaluation. We sampled 50 sentences tivity on sentence level. There is a long line of reper corpus for human annotations. For each sen- search focusing on sentence classification utilizing tence, 3 qualified Turkers were asked to pick the methods based on linguistic features or handcrafted biased spans without length constraints. We con- rules (Riloff and Wiebe, 2003; Wiebe and Riloff, 1806 2005; Pang and Lee, 2004; Lin et al., 2011; Murray and Carenini, 2009; Yang et al., 2017), then neural models (Morstatter et al., 2018; Hube and Fetahu, 2018; Pant et al., 2020; Hube and Fetahu, 2019). Work of Recasens et al. (2013) and Pryzant et al. (2020) on detecting biased language over singleword edit is closely related to our work, but we study the biased language on a broader scale to cover multi-word spans. Debiasing Generation. Generating debiased text can be viewed as a stylistic transferring task. Supervised approaches with parallel corpus have been shown to be effective across multiple styles (Xu et al., 2012; Hu et al., 2017; Reddy and Knight, 2016; Xu et al., 2015;"
2021.gem-1.10,2020.acl-main.424,0,0.0251404,"les WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling prog"
2021.gem-1.10,2020.acl-main.766,0,0.0426234,"sh datasets were ranked lower, with only WebNLG and MLSum among the top 15 datasets. We grouped all datasets by their high-level tasks and selected a group that would not violate the selection principles (e.g., only high-resource tasks). If two datasets fit, we picked the one with a higher interest rating. Among the 11 datasets, we have 18different languages, and the dataset sizes range from 5,000 examples to 1.5M, with most datasets between 50-150k examples. Two of them do not include English at all, which we hope reduces the dependence of the modeling approaches on anglocentric pretraining (Anastasopoulos and Neubig, 2020). The high-level tasks include Dialog, Summarization, Data-to-Text, and Simplification. About half of the datasets have multiple references and more than half had post-processing steps applied to them to ensure high data quality. 3.1 GEMifying the data We produce data cards (Bender and Friedman, 2018; Gebru et al., 2018) for all data sets in GEM, for which we developed an NLG-specific template.7 In addition to describing the data itself, the cards acknowledge potential limitations of a dataset regarding its creation process and describe its real-world use cases to ensure that the research is c"
2021.gem-1.10,W05-0909,0,0.165721,"otebook within 2-3 hours. 4.2 avoid overfitting to known metrics, we will use metrics on the test submissions that are not included in this initial writeup. Consequentially, the baseline results are an incomplete list which will be expanded upon the announcement of the test metrics. The set of metrics can be computed via the framework described at https://gem-benchmark. com/shared_task which comprises metrics in the following categories: Lexical Similarity. We include multiple “traditional” metrics as baseline metrics, notably BLEU (Papineni et al., 2002), ROUGE-1/2/L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). These metrics can often be gamed, for example, ROUGE can be improved by increased the output length of the model (Sun et al., 2019). Moreover, the reliability of these metrics depends on the quality and number of the references (Mathur et al., 2020a; Freitag et al., 2020). However, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 10"
2021.gem-1.10,W11-2832,0,0.0915033,"Missing"
2021.gem-1.10,2020.inlg-1.24,1,0.864971,"human evaluation standards. In recent work, Howcroft et al. (2020) investigated NLG papers from the last 2 For a more complete description of recent developments in NLG evaluation, we refer to the survey by Celikyilmaz et al. (2020). 99 twenty years and the evaluation methodologies differ drastically across papers. Moreover, in most cases, it is not even mentioned what the human evaluation aims to measure and that definitions of measures like “accuracy” or “fluency” are inconsistent. They thus suggest reporting standards for criteria and methods, following a classification system proposed by Belz et al. (2020). In addition, regularly scheduled shared tasks like WMT have lead to standardization of human evaluation setups and enabled controlled experimentation with them. GEM has the opportunity to develop reproducible standards for how human evaluation for NLG tasks beyond translation should be conducted while at the same time incorporating lessons from related work. Acting on the same need, the recently proposed GENIE (Khashabi et al., 2021) system aims to automate and standardize the human evaluation of different NLG systems, however with the contrasting goal of reducing the evaluating to a leaderb"
2021.gem-1.10,Q18-1041,0,0.236179,"han half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard ("
2021.gem-1.10,W17-4755,0,0.0383891,"Missing"
2021.gem-1.10,W16-2302,0,0.0489314,"Missing"
2021.gem-1.10,N18-2097,0,0.0498492,"Missing"
2021.gem-1.10,N19-1423,0,0.0210653,"provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation appr"
2021.gem-1.10,P19-1483,1,0.811795,"tions. 106 Figure 2: A screenshot of the interactive result exploration tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek"
2021.gem-1.10,K19-1037,0,0.027404,"Missing"
2021.gem-1.10,P17-1123,0,0.0287395,"Missing"
2021.gem-1.10,2020.acl-main.454,1,0.84177,"LEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen"
2021.gem-1.10,W19-8652,1,0.897973,"Missing"
2021.gem-1.10,C16-1191,0,0.0653297,"Missing"
2021.gem-1.10,P18-1082,0,0.0705277,"Missing"
2021.gem-1.10,W16-3622,1,0.897392,"Missing"
2021.gem-1.10,P16-2008,1,0.869379,"Missing"
2021.gem-1.10,W19-8670,1,0.884454,"Missing"
2021.gem-1.10,2020.emnlp-main.393,0,0.179031,"le to be able to address newly found limitations, and that the benchmark should focus on climbing a leaderboard. Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench,1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able mu"
2021.gem-1.10,N19-1395,0,0.0587235,"Missing"
2021.gem-1.10,2020.emnlp-main.751,0,0.503468,"nly on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover,"
2021.gem-1.10,P19-1346,1,0.897232,"Missing"
2021.gem-1.10,2020.webnlg-1.7,1,0.843551,"Missing"
2021.gem-1.10,2020.findings-emnlp.195,1,0.835947,"Missing"
2021.gem-1.10,2020.emnlp-main.5,0,0.126308,"e machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differently across tasks, setups, and languages, a multi-task NLG benchmark has the opportunity to act as a testbed to evaluate how the latest advances in automated metrics perform on these different tasks. The benchmark can facilitate this research through the release of system outputs and associated human annotations, which is what we are planning to do with GEM. Moreover, we allow the integrat"
2021.gem-1.10,2020.emnlp-main.489,0,0.0422168,"les 3 and 4. Our interactive system is centered around a parallel coordinates plot (Inselberg, 1985) which shows all results as lines through parallel axes. Every line intersects the axes at the corresponding mapped value. For instance, see the red line representing the results for task “ToTTo” of baseline “t5-small”. Filters can be applied along axes (see BLEURT axis in Figure 2) and the filtered selection is highlighted through bold lines. A selection can be a set of metrics, systems, or tasks. This style of presentation has not been used before for a benchmark. The closest prior work is by Fu et al. (2020) for namedentity recognition which allows similar filtering and sorting, but presents the results in a table. However, the parallel coordinates approach can scale to a much greater number of metrics than a table. Moreover, by using a parallel coordinates plot instead of a table, it is easy to spot patterns that span multiple metrics, systems, or tasks. For example, the highlighted line in Figure 2 uncovers that, for the T5 baseline on ToTTo, the diversity metrics score higher than other systems while scoring lower on reference-based metrics. Since we only have a single baseline for ToTTo, it i"
2021.gem-1.10,W17-3518,1,0.863605,"Missing"
2021.gem-1.10,N19-1169,1,0.923485,"eiter and Dale, 2000). These texts aim to fulfill an underlying communicative goal (e.g., to produce a summary of an article) while remaining faithful to the input information, fluent, grammatical, and natural-looking. An NLG system needs to be robust to shifts in the data distribution and be able to produce text in many different languages. Finally, it is often desired that repeated interactions with the model produce diverse outputs, for example, to explain concepts in multiple ways or to become a more interesting conversational agent. These optimization objectives can often be conflicting (Hashimoto et al., 2019) and, as a result, evaluations that focus only on a single aspect may fail to recognize the drawbacks of a particular method. To demonstrate this trade-off, consider an improvement on the CNN-DM summarization dataset (Hermann et al., 2015; Nallapati et al., 2016) measured by the ROUGE-L metWe introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent"
2021.gem-1.10,2020.ngt-1.1,0,0.0253511,"first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution"
2021.gem-1.10,2020.inlg-1.23,1,0.841029,"Missing"
2021.gem-1.10,D15-1229,0,0.0462438,"Missing"
2021.gem-1.10,2020.acl-main.709,1,0.887061,"Missing"
2021.gem-1.10,2020.acl-main.560,0,0.077305,"on does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b; Liu et al., 2020a; McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and Wiki"
2021.gem-1.10,2020.evalnlgeval-1.4,0,0.0163557,"ation tool. [Top Left] The selection of tasks, task-groups, or individual submissions. [Top Right] The selection of metric-groups or metrics [Bottom] The parallel coordinates visualization of the selection. The selection here can be filtered by brushing over a section of an individual metric, as is shown here for BLEURT. Hovering over a line presents detailed information of the particular submission. grained and interpretable evaluation metrics, for example to measure consistency in data-to-text problems (Opitz and Frank, 2020; Dhingra et al., 2019). We are using one such metric called NUBIA (Kane et al., 2020), the NeUral Based Interchangeability Assessor, which combines multiple measures such as entailment and similarity into a decomposable and interpretable score. Diversity. As argued by Hashimoto et al. (2019) among many others, NLG models intrinsically trade off diversity and quality. A model can produce more diverse outputs through sampling but at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These inclu"
2021.gem-1.10,D18-1208,0,0.0650198,"Missing"
2021.gem-1.10,Q18-1023,0,0.0642232,"Missing"
2021.gem-1.10,2020.findings-emnlp.360,1,0.934785,"e a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmarks are critical in measuring modeling progress. and conducting in-depth ana"
2021.gem-1.10,D16-1128,0,0.0680679,"Missing"
2021.gem-1.10,2020.acl-main.703,0,0.214186,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,2020.acl-main.653,0,0.233553,"esent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). As a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared"
2021.gem-1.10,N16-1014,0,0.0410993,"t at the cost of output quality. To account for this aspect, we compute multiple diversity metrics, starting with those proposed for the analysis of the results of the E2E NLG challenge (Dusek et al., 2020) and by van Miltenburg et al. (2018). These include the Shannon Entropy (Shannon and Weaver, 1963) over unigrams and bigrams (H1 , H2 ), the mean segmented type token ratio over segment lengths of 100 (MSTTR, Johnson, 1944), the ratio of distinct n-grams over the total number of n-grams (Distinct1,2 ), and the count of n-grams that only appear once across the entire test output (Unique1,2 , Li et al., 2016). focus of this section will be on qualitative descriptions through model cards, we also gather quantitative information that is not necessarily associated with a judgment. As part of this, we collect the number of parameters of a system, as suggested by Ethayarajh and Jurafsky (2020). For each task, we additionally report the vocabulary size over the output (|V|) and the mean output length of a system (Sun et al., 2019). 5 Results One of the central aims of GEM is to measure the progress in NLG without misrepresenting the complex interactions between the sometimes contradicting measures. We t"
2021.gem-1.10,2020.findings-emnlp.165,0,0.0883829,"Missing"
2021.gem-1.10,W04-1013,0,0.355472,"n be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets"
2021.gem-1.10,2020.acl-main.465,0,0.123121,"; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generation, Evaluation, and Metrics) that aims to enable research on a wide range of NLG challenges. To avoid the fallacy of encouraging hill climbing on a leaderboard (Linzen, 2020), GEM focuses on an in-depth evaluation of model outputs across human and automatic evaluation that aims to uncover shortcomings and opportunities for progress. As datasets, metrics, and models improve, the benchmark environment will improve as well, replacing “solved” tasks with more challenging ones, incorporating newly developed metrics, and addressing discovered flaws in the experimental setup, as demonstrated in Figure 1. Making all model outputs available under an open-source license will support evaluation research and integrating new metrics will, in turn, help their adoption and incre"
2021.gem-1.10,2020.tacl-1.47,0,0.129414,"ich has a static evaluation, but interactively adds more test However, they also pose a risk that progress is reduced to the single number shown in a benchmark’s leaderboard and thus may encourage blindly optimizing it without regard to other considerations like model size or fairness (Ethayarajh and Jurafsky, 2020). This is especially challenging for benchmarks in NLG since, as discussed above, the performance cannot be described through a single metric and it is often not clear what metric to optimize for. This shortfall can be seen in benchmarks like DecaNLP (McCann et al., 2018) and GLGE (Liu et al., 2020a) which include NLG tasks but focus only on a single metric and, as a result, may mischaracterize a system’s performance. Moreover, an easy-to-use data infrastructure also disincentivizes researchers from interacting with 1 98 https://dynabench.org/ data through a human-in-the-loop approach. able much richer evaluation (as described in the next sections), and promote non-English datasets. In addition, it can ensure that the datasets created for those shared tasks continue being evaluated. Increasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is t"
2021.gem-1.10,2021.ccl-1.108,0,0.0502233,"Missing"
2021.gem-1.10,W15-4640,0,0.0777152,"Missing"
2021.gem-1.10,W18-6450,0,0.0170006,"ever, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automa"
2021.gem-1.10,W19-5302,0,0.0535855,"Missing"
2021.gem-1.10,2020.coling-main.420,0,0.022745,"s many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured quantities (Howcroft et al., 2020). Improving Data Improving Metrics Evaluation with gameable metrics Improving Models Consistent Human Eval Non-repeatable human evaluation Figure 1: The opportunities of living benchmarks and pitfalls of evaluation. As models improve, we need consistent evaluations such that models can be compared to each other. This can only happen if we develop robust human evaluation standards and improve our automated metrics. Ot"
2021.gem-1.10,2020.acl-main.448,0,0.212993,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.wmt-1.77,0,0.124839,"ere is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achieve a high correlation (Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019; Mathur et al., 2020b). The latest metrics focus on semantic equivalence instead of lexical similarity, which improves the correlations drastically. However, recent work by Fabbri et al. (2020) demonstrates that this may not hold in summarization, where the automated metric BERTScore (Zhang et al., 2020b) does not improve upon the correlation of ROUGE. Moreover, Mathur et al. (2020a) and Freitag et al. (2020) find that when comparing two high-quality systems, differences according to a metric may also stem from how references are written or flaws in the metric itself.2 Given that automated metrics perform differe"
2021.gem-1.10,2020.acl-main.173,1,0.838186,"to which we invite the entire NLG community to participate. * Correspondence to gehrmann@google.com 96 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicti"
2021.gem-1.10,W18-3601,1,0.834682,"where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way"
2021.gem-1.10,2020.msr-1.1,1,0.821875,"Missing"
2021.gem-1.10,C18-1147,1,0.871186,"Missing"
2021.gem-1.10,2020.emnlp-main.466,0,0.0504036,"Missing"
2021.gem-1.10,D15-1238,0,0.013053,"d this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the in"
2021.gem-1.10,K16-1028,0,0.065799,"Missing"
2021.gem-1.10,D18-1206,1,0.894493,"Missing"
2021.gem-1.10,W17-5525,1,0.880333,"Missing"
2021.gem-1.10,P02-1040,0,0.114424,"n NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated metrics, such as ROUGE (Lin, 2004) and BLEU (Papineni et al., 2002), measure the n-gram overlap between a reference and the generated text. However, in most cases, there is more than one correct way to generate a text, especially in tasks with a latent content planning or selection step (Reiter and Dale, 2000). That means that a correct solution may score low on a metric. While multiple references alleviate the issue somewhat, these metrics still have a low correlation with human judgments (Reiter, 2018; Fabbri et al., 2020). To address the issue, the machine translation community has been organizing yearly metrics shared tasks which produce metrics that achi"
2021.gem-1.10,2020.emnlp-main.89,1,0.895725,"Missing"
2021.gem-1.10,W17-3537,1,0.910798,"trics saturate, we need to evaluate them on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmar"
2021.gem-1.10,2020.emnlp-main.185,0,0.0484252,"Missing"
2021.gem-1.10,2021.acl-long.186,0,0.0851423,"Missing"
2021.gem-1.10,P19-1195,0,0.0585453,"Missing"
2021.gem-1.10,N18-1012,0,0.0173282,"akers of low-resourced languages through a participatory research approach, as suggested by (∀ et al., 2020). Toward this goal, GEM welcomes anyone interested in collaborating on this effort. 7.2 Personalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the result"
2021.gem-1.10,Q19-1016,0,0.0583227,"Missing"
2021.gem-1.10,J18-3002,0,0.0872491,"for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where MLSum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020). Unfortunately, XGLUE reduces the generation evaluation to BLEU-4, a metric that is inadequate for NLG (Reiter, 2018). There have also been multiple shared tasks in NLG that focus on multilingualism, for instance, the shared task on multilingual surface realization which includes eleven languages (Mille et al., 2018, 2019, 2020). The shared task on document-level generation and translation featured German and English generation challenges (Heafield et al., 2020). The WebNLG+ shared task asked participants to contribute models that can realize text in Russian and English (Ferreira et al., 2020). A benchmark that focuses only on NLG can enProviding a testbed for automated evaluation. Most traditional automated"
2021.gem-1.10,2020.acl-main.442,0,0.137138,"hem on more challenging datasets instead of continuing to move sideways on old ones. GEM aims to provide this environment for natural language generation. than half were post-processed to improve data quality. The sizes range from 5k to 500k data points. GEM features 18 languages across all tasks and two of the datasets do not include English at all. To be able to properly assess the performance of models in a way robust to the shortcuts a model can take, we additionally introduce ten types of challenging test sets that probe for specific modeling aspects (Perez-Beltrachini and Gardent, 2017; Ribeiro et al., 2020). To ensure that research with GEM is conducted responsibly, all the datasets are documented in an NLG-specific version of data cards (Bender and Friedman, 2018; Gebru et al., 2018) we developed and for which we release a template and guide. Moreover, all submitted models will have an associated data card (Mitchell et al., 2019). This paper describes the selection and construction of the GEM datasets in support of the announcement of the shared task at ACL 2021. More detailed information can be found on our website https://gem-benchmark.com/. We propose a living benchmark called GEM (Generatio"
2021.gem-1.10,2020.emnlp-main.647,0,0.0370713,"Missing"
2021.gem-1.10,2021.emnlp-main.529,0,0.024648,"on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291 63.5 64.0 32.5 29.4 55.1 54.5 27.5 26.4 0.943 0.942 -0.400 -0.412 mT5-small mT5-base mT5-large mT5-XL TGen TGen+ TGen++ 0.229 0.23 0.233 0.229 0.152 0.151 0.167 47.3 48.1 51.3 52.1 13.6 13.8 9.7 28.6 28.8 30.0 31.3 0.0 0.0 0.0 43.0 44.2 46.4 47.3 13.6 13.8 9.7 17.9 17.1 17.5 17.0 0.03 0.03 0.03 0.895 0.898 0.902 0.905 0.6"
2021.gem-1.10,D19-1320,0,0.0210218,"ence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTSc"
2021.gem-1.10,2020.acl-main.704,1,0.824209,"er, on a system-level, they still correlate well with human judgments for some tasks (Reiter, 2018). Automated Evaluation As mentioned above, GEM provides a testbed for automated metrics and can be used to popularize newly developed ones. Thus, models are evaluated via a constantly expanding list of metrics and, to 104 Semantic Equivalence. More recently, metrics that rely on pretrained language models have shown improved correlations with human judgments on the segment-level. We thus include BERTScore (Zhang et al., 2020b), a metric based on the similarity of sentence embeddings, and BLEURT (Sellam et al., 2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang e"
2021.gem-1.10,P19-1212,0,0.0526263,"Missing"
2021.gem-1.10,P19-1646,0,0.0614703,"Missing"
2021.gem-1.10,Q16-1029,1,0.821779,"in a news article en *25k Articles WebNLG (Gardent et al., 2017) Produce a text that verbalises the input triples in a grammatical and natural way. en/ru 50k RDF triple en 594k Sentence Dataset CommonGEN (Lin et al., 2020) Communicative Goal Language(s) Produce a likely sentence which mentions en all of the source concepts. Czech Restaurant (Dušek and Jurˇcíˇcek, 2019) Produce a text expressing the given intent and covering the specified attributes. DART (Radev et al., 2020) WikiAuto + Turk/ASSET Communicate the same information as (Jiang et al., 2020) the source sentence using simpler words (Xu et al., 2016) and grammar. (Alva-Manchego et al., 2020) WikiLingua (Ladhak et al., 2020) *ar/cs/de/en Produce high quality summaries of an es/fr/hi/id/it *550k instructional article. ja/ko/nl/pt/ru th/tr/vi/zh Article Table 1: A description of all the datasets included in GEM. The tasks vary in communicative goal, data size, and input type. * indicates changes from the originally published dataset made for GEM. (NLU) tasks. They aggregate multiple tasks under a unified evaluation framework, which enables researchers to fairly compare their models to others. Due to the improved model comparability, benchmar"
2021.gem-1.10,W15-3031,0,0.06329,"Missing"
2021.gem-1.10,W19-2303,0,0.167787,"of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96–120 August 5–6, 2021. ©2021 Association for Computational Linguistics Varying experimental setups Evaluation on “solved” data ric (Lin, 2004). Since ROUGE only tests the extent to which a generated summary has a lexical overlap with a reference summary, it can erroneously produce high scores for fluent, yet meaningless and unfaithful outputs as long as many of the same words are used (Maynez et al., 2020; Gabriel et al., 2020). Moreover, ROUGE tends to favor systems that produce longer summaries (Sun et al., 2019). It is thus crucial to carefully assess the progress of NLG toward all of its goals at the same time in ways that evolve alongside the models. This is currently not the case; new models are evaluated on different datasets, most of which focus only on the English language (Bender, 2019), and using these flawed metrics. Moreover, while human evaluations of generated texts can provide complementary insights to automatic evaluation (Manning et al., 2020), it can also lead to contradicting results since studies often omit crucial replication details and assume different definitions of the measured"
2021.gem-1.10,2020.nlp4convai-1.13,0,0.0861626,"Missing"
2021.gem-1.10,D16-1033,0,0.064867,"Missing"
2021.gem-1.10,2020.acl-main.450,0,0.015505,"2020), a metric that is fine-tuned on human ratings. The reported baseline results use RoBERTa-large (Liu et al., 2019) and mBERT (Devlin et al., 2019) for BERTScore and the English-only BLEURT-base-128 for BLEURT. Probing for Faithfulness. Another approach that has shown promise in summarization. The approach relies on the insight that a reader of a reference and generated summary should be able to answer the same question, regardless of how the summary is phrased. There has been much development toward these QA-based approaches (Eyal et al., 2019; Scialom et al., 2019; Durmus et al., 2020; Wang et al., 2020, among others) and they can provide an alternative angle to model evaluation that does not highly correlate with other evaluation approaches (Fabbri et al., 2020). While most related work on these metrics is limited to summarization, we are evaluating systems using a QA-based method called QuestEval (Scialom et al., 2021) that supports all of our tasks. In addition to QA-based evaluation, there have also been related efforts to develop more fineMetrics (Lexical Similarity and Semantic Equivalence) Dataset Model METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT CommonGen BART T5 0.301 0.291"
2021.gem-1.10,P18-1205,0,0.0245839,"nalizing and Controlling NLG GEM currently focuses on tasks that deterministically transform an input into an output. With the increasing use of NLG models in real-world applications, how to enable and evaluate personalized NLG systems (e.g., in dialect or formality) remains challenging. Several related tasks have been proposed, for example, the transfer of writing style from informal to formal (Rao and Tetreault, 2018), personalization of machine translation systems to align with particular personal traits (Mirkin and Meunier, 2015), or persona-guided response generation of dialogue systems (Zhang et al., 2018). We envision our framework to be extended (e.g., dataset, evaluation) to incorporate this line of userfocused NLG. 7.3 Regular updates to the living benchmark To activate the benefits of a living benchmark that is focused on evaluation, we commit to regular updates for GEM. We invite contributions in the form of model outputs, analyses, and metrics at any time and will automatically update the results presented on our website to incorporate them. For the updates to the dataset selection, we want to consider the input of the wider NLG research community. To do so, we will set up a yearly selec"
2021.naacl-main.277,P18-2114,0,0.0217597,"for sharing the vided into three categories: (1) Lexical Simplifica- data and NVIDIA for providing GPU computing tion (Specia et al., 2012; Horn et al., 2014; Glavaš resources. This research is supported in part by and Štajner, 2015; Paetzold and Specia, 2017, 2015; the NSF award IIS-1822754, ODNI and IARPA Maddela and Xu, 2018; Qiang et al., 2020), where via the BETTER program contract 19051600004. complex words are substituted with simpler words. The views and conclusions contained herein are (2) Syntactic Simplification (Siddharthan, 2006; those of the authors and should not be interpreted Aharoni and Goldberg, 2018; Botha et al., 2018; as necessarily representing the official policies, eiNiklaus et al., 2019), which deals exclusively with ther expressed or implied, of NSF, ODNI, IARPA, sentence splitting, and (3) Sentence Compression or the U.S. Government. The U.S. Government is (Filippova et al., 2015; Rush et al., 2015; Nallapati authorized to reproduce and distribute reprints for et al., 2016; See et al., 2017; Baziotis et al., 2019), governmental purposes notwithstanding any copywhere the goal is to shorten the input sentence by right annotation therein. removing its irrelevant content. 3544 Refere"
2021.naacl-main.277,I17-1030,1,0.826721,"t simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules (Carroll et al., 1999; Siddharthan, 2002; Siddharthan et al., 2004) or data-driven methods based on parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Following neural machine translation, the trend changed 4.2 Error Analysis to performing all the operations together end-toTo understand the errors generated by our model, end (Zhang and Lapata, 2017; Nisioi et al., 2017; we manually classified 200 simplifications from the Zhao et al., 2018; Alva-Manchego et al., 2017; Vu 3543 Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) System Outputs Since 2010, project researchers have uncovered documents in Portugal that have revealed who owned the ship. Since 2010, experts have been figuring out who owned the ship. since 2010, the project scientists have uncovered documents in portugal that have about who owns the ship. since 2010, scientists have uncovered documents in portugal tha"
2021.naacl-main.277,2020.acl-main.424,1,0.880055,"Missing"
2021.naacl-main.277,N19-1071,0,0.0610137,"Missing"
2021.naacl-main.277,D18-1080,0,0.23856,"Missing"
2021.naacl-main.277,E99-1042,0,0.51554,"Fluency Errors, where the model generated ungrammatical output, (d) Anaphora Resolution, where it was difficult to resolve pronouns in the output. (e) Bad substitution, where the model inserted an incorrect simpler phrase, and (e) Human Reference Errors, where the reference does not reflect the source sentence. Note that a simplification can belong to multiple error categories. Table 7 shows the examples of each category. 5 Related Work Before the advent of neural networks, text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules (Carroll et al., 1999; Siddharthan, 2002; Siddharthan et al., 2004) or data-driven methods based on parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Following neural machine translation, the trend changed 4.2 Error Analysis to performing all the operations together end-toTo understand the errors generated by our model, end (Zhang and Lapata, 2017; Nisioi et al., 2017; we manually classified 200 simplifications from the Zhao et al., 2018; Alva-Manchego et al., 2017; Vu 3543 Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our M"
2021.naacl-main.277,C96-2183,0,0.857926,"Missing"
2021.naacl-main.277,C12-1034,0,0.0741409,"Missing"
2021.naacl-main.277,P11-2117,0,0.0490141,"Missing"
2021.naacl-main.277,E17-2006,0,0.0179067,"ons. We showed that our model can control various attributes of the simplified text, such as number of sentence splits, length, and number of words copied from the input. Acknowledgments Another long body of research focuses on a sin- We thank the anonymous reviewers for their valugle simplification operation and can be broadly di- able feedback. We thank Newsela for sharing the vided into three categories: (1) Lexical Simplifica- data and NVIDIA for providing GPU computing tion (Specia et al., 2012; Horn et al., 2014; Glavaš resources. This research is supported in part by and Štajner, 2015; Paetzold and Specia, 2017, 2015; the NSF award IIS-1822754, ODNI and IARPA Maddela and Xu, 2018; Qiang et al., 2020), where via the BETTER program contract 19051600004. complex words are substituted with simpler words. The views and conclusions contained herein are (2) Syntactic Simplification (Siddharthan, 2006; those of the authors and should not be interpreted Aharoni and Goldberg, 2018; Botha et al., 2018; as necessarily representing the official policies, eiNiklaus et al., 2019), which deals exclusively with ther expressed or implied, of NSF, ODNI, IARPA, sentence splitting, and (3) Sentence Compression or the U."
2021.naacl-main.277,C04-1129,0,0.167529,"d ungrammatical output, (d) Anaphora Resolution, where it was difficult to resolve pronouns in the output. (e) Bad substitution, where the model inserted an incorrect simpler phrase, and (e) Human Reference Errors, where the reference does not reflect the source sentence. Note that a simplification can belong to multiple error categories. Table 7 shows the examples of each category. 5 Related Work Before the advent of neural networks, text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules (Carroll et al., 1999; Siddharthan, 2002; Siddharthan et al., 2004) or data-driven methods based on parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Following neural machine translation, the trend changed 4.2 Error Analysis to performing all the operations together end-toTo understand the errors generated by our model, end (Zhang and Lapata, 2017; Nisioi et al., 2017; we manually classified 200 simplifications from the Zhao et al., 2018; Alva-Manchego et al., 2017; Vu 3543 Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) Complex Simple Hybrid-NG LSTM"
2021.naacl-main.277,S12-1046,0,0.0389555,"e the model to paraphrase. We created a new dataset, N EWSELA -T URK, to evaluate paraphrasing-focused simplifications. We showed that our model can control various attributes of the simplified text, such as number of sentence splits, length, and number of words copied from the input. Acknowledgments Another long body of research focuses on a sin- We thank the anonymous reviewers for their valugle simplification operation and can be broadly di- able feedback. We thank Newsela for sharing the vided into three categories: (1) Lexical Simplifica- data and NVIDIA for providing GPU computing tion (Specia et al., 2012; Horn et al., 2014; Glavaš resources. This research is supported in part by and Štajner, 2015; Paetzold and Specia, 2017, 2015; the NSF award IIS-1822754, ODNI and IARPA Maddela and Xu, 2018; Qiang et al., 2020), where via the BETTER program contract 19051600004. complex words are substituted with simpler words. The views and conclusions contained herein are (2) Syntactic Simplification (Siddharthan, 2006; those of the authors and should not be interpreted Aharoni and Goldberg, 2018; Botha et al., 2018; as necessarily representing the official policies, eiNiklaus et al., 2019), which deals ex"
2021.naacl-main.277,P15-2135,0,0.0639369,"Missing"
2021.naacl-main.277,W16-3411,0,0.230014,"Missing"
2021.naacl-main.277,N18-2013,0,0.0397153,"Missing"
2021.naacl-main.277,D11-1038,0,0.0169351,"the output. (e) Bad substitution, where the model inserted an incorrect simpler phrase, and (e) Human Reference Errors, where the reference does not reflect the source sentence. Note that a simplification can belong to multiple error categories. Table 7 shows the examples of each category. 5 Related Work Before the advent of neural networks, text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules (Carroll et al., 1999; Siddharthan, 2002; Siddharthan et al., 2004) or data-driven methods based on parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Following neural machine translation, the trend changed 4.2 Error Analysis to performing all the operations together end-toTo understand the errors generated by our model, end (Zhang and Lapata, 2017; Nisioi et al., 2017; we manually classified 200 simplifications from the Zhao et al., 2018; Alva-Manchego et al., 2017; Vu 3543 Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) System"
2021.naacl-main.277,Q15-1021,1,0.94544,"Missing"
2021.naacl-main.277,P13-2123,0,0.235105,"Missing"
2021.naacl-main.277,2020.emnlp-main.91,0,0.0416977,"V . 2.2 Candidate Ranking We design a neural ranking model to score all the candidates that underwent splitting and deletion, V = {v1 , v2 , . . . , vn }, then feed the top-ranked one to the lexical paraphrasing model for the final output. We train the model on a standard text simplification corpus consisting of pairs of complex sentence x and manually simplified reference y. Scoring Function. To assess the “goodness” of each candidate vi during training, we define the gold scoring function g ∗ as a length-penalized BERTscore: g ∗ (vi , y) = e−λ||φvi −φy ||× BERT Score(vi , y) (1) BERTScore (Zhang et al., 2020b) is a text similarity metric that uses BERT (Devlin et al., 2019) embeddings to find soft matches between word pieces (Wu et al., 2016) instead of exact string matching. We introduce a length penalty to favor the candidates that are of similar length to the human reference y and penalize those that deviate from the target compression ratio φy . λ defines the extent of penalization and is set to 1 in our experiments. φvi represents the compression ratios of vi compared to the input x. In principle, other similarity metrics can also be used for scoring. candidate pair, and learn to minimize th"
2021.naacl-main.277,D18-1081,0,0.0331732,"Missing"
2021.naacl-main.277,N18-1063,0,0.0182214,"r- tions V = {v1 , v2 , . . . , vn } that have undergone splitting and deletion (§2.1). These intermediate den of sentence splitting (e.g., split at comma) and deletion (e.g., remove trailing preposition phrases) sentences are then used for two purposes: (1) Sedecisions to a separate component. Previous hy- lected by a pairwise neural ranking model (§2.2) based on the simplification quality and then rewritbrid approaches for simplification (Narayan and Gardent, 2014; Siddharthan and Mandya, 2014; ten by the paraphrasing component; (2) Used for data augmentation to improve the diversity of the Sulem et al., 2018c) used splitting and deletion paraphrasing model (§2.3). rules in a deterministic step before applying an MT-based paraphrasing model. In contrast, our 2.1 Splitting and Deletion approach provides a more flexible and dynamic inWe leverage the state-of-the-art system for structegration of linguistic rules with the neural models through ranking and data augmentation (Figure 1). tural simplification, called DisSim (Niklaus et al., that foWe compare our method to several state-of-the- 2019), to generate candidate simplifications 2 The English version cus on splitting and deletion. art systems in"
2021.naacl-main.277,D17-1062,0,0.241414,"r each sentence. 3.2 Existing Methods We use the following simplification approaches as baselines: (i) BERT-Initialized Transfomer (?), where the encoder is initialized with BERTbase checkpoint and the decoder is randomly initialized. It is the current state-of-the-art for text simplification (Jiang et al., 2020). (ii) EditNTS (Dong et al., 2019),6 another state-of-the-art model that uses a neural programmer-interpreter (Reed and de Freitas, 2016) to predict the edit operation on each word, and then generates the simplified sentence. (iii) LSTM baseline, a vanilla encoderdecoder model used in Zhang and Lapata (2017). (iv) Hybrid-NG (Narayan and Gardent, 2014),7 one of the best existing hybrid systems that performs splitting and deletion using a probabilistic model and lexical substitution with a phrase-based machine translation system. We retrained all the models on the N EWSLA -AUTO dataset. 5 We provide instructions in Appendix B https://github.com/yuedongP/EditNTS 7 https://github.com/shashiongithub/ Sentence-Simplification-ACL14 3540 6 Models SARI add Complex (input) 22.3 0.0 Simple (reference) 62.3 44.8 Hybrid-NG 38.2 2.8 36.0 3.3 Transformerbert EditNTS 37.4 1.6 Our Model 38.1 3.9 Our Model (no spl"
2021.naacl-main.277,P18-1016,0,0.0190053,"r- tions V = {v1 , v2 , . . . , vn } that have undergone splitting and deletion (§2.1). These intermediate den of sentence splitting (e.g., split at comma) and deletion (e.g., remove trailing preposition phrases) sentences are then used for two purposes: (1) Sedecisions to a separate component. Previous hy- lected by a pairwise neural ranking model (§2.2) based on the simplification quality and then rewritbrid approaches for simplification (Narayan and Gardent, 2014; Siddharthan and Mandya, 2014; ten by the paraphrasing component; (2) Used for data augmentation to improve the diversity of the Sulem et al., 2018c) used splitting and deletion paraphrasing model (§2.3). rules in a deterministic step before applying an MT-based paraphrasing model. In contrast, our 2.1 Splitting and Deletion approach provides a more flexible and dynamic inWe leverage the state-of-the-art system for structegration of linguistic rules with the neural models through ranking and data augmentation (Figure 1). tural simplification, called DisSim (Niklaus et al., that foWe compare our method to several state-of-the- 2019), to generate candidate simplifications 2 The English version cus on splitting and deletion. art systems in"
2021.naacl-main.277,P08-1040,0,0.269154,"Missing"
2021.naacl-main.277,D18-1355,0,0.0187326,"eural networks, text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules (Carroll et al., 1999; Siddharthan, 2002; Siddharthan et al., 2004) or data-driven methods based on parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Following neural machine translation, the trend changed 4.2 Error Analysis to performing all the operations together end-toTo understand the errors generated by our model, end (Zhang and Lapata, 2017; Nisioi et al., 2017; we manually classified 200 simplifications from the Zhao et al., 2018; Alva-Manchego et al., 2017; Vu 3543 Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) System Outputs Since 2010, project researchers have uncovered documents in Portugal that have revealed who owned the ship. Since 2010, experts have been figuring out who owned the ship. since 2010, the project scientists have uncovered documents in portugal that have about who owns the ship. since 2010, scientists have uncover"
2021.naacl-main.277,C10-1152,0,0.0856812,"esolve pronouns in the output. (e) Bad substitution, where the model inserted an incorrect simpler phrase, and (e) Human Reference Errors, where the reference does not reflect the source sentence. Note that a simplification can belong to multiple error categories. Table 7 shows the examples of each category. 5 Related Work Before the advent of neural networks, text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules (Carroll et al., 1999; Siddharthan, 2002; Siddharthan et al., 2004) or data-driven methods based on parallel corpora (Zhu et al., 2010; Woodsend and Lapata, 2011; Narayan and Gardent, 2014). Following neural machine translation, the trend changed 4.2 Error Analysis to performing all the operations together end-toTo understand the errors generated by our model, end (Zhang and Lapata, 2017; Nisioi et al., 2017; we manually classified 200 simplifications from the Zhao et al., 2018; Alva-Manchego et al., 2017; Vu 3543 Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) Our Model (cp = 0.8) Complex Simple Hybrid-NG LSTM Transformerbert EditNTS Our Model (cp = 0.6) Our Model (cp = 0.7) O"
C12-1177,P05-1074,0,0.0667073,"Missing"
C12-1177,N03-1003,0,0.0552004,"Missing"
C12-1177,D08-1021,0,0.0816547,"Missing"
C12-1177,C08-1013,0,0.0540476,"Missing"
C12-1177,C96-2183,0,0.047339,"Missing"
C12-1177,P11-1020,0,0.164662,"Missing"
C12-1177,P09-1053,0,0.0276086,"Missing"
C12-1177,C04-1051,1,0.720944,"Missing"
C12-1177,C04-1088,0,0.0176159,"Missing"
C12-1177,W07-1401,1,0.326763,"Missing"
C12-1177,D10-1051,0,0.0161918,"Missing"
C12-1177,P07-2045,0,0.0606319,"Missing"
C12-1177,N10-1017,0,0.0920108,"Missing"
C12-1177,D10-1090,0,0.0393625,"Missing"
C12-1177,J10-3003,0,0.0626954,"Missing"
C12-1177,moore-2002-fast,0,0.0606931,"Missing"
C12-1177,J03-1002,0,0.0531454,"Missing"
C12-1177,P02-1040,0,0.110879,"Missing"
C12-1177,W04-3219,0,0.0160521,"Missing"
C12-1177,P10-2008,0,0.0191379,"Missing"
C12-1177,W03-1609,0,0.0609861,"Missing"
C12-1177,D08-1027,0,0.0349755,"Missing"
C12-1177,N10-1056,0,0.0714576,"Missing"
C12-1177,P03-1021,0,\N,Missing
C18-1328,S14-2010,0,0.0286554,"the Quora website. This dataset has balanced positive and negative labels indicating whether the questions are duplicated or not. • Twitter-URL (Lan et al., 2017) includes 50k sentence pairs collected from tweets that share the same URL of news articles. This dataset contains both formal and informal language. • PIT-2015 (Xu et al., 2015) comes from SemEval-2015 and was collected from tweets under the same trending topic. It contains naturally occurred (i.e. written by independent Twitter users spontaneously) paraphrases and non-paraphrases with varied topics and language styles. • STS-2014 (Agirre et al., 2014) is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes (Hovy et al., 2006). • WikiQA (Yang et al., 2015) is an open-domain question-answering dataset. Following He and Lin (2016), questions without correct candidate answer sentences are excluded, and answer sentences are truncated to 40 tokens, resulting in 12k question-answer pairs for our experiments. • TrecQA (Wang et al., 2007) is an answer selection task of 56k question-answer pairs and created in Text Retrieval Conferences (TREC). For both WikiQA and TrecQA datasets, the b"
C18-1328,D15-1075,0,0.452321,"P tasks, including the following: • Semantic Textual Similarity (STS), which measures the degree of equivalence in the underlying semantics of paired snippets of text (Agirre et al., 2016). • Paraphrase Identification (PI), which identifies whether two sentences express the same meaning (Dolan and Brockett, 2005; Xu et al., 2015). • Natural Language Inference (NLI), also known as recognizing textual entailment (RTE), which concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise (Dagan et al., 2006; Bowman et al., 2015). • Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer. (Rajpurkar et al., 2016). Traditionally, researchers had to develop different methods specific for each task. Now neural networks can perform all the above tasks with the same architecture by training end to end. Various neural models (He and Lin, 2016; Chen et al., 2017;"
C18-1328,P16-1139,0,0.0541235,"Missing"
C18-1328,P17-1152,0,0.273281,"nsively across eight datasets, including paraphrase identification, semantic textual similarity, natural language inference, and question answering tasks. Although most of these models have claimed state-of-the-art performance, the original papers often reported on only one or two selected datasets. We provide a systematic study and show that (i) encoding contextual information by LSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter datasets, (iii) the Enhanced Sequential Inference Model (Chen et al., 2017) is the best so far for larger datasets, while the Pairwise Word Interaction Model (He and Lin, 2016) achieves the best performance when less data is available. We release our implementations as an open-source toolkit. 1 Introduction Sentence pair modeling is a fundamental technique underlying many NLP tasks, including the following: • Semantic Textual Similarity (STS), which measures the degree of equivalence in the underlying semantics of paired snippets of text (Agirre et al., 2016). • Paraphrase Identification (PI), which identifies whether two sentences express the same meaning (Dolan and"
C18-1328,D17-1070,0,0.229927,"mizations? This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: 3890 Proceedings of the 27th International Conference on Computational Linguistics, pages 3890–3902 Santa Fe, New Mexico, USA, August 20-26, 2018. http:// To answer these questions and better understand different network designs, we systematically analyze and compare the state-of-the-art neural models across multiple tasks and multiple domains. Namely, we implement five models and their variations on the same PyTorch platform: InferSent model (Conneau et al., 2017), Shortcut-stacked Sentence Encoder Model (Nie and Bansal, 2017), Pairwise Word Interaction Model (He and Lin, 2016), Decomposable Attention Model (Parikh et al., 2016), and Enhanced Sequential Inference Model (Chen et al., 2017). They are representative of the two most common approaches: sentence encoding models that learn vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance and sentence pair interaction models that use some sorts of word alignment mechanisms (e.g., attention) then aggregate inter-sentence inter"
C18-1328,I05-5002,0,0.193786,"l., 2017) is the best so far for larger datasets, while the Pairwise Word Interaction Model (He and Lin, 2016) achieves the best performance when less data is available. We release our implementations as an open-source toolkit. 1 Introduction Sentence pair modeling is a fundamental technique underlying many NLP tasks, including the following: • Semantic Textual Similarity (STS), which measures the degree of equivalence in the underlying semantics of paired snippets of text (Agirre et al., 2016). • Paraphrase Identification (PI), which identifies whether two sentences express the same meaning (Dolan and Brockett, 2005; Xu et al., 2015). • Natural Language Inference (NLI), also known as recognizing textual entailment (RTE), which concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise (Dagan et al., 2006; Bowman et al., 2015). • Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the tex"
C18-1328,N18-1132,0,0.127051,"c pooling + MLP (Yin et al., 2016) CNN attention matrix logistic regression DecAtt (Parikh et al., 2016) - summation + MLP PWIM (He and Lin, 2016) BiLSTM dot product + soft alignment cosine, Euclidean, dot product + hard alignment word-by-word neural attention MLP dot product + soft alignment average and max pooling + MLP (Shen et al., 2017b) (Choi et al., 2017) (Wieting and Gimpel, 2017) (Wang and Jiang, 2017) ESIM (Chen et al., 2017) LSTM encodes both context and attention BiLSTM (Tree-LSTM) before and after attention (Wang et al., 2017) BiLSTM (Shen et al., 2017a) BiLSTM + intra-attention (Ghaeini et al., 2018) dependent reading BiLSTM multi-perspective matching soft alignment + orthogonal decomposition dot product + soft alignment CNN + MLP BiLSTM + MLP MLP average and max pooling+MLP Table 1: Summary of representative neural models for sentence pair modeling. The upper half contains sentence encoding models, and the lower half contains sentence pair interaction models. (a) InferSent (b) SSE (c) Classification Layer Figure 1: Sentence encoding models focus on learning vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distan"
C18-1328,N16-1108,0,0.130166,"al language inference, and question answering tasks. Although most of these models have claimed state-of-the-art performance, the original papers often reported on only one or two selected datasets. We provide a systematic study and show that (i) encoding contextual information by LSTM and inter-sentence interactions are critical, (ii) Tree-LSTM does not help as much as previously claimed but surprisingly improves performance on Twitter datasets, (iii) the Enhanced Sequential Inference Model (Chen et al., 2017) is the best so far for larger datasets, while the Pairwise Word Interaction Model (He and Lin, 2016) achieves the best performance when less data is available. We release our implementations as an open-source toolkit. 1 Introduction Sentence pair modeling is a fundamental technique underlying many NLP tasks, including the following: • Semantic Textual Similarity (STS), which measures the degree of equivalence in the underlying semantics of paired snippets of text (Agirre et al., 2016). • Paraphrase Identification (PI), which identifies whether two sentences express the same meaning (Dolan and Brockett, 2005; Xu et al., 2015). • Natural Language Inference (NLI), also known as recognizing text"
C18-1328,D15-1181,0,0.302364,"mework: • The Input Embedding Layer takes vector representations of words as input, where pretrained word embeddings are most commonly used, e.g. GloVe (Pennington et al., 2014) or Word2vec (Mikolov et al., 2013). Some work used embeddings specially trained on phrase or sentence pairs that are paraphrases (Wieting and Gimpel, 2017; Tomar et al., 2017); some used subword embeddings, which showed improvement on social media data (Lan and Xu, 2018). • The Context Encoding Layer incorporates word context and sequence order into modeling for better vector representation. This layer often uses CNN (He et al., 2015), LSTM (Chen et al., 2017), recursive neural network (Socher et al., 2011), or highway network (Gong et al., 2017). The sentence encoding type of model will stop at this step, and directly use the encoded vectors to compute the semantic similarity through vector distances and/or the output classification layer. • The Interaction and Attention Layer calculates word pair (or n-gram pair) interactions using the outputs of the encoding layer. This is the key component for the interaction-aggregation type of model. In the PWIM model (He and Lin, 2016), the interactions are calculated by cosine simi"
C18-1328,N16-1162,0,0.0235261,"g vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance. 3892 3.1 The Bi-LSTM Max-pooling Network (InferSent) We choose the simple Bi-LSTM max-pooling network from InferSent (Conneau et al., 2017): ← → ← → h i = BiLST M (xi , h i−1 ) (1) ← → ← → ← → v = max( h 1 , h 2 , ..., h n ) (2) ← → where h i represents the concatenation of hidden states in both directons. It has shown better transfer learning capabilities than several other sentence embedding models, including SkipThought (Kiros et al., 2015) and FastSent (Hill et al., 2016), when trained on the natural language inference datasets. 3.2 The Shortcut-Stacked Sentence Encoder Model (SSE) The Shortcut-Stacked Sentence Encoder model (Nie and Bansal, 2017) is a sentence-based embedding model, which enhances multi-layer Bi-LSTM with skip connection to avoid training error accumulation, and calculates each layer as follows: ← →k ← → h i = BiLST M (xki , h ki−1 ) (3) ← → ← → ← → x1i = wi (k = 1), xki = [wi , h k−1 , h k−2 , ..., h 1i ] (k &gt; 1) (4) i i ← →m ← →m ← →m v = max( h 1 , h 2 , ..., h n ) (5) where xki is the input of the kth Bi-LSTM layer at time step i, which i"
C18-1328,N06-2015,0,0.041746,"RL (Lan et al., 2017) includes 50k sentence pairs collected from tweets that share the same URL of news articles. This dataset contains both formal and informal language. • PIT-2015 (Xu et al., 2015) comes from SemEval-2015 and was collected from tweets under the same trending topic. It contains naturally occurred (i.e. written by independent Twitter users spontaneously) paraphrases and non-paraphrases with varied topics and language styles. • STS-2014 (Agirre et al., 2014) is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes (Hovy et al., 2006). • WikiQA (Yang et al., 2015) is an open-domain question-answering dataset. Following He and Lin (2016), questions without correct candidate answer sentences are excluded, and answer sentences are truncated to 40 tokens, resulting in 12k question-answer pairs for our experiments. • TrecQA (Wang et al., 2007) is an answer selection task of 56k question-answer pairs and created in Text Retrieval Conferences (TREC). For both WikiQA and TrecQA datasets, the best answer is selected according to the semantic relatedness with the question. 4.2 Implementation Details We implement all the models with"
C18-1328,D17-1126,1,0.795914,"have avoided a catastrophe. sb : Then we might have been able to avoid a disaster. score [0, 5] 4.6 sa : How much is 1 tablespoon of water? sb : In Australia one tablespoon (measurement unit) is 20 mL. true false sa : Who was Lincoln’s Secretary of State? sb : William Seward true false Table 2: Basic statistics and examples of different datasets for sentence pair modeling tasks. • Quora (Iyer et al., 2017) contains 400k question pairs collected from the Quora website. This dataset has balanced positive and negative labels indicating whether the questions are duplicated or not. • Twitter-URL (Lan et al., 2017) includes 50k sentence pairs collected from tweets that share the same URL of news articles. This dataset contains both formal and informal language. • PIT-2015 (Xu et al., 2015) comes from SemEval-2015 and was collected from tweets under the same trending topic. It contains naturally occurred (i.e. written by independent Twitter users spontaneously) paraphrases and non-paraphrases with varied topics and language styles. • STS-2014 (Agirre et al., 2014) is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes (Hovy et al., 2006). •"
C18-1328,D16-1176,0,0.0341161,"Missing"
C18-1328,W17-5308,0,0.530834,"ution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: 3890 Proceedings of the 27th International Conference on Computational Linguistics, pages 3890–3902 Santa Fe, New Mexico, USA, August 20-26, 2018. http:// To answer these questions and better understand different network designs, we systematically analyze and compare the state-of-the-art neural models across multiple tasks and multiple domains. Namely, we implement five models and their variations on the same PyTorch platform: InferSent model (Conneau et al., 2017), Shortcut-stacked Sentence Encoder Model (Nie and Bansal, 2017), Pairwise Word Interaction Model (He and Lin, 2016), Decomposable Attention Model (Parikh et al., 2016), and Enhanced Sequential Inference Model (Chen et al., 2017). They are representative of the two most common approaches: sentence encoding models that learn vector representations of individual sentences and then calculate the semantic relationship between sentences based on vector distance and sentence pair interaction models that use some sorts of word alignment mechanisms (e.g., attention) then aggregate inter-sentence interactions. We focus on identifying important network designs and p"
C18-1328,D16-1244,0,0.257777,"Missing"
C18-1328,D14-1162,0,0.0938715,"to a fixed-length vector and then computes sentence similarity directly. The model of this type has advantages in the simplicity of the network design and generalization to other NLP tasks. The sentence pair interaction approach takes word alignment and interactions between the sentence pair into account and often show better performance when trained on in-domain data. Here we outline the two types of neural networks under the same general framework: • The Input Embedding Layer takes vector representations of words as input, where pretrained word embeddings are most commonly used, e.g. GloVe (Pennington et al., 2014) or Word2vec (Mikolov et al., 2013). Some work used embeddings specially trained on phrase or sentence pairs that are paraphrases (Wieting and Gimpel, 2017; Tomar et al., 2017); some used subword embeddings, which showed improvement on social media data (Lan and Xu, 2018). • The Context Encoding Layer incorporates word context and sequence order into modeling for better vector representation. This layer often uses CNN (He et al., 2015), LSTM (Chen et al., 2017), recursive neural network (Socher et al., 2011), or highway network (Gong et al., 2017). The sentence encoding type of model will stop"
C18-1328,D16-1264,0,0.042835,"ge Inference (NLI), also known as recognizing textual entailment (RTE), which concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise (Dagan et al., 2006; Bowman et al., 2015). • Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer. (Rajpurkar et al., 2016). Traditionally, researchers had to develop different methods specific for each task. Now neural networks can perform all the above tasks with the same architecture by training end to end. Various neural models (He and Lin, 2016; Chen et al., 2017; Parikh et al., 2016; Wieting et al., 2016; Tomar et al., 2017; Wang et al., 2017; Shen et al., 2017a; Yin et al., 2016) have declared state-of-the-art results for sentence pair modeling tasks; however, they were carefully designed and evaluated on selected (often one or two) datasets that can demonstrate the superiority of the model. The research qu"
C18-1328,D17-1122,0,0.0741819,"sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer. (Rajpurkar et al., 2016). Traditionally, researchers had to develop different methods specific for each task. Now neural networks can perform all the above tasks with the same architecture by training end to end. Various neural models (He and Lin, 2016; Chen et al., 2017; Parikh et al., 2016; Wieting et al., 2016; Tomar et al., 2017; Wang et al., 2017; Shen et al., 2017a; Yin et al., 2016) have declared state-of-the-art results for sentence pair modeling tasks; however, they were carefully designed and evaluated on selected (often one or two) datasets that can demonstrate the superiority of the model. The research questions are as follows: Do they perform well on other tasks and datasets? How much performance gain is due to certain system design choices and hyperparameter optimizations? This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: 3890 Proceedings of the 27th Inte"
C18-1328,W17-4121,0,0.0370004,"Missing"
C18-1328,D07-1003,0,0.0499102,"urred (i.e. written by independent Twitter users spontaneously) paraphrases and non-paraphrases with varied topics and language styles. • STS-2014 (Agirre et al., 2014) is from SemEval-2014, constructed from image descriptions, news headlines, tweet news, discussion forums, and OntoNotes (Hovy et al., 2006). • WikiQA (Yang et al., 2015) is an open-domain question-answering dataset. Following He and Lin (2016), questions without correct candidate answer sentences are excluded, and answer sentences are truncated to 40 tokens, resulting in 12k question-answer pairs for our experiments. • TrecQA (Wang et al., 2007) is an answer selection task of 56k question-answer pairs and created in Text Retrieval Conferences (TREC). For both WikiQA and TrecQA datasets, the best answer is selected according to the semantic relatedness with the question. 4.2 Implementation Details We implement all the models with the same PyTorch framework.23 Below, we summarize the implementation details that are key for reproducing results for each model: • SSE: This model can converge very fast, for example, 2 or 3 epochs for the SNLI dataset. We control 1 the convergence speed by updating the learning rate for each epoch: specific"
C18-1328,P17-1190,0,0.134083,"generalization to other NLP tasks. The sentence pair interaction approach takes word alignment and interactions between the sentence pair into account and often show better performance when trained on in-domain data. Here we outline the two types of neural networks under the same general framework: • The Input Embedding Layer takes vector representations of words as input, where pretrained word embeddings are most commonly used, e.g. GloVe (Pennington et al., 2014) or Word2vec (Mikolov et al., 2013). Some work used embeddings specially trained on phrase or sentence pairs that are paraphrases (Wieting and Gimpel, 2017; Tomar et al., 2017); some used subword embeddings, which showed improvement on social media data (Lan and Xu, 2018). • The Context Encoding Layer incorporates word context and sequence order into modeling for better vector representation. This layer often uses CNN (He et al., 2015), LSTM (Chen et al., 2017), recursive neural network (Socher et al., 2011), or highway network (Gong et al., 2017). The sentence encoding type of model will stop at this step, and directly use the encoded vectors to compute the semantic similarity through vector distances and/or the output classification layer. • T"
C18-1328,S15-2001,1,0.94127,"ar for larger datasets, while the Pairwise Word Interaction Model (He and Lin, 2016) achieves the best performance when less data is available. We release our implementations as an open-source toolkit. 1 Introduction Sentence pair modeling is a fundamental technique underlying many NLP tasks, including the following: • Semantic Textual Similarity (STS), which measures the degree of equivalence in the underlying semantics of paired snippets of text (Agirre et al., 2016). • Paraphrase Identification (PI), which identifies whether two sentences express the same meaning (Dolan and Brockett, 2005; Xu et al., 2015). • Natural Language Inference (NLI), also known as recognizing textual entailment (RTE), which concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise (Dagan et al., 2006; Bowman et al., 2015). • Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that cont"
C18-1328,D15-1237,0,0.180414,"t al., 2016). • Paraphrase Identification (PI), which identifies whether two sentences express the same meaning (Dolan and Brockett, 2005; Xu et al., 2015). • Natural Language Inference (NLI), also known as recognizing textual entailment (RTE), which concerns whether a hypothesis can be inferred from a premise, requiring understanding of the semantic similarity between the hypothesis and the premise (Dagan et al., 2006; Bowman et al., 2015). • Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer. (Rajpurkar et al., 2016). Traditionally, researchers had to develop different methods specific for each task. Now neural networks can perform all the above tasks with the same architecture by training end to end. Various neural models (He and Lin, 2016; Chen et al., 2017; Parikh et al., 2016; Wieting et al., 2016; Tomar et al., 2017; Wang et al., 2017; Shen et al., 2017a; Yin et al., 2016) have declared state-of-the-art results for sentence p"
C18-1328,Q16-1019,0,0.0944976,"based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer. (Rajpurkar et al., 2016). Traditionally, researchers had to develop different methods specific for each task. Now neural networks can perform all the above tasks with the same architecture by training end to end. Various neural models (He and Lin, 2016; Chen et al., 2017; Parikh et al., 2016; Wieting et al., 2016; Tomar et al., 2017; Wang et al., 2017; Shen et al., 2017a; Yin et al., 2016) have declared state-of-the-art results for sentence pair modeling tasks; however, they were carefully designed and evaluated on selected (often one or two) datasets that can demonstrate the superiority of the model. The research questions are as follows: Do they perform well on other tasks and datasets? How much performance gain is due to certain system design choices and hyperparameter optimizations? This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: 3890 Proceedings of the 27th International Conference"
C18-1328,S16-1082,0,\N,Missing
D11-1119,P10-1089,0,0.400416,"y slightly. 1 Introduction The function of context-sensitive text correction is to identify word-choice errors in text (Bergsma et al., 2009). It can be viewed as a lexical disambiguation task (Lapata and Keller, 2005), where a system selects from a predefined confusion word set, such as {affect, effect} or {complement, compliment}, and provides the most appropriate word choice given the context. Typically, one determines if a word has been used correctly based on lexical, syntactic and semantic information from the context of the word. One of the top performing models of spelling correction (Bergsma et al., 2010) is based on web-scale n-gram counts, which reflect both syntax and meaning. However, even with a large-scale n-gram corpus, data sparsity can hurt performance in two ways. ∗ This work was done when the first author was an intern for Educational Testing Service. Take a sentence from The New York Times (NYT) for example: “‘This fellow’s won a war,’ the dean of the capital’s press corps, David Broder, announced on ‘Meet the Press’ after complimenting the president on the ‘great sense of authority and command’ he exhibited in a flight suit.” Unfortunately, neither the phrase “complementing the pr"
D11-1119,W07-1604,1,0.878565,"antic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for"
D11-1119,W02-1005,0,0.0287773,"udanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model"
D11-1119,de-marneffe-etal-2006-generating,0,0.0106983,"Missing"
D11-1119,P08-2008,0,0.0201236,"ey focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the tes"
D11-1119,P97-1067,0,0.0409743,"mplimenting the president” exists in the web-scale Google N-gram corpus (Brants and Franz, 2006). The n-gram models decide solely based on the frequency of the bi-grams “after comple(i)menting” and “comple(i)menting the”, which are common usages for both words. The real question is whether we are more likely to “compliment” or “complement” a person, the “president”. Several clues could help us answer that question. A dependency parser can identify the word “president” as the subject of “compliment” or “complement” which also may be the case in some of the training data. Lexical co-occurrence (Edmonds, 1997) and semantic word relatedness measurements, such as Random Indexing (Sahlgren, 2006), could provide evidence that “compliment” is more likely to co-occur with “president” than “complement”. Fur1291 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291–1300, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics thermore, some important clues can be quite distant from the target word, e.g. outside the 9-word context window Bergsma et al. (2010) and Carlson (2007) used. Consider another sentence in the NYT corpus,"
D11-1119,P98-1059,0,0.0306819,"sight, site}, {peace, piece} and {raise, rise}. They reported that the SVM model with NG features outperformed its unsupervised version, sumLM. However, the limited confusion word sets they evaluated may not comprehensively represent the word usage errors that writers typically make. In this paper, we test nine additional commonly confused word pairs to expand the scope of the evaluation. These words were selected based on their lower frequencies compared to the five pairs in the above work (as shown later in Table 2). 1293 3 Enhanced N-gram Models with Parse Features To our knowledge, only (Elmi and Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et"
D11-1119,C08-1022,0,0.0241586,"Missing"
D11-1119,I08-1059,0,0.0242171,"use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for contextual spelling correction and other lexical disambiguation tasks. These systems make the word choice depending on how frequently each candidate word has been seen in the given context in web-scale data. As n-gram data has become more readily available, such as the Google N-gram Corpus, the likelihood of a word being used in a certain context can be better estimated. Bergsma et al. (2009; 2010) presented a series of simple but powerful models which relied heavily on web-scale n-gram counts."
D11-1119,W95-0104,0,0.419802,"e intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has be"
D11-1119,hermet-etal-2008-using,0,0.0660695,"Missing"
D11-1119,P03-2026,0,0.0382898,"redicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to"
D11-1119,A97-1025,0,0.0775056,"and how our approach differs from these approaches. In Sections 3 and 4, we discuss our methods for using parse features and word co-occurrence information. In Section 5, we present experimental results and analysis. 2 Related Work A variety of approaches have been proposed for context-sensitive spelling correction ranging from semantic methods to machine learning classifiers to large-scale n-gram models. Some semantics-based systems have been developed based on an intuitive assumption that the intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), trans"
D11-1119,W06-1605,0,0.0690266,"Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for conte"
D11-1119,N10-1018,0,0.0119634,"ny word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion words. The problem with mor"
D11-1119,C08-1109,1,0.808685,"istance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In"
D11-1119,P10-2065,1,0.802253,"Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by differ"
D11-1119,D09-1093,0,0.0288853,"g errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion"
D11-1119,C98-1057,0,\N,Missing
D16-1030,P13-1009,0,0.371269,"e (Chang and Manning, 2012), outperforming other state-of-the-art time expression resolvers HeidelTime (Str¨otgen and Gertz, 2013), TempEX (Mani and Wilson, 2000) and UWTime (Lee et al., 2014) as well. Our approach also produces a confidence score that allows us to trade recall for precision. To the best of our knowledge, TweeTIME is the first time resolver designed specifically for social media data.3 This is also the first time that distant supervision is successfully applied for end-to-end temporal recognition and normalization. Previous distant supervision approaches (Angeli et al., 2012; Angeli and Uszkoreit, 2013) only address the normalization problem, assuming gold time mentions are available at test time. 2 System Overview Our TweeTIME system consists of two major components as shown in Figure 3: 1. A Temporal Recognizer which identifies time expressions (e.g. Monday) in English text and outputs 5 different temporal types (described in Table 1) indicating timeline direction, month of year, date of month, day of week or no temporal information (NA). It is realized as a multipleinstance learning model, and in an enhanced version, as a missing data model. Based on this assumption, tweets that contain t"
D16-1030,N12-1049,0,0.221093,"ns. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news arti"
D16-1030,D14-1164,0,0.0140877,") Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision"
D16-1030,S16-1165,0,0.191973,"blicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical texts (UzZaman et al., 2013; Bethard and Savova, 2016). Resolving time expressions in social media is a non-trivial problem. Besides many spelling variations, time expressions are more likely to refer to future dates than in newswire. For the example in 307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 307–318, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Figure 1, we need to recognize that Monday refers to the upcoming Monday and not the previous one to resolve to its correct normalized date (5/9/2016). We also need to identify that the word Sun is not referri"
D16-1030,S13-2002,0,0.189164,"-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal re"
D16-1030,D13-1078,0,0.0946954,"-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal re"
D16-1030,P07-1073,0,0.0459083,"e (SUTime, HeidelTime) None (SUTime, HeidelTime) 2002-03-12 2015-03-12 (TweeTime) None 2015-12-08 (TweeTime) Table 7: Representative Examples of System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2"
D16-1030,S13-2012,0,0.0222482,"ion extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and theref"
D16-1030,chang-manning-2012-sutime,0,0.0332083,"ction: Unlike rule-based systems, TweeTIME has a tendency to over-predict when there is no explicit time expression in the tweets, possibly because of the presence of present tense verbs. Such mistakes could also happen in some past tense verbs. Because TweeTIME resolves time expressions using a very different approach compared to traditional methods, its distribution of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova,"
D16-1030,N16-1045,0,0.0564644,"Missing"
D16-1030,P13-2114,0,0.0228848,"ished on Friday 5/6/2016 that contains the temporal expression Monday referring to the date of the event (5/9/2016), which a generic temporal tagger failed to resolve correctly. Figure 2: A tweet that contains a simple explicit time mention and an event (Mercury, 5/9/2016) that can be identified by an Introduction open-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requi"
D16-1030,D12-1062,0,0.0301003,"tains the temporal expression Monday referring to the date of the event (5/9/2016), which a generic temporal tagger failed to resolve correctly. Figure 2: A tweet that contains a simple explicit time mention and an event (Mercury, 5/9/2016) that can be identified by an Introduction open-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temp"
D16-1030,S13-2009,0,0.107116,"ferent approach compared to traditional methods, its distribution of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova, 2016) offers an evaluation in the clinical domain besides newswire, most participants used the provided annotated corpus to train supervised models in addition to employing hand-coded rules. Previous work on adapting temporal taggers primarily focus on scaling up to more languages. HeidelTime was exten"
D16-1030,P11-1055,0,0.240785,"future (TL=f uture). The multiple instance learning assumption implies that at least one word must be tagged with each of these present temporal tags. For example, ideally after training, the model will learn to assign z8 to tag a and z1 to tag b. lem, we never directly observe the words’ tags (z = z1 , . . . , zn ) during learning. Instead, they are latent and we only observe the date of an event mentioned in the text, from which we derive sentencelevel binary variables t = t1 , . . . , tk corresponding to temporal tags for the sentence. Following previous work on multiple-instance learning (Hoffmann et al., 2011a; Xu et al., 2014), we model the connection between sentence-level labels and word-level tags using a set of deterministic-OR factors φsent . The overall conditional probability of our model is defined as: P (t, z|w; θ r ) k n Y 1 Y sent = φ (ti , z) × φword (zj , wj ) Z = 1 Z i=1 j=1 k Y n Y i=1 φsent (ti , z) × eθ r ·f(z (1) j ,wj ) j=1 where f(zj , wj ) is a feature vector and   1 if ti = true ∧ ∃j : zj = i sent φ (ti , z) = 1 if ti = f alse ∧ ∀j : zj 6= i   0 otherwise (2) We include a standard set of tagging features that 310 X X t,z z P (z|w, t; θ r ) · f(z, w) P (t, z|w; θ r ) · f"
D16-1030,S10-1072,0,0.027782,"E resolves time expressions using a very different approach compared to traditional methods, its distribution of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova, 2016) offers an evaluation in the clinical domain besides newswire, most participants used the provided annotated corpus to train supervised models in addition to employing hand-coded rules. Previous work on adapting temporal taggers primarily focus on scaling u"
D16-1030,P14-1135,0,0.353564,"pressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical t"
D16-1030,P00-1010,0,0.664434,"by an Introduction open-domain information extraction system. Temporal expressions are words or phrases that refer to dates, times or durations. Resolving time expressions is an important task in information extraction (IE) that enables downstream applications such as calendars or timelines of events (Derczynski and Gaizauskas, 2013; Do et al., 2012; Ritter et al., 2012; Ling and Weld, 2010), knowledge base population (Ji et al., 2011), information retrieval (Alonso et al., 2007), automatically scheduling meetings from email and more. Previous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses gene"
D16-1030,E12-1062,0,0.0812064,"They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision. We presented a method for learning a wordlevel temporal tagging models from tweets that are heuristically labeled with only sentence-level labels. This approach was further extended to account for 3"
D16-1030,P09-1113,0,0.532674,"from the list that are also written within ±7 days of the event. These tweets and the dates of the known events serve as labeled examples that are likely to mention a known date. We also include a set of pseudo-negative examples, that are unlikely to refer to any event, by gathering a random sample of tweets that do not mention any of the top 10, 000 events and where TempEx does not extract any date. 5.2 Large-Scale Heuristic Evaluation We first evaluate our tagging model, by testing how well it can predict the heuristically generated labels. As noted in previous work on distant supervision (Mintz et al., 2009a), this type of evaluation usually under-estimates precision, however it provides us with a useful intrinsic measure of performance. In order to provide even coverage of months in the training and test set, we divide the twitter corpus into 3 subsets based on the mod-5 week of each tweet’s creation date. To train system we use tweets that are created in 1st, 2nd or 3rd weeks. To tune parameters of the MiDaT model we used tweets from 5th weeks, and to evaluate the performance of the trained model we used tweets from 4th weeks. MultiT MiDaT Precision 0.61 0.67 Recall 0.21 0.31 F-value 0.32 0.42"
D16-1030,P11-2048,0,0.0165397,"12-08 (TweeTime) Table 7: Representative Examples of System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large"
D16-1030,C14-1168,0,0.0239454,"013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision. We presented a method for learning a wordlevel temporal tagging models from tweets that are heuristically labe"
D16-1030,E12-1049,0,0.0297372,"y, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events as distant supervision. We presented a method for learning a wordlevel temporal tagging models from tweets that are heuristically labeled with only sentence-level labels. Th"
D16-1030,D11-1141,1,0.930792,"f model training. Temporal Types Timeline (TL) Day of Week (DOW) Day of Month (DOM) Month of Year (MOY) None (NA) Possible Values (tags) past, present, f uture M on, T ue, . . . , Sun 1, 2, 3, . . . , 31 Jan, F eb, . . . , Dec NA Table 1: Our Temporal Recognizer can extract five different temporal types and assign one of their values to each word of a tweet. proposed by Ritter et al. (2012). Each event consists of one or more named entities, in addition to the date on which the event takes place, for example [Mercury, 5/9/2016]. Tweets are first processed by a Twitter named entity recognizer (Ritter et al., 2011), and a generic date resolver (Mani and Wilson, 2000). Events are then extracted based on the strength of association between each named entity and calendar date, as measured by a G2 test on their co-occurrence counts. More details of the Event Extractor can be found in Section 5.1. The following two sections describe the details of our Temporal Recognizer and Temporal Normalizer separately. 3 Distant Supervision for Recognizing Time Expressions The goal of the recognizer is to predict the temporal tag of each word, given a sentence (or a tweet) w = w1 , . . . , wn . We propose a multiple-inst"
D16-1030,Q13-1030,1,0.841866,"HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a database of known events"
D16-1030,N15-1044,0,0.0215173,"ous work in this area has applied rule-based systems (Mani and Wilson, 2000; Bethard, 2013b; Chambers, 2013) or supervised machine learning on small collections of hand-annotated news documents (Angeli et al., 2012; Lee et al., 2014). 1 Our code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical texts (UzZaman et al., 2013; Bethard and Savova, 2016). Resolving time expressions in social media is a non-trivial problem. Besides many spelling variations, time expressions are more likely to refer to future dates than in newswire. For the example in 307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 307–318, c Austin, Texas, November 1-5"
D16-1030,strotgen-gertz-2012-temporal,0,0.0735469,"Missing"
D16-1030,D15-1063,0,0.105871,"Missing"
D16-1030,D12-1042,0,0.022492,"presentative Examples of System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled t"
D16-1030,S10-1062,0,0.0994182,"ion of errors is quite distinct, as illustrated in Figure 6. 6 Related Work Temporal Resolvers primarily utilize either rulebased or probabilistic approaches. Notable rulebased systems such as TempEx (Mani and Wilson, 2000), SUTime (Chang and Manning, 2012) and HeidelTime (Str¨otgen and Gertz, 2013) provide particularly competitive performance compared to the state-of-the-art machine learning methods. Probabilistic approaches use supervised classifiers trained on in-domain annotated data (Kolomiyets and Moens, 2010; Bethard, 2013a; Filannino et al., 2013) or hybrid with hand-engineered rules (UzZaman and Allen, 2010; Lee et al., 2014). UWTime (Lee et al., 2014) is one of the most recent and competitive systems and uses Combinatory Categorial Grammar (CCG). Although the recent research challenge TempEval (UzZaman et al., 2013; Bethard and Savova, 2016) offers an evaluation in the clinical domain besides newswire, most participants used the provided annotated corpus to train supervised models in addition to employing hand-coded rules. Previous work on adapting temporal taggers primarily focus on scaling up to more languages. HeidelTime was extended to multilingual (Str¨otgen and Gertz, 2015), colloquial (S"
D16-1030,S13-2001,0,0.415971,"r code and data are publicly available at https:// github.com/jeniyat/TweeTime. Social media especially contains time-sensitive information and requires accurate temporal analysis, for example, for detecting real-time cybersecurity events (Ritter et al., 2015; Chang et al., 2016), disease outbreaks (Kanhabua et al., 2012) and extracting personal information (Schwartz et al., 2015). However, most work on social media simply uses generic temporal resolvers and therefore suffers from suboptimal performance. Recent work on temporal resolution focuses primarily on news articles and clinical texts (UzZaman et al., 2013; Bethard and Savova, 2016). Resolving time expressions in social media is a non-trivial problem. Besides many spelling variations, time expressions are more likely to refer to future dates than in newswire. For the example in 307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 307–318, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Figure 1, we need to recognize that Monday refers to the upcoming Monday and not the previous one to resolve to its correct normalized date (5/9/2016). We also need to identify that"
D16-1030,P13-2117,1,0.82717,"f System (SUTime, HeidelTime, TweeTIME) Errors annotated data. One existing work used distant supervision (Angeli et al., 2012; Angeli and Uszkoreit, 2013), but for normalization only, assuming gold time mentions as input. They used an EM-style bootstrapping approach and a CKY parser. Distant Supervision has recently become popular in natural language processing. Much of the work has focused on the task of relation extraction (Craven and Kumlien, 1999; Bunescu and Mooney, 2007; Mintz et al., 2009b; Riedel et al., 2010; Hoffmann et al., 2011b; Nguyen and Moschitti, 2011; Surdeanu et al., 2012; Xu et al., 2013; Ritter et al., 2013; Angeli et al., 2014). Recent work also shows exciting results on extracting named entities (Ritter et al., 2011; Plank et al., 2014), emotions (Purver and Battersby, 2012), sentiment (Marchetti-Bowick and Chambers, 2012), as well as finding evidence in medical publications (Wallace et al., 2016). Our work is closely related to the joint word-sentence model that exploits multiple-instance learning for paraphrase identification (Xu et al., 2014) in Twitter. 7 Conclusions In this paper, we showed how to learn time resolvers from large amounts of unlabeled text, using a data"
D16-1030,Q14-1034,1,0.944486,"multiple instance learning assumption implies that at least one word must be tagged with each of these present temporal tags. For example, ideally after training, the model will learn to assign z8 to tag a and z1 to tag b. lem, we never directly observe the words’ tags (z = z1 , . . . , zn ) during learning. Instead, they are latent and we only observe the date of an event mentioned in the text, from which we derive sentencelevel binary variables t = t1 , . . . , tk corresponding to temporal tags for the sentence. Following previous work on multiple-instance learning (Hoffmann et al., 2011a; Xu et al., 2014), we model the connection between sentence-level labels and word-level tags using a set of deterministic-OR factors φsent . The overall conditional probability of our model is defined as: P (t, z|w; θ r ) k n Y 1 Y sent = φ (ti , z) × φword (zj , wj ) Z = 1 Z i=1 j=1 k Y n Y i=1 φsent (ti , z) × eθ r ·f(z (1) j ,wj ) j=1 where f(zj , wj ) is a feature vector and   1 if ti = true ∧ ∃j : zj = i sent φ (ti , z) = 1 if ti = f alse ∧ ∀j : zj 6= i   0 otherwise (2) We include a standard set of tagging features that 310 X X t,z z P (z|w, t; θ r ) · f(z, w) P (t, z|w; θ r ) · f(z, w) (3) This gra"
D17-1126,J08-4004,0,0.0788751,"Missing"
D17-1126,P05-1074,0,0.381777,"Missing"
D17-1126,W03-1004,0,0.0973994,"Missing"
D17-1126,N03-1003,0,0.484548,"Missing"
D17-1126,S14-2114,0,0.0592908,"Missing"
D17-1126,D08-1021,0,0.104947,"Missing"
D17-1126,P11-1020,0,0.19134,"Missing"
D17-1126,J08-4005,0,0.0735032,"Missing"
D17-1126,P09-1053,0,0.0926331,"s and nonparaphrases: MSR Paraphrase Corpus [MSRP] (Dolan et al., 2004; Dolan and Brockett, 2005) This corpus contains 5,801 pairs of sentences from news articles, with 4,076 for training and the remaining 1,725 for testing. It was created from clustered news articles by using an SVM classifier (using 3 Another 12 name variations are omitted in the paper due to their offensive nature. features including string similarity and WordNet synonyms) to gather likely paraphrases, then annotated by human on semantic equivalence. The MSRP corpus has a known deficiency skewed toward over-identification (Das and Smith, 2009), because the “purpose was not to evaluate the potential effectiveness of the classifier itself, but to identify a reasonably large set of both positive and plausible ‘near-miss’ negative examples” (Dolan and Brockett, 2005). It contains a large portion of sentence pairs with many ngrams shared in common. Twitter Paraphrase Corpus [PIT-2015] (Xu et al., 2014, 2015) This corpus was derived from Twitter’s trending topic data. The training set contains 13,063 sentence pairs on 400 distinct topics, and the test set contains 972 sentence pairs on 20 topics. As numerous Twitter users spontaneously t"
D17-1126,I05-5002,0,0.6129,"; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks. In this paper, we address a major challenge in paraphrase research — the lack of parallel corpora. There are only two publicly available datasets of naturally occurring sentential paraphrases and non-paraphrases:2 the MSRP corpus derived from clustered news articles (Dolan and Brockett, 2005) and the PIT-2015 corpus from Twitter trending topics (Xu et al., 2014, 2015). Our goal is not only to create a new annotated paraphrase corpus, but to identify a new data source and method that can narrow down the search space of paraphrases without using the classifier-biased or human-in-the-loop data selection as in MSRP and PIT-2015. This is so that sentential paraphrases can be conveniently and continuously harvested in large quantities to benefit downstream applications. We present an effective method to collect sentential paraphrases from tweets that refer to the same URL and contribute"
D17-1126,C04-1051,0,0.739448,"iew, FBI supports CIA assertion, FBIClapper back CIA’s view, The FBI backs the CIA’s assessment, FBI Backs CIA, Donald Trump, DJT, Mr Trump, Donald @realTrump, D*nald Tr*mp, Comrade #Trump, GOPTrump, Preselect Trump, President-Elect Trump, President-elect Donald J. Trump, PEOTUS Trump, He-Who-MustNot-Be-Named3 Table 2: Up-to-date phrasal paraphrases automatically extracted from Twitter with our new method. 2 Existing Paraphrase Corpora and Their Limitations To date, there exist only two publicly available corpora of both sentential paraphrases and nonparaphrases: MSR Paraphrase Corpus [MSRP] (Dolan et al., 2004; Dolan and Brockett, 2005) This corpus contains 5,801 pairs of sentences from news articles, with 4,076 for training and the remaining 1,725 for testing. It was created from clustered news articles by using an SVM classifier (using 3 Another 12 name variations are omitted in the paper due to their offensive nature. features including string similarity and WordNet synonyms) to gather likely paraphrases, then annotated by human on semantic equivalence. The MSRP corpus has a known deficiency skewed toward over-identification (Das and Smith, 2009), because the “purpose was not to evaluate the pot"
D17-1126,P13-1158,0,0.188393,"Missing"
D17-1126,N15-1184,0,0.0752019,"Missing"
D17-1126,N13-1092,0,0.104065,"Missing"
D17-1126,P11-2008,0,0.0612027,"Missing"
D17-1126,D16-1236,0,0.0413576,"Missing"
D17-1126,P12-1091,0,0.0516709,"Missing"
D17-1126,C14-1047,0,0.0610719,"Missing"
D17-1126,P11-1109,0,0.0919282,"Missing"
D17-1126,N16-1108,1,0.903708,"Missing"
D17-1126,W11-2123,0,0.0348843,"Missing"
D17-1126,D13-1090,0,0.0661736,"Missing"
D17-1126,P07-2045,0,0.00619346,"Missing"
D17-1126,D16-1237,0,0.0404461,"wn benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al., 1 The code and data can be obtained from the first and last author’s websites. 2015), machine translation (Mehdizadeh Seraj et al., 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks. In this paper, we address a major challenge in paraphrase research — the lack of parallel corpora. There are only two publicly available datasets of naturally occurring sentential paraphrases and non-paraphrases:2 the MSRP corpus derived from clustered news articles (Dolan and Brockett, 2005) and the PIT-2015 corpus from Twitter trending topics (Xu et al., 2014, 2015). Our goal is not only to create a new annotated paraphrase cor"
D17-1126,J10-3003,0,0.125172,"Missing"
D17-1126,S14-2001,0,0.0446941,"n human language, as formalized in the Meaning-Text linguistic theory which defines meaning as ‘invariant of paraphrases’ (Mili´cevi´c, 2006). Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al., 1 The code and data can be obtained from the first and last author’s websites. 2015), machine translation (Mehdizadeh Seraj et al., 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks. In this paper, we address a major challenge in paraphrase research — the lack of parallel corpora. There are only two publicly available datasets of naturally occurring sentential paraphrases and non-paraphrases:2 the MSRP corpus derived from clustered news articles ("
D17-1126,D15-1163,0,0.0977838,"erent expressions (Bhagat and Hovy, 2013). It is a fundamental semantic relation in human language, as formalized in the Meaning-Text linguistic theory which defines meaning as ‘invariant of paraphrases’ (Mili´cevi´c, 2006). Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al., 1 The code and data can be obtained from the first and last author’s websites. 2015), machine translation (Mehdizadeh Seraj et al., 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks. In this paper, we address a major challenge in paraphrase research — the lack of parallel corpora. There are only two publicly available datasets of naturally occurring sentential parap"
D17-1126,D12-1104,0,0.100852,"Missing"
D17-1126,P02-1040,0,0.09934,"Missing"
D17-1126,P15-2070,0,0.0587392,"Missing"
D17-1126,D14-1162,0,0.0857936,"Missing"
D17-1126,P06-2094,0,0.0536028,"ugh phrasal paraphrase extraction. We make our code and data freely available.1 1 Introduction A paraphrase is a restatement of meaning using different expressions (Bhagat and Hovy, 2013). It is a fundamental semantic relation in human language, as formalized in the Meaning-Text linguistic theory which defines meaning as ‘invariant of paraphrases’ (Mili´cevi´c, 2006). Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al., 1 The code and data can be obtained from the first and last author’s websites. 2015), machine translation (Mehdizadeh Seraj et al., 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks. In this paper, we address a majo"
D17-1126,Q14-1018,0,0.0485932,"Missing"
D17-1126,W13-2515,1,0.935296,"Missing"
D17-1126,P13-2123,0,0.0667107,"Missing"
D17-1126,D11-1061,0,0.0551803,"Missing"
D17-1126,D16-1033,0,0.0734593,"Missing"
D17-1126,D13-1008,0,0.0459984,"Missing"
D17-1126,Q15-1025,0,0.0307316,"aning as ‘invariant of paraphrases’ (Mili´cevi´c, 2006). Researchers have shown benefits of using paraphrases in a wide range of applications (Madnani and Dorr, 2010), including question answering (Fader et al., 2013), semantic parsing (Berant and Liang, 2014), information extraction (Sekine, 2006; Zhang et al., 1 The code and data can be obtained from the first and last author’s websites. 2015), machine translation (Mehdizadeh Seraj et al., 2015), textual entailment (Dagan et al., 2006; Bjerva et al., 2014; Marelli et al., 2014; Izadinia et al., 2015), vector semantics (Faruqui et al., 2015; Wieting et al., 2015), and semantic textual similarity (Agirre et al., 2015; Li and Srikumar, 2016). Studying paraphrases in Twitter can also help track unfolding events (Vosoughi and Roy, 2016) or the spread of information (Bakshy et al., 2011) on social networks. In this paper, we address a major challenge in paraphrase research — the lack of parallel corpora. There are only two publicly available datasets of naturally occurring sentential paraphrases and non-paraphrases:2 the MSRP corpus derived from clustered news articles (Dolan and Brockett, 2005) and the PIT-2015 corpus from Twitter trending topics (Xu et a"
D17-1126,S15-2001,1,0.915238,"Missing"
D17-1126,S15-2045,0,\N,Missing
D17-1126,Q14-1034,1,\N,Missing
D17-1126,Q15-1009,0,\N,Missing
D17-1126,P14-1133,0,\N,Missing
D17-1126,J13-3001,0,\N,Missing
D18-1410,D12-1091,0,0.0138517,"the best performing baseline (Pavlick and Callison-Burch, 2016). paraphrases of ‘should reject’ SimplePPDB refuse, discard, repudiate, shun, dismiss SimplePPDB++ vote against, set aside, throw out, say no to, turn away Table 3: SimplePPDB++ includes lexical and phrasal paraphrases with improved readability ranking scores by our NRRall+binning+W C model. Shown are the top 5 ranked simplifications according to SimplePPDB++ for several input words/phrases, in comparison to the previous work of SimplePPDB (Pavlick and Callison-Burch, 2016). the-art for both measures. We use paired bootstrap test (Berg-Kirkpatrick et al., 2012; Efron and Tibshirani, 1993) as it can be applied to any performance metric. We also conducted ablation experiments to show the effectiveness of the Gaussianbased feature vectorization layer (+binning ) and the word-complexity lexicon (+W C ). 4.2 Acc. 49.4 50.1 56.2 60.4 62.1 59.4 64.1 65.3* SimplePPDB++ We also can apply our NRR model to rank the lexical and phrasal paraphrase rules in the Paraphrase Database (PPDB) (Pavlick et al., 2015), and identify good simplifications (see examples in Table 3). The resulting lexical resource, SimplePPDB++, contains all 13.1 million lexical and phrasal"
D18-1410,P11-2087,0,0.496945,"ion Details We use PyTorch framework to implement the NRR model, which consists of an input layer, three hidden layers with eight nodes in each layer and the tanh activation function, and a single node linear output layer. The training objective is to minimize the Mean Squared Error (MSE): L(θ) = m 1 X (yi − yˆi )2 m i=1 Figure 2: t-SNE visualization of the complexity scores, ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional representations by applying Gaussian radial basis functions. (3) 3752 4 Lexical Simplification Applications Biran et al. (2011) Jauhar & Specia (2012) Kajiwara et al. (2013) Horn et al. (2014) ˇ Glavaˇs & Stajner (2015) Boundary Ranker Paetzold & Specia (2017) NRRall NRRall+binning NRRall+binning+W C As the lexical simplification research field traditionally studies multiple sub-tasks and datasets, we present a series of experiments to demonstrate the effectiveness of our newly created lexicon and neural readability ranking (NRR) model. 4.1 Substitution Ranking Given an instance consisting of a target complex word in a sentence and a set of candidate substitutions, the goal of the Substitution Ranking task is to rank"
D18-1410,C12-1023,0,0.143879,"Missing"
D18-1410,E99-1042,0,0.0782473,"ata. We use the dataset from (Pavlick and Callison-Burch, 2016), which contains 100 unique target words/phrases sampled from the Newsela Simplification Corpus (Xu et al., 2015) of news articles, and follow the same evaluation procedure. We ask two annotators to evaluate whether the generated substitutions are good simplifications. Comparison to existing methods. We evaluate the correctness of the substitutions generated by SimplePPDB++ in comparison to several existˇ ing methods: Glavaˇs (Glavaˇs and Stajner, 2015), Kauchak (Coster and Kauchak, 2011), WordNet Generator (Devlin and Tait, 1998; Carroll et al., 1999), and SimplePPDB (Pavlick and CallisonBurch, 2016). Glavaˇs obtains candidates with the highest similarity scores in the GloVe (Pennington et al., 2014) word vector space. Kauchak’s generator is based on Simple Wikipedia and normal Wikipedia parallel corpus and automatic word alignment. WordNet-based generator simply uses the synonyms of word in WordNet (Miller, 1995). For all the existing methods, we report the results based on the implementations in (Pavlick and Callison-Burch, 2016), which used SVMbased ranking. For both SimplePPDB and SimplePPDB++, extracted candidates are high quality par"
D18-1410,D14-1082,0,0.0464405,"Missing"
D18-1410,W11-1601,0,0.125093,"phrase rules, by applying our model to the Paraphrase Database (PPDB).1 1 Introduction Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases. There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substituˇ tion Generation (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: https://github.com/ mounicam/lexical_simplification. (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2"
D18-1410,1993.eamt-1.1,0,0.375693,"avlick and Callison-Burch, 2016). paraphrases of ‘should reject’ SimplePPDB refuse, discard, repudiate, shun, dismiss SimplePPDB++ vote against, set aside, throw out, say no to, turn away Table 3: SimplePPDB++ includes lexical and phrasal paraphrases with improved readability ranking scores by our NRRall+binning+W C model. Shown are the top 5 ranked simplifications according to SimplePPDB++ for several input words/phrases, in comparison to the previous work of SimplePPDB (Pavlick and Callison-Burch, 2016). the-art for both measures. We use paired bootstrap test (Berg-Kirkpatrick et al., 2012; Efron and Tibshirani, 1993) as it can be applied to any performance metric. We also conducted ablation experiments to show the effectiveness of the Gaussianbased feature vectorization layer (+binning ) and the word-complexity lexicon (+W C ). 4.2 Acc. 49.4 50.1 56.2 60.4 62.1 59.4 64.1 65.3* SimplePPDB++ We also can apply our NRR model to rank the lexical and phrasal paraphrase rules in the Paraphrase Database (PPDB) (Pavlick et al., 2015), and identify good simplifications (see examples in Table 3). The resulting lexical resource, SimplePPDB++, contains all 13.1 million lexical and phrasal paraphrase rules in the XL ve"
D18-1410,W07-1007,0,0.263777,"ter and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: https://github.com/ mounicam/lexical_simplification. (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010), or people with reading disabilities (Rello et al., 2013). Most current approaches to lexical simplification heavily rely on corpus statistics and surface level features, such as word length and corpusbased word frequencies (read more in §5). Two of the most commonly used assumptions are that simple words are associated with shorter lengths and higher frequencies in a corpus. However, these assumptions are not always accurate and are often the major source of errors in the simplification pipeline (Shardlow, 2014). For instance, the word foolishness is simpler th"
D18-1410,N13-1092,0,0.145299,"Missing"
D18-1410,P15-2011,0,0.0371435,"Missing"
D18-1410,C18-1039,0,0.028828,"of our knowledge, the work by Paetzold and Specia (2017) is the first neural model for lexical simplification which uses a feedforward network with language model probability features. Our NRR model is the first pairwise neural ranking model to vectorize numeric features and to embed human judgments using a word-complexity lexicon of 15,000 English words. Besides lexical simplification, another line of relevant research is sentence simplification that uses statistical or neural machine translation (MT) approaches (Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018). It has shown possible to integrate paraphrase rules in PPDB into statistical MT for sentence simplification (Xu et al., 2016) and bilingual translation (Mehdizadeh Seraj et al., 2015), while how to inject SimplePPDB++ into neural MT remains an open research question. con with human judgments that has shown substantial improvements on automatic simplification systems. We were encouraged by the success of the word-emotion lexicon (Mohammad and Turney, 2013) and the word-happiness lexicon (Dodds et al., 2011, 2015). Lexica for simplification: There have been previous attempts to use manually cr"
D18-1410,P14-2075,0,0.377667,"hich consists of an input layer, three hidden layers with eight nodes in each layer and the tanh activation function, and a single node linear output layer. The training objective is to minimize the Mean Squared Error (MSE): L(θ) = m 1 X (yi − yˆi )2 m i=1 Figure 2: t-SNE visualization of the complexity scores, ranging between 1.0 and 5.0, of 300 random words from the word-complexity lexicon vectorized into 10-dimensional representations by applying Gaussian radial basis functions. (3) 3752 4 Lexical Simplification Applications Biran et al. (2011) Jauhar & Specia (2012) Kajiwara et al. (2013) Horn et al. (2014) ˇ Glavaˇs & Stajner (2015) Boundary Ranker Paetzold & Specia (2017) NRRall NRRall+binning NRRall+binning+W C As the lexical simplification research field traditionally studies multiple sub-tasks and datasets, we present a series of experiments to demonstrate the effectiveness of our newly created lexicon and neural readability ranking (NRR) model. 4.1 Substitution Ranking Given an instance consisting of a target complex word in a sentence and a set of candidate substitutions, the goal of the Substitution Ranking task is to rank the candidates in the order of their simplicity. In this section,"
D18-1410,S12-1066,0,0.0285922,". ˇ (2013), and Glavaˇs & Stajner (2015), which use carefully designed heuristic scoring functions to combine various information such as corpus statistics and semantic similarity measures from WordNet; Horn et al. (2014) and the Boundary Ranker (Paetzold and Specia, 2015), which respectively use a supervised SVM ranking model and pairwise linear classification model with various features. All of these methods have been implemented as part of the LEXenstein toolkit (Paetzold and Specia, 2015), which we use for the experimental comparisons here. In addition, we also compare to the best system (Jauhar and Specia, 2012) among participants at SemEval 2012, which used SVMbased ranking. Results. Table 2 compares the performances of our NRR model to the state-of-the-art results reported by Paetzold and Specia (2017). We use precision of the simplest candidate (P@1) and Pearson correlation to measure performance. P@1 is equivalent to TRank (Specia et al., 2012), the official metric for the SemEval 2012 English Lexical Simplification task. While P@1 captures the practical utility of an approach, Pearson correlation indicates how well the system’s rankings correlate with human judgment. We train our NRR model with"
D18-1410,O13-1007,0,0.500086,"ffectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substituˇ tion Generation (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: https://github.com/ mounicam/lexical_simplification. (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010), or people with reading disabilities (Rello et al., 2013). Most current approaches to lexical simplification heavily rely on corpus statistics and surface level features, such as word length and corpusbased word frequencies (read more in §5). Two of the most commonly used assumptions are that simple words are asso"
D18-1410,S16-1149,0,0.349158,"the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).1 1 Introduction Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases. There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substituˇ tion Generation (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on"
D18-1410,N18-1019,0,0.024711,"Missing"
D18-1410,E17-2006,0,0.52582,"to measure the complexity of any given word or phrase (including those outside the lexicon and/or with sentential context). Our model significantly outperforms the state-of-the-art on the benchmark SemEval-2012 evaluation for Substitution Ranking (Specia et al., 2 PPDB is a large paraphrase database derived from static bilingual translation data available at: http:// paraphrase.org 3749 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3749–3760 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2012; Paetzold and Specia, 2017), with or without using the manually created word-complexity lexicon, achieving a Pearson correlation of 0.714 and 0.702 respectively. We also apply the new ranking model to identify lexical simplifications (e.g., commemorate → celebrate) among the large number of paraphrase rules in PPDB with improved accuracy compared to previous work for Substitution Generation. At last, by utilizing the wordcomplexity lexicon, we establish a new state-ofthe-art on two common test sets for Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017). We make our code, the wordcomplexity lexi"
D18-1410,C18-1019,0,0.0471487,"15). Lexica for simplification: There have been previous attempts to use manually created lexica for simplification. For example, Elhadad and Sutaria (2007) used UMLS lexicon (Bodenreider, 2007), a repository of technical medical terms; Ehara et al. (2010) asked non-native speakers to answer multiple-choice questions corresponding to 12,000 English words to study each user’s familiarity of vocabulary; Kaji et al. (2012) and Kajiwara et al. (2013) used a dictionary of 5,404 Japanese words based on the elementary school textbooks; Xu et al. (2016) used a list of 3,000 most common English words; Lee and Yeung (2018) used an ensemble of vocabulary lists of different complexity levels. However, to the best of our knowledge, there is no previous study on manually building a large word-complexity lexiWe thank anonymous reviewers for their thoughtful comments. We thank Avirup Sil and Anastasios Sidiropoulos for valuable discussions, Sanja ˇ Stajner and Seid Muhie Yimam for sharing their code and data. We also thank the annotators: Jeniya Tabassum, Ashutosh Baheti, Wuwei Lan, Fan Bai, Alexander Konovalov, Chaitanya Kulkarni, Shuaichen Chang, Jayavardhan Reddy, Abhishek Kumar and Shreejit Gangadharan. This mate"
D18-1410,W16-4912,0,0.0400554,"the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).1 1 Introduction Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases. There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substituˇ tion Generation (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on"
D18-1410,P16-2024,0,0.362315,"verts each numerical feature, such as the lexicon scores and n-gram probabilities, into a vector representation by a series of Gaussian radial basis functions. 3. A feedforward neural network performing regression with one task-specific output node that adapts the model to different lexical simplification tasks (§4). Our model first processes each input word or phrase in parallel, producing vectorized features. All the features are then fed into a joint feedforward neural network. 3.2 Features We use a combination of rating scores from the word-complexity lexicon, lexical and corpus features (Pavlick and Callison-Burch, 2016) and collocational features (Paetzold and Specia, 2017). We inject the word-complexity lexicon into the NRR model by adding two features for each input word or phrase: a 0-1 binary feature representing the presence of a word (the longest word in a multi-word phrase) in the lexicon, and the corresponding word complexity score. For out-ofvocabulary words, both features have the value 0. We back-off to the complexity score of the lemmatized word if applicable. We also extract the following features: phrase length in terms of words and characters, number of syllables, frequency with respect to Goo"
D18-1410,D15-1163,0,0.0209443,"s. Our NRR model is the first pairwise neural ranking model to vectorize numeric features and to embed human judgments using a word-complexity lexicon of 15,000 English words. Besides lexical simplification, another line of relevant research is sentence simplification that uses statistical or neural machine translation (MT) approaches (Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018). It has shown possible to integrate paraphrase rules in PPDB into statistical MT for sentence simplification (Xu et al., 2016) and bilingual translation (Mehdizadeh Seraj et al., 2015), while how to inject SimplePPDB++ into neural MT remains an open research question. con with human judgments that has shown substantial improvements on automatic simplification systems. We were encouraged by the success of the word-emotion lexicon (Mohammad and Turney, 2013) and the word-happiness lexicon (Dodds et al., 2011, 2015). Lexica for simplification: There have been previous attempts to use manually created lexica for simplification. For example, Elhadad and Sutaria (2007) used UMLS lexicon (Bodenreider, 2007), a repository of technical medical terms; Ehara et al. (2010) asked non-na"
D18-1410,N15-1023,0,0.0567648,"ing two features for each input word or phrase: a 0-1 binary feature representing the presence of a word (the longest word in a multi-word phrase) in the lexicon, and the corresponding word complexity score. For out-ofvocabulary words, both features have the value 0. We back-off to the complexity score of the lemmatized word if applicable. We also extract the following features: phrase length in terms of words and characters, number of syllables, frequency with respect to Google Ngram corpus (Brants and Franz, 2006), the relative frequency in Simple Wikipedia with respect to normal Wikipedia (Pavlick and Nenkova, 2015) and ngram probabilities from a 5-gram language model trained on the SubIMDB corpus (Paetzold and Specia, 2016c), which has been shown to work well for lexical simplification. For a word w, we take language model probabilities of all the possible n-grams within the context window of 2 to the left and right of w. When w is a multi-word phrase, we break w into possible n-grams and average the probabilities for a specific context window. For an input pair of words/phrases hwa , wb i, we include individual features f (w1 ), f (w2 ) and the differences f (wa )−f (wb ). We also use pairwise features"
D18-1410,W16-6620,0,0.0208202,"eneration is arguably the most challenging research problem in lexical simplification, which involves producing candidate substitutions for each target complex word/phrase, followed by the substitution ranking. The key focus is to not only have better rankings, but more importantly, to have a larger number of simplifying substitutions generated. This is a more realistic evaluation to demonstrate the utility of SimplePPDB++ and the effectiveness of the NRR ranking model we used to create it, and how likely such lexical resources can benefit developing end-to-end sentence simplification system (Narayan and Gardent, 2016; Zhang and Lapata, 2017) in future work. Data. We use the dataset from (Pavlick and Callison-Burch, 2016), which contains 100 unique target words/phrases sampled from the Newsela Simplification Corpus (Xu et al., 2015) of news articles, and follow the same evaluation procedure. We ask two annotators to evaluate whether the generated substitutions are good simplifications. Comparison to existing methods. We evaluate the correctness of the substitutions generated by SimplePPDB++ in comparison to several existˇ ing methods: Glavaˇs (Glavaˇs and Stajner, 2015), Kauchak (Coster and Kauchak, 2011),"
D18-1410,P17-2014,0,0.0228966,"to apply neural networks to simplification tasks. To the best of our knowledge, the work by Paetzold and Specia (2017) is the first neural model for lexical simplification which uses a feedforward network with language model probability features. Our NRR model is the first pairwise neural ranking model to vectorize numeric features and to embed human judgments using a word-complexity lexicon of 15,000 English words. Besides lexical simplification, another line of relevant research is sentence simplification that uses statistical or neural machine translation (MT) approaches (Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018). It has shown possible to integrate paraphrase rules in PPDB into statistical MT for sentence simplification (Xu et al., 2016) and bilingual translation (Mehdizadeh Seraj et al., 2015), while how to inject SimplePPDB++ into neural MT remains an open research question. con with human judgments that has shown substantial improvements on automatic simplification systems. We were encouraged by the success of the word-emotion lexicon (Mohammad and Turney, 2013) and the word-happiness lexicon (Dodds et al., 2011, 2015). Lexica for simplifi"
D18-1410,P15-4015,0,0.0334216,"and Specia, 2017) and other methods. ∗ indicates statistical significance (p < 0.05) compared to the best performing baseline (Paetzold and Specia, 2017). and Specia, 2017) for substitution ranking with the best reported results on the SemEval 2012 dataset. Our baselines also include several other existing methods: Biran et al. (2011), Kajiwara et al. ˇ (2013), and Glavaˇs & Stajner (2015), which use carefully designed heuristic scoring functions to combine various information such as corpus statistics and semantic similarity measures from WordNet; Horn et al. (2014) and the Boundary Ranker (Paetzold and Specia, 2015), which respectively use a supervised SVM ranking model and pairwise linear classification model with various features. All of these methods have been implemented as part of the LEXenstein toolkit (Paetzold and Specia, 2015), which we use for the experimental comparisons here. In addition, we also compare to the best system (Jauhar and Specia, 2012) among participants at SemEval 2012, which used SVMbased ranking. Results. Table 2 compares the performances of our NRR model to the state-of-the-art results reported by Paetzold and Specia (2017). We use precision of the simplest candidate (P@1) an"
D18-1410,P15-2070,0,0.0523889,"Missing"
D18-1410,W14-1210,0,0.0279916,"eneration (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: https://github.com/ mounicam/lexical_simplification. (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010), or people with reading disabilities (Rello et al., 2013). Most current approaches to lexical simplification heavily rely on corpus statistics and surface level features, such as word length and corpusbased word frequencies (read more in §5). Two of the most commonly used assumptions are that simple words are associated with shorter lengths and higher frequencies in a corpus. However, these assumptions are not always accurate and are often the major source of errors in the simplification pipeline (Shardlow, 2014). For inst"
D18-1410,D14-1162,0,0.0916707,"on Corpus (Xu et al., 2015) of news articles, and follow the same evaluation procedure. We ask two annotators to evaluate whether the generated substitutions are good simplifications. Comparison to existing methods. We evaluate the correctness of the substitutions generated by SimplePPDB++ in comparison to several existˇ ing methods: Glavaˇs (Glavaˇs and Stajner, 2015), Kauchak (Coster and Kauchak, 2011), WordNet Generator (Devlin and Tait, 1998; Carroll et al., 1999), and SimplePPDB (Pavlick and CallisonBurch, 2016). Glavaˇs obtains candidates with the highest similarity scores in the GloVe (Pennington et al., 2014) word vector space. Kauchak’s generator is based on Simple Wikipedia and normal Wikipedia parallel corpus and automatic word alignment. WordNet-based generator simply uses the synonyms of word in WordNet (Miller, 1995). For all the existing methods, we report the results based on the implementations in (Pavlick and Callison-Burch, 2016), which used SVMbased ranking. For both SimplePPDB and SimplePPDB++, extracted candidates are high quality paraphrase rules (quality score ≥3.5 for words and Glavaˇs(n=95) WordNet(n=82) Kauchak(n=48) SimplePPDB(n=100) SimplePPDB++(n=100) #PPs — 6.63 4.39 8.77 9."
D18-1410,P13-3015,0,0.191093,"tion tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).1 1 Introduction Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases. There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substituˇ tion Generation (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: ht"
D18-1410,W13-2908,0,0.0197736,"tion tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).1 1 Introduction Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases. There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substituˇ tion Generation (Glavaˇs and Stajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: ht"
D18-1410,D17-1062,0,0.140284,"ost challenging research problem in lexical simplification, which involves producing candidate substitutions for each target complex word/phrase, followed by the substitution ranking. The key focus is to not only have better rankings, but more importantly, to have a larger number of simplifying substitutions generated. This is a more realistic evaluation to demonstrate the utility of SimplePPDB++ and the effectiveness of the NRR ranking model we used to create it, and how likely such lexical resources can benefit developing end-to-end sentence simplification system (Narayan and Gardent, 2016; Zhang and Lapata, 2017) in future work. Data. We use the dataset from (Pavlick and Callison-Burch, 2016), which contains 100 unique target words/phrases sampled from the Newsela Simplification Corpus (Xu et al., 2015) of news articles, and follow the same evaluation procedure. We ask two annotators to evaluate whether the generated substitutions are good simplifications. Comparison to existing methods. We evaluate the correctness of the substitutions generated by SimplePPDB++ in comparison to several existˇ ing methods: Glavaˇs (Glavaˇs and Stajner, 2015), Kauchak (Coster and Kauchak, 2011), WordNet Generator (Devli"
D18-1410,shardlow-2014-open,0,0.102353,"se rules in PPDB as potential candidates. The better NRR model we used in creating SimplePPDB++ allows improved selections and rankings of simplifying paraphrase rules than the previous version of SimplePPDB. As an additional reference, we also include the measurements for the other existing methods based on (Pavlick and Callison-Burch, 2016), which, by evaluation design, are focused on the comparison of precision while PPDB has full coverage. 4.4 Complex Word Identification Complex Word Identification (CWI) identifies the difficult words in a sentence that need to be simplified. According to Shardlow (2014), this step can improve the simplification system by avoiding mistakes such as overlooking challenging words or oversimplifying simple words. In this section, we demonstrate how our word-complexity lexicon helps with the CWI task by injecting human ratings into the state-of-the-art systems. Data. The task is to predict whether a target word/phrase in a sentence is ‘simple’ or ‘complex’, and an example instance is as follows: Nine people were killed in the bombardment. We conduct experiments on two datasets: (i) Semeval 2016 CWI shared-task dataset (Paetzold 3755 Length Senses SimpleWiki Neares"
D18-1410,N10-1144,0,0.0233994,"h involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity. Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers 1 The code and data are publicly available on the authors’ homepages and GitHub: https://github.com/ mounicam/lexical_simplification. (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010), or people with reading disabilities (Rello et al., 2013). Most current approaches to lexical simplification heavily rely on corpus statistics and surface level features, such as word length and corpusbased word frequencies (read more in §5). Two of the most commonly used assumptions are that simple words are associated with shorter lengths and higher frequencies in a corpus. However, these assumptions are not always accurate and are often the major source of errors in the simplification pipeline (Shardlow, 2014). For instance, the word foolishness is simpler than its meaningpreserving substi"
D18-1410,N18-2013,0,0.0283044,"asks. To the best of our knowledge, the work by Paetzold and Specia (2017) is the first neural model for lexical simplification which uses a feedforward network with language model probability features. Our NRR model is the first pairwise neural ranking model to vectorize numeric features and to embed human judgments using a word-complexity lexicon of 15,000 English words. Besides lexical simplification, another line of relevant research is sentence simplification that uses statistical or neural machine translation (MT) approaches (Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018). It has shown possible to integrate paraphrase rules in PPDB into statistical MT for sentence simplification (Xu et al., 2016) and bilingual translation (Mehdizadeh Seraj et al., 2015), while how to inject SimplePPDB++ into neural MT remains an open research question. con with human judgments that has shown substantial improvements on automatic simplification systems. We were encouraged by the success of the word-emotion lexicon (Mohammad and Turney, 2013) and the word-happiness lexicon (Dodds et al., 2011, 2015). Lexica for simplification: There have been previous attempts"
D18-1410,Q16-1029,1,0.85661,"searchers started to apply neural networks to simplification tasks. To the best of our knowledge, the work by Paetzold and Specia (2017) is the first neural model for lexical simplification which uses a feedforward network with language model probability features. Our NRR model is the first pairwise neural ranking model to vectorize numeric features and to embed human judgments using a word-complexity lexicon of 15,000 English words. Besides lexical simplification, another line of relevant research is sentence simplification that uses statistical or neural machine translation (MT) approaches (Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Vu et al., 2018; Guo et al., 2018). It has shown possible to integrate paraphrase rules in PPDB into statistical MT for sentence simplification (Xu et al., 2016) and bilingual translation (Mehdizadeh Seraj et al., 2015), while how to inject SimplePPDB++ into neural MT remains an open research question. con with human judgments that has shown substantial improvements on automatic simplification systems. We were encouraged by the success of the word-emotion lexicon (Mohammad and Turney, 2013) and the word-happiness lexicon (Dodds et al., 2011, 2015)"
D18-1410,W18-0507,0,0.159473,"Missing"
D19-1032,P08-1030,0,0.542162,"Missing"
D19-1032,C16-1215,0,0.131726,"edical reports, etc. 2 Related Work Recent development on information extraction has been advancing in building the joint model that can extract entities and identify structures (relations or events) among them simultaneously. For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations. In the meantime, the same to the focus of this paper, a few studies aimed at designing joint models for the entity and event extraction, such as handcrafted-feature-based (Li et al., 2014; Yang and Mitchell, 2016; Judea and Strube, 2016) and neural-network-based (Zhang and Ji, 2018; Nguyen and Nguyen, 2019) models. Nevertheless, these models did not present how to handle argument candidates beyond the sentence scope. (Yang and Mitchell, 2016) claimed to handle event-argument relations across sentences with the prerequisite of well-defined features, which, unfortunately, is nontrivial. In addition to the modeling challenge, another big obstacle for democratizing EE is the lack of 3 Preliminaries We first clarify several key notions: 1) entity mention: an entity mention is a text span that refers to an entity object; 2) event r"
D19-1032,P13-1008,0,0.318661,"Missing"
D19-1032,D14-1198,0,0.0900098,"doctor instructions identification from medical reports, etc. 2 Related Work Recent development on information extraction has been advancing in building the joint model that can extract entities and identify structures (relations or events) among them simultaneously. For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations. In the meantime, the same to the focus of this paper, a few studies aimed at designing joint models for the entity and event extraction, such as handcrafted-feature-based (Li et al., 2014; Yang and Mitchell, 2016; Judea and Strube, 2016) and neural-network-based (Zhang and Ji, 2018; Nguyen and Nguyen, 2019) models. Nevertheless, these models did not present how to handle argument candidates beyond the sentence scope. (Yang and Mitchell, 2016) claimed to handle event-argument relations across sentences with the prerequisite of well-defined features, which, unfortunately, is nontrivial. In addition to the modeling challenge, another big obstacle for democratizing EE is the lack of 3 Preliminaries We first clarify several key notions: 1) entity mention: an entity mention is a tex"
D19-1032,P10-1081,0,0.786793,"Missing"
D19-1032,P17-1164,0,0.150631,"Missing"
D19-1032,P09-1113,0,0.0389205,"g et al., 2019) have been put on EE, most of them are based on ACE 20052 , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously overlooks the arguments-scattering challenge. In contrast, EE on financial documents, such as ChFinAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event. The most recent work, DCFEE (Yang et al., 2018), attempted to explore DEE on ChFinAnn, by employing distant supervision (DS) (Mintz et al., 2009) to generate EE data and performing a two-stage extraction: 1) a sequence tagging model for SEE, and 2) a key-event-sentence detection model to detect the key-event sentence, coupled with a heuristic strategy that padded missing arguments from surrounding sentences, for DEE. 2 https://www.ldc.upenn.edu/ collaborations/past-projects/ace 3 338 Estimated by their Table 1 as 2∗NO.ANN−NO.POS . NO.ANN training data due to the enormous cost to obtain expert annotations. To address this problem, some researches attempted to adapt distant supervision (DS) to the EE setting, since DS has shown promising"
D19-1032,W06-0901,0,0.832388,"Missing"
D19-1032,N16-1034,0,0.573993,"Missing"
D19-1032,P17-1038,0,0.0518986,"uments from surrounding sentences, for DEE. 2 https://www.ldc.upenn.edu/ collaborations/past-projects/ace 3 338 Estimated by their Table 1 as 2∗NO.ANN−NO.POS . NO.ANN training data due to the enormous cost to obtain expert annotations. To address this problem, some researches attempted to adapt distant supervision (DS) to the EE setting, since DS has shown promising results by employing knowledge bases to automatically generate training data for relation extraction (Mintz et al., 2009). However, the vanilla EE required the trigger words that were absent on factual knowledge bases. Therefore, (Chen et al., 2017; Yang et al., 2018) employed either linguistic resources or predefined dictionaries for trigger-words labeling. On the other hand, another recent work (Zeng et al., 2018b) showed that directly labeling event arguments without trigger words was also feasible. However, they only considered the SEE setting and their methods cannot be directly extended to the DEE setting, which is the main focus of this work. Traditionally, when applying DS to relation extraction, researchers put huge efforts into alleviating labeling noises (Riedel et al., 2010; Lin et al., 2016; Feng et al., 2018; Zheng et al.,"
D19-1032,P15-1017,0,0.355345,"Missing"
D19-1032,D11-1001,0,0.180499,"Missing"
D19-1032,P18-4009,0,0.396094,"t al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 20052 , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously overlooks the arguments-scattering challenge. In contrast, EE on financial documents, such as ChFinAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event. The most recent work, DCFEE (Yang et al., 2018), attempted to explore DEE on ChFinAnn, by employing distant supervision (DS) (Mintz et al., 2009) to generate EE data and performing a two-stage extraction: 1) a sequence tagging model for SEE, and 2) a key-event-sentence detection model to detect the key-event sentence, coupled with a heuristic strategy that padded missing arguments from surrounding sentences, for DEE. 2 https://www.ldc.upenn.edu/ collaborations/past-projects/ace 3 338 Estimated by their Table 1 as 2∗NO.ANN−NO.POS . NO.ANN training data due to the enormous cost to obtain expert annotations. To address this problem, some rese"
D19-1032,P18-1047,0,0.142214,"ining data due to the enormous cost to obtain expert annotations. To address this problem, some researches attempted to adapt distant supervision (DS) to the EE setting, since DS has shown promising results by employing knowledge bases to automatically generate training data for relation extraction (Mintz et al., 2009). However, the vanilla EE required the trigger words that were absent on factual knowledge bases. Therefore, (Chen et al., 2017; Yang et al., 2018) employed either linguistic resources or predefined dictionaries for trigger-words labeling. On the other hand, another recent work (Zeng et al., 2018b) showed that directly labeling event arguments without trigger words was also feasible. However, they only considered the SEE setting and their methods cannot be directly extended to the DEE setting, which is the main focus of this work. Traditionally, when applying DS to relation extraction, researchers put huge efforts into alleviating labeling noises (Riedel et al., 2010; Lin et al., 2016; Feng et al., 2018; Zheng et al., 2019). In contrast, this work shows that combining DS with some simple constraints can obtain pretty good labeling quality for DEE, where the reasons are two folds: 1) b"
D19-1032,N19-1105,0,0.397783,"Missing"
D19-1032,P19-1137,1,0.775041,"et al., 2017; Yang et al., 2018) employed either linguistic resources or predefined dictionaries for trigger-words labeling. On the other hand, another recent work (Zeng et al., 2018b) showed that directly labeling event arguments without trigger words was also feasible. However, they only considered the SEE setting and their methods cannot be directly extended to the DEE setting, which is the main focus of this work. Traditionally, when applying DS to relation extraction, researchers put huge efforts into alleviating labeling noises (Riedel et al., 2010; Lin et al., 2016; Feng et al., 2018; Zheng et al., 2019). In contrast, this work shows that combining DS with some simple constraints can obtain pretty good labeling quality for DEE, where the reasons are two folds: 1) both the knowledge base and text documents are from the same domain; 2) an event record usually contains multiple arguments, while a common relational fact only covers two entities. form state-of-the-art methods when facing DEEspecific challenges. In summary, our contributions include: • We propose a novel model, Doc2EDAG, which can directly generate event tables based on a document, to address unique challenges of DEE effectively. •"
D19-1032,P17-1113,0,0.0280732,"tackle those DEE-specific challenges without any domain-specific assumption. Therefore, our general labeling and modeling strategies can directly benefit many other business domains with similar challenges, such as criminal facts and judgments extraction from legal documents, disease symptoms and doctor instructions identification from medical reports, etc. 2 Related Work Recent development on information extraction has been advancing in building the joint model that can extract entities and identify structures (relations or events) among them simultaneously. For instance, (Ren et al., 2017; Zheng et al., 2017; Zeng et al., 2018a; Wang et al., 2018) focused on jointly extracting entities and inter-entity relations. In the meantime, the same to the focus of this paper, a few studies aimed at designing joint models for the entity and event extraction, such as handcrafted-feature-based (Li et al., 2014; Yang and Mitchell, 2016; Judea and Strube, 2016) and neural-network-based (Zhang and Ji, 2018; Nguyen and Nguyen, 2019) models. Nevertheless, these models did not present how to handle argument candidates beyond the sentence scope. (Yang and Mitchell, 2016) claimed to handle event-argument relations ac"
D19-1032,N16-1033,0,0.462243,"cuments include multiple event records. Extensive experiments demonstrate that Doc2EDAG can significantly outperthe correct Pledged Shares at the document level should be “[SHARE2]”. Similarly, “[DATE3]” is the correct End Date at the sentence level (ID 9) but incorrect at the document level (ID 10). Moreover, some summative arguments, such as “[SHARE5]” and “[RATIO]”, are often stated at the end of the document. Although a great number of efforts (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Riedel and McCallum, 2011; Li et al., 2013, 2014; Chen et al., 2015; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2017; Sha et al., 2018; Zhang and Ji, 2018; Nguyen and Nguyen, 2019; Wang et al., 2019) have been put on EE, most of them are based on ACE 20052 , an expert-annotated benchmark, which only tagged event arguments within the sentence scope. We refer to such task as the sentence-level EE (SEE), which obviously overlooks the arguments-scattering challenge. In contrast, EE on financial documents, such as ChFinAn, requires document-level EE (DEE) when facing arguments-scattering, and this challenge gets much harder when coupled with multi-event. The most recent wor"
D19-1032,P11-1113,0,\N,Missing
D19-1032,P16-1200,0,\N,Missing
I11-1117,W08-1805,0,0.0302105,"ities as slot types. In past KBP competitions, many participants (Li et al., 2009; Byrne and Dunnion, 2010; Chen et al., 2010) exploited a QA system to fill slots by constructing queries based on target entities and slot types. However, their query templates contain only a few additional query terms other than the target entity name, which are mostly obtained manually. Most of QA systems use the question words as-is or with expansion to form the retrieval system query. Various query expansion approaches have been used to tackle the passage-query mismatch problem, including relevance feedback (Derczynski et al., 2008), ontologies (Bhogal et al., 2007), semantic lexica (Ofoghi et al., 2006), etc. As a data-driven approach, relevance feedback is sensitive to the quality of first time retrieval. Our use of Freebase, a freely available large semantic database, to provide distant supervision requires neither labeled data nor costly constructed knowledge models. Some researchers (Grishman and Min, 2010; Chrupala et al., 2010; Surdeanu et al., 2010) integrated IR and IE together. Surdeanu et al. (2010) coupled the entity name with a handful of hand-selected trigger words for each slot type as queries to IR system"
I11-1117,P05-1045,0,0.00390891,"ather arbitrarily. 1) The target named entity ( ): We ensure that passages that (partly) include the string of entity names rank higher than those only containing pronouns. 1049 ( ) { ( ) where are arbitrary constants larger than any possible value of . The passage partly contains the entity when it shares at least a word in common with the entity name. 2) The named entity of the expected type ( ): The named entity of the type that is to be found for the relation is called the expected named entity, (e.g. „ORGANIZATION‟ for relation „employee_of‟). We run the Stanford Named Entity Recognizer (Finkel et al., 2005) on Lemur‟s passage retrieval outputs, preferring passages that contain a named entity of the sought type, and more strongly preferring names that have not appeared in previously retrieved passages (novel names). ( ) ( ) { where and are arbitrary constants larger than any possible value of . 3) The expansion terms ( ): The expansion terms include predefined words that are predictive of or related to a specific relation. We adapt the indicative word list used by Surdeanu et al. (2010) in their KBP system, which include several words for each relation. We also use a list containing 635 common ti"
I11-1117,P08-1030,1,0.815256,"semantic and ontological knowledge. To generate the final results, there could be different strategies. We use a simple strategy in this paper, which suffices to show the capability of our system, outputting the answers of the top-ranked passages that provide an answer (possibly duplicate). A more delicate design is not a focus of this research. Traditional IE Pipeline We exploit a simple two-stage pipeline architecture for the KBP task. First, we retrieve passages related to the target entity. Then we apply to those passages a traditional information extraction system (Grishman et al., 2005; Ji and Grishman, 2008) to extract relations, which was originally created for the NIST Automatic Content Extraction (ACE) Evaluations. Its relation QA-like Pipeline The new passage retrieval IKFB system also allows us to create a QA-like pipeline for largescale information extraction. Besides applying a sophisticated IE system to the retrieved passages using deep NLP techniques, such as coreference resolution, we can exploit answer extraction/selection components similar to many QA systems. Some common answer extraction/selection approaches, e.g. using distance from keywords, can possibly boost the speed by avoidin"
I11-1117,P09-1113,0,0.698663,". However, unlike QA, we have a fixed inventory of relations and a fixed set of expected answer Le Zhao* *Carnegie Mellon University Pittsburgh, PA, USA lezhao@cs.cmu.edu types (e.g. employer of a person). This allows us to bring to bear the more specialized learning methods of IE to tune the passage retrieval for each relation of interest. To the best of our knowledge, we are the first to systematically study the passage retrieval algorithm for information extraction and propose a novel distant supervision approach to obtain a list of weighted keywords for each relation. Distant supervision (Mintz et al., 2009) makes use of noisy training data generated automatically from a related, but different, type dataset to solve problems on another type of data. Instead of a handful of human-selected keywords, we automatically learn hundreds or thousands of indicative keywords from a freely available online resource, Freebase, which is similar to Wikipedia Infoboxes. Passages are ranked and retrieved based on these keywords indicative of certain relations. We then feed individual passages to a traditional IE system or to an answer extraction component as used in QA systems to obtain the final outputs. Both th"
I11-1117,N07-2032,0,0.18625,"which is to return a ranked list of related entities given an expected type of entity and a brief description (query) of the relation in free text. Fang et al. (2010) ranked entities by their relevance to the query at the document, passage and entity level, primarily based on the similarity between terms. In all this previous work, the limited number of query terms has become the performance bottleneck of the passage retrieval for large-corpus information extraction. Perhaps most similar to our distant supervision keyword learning approach for passage retrieval is the semi-automatic method of Nguyen et al. (2007), who extract only several keywords for each relation from Wikipedia and study only the dependency subtrees that contain those keywords. In contrast to their tf-idf model followed by a manual selection step, our algorithm allows us to fully automatically extract hundreds or even thousands of keywords with a weight indicating their relevance to each relation. Mintz et al. (2009) proposed a distant supervision approach for relation extraction using a richfeatured logistic regression model. Like us, they used Freebase as a source of known relation instances and Wikipedia as a text source to creat"
N15-1072,W10-0710,0,0.0588893,"Missing"
N15-1072,W10-0701,1,0.766567,"s, translations produced via crowdousrcing may be low quality. Previous work has addressed this problem, showing that non-professional translators hired on Amazon Mechanical Turk (MTurk) can achieve professional-level quality, by soliciting multiple translations of each source sentence and then choosing the best translation (Zaidan and CallisonBurch, 2011). Introduction Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost. Many NLP researchers have started creating speech and language data through crowdsourcing (for example, Snow et al. (2008), Callison-Burch and Dredze (2010) and others). One NLP application that has been the focus of crowdsourced data collection is statistical machine translation (SMT) In this paper we focus on a different aspect of crowdsourcing than Zaidan and Callison-Burch (2011). We attempt to achieve the same high quality while minimizing the associated costs. We propose two complementary methods: (1) We reduce the number of translations that we solicit for each source sentence. Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one. We do so by building"
N15-1072,N10-1024,1,0.844983,"performance can be identified and blocked. Lin et al. (2014) examined the relationship between worker accuracy and budget in the context of using crowdsourcing to train a machine learning classifier. They show that if the goal is to train a classifier on the labels, that the properties of the classifier will determine whether it is better to re-label data (resulting in higher quality labels) or get more single labeled items (of lower quality). They showed that classifiers with weak inductive bias benefit more from relabeling, and that relabeling is more important when worker accuracy is low. Novotney and Callison-Burch (2010) showed a similar result for training an automatic speech recognition (ASR) system. When creating training data for an ASR system, given a fixed budget, their system’s accuracy was higher when it is trained on more low quality transcription data compared to when it was trained on fewer high quality transcriptions. 8 Conclusion In this paper, we propose two mechanisms to optimize cost: a translation reducing method and a translator reducing method. They have different applicable scenarios for large corpus construction. The translation reducing method works if there exists a specific requirement"
N15-1072,P03-1021,0,0.229126,"Missing"
N15-1072,P02-1040,0,0.103316,"Missing"
N15-1072,W13-2323,0,0.0121257,"l from noisy labels. We cannot always get high-quality labeled data from crowdsourcing, but we can still ensure that a model trained on the data is accurate by redundantly labeling the data. Sheng et al. (2008) proposed a framework for repeated-labeling that resolves the uncertainty in labeling via majority voting. The experimental results show that a model’s accuracy is improved even if labels in its training data are noisy and imperfect. As long as the integrated quality (the probability of the integrated labeling being correct) is higher than 0.5, repeated labeling benefits model training. Passonneau and Carpenter (2013) created a Bayesian model of annotation. They applied it to the problem of word sense annotation. Passonneau and Carpenter (2013) also proposed an approach to detect and avoid spam workers. They measured the performance of worker by comparing worker’s labels to the current majority labels. Workers with bad performance can be identified and blocked. Lin et al. (2014) examined the relationship between worker accuracy and budget in the context of using crowdsourcing to train a machine learning classifier. They show that if the goal is to train a classifier on the labels, that the properties of th"
N15-1072,W12-3152,1,0.872422,"Missing"
N15-1072,D08-1027,0,0.034917,"Missing"
N15-1072,P11-1122,1,0.903411,"soliciting multiple translations of each source sentence and then choosing the best translation (Zaidan and CallisonBurch, 2011). Introduction Crowdsourcing is a promising new mechanism for collecting large volumes of annotated data at low cost. Many NLP researchers have started creating speech and language data through crowdsourcing (for example, Snow et al. (2008), Callison-Burch and Dredze (2010) and others). One NLP application that has been the focus of crowdsourced data collection is statistical machine translation (SMT) In this paper we focus on a different aspect of crowdsourcing than Zaidan and Callison-Burch (2011). We attempt to achieve the same high quality while minimizing the associated costs. We propose two complementary methods: (1) We reduce the number of translations that we solicit for each source sentence. Instead of soliciting a fixed number of translations for each foreign sentence, we stop soliciting translations after we get an acceptable one. We do so by building models to distinguish between 705 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 705–713, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Ling"
N15-1072,N12-1006,1,0.90294,"Missing"
N15-1072,N13-1069,0,0.0475018,"Missing"
N18-2016,H05-1059,0,\N,Missing
N18-2016,P05-1053,0,\N,Missing
N18-2016,P14-1038,0,\N,Missing
N18-2016,J02-3001,0,\N,Missing
N18-2016,Q13-1005,0,\N,Missing
N18-2016,D14-1082,0,\N,Missing
N18-2016,P15-2072,0,\N,Missing
N18-2016,D15-1090,0,\N,Missing
N18-2016,N16-1030,0,\N,Missing
N18-2016,D16-1264,0,\N,Missing
N18-2016,P16-1101,0,\N,Missing
N18-2016,P09-1010,0,\N,Missing
N18-2016,P10-1129,0,\N,Missing
N18-2025,D17-1126,1,0.909978,"Missing"
N18-2025,D15-1176,0,0.0914885,"Missing"
N18-2025,I13-1041,0,0.0487353,"Missing"
N18-2025,D15-1075,0,0.0388473,"en systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification. 1 Introduction Recently, there have been various neural network models proposed for sentence pair modeling tasks, including semantic similarity (Agirre et al., 2015), paraphrase identification (Dolan et al., 2004; Xu et al., 2015), natural language inference (Bowman et al., 2015), etc. Most, if not all, of these state-of-the-art neural models (Yin et al., 2016; Parikh et al., 2016; He and Lin, 2016; Tomar et al., 2017; Shen et al., 2017) have achieved the best performances for these tasks by using pretrained word embeddings, but results without pretraining are less frequently reported or noted. In fact, we will show that, even with fixed randomized word vectors, the pairwise word interaction model (He and Lin, 2016) based on contextual word vector 157 Proceedings of NAACL-HLT 2018, pages 157–163 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computat"
N18-2025,D16-1176,0,0.0203374,"63 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics and dot product over the outputs of the encoding layer: new state-of-the-art results in two social media datasets and competitive results in a news dataset for paraphrase identification without using any pretrained word embeddings. 2 → − → − → − → − D( h i , h j ) = [cos( h i , h j ), (5) → − → − L2Euclid( h i , h j ), → − → − DotP roduct( h i , h j )]. Sentence Pair Modeling with Subwords The current neural networks for sentence pair modeling (Yin et al., 2016; Parikh et al., 2016; He and Lin, 2016; Liu et al., 2016; Tomar et al., 2017; Wang et al., 2017; Shen et al., 2017, etc) follow a more or less similar design with three main components: (a) contextualized word vectors generated via Bi-LSTM, CNN, or attention, as inputs; (b) soft or hard word alignment and interactions across sentences; (c) and the output classification layer. Different models vary in implementation details, and most importantly, to capture the same essential intuition in the word alignment (also encoded with contextual information) – the semantic relation between two sentences depends largely on the relations of aligned chunks (Agi"
N18-2025,P15-1002,0,0.0998484,"Missing"
N18-2025,P16-2058,0,0.0458701,"Missing"
N18-2025,C04-1051,0,0.440119,"luding language modeling and machine translation, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification. 1 Introduction Recently, there have been various neural network models proposed for sentence pair modeling tasks, including semantic similarity (Agirre et al., 2015), paraphrase identification (Dolan et al., 2004; Xu et al., 2015), natural language inference (Bowman et al., 2015), etc. Most, if not all, of these state-of-the-art neural models (Yin et al., 2016; Parikh et al., 2016; He and Lin, 2016; Tomar et al., 2017; Shen et al., 2017) have achieved the best performances for these tasks by using pretrained word embeddings, but results without pretraining are less frequently reported or noted. In fact, we will show that, even with fixed randomized word vectors, the pairwise word interaction model (He and Lin, 2016) based on contextual word vector 157 Proceedings of NAACL-HLT 2018, pages 157–163 c New"
N18-2025,I05-5002,0,0.151489,"Missing"
N18-2025,D16-1244,0,0.0712129,"Missing"
N18-2025,W15-3904,0,0.0723379,"Missing"
N18-2025,S15-2011,0,0.0441555,"Missing"
N18-2025,D14-1162,0,0.107925,"two different composition functions to assemble subword embeddings into word embedding: Pairwise Word Interaction (PWI) Model Char C2W (Ling et al., 2015) applies Bi-LSTM to subword sequence c1 , ..., ck , then the last hid→ − in forward direction and the first den state h char k ← −char hidden state h 0 of the backward direction are linearly combined into word-level embedding w: a ) and w b = (w a , ..., w b ) Let wa = (w1a , ..., wm n 1 be the input sentence pair consisting of m and n tokens, respectively. Each word vector wi ∈ Rd is initialized with pretrained d-dimensional word embedding (Pennington et al., 2014; Wieting et al., 2015, 2016), then encoded with word context and sequence order through bidirectional LSTMs: → − → − h i = LST M f (wi , h i−1 ) ← − ← − h i = LST M b (wi , h i+1 ) ← → → − ← − h i = [ h i, h i] → − ← − h+ i = hi + hi Embedding Subwords in PWI Model → − ← − w = Wf · h char + Wb · h char +b 0 k (6) where Wf , Wb and b are parameters. (1) Char CNN (Kim et al., 2016) applies a convolution operation between subword sequence matrix 0 C and a filter F ∈ Rd ×l of width l to obtain a feature map f ∈ Rk−l+1 : (2) (3) (4) fj = tanh(hC[∗, j : j + l − 1], Fi + b) → − ← − where h i represe"
N18-2025,N16-1108,0,0.524304,"r experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification. 1 Introduction Recently, there have been various neural network models proposed for sentence pair modeling tasks, including semantic similarity (Agirre et al., 2015), paraphrase identification (Dolan et al., 2004; Xu et al., 2015), natural language inference (Bowman et al., 2015), etc. Most, if not all, of these state-of-the-art neural models (Yin et al., 2016; Parikh et al., 2016; He and Lin, 2016; Tomar et al., 2017; Shen et al., 2017) have achieved the best performances for these tasks by using pretrained word embeddings, but results without pretraining are less frequently reported or noted. In fact, we will show that, even with fixed randomized word vectors, the pairwise word interaction model (He and Lin, 2016) based on contextual word vector 157 Proceedings of NAACL-HLT 2018, pages 157–163 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics and dot product over the outputs of the encoding layer: new state-of-the-art results in two social medi"
N18-2025,P16-2067,0,0.0608778,"Missing"
N18-2025,P17-1194,0,0.0273943,".2). 2015) comes from the Task 1 of Semeval 2015 and was collected from tweets under the same trending topic, which contains varied topics and language styles. MSRP (Dolan and Brockett, 2005) was derived from clustered news articles reporting the same event in formal language. Table 1 shows vital statistics for all three datasets. After applying q filters with varied lengths, we can get the array w = [y1 , ..., yq ], which is followed by a one-layer highway network to generate final word embedding. 2.3 Auxiliary Language Modeling (LM) We adapted a multi-task structure, originally proposed by (Rei, 2017) for sequential tagging, to further improve the subword representations in sentence pair modeling. In addition to training the model for sentence pair tasks, we used a secondary language modeling objective that predicts the next word and previous word using softmax over the hidden states of Bi-LSTM as follows: T −1 X → − →)) E LM = − (log(P (wt+1 |− m t ← − E LM = − t=1 T X t=2 (log(P (wt−1 |← m−t )) 3.2 To compare models fairly without implementation variations, we reimplemented all models into a single PyTorch framework.1 We followed the setups in (He and Lin, 2016) and (Lan et al., 2017) fo"
N18-2025,C16-1030,0,0.0428003,"Missing"
N18-2025,D17-1122,0,0.041475,"Missing"
N18-2025,W17-4121,0,0.0370323,"Missing"
N18-2025,P17-1184,0,0.0400746,"Missing"
N18-2025,Q15-1025,0,0.0226293,"functions to assemble subword embeddings into word embedding: Pairwise Word Interaction (PWI) Model Char C2W (Ling et al., 2015) applies Bi-LSTM to subword sequence c1 , ..., ck , then the last hid→ − in forward direction and the first den state h char k ← −char hidden state h 0 of the backward direction are linearly combined into word-level embedding w: a ) and w b = (w a , ..., w b ) Let wa = (w1a , ..., wm n 1 be the input sentence pair consisting of m and n tokens, respectively. Each word vector wi ∈ Rd is initialized with pretrained d-dimensional word embedding (Pennington et al., 2014; Wieting et al., 2015, 2016), then encoded with word context and sequence order through bidirectional LSTMs: → − → − h i = LST M f (wi , h i−1 ) ← − ← − h i = LST M b (wi , h i+1 ) ← → → − ← − h i = [ h i, h i] → − ← − h+ i = hi + hi Embedding Subwords in PWI Model → − ← − w = Wf · h char + Wb · h char +b 0 k (6) where Wf , Wb and b are parameters. (1) Char CNN (Kim et al., 2016) applies a convolution operation between subword sequence matrix 0 C and a filter F ∈ Rd ×l of width l to obtain a feature map f ∈ Rk−l+1 : (2) (3) (4) fj = tanh(hC[∗, j : j + l − 1], Fi + b) → − ← − where h i represents forward hidden sta"
N18-2025,S15-2001,1,0.857037,"ling and machine translation, they have not been systematically studied in sentence pair modeling tasks where the semantic and string similarities between texts matter. Our experiments show that subword models without any pretrained word embedding can achieve new state-of-the-art results on two social media datasets and competitive results on news data for paraphrase identification. 1 Introduction Recently, there have been various neural network models proposed for sentence pair modeling tasks, including semantic similarity (Agirre et al., 2015), paraphrase identification (Dolan et al., 2004; Xu et al., 2015), natural language inference (Bowman et al., 2015), etc. Most, if not all, of these state-of-the-art neural models (Yin et al., 2016; Parikh et al., 2016; He and Lin, 2016; Tomar et al., 2017; Shen et al., 2017) have achieved the best performances for these tasks by using pretrained word embeddings, but results without pretraining are less frequently reported or noted. In fact, we will show that, even with fixed randomized word vectors, the pairwise word interaction model (He and Lin, 2016) based on contextual word vector 157 Proceedings of NAACL-HLT 2018, pages 157–163 c New Orleans, Louisian"
N18-2025,Q14-1034,1,0.850219,"ults Table 2 shows the experiment results on three datasets. We reported maximum F1 scores of any point on the precision-recall curve (Lipton et al., 2014) following previous work. Datasets We performed experiments on three benchmark datasets for paraphrase identification; each contained pairs of naturally occurring sentences manually labeled as paraphrases and non-paraphrases for binary classification: Twitter URL (Lan et al., 2017) was collected from tweets sharing the same URL with major news outlets such as @CNN. This dataset keeps a balance between formal and informal language. PIT-2015 (Xu et al., 2014, Word Models The word-level pairwise interaction models, even without pretraining (randomzied) or fine-tuning (fixed), showed strong performance across all three datasets. This reflects 1 The code and data can be obtained from the first and second author’s websites. 159 Word Models Subword Models Subword+LM Model Variations Logistic Regression (Lan et al., 2017) pretrained, fixed pretrained, updated randomized, fixed randomized, updated C2W, unigram C2W, bigram C2W, trigram CNN, unigram CNN, bigram CNN, trigram LM, C2W, unigram LM, C2W, bigram LM, C2W, trigram LM, CNN, unigram LM, CNN, bigram"
N18-2025,Q16-1019,0,0.0551442,"Missing"
N18-2025,D16-1209,0,\N,Missing
N18-2025,S16-1082,0,\N,Missing
P06-1047,W04-1017,0,0.225201,"Missing"
P06-1047,W03-0502,0,0.0620901,"Missing"
P06-1047,P05-3013,0,0.00625203,"ir term vectors was used to generate links and define link strength. The same idea was followed and investigated exten370 occurrences. They roughly relate to “did What”. One or more associated named entities are considered as what are denoted by linguists as event arguments. Four types of named entities are currently under the consideration. These are &lt;Person&gt;, &lt;Organization&gt;, &lt;Location&gt; and &lt;Date&gt;. They convey the information of “Who”, “Whom”, “When” and “Where”. A verb or an action noun is deemed as an event term only when it presents itself at least once between two named entities. sively (Mihalcea, 2005). Yoshioka and Haraguchi (2004) went one step further toward eventbased summarization. Two sentences were linked if they shared similar events. When tested on TSC-3, the approach favoured longer summaries. In contrast, the importance of the verbs and nouns constructing events was evaluated with PageRank as individual nodes aligned by their dependence relations (Vanderwende, 2004; Leskovec, 2004). Although we agree that the fabric of event constitutions constructed by their syntactic relations can help dig out the important events, we have two comments. First, not all verbs denote event happeni"
P06-1047,P05-1018,0,0.0200939,"Missing"
P06-1047,N04-3012,0,0.0634733,"Missing"
P06-1047,N03-1020,0,\N,Missing
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P13-2117,P07-1073,0,0.401219,"Missing"
P13-2117,P11-2048,0,0.373716,"Missing"
P13-2117,D10-1048,0,0.017931,"additional latent variables to a multi-instance multi-label model (Surdeanu et al., 2012) to solve this same problem.  Passage Retriever Figure 2: Overall system architecture: The system (1) matches relation instances to sentences and (2) learns a passage retrieval model to (3) provide relevance feedback on sentences; Relevant sentences (4) yield new relation instances which are added to the knowledge base; Finally, instances are again (5) matched to sentences to (6) create training data for relation extraction. 2 Encouraged by the recent success of simple methods for coreference resolution (Raghunathan et al., 2010) and inspired by pseudo-relevance feedback (Xu and Croft, 1996; Lavrenko and Croft, 2001; Matveeva et al., 2006; Cao et al., 2008) in the field of information retrieval, which expands or reformulates query terms based on the highest ranked documents of an initial query, we propose to increase the quality and quantity of training data generated by distant supervision for information extraction task using pseudo feedback. As shown in Figure 2, we expand an original knowledge base with possibly missing relation instances with information from the highest ranked sentences returned by a passage ret"
P13-2117,P11-1055,1,0.887885,"can significantly improve the performance of distant supervision. In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance * This work was done while Le Zhao was at Carnegie Mellon University. 1 http://www.freebase.com 665 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Documents   Relation Extractor Knowledge Base   Pseudo-relevant Relation Instances  training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these iss"
P13-2117,D12-1042,0,0.869486,"ve the performance of distant supervision. In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance * This work was done while Le Zhao was at Carnegie Mellon University. 1 http://www.freebase.com 665 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Documents   Relation Extractor Knowledge Base   Pseudo-relevant Relation Instances  training data due to their high-precision low-recall features, which were originally proposed by Mintz et al. (2009). We present a reliable and novel way to address these issues and achieve signific"
P13-2117,P12-1076,0,0.656475,"Missing"
P13-2117,D11-1132,0,0.0165186,"Missing"
P13-2117,N13-1095,1,0.731943,"Missing"
P13-2117,P09-1113,0,0.982706,"and are thus mislabeled as negative. These mislabelings dilute the discriminative capability of useful features and confuse the models. In this paper, we will show how reducing this source of noise can significantly improve the performance of distant supervision. In fact, our system corrects the relation labels of the above 6 sentences before training the relation extractor. Introduction A recent approach for training information extraction systems is distant supervision, which exploits existing knowledge bases instead of annotated texts as the source of supervision (Craven and Kumlien, 1999; Mintz et al., 2009; Nguyen and Moschitti, 2011). To combat the noisy training data produced by heuristic labeling in distant supervision, researchers (Bunescu and Mooney, 2007; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) exploited multi-instance * This work was done while Le Zhao was at Carnegie Mellon University. 1 http://www.freebase.com 665 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 665–670, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Documents   Relation Extractor Knowledge Base   Pseudo-rel"
P13-2117,I11-1117,1,\N,Missing
P14-2119,P09-1113,0,0.515421,"(Yarowsky, 2013; Blum and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our proIntroduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2 ) that participate in a relation, r, is likely to express the fact r(e1 ,e2 ) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example."
P14-2119,P07-1073,0,0.122629,"um and Mitchell, 1998; Collins and Singer, 2011; Nigam, 2001, and others), this is a learning scheme that combines unlabeled text and two training sources whose quantity and quality are radically different (Liang et al., 2009). To demonstrate the effectiveness of our proIntroduction Relation extraction is the task of tagging semantic relations between pairs of entities from free text. Recently, distant supervision has emerged as an important technique for relation extraction and has attracted increasing attention because of its effective use of readily available databases (Mintz et al., 2009; Bunescu and Mooney, 2007; Snyder and Barzilay, 2007; Wu and Weld, 2007). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2 ) that participate in a relation, r, is likely to express the fact r(e1 ,e2 ) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instan"
P14-2119,W99-0613,0,0.436382,"Missing"
P14-2119,P11-2048,0,0.0398241,"Missing"
P14-2119,P10-1030,0,0.12917,"Missing"
P14-2119,P11-1055,0,0.354374,"7). It automatically labels its own training data by heuristically aligning a knowledge base of facts with an unlabeled corpus. The intuition is that any sentence which mentions a pair of entities (e1 and e2 ) that participate in a relation, r, is likely to express the fact r(e1 ,e2 ) and thus forms a positive training example of r. One of most crucial problems in distant supervision is the inherent errors in the automatically generated training data (Roth et al., 2013). Table 1 illustrates this problem with a toy example. Sophisticated multi-instance learning algorithms (Riedel et al., 2010; Hoffmann et al., 2011; ∗ Most of the work was done when this author was at New York University 732 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 732–738, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Guideline g = {gi |i = 1, 2, 3}: types of entities, dependency path, span word (optional) person person, nsubj →← dobj, married person organization, nsubj →← prep of , became organization organization, nsubj →← prep of , company person person, poss →← appos, sister person person, poss →← appos, father person t"
P14-2119,P11-1115,1,0.771299,"Missing"
P14-2119,P03-1054,0,0.0202535,"Missing"
P14-2119,N13-1095,1,0.602879,"Missing"
P14-2119,D12-1042,0,0.465982,"ined by running experiments on the development dataset). Table 2 shows some examples in the final set G of extracted guidelines. 3 • zij ∈R ∪ NR: a latent variable that denotes the relation of the jth mention in the ith bag • hij ∈ R ∪ NR: a latent variable that denotes the refined relation of the mention xij We define relabeled relations hij as following:  r(g), if ∃!g ∈ G s.t.g ={gk}⊆{xij} hij (xij , zij )= zij , otherwise Guided DS Our goal is to jointly model human-labeled ground truth and structured data from a knowledge base in distant supervision. To do this, we extend the MIML model (Surdeanu et al., 2012) by adding a new layer as shown in Figure 1. The input to the model consists of (1) distantly supervised data, represented as a list of n bags1 with a vector yi of binary gold-standard labels, either P ositive(P ) or N egative(N ) for each relation r∈R; (2) generalized human-labeled ground truth, represented as a set G of feature conjunctions g={gi |i=1,2,3} associated with a unique relation r(g). Given a bag of sentences, xi , which mention an ith entity pair (e1 , e2 ), our goal is to correctly predict which relation is mentioned in each sentence, or NR if none of the relations under conside"
P14-2119,P12-1076,0,0.114183,"Missing"
P14-2119,P13-2117,1,0.827334,"Missing"
P14-2119,P95-1026,0,0.349402,"Missing"
P14-2119,P12-1087,0,0.581099,"Missing"
P14-2119,P05-1053,0,0.0361621,"ween the two arguments The Challenge Simply taking the union of the hand-labeled data and the corpus labeled by distant supervision is not effective since hand-labeled data will be swamped by a larger amount of distantly labeled data. An effective approach must recognize that the handlabeled data is more reliable than the automatically labeled data and so must take precedence in cases of conflict. Conflicts cannot be limited to those cases where all the features in two examples are the same; this would almost never occur, because of the dozens of features used by a typical relation extractor (Zhou et al., 2005). Instead we propose to perform feature selection to generalize human labeled data into training guidelines, and integrate them into latent variable model. 2.1 These three features are strong indicators of the type of relation between two entities. In some cases the semantic types of the arguments alone narrows the possibilities to one or two relation types. For example, entity types such as person and title often implies the relation personTitle. Some lexical items are clear indicators of particular relations, such as “brother” and “sister” for a sibling relationship We extract guidelines fro"
P14-2119,Q13-1030,0,\N,Missing
P15-1109,W13-3820,0,0.0528327,"Introduction Semantic role labeling (SRL) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is con"
P15-1109,D07-1002,0,0.168254,"he predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of"
P15-1109,P82-1020,0,0.899181,"Missing"
P15-1109,D14-1181,0,0.00204088,", 2008b). The constraints comes from annotation conventions of the task and other linguistic considerations. With dynamic programming, (T¨ackstr¨om et al., 2015) enhance the inference efficiency further. But designation of the constraints depends much on the linguistic knowledge. Nevertheless, the attempts of building end-toend systems for NLP become popular in recent years. Inspired by the work in computer vision, people hierarchically organized a window of words through convolution layers in deep form to account for the higher level of organization to solve the document classification task (Kim, 2014; Zhang and LeCun, 2015). Step further, people have also achieved success in directly mapping the sequence to sequence level target as the work in dependency parsing and machine translation (Vinyals et al., 2014; Sutskever et al., 2014). 3 Approaches In this paper, we propose an end-to-end system based on recurrent topology. Recurrent neural network (RNN) has natural advantage in modeling sequence problems. The past information is built 1128 up through the recurrent layer when model consumes the sequence word by word as shown in Eq. 1. x and y are the input and output of the recurrent layer wi"
P15-1109,W05-0620,0,0.182324,"Missing"
P15-1109,W05-0625,0,0.0648048,"ided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntactic tree provides an intermediate tag for improving the performance. However, building this syntactic tree also introduces the prediction risk inevitably. The analysis in (Pradhan et al., 2005) found that the major source of the incorrect predictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al., 2007; Koomen et al., 2005; Pradhan et al., 2005). Besides, feature templates in this classification task strongly rely on the expert experience. They need iterative modification after analyzing how the system performs on development data. When the corpus and data distribution are changed, or when people move to another language, the feature templates have to be re-designed. To address the above issues, (Collobert et al., 1127 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1127–1137, c Beijing, Chin"
P15-1109,P05-1022,0,0.0106519,"ple solve SRL problems in two major ways. The first one follows the traditional spirit widely used in NLP basic problems. A linear classifier is employed with feature templates. Most efforts focus on how to extract the feature templates that can best describe the text properties from training corpus. One of the most important features is from syntactic parsing, although syntactic parsing is also considered as a difficult problem. Thus system combination appear to be the general solution. In the work of (Pradhan et al., 2005), the syntactic tags are produced by Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) and Collins parser (Collins, 2003) respectively. Based on this, different systems are built to generate SRL tags. These SRL tags are used to extend the original feature templates, along with flat syntactic chunking results. At last another classifier learns the final SRL tag from the above results. In their analysis, the combination of three different syntactic view brings large improvement for the system. Similarly, Koomen et al. (Koomen et al., 2005) combined the system in another way. They built multiple classifiers and then all outputs are combined through an optimization problem. Surdean"
P15-1109,A00-2018,0,0.0718348,"Missing"
P15-1109,J03-4003,0,0.0340008,"rst one follows the traditional spirit widely used in NLP basic problems. A linear classifier is employed with feature templates. Most efforts focus on how to extract the feature templates that can best describe the text properties from training corpus. One of the most important features is from syntactic parsing, although syntactic parsing is also considered as a difficult problem. Thus system combination appear to be the general solution. In the work of (Pradhan et al., 2005), the syntactic tags are produced by Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) and Collins parser (Collins, 2003) respectively. Based on this, different systems are built to generate SRL tags. These SRL tags are used to extend the original feature templates, along with flat syntactic chunking results. At last another classifier learns the final SRL tag from the above results. In their analysis, the combination of three different syntactic view brings large improvement for the system. Similarly, Koomen et al. (Koomen et al., 2005) combined the system in another way. They built multiple classifiers and then all outputs are combined through an optimization problem. Surdeanu et al. fully discussed the combin"
P15-1109,W09-4621,0,0.00929256,"semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem"
P15-1109,W05-0634,0,0.730736,"this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntactic tree provides an intermediate tag for improving the performance. However, building this syntactic tree also introduces the prediction risk inevitably. The analysis in (Pradhan et al., 2005) found that the major source of the incorrect predictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al., 2007; Koomen et al., 2005; Pradhan et al., 2005). Besides, feature templates in this classification task strongly rely on the expert experience. They need iterative modification after analyzing how the system performs on development data. When the corpus and data distribution are changed, or when people move to another language, the feature templates have to be re-designed."
P15-1109,W13-3516,0,0.0651288,"Missing"
P15-1109,J08-2005,0,0.958081,"and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntactic tree provides an intermediate tag for improving the performance. However, building this syntactic tree also introduces the prediction risk inevitably. The analysis in (Pradhan et al., 2005) found that the major source of the incorrect predictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al., 2007; Koomen et al., 2005; Pradhan et al., 2005). Besides, feature templates in this classification task strongly rely on the expert experience"
P15-1109,P15-1150,0,0.0596,"Missing"
P15-1109,J08-2002,0,0.406663,"Missing"
P15-1109,P03-1002,0,0.0168905,"structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntacti"
P15-1109,Q15-1003,0,0.168922,"Missing"
P16-1076,P13-1158,0,0.0968794,"n of the questions are about facts or trivia. It has been a long pursuit to enable machines to answer such questions automatically. In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in"
P16-1076,P14-1133,0,0.0364646,"re about facts or trivia. It has been a long pursuit to enable machines to answer such questions automatically. In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in training data. Faced with"
P16-1076,Q15-1039,0,0.0328417,"ral efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in training data. Faced with these challenges, existing methods have exploited to incorporate prior knowledge into semantic parsers, to design models and represe"
P16-1076,D15-1038,0,0.0149571,"e training E(r), is insufficient in practice — while a large-scale KB has millions of subjects, only thousands of question-triple pairs are available for training. To alleviate the problem, we seek two potential solutions: a) pretrained embeddings, and b) type vector representation. The pretrained embedding approach utilizes unsupervised method to train entity embedings. In particular, we employ the TransE (Bordes et al., 2013), which trains the embedings of entities and relations by enforcing E(s) + E(r) = E(o) for every observed triple (s, r, o) ∈ K. As there exists other improved variants (Gu et al., 2015), TransE scales the best when KB size grows. Alternatively, type vector is a fixed (not trainable) vector representation of entities using type information. Since each entity in the KB has one or more predefined types, we can encode the entity as a vector (bag) of types. Each dimension of a type vector is either 1 or 0, indicating whether the entity is associated with a specific type or not. Thus, the dimensionality of a type vector is equal to the number of types in KB. Under this setting, with E(s) being a binary vector, let g(q) be a continuous vector with arbitrary value range can be probl"
P16-1076,D13-1160,0,0.263614,"elies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015), and more (Peng et al., 2015; Lee et al., 2015) have been explored under the bAbI syn801 A key difference from prior multi-step approach is that our method do not assume any independenc"
P16-1076,D14-1070,0,0.0292823,"Missing"
P16-1076,D14-1067,0,0.466091,"Missing"
P16-1076,D13-1161,0,0.0212997,"works (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015), and more (Peng et al., 2015; Lee et al., 201"
P16-1076,P13-1042,0,0.0906336,"recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015), and more (Peng et"
P16-1076,P11-1060,0,0.013846,"r, we propose to use recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, (Iyyer et al., 2014) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks (Weston et al., 2015), dynamic memory networks (Kumar et al., 2015"
P16-1076,D14-1179,0,0.0201928,"Missing"
P16-1076,J13-2005,0,0.00877152,"ur proposed method is closely related to the second line of research, since neural models are employed to learn semantic representations. As in (Bordes et al., 2015; Yih et al., 2014), we focus on single-fact questions. However, we propose to use recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement. Related Work The research of KB supported QA has evolved from earlier domain-specific QA (Zelle and Mooney, 1996; Tang and Mooney, 2001; Liang et al., 2013) to open-domain QA based on largescale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries (Liang et al., 2011; Cai and Yates, 2013; Kwiatkowski et al., 2013; Yao and Van Durme, 2014). Recent progresses include designing KB specific logical representation and parsing grammar (Berant et al., 2013), using distant supervision (Berant et al., 2013), utilizing Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences."
P16-1076,D14-1162,0,0.0861204,"Missing"
P16-1076,Q14-1030,0,0.019754,"achines to answer such questions automatically. In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions. A knowledge ∗ Wei Xu Baidu Research xuwei06@baidu.com sˆ, rˆ = arg max p(s, r|q) Part of the work was done while at Baidu. s,r∈K 800 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 800–810, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (1) paraphrase information (Fader et al., 2013; Berant and Liang, 2014), requiring little questionanswer pairs (Reddy et al., 2014), and exploiting ideas from agenda-based parsing (Berant and Liang, 2015). Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts — a large-scale KB such as Freebase contains billions of triples, b) the huge variety of language — there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision — most combinations of s, r, q are not expressed in training data. Faced with these challenges, existing methods have exploited to incorp"
P16-1076,P05-1044,0,0.0251165,"41,880 facts. There are alternative datasets available, such as WebQuestions (Berant et al., 2013) and (17) where r(j) is one of the Mr negative samples (i.e. s(i) 6→ r(j) ) randomly sampled from R, and γr is 2 N X K  X i=1 k=1 As the two problems defined by equation (16) take the standard form of classification, theoretically, cross entropy can used as the training objective. However, computing the partition function is often intractable, especially for pθs (s|r, q), since there can be millions of entities in the KB. Faced with this problem, classic solutions include contrastive estimation (Smith and Eisner, 2005), importance sampling approximation (Bengio et al., 2003), and hinge loss with negative samples (Collobert and Weston, 2008). In this work, we utilize the hinge loss with negative samples as the training objective. Specifically, the loss function w.r.t θr has the form L(θr ) =  + u(s(j) , r(i) , q (i) )     + 1 − E(s(i) )k log 1 − g(q (i) )k Approximation with Negative Samples Mr N X X  max 0, γs − u(s(i) , r(i) , q (i) ) Despite the negative sample based approximation, there is another practical difficulty when type vector is used as the subject representation. Specifically, computing"
P16-1076,D14-1071,0,0.0580256,"with large-scale knowledge bases. The contributions of this paper are, In contrast, another line of research tackles the problem by deep learning powered similarity matching. The core idea is to learn semantic representations of both the question and the knowledge from observed data, such that the correct supporting evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015; Yang et al., 2014) use relatively shallow embedding models to represent the question and knowledge, (Yih et al., 2014; Yih et al., 2015) employ a convolutional neural network (CNN) to produce the representation. In the latter case, both the question and the relation are treated as a sequence of letter-trigram patterns, and fed into two parameter shared CNNs to get their embeddings. What’s more, instead of measuring the similarity between a question and an evidence triple with a single model as in (Bordes et al., 2015), (Yih et al., 2014; Yih et al., 2015) adopt a multi-stage approach. In each stage, one element"
P16-1076,P14-1090,0,0.202769,"Missing"
P16-1076,P14-2105,0,0.509324,"research tackles the problem by deep learning powered similarity matching. The core idea is to learn semantic representations of both the question and the knowledge from observed data, such that the correct supporting evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015; Yang et al., 2014) use relatively shallow embedding models to represent the question and knowledge, (Yih et al., 2014; Yih et al., 2015) employ a convolutional neural network (CNN) to produce the representation. In the latter case, both the question and the relation are treated as a sequence of letter-trigram patterns, and fed into two parameter shared CNNs to get their embeddings. What’s more, instead of measuring the similarity between a question and an evidence triple with a single model as in (Bordes et al., 2015), (Yih et al., 2014; Yih et al., 2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question to produce a partial similarity score by a dedicated m"
P16-1076,P15-1128,0,0.758684,"he problem by deep learning powered similarity matching. The core idea is to learn semantic representations of both the question and the knowledge from observed data, such that the correct supporting evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While (Bordes et al., 2014b; Bordes et al., 2014a; Bordes et al., 2015; Yang et al., 2014) use relatively shallow embedding models to represent the question and knowledge, (Yih et al., 2014; Yih et al., 2015) employ a convolutional neural network (CNN) to produce the representation. In the latter case, both the question and the relation are treated as a sequence of letter-trigram patterns, and fed into two parameter shared CNNs to get their embeddings. What’s more, instead of measuring the similarity between a question and an evidence triple with a single model as in (Bordes et al., 2015), (Yih et al., 2014; Yih et al., 2015) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question to produce a partial similarity score by a dedicated model. Then, these p"
P16-1185,J93-2003,0,0.0781716,"Missing"
P16-1185,P05-1033,0,0.174897,"t show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language model like SMT"
P16-1185,W14-4012,0,0.0289659,"Missing"
P16-1185,D14-1061,0,0.0330026,"Missing"
P16-1185,P15-1001,0,0.069953,"Missing"
P16-1185,D13-1176,0,0.0583835,"arget and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they h"
P16-1185,P13-1140,0,0.0205566,"Missing"
P16-1185,E12-1014,0,0.015191,"pora for machine translation and (2) autoencoders in unsupervised and semi-supervised learning. 4.1 Exploiting Monolingual Corpora for Machine Translation Exploiting monolingual corpora for conventional SMT has attracted intensive attention in recent years. Several authors have introduced transductive learning to make full use of monolingual corpora (Ueffing et al., 2007; Bertoldi and Federico, 2009). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until convergence. While Klementiev et al. (2012) propose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval techniques. Another important line of research is to treat translation on monolingual corpora as a decipherment problem (Ravi and Knight, 2011; Dou et al., 2014). 1972 Closely related to Gulccehre et al. (2015) and Sennrich et al. (2015), our approach focuses on learning birectional NMT models via autoencoders on monolingual corpora. The major advantages of our approach are the transparency to network arch"
P16-1185,N03-1017,0,0.145099,"hineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems. 1 Introduction End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increasing attention in recent several years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005), NMT has proven to excel in model∗ Yang Liu is the corresponding author. ing long-distance dependencies by enhancing recurrent neural networks (RNNs) with the gating (Hochreiter and Schmidhuber, 1993; Cho et al., 2014; Sutskever et al., 2014) and attention mechanisms (Bahdanau et al., 2015). However, most existing NMT approaches suffer from a major drawback: they heavily rely on parallel corpora for training translation models. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a separate language"
P16-1185,P07-2045,0,0.0608607,"ts serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of tra"
P16-1185,P15-1002,0,0.100715,"N SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingual corpus, and λ1 = 0 and λ2 = 0.1 for source monolingual corpus incorporation. The threshold of gradient clipping is set to 0.05. The parameters of our model are initialized by the model trained on parallel corpus. 3.2 Effect of Sample Size k As the inference of our approach is intracta"
P16-1185,P03-1021,0,0.0280237,"sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow Luong et al. (2015) to address rare words. On top of RNNS EARCH, our approach is capable of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ1 = 0.1 and 1969 λ2 = 0 when we add the target monolingua"
P16-1185,P02-1040,0,0.119118,"nese validation set. For Chinese-to-English translation, we use the NIST 2006 Chinese-English dataset as the validation set for hyper-parameter optimization and model selection. The NIST 2002, 2003, 2004, and 2005 datasets serve as test sets. Each Chinese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference translations as a source sentence and the original input Chinese sentence as the single reference translation. The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. We compared our approach with two state-ofthe-art SMT and NMT systems: 2. RNN SEARCH (Bahdanau et al., 2015): an attention-based NMT system. 1. M OSES (Koehn et al., 2007): a phrase-based SMT system; For M OSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm (Och, 2003). We use the SRILM toolkit (Stolcke, 2002) to train 4-gram language models. For RNNS EARCH, we use the parallel corpus to train the attention-base"
P16-1185,P11-1002,0,0.0466152,"Missing"
P16-1185,D11-1014,0,0.0792271,"icted to government documents and news reports. Therefore, the availability of large-scale, high-quality, and wide-coverage parallel corpora becomes a major obstacle for NMT. 2.2 the observed source sentence via a latent target sentence: → − ← − P (x0 |x; θ , θ ) X ← − = P (x0 , y|x; θ ) y Autoencoders on Monolingual Corpora It is appealing to explore the more readily available, abundant monolingual corpora to improve NMT. Let us first consider an unsupervised setting: how to train NMT models on a monolingual corpus T = {y(t) }Tt=1 ? Our idea is to leverage autoencoders (Vincent et al., 2010; Socher et al., 2011): (1) encoding an observed target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model. For example, as shown in Figure 1(b), given an observed English sentence “Bush held a talk with Sharon”, a target-to-source translation model (i.e., encoder) transforms it into a Chinese translation “bushi yu shalong juxing le huitan” that is unobserved on the training data (highlighted in grey). Then, a source-to-target translation model (i.e., decoder) reconstructs t"
P16-1185,P07-1004,0,0.451948,"y of the observed source sentence x0 from the latent target sentence. As a result, monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in a semi-supervised setting. and attention model are fixed when training on these pseudo parallel sentence pairs. In the second approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus. The monolingual corpus and its translations constitute an additional pseudo parallel corpus. Similar ideas have also been suggested in conventional SMT (Ueffing et al., 2007; Bertoldi and Federico, 2009). Sennrich et al. (2015) report that their approach significantly improves translation quality across a variety of language pairs. In this paper, we propose semi-supervised learning for neural machine translation. Given labeled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source translation models. The key idea is to append a reconstruction term to the training objective, which aims to reconstruct the observed monolingual corpora using an autoencoder. In the autoencoder, the sou"
P16-1185,W09-0432,0,\N,Missing
P18-1243,D14-1179,0,0.00466341,"guage from scratch, acquire the trans2609 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2609–2619 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ferable skill of actively seeking and memorizing information about novel objects, and develop the one-shot learning ability, purely through conversational interaction with a teacher. 2 Related Work Supervised Language Learning. Deep neural network-based language learning has seen great success on many applications, including machine translation (Cho et al., 2014b), dialogue generation (Wen et al., 2015; Serban et al., 2016), image captioning and visual question answering (?Antol et al., 2015). For training, a large amount of labeled data is needed, requiring significant efforts to collect. Moreover, this setting essentially captures the statistics of training data and does not respect the interactive nature of language learning, rendering it less flexible for acquiring new knowledge without retraining or forgetting (Stent and Bangalore, 2014). Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the perfo"
P18-1243,P16-1153,0,0.116659,"fforts to collect. Moreover, this setting essentially captures the statistics of training data and does not respect the interactive nature of language learning, rendering it less flexible for acquiring new knowledge without retraining or forgetting (Stent and Bangalore, 2014). Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model according to certain metrics (Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which explored language learning through pure textual dialogues. However, in these works (He et al., 2016; Weston, 2016; Li et al., 2017), a set of candidate sequences is provided and the action is to select one from the set. Our main focus is rather on learning language from scratch: the agent has to learn to generate a sequence action rather than to simply select one from a provided candidate set. Communication and Emergence of Language. Recent studies have examined learning to communicate (Foerster et al., 2016; Sukhb"
P18-1243,D16-1127,0,0.019229,"Antol et al., 2015). For training, a large amount of labeled data is needed, requiring significant efforts to collect. Moreover, this setting essentially captures the statistics of training data and does not respect the interactive nature of language learning, rendering it less flexible for acquiring new knowledge without retraining or forgetting (Stent and Bangalore, 2014). Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model according to certain metrics (Ranzato et al., 2016; Bahdanau et al., 2017; Li et al., 2016; Yu et al., 2017). Our work is also related to RL in natural language action space (He et al., 2016) and shares a similar motivation with Weston (2016) and Li et al. (2017), which explored language learning through pure textual dialogues. However, in these works (He et al., 2016; Weston, 2016; Li et al., 2017), a set of candidate sequences is provided and the action is to select one from the set. Our main focus is rather on learning language from scratch: the agent has to learn to generate a sequence action rather than to simply select one from a provided candidate set. Communication and Emer"
P18-1243,P16-1224,0,0.0575273,"enerating a response through interpreter and speaker. Left: Initially, the learner can barely say anything meaningful. Middle: Later it can produce meaningful responses for interaction. Right: After training, when confronted with an image of cherry, which is a novel class that the learner never saw before during training, the learner can ask a question about it (“what is it”) and generate a correct statement (“this is cherry”) for another instance of cherry after only being taught once. teacher. This makes our game distinct from other seemingly relevant games, in which the agent cannot speak (Wang et al., 2016) or “speaks” by selecting a candidate from a provided set (He et al., 2016; Weston, 2016; Li et al., 2017) rather than generating sentences by itself, or games mainly focus on slow learning (Das et al., 2017; Strub et al., 2017) and falls short on one-shot learning. In this game, sessions (Sl ) are randomly instantiated during interaction. Testing sessions are constructed with a separate dataset with concepts that never appear before during training to evaluate the language and one-shot learning ability. Within a session, the teacher randomly selects an object and interacts with the learner ab"
P18-1243,D15-1199,0,0.0141826,"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2609–2619 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ferable skill of actively seeking and memorizing information about novel objects, and develop the one-shot learning ability, purely through conversational interaction with a teacher. 2 Related Work Supervised Language Learning. Deep neural network-based language learning has seen great success on many applications, including machine translation (Cho et al., 2014b), dialogue generation (Wen et al., 2015; Serban et al., 2016), image captioning and visual question answering (?Antol et al., 2015). For training, a large amount of labeled data is needed, requiring significant efforts to collect. Moreover, this setting essentially captures the statistics of training data and does not respect the interactive nature of language learning, rendering it less flexible for acquiring new knowledge without retraining or forgetting (Stent and Bangalore, 2014). Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model ac"
P19-1137,P17-1038,0,0.185671,"Missing"
P19-1137,D14-1164,0,0.0357125,"Missing"
P19-1137,P11-1055,0,0.181204,"a small number of human annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et a"
P19-1137,P18-1161,1,0.894749,"Missing"
P19-1137,P16-1200,1,0.908825,"e a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken th"
P19-1137,N16-1104,0,0.153144,"a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the k"
P19-1137,D17-1005,0,0.213057,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,D17-1189,0,0.680844,"et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorporate pattern-based labeling, where the key idea was to regard both DS and pattern-based heuristics as the weak supervision sources and develop a weak-label-fusion (WLF) model to produce denoised labels. However, the major limitation of the WLF paradigm lies in the requirement of human experts to write relation-specific patterns. Unfortunately, writing good patterns is both a highskill and labor-intensive task that requires experts to learn detailed pattern-composing instructions, examine adequate examples, tune patterns for different corner cases, etc. For example, the sp"
P19-1137,P17-1040,0,0.0220481,"et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant a"
P19-1137,P09-1113,0,0.925853,"s defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are t"
P19-1137,D12-1104,0,0.0947384,"improved the performance of the vanilla LSTM (Hochreiter and Schmidhuber, 1997) by utilizing RL to discover structured representations and Li et al. (2016) interpreted the sentiment prediction of neural models by employing RL to find the decision-changing phrases. However, NRE models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives"
P19-1137,C14-1220,0,0.100927,"atives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent ne"
P19-1137,D14-1162,0,0.0810357,"Missing"
P19-1137,P12-1087,0,0.0275027,"Missing"
P19-1137,P14-2119,1,0.82305,"itives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a result heavily weaken the model performance (Riedel et al., 2010). Recent research has realized that introducing appropriate human efforts is essential for reducing such labeling noises. For example, Zhang et al. (2012); Pershina et al. (2014); Angeli et al. (2014); Liu et al. (2016) mixed a small set of crowd-annotated labels with purely DS-generated noise labels. However, they found that only sufficiently large and high-quality human labels can bring notable improvements, because there are 1419 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1419–1429 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics significantly larger number of noise labels. To enlarge the impact of human efforts, Ratner et al. (2016); Liu et al. (2017a) proposed to incorp"
P19-1137,P18-1199,0,0.0429382,"se models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2018; Qin et al., 2018) adopted RL to train an agent that interacts with the NRE model to learn how to remove or alter noise labels. These methods work without human intervention by utilizing the consistency and difference between DS-generated labels and model-predicted ones. However, such methods can neither discover 1420 noise labels that coincide with the model predictions nor explain the reasons for removed or altered labels. As discussed in the introduction, introducing human efforts is a promising direction to contribute both significant and interpretable improvements, which is also the focus of this paper. As"
P19-1137,P05-1053,0,0.214768,": Two types of error labels, false negatives (FN) and false positives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it"
P19-1137,P16-2034,0,0.0665773,"Missing"
P19-1137,P15-1061,0,0.0475427,"ositives (FP), caused by DS. Introduction Relation extraction aims to extract relational facts from the plain text and can benefit downstream knowledge-driven applications. A relational fact is defined as a relation between a head entity and a tail entity, e.g., (Letizia Moratti, Birthplace, Milan). The conventional methods often regard relation extraction as a supervised classification task that predicts the relation type between two detected entities mentioned in a sentence, including both statistical models (Zelenko et al., 2003; Zhou et al., 2005) and neural models (Zeng et al., 2014; dos Santos et al., 2015). These supervised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction ("
P19-1137,D12-1042,0,0.111105,"n annotations; • presenting both significant and interpretable performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al.,"
P19-1137,P12-1076,0,0.0302716,"models are unique because we only care about the semantic inter-entity relation mentioned in the sentence. To the best of our knowledge, we are the first to extract patterns from NRE models by RL. We also note that the relational-pattern mining has been extensively studied (Califf and Mooney, 1999; Carlson et al., 2010; Nakashole et al., 2012; Jiang et al., 2017). Different from those studies, our pattern-extraction method 1) is simply based on RL, 2) does not rely on any lexical or syntactic annotation, and 3) can be aware of the pattern importance via the prediction of NRE models. Besides, Takamatsu et al. (2012) inferred negative syntactic patterns via the example-pattern-relation co-occurrence and removed the false-positive labels accordingly. In contrast, built upon modern neural models, our method not only reduces negative patterns to alleviate false positives but also reinforces positive patterns to address false negatives at the same time. 3 Methodology Provided with DS-generated data and NRE models trained on them, DIAG-NRE can generate high-quality patterns for the WLF stage to produce denoised labels. As Figure 2 shows, DIAG-NRE contains two key stages in general: pattern extraction (Section"
P19-1137,D17-1187,0,0.107221,"le performance improvements as well as intuitive diagnostic analyses. Particularly, for one relation with severe false negative noises, we improve the F1 score by about 0.4. To the best of our knowledge, we are the first to explicitly reveal and address this severe noise problem for that dataset. 2 Related Work To reduce labeling noises of DS, earlier work attempted to design specific model architectures that can better tolerate labeling noises, such as the multi-instance learning paradigm (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017). These models relax the raw assumption of DS by grouping multiple sentences that mention the same entity pair together as a bag and then assuming that at least one sentence in this bag expresses the relation. This weaker assumption can alleviate the noisy-labeling problem to some extent, but this problem still exists at the bag level, and Feng et al. (2018) discovered that bag-level models struggled to do sentence-level predictions. Later work tried to design a dynamic labeladjustment strategy for training (Liu et al., 2017b; Luo et al., 2017). Especially, the most recent work (Feng et al., 2"
P19-1137,D15-1203,0,0.38233,"vised models require a large number of human-annotated data to train, which are both expensive and time-consuming to collect. Therefore, Craven et al. (1999); Mintz et al. (2009) proposed distant supervision (DS) to automatically generate large-scale training data for relation extraction, by aligning relational facts from a knowledge base (KB) to plain text and assuming that every sentence mentioning two entities can describe their relationships in the KB. As DS can acquire large-scale data without human annotation, it has been widely adopted by recent neural relation extraction (NRE) models (Zeng et al., 2015; Lin et al., 2016). Although DS is both simple and effective in many cases, it inevitably introduces intolerable labeling noises. As Figure 1 shows, there are two types of error labels, false negatives and false positives. The reason for false negatives is that a sentence does describe two entities about a target relation, but the fact has not been covered by the KB yet. While for false positives, it is because not all sentences mentioning entity pairs actually express their relations in the KB. The noisy-labeling problem can become severe when the KB and text do not match well and as a resul"
P19-1242,P17-1067,0,0.0224241,"and modeling user interests for recommendations (Chen et al., 2016). It can also help in collecting data of higher volume and quality by providing a more nuanced interpretation of its content, as shown for emotion analysis (Qadir and Riloff, 2014), sarcasm and irony detection (Maynard and Greenwood, 2014; Huang et al., 2018). Better semantic analysis of hashtags can also potentially be applied to hashtag annotation (Wang et al., 2019), to improve distant supervision labels in training classifiers for tasks such as sarcasm (Bamman and Smith, 2015), sentiment (Mohammad et al., 2013), emotions (Abdul-Mageed and Ungar, 2017); and, more generally, as labels for pre-training representations of words (Weston et al., 2014), sentences (Dhingra et al., 2016), and images (Mahajan et al., 2018). 8 Conclusion We proposed a new pairwise neural ranking model for hashtag segmention and showed significant performance improvements over the state-of-theart. We also constructed a larger and more curated dataset for analyzing and benchmarking Ofcourse #clownshoes #altright #IllinoisNazis #FinallyAtpeaceWith people calling me “Kim Fatty the Third” Leslie Odom Jr. sang that. #ThankYouObama After some 4 months of vegetarianism .. it"
P19-1242,D12-1091,0,0.0349539,"Missing"
P19-1242,W16-3915,0,0.157927,"Berg-Kirkpatrick et al., 2012). (C ¸ elebi and Ozg¨ 5 Experiments In this section, we present experimental results that compare our proposed method with the other state-of-the-art approaches on hashtag segmentation datasets. The next section will show experiments of applying hashtag segmentation to the popular task of sentiment analysis. 5.1 Existing Methods We compare our pairwise neural ranker with the following baseline and state-of-the-art approaches: (a) The original hashtag as a single token; (b) A rule-based segmenter, which employs a set of word-shape rules with an English dictionary (Billal et al., 2016); (c) A Viterbi model which uses word frequencies from a book corpus7 (Berardi et al., 2011); 7 (d) The specially developed GATE Hashtag Tokenizer from the open source toolkit,8 which combines dictionaries and gazetteers in a Viterbi-like algorithm (Maynard and Greenwood, 2014); (e) A maximum entropy classifier (MaxEnt) trained on the STANlarge training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current ¨ ur, 2017); state-of-the-art (C¸elebi and Ozg¨ (f) Our reimplementation of the Word Breaker algorithm which uses beam search and a Twitt"
P19-1242,P16-2044,0,0.0237715,"providing a more nuanced interpretation of its content, as shown for emotion analysis (Qadir and Riloff, 2014), sarcasm and irony detection (Maynard and Greenwood, 2014; Huang et al., 2018). Better semantic analysis of hashtags can also potentially be applied to hashtag annotation (Wang et al., 2019), to improve distant supervision labels in training classifiers for tasks such as sarcasm (Bamman and Smith, 2015), sentiment (Mohammad et al., 2013), emotions (Abdul-Mageed and Ungar, 2017); and, more generally, as labels for pre-training representations of words (Weston et al., 2014), sentences (Dhingra et al., 2016), and images (Mahajan et al., 2018). 8 Conclusion We proposed a new pairwise neural ranking model for hashtag segmention and showed significant performance improvements over the state-of-theart. We also constructed a larger and more curated dataset for analyzing and benchmarking Ofcourse #clownshoes #altright #IllinoisNazis #FinallyAtpeaceWith people calling me “Kim Fatty the Third” Leslie Odom Jr. sang that. #ThankYouObama After some 4 months of vegetarianism .. it’s all the same industry. #cutoutthecrap Table 10: Sentiment analysis examples where our HashtagMaster segmentation tool helped. R"
P19-1242,N13-1037,0,0.0124006,"ons (e.g., lion head vs. lionhead). Table 1 shows several examples of single- and multi-token hashtags. While most hashtags represent a mix of standard tokens, named entities and event names are prevalent and pose challenges to both human and automatic comprehension, as these are more likely to be rare tokens. Hashtags also tend to be shorter to allow fast typing, to attract attention or to satisfy length limitations imposed by some social media platforms. Thus, they tend to contain a large number of abbreviations or non-standard spelling variations (e.g., #iloveu4eva) (Han and Baldwin, 2011; Eisenstein, 2013), which hinders their understanding. The goal of our study is to build efficient methods for automatically splitting a hashtag into a meaningful word sequence. Our contributions are: • A larger and better curated dataset for this task; • Framing the problem as pairwise ranking using novel neural approaches, in contrast to previous work which ignored the relative order of candidate segmentations; • A multi-task learning method that uses different sets of features to handle different types of 2538 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2538"
P19-1242,W10-0713,0,0.25171,"tegorize the millions of posts generated daily on Twitter, Instagram, etc. They are useful in search, tracking content about a certain topic (Berardi et al., 2011; Ozdikis et al., 2012), or discovering emerging trends (Sampson et al., 2016). Hashtags often carry very important information, such as emotion (Abdul-Mageed and Ungar, 1 Multi-token #toyotaprius #ipv6summit #epicfall #iloveu4eva Our toolkit along with the code and data are publicly available at https://github.com/mounicam/ hashtag_master 2017), sentiment (Mohammad et al., 2013), sarcasm (Bamman and Smith, 2015), and named entities (Finin et al., 2010; Ritter et al., 2011). However, inferring the semantics of hashtags is nontrivial since many hashtags contain multiple tokens joined together, which frequently leads to multiple potential interpretations (e.g., lion head vs. lionhead). Table 1 shows several examples of single- and multi-token hashtags. While most hashtags represent a mix of standard tokens, named entities and event names are prevalent and pose challenges to both human and automatic comprehension, as these are more likely to be rare tokens. Hashtags also tend to be shorter to allow fast typing, to attract attention or to satis"
P19-1242,L16-1476,0,0.05832,"Missing"
P19-1242,P11-1038,0,0.0420393,"potential interpretations (e.g., lion head vs. lionhead). Table 1 shows several examples of single- and multi-token hashtags. While most hashtags represent a mix of standard tokens, named entities and event names are prevalent and pose challenges to both human and automatic comprehension, as these are more likely to be rare tokens. Hashtags also tend to be shorter to allow fast typing, to attract attention or to satisfy length limitations imposed by some social media platforms. Thus, they tend to contain a large number of abbreviations or non-standard spelling variations (e.g., #iloveu4eva) (Han and Baldwin, 2011; Eisenstein, 2013), which hinders their understanding. The goal of our study is to build efficient methods for automatically splitting a hashtag into a meaningful word sequence. Our contributions are: • A larger and better curated dataset for this task; • Framing the problem as pairwise ranking using novel neural approaches, in contrast to previous work which ignored the relative order of candidate segmentations; • A multi-task learning method that uses different sets of features to handle different types of 2538 Proceedings of the 57th Annual Meeting of the Association for Computational Ling"
P19-1242,W11-2123,0,0.0221518,"andidate, the length of each word, the proportion of words in an English dictionary3 or Urban Dictionary4 (Nguyen et al., 2018), ngram counts from Google Web 1TB corpus (Brants and Franz, 2006), and ngram probabilities from trigram language models trained on the Gigaword corpus (Graff and Cieri, 2003) and 2541 3 4 https://pypi.org/project/pyenchant https://www.urbandictionary.com 1.1 billion English tweets from 2010, respectively. We train two language models on each corpus: one with Good-Turing smoothing using SRILM (Stolcke, 2002) and the other with modified KneserNey smoothing using KenLM (Heafield, 2011). We also add boolean features, such as if the candidate is a named-entity present in the list of Wikipedia titles, and if the candidate segmentation s and its corresponding hashtag h satisfy certain word-shapes (more details in appendix A.1). Similarly, for hashtag h, we extract the feature vector h consisting of hashtag length, ngram count of the hashtag in Google 1TB corpus (Brants and Franz, 2006), and boolean features indicating if the hashtag is in an English dictionary or Urban Dictionary, is a named-entity, is in camel case, ends with a number, and has all the letters as consonants. We"
P19-1242,D11-1125,0,0.0198697,"tteers in a Viterbi-like algorithm (Maynard and Greenwood, 2014); (e) A maximum entropy classifier (MaxEnt) trained on the STANlarge training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current ¨ ur, 2017); state-of-the-art (C¸elebi and Ozg¨ (f) Our reimplementation of the Word Breaker algorithm which uses beam search and a Twitter ngram language model (Wang et al., 2011); (g) A pairwise linear ranker which we implemented for comparison purposes with the same features as our neural model, but using perceptron as the underlying classifier (Hopkins and May, 2011) and minimizing the hinge Project Gutenberg http://norvig.com/big.txt 2543 8 https://gate.ac.uk/ All A MRR 74.9 85.1 88.6 92.6 80.1 87.0 88.9 92.7 90.0 93.4 ne G seroo N Li d-T ey ng ur ui ing sti c Multi A MRR 56.0 75.3 85.9 91.8 71.6 82.6 86.2 92.3 89.0 93.7 K Single A MRR Kneser-Ney 95.4 95.7 Good-Turing (GT) 91.4 93.5 Linguistic (Ling) 89.4 91.7 GT + Ling 92.4 93.9 All Features 91.1 93.1 ◦ • ◦ ◦ • • ◦ • Table 6: Evaluation of automatic hashtag segmentation (MSE) with different features on the STANlarge dev set. A denotes accuracy@1. While Kneser-Ney features perform well on single-token ha"
P19-1242,P18-2122,0,0.0232524,"entiment words only in the hashtags but not in the rest of the tweet. 7 Other Related Work Automatic hashtag segmentation can improve the performance of many applications besides sentiment analysis, such as text classification (Billal et al., 2016), named entity linking (Bansal et al., 2015) and modeling user interests for recommendations (Chen et al., 2016). It can also help in collecting data of higher volume and quality by providing a more nuanced interpretation of its content, as shown for emotion analysis (Qadir and Riloff, 2014), sarcasm and irony detection (Maynard and Greenwood, 2014; Huang et al., 2018). Better semantic analysis of hashtags can also potentially be applied to hashtag annotation (Wang et al., 2019), to improve distant supervision labels in training classifiers for tasks such as sarcasm (Bamman and Smith, 2015), sentiment (Mohammad et al., 2013), emotions (Abdul-Mageed and Ungar, 2017); and, more generally, as labels for pre-training representations of words (Weston et al., 2014), sentences (Dhingra et al., 2016), and images (Mahajan et al., 2018). 8 Conclusion We proposed a new pairwise neural ranking model for hashtag segmention and showed significant performance improvements"
P19-1242,R15-1015,0,0.0673605,"Missing"
P19-1242,D14-1127,0,0.0177656,"in the Twitterbased sentiment lexicon (Tang et al., 2014) and 125 tweets contain sentiment words only in the hashtags but not in the rest of the tweet. 7 Other Related Work Automatic hashtag segmentation can improve the performance of many applications besides sentiment analysis, such as text classification (Billal et al., 2016), named entity linking (Bansal et al., 2015) and modeling user interests for recommendations (Chen et al., 2016). It can also help in collecting data of higher volume and quality by providing a more nuanced interpretation of its content, as shown for emotion analysis (Qadir and Riloff, 2014), sarcasm and irony detection (Maynard and Greenwood, 2014; Huang et al., 2018). Better semantic analysis of hashtags can also potentially be applied to hashtag annotation (Wang et al., 2019), to improve distant supervision labels in training classifiers for tasks such as sarcasm (Bamman and Smith, 2015), sentiment (Mohammad et al., 2013), emotions (Abdul-Mageed and Ungar, 2017); and, more generally, as labels for pre-training representations of words (Weston et al., 2014), sentences (Dhingra et al., 2016), and images (Mahajan et al., 2018). 8 Conclusion We proposed a new pairwise neural ranki"
P19-1242,E03-1076,0,0.0450874,"he official metric for the task. 2 Background and Preliminaries Current approaches for hashtag segmentation can be broadly divided into three categories: (a) gazeteer and rule based (Maynard and Greenwood, 2014; Declerck and Lendvai, 2015; Billal et al., 2016), (b) word boundary detection (C ¸ elebi and ¨ ur, 2017, 2016), and (c) ranking with lanOzg¨ guage model and other features (Wang et al., 2011; Bansal et al., 2015; Berardi et al., 2011; Reuter et al., 2016; Simeon et al., 2016). Hashtag segmentation approaches draw upon work on compound splitting for languages such as German or Finnish (Koehn and Knight, 2003) and word segmentation (Peng and Schuurmans, 2001) for languages with no spaces between words such as Chinese (Sproat and Shih, 1990; Xue and Shen, 2003). Similar to our work, Bansal et al. (2015) extract an initial set of candidate segmentations using a sliding window, then rerank them using a linear regression model trained on lexical, bigram and other corpus-based features. The current state-of¨ ur, 2017, 2016) the-art approach (C ¸ elebi and Ozg¨ uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is"
P19-1242,maynard-greenwood-2014-cares,0,0.217171,"lying hashtag segmentation to the popular task of sentiment analysis. 5.1 Existing Methods We compare our pairwise neural ranker with the following baseline and state-of-the-art approaches: (a) The original hashtag as a single token; (b) A rule-based segmenter, which employs a set of word-shape rules with an English dictionary (Billal et al., 2016); (c) A Viterbi model which uses word frequencies from a book corpus7 (Berardi et al., 2011); 7 (d) The specially developed GATE Hashtag Tokenizer from the open source toolkit,8 which combines dictionaries and gazetteers in a Viterbi-like algorithm (Maynard and Greenwood, 2014); (e) A maximum entropy classifier (MaxEnt) trained on the STANlarge training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current ¨ ur, 2017); state-of-the-art (C¸elebi and Ozg¨ (f) Our reimplementation of the Word Breaker algorithm which uses beam search and a Twitter ngram language model (Wang et al., 2011); (g) A pairwise linear ranker which we implemented for comparison purposes with the same features as our neural model, but using perceptron as the underlying classifier (Hopkins and May, 2011) and minimizing the hinge Project Gutenber"
P19-1242,S13-2053,0,0.214973,"Hashtags play a central role in online communication by providing a tool to categorize the millions of posts generated daily on Twitter, Instagram, etc. They are useful in search, tracking content about a certain topic (Berardi et al., 2011; Ozdikis et al., 2012), or discovering emerging trends (Sampson et al., 2016). Hashtags often carry very important information, such as emotion (Abdul-Mageed and Ungar, 1 Multi-token #toyotaprius #ipv6summit #epicfall #iloveu4eva Our toolkit along with the code and data are publicly available at https://github.com/mounicam/ hashtag_master 2017), sentiment (Mohammad et al., 2013), sarcasm (Bamman and Smith, 2015), and named entities (Finin et al., 2010; Ritter et al., 2011). However, inferring the semantics of hashtags is nontrivial since many hashtags contain multiple tokens joined together, which frequently leads to multiple potential interpretations (e.g., lion head vs. lionhead). Table 1 shows several examples of single- and multi-token hashtags. While most hashtags represent a mix of standard tokens, named entities and event names are prevalent and pose challenges to both human and automatic comprehension, as these are more likely to be rare tokens. Hashtags also"
P19-1242,W02-1011,0,0.0223426,"Missing"
P19-1242,D11-1141,0,0.244836,"Missing"
P19-1242,S17-2088,0,0.0801271,"Missing"
P19-1242,C14-1018,0,0.415145,"hashtags from the year 2019. Extrinsic Evaluation: Twitter Sentiment Analysis Experimental Setup We compare the performance of the BiLSTM+Lex (Teng et al., 2016) sentiment analysis model under three configurations: (a) tweets with hashtags removed, (b) tweets with hashtags as single tokens excluding the # symbol, and (c) tweets with hashtags as segmented by our system, HashtagMaster. BiLSTM+Lex is a state-of-the-art open source system for predicting tweet-level sentiment (Tay et al., 2018). It learns a context-sensitive sentiment intensity score by leveraging a Twitterbased sentiment lexicon (Tang et al., 2014). We use the same settings as described by Teng et al. (2016) to train the model. We use the dataset from the Sentiment Analysis in Twitter shared task (subtask A) at SemEval 2017 (Rosenthal et al., 2017). 9 Given a tweet, the goal is to predict whether it expresses POSITIVE, NEGATIVE or NEUTRAL sentiment. The training and development sets consist of 49,669 tweets and we use 40,000 for training and the rest for development. There are a total of 4,840 tweets containing 12,128 hashtags in the SemEval 2017 test set, and our hashtag segmenter ended up splitting 6,975 of those hashtags present in 3"
P19-1242,D18-1381,0,0.0210555,"Twitter LM 92.1 93.9 94.7 Pairwise neural ranker (MSE+multitask) 94.6 95.6 96.7 Table 8: Evaluation results on 500 random hashtags from the year 2019. Extrinsic Evaluation: Twitter Sentiment Analysis Experimental Setup We compare the performance of the BiLSTM+Lex (Teng et al., 2016) sentiment analysis model under three configurations: (a) tweets with hashtags removed, (b) tweets with hashtags as single tokens excluding the # symbol, and (c) tweets with hashtags as segmented by our system, HashtagMaster. BiLSTM+Lex is a state-of-the-art open source system for predicting tweet-level sentiment (Tay et al., 2018). It learns a context-sensitive sentiment intensity score by leveraging a Twitterbased sentiment lexicon (Tang et al., 2014). We use the same settings as described by Teng et al. (2016) to train the model. We use the dataset from the Sentiment Analysis in Twitter shared task (subtask A) at SemEval 2017 (Rosenthal et al., 2017). 9 Given a tweet, the goal is to predict whether it expresses POSITIVE, NEGATIVE or NEUTRAL sentiment. The training and development sets consist of 49,669 tweets and we use 40,000 for training and the rest for development. There are a total of 4,840 tweets containing 12,"
P19-1242,D16-1169,0,0.0606339,"Missing"
P19-1242,N19-1164,0,0.0263746,"entation can improve the performance of many applications besides sentiment analysis, such as text classification (Billal et al., 2016), named entity linking (Bansal et al., 2015) and modeling user interests for recommendations (Chen et al., 2016). It can also help in collecting data of higher volume and quality by providing a more nuanced interpretation of its content, as shown for emotion analysis (Qadir and Riloff, 2014), sarcasm and irony detection (Maynard and Greenwood, 2014; Huang et al., 2018). Better semantic analysis of hashtags can also potentially be applied to hashtag annotation (Wang et al., 2019), to improve distant supervision labels in training classifiers for tasks such as sarcasm (Bamman and Smith, 2015), sentiment (Mohammad et al., 2013), emotions (Abdul-Mageed and Ungar, 2017); and, more generally, as labels for pre-training representations of words (Weston et al., 2014), sentences (Dhingra et al., 2016), and images (Mahajan et al., 2018). 8 Conclusion We proposed a new pairwise neural ranking model for hashtag segmention and showed significant performance improvements over the state-of-theart. We also constructed a larger and more curated dataset for analyzing and benchmarking"
P19-1242,D14-1194,0,0.0250183,"Missing"
P19-1242,W03-1728,0,0.0277325,"gazeteer and rule based (Maynard and Greenwood, 2014; Declerck and Lendvai, 2015; Billal et al., 2016), (b) word boundary detection (C ¸ elebi and ¨ ur, 2017, 2016), and (c) ranking with lanOzg¨ guage model and other features (Wang et al., 2011; Bansal et al., 2015; Berardi et al., 2011; Reuter et al., 2016; Simeon et al., 2016). Hashtag segmentation approaches draw upon work on compound splitting for languages such as German or Finnish (Koehn and Knight, 2003) and word segmentation (Peng and Schuurmans, 2001) for languages with no spaces between words such as Chinese (Sproat and Shih, 1990; Xue and Shen, 2003). Similar to our work, Bansal et al. (2015) extract an initial set of candidate segmentations using a sliding window, then rerank them using a linear regression model trained on lexical, bigram and other corpus-based features. The current state-of¨ ur, 2017, 2016) the-art approach (C ¸ elebi and Ozg¨ uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is the beginning of a new word. Generating Candidate Segmentations. Microsoft Word Breaker (Wang et al., 2011) is, among the existing methods, a strong bas"
Q14-1034,S12-1051,0,0.0148988,"f Green Ryu The Clippers Candice Robert Woods Amber Reggie Miller filtered random 0.0 0.2 0.4 0.6 0.8 Percentage of Positive Judgements Figure 5: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. sio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on 971 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different topics. method is inspired by a typical problem in extractive summarization, that the salient sentences are likely redundant (paraphrases) and need to be removed in the output summaries. We employ the scoring method used in SumBasic (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007), a sim"
Q14-1034,J08-4004,0,0.0498405,"Missing"
Q14-1034,R13-1026,1,0.754437,"otation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the pair, specifying whether the two words have same or different POS tags and what the specific tags are. We use the Twitter Part-Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3” in tweets about basketball game, “RIP” in tweets about a celebrity’s death. We use G2 log-likelihoodratio statistic, which has been frequently used in NLP, as a measure of word associations (Dunning, 1993; Moore, 2004). The significant scores are computed for"
Q14-1034,P12-1056,0,0.0569989,"Missing"
Q14-1034,C04-1051,0,0.816209,"emantic similarity systems. We make this dataset available to the research community.2 2 Joint Word-Sentence Paraphrase Model We present a new latent variable model that jointly captures paraphrase relations between sentence pairs and word pairs. It is very different from previous approaches in that its primary design goal and motivation is targeted towards short, lexically diverse text on the social web. 2.1 At-least-one-anchor Assumption Much previous work on paraphrase identification has been developed and evaluated on a specific benchmark dataset, the Microsoft Research Paraphrase Corpus (Dolan et al., 2004), which is de2 The dataset and code are made available at: SemEval-2015 shared task http://alt.qcri.org/semeval2015/ task1/ and https://github.com/cocoxu/ twitterparaphrase/ Corpus News (Dolan and Brockett, 2005) Twitter (This Work) Examples ◦ Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier. ◦ With the scandal hanging over Stewart’s company, revenue in the first quarter of the year dropped 15 percent from the same period a year earlier. ◦ The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq."
Q14-1034,W10-0735,0,0.0293585,"Missing"
Q14-1034,I05-5002,1,0.535018,"Missing"
Q14-1034,P11-1020,1,0.373917,"2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking Mechanical Turk workers to caption the action in short video segments. Similarly, Burrows et al. (2012) asked crowdsourcing workers to rewrite selected excerpts from books. Ling et al. (2014) crowdsourced bilingual parallel text using Twitter as the source of data. In contrast, we design a simple crowdsourcing task requiring only binary judgements on sentences collected from Twitter. There are several advantages as compared to existing work: a) the corpus also covers a very diverse range of topics and linguistic expressions, especially colloq"
Q14-1034,W02-1001,0,0.012317,"-or function; that is, if there exists at least one j such that zj = 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ"
Q14-1034,P09-1053,0,0.877987,"ng to Twitter, trends are determined by an algorithm which 6 More information about Twitter’s APIs: https://dev. twitter.com/docs/api/1.1/overview 442 =4 turk =5 turk turk =3 =2 =1 turk turk turk =0 The resulting system M ULTI P-PE provides consistently better precision and recall over the LEXLATENT model, as shown on the right in Figure 3. The M ULTI P-PE system outperforms LEXLATENT significantly according to a paired ttest with ρ less than 0.05. Our proposed M UL TI P takes advantage of Twitter’s specific properties and provides complementary information to previous approaches. Previously, Das and Smith (2009) has also used a product of experts to combine a lexical and a syntax-based model together. expert=0 Figure 4: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are"
Q14-1034,H93-1035,0,0.218833,"Missing"
Q14-1034,W04-3208,0,0.0872468,"Missing"
Q14-1034,C04-1151,0,0.0139898,"Missing"
Q14-1034,P12-1091,0,0.20052,"al Linguistics. (a) (b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined system shows significa"
Q14-1034,P13-1024,0,0.0363338,"on of a strong baseline used by Das and Smith (2009). competitive performance on the MSR corpus. The second baseline is a state-of-the-art unsupervised method, Weighted Textual Matrix Factorization (WTMF),4 which is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The original model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). We enhance the model with 1.6 million sentences from Twitter as suggested by Guo et al. (2013). Ji and Eisenstein (2013) presented a state-ofthe-art ensemble system, which we call LEXDISCRIM.5 It directly combines both discriminativelytuned latent features and surface lexical features into a SVM classifier. Specifically, the latent representation of a pair of sentences v~1 and v~2 is converted into a feature vector, [v~1 + v~2 , |v~1 − v~2 |], by concatenating the element-wise sum v~1 + v~2 and absolute different |v~1 − v~2 |. We also introduce a new baseline, LEXLATENT, which is a simplified version of LEXDISCRIM and easy to reproduce. It uses the same method to combine latent feature"
Q14-1034,D12-1039,0,0.0269215,"r to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the pair, specifying whether the two words have same or different POS tags and what the specific tags are. We use the Twitter Part-Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3"
Q14-1034,P11-1055,0,0.709502,"= 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ ← 0 2: for i ← 1 to n do 3: extract all possible word pairs wi exp(θ · f (z"
Q14-1034,N06-2015,0,0.015456,"rent paraphrase identification approaches on Twitter data. *An enhanced version that uses additional 1.6 million sentences from Twitter. ** Reimplementation of a strong baseline used by Das and Smith (2009). competitive performance on the MSR corpus. The second baseline is a state-of-the-art unsupervised method, Weighted Textual Matrix Factorization (WTMF),4 which is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The original model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). We enhance the model with 1.6 million sentences from Twitter as suggested by Guo et al. (2013). Ji and Eisenstein (2013) presented a state-ofthe-art ensemble system, which we call LEXDISCRIM.5 It directly combines both discriminativelytuned latent features and surface lexical features into a SVM classifier. Specifically, the latent representation of a pair of sentences v~1 and v~2 is converted into a feature vector, [v~1 + v~2 , |v~1 − v~2 |], by concatenating the element-wise sum v~1 + v~2 and absolute different |v~1 − v~2 |. We also"
Q14-1034,D13-1090,1,0.286981,"(b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined system shows significant improvement over the s"
Q14-1034,P06-1096,0,0.0226525,"Missing"
Q14-1034,W14-3356,0,0.0220602,"at at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking Mechanical Turk workers to caption the action in short video segments. Similarly, Burrows et al. (2012) asked crowdsourcing workers to rewrite selected excerpts from books. Ling et al. (2014) crowdsourced bilingual parallel text using Twitter as the source of data. In contrast, we design a simple crowdsourcing task requiring only binary judgements on sentences collected from Twitter. There are several advantages as compared to existing work: a) the corpus also covers a very diverse range of topics and linguistic expressions, especially colloquial language, which is different from and thus complements previous paraphrase corpora; b) the paraphrase corpus collected contains a representative proportion of both negative and positive instances, while lack of good negative examples was"
Q14-1034,D08-1084,0,0.0999499,"set zτ∗ = 1 and zj∗ = arg maxx∈0,1 φ(x, wj ; θ) for all j 6= τ ; for a negative example, we set z∗i = 0. The time complexity of both inferences for one sentence pair is O(|W (s)|2 ), where |W (s)|2 is the number of word pairs. In practice, we use online learning instead of optimizing the full objective. The detailed learning algorithm is presented in Figure 2. Following Hoffmann et al. (2011), we use 50 iterations in the experiments. 2.4 Feature Design 3 At the word-level, our discriminative model allows use of arbitrary features that are similar to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tag"
Q14-1034,J10-3003,0,0.0844497,"l−m e arms that have the highest estimated reward until reaching the maximum l = 10 annotations for any topic to insure data diversity. We tune the parameters m to be 1 and  to be between 0.35 ∼ 0.55 through simulation experiments, by artificially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by 444 Related Work Automatic Paraphrase Identification has been widely studied (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of various techniques.8 Many recent high-performance approaches use system combination (Das and Smith, 2009; Madnani et al., 2012; Ji and Eisenstein, 2013). For example, Madnani et al. (2012) combines multiple sophisticated machine translation metrics using a metaclassifier. An earlier attempt on Twitter data is that of Xu et al. (2013b). They limited the search space to only the tweets that explicitly mention a same date and a same named entity, however there remain a considerable amount of mislabels in their data.9 Zanzotto et al. (2011) also experim"
Q14-1034,N12-1019,0,0.30475,"to be between 0.35 ∼ 0.55 through simulation experiments, by artificially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by 444 Related Work Automatic Paraphrase Identification has been widely studied (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010). The ACL Wiki gives an excellent summary of various techniques.8 Many recent high-performance approaches use system combination (Das and Smith, 2009; Madnani et al., 2012; Ji and Eisenstein, 2013). For example, Madnani et al. (2012) combines multiple sophisticated machine translation metrics using a metaclassifier. An earlier attempt on Twitter data is that of Xu et al. (2013b). They limited the search space to only the tweets that explicitly mention a same date and a same named entity, however there remain a considerable amount of mislabels in their data.9 Zanzotto et al. (2011) also experimented with SVM tree kernel methods on Twitter data. Departing from the previous work, we propose a latent variable model to jointly infer the correspondence between words"
Q14-1034,W04-3243,0,0.0161665,"Of-Speech tagger developed by Derczynski et al. (2013). We add new fine-grained tags for variations of the eight words: “a”, “be”, “do”, “have”, “get”, “go”, “follow” and “please”. For example, we use a tag HA for words “have”, “has” and “had”. Topical Features that relate to the strength of a word’s association to the topic. This feature identifies the popular words in each topic, e.g. “3” in tweets about basketball game, “RIP” in tweets about a celebrity’s death. We use G2 log-likelihoodratio statistic, which has been frequently used in NLP, as a measure of word associations (Dunning, 1993; Moore, 2004). The significant scores are computed for each trend on an average of about 1500 sentences and converted to binary features for every word pair, indicating whether the two words are both significant or not. Our topical features are novel and were not used in previous work. Following Riedel et al. (2010) and Hoffmann et al. (2011), we also incorporate conjunction features into our system for better accuracy, namely Word+POS, Word+Topical and Word+POS+Topical features. 3 https://github.com/knowitall/morpha 439 Experiments Data It is nontrivial to gather a gold-standard dataset of naturally occur"
Q14-1034,N12-1034,0,0.334376,"Missing"
Q14-1034,Q13-1030,1,0.648609,"ata is released by Xu et al. (2013b) at: https:// github.com/cocoxu/twitterparaphrase/ 2011; Yao et al., 2013a,b), but different in that the paraphrase task requires additional sentence alignment modeling with no word alignment data. Our approach is also inspired by Fung and Cheung’s (2004a; 2004b) work on bootstrapping bilingual parallel sentence and word translations from comparable corpora. Multiple Instance Learning (Dietterich et al., 1997) has been used by different research groups in the field of information extraction (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraphrase corpus by asking"
Q14-1034,I05-5011,0,0.0125198,"and Yangfeng Ji4 1 University of Pennsylvania, Philadelphia, PA, USA {xwe, ccb}@cis.upenn.edu 2 The Ohio State University, Columbus, OH, USA ritter.1492@osu.edu 3 Microsoft Research, Redmond, WA, USA billdol@microsoft.com 4 Georgia Institute of Technology, Atlanta, GA, USA jiyfeng@gatech.edu Abstract (e.g. oscar nom’d doc ↔ Oscar-nominated documentary). In this paper, we investigate the task of determining whether two tweets are paraphrases. Previous work has exploited a pair of shared named entities to locate semantically equivalent patterns from related news articles (Shinyama et al., 2002; Sekine, 2005; Zhang and Weld, 2013). But short sentences in Twitter do not often mention two named entities (Ritter et al., 2012) and require nontrivial generalization from named entities to other words. For example, consider the following two sentences about basketball player Brook Lopez from Twitter: We present M ULTI P (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent vari"
Q14-1034,D12-1042,0,0.031759,"te_of_ the_art) 9 The data is released by Xu et al. (2013b) at: https:// github.com/cocoxu/twitterparaphrase/ 2011; Yao et al., 2013a,b), but different in that the paraphrase task requires additional sentence alignment modeling with no word alignment data. Our approach is also inspired by Fung and Cheung’s (2004a; 2004b) work on bootstrapping bilingual parallel sentence and word translations from comparable corpora. Multiple Instance Learning (Dietterich et al., 1997) has been used by different research groups in the field of information extraction (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Ritter et al., 2013; Xu et al., 2013a). The idea is to leverage structured data as weak supervision for tasks such as relation extraction. This is done, for example, by making the assumption that at least one sentence in the corpus which mentions a pair of entities (e1 , e2 ) participating in a relation (r) expresses the proposition: r(e1 , e2 ). Crowdsourcing Paraphrase Acquisition: Buzek et al. (2010) and Denkowski et al. (2010) focused specifically on collecting paraphrases of text to be translated to improve machine translation quality. Chen and Dolan (2011) gathered a large-scale paraph"
Q14-1034,P11-2044,0,0.0133162,"maxx∈0,1 φ(x, wj ; θ) for all j 6= τ ; for a negative example, we set z∗i = 0. The time complexity of both inferences for one sentence pair is O(|W (s)|2 ), where |W (s)|2 is the number of word pairs. In practice, we use online learning instead of optimizing the full objective. The detailed learning algorithm is presented in Figure 2. Following Hoffmann et al. (2011), we use 50 iterations in the experiments. 2.4 Feature Design 3 At the word-level, our discriminative model allows use of arbitrary features that are similar to those in monolingual word alignment models (MacCartney et al., 2008; Thadani and McKeown, 2011; Yao et al., 2013a,b). But unlike discriminative monolingual word alignment, we only use sentence-level training labels instead of word-level alignment annotation. For every word pair, we extract the following features: 3.1 String Features that indicate whether the two words, their stemmed forms and their normalized forms are the same, similar or dissimilar. We used the Morpha stemmer (Minnen et al., 2001),3 Jaro-Winkler string similarity (Winkler, 1999) and the Twitter normalization lexicon by Han et al. (2012). POS Features that are based on the part-of-speech tags of the two words in the p"
Q14-1034,U06-1019,0,0.813586,"on for Computational Linguistics. (a) (b) Figure 1: (a) a plate representation of the M ULTI P model (b) an example instantiation of M ULTI P for the pair of sentences “Manti bout to be the next Junior Seau” and “Teo is the little new Junior Seau”, in which a new American football player Manti Te’o was being compared to a famous former player Junior Seau. Only 4 out of the total 6 × 5 word pairs, z1 - z30 , are shown here. icon, to discriminatively determine word pairs as paraphrastic anchors or not. Our graphical model is a major departure from popular surface- or latent- similarity methods (Wan et al., 2006; Guo and Diab, 2012; Ji and Eisenstein, 2013, and others). Our approach to extract paraphrases from Twitter is general and can be combined with various topic detecting solutions. As a demonstration, we use Twitter’s own trending topic service1 to collect data and conduct experiments. While having a principled and extensible design, our model alone achieves performance on par with a state-of-the-art ensemble approach that involves both latent semantic modeling and supervised classification. The proposed model also captures radically different paraphrases from previous approaches; a combined sy"
Q14-1034,D13-1008,0,0.0679257,"a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one word pair (we call it"
Q14-1034,P13-2117,1,0.909424,"us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one"
Q14-1034,W13-2515,1,0.806798,"us to crowdsource a paraphrase corpus from Twitter. We make this new dataset available to the research community. 1 ◦ That boy Brook Lopez with a deep 3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one"
Q14-1034,P13-2123,1,0.512784,"Missing"
Q14-1034,D13-1056,1,0.747986,"Missing"
Q14-1034,D11-1061,0,0.281688,"3 ◦ brook lopez hit a 3 and i missed it Introduction Paraphrases are alternative linguistic expressions of the same or similar meaning (Bhagat and Hovy, 2013). Twitter engages millions of users, who naturally talk about the same topics simultaneously and frequently convey similar meaning using diverse linguistic expressions. The unique characteristics of this user-generated text presents new challenges and opportunities for paraphrase research (Xu et al., 2013b; Wang et al., 2013). For many applications, like automatic summarization, first story detection (Petrovi´c et al., 2012) and search (Zanzotto et al., 2011), it is crucial to resolve redundancy in tweets Although these sentences do not have many words in common, the identical word “3” is a strong indicator that the two sentences are paraphrases. We therefore propose a novel joint word-sentence approach, incorporating a multi-instance learning assumption (Dietterich et al., 1997) that two sentences under the same topic (we highlight topics in bold) are paraphrases if they contain at least one word pair (we call it an anchor and highlight with underscores; the words in the anchor pair need not be identical) that is indicative of sentential paraphra"
Q14-1034,D07-1071,0,0.0109552,"sts at least one j such that zj = 1, then the sentence pair is a paraphrase. Our conditional paraphrase identification model is defined as follows: m Y P (zi , yi |wi ; θ) = φ(zj , wj ; θ) × σ(zi , yi ) j=1 = m Y j=1 (1) 2.3 Learning To learn the parameters of the word-level paraphrase anchor classifier, θ, we maximize likelihood over the sentence-level annotations in our paraphrase corpus: θ∗ = arg max P (y|w; θ) θ YX P (zi , yi |wi ; θ) = arg max θ i (3) zi An iterative gradient-ascent approach is used to estimate θ using perceptron-style additive updates (Collins, 2002; Liang et al., 2006; Zettlemoyer and Collins, 2007; Hoffmann et al., 2011). We define an update based on the gradient of the conditional log likelihood using Viterbi approximation, as follows: X ∂ log P (y|w; θ) = EP (z|w,y;θ) ( f (zi , wi )) ∂θ i X − EP (z,y|w;θ) ( f (zi , wi )) (4) ≈ X i f (z∗i , wi ) i − X f (z0i , wi ) i where we define P the feature sum for each sentence f (zi , wi ) = j f (zj , wj ) over all word pairs. These two above expectations are approximated by solving two simple inference problems as maximizations: z∗ = arg max P (z|w, y; θ) z 0 1: initialize parameter vector θ ← 0 2: for i ← 1 to n do 3: extract all possible wo"
Q14-1034,D13-1183,0,0.0609342,"i4 1 University of Pennsylvania, Philadelphia, PA, USA {xwe, ccb}@cis.upenn.edu 2 The Ohio State University, Columbus, OH, USA ritter.1492@osu.edu 3 Microsoft Research, Redmond, WA, USA billdol@microsoft.com 4 Georgia Institute of Technology, Atlanta, GA, USA jiyfeng@gatech.edu Abstract (e.g. oscar nom’d doc ↔ Oscar-nominated documentary). In this paper, we investigate the task of determining whether two tweets are paraphrases. Previous work has exploited a pair of shared named entities to locate semantically equivalent patterns from related news articles (Shinyama et al., 2002; Sekine, 2005; Zhang and Weld, 2013). But short sentences in Twitter do not often mention two named entities (Ritter et al., 2012) and require nontrivial generalization from named entities to other words. For example, consider the following two sentences about basketball player Brook Lopez from Twitter: We present M ULTI P (Multi-instance Learning Paraphrase Model), a new model suited to identify paraphrases within the short messages on Twitter. We jointly model paraphrase relations between word and sentence pairs and assume only sentence-level annotations during learning. Using this principled latent variable model alone, we ac"
Q14-1034,N03-1031,0,\N,Missing
Q14-1034,W10-0711,0,\N,Missing
Q14-1034,J13-3001,0,\N,Missing
Q15-1021,C14-1188,0,0.0964198,"rasing. Although the inadequacy of text simplification evaluations has been discussed before (Siddharthan, 2014), we focus on these two common deficiencies and suggest two future directions. 4.1 Targeting specific audiences Simplification has many subtleties, since what constitutes simplification for one type of user may not be appropriate for another. Many researchers have studied simplification in the context of different audiences. However, most recent automatic simplification systems are developed and evaluated with little consideration of target reader population. There is one attempt by Angrosh et al. (2014) who evaluate their system by asking non-native speakers comprehension questions. They conducted an English vocabulary size test to categorize the users into different levels of language skills. The Newsela corpus allows us to target children at different grade levels. From the application point of view, making knowledge accessible to all children is an important yet challenging part of education (Scarton et al., 2010; Moraes et al., 2014). From the technical point of view, reading grade level is a clearly defined objective for both simplification systems and human annotators. Once there is a"
Q15-1021,I11-1053,0,0.017708,"s in the Parallel Wikipedia Simplification (PWKP) corpus. ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset. It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research. We are not the only researchers to notice problems with Simple Wikipedia. There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments. Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refoc"
Q15-1021,J03-4003,0,0.0443548,"sional simplifications. Wikipedia has good coverage on certain words, such as “approximately”, because of its large volume. 3.4 Log-odds-ratio analysis of syntax patterns We can also reveal the syntax patterns that are most strongly associated with simple text versus complex text using the log-odds-ratio technique. Table 9 shows syntax patterns that represent “parent node (head word) → children node(s)&quot; structures from a constituency parse tree. To extract theses patterns we parsed our corpus with the Stanford Parser (Klein and Manning, 2002) and applied its built-in head word identifier from Collins (2003). Both the Newsela and Wikipedia corpora exhibit syntactic differences that are intuitive and interesting. However, as with word frequency (Table 8), 290 complex syntactic patterns are retained more often in Wikipedia’s simplifications than in Newsela’s. In order to show interesting syntax patterns in the Wikipedia parallel data for Table 9, we first had to discard 3613 sentences in PWKP that contain both &quot;is a commune&quot; and &quot;France&quot;. As the word-level analysis in Tables 6 and 7 hints, there is an exceeding number of sentences about communes in France in the PWKP corpus, such as the sentence pa"
Q15-1021,P11-2117,0,0.0386432,"P) corpus. ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset. It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research. We are not the only researchers to notice problems with Simple Wikipedia. There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments. Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in the future (Eisenstein, 2013"
Q15-1021,J08-1001,0,0.0123409,"te sentence from a stub geographic article and its deterministic simplification. The influence of this template sentence is more overwhelming in the syntax-level analysis than in the word-level analysis —- about 1/3 of the top 30 syntax patterns would be related to these sentence pairs if they were not discarded. tions. However, previous research that uses SimpleNormal Wikipedia largely focuses on sentence-level transformation, without taking large discourse structure into account. 3.5 Document-level compression There are few publicly accessible document-level parallel simplification corpora (Barzilay and Lapata, 2008). The Newsela corpus will enable more research on document-level simplification, such as anaphora choice (Siddharthan and Copestake, 2002), content selection (Woodsend and Lapata, 2011b), and discourse relation preservation (Siddharthan, 2003). Simple Wikipedia is rarely used to study document-level simplification. Woodsend and Lapata (2011b) developed a model that simplifies Wikipedia articles while selecting their most important content. However, they could only use Simple Wikipedia in very limited ways. They noted that Simple Wikipedia is “less mature” with many articles that are just “stub"
Q15-1021,N13-1037,0,0.0273424,"nd Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in the future (Eisenstein, 2013). We hope to 284 inspire the creation of higher quality simplification datasets, and to encourage researchers to think critically about existing resources and evaluation methods. We believe this will lead to breakthroughs in text simplification research. 2 Simple Wikipedia is not that simple The Parallel Wikipedia Simplification (PWKP) corpus (Zhu et al., 2010) contains approximately 108,000 automatically aligned sentence pairs from cross-linked articles between Simple and Normal English Wikipedia. It has become a benchmark dataset for simplification largely because of its size and availabilit"
Q15-1021,C08-1013,1,0.833475,"Missing"
Q15-1021,W07-1007,0,0.085537,"Missing"
Q15-1021,E99-1042,0,0.429301,"Missing"
Q15-1021,C96-2183,0,0.887897,"Missing"
Q15-1021,P11-1020,0,0.0231991,"r output tends to result in lower adequacy judgements (Napoles et al., 2011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introduced a new, high-quality corpus of professionally simplified news articles, Newsela, as an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequac"
Q15-1021,C12-1034,0,0.186369,"Missing"
Q15-1021,D11-1108,1,0.391062,"Missing"
Q15-1021,gerber-hovy-1998-improving,0,0.175719,"Missing"
Q15-1021,N15-1022,0,0.103388,"ntence is still complex. The main causes of non-simplifications and partial-simplifications in the parallel Wikipedia corpus include: 1) The Simple Wikipedia was created by volunteer contributors with no specific objective; 2) Very rarely are the simple articles complete re-writes of the regular articles in Wikipedia (Coster and Kauchak, 2011), which makes automatic sentence alignment errors worse; 3) As an encyclopedia, Wikipedia contains many difficult sentences with complex terminology. The difficulty of sentence alignment between Normal-Simple Wikipedia is highlighted by a recent study by Hwang et al. (2015) that achieves state-of-the-art performance of 0.712 maximum F1 score (over the precisionrecall curve) by combining Wiktionary-based and dependency-parse-based sentence similarities. And in fact, even the simple side of the PWKP corpus contains an extensive English vocabulary of 78,009 unique words. 6,669 of these words do not exist in the normal side (Table 2). Below is a sentence from an article entitled “Photolithography&quot; in Simple Wikipedia: Microphototolithography is the use of photolithography to transfer geometric shapes on a photomask to the surface of a semiconductor wafer for making"
Q15-1021,W03-1602,0,0.0442639,"Missing"
Q15-1021,P13-1151,0,0.104475,"les while selecting their most important content. However, they could only use Simple Wikipedia in very limited ways. They noted that Simple Wikipedia is “less mature” with many articles that are just “stubs, comprising a single paragraph of just one or two sentences”. We quantify their observation in Figure 2, plotting the documentlevel compression ratio of Simple vs. Normal Wikipedia articles. The compression ratio is the ratio of the number of characters between each simple-complex article pair. In the plot, we use all 60 thousand article pairs from the Simple-Normal Wikipedia collected by Kauchak (2013) in May 2011. The overall compression ratio is skewed towards almost 0. For comparison, we also plot the ratio between the simplest version (Simp-4) and the original version (Original) of the news articles in the Newsela corpus. The Newsela corpus has a much more reasonable compression ratio and is therefore likely to be more suitable for studying documentlevel simplification. 3.6 Analysis of discourse connectives Although discourse is known to affect readability, the relation between discourse and text simplification is still under-studied with the use of statistical methods (Williams et al.,"
Q15-1021,Q13-1028,0,0.0106967,"human annotators. Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems. Newsela provides human simplification at different grade levels and reading comprehension quizzes alongside each article. In addition, readability is widely studied and can be automatically estimated (Kincaid et al., 1975; Pitler and Nenkova, 2008; Petersen and Ostendorf, 2009). Although existing readability metrics assume text is well-formed, they can potentially be used in combination with text quality metrics (Post, 2011; Louis and Nenkova, 2013) to evaluate simplifications. They can also be used to aid humans in the creation of reference simplifications. 4.2 Evaluating sub-tasks separately It is widely accepted that sentence simplification involves three different elements: splitting, deletion and paraphrasing (Feng, 2008; Narayan and Gardent, 2014). Splitting breaks a long sentence into a few short sentences to achieve better readability. Deletion reduces the complexity by removing unimportant parts of a sentence. Paraphrasing rewrites text into a simpler version via reordering, substitution and occasionally expansion. Most state-of"
Q15-1021,C10-1089,0,0.0180268,"Missing"
Q15-1021,W11-1611,1,0.810656,"Missing"
Q15-1021,P14-1041,0,0.675874,"s to think critically about existing resources and evaluation methods. We believe this will lead to breakthroughs in text simplification research. 2 Simple Wikipedia is not that simple The Parallel Wikipedia Simplification (PWKP) corpus (Zhu et al., 2010) contains approximately 108,000 automatically aligned sentence pairs from cross-linked articles between Simple and Normal English Wikipedia. It has become a benchmark dataset for simplification largely because of its size and availability, and because follow-up papers (Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014) often compare with Zhu et al.’s system outputs to demonstrate further improvements. The large quantity of parallel text from Wikipedia made it possible to build simplification systems using statistical machine translation (SMT) technology. But after the initial success of these firstgeneration systems, we started to suffer from the inadequacy of the parallel Wikipedia simplification datasets. There is scattered evidence in the literature. Bach et al. (2011) mentioned they have attempted to use parallel Wikipedia data, but opted to construc"
Q15-1021,D08-1020,0,0.0128282,"ortant yet challenging part of education (Scarton et al., 2010; Moraes et al., 2014). From the technical point of view, reading grade level is a clearly defined objective for both simplification systems and human annotators. Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems. Newsela provides human simplification at different grade levels and reading comprehension quizzes alongside each article. In addition, readability is widely studied and can be automatically estimated (Kincaid et al., 1975; Pitler and Nenkova, 2008; Petersen and Ostendorf, 2009). Although existing readability metrics assume text is well-formed, they can potentially be used in combination with text quality metrics (Post, 2011; Louis and Nenkova, 2013) to evaluate simplifications. They can also be used to aid humans in the creation of reference simplifications. 4.2 Evaluating sub-tasks separately It is widely accepted that sentence simplification involves three different elements: splitting, deletion and paraphrasing (Feng, 2008; Narayan and Gardent, 2014). Splitting breaks a long sentence into a few short sentences to achieve better read"
Q15-1021,P11-2038,0,0.0229599,"systems and human annotators. Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems. Newsela provides human simplification at different grade levels and reading comprehension quizzes alongside each article. In addition, readability is widely studied and can be automatically estimated (Kincaid et al., 1975; Pitler and Nenkova, 2008; Petersen and Ostendorf, 2009). Although existing readability metrics assume text is well-formed, they can potentially be used in combination with text quality metrics (Post, 2011; Louis and Nenkova, 2013) to evaluate simplifications. They can also be used to aid humans in the creation of reference simplifications. 4.2 Evaluating sub-tasks separately It is widely accepted that sentence simplification involves three different elements: splitting, deletion and paraphrasing (Feng, 2008; Narayan and Gardent, 2014). Splitting breaks a long sentence into a few short sentences to achieve better readability. Deletion reduces the complexity by removing unimportant parts of a sentence. Paraphrasing rewrites text into a simpler version via reordering, substitution and occasionall"
Q15-1021,N03-1026,0,0.0144108,"ria (grammaticality, simplicity and adequacy) do not explain which components in a system are good or bad. More importantly, deletion may be unfairly penalized since shorter output tends to result in lower adequacy judgements (Napoles et al., 2011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introd"
Q15-1021,N10-2011,0,0.0498743,"Missing"
Q15-1021,W03-2314,0,0.317258,"be related to these sentence pairs if they were not discarded. tions. However, previous research that uses SimpleNormal Wikipedia largely focuses on sentence-level transformation, without taking large discourse structure into account. 3.5 Document-level compression There are few publicly accessible document-level parallel simplification corpora (Barzilay and Lapata, 2008). The Newsela corpus will enable more research on document-level simplification, such as anaphora choice (Siddharthan and Copestake, 2002), content selection (Woodsend and Lapata, 2011b), and discourse relation preservation (Siddharthan, 2003). Simple Wikipedia is rarely used to study document-level simplification. Woodsend and Lapata (2011b) developed a model that simplifies Wikipedia articles while selecting their most important content. However, they could only use Simple Wikipedia in very limited ways. They noted that Simple Wikipedia is “less mature” with many articles that are just “stubs, comprising a single paragraph of just one or two sentences”. We quantify their observation in Figure 2, plotting the documentlevel compression ratio of Simple vs. Normal Wikipedia articles. The compression ratio is the ratio of the number o"
Q15-1021,E14-1076,0,0.0745086,"Missing"
Q15-1021,N10-1144,0,0.272647,"ntitative-comparative approach to study the quality of simplification data resources. 1 Introduction The goal of text simplification is to rewrite complex text into simpler language that is easier to understand. Research into this topic has many potential practical applications. For instance, it can provide reading aids for people with disabilities (Carroll et al., 1999; Canning et al., 2000; Inui et al., 2003), low-literacy (Watanabe et al., 2009; De Belder and Moens, 2010), non-native backgrounds (Petersen and Ostendorf, 2007; Allen, 2009) or non-expert knowledge (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010). Text simplification may also help improve the performance of many natural language processing (NLP) tasks, such as parsing (Chandrasekar et al., 1996), summarization (Siddharthan et al., 2004; Klebanov et al., 2004; Vanderwende et al., 2007; Xu and Grishman, 2009), semantic role labeling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Gerber and Hovy, 1998; Chen et al., 2012), by transforming long, complex sentences into ones that are more easily processed. The Parallel Wikipedia Simplification (PWKP) corpus prepared by Zhu et al. (2010), has b"
Q15-1021,C04-1129,0,0.0857544,"Missing"
Q15-1021,P08-1040,0,0.445463,"Missing"
Q15-1021,E14-1021,1,0.818695,"011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introduced a new, high-quality corpus of professionally simplified news articles, Newsela, as an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequacies in comparison. We further discussed problems with current simplific"
Q15-1021,W03-2317,0,0.0777258,"y Kauchak (2013) in May 2011. The overall compression ratio is skewed towards almost 0. For comparison, we also plot the ratio between the simplest version (Simp-4) and the original version (Original) of the news articles in the Newsela corpus. The Newsela corpus has a much more reasonable compression ratio and is therefore likely to be more suitable for studying documentlevel simplification. 3.6 Analysis of discourse connectives Although discourse is known to affect readability, the relation between discourse and text simplification is still under-studied with the use of statistical methods (Williams et al., 2003; Siddharthan, 2006; Siddharthan and Katsos, 2010). Text simplification often involves splitting one sentence into multiple sentences, which is likely to require discourse-level changes such as introducing explicit rhetorical rela291 Figure 3: A radar chart that visualizes the odds ratio (radius axis) of discourse connectives in simple side vs. complex side. An odds ratio larger than 1 indicates the word is more likely to occur in the simplified text than in the complex text, and vice versa. Simple cue words (in the shaded region), except “hence”, are more likely to be added during Newsela’s s"
Q15-1021,D11-1038,0,0.222754,"ikipedia Simplification (PWKP) corpus. ple grade levels. This parallel corpus is higher quality and its size is comparable to the PWKP dataset. It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research. We are not the only researchers to notice problems with Simple Wikipedia. There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments. Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed. Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”. To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question. We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in t"
Q15-1021,P12-1107,0,0.336299,"Missing"
Q15-1021,W09-2809,1,0.844138,"Missing"
Q15-1021,C12-1177,1,0.0368874,"ents (Napoles et al., 2011). We therefore advocate for a more informative evaluation that separates out each sub-task. We believe this will lead to more easily quantifiable metrics and possibly the development of automatic metrics. For example, early work shows potential use of precision and recall to evaluate splitting (Siddharthan, 2006; Gasperin et al., 2009) and deletion (Riezler et al., 2003; Filippova and Strube, 2008). Several studies also have investigated various metrics for evaluating sentence paraphrasing (CallisonBurch et al., 2008; Chen and Dolan, 2011; Ganitkevitch et al., 2011; Xu et al., 2012, 2013; Weese et al., 2014). 5 Summary and recommendations In this paper, we presented the first systematic analysis of the quality of Simple Wikipedia as a simpli293 fication data resource. We conducted a qualitative manual examination and several statistical analyses (including vocabulary change matrices, compression ratio histograms, log-odds-ratio calculations, etc.). We introduced a new, high-quality corpus of professionally simplified news articles, Newsela, as an alternative resource, that allowed us to demonstrate Simple Wikipedia’s inadequacies in comparison. We further discussed prob"
Q15-1021,W13-2515,1,0.397575,"Missing"
Q15-1021,C10-1152,0,0.482887,"arthan and Katsos, 2010). Text simplification may also help improve the performance of many natural language processing (NLP) tasks, such as parsing (Chandrasekar et al., 1996), summarization (Siddharthan et al., 2004; Klebanov et al., 2004; Vanderwende et al., 2007; Xu and Grishman, 2009), semantic role labeling (Vickrey and Koller, 2008), information extraction (Miwa et al., 2010) and machine translation (Gerber and Hovy, 1998; Chen et al., 2012), by transforming long, complex sentences into ones that are more easily processed. The Parallel Wikipedia Simplification (PWKP) corpus prepared by Zhu et al. (2010), has become the benchmark dataset for training and evaluating automatic text simplification systems. An associated test set of 100 sentences from Wikipedia has been used for comparing the state-of-the-art approaches. The collection of simple-complex parallel sentences sparked a major advance for machine translationbased approaches to simplification. However, we will show that this dataset is deficient and should be considered obsolete. In this opinion paper, we argue that Wikipedia as a simplification data resource is suboptimal for several reasons: 1) It is prone to automatic sentence alignm"
Q15-1021,W08-1105,0,\N,Missing
Q15-1021,W14-4409,0,\N,Missing
Q16-1027,buck-etal-2014-n,0,0.112276,"Missing"
Q16-1027,D14-1179,0,0.13632,"Missing"
Q16-1027,W14-3309,0,0.0984371,"can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source se"
Q16-1027,P15-1001,0,0.423363,"two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (Luong et al., 2015; Jean et al., 2015). However, a single neural model of either of the above types has not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on the WMT’14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) achieves 37.0. We focus on improving the single model perfor371 Transactions of the Association for Computational Linguistics, vol. 4, pp. 371–383, 2016. Action Editor: Holger Schwenk. Submission batch: 1/2016; Revision batch: 4/2016; Published 7/2016. c 20"
Q16-1027,D13-1176,0,0.134234,".2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies"
Q16-1027,N03-1017,0,0.0228491,"0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network r"
Q16-1027,N06-1014,0,0.0113091,"e+PosUnk Ensemble Ensemble+PosUnk Durrani, 2014 Ensemble+PosUnk Data 36M 36M 36M 36M 36M 36M 36M Voc 80K 80K 80K 80K 80K Full 80K BLEU 36.3 37.7 39.2 38.9 40.4 37.0 37.5 Table 8: BLEU scores of different models. The first two blocks are our results of two single models and models with post processing. In the last block we list two baselines of the best conventional SMT system and NMT system. Second, we recover the unknown words in the generated sequences with the Positional Unknown (PosUnk) model introduced in (Luong et al., 2015). The full parallel corpus is used to obtain the word mappings (Liang et al., 2006). We find this method provides an additional 1.5 BLEU points, which is consistent with the conclusion in Luong et al. (2015). We obtain the new BLEU score of 39.2 with a single Deep-Att model. For the ensemble models of Deep-Att, the BLEU score rises to 40.4. In the last two lines, we list the conventional SMT model (Durrani et al., 2014) and the previous best neural models based system Enc-Dec (Luong et al., 2015) for comparison. We find our best score outperforms the previous best score by nearly 3 points. 380 5 4 7 8 12 17 22 Sentences by Length 28 35 79 Figure 3: BLEU scores vs. source seq"
Q16-1027,P15-1002,0,0.151765,"general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (Luong et al., 2015; Jean et al., 2015). However, a single neural model of either of the above types has not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on the WMT’14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) achieves 37.0. We focus on improving the single model perfor371 Transactions of the Association for Computational Linguistics, vol. 4, pp. 371–383, 2016. Action Editor: Holger Schwenk. Submission batch: 1/2016; Revision batch: 4/2016; Pu"
Q16-1027,P15-1109,1,0.824274,"is the dropout operation (Hinton et al., 2012) which randomly sets an element of h to zero with a certain probability. The use of Half(·) is to reduce the parameter size and does not affect the performance. We observed noticeable performance degradation when using only the first third of the elements of “f”. ftk = Wfk · [Half(ftk−1 ), Dr(hk−1 )], k &gt; 1 (6) t With the F-F connections, we build a fast channel to propagate the gradients in the deep topology. F-F connections can accelerate the model convergence and while improving the performance. A similar idea was also used in (He et al., 2016; Zhou and Xu, 2015). Encoder: The LSTM layers are stacked following Eq. 5. We call this type of encoder interleaved bidirectional encoder. In addition, there are two similar columns (a1 and a2 ) in the encoder part. Each column consists of ne stacked LSTM layers. There is no connection between the two columns. The first layers of the two columns process the word representations of the source sequence in different directions. At the last LSTM layers, there are two groups of vectors representing the source sequence. The group size is the same as the length of the input sequence. Interface: Prior encoder-decoder mo"
Q16-1029,W14-1214,0,0.104671,"Missing"
Q16-1029,C14-1188,0,0.155152,"s dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent,"
Q16-1029,P05-1074,1,0.147414,"ber of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns."
Q16-1029,P11-2087,0,0.0097729,"r knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification q"
Q16-1029,E99-1042,0,0.71526,"development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and"
Q16-1029,W11-2504,1,0.904086,"Missing"
Q16-1029,C96-2183,0,0.904046,"ions Poperation and recalls Roperation : SARI = d1 Fadd + d2 Fkeep + d3 Pdel (7) where d1 = d2 = d3 = 1/3 and Poperation = Roperation 1 k 1 = k Foperation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal"
Q16-1029,P12-1098,0,0.134153,"text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important"
Q16-1029,P11-1020,0,0.0443074,"y word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one cruc"
Q16-1029,C12-1034,0,0.303799,"text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important"
Q16-1029,J07-2003,0,0.0340451,"patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns. The key differences between the paraphrase rules from PPDB and the transformations learned by the naive application of SMT to the Normal-Simple Wikipedia parallel corpus, are that the PPDB pa"
Q16-1029,P06-1048,0,0.042514,"), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they ha"
Q16-1029,W11-1601,0,0.212736,"includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing proble"
Q16-1029,P11-2117,0,0.0757764,"includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing proble"
Q16-1029,N12-1067,0,0.0258186,"ain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statis"
Q16-1029,W14-1215,0,0.106191,", which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; C"
Q16-1029,N15-1060,0,0.0389548,"data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objectiv"
Q16-1029,D15-1042,0,0.0371305,"accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it"
Q16-1029,W08-1105,0,0.0147252,"et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifyin"
Q16-1029,P15-2073,0,0.0139864,"be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives."
Q16-1029,W12-3134,1,0.938317,"ctions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which governs whether the system prefers fewer longer phrases or a greater number of shorter phrases), and a language model probability. Together these features are what the model uses to distinguish between good and bad translations. For monolingual translation tasks, previous research suggests that features like paraphrase probability and distributional similarity are potentially helpful in picking out good paraphrases (Chan et al., 2011) and for text-to-text generation (Ganitkevitch et al., 2012b). While these two features quantify how good a paraphrase rule is in general, they do not indicate how good the rule is for a specific task, like simplification. For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add 406 9 new features for simplification purposes:5 length in characters, length in words, number of syllables, language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word with the maximum number of syllables on each side and the difference between th"
Q16-1029,S12-1034,1,0.867442,"Missing"
Q16-1029,N13-1092,1,0.395478,"Missing"
Q16-1029,D11-1125,0,0.0478686,"on that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previous work on text simplification has only used BLEU for tuning, which is insufficient as we show empirically in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references. Unlike machine translation metrics which do not compare against the (foreign) input sentence, it is necessary to compare simplification system outputs against the inputs to as"
Q16-1029,P14-2075,0,0.0134708,"ttempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previou"
Q16-1029,N15-1022,0,0.0688657,"n or sentence splitting could be applied as pre- or post-processing steps. 2 Background Xu et al. (2015) laid out a series of problems that are present in current text simplification research, and argued that we should deviate from the previous state-of-the-art benchmarking setup. First, the Simple English Wikipedia data has dominated simplification research since 2010 (Zhu et al., 2010; Siddharthan, 2014), and is used together with Standard English Wikipedia to create parallel text to train MT-based simplification systems. However, recent studies (Xu et al., 2015; Amancio and ˇ Specia, 2014; Hwang et al., 2015; Stajner et al., 2015) showed that the parallel Wikipedia simplification corpus contains a large proportion of inadequate (not much simpler) or inaccurate (not aligned or only partially aligned) simplifications. It is one of the leading reasons that existing simplification systems struggle to generate simplifying paraphrases and leave the input sentences unchanged (Wubben 1 Our code and data are made available at: https:// github.com/cocoxu/simplification/ 402 et al., 2012). Previously researchers attempted some quick fixes by adding phrasal deletion rules (Coster and Kauchak, 2011a) or reran"
Q16-1029,W03-1602,0,0.0887002,"ation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is ac"
Q16-1029,P02-1028,0,0.0887695,"operation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that"
Q16-1029,P13-1151,0,0.30462,"c task, like simplification. For each paraphrase rule, we use all the 33 features that were distributed with PPDB 1.0 and add 406 9 new features for simplification purposes:5 length in characters, length in words, number of syllables, language model scores, and fraction of common English words in each rule. These features are computed for both sides of a paraphrase pattern, the word with the maximum number of syllables on each side and the difference between the two sides, when it is applicable. We use language models built from the Gigaword corpus and the Simple Wikipedia corpus collected by Kauchak (2013). We also use a list of 3000 most common US English words compiled by Paul and Bernice Noll.6 3.4 Creating Multiple References Like with machine translation, where there are many equally good translations, in simplification there may be several ways of simplifying a sentence. Most previous work on text simplification only uses a single reference simplification, often from the Simple Wikipedia. This is undesirable since the Simple Wikipedia contains a large proportion of inadequate or inaccurate simplifications (Xu et al., 2015) . In this study, we collect multiple human reference simplificatio"
Q16-1029,W10-1754,0,0.0250509,"ression, in which compression of word and sentence lengths can be more straightforwardly implemented in features and the objective function in the SMT framework. We want to stress that sentence simplification is not a simple extension of sentence compression, but is a much more complicated task, primarily because high-quality data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a gi"
Q16-1029,C10-1089,0,0.0709952,"slation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion remo"
Q16-1029,P15-2097,1,0.8131,"because high-quality data is much harder to obtain and the solution space is more constrained by word choice and grammar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simpli"
Q16-1029,P14-1041,0,0.733939,"language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 2007; Narayan and Gardent, 2014; Angrosh et al., 2014) and deletion (Knight and Marcu 2002; Clarke and Lapata 2006; Filippova and Strube 2008; Filippova et al. 2015; Rush et al. 2015; and others) have been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b;"
Q16-1029,P03-1021,0,0.0352014,"exical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic evaluation metrics to be used as training objectives. Training algorithms, such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), then directly optimize the model parameters such that the end-to-end simplification quality is optimal. Unfortunately, previous work on text simplification has only used BLEU for tuning, which is insufficient as we show empirically in Section 4. We propose two new light-weight metrics instead: FKBLEU that explicitly measures readability and SARI that implicitly measures it by comparing against the input and references. Unlike machine translation metrics which do not compare against the (foreign) input sentence, it is necessary to compare simplification system o"
Q16-1029,P15-1146,1,0.25619,"ules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than"
Q16-1029,P16-2024,1,0.58114,"stral, old, age-old, archeological, former, antiquated, longstanding, archaic, centuries-old, and so on. However, there is nothing inherent in the rule extraction process to say which of the PPDB paraphrases are simplifications. In this paper, we model the task by incorporating rich features into each rule and let SMT advances in decoding and optimization determine how well a rule simplifies an input phrase. An alternative way of using PPDB for simplification would be to simply discard any of its rules which did not result in a simplified output, possibly using a simple supervised classifier (Pavlick and Callison-Burch, 2016). 3.3 Simplification-specific Features for Paraphrase Rules Designing good features is an essential aspect of modeling. For each input sentence i and its candidate output sentence j, a vector of feature functions ϕ ~ = {ϕ1 ...ϕN } are combined with a weight vector w ~ in a linear model to obtain a single score hw~ : hw~ (i, j) = w ~ ·ϕ ~ (i, j) (8) In SMT, typical feature functions are phrase translation probabilities, word-for-word lexical translation probabilities, a rule application penalty (which governs whether the system prefers fewer longer phrases or a greater number of shorter phrases"
Q16-1029,W14-1210,0,0.0624395,"sentences, and randomly split them into 2000 sentences for tuning, 350 for evaluation. Many crowdsourcing workers were able to provide simplifications of good quality and diversity (see Table 2 5 We release the data with details for each feature. http://www.manythings.org/vocabulary/ lists/l/noll-about.php 6 for an example and Table 4 for the manual quality evaluation). Having multiple references allows us to develop automatic metrics similar to BLEU to take advantage of the variation across many people’s simplifications. We leave more in-depth investigations on crowdsourcing simplification (Pellow and Eskenazi, 2014a,b) for future work. 3.5 Tuning Parameters Like in statistical machine translation, we set the weights of the linear model w ~ in the Equation (8) so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000 sentence development set. We use the pairwise ranking optimization (PRO) algorithm (Hopkins and May, 2011) implemented in the open-source Joshua toolkit (Ganitkevitch et al., 2012a; Post et al., 2013) for tuning. Specifically, we train the system to distinguish a good candidate output j from a bad candidate j 0 , measured by an objective function o"
Q16-1029,W13-2226,1,0.83823,"to take advantage of the variation across many people’s simplifications. We leave more in-depth investigations on crowdsourcing simplification (Pellow and Eskenazi, 2014a,b) for future work. 3.5 Tuning Parameters Like in statistical machine translation, we set the weights of the linear model w ~ in the Equation (8) so that the system’s output is optimized with respect to the automatic evaluation metric on the 2000 sentence development set. We use the pairwise ranking optimization (PRO) algorithm (Hopkins and May, 2011) implemented in the open-source Joshua toolkit (Ganitkevitch et al., 2012a; Post et al., 2013) for tuning. Specifically, we train the system to distinguish a good candidate output j from a bad candidate j 0 , measured by an objective function o (Section 3.1), for an input sentence i: o(i, j) >o(i, j 0 ) ⇐⇒ hw~ (i, j) > hw~ (i, j 0 ) ⇐⇒ hw~ (i, j) − hw~ (i, j 0 ) > 0 ⇐⇒ w ~ ·ϕ ~ (i, j) − w ~ ·ϕ ~ (i, j 0 ) > 0 (9) ⇐⇒ w ~ · (~ ϕ(i, j) − ϕ ~ (i, j 0 )) > 0 Thus, the optimization reduces to a binary classification problem. Each training instance is the difference vector ϕ ~ (i, j) − ϕ ~ (i, j 0 )) of a pair of candidates, and its training label is positive or negative depending on whether"
Q16-1029,D15-1044,0,0.0729444,"Missing"
Q16-1029,E14-1076,0,0.0314472,"text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pairs with 2 million words), the diversity and coverage of patterns that can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bi"
Q16-1029,N10-1144,0,0.0389537,"first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sentences. Deletion removes less important parts of a sentence. The paraphrasing operation includes reordering, lexical substitutions and syntactic transformations. While sentence splitting (Siddharthan, 2006; Petersen and Ostendorf, 20"
Q16-1029,C04-1129,0,0.0352311,"f statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task. 1 Introduction The goal of text simplification is to rewrite an input text so that the output is more readable. Text simplification has applications for reducing input complexity for natural language processing (Siddharthan et al., 2004; Miwa et al., 2010; Chen et al., 2012b) and providing reading aids for people with limited language skills (Petersen and Ostendorf, 2007; Watanabe et al., 2009; Allen, 2009; De Belder and Moens, 2010; Siddharthan and Katsos, 2010) or language impairments such as dyslexia (Rello et al., 2013), autism (Evans et al., 2014), and aphasia (Carroll et al., 1999). It is widely accepted that sentence simplification can be implemented by three major types of operations: splitting, deletion and paraphrasing (Feng, 2008). The splitting operation decomposes a long sentence into a sequence of shorter sente"
Q16-1029,S12-1046,0,0.0151082,"Missing"
Q16-1029,P15-2135,0,0.164802,"Missing"
Q16-1029,W14-1201,0,0.576387,"rules, which is important given the fact that Simple Wikipedia and the newly released Newsela simplification corpus (Xu et al., 2015) are only available for English. Second, previous evaluation used in the simplification literature is uninformative and not comparable across models due to the complications between the three different operations of paraphrasing, deletion, and splitting. This, combined with the unreliable quality of Simple Wikipedia as a gold reference for evaluation, has been the bottleneck for developing automatic metrics. There exist only a few studˇ ies (Wubben et al., 2012; Stajner et al., 2014) on automatic simplification evaluation using existing MT metrics which show limited correlation with human assessments. In this paper, we restrict ourselves to lexical simplification, where we believe MT-derived evaluation metrics can best be deployed. Our newly proposed metric is the first automatic metric that shows reasonable correlation with human evaluation on the text simplification task. We also introduce multiple references to make automatic evaluation feasible. The most related work to ours is that of Ganitkevitch et al. (2013) on sentence compression, in which compression of word an"
Q16-1029,N10-1056,0,0.0281531,"Missing"
Q16-1029,P12-2008,0,0.533776,"also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to design automatic eva"
Q16-1029,C10-1152,0,0.796098,"ave been intensively studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing problem: given an input text, rewrite it subject to the constraints that the output should be simpler than the input, while preserving as much meaning of the input as possible, and maintaining the well-formedness of the text. Going beyond previous work, we make di401 Transactions of the Association for Computational Linguistics, vol. 4,"
Q16-1029,P08-1040,0,0.495345,"d3 Pdel (7) where d1 = d2 = d3 = 1/3 and Poperation = Roperation 1 k 1 = k Foperation = X poperation (n) n=[1,...,k] X roperation (n) n=[1,...,k] 2 × Poperation × Roperation Poperation + Roperation operation ∈ [del, keep, add] where k is the highest n-gram order and set to 4 in our experiments. 3.2 Incorporating Large-Scale Paraphrase Rules Another challenge for text simplification is generating an ample set of rewrite rules that potentially simplify an input sentence. Most early work has relied on either hand-crafted rules (Chandrasekar et al., 1996; Carroll et al., 1999; Siddharthan, 2006; Vickrey and Koller, 2008) or dictionaries like WordNet (Devlin et al., 1999; Kaji et al., 2002; Inui et al., 2003). Other more recent studies have relied on 405 the parallel Normal-Simple Wikipedia Corpus to automatically extract rewrite rules (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011b; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014). This technique does manage to learn a small number of transformations that simplify. However, we argue that because the size of the Normal-Simple Wikipedia parallel corpus is quite small (108k sentence pair"
Q16-1029,W11-2160,1,0.839407,"can be learned is actually quite limited. In this paper we will leverage the large-scale Paraphrase Database (PPDB)4 (Ganitkevitch et al., 2013; Pavlick et al., 2015) as a rich source of lexical, phrasal and syntactic simplification operations. It is created by extracting English paraphrases from bilingual parallel corpora using a technique called “bilingual pivoting” (Bannard and Callison-Burch, 2005). The PPDB is represented as a synchronous context-free grammar (SCFG), which is commonly used as the formalism for syntax-based machine translation (Zollmann and Venugopal, 2006; Chiang, 2007; Weese et al., 2011). Table 1 shows some example paraphrase rules in the PPDB. PPDB employs 1000 times more data (106 million sentence pairs with 2 billion words) than the Normal-Simple Wikipedia parallel corpus. The English portion of PPDB contains over 220 million paraphrase rules, consisting of 8 million lexical, 73 million phrasal and 140 million syntactic para4 http://paraphrase.org phrase patterns. The key differences between the paraphrase rules from PPDB and the transformations learned by the naive application of SMT to the Normal-Simple Wikipedia parallel corpus, are that the PPDB paraphrases are much mo"
Q16-1029,D11-1038,0,0.156758,"ly studied, there has been considerably less research on developing new paraphrasing models for text simplification — most previous work has used off-the-shelf statistical machine translation (SMT) technology and achieved reasonable results (Coster and Kauchak, 2011a,b; Wubben ˇ et al., 2012; Stajner et al., 2015). However, they have either treated the judgment technology as a black (Coster and Kauchak, 2011a,b; Narayan and Garˇ dent, 2014; Angrosh et al., 2014; Stajner et al., 2015) or they have been limited to modifying only one aspect of it, such as the translation model (Zhu et al., 2010; Woodsend and Lapata, 2011) or the reranking component (Wubben et al., 2012). In this paper, we present a complete adaptation of a syntax-based machine translation framework to perform simplification. Our methodology poses text simplification as a paraphrasing problem: given an input text, rewrite it subject to the constraints that the output should be simpler than the input, while preserving as much meaning of the input as possible, and maintaining the well-formedness of the text. Going beyond previous work, we make di401 Transactions of the Association for Computational Linguistics, vol. 4, pp. 401–415, 2016. Action E"
Q16-1029,P12-1107,0,0.19806,"Missing"
Q16-1029,Q15-1021,1,0.646143,"ct modifications to four key components in the SMT pipeline:1 1) two novel simplification-specific tunable metrics; 2) large-scale paraphrase rules automatically derived from bilingual parallel corpora, which are more naturally and abundantly available than manually simplified texts; 3) rich rule-level simplification features; and 4) multiple reference simplifications collected via crowdsourcing for tuning and evaluation. In particular, we report the first study that shows promising correlations of automatic metrics with human evaluation. Our work answers the call made in a recent TACL paper (Xu et al., 2015) to address problems in current simplification research — we amend human evaluation criteria, develop automatic metrics, and generate an improved multiple reference dataset. Our work is primarily focused on lexical simplification (rewriting words or phrases with simpler versions), and to a lesser extent on syntactic rewrite rules that simplify the input. It largely ignores the important subtasks of sentence splitting and deletion. Our focus on lexical simplification does not affect the generality of the presented work, since deletion or sentence splitting could be applied as pre- or post-proce"
Q16-1029,C12-1177,1,0.164975,"mar. Our work is also related to other tunable metrics designed to be very simple and light-weight to ensure fast repeated computation for tuning bilingual translation models (Liu et al., 2010; Chen et al., 2012a). To the best of our knowledge, no tunable metric has been attempted for simplification, except for BLEU. Nor do any evaluation metrics exist for simplification, although there are several designed for other text-to-text generation tasks: grammatical error correction (Napoles et al., 2015; Felice and Briscoe, 2015; Dahlmeier and Ng, 2012), paraphrase generation (Chen and Dolan, 2011; Xu et al., 2012; Sun and Zhou, 2012), and conversation generation (Galley et al., 2015). Another line of related work is lexical simplification that focuses on finding simpler synonyms of a given complex word (Yatskar et al., 2010; Biran et al., 2011; Specia et al., 2012; Horn et al., 2014). 3 Adapting Machine Translation for Simplification We adapt the machinery of statistical machine translation to the task of text simplification by making changes in the following four key components: 3.1 Simplification-specific Objective Functions In the statistical machine translation framework, one crucial element is to"
Q16-1029,W06-3119,0,\N,Missing
Q16-1029,N15-1072,1,\N,Missing
S15-2001,S12-1051,0,0.130172,"Figure 2: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. 4.3 Annotation Quality We remove problematic annotators by checking their Cohen’s Kappa agreement (Artstein and Poesio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on the test dataset of 972 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different topics. Although the two scales of expert and crowdsourcing annotation are defined differently, their Pearson correlation coefficient reaches 0.735 (twotailed significance 0.001). Figure 1 shows a heatmap representing the detailed overlap between the two annotations. It suggests that the graded s"
S15-2001,J08-4004,0,0.0181162,"010): https://github.com/brendano/tweetmotif Trending Topics U.S. Facebook Dwight Howard GWB Netflix Ronaldo Dortmund Momma Dee Morning Huck Klay Milwaukee Harvick Jeff Green Ryu The Clippers Candice Robert Woods Amber Reggie Miller filtered random 0.0 0.2 0.4 0.6 0.8 Percentage of Positive Judgements Figure 2: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield. 4.3 Annotation Quality We remove problematic annotators by checking their Cohen’s Kappa agreement (Artstein and Poesio, 2008) with other annotators. We also compute inter-annotator agreement with an expert annotator on the test dataset of 972 sentence pairs. In the expert annotation, we adopt a 5-point Likert scale to measure the degree of semantic similarity between sentences, which is defined by Agirre et al. (2012) as follows: 5: Completely equivalent, as they mean the same thing; 4: Mostly equivalent, but some unimportant details differ; 3: Roughly equivalent, but some important information differs/missing. 2: Not equivalent, but share some details; 1: Not equivalent, but are on the same topic; 0: On different t"
S15-2001,S15-2004,0,0.0261708,"adopts typical machine learning classifiers and uses a variety of features, such as surface text, semantic level, textual entailment, word distributional representations by deep learning methods. FBK-HLT (Ngoc Phuoc An Vo and Popescu, 2015): This team uses supervised learning model with different features for the 2 runs, such as n-gram overlap, word alignment and edit distance. Hassy: This team uses a bag-of-embeddings approach via supervised learning. Two sentences are first embedded into a vector space, and then the system computes the dot-product of the two sentence embeddings. HLTC-HKUST (Bertero and Fung, 2015): This team uses supervised classification with a standard two-layer neural network classifier. The features used include translation metrics, lexical, syntactic and semantic similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, whil"
S15-2001,S14-2085,0,0.0857023,"Missing"
S15-2001,S15-2010,0,0.0246542,"Gram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and 8 RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP) for predicting semantic similarity where indicators of translatability are used as features (Bic¸ici and Way, 2014) and instance selection for RTM is performed with FDA5 (Bic¸ici and Yuret, 2014). RTM works as follows: FDA5 → MTPP → ML training → predict. Rob (van der Goot and van Noord, 2015): This system is inspired by a state-of-the-art semantic relatedness prediction system by Bjerva et al. (2014). It combines features from different parses with lexical and compositional dis"
S15-2001,S14-2114,0,0.0383579,"Missing"
S15-2001,D12-1050,0,0.0203303,"arch shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http"
S15-2001,P11-1020,0,0.104844,"Missing"
S15-2001,P09-1053,0,0.834216,"op ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has"
S15-2001,R13-1026,0,0.018233,"Missing"
S15-2001,C04-1051,1,0.79413,"Missing"
S15-2001,S15-2011,0,0.319055,"Figure 4: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC. AJ: This team utilizes TERp and BLEU – automatic evaluation metrics for Machine Translation. The system uses a logistic regression model and performs threshold selection. AMRITACEN: This team uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hi"
S15-2001,S15-2012,0,0.0178189,"Missing"
S15-2001,P12-1091,0,0.400672,"mmary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 1–11, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Paraphrase? yes yes yes no no debatable debatable Sentence 1 Ezekiel Ansah wearing 3D glasses wout the lens Marriage equality law pass"
S15-2001,C14-1047,0,0.0465864,"m uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM and logistic reThere are in total 19 teams participated: 6 Rank PI SS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 8 1 3 4 5 9 10 11 12 2 15 16 17 late 18 6 7 13 late Team Run Human Upperbound ASOBEK 01 svckernel ASOBEK 02 linearsvm MITRE 01 ikr ECNU 02 nnfeats FBK-HLT 01 voted TKLBLIIR 02 gs0105"
S15-2001,S13-1005,0,0.0243588,"d edit distance. Hassy: This team uses a bag-of-embeddings approach via supervised learning. Two sentences are first embedded into a vector space, and then the system computes the dot-product of the two sentence embeddings. HLTC-HKUST (Bertero and Fung, 2015): This team uses supervised classification with a standard two-layer neural network classifier. The features used include translation metrics, lexical, syntactic and semantic similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between"
S15-2001,N06-2015,0,0.0323369,"ith (2009). It uses simple n-gram (also in stemmed form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). It uses 0.5 as cutoff in the binary paraphrase identification task. 6 Systems and Results A total of 18 teams participated in the PI task (required), 13 of which also submitted to the SS task (optional). Every team submitted 2 runs except one (up to 2 were are allowed). 6.1 Evaluation Results Table 3 shows the evaluation results. We use the F1score and Pearson correlation as the primary evaluation metric for the PI and SS task respectively. The results are very exciting that most systems outperformed the two strong baselines we chose,"
S15-2001,D13-1090,0,0.100953,"witter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http://aclweb.org/aclwiki/index.php? title=Paraphras"
S15-2001,S15-2013,0,0.0114643,"rategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC. AJ: This team utilizes TERp and BLEU – automatic evaluation metrics for Machine Translation. The system uses a logistic regression model and performs threshold selection. AMRITACEN: This team uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM an"
S15-2001,J10-3003,0,0.0530457,"model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach &gt;0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available.1 1 Introduction The ability to identify paraphrases, i.e. alternative expressions of the same (or similar) meaning, and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detect"
S15-2001,N12-1019,0,0.281154,"echniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2 http://aclweb.org/aclwiki/"
S15-2001,W06-1603,0,0.0987618,"ominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexic"
S15-2001,D11-1141,0,0.00931635,"This causes two problems: a) it is expensive to obtain paraphrases via manual annotation; b) non-expert annotators tend to loosen the criteria and are more likely to make false positive errors. To address these challenges, we design a simple annotation task and introduce two selection mechanisms to select sentences which are more likely to be paraphrases, while preserving diversity and representativeness. 3 The tokenizer was developed by O’Connor et al. (2010): https://github.com/brendano/tweetmotif 4 The POS tagger was developed by Derczynski et al. (2013) and the NER tagger was developed by Ritter et al. (2011): https://github.com/aritter/twitter_nlp 3 =5 turk =4 turk =3 turk =2 turk turk Figure 1: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are nonparaphrases; 4,5 a"
S15-2001,S15-2009,0,0.0191271,"lap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM and logistic reThere are in total 19 teams participated: 6 Rank PI SS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 8 1 3 4 5 9 10 11 12 2 15 16 17 late 18 6 7 13 late Team Run Human Upperbound ASOBEK 01 svckernel ASOBEK 02 linearsvm MITRE 01 ikr ECNU 02 nnfeats FBK-HLT 01 voted TKLBLIIR 02 gs0105 MITRE 02 bieber HLTC-HKUST 02 run2 HLTC-HKUST 01 run1 ECNU 01 mlfeats AJ 01 first DEPTH 02 modelx23 CDTDS 01 simple CDTDS 02 simplews DEPTH 01 modelh22 FBK-HLT 02 multilayer ROB 01 all EBIQUITY 01 run TKLBLIIR 01 gsc054 EBIQUITY 02 run BASELINE logistic reg. COLUMBIA 02 ormf  HA"
S15-2001,S15-2007,0,0.0719415,"Missing"
S15-2001,U06-1019,0,0.299872,"ry; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used"
S15-2001,D13-1008,0,0.0670419,"useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent s"
S15-2001,Q14-1034,1,0.728134,"ar task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are nonparaphrases; 4,5 are paraphrases. Medium-scored cases (2 for crowdsourcing; 3 for expert annotation) are discarded in the system evaluation of the PI sub-task. 4.1 4 =1 expert=0 =0 In this shared task, we use the Twitter Paraphrase Corpus that we first presented in (Xu, 2014) and (Xu et al., 2014). Table 2 shows the basic statistics of the corpus. The sentences are preprocessed with tokenization,3 POS and named entity tags.4 The training and development set consists of 17,790 sentence pairs posted between April 24th and May 3rd, 2013 from 500+ trending topics featured on Twitter (excluding hashtags). The training and development set is a random split. Each sentence pair is annotated by 5 different crowdsourcing workers. For the test set, we obtain both crowdsourced and expert labels on 972 sentence pairs from 20 randomly sampled Twitter trending topics between May 13th and June 10th, 2"
S15-2001,W13-2515,1,0.744453,"larity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2"
S15-2001,D11-1061,0,0.174929,"meaning, and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1 http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam"
S15-2001,S15-2002,0,0.0446902,"similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and 8 RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP) for predicting semantic similarity where indicators of translatability are used as features (Bic¸ici and Way, 2014) and instance selection for RTM"
S15-2001,N12-1034,0,\N,Missing
S15-2001,S15-2006,0,\N,Missing
W09-2423,P98-1013,0,0.0111712,"Missing"
W09-2423,P01-1017,0,0.22112,"Missing"
W09-2423,N06-1024,0,0.0607349,"Missing"
W09-2423,W04-3228,0,0.0260109,"Missing"
W09-2423,P06-2055,1,0.767512,"Missing"
W09-2423,C02-1122,0,0.0190732,"rse of eight years or so. In contrast, the Chinese and Japanese systems are newer and considerably less time was spent developing them. Thus they currently do not represent as many regularizations. One obstacle is that we do not currently use subcategorization dictionaries for either language, while we have several for English. In particular, these would be helpful in predicting and filling relative clause and others gaps. We are considering auto153 matically acquiring simple dictionaries by recording frequently occurring argument types of verbs over a larger corpus, e.g., along the lines of (Kawahara and Kurohashi, 2002). In addition, existing Japanese dictionaries such as the IPAL (monolingual) dictionary (technology Promotion Agency, 1987) or previously acquired case information reported in (Kawahara and Kurohashi, 2002). Finally, we are investigating several avenues for using this system output for Machine Translation (MT) including: (1) aiding word alignment for other MT system (Wang et al., 2007); and (2) aiding the creation various MT models involving analyzed text, e.g., (Gildea, 2004; Shen et al., 2008). Acknowledgments This work was supported by NSF Grant IIS0534700 Structure Alignment-based MT. Refe"
W09-2423,W04-2705,1,0.83884,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,meyers-etal-2004-cross,1,0.822383,"conjunctions (e.g., and, or, but) and their conjuncts are transparent CONJ relations. Thus although and links together John and Mary, it is these dependents that determine that the resulting phrase is noun-like (an NP in phrase structure terminology) and sentient (and thus can occur as the subject of verbs like ate). Another common example of transparent relations are the relations connecting certain nouns and the prepositional objects under them, e.g., the box of cookies is edible, because cookies are edible even though boxes are not. These features are marked in the NOMLEX-PLUS dictionary (Meyers et al., 2004b). In Figure 4, we represent transparent relations, by prefixing the LOGIC1 label with asterisks. The above description most accurately describes English GLARF. However, Chinese GLARF has most of the same properties, the main exception being that PDTB arguments are not currently marked. 149 For Japanese, we have only a preliminary representation of LOGIC2 relations and they are not derived from PropBank/NomBank/PDTB. 2.1 Scoring the LOGIC1 Structure For purposes of scoring, we chose to focus on LOGIC1 relations, our proposed high-performance level of semantics. We scored with respect to: the"
W09-2423,W07-1529,1,0.799455,"Missing"
W09-2423,W04-2703,0,0.139121,"Missing"
W09-2423,I05-4002,0,0.0390298,"Missing"
W09-2423,J05-1004,0,0.21622,"Missing"
W09-2423,W05-0302,1,0.852675,"els; we have a different set of relational labels; and finally, our approach is designed to be compatible with the Penn Treebank framework and therefore, Penn-Treebankbased parsers. In addition, the expansion of our theory is governed more by available resources than by the underlying theory. As our main goal is to use our system to regularize data, we freely incorporate any analysis that fits this goal. Over time, we have found ways of incorporating Named Entities, PropBank, NomBank and the Penn Discourse Treebank. Our agenda also includes incorporating the results of other research efforts (Pustejovsky et al., 2005). For each sentence, we generate a feature structure (FS) representing our most complete analysis. We distill a subset of this information into a dependency structure governed by theoretical assumptions, e.g., about identifying functors of phrases. Each GLARF dependency is between a functor and an argument, where the functor is the head of a phrase, conjunction, complementizer, or other function word. We have built applications that use each of these two representations, e.g., the dependency representation is used in (Shinyama, 2007) and the FS representation is used in (K. Parton and K. R. Mc"
W09-2423,P08-1066,0,0.0264245,"Missing"
W09-2423,P03-1010,0,0.0528115,"Missing"
W09-2423,D07-1077,0,0.0600045,"Missing"
W09-2423,J08-2004,1,0.877787,"Missing"
W09-2423,J93-2004,0,\N,Missing
W09-2423,W08-2121,1,\N,Missing
W09-2423,J03-4003,0,\N,Missing
W09-2423,P09-1048,1,\N,Missing
W09-2423,C98-1013,0,\N,Missing
W09-2423,W09-1201,1,\N,Missing
W09-2809,P06-1048,0,0.0611244,"gh the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally by Hori and Furui (2004) for spoken text in Japanese, and later modified by Clarke and Lapata (2006) for English text. Their model searches for the compression with highest score according to the significance of each word, the existence of SubjectVerb-Object structures and the language model probability of the resulting word combination. The weight factors to balance the three measurements are experimentally optimized by a parallel corpus or estimated by experience. Turner and Charniak (2005) present semisupervised and unsupervised variants of the noisy channel model. They approximate the rules of compression from a non-parallel corpus (e.g. the Penn Treebank) based on probabilistic context"
W09-2809,D07-1008,0,0.0537197,"t previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally"
W09-2809,P05-1036,0,0.023337,"s have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally by Hori and Furui (2004) for spoken text in Japanese, and later modified by Clarke and Lapata (2006) for English text. Their model searches for the compression with highest score according to the significance of each word, the existence of SubjectVerb-Object structures and the language model probability of the resulting word combination"
W09-2809,W03-0501,0,0.421676,"possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora was proposed originally by Hori and Furui (2004) for spoken text in Japanese, and later modified by Clarke and Lapata (2006) for English text. Their model searches for the compression with highest score according to the significance of each word, the existence of SubjectVerb-Object structures and the language m"
W09-2809,N07-1023,0,0.0258587,"ly useless compressed sentence. Another major drawback is that it requires considerable linguistic skill to produce proper rules in a proper order. Previous Work Most previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English."
W09-2809,A00-1043,0,0.065826,"Missing"
W09-2809,P06-1047,1,0.903871,"n contrast to probabilistic methods, the heuristics are more likely to produce grammatical and fluent compressed sentences. We reduce the difficulty and linguistic skills required for composing heuristics by only requiring these heuristics to identify possibly removable constituents instead of selecting specific constituents for removal. The word significance helps to preserve informative constituents and overcome some POS and parsing errors. In particular, we seek to assess the event information during the compression process, according to the previous successes in event-based summarization (Li et al, 2006) and a new eventoriented 5W summarization task (Parton et al, 2009). The next section presents previous approaches to sentence compression. In section 3, we describe our system with three modules, viz. linguistically-motivated heuristics, word significance scoring and candidate compression selection. We also develop a heuristics-only approach for comparison. In section 4, we evaluate the compressions in terms of grammaticality, inforAbstract In this paper, we propose an event-based approach for Chinese sentence compression without using any training corpus. We enhance the linguistically-motiva"
W09-2809,E06-1038,0,0.0184001,"revious Work Most previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorithm making limited use of training corpora"
W09-2809,C04-1107,0,0.0139409,"ll to produce proper rules in a proper order. Previous Work Most previous studies relied on a parallel corpus to learn the correspondences between original and compressed sentences. Typically sentences are represented by features derived from parsing results, and used to learn the transformation rules or estimate the parameters in the score function of a possible compression. A variety of models have been developed, including but not limited to the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007), the decision-tree model (Knight and Marcu, 2002), support vector machines (Nguyen et al, 2004) and large-margin learning (McDonald, 2006; Cohn and Lapata 2007). Approaches which do not employ parallel corpora are less popular, even though the parallel sentence/compression corpora are not as easy to obtain as multilingual corpora for machine translation. Only a few studies have been done requiring no or minimal training corpora (Dorr et al, 2003; Hori and Furui, 2004; Turner and Charniak, 2005). The scarcity of parallel corpora also constrains the development in languages other than English. To the best of our knowledge, no study has been done on Chinese sentence compression. An algorit"
W09-2809,P09-1048,1,\N,Missing
W09-2809,D09-1087,0,\N,Missing
W09-3019,P03-1010,0,0.0268446,"Missing"
W09-3019,P01-1017,0,0.0441748,"our grammars of Japanese and Chinese do not currently.; (5) a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automati"
W09-3019,J02-3001,0,0.015508,"a logic2 label (L2) for Chinese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generat"
W09-3019,J08-2004,1,0.84166,"inese and English, which represents PropBank, NomBank and Penn Discourse Treebank relations; and (6) Asterisks (*) indicate transparent relations, relations where the functor inherits semantic properties of certain special arguments (*CONJ, *OBJ, *PRD, *COMP). GLARF relations are generated from treebank and parses for English, Chinese and Japanese. Our evaluation of system output for these input types requires consideration of multiple correct answers.1 1 Introduction Systems, such as treebank-based parsers (Charniak, 2001; Collins, 1999) and semantic role labelers (Gildea and Jurafsky, 2002; Xue, 2008), are trained and tested on hand-annotated data. Evaluation is based on differences between system output and test data. Other systems use these programs to perform tasks unrelated to the original annotation. For example, participating systems in CONLL (Surdeanu et al., 2008; Hajiˇc et al., 2009), ACE and GALE tasks merged the results of several processors (parsers, named entity recognizers, etc.) not initially designed for the task at hand. This paper discusses differences between handannotated data and automatically generated data with respect to our GLARFers, systems for generating Grammati"
W09-3019,W09-1201,1,0.825301,"Missing"
W09-3019,D09-1087,1,0.855247,"Missing"
W09-3019,P06-2055,1,0.900802,"Missing"
W09-3019,W08-2121,1,\N,Missing
W09-3019,J03-4003,0,\N,Missing
W09-3019,W04-0413,1,\N,Missing
W09-3019,W09-2423,1,\N,Missing
W13-1103,J93-1003,0,0.217259,"another one with a flexible number (vary from 1 to 4) of tweets. Both ↵ and are set to 0.1 in our implementation. All parameters are set experimentally over a small development dataset consisting of 10 events in Twitter data of September 2012. EventRank−Flexible 0 a Twitter specific name entity tagger1 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Tweets published during the whole period are aggregated together to find top events that happen on each calendar day. We applied the G2 test for statistical significance (Dunning, 1993) to rank the event clusters, considering the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. We randomly picked the events of one day for human evaluation, that is the day of January 16, 2013 with 38 events and an average of 465 tweets per event cluster. 4 5 compactness completeness overall EventRank−Flexible EventRank−Fixed SumBasic Figure 5: human judgments evaluating tweet summarization systems Event System EventRank (Flexible) Google 1/16/2013 SumBasic EventRank (Flexible) Instagram 1/16/2013 SumBa"
W13-1103,P11-2008,0,0.0183447,"Missing"
W13-1103,P06-1047,1,0.898837,"(e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explici"
W13-1103,W11-0709,0,0.423753,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P11-1037,0,0.32789,"er of messages, many containing irrelevant and redundant information, quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natura"
W13-1103,P12-3003,0,0.028684,"generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects of the initial event to be summarized to create comprehen20 Proceedings of the W"
W13-1103,P00-1010,0,0.136012,"epresent the relationship between them. We then rank and partition the events using PageRank-like algorithms, and create summaries of variable length for different topics. 3.1 Event Extraction from Tweets As a first step towards summarizing popular events discussed on Twitter, we need a way to identify events from Tweets. We utilize several natural language processing tools that specially developed for noisy text to extract text phrases that bear essential event information, including named entities (Ritter et al., 2011), event-referring phrases (Ritter et al., 2012) and temporal expressions (Mani and Wilson, 2000). Both the named entity and event taggers utilize Conditional Random Fields models (Lafferty, 2001) trained on annotated data, while the temporal expression resolver uses a mix of hand-crafted and machine-learned rules. Example event information extracted from Tweets are presented in Table 2. The self-contained nature of tweets allows efficient extraction of event information without deep analysis (e.g. co-reference resolution). On the other hand, individual tweets are also very terse, often lacking sufficient context to access the importance of events. It is crucial to exploit the highly redu"
W13-1103,radev-etal-2004-mead,0,0.0270509,"event cluster with a single but complex focus 25 4.2 Figure 4: Event graph of ’West Ham - 1/16/2013’, an example of event cluster with a single focus Baseline SumBasic (Vanderwende et al., 2007) is a simple and effective summarization approach based on term frequency, which we use as our baseline. It uses word probabilities with an update function to avoid redundancy to select sentences or posts in a social media setting. It is shown to outperform three other well-known multi-document summarization methods, namely LexRank (Erkan and Radev, 2004), TextRank (Mihalcea and Tarau, 2004) and MEAD (Radev et al., 2004) on tweets in (Inouye and Kalita, 2011), possibly because that the relationship between tweets is much simpler than between sentences in news articles and can be well captured by simple frequency methods. The improvement over the LexRank model on tweets is gained by considering the number of retweets and influential users is another side-proof (Wei et al., 2012) of the effectiveness of frequency. Annotator 1 1 https://github.com/aritter/twitter_nlp 26 3 2 1 0 EventRank−Fixed SumBasic Annotator 2 2 3 4 5 compactness completeness overall 1 For each cluster, our systems produce two versions of su"
W13-1103,D11-1141,1,0.233403,"ted advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to extract named entities and events in the tweets as has been shown for classic multi-document summarization (Li et al., 2006). Recent progress on building NLP tools for Twitter (Ritter et al., 2011; Gimpel et al., 2011; Liu et al., 2011b; Ritter et al., 2012; Liu et al., 2012) makes it possible to investigate an approach to summarizing Twitter events which is based on Information Extraction techniques. We investigate a graph-based approach which leverages named entities, event phrases and their connections across tweets. A similar idea has been studied by Li et al. (2006) to rank the salience of event concepts in summarizing news articles. However, the extreme redundancy and simplicity of tweets allows us to explicitly split the event graph into subcomponents that cover various aspects"
W13-1103,C12-1047,0,0.0630086,"quickly leads to a situation of information overload. This motivates the need for automatic summarization systems which can select a few messages for presentation to a user which cover the most important information relating to the event without redundancy and filter out irrelevant and personal information that is not of interest beyond the user’s immediate social network. Although there is much recent work focusing on the task of multi-tweet summarization (Becker et al., 2011; Inouye and Kalita, 2011; Zubiaga et al., 2012; Liu et al., 2011a; Takamura et al., 2011; Harabagiu and Hickl, 2011; Wei et al., 2012), most previous work relies only on surface lexical clues, redundancy and social network specific signals (e.g. user relationship), and little work has considered taking limited advantage of information extraction techniques (Harabagiu and Hickl, 2011) in generative models. Because of the noise and redundancy in social media posts, the performance of off-the-shelf news-trained natural language process systems is degraded while simple term frequency is proven powerful for summarizing tweets (Inouye and Kalita, 2011). A natural and interesting research question is whether it is beneficial to ext"
W13-1103,W04-3252,0,\N,Missing
W13-2515,N03-1003,0,0.0737541,"This paper presents the first investigation into automatically collecting a large paraphrase corpus of tweets, which can be used for building paraphrase systems adapted to Twitter using techniques from statistical machine translation (SMT). We show experimental results demonstrating the benefits of an in-domain parallel corpus when paraphrasing tweets. In addition, our paraphrase models can be applied to the task of normalizing noisy text where we show improvements over the state-of-the-art. Relevant previous work has extracted sentencelevel paraphrases from news corpora (Dolan et al., 2004; Barzilay and Lee, 2003; Quirk et al., 2004). Paraphrases gathered from noisy usergenerated text on Twitter have unique characteristics which make this comparable corpus a valuable new resource for mining sentence-level paraphrases. Twitter also has much less context than news articles and much more diverse content, thus posing new challenges to control the noise in mining paraphrases while retaining the desired superficial dissimilarity. We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a"
W13-2515,2011.iwslt-evaluation.18,0,0.00622054,"Missing"
W13-2515,P11-1020,0,0.229551,"ine at https://github.com/cocoxu/ twitterparaphrase/ 121 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121–128, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics we apply a typical phrase-based statistical MT pipeline, performing word alignment on the parallel data using GIZA++ (Och and Ney, 2003), then extracting phrase pairs and performing decoding uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task"
W13-2515,C04-1051,0,0.644793,"Missing"
W13-2515,J10-3003,0,0.141397,"ty of this new resource on the task of paraphrasing and normalizing noisy text, showing improvement over several state-of-the-art paraphrase and normalization systems 1 . 1 Introduction Social media services provide a massive amount of valuable information and demand NLP tools specifically developed to accommodate their noisy style. So far not much success has been reported on a key NLP technology on social media data: paraphrasing. Paraphrases are alternative ways to express the same meaning in the same language and commonly employed to improve the performance of many other NLP applications (Madnani and Dorr, 2010). In the case of Twitter, Petrovi´c et al. (2012) showed improvements on first story detection by using paraphrases extracted from WordNet. Learning paraphrases from tweets could be especially beneficial. First, the high level of information redundancy in Twitter provides a good opportunity to collect many different expressions. Second, tweets contain many kinds of paraphrases not available elsewhere including typos, abbreviations, ungrammatical expressions and slang, 2 Related Work There are several key strands of related work, including previous work on gathering parallel monolingual text fr"
W13-2515,I08-1059,0,0.015274,"Missing"
W13-2515,P00-1010,0,0.0216211,"se pair in our system, in addition to the four basic components (translation model, distortion model, language model and word penalty) in SMT. Paraphrasing Tweets 5.1.1 Data Our paraphrase dataset is distilled from a large corpus of tweets gathered over a one-year period spanning November 2011 to October 2012 using the Twitter Streaming API. Following Ritter et al. (2012), we grouped together all tweets which mention the same named entity (recognized using a Twitter specific name entity tagger3 ) and a reference to the same unique calendar date (resolved using a temporal expression processor (Mani and Wilson, 2000)). Then we applied a statistical significance test (the G test) to rank the events, which considers the corpus frequency of the named entity, the number of times the date has been mentioned, and the number of tweets which mention both together. Altogether we collected more than 3 million tweets from the 50 top events of each day according to the p-value from the statistical test, with an average of 229 tweets per event cluster. Each of these tweets was passed through a Twitter tokenizer4 and a simple sentence splitter, which also removes emoticons, URLs and most of the hashtags and usernames."
W13-2515,W11-2210,0,0.0157816,"ecause Twitter contains both normal and noisy language, with appropriate tuning, our models have the capability to translate between these two styles, e.g. paraphrasing into noisy style or normalizing into standard language. Here we demonstrate its capability to normalize tweets at the sentence-level. 5.2.1 Baselines Much effort has been devoted recently for developing normalization dictionaries for Microblogs. One of the most competitive dictionaries available today is HB-dict+GHM-dict+S-dict used by Han et al. (2012), which combines a manuallyconstructed Internet slang dictionary , a small (Gouws et al., 2011) and a large automaticallyTable 4: Example paraphrases of a given sentence “who want to get a beer” 126 derived dictionary based on distributional and string similarity. We evaluate two baselines using this large dictionary consisting of 41181 words; following Han et. al. (2012), one is a simple dictionary look up. The other baseline uses the machinery of statistical machine translation using this dictionary as a phrase table in combination with Twitter and NYT language models. 5.2.2 No-Change SMT+TwitterLM SMT+TwitterNYTLM Dictionary Dicionary+TwitterNYTLM SMT+Dictionary+TwitterNYTLM System D"
W13-2515,moore-2002-fast,0,0.019796,"and less agreement. 123 In addition, because no previous work has evaluated these metrics in the context of noisy Twitter data, we perform a human evaluation in which annotators are asked to choose which system generates the best paraphrase. Finally we evaluate our phrase-based normalization system against a state-of-the-art word-based normalizer developed for Twitter (Han et al., 2012). 5.1 velopment data and the exact configuration are released together with the phrase table for system replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure ever"
W13-2515,P11-1038,0,0.387701,"ng uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent"
W13-2515,W04-3243,0,0.0749033,"tem replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure every phrase has an option to remain unchanged during paraphrasing and normalization. Without these noise reduction steps, our system will produce paraphrases with serious errors (e.g. change a person’s last name) for 100 out of 200 test tweets in the evaluation in §5.1.5. At the same time, it is also important to promote lexical dissimilarity in the paraphrase task. Following Ritter et. al. (2011) we add a lexical similarity penalty to each phrase pair in our system, in addition to the fo"
W13-2515,D12-1039,0,0.361981,"al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying"
W13-2515,J03-1002,0,0.00357666,"d Work There are several key strands of related work, including previous work on gathering parallel monolingual text from topically clustered news articles, normalizing noisy Twitter text using word-based 1 Our Twitter paraphrase models are available online at https://github.com/cocoxu/ twitterparaphrase/ 121 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 121–128, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics we apply a typical phrase-based statistical MT pipeline, performing word alignment on the parallel data using GIZA++ (Och and Ney, 2003), then extracting phrase pairs and performing decoding uses Moses (Koehn et al., 2007). models, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related"
W13-2515,P02-1040,0,0.104655,"cal phrase-based statistical MT pipeline on the parallel data, which uses GIZA++ for word alignment and Moses for extracting phrase pairs, training and decoding. We use a language model trained on the 3 million collected tweets in the decoding process. The parameters are tuned over de5.1.2 Evaluation Details The beauty of lexical similarity penalty is that it gives control over the degree of paraphrasing by adjusting its weight versus the other components. Thus we can plot a BLEU-PINC curve to express the tradeoff between semantic adequacy and lexical dissimilarity with the input, where BLUE (Papineni et al., 2002) and PINC (Chen and Dolan, 2011) are previously proposed automatic evaluation metrics to measure respectively the two criteria of paraphrase quality. To compute these automatic evaluation metrics, we manually prepared a dataset of gold paraphrases by tracking the trending topics on Twitter5 and gathering groups of paraphrases in November 2012. In total 20 sets of sentences were collected and each set contains 5 different sentences that express the same meaning. Each sentence is used 3 https://github.com/aritter/twitter_ nlp 4 https://github.com/brendano/ tweetmotif 5 https://support.twitter.co"
W13-2515,W12-3153,0,0.0127172,"ses tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain paraphrase systems to improve performance at first story detection in Twitter (Petrovi´c et al., 2012). By building better paraphrase models adapted to Twitter, it should be possible to improve performance at such tasks, which benefit from paraphrasing Tweets. 3 3.1 Extracting Events from Tweets As a first step towards extracting paraphrases from popular events discussed on Twitter, we need a way to identify Tweets which mentio"
W13-2515,N12-1034,0,0.367404,"Missing"
W13-2515,W04-3219,0,0.0109492,"first investigation into automatically collecting a large paraphrase corpus of tweets, which can be used for building paraphrase systems adapted to Twitter using techniques from statistical machine translation (SMT). We show experimental results demonstrating the benefits of an in-domain parallel corpus when paraphrasing tweets. In addition, our paraphrase models can be applied to the task of normalizing noisy text where we show improvements over the state-of-the-art. Relevant previous work has extracted sentencelevel paraphrases from news corpora (Dolan et al., 2004; Barzilay and Lee, 2003; Quirk et al., 2004). Paraphrases gathered from noisy usergenerated text on Twitter have unique characteristics which make this comparable corpus a valuable new resource for mining sentence-level paraphrases. Twitter also has much less context than news articles and much more diverse content, thus posing new challenges to control the noise in mining paraphrases while retaining the desired superficial dissimilarity. We present a new and unique paraphrase resource, which contains meaningpreserving transformations between informal user-generated text. Sentential paraphrases are extracted from a comparable corpus of"
W13-2515,D07-1103,0,0.0134095,"ormalizer developed for Twitter (Han et al., 2012). 5.1 velopment data and the exact configuration are released together with the phrase table for system replication. Sentence alignment in comparable corpora is more difficult than between direct translations (Moore, 2002), and Twitter’s noisy style, short context and broad range of content present additional complications. Our automatically constructed parallel corpus contains some proportion of unrelated sentence pairs and therefore does result in some unreasonable paraphrases. We prune out unlikely phrase pairs using a technique proposed by Johnson et al. (2007) with their recommended setting, which is based on the significance testing of phrase pair co-occurrence in the parallel corpus (Moore, 2004). We further prevent unreasonable translations by adding additional entries to the phrase table to ensure every phrase has an option to remain unchanged during paraphrasing and normalization. Without these noise reduction steps, our system will produce paraphrases with serious errors (e.g. change a person’s last name) for 100 out of 200 test tweets in the evaluation in §5.1.5. At the same time, it is also important to promote lexical dissimilarity in the"
W13-2515,D11-1054,1,0.557611,"Missing"
W13-2515,D08-1027,0,0.0184776,"Missing"
W13-2515,P13-1018,0,0.0118995,"much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain paraphrase systems to improve performance at first story detection in Twitter (Petrovi´c et al., 2012). By building better paraphrase models adapted to Twitter, it should be possible to improve performance at such tasks, which benefit from paraphrasing Tweets. 3 3.1 Extracting Events from Tweets As a first step towards extracting paraphrases from popular events discussed on Twitter, we need a way to identify Tweets which mention the same event. To"
W13-2515,P12-1109,0,0.0224913,"s, and applying out-of-domain paraphrase systems to improve NLP tasks in Twitter. On the observation of the lack of a large paraphrase corpus, Chen and Dolan (2011) have resorted to crowdsourcing to collect paraphrases by asking multiple independent users for descriptions of the same short video. As we show in §5, however, this data is very different from Twitter, so paraphrase systems trained on in-domain Twitter paraphrases tend to perform much better. The task of paraphrasing tweets is also related to previous work on normalizing noisy Twitter text (Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012). Most previous work on normalization has applied word-based models. While there are challenges in applying Twitter paraphrase systems to the task of normalization, access to parallel text allows us to make phrase-based transformations to the input string rather than relying on word-to-word mappings (for more details see §4). Also relevant is recent work on collecting bilingual parallel data from Twitter (Jehl et al., 2012; Ling et al., 2013). In contrast, we focus on monolingual paraphrases rather than multilingual translations. Finally we highlight recent work on applying out-of-domain parap"
W13-2515,P07-2045,0,\N,Missing
W15-4319,W15-4308,0,0.196632,"et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supra"
W15-4319,P06-2005,0,0.224961,"Missing"
W15-4319,W15-4312,0,0.040755,"d using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the basis of n-grams tatistics. BEKLI (Beckley, 2015) A substitution dictionary is constructed in which keys are non-standard words and values are lists of potential normalizations. Frequent morphology errors are captured by hand-crafted rules. Finally, the Viterbi algorithm is applied to bigram sequences to decode the normalized sentence with maximum probability. LYSGROUP (Mosquera et al., 2015) A system originally developed for Spanish text normalization was adapted to English text normalization. The method consists of a cascaded pipeline of several data adaptors and processors, such as a Twitter POS tagger and a spell checker. 3 Named Entity"
W15-4319,W15-4318,0,0.105136,"Missing"
W15-4319,N15-1075,0,0.0319326,"Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Twitter NER (Ritter et al., 2011), which disti"
W15-4319,W15-4307,0,0.270391,"tain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-competition analysis of the effect of training on development sets is presented in the NRC system description paper (Cherry et al., 2015). 131 Figure 2: Annotation interface. POS Orthographic Gazetteers Brown clustering – X X X – – – X X X – X X – X – X X X – X – – X X X X – X – X – X X – X BASELINE Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Word embedding ML – correlation analysis – – word2vec word2vec & GloVe word2vec X – CRFsuite CRFsuite CRF++ CRF wapiti FFNN CRF++ semi-Markov MIRA entity linking CRF L-BFGS Table 7: Features and machine learning approach taken by each team. Precision Recall F ousia NLANGP nrc multimedialab USFD iitp Hallym lattice 57.66 63.62 53.24 49.52 45.72 60.68 39.59 55.17 55.22 43.12 38.5"
W15-4319,P14-2111,0,0.0445576,"r data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods,"
W15-4319,W09-2010,0,0.0437694,"guistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-"
W15-4319,W15-4306,0,0.105059,"hallenge is concept drift (Dredze et al., 2010; Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Tw"
W15-4319,W15-4322,0,0.0251441,"Missing"
W15-4319,P11-1038,1,0.912118,"Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can b"
W15-4319,D12-1039,1,0.757294,"zation examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words tha"
W15-4319,P13-1155,0,0.0178022,"rds can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task"
W15-4319,W15-4313,0,0.0798376,"shown in Tables 3 and 4. Overall, common approaches were lexicon-based methods, CRFs, and neural network-based approaches. Among the constrained systems, neural networks achieved strong results, even without off-the-shelf tools. In contrast, CRF- and lexicon-based approaches were shown to be effective in the unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et"
W15-4319,D14-1108,0,0.0271234,"new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited t"
W15-4319,W15-4323,0,0.0427639,"ING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015)"
W15-4319,D10-1057,0,0.157913,"Missing"
W15-4319,D13-1008,0,0.00982812,"level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on Engli"
W15-4319,P11-2013,0,0.247801,"dard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined paralle"
W15-4319,W15-4321,0,0.0536682,"s first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as activation function in combination with dropout to prevent overfitting. A context window of 5 words was used As input (2 words left and right). The output is a single tag of the middle word. Afterwards, a rule-based post-processing step was executed to ensure every I-tag has a B-tag in front of it and that all tags within a single span are of the same type. Train and dev were used as training data and used dev 2015 as validation set. NLANGP (Toh et al., 2015) The NLANGP team modeled the problem as a sequential labeling task and used Conditional Random Fields. Several post-processing steps (e.g. rulebased matching) were applied to refine the system output. Besides Brown clusters, Kmeans clusters were also used; the K-means clusters were generated based on word embeddings. nrc (Cherry et al., 2015) NRC applied a MIRAtrained semi-Markov tagger with Gazetteer, Brown cluster and Word Embedding features. The Word Embeddings were built over phrases using Word2Vec’s phrase finder tool, and were modified using an auto-encoder to be predictive of Gazetteer"
W15-4319,P12-1109,0,0.0482015,"es. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical no"
W15-4319,P10-1040,0,0.00514115,"d resources. There were 6 official submissions in the constrained category, and 5 official submissions in the unconstrained category. Overall, deep learning methods and methods based on lexicon-augmented conditional random fields (CRFs) achieved the best results. The winning team achieved a precision of 0.9061 precision, recall of 0.7865, and F1 of 0.8421. The named entity recognition task attracted 8 participants. The majority of teams built their systems using linear-chain conditional random fields (Lafferty et al., 2001), and many teams also used brown clusters and word embedding features (Turian et al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al."
W15-4319,P12-3005,1,0.690086,"e their system categories: • Constrained: participants could not use any data other than the provided training data to perform the text normalization task. They were allowed to use pre-trained tools (e.g., Twitter POS taggers), but no normalization lexicons or extra tweet data. • Unconstrained: participants could use any publicly accessible data or tools to perform the text normalization task. Evaluation was based on token-level precision, recall and F-score. 2.2.1 Preprocessing We first collected tweets using the Twitter Streaming API over the period 23–29 May, 2014, and then used langid.py (Lui and Baldwin, 2012)1 to remove all non-English tweets. Tokenization was performed with CMU-ARK tokeniser.2 To ensure that tweets had a high likelihood of requiring lexical normalization, we filtered out tweets with less than 2 non-standard words (i.e. words not occurring in our dictionary — see Section 2.2.3). While this biases the sample of tweets, the decision was made at a pragmatic level to ensure a reasonable level of lexical normalization and “annotation density”. This was based on a pilot study over a random sample of English tweets, in which we found that many non-standard words were actually unknown nam"
W15-4319,W15-4314,0,0.0181512,"Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction mo"
W15-4319,W15-4317,0,0.0288445,"unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al."
W15-4319,W15-4315,0,0.0571066,"Missing"
W15-4319,C14-1168,0,0.0359301,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,E14-1078,0,0.0294125,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,D11-1141,1,0.828298,"Missing"
W15-4319,N13-1050,0,0.00844789,"(e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One ma"
W15-4319,W15-4320,0,0.0994733,"all 56.64 57.52 57.07 =1 Table 5: Precision and recall comparing one annotator against the other. Cohen’s kappa between the annotators was 0.607. Disagreements between the annotators resolved by a 3rd adjudicator for the final datasets. Team ID Affiliation Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Hallym University Indian Institute of Technology Patna University Paris 3 UGent - iMinds Institute for Infocomm Research National Research Council Canada Studio Ousia University of Sheffield Table 6: Team ID and affiliation of the named entity recognition shared task participants. sia (Yamada et al., 2015). All the other teams used CRFs. On top of a CRF, the iitp team used a differential evolution based technique to obtain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-co"
W15-4319,D13-1007,0,0.160415,"anslation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words that happen to coincide in spel"
W15-4319,W15-4311,0,0.0298261,"2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the ba"
W15-4319,W15-4309,0,0.0208179,".14 54.31 56.28 55.22 54.61 51.44 48.5 25.72 70.63 60.29 59.81 58.82 58.13 56.81 53.01 35.71 BASELINE 53.86 46.44 49.88 =1 Table 8: Results segmenting and categorizing entities into 10 types. Hallym (Yang and Kim, 2015) The Hallym team used an approach based on CRFs using both Brown clusters and word embeddings trained using Canonical Correlation Analysis as features. iitp (Akhtar et al., 2015a) The iitp team pro=1 Table 9: Results on segmentation only (no types). posed a multi-objective differential evolution based technique for feature selection in twitter named entity recognition. lattice (Tian, 2015) Lattice employed a CRF model using Wapiti. The feature templates consisted of standard features used in stateof-the-art. They trained first a model with 132 dev 2015 and evaluated this model on train and dev. multimedialab (Godin et al., 2015) The goal of the multimedia lab system was to only use neural networks and word embeddings to show the power of automatic feature learning and semi-supervised methods. A FeedForward Neural Network was first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as act"
W15-4319,W03-0419,0,0.581711,"Missing"
W15-4319,fromreide-etal-2014-crowdsourcing,0,\N,Missing
W15-4319,W15-4316,0,\N,Missing
W16-3919,W15-4319,1,0.519066,"Missing"
W16-3919,D10-1057,0,0.0761634,"n when applied to noisy Twitter data. But tweets often contain more up-to-date information than news, in addition the increased volume of text offers opportunities to exploit redundancy of information which is very beneficial for information extraction (Downey et al., 2005). To exploit the opportunities for information extraction on top of social media, there is a crucial need for in-domain annotated data to train and evaluate named entity recognition systems on this noisy style of text. Twitter processing has the additional challenge that the language people use on Twitter changes over time (Dredze et al., 2010; Fromreide et al., 2014). The previous edition of this task (Baldwin et al., 2015) addressed this issue by evaluating on a test set collected from a later time period then the training and development data. This year we take a similar approach, providing a new test dataset of tweets gathered from 2016. In addition to enabling research on adapting named entity recognition to new language over time, we hope this new dataset will be useful for adapting future Twitter named entity recognition systems, improving their performance on up-to-date data. Additionally, this year we address the issue of"
W16-3919,W16-3924,0,0.0132806,"features based on lexicon-type information such as stop word matching, word frequencies, and entries in the shared task lexicon and Babelfy (Moro et al., 2014). Talos (Ioannis Partalas and Kalitvianski, 2016) The system uses three types of features: lexical and morpho-syntactic features, contextual enrichment features using Linked Open Data, and features based on distributed representation of words. The system also exploits words clustering to enhance performance. The learning algorithm was solved by using Learning to search (L2S) that resembles a reinforcement learning algorithm. DeepNNNER (Dugas and Nichols, 2016) The system uses a bidirectional LSTM-CNN model with word embedding trained on a large scale Web corpus. Additionally, the system uses automatically constructed lexicons with a partial matching algorithm and text normalization to handle the large vocabulary problem in Web texts. 141 CambridgeLTL Talos akora NTNU ASU DeepNNNER DeepER hjpwhu UQAM-NTL LIOX Precision Recall F1 60.77 58.51 51.70 53.19 40.58 54.97 45.40 48.90 40.73 40.15 46.07 38.12 39.48 32.13 37.58 28.16 31.15 28.76 23.52 12.69 52.41 46.16 44.77 40.06 39.02 37.24 36.95 36.22 29.82 19.26 CambridgeLTL NTNU Talos akora ASU DeepER Dee"
W16-3919,P05-1045,0,0.0488289,"the location of the shooting event. The shooting domain contains 8,963 tokens with 751 phrases. Computer hacking events were found by searching for tweets including the keyword “breach”. The breach domain contains 5,537 tokens with 603 phrases. The additional data annotated this year was completed by a single annotator instructed to follow the annotation guidelines of the prior annotations. The annotator was presented with a set of simple guidelines2 that cover common ambiguous cases and was also instructed to refer to the September 2010 data 1 2 For example, the Stanford named entity tagger (Finkel et al., 2005) achieves an F1 score of 0.86 on the CoNLL data set. http://bit.ly/1FSP6i2 139 for reference (Ritter et al., 2011). The BRAT tool3 was used for annotation. Figure 1 is a screenshot of the interface presented to the annotators. To ensure that the new annotations were consistent with the earlier annotations, 100 tweets were annotated in both tasks to calculate agreement. The new annotator proved a high agreement with the old data set with a F1 score of 67.67. Table 1 presents the count of each of the 10 named entity types labeled by the annotators in the training, development and test sets creat"
W16-3919,W16-3923,0,0.0295917,"Missing"
W16-3919,W16-3920,0,0.153678,"ML – – – – GloVe Multiple – – CRFsuite LSTM LSTM CRF L2S LSTM-CNN LSTM CRF Table 3: Features and machine learning approach taken by each team. 2.3 System Descriptions This section briefly describes the approach taken by each team. Overall we noticed different trends between the types of systems submitted this year and last year. The most notable change is the use of LSTM-based systems. Four of the seven submissions were LSTM-based as opposed to zero submissions last year. The previous year Conditional Random Fields was the most popular ML technique for extracting named entities. CambridgeLTL (Limsopatham and Collier, 2016) The system uses bidirectional LSTM to automatically induce and leverage orthographic features for performing Named Entity Recognition in Twitter messages. akora (Kurt Junshean Espinosa and Ananiadou, 2016) This system uses bidirectional LSTM networks and exploits weakly annotated data to bootstrap sparse entity types. NTNU (Sikdar and Gamb¨ack, 2016) This system is based on classification using Conditional Random Fields, a supervised machine learning approach. The system utilizes a large feature set developed specifically for the task, with eight types of features based on actual characters a"
W16-3919,Q14-1019,0,0.00717094,"and exploits weakly annotated data to bootstrap sparse entity types. NTNU (Sikdar and Gamb¨ack, 2016) This system is based on classification using Conditional Random Fields, a supervised machine learning approach. The system utilizes a large feature set developed specifically for the task, with eight types of features based on actual characters and token internal data, five types of features built through context and chunk information, and five types of features based on lexicon-type information such as stop word matching, word frequencies, and entries in the shared task lexicon and Babelfy (Moro et al., 2014). Talos (Ioannis Partalas and Kalitvianski, 2016) The system uses three types of features: lexical and morpho-syntactic features, contextual enrichment features using Linked Open Data, and features based on distributed representation of words. The system also exploits words clustering to enhance performance. The learning algorithm was solved by using Learning to search (L2S) that resembles a reinforcement learning algorithm. DeepNNNER (Dugas and Nichols, 2016) The system uses a bidirectional LSTM-CNN model with word embedding trained on a large scale Web corpus. Additionally, the system uses a"
W16-3919,I11-1108,0,0.0110506,"presents the results of the Twitter Named Entity Recognition shared task associated with W-NUT 2016: a named entity tagging task with 10 teams participating. We outline the shared task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task. 1 Introduction The increasing flood of user-generated text on social media has created an enormous opportunity for new data analysis techniques to extract and aggregate information about breaking news (Ritter et al., 2012), disease outbreaks (Paul and Dredze, 2011), natural disasters (Neubig et al., 2011), cyber-attacks (Ritter et al., 2015) and more. Named entity recognition is an important first step in most information extraction pipelines. However, performance of state-of-the-art NER on social media still lags behind well edited text genres. This motivates the need for continued research, in addition to new datasets and tools adapted to this noisy text genre. In this paper, we present the development and evaluation of a shared task on named entity recognition in Twitter, which was held at the 2nd Workshop on Noisy User-generated Text (W-NUT 2016) and attracted 10 participating teams, 7 of"
W16-3919,W16-3926,0,0.0197598,"n segmenting and categorizing entities into 10 types. ASU (Michel Naim Gerguis and Gerguis, 2016) The system shows an experimental study on using word embeddings, Brown clusters, part-of-speech tags, shape features, gazetteers, and local context to create a feature representation along with a set of experiments for the network design. A Wikipedia-based classifier framework was adopted to extract lists of fine-grained entities out of few input examples to be used as gazetteers. The model uses the LSTM algorithm to learn a NE classifier from the feature representation. UQAM-NTL (Ngoc Tan LE and Sadat, 2016) The system is based on supervised machine learning and trained with a sequential labeling algorithm, using Conditional Random Fields to learn a classifier for Twitter NE extraction. The model uses 6 different categories of features including (1) orthographic, (2) lexical and (3) syntactic features as well as (4) part-of-speech tags, (5) polysemy count and (6) longest n-gram length in order to create a feature representation. 3 Summary In this paper, we presented a shared task for Named Entity Recognition in Twitter data. We detailed the task setup and datasets used in the respective shared ta"
W16-3919,D11-1141,1,0.92682,"with general domain data. Both the time period and topic selection of the evaluation data were not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. Teams had 7 days to submit their results on the test data, which were subsequently scored and gold annotations were released to participants. Evaluating the NER systems on these domains specific Twitter data provides information about possible system weakness. 2.1 Training and Development Data The training and development data for our task was taken from prior work on Twitter NER (Ritter et al., 2011; Baldwin et al., 2015), which distinguishes 10 different named entity types (see Table 1 for the set of types). The training data was created from the union of the training and development data from the 2015 task (Baldwin et al., 2015). The data was split into 2,394 annotated tweets for training and 1,000 as a development set. We also provided an additional 425 annotated tweets from the 2015 development data set (Baldwin et al., 2015). 2.2 Test Data Annotation The data set we created for testing is new for this shared task. We collected general Twitter data and domain specific Twitter data. I"
W16-3919,W16-3922,0,0.121077,"Missing"
W16-3919,W03-0419,0,0.420022,"Missing"
W17-4901,K15-1011,0,0.0225,"ul” while male users use “superb”) are much more subtle than topical preferences (e.g. using word “husband” is a strong indicator of female user). Our recent work (Preot¸iuc-Pietro et al., 2016) isolated stylistic differences from topic bias by using paraphrase pairs and clusters, and showed their predictive power in user profiling and potential for future work. We also found crowdsourcing workers are surprisingly good at perceiving gender from lexical choices when aggregating their judgments – an infamous phenomenon of so-called The Wisdom of Crowds (Surowiecki, 2005). Beyond lexical choice, Johannsen et al. (2015) further showed demographic differences in syntactic variances using multilingual data of online customer reviews and universal dependency parsing. Colloquial and Internet As social media started booming, especially after Twitter released the streaming API for free in 2010 that provides real-time tweets as posted, there is a huge explosion on social media research. Multiple workshops are dedicated to this special type of text including the Workshop on Noisy User-generated Text (WNUT) and Workshop on Making Sense of Microposts (#microposts) that hold annual shared tasks. Before that, most unedi"
W17-4901,W15-4302,0,0.0239439,"et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are also commonly studied for social science and strongly motivated by commercial usages of profiling users and personalized services. Leveraging user demographic factors also shows benefits on improving natural language processing applications such as sentiment analysis (Volkova et al., 2013) and sarcasm detection (Bamman and Smith, 2015). One particularly interesting challenge is how to handle the situation that stylistic differences (e.g. female users more likely use “wonderful” while male users use “superb”)"
W17-4901,D13-1145,0,0.0128497,"related to, sometimes used interchangeably with, though different from, abstractive summarization, headline generation, sentence fusion. 2 tial paraphrase of “Aaaaaaaaand stephen curry is on fire” and “What a incredible performance from Stephen Curry”. Semantic equivalences, as formal as “fetuses” and “fetal tissue” (Lan et al., 2017) or as informal as “gets the boot from” and “has been sacked by” (Xu et al., 2014; Xu, 2014), can also be learned automatically from Twitter data. Not to mention that there are also studies that focus on multiword expressions (Schneider and Smith, 2015), idioms (Muzny and Zettlemoyer, 2013), and slang. (Plank, 2018). The most apparent case is outof-vocabulary (OOV) words (van der Wees et al., 2015; Seraj et al., 2015), especially new emerging named entities and newly coined words (e.g. “selfie”, “Brexiteers”). This problem will become more pressing and more feasible to study as more and more time-sensitive online text data is accumulating. Learning up-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data"
W17-4901,W11-1611,0,0.0767684,"Missing"
W17-4901,D14-1108,0,0.0251067,"type of text including the Workshop on Noisy User-generated Text (WNUT) and Workshop on Making Sense of Microposts (#microposts) that hold annual shared tasks. Before that, most unedited text data (vs. well-edited such as news) is from web forums and blogs, while short message service (SMS) and email data are limited to rather small amounts due to privacy reasons (Baldwin et al., 2013). Interesting research falls into two camps: normalize lexical variants to standard form (Han and Baldwin, 2011; Xu et al., 2013) or develop domain adapted NLP systems (Ritter et al., 2011; Gimpel et al., 2011; Kong et al., 2014; Tabassum et al., 2016). The iconic opinion paper What to do about bad language on the Internet by Jacob Eisenstein (2013) highlighted this divide. There is a third point we have often missed. Besides the noisy hard-to-understand Internet language, many users also use rather standard language on social networks, formal or colloquial. Don’t forget that all the traditional news agencies also have Twitter accounts (Hu et al., 2013). Can we make the connections between the formal and colloquial languages as they are heavily mixed on social media? I think the answer is yes, and the twin research t"
W17-4901,L16-1491,0,0.0213106,"of things in an indistinguishable way. Moreover, there is also a huge variance from one man to another, one woman to another. The styles are often fused together in the data and not easy to separate out or make black-and-white judgements on. This also leads to challenges in data annotation or data collection, comparing to other NLP tasks (e.g. question answering). Throughout the rest of this paper, we shall see many creative solutions, interesting work, and promising potential. 1 Lexical simplification as a subtask can utilize or bypass ˇ the need of parallel data (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016). 1 Proceedings of the Workshop on Stylistic Variation, pages 1–9 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics unacceptable when deletion is involved, as it unfairly biases towards deletions over paraphrasing. There has been some progress on creating automatic evaluation metrics (Xu et al., 2016) and exploring new human evaluation methodologies (Xu et al., 2016; Nisioi et al., 2017; Siddharthan and Mandya, 2014). We are going to need more data, clever ideas and careful evaluation designs. For the record, everythi"
W17-4901,P16-2024,0,0.0133505,"ishable way. Moreover, there is also a huge variance from one man to another, one woman to another. The styles are often fused together in the data and not easy to separate out or make black-and-white judgements on. This also leads to challenges in data annotation or data collection, comparing to other NLP tasks (e.g. question answering). Throughout the rest of this paper, we shall see many creative solutions, interesting work, and promising potential. 1 Lexical simplification as a subtask can utilize or bypass ˇ the need of parallel data (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016). 1 Proceedings of the Workshop on Stylistic Variation, pages 1–9 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics unacceptable when deletion is involved, as it unfairly biases towards deletions over paraphrasing. There has been some progress on creating automatic evaluation metrics (Xu et al., 2016) and exploring new human evaluation methodologies (Xu et al., 2016; Nisioi et al., 2017; Siddharthan and Mandya, 2014). We are going to need more data, clever ideas and careful evaluation designs. For the record, everything about sentence simplification is"
W17-4901,W14-1210,0,0.013087,"h Wikipedia became available. It is worth noting that the Simple Wikipedia data has some issues on the quality and degree of simplicity (Xu et al., 2015b). The shortage of high quality data is becoming gradually alleviated as the Newsela corpus (Xu et al., 2015b) of professionally edited 1000+ articles is released, and as more and more attention and appreciation are given by the research community to data construction (Brunato et al., 2016; Hwang et al., 2015). Multiple studies have shown crowcourcing workers can produce high quality simplifications (Xu et al., 2016; Amancio and Specia, 2014; Pellow and Eskenazi, 2014), though it is costly to scale up. Data will remain a central problem1 as the data-hungry neural generation models (Nisioi et al., 2017) are a promising direction for future work. Besides data, another severe problem is evaluation. In fact, one common human evaluation that uses a five point Likert scale on grammaticality, meaning and simplicity should be considered Top Three Problems The top three problems for studying language styles are data, data and data. More specifically, they are data shortage, data fusion, and data annotation problems. The data shortage problem has been improving, whic"
W17-4901,D17-1126,1,0.842335,"Missing"
W17-4901,D16-1262,0,0.0315648,"Missing"
W17-4901,W15-2913,0,0.0219713,"models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are also commonly studied for social science and strongly motivated by commercial usages of profiling users and personalized services. Leveraging user demographic factors also shows benefits on improving natural language processing applications such as sentiment analysis (Volkova et al., 2013) and sarcasm detection (Bamman and Smith, 2015). One particularly interesting challenge is how to handle the situation that stylistic differences (e.g. female users more likely use “wonderful” while male users use “superb”) are much more subtle than topical preferences (e.g. using word “husband” is a str"
W17-4901,P16-1094,0,0.0484197,"speare style (Xu et al., 2012b), we found gendered language style much harder to impose. It is possibly that because we have not found the right data for evaluation, for instance, it is hard to expect a randomly drawn sentence to be possible to take on a feminine or masculine style. It could also be the case that it is easier for finer-grained language style to show distinctions. One evident example is author recognition based on an individual’s frequent word choices (Clark and Hannon, 2007). Another example is persona-based dialog system that not only captures background knowledge of a user (Li et al., 2016) but also speaking style (Mizukami et al., 2015). It is not a coincident that the later work (Mizukami et al., 2015) is on spoken Japanese, which exhibits extensive gender differences as well as honorifics (not as much in written Japanese). 2.6 2.7 Polite and Abusive Another angle that has been looked at is the politeness conveyed in language. Unlike many other styles that come in close pairs (e.g. formal vs. informal, feminine vs. masculine), the polite language does not necessarily have an impolite counterpart. In addition, politeness is expressed more through function words. For example, sh"
W17-4901,D15-1176,0,0.0134628,"e also studies that focus on multiword expressions (Schneider and Smith, 2015), idioms (Muzny and Zettlemoyer, 2013), and slang. (Plank, 2018). The most apparent case is outof-vocabulary (OOV) words (van der Wees et al., 2015; Seraj et al., 2015), especially new emerging named entities and newly coined words (e.g. “selfie”, “Brexiteers”). This problem will become more pressing and more feasible to study as more and more time-sensitive online text data is accumulating. Learning up-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are"
W17-4901,P16-1030,0,0.027776,"a. Journal of Sociolinguistics 18(2):135– 160. David Bamman and Noah A. Smith. 2015. Contextualized sarcasm detection on Twitter. In ICWSM. Conclusion At this point of the development, natural language processing research ranges a wide variety of genre, domain, register or type of data. I think the term style is an all-in-one umbrella concept to bring researchers and scattered attentions in various NLP subareas to a common place. There are certainly many nuances in language styles besides those mentioned in this paper. For example, connotation (e.g. “childlike” vs. “childish” vs. “youthful”) (Rashkin et al., 2016; Carpuat, 2015) and geographical lexical variations from regional (e.g. “sode” vs. “coke” vs. “pop”) to cross-country (e.g. Austrilian vs. American English) (Eisenstein et al., 2010; Garimella et al., 2016; Han et al., 2016). There are also certainly many other relevant works besides those mentioned in this paper. Last but not least, we would like to point out Dan Jurafsky’s recent book The Language of Food (2014) and one more paper: Do Linguistic Style and Readability of Scientific Abstracts Affect their Virality? (Guerini et al., 2012). Taylor Berg-Kirkpatrick and Dan Klein. 2014. Improved"
W17-4901,P13-1162,0,0.0787103,"Missing"
W17-4901,D16-1183,0,0.0173353,"simplification, previously, sentence compression also use human evaluation with Likert scale on grammaticality and meaning. However, it is shown to be problematic without controlling for compression ratio (Napoles et al., 2011). Now sentence compression systems are mostly compared at the same compression ratio. It is also worth noting that neural compression is similarly lacking in largescale parallel data (Toutanova et al., 2016) and currently relies on news headline data which results in headline-like outputs (Filippova et al., 2015; Rush et al., 2015). 2.2 network models (Lee et al., 2016; Misra and Artzi, 2016). The 1st Workshop on Language Grounding for Robotics (RoboNLP) will be held at ACL 2017. We shall expect research on instructional language become more and more fruitful in the near future. 2.3 Historical and Evolving The rise of digital humanities certainly helps to provide more digitized materials for leaning techniques. Historical documents are proven fun (in the other word, hard) to work with. Garrette and Alpert-Abrams (2016) used the following example to present the challenges of having multiple unknown fonts and inking on a single page of a book in the Primeros Libros corpus: A series"
W17-4901,P09-1026,0,0.0732761,"Missing"
W17-4901,C16-1030,0,0.0196927,"focus on multiword expressions (Schneider and Smith, 2015), idioms (Muzny and Zettlemoyer, 2013), and slang. (Plank, 2018). The most apparent case is outof-vocabulary (OOV) words (van der Wees et al., 2015; Seraj et al., 2015), especially new emerging named entities and newly coined words (e.g. “selfie”, “Brexiteers”). This problem will become more pressing and more feasible to study as more and more time-sensitive online text data is accumulating. Learning up-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are also commonly stud"
W17-4901,D11-1141,0,0.0708575,"le workshops are dedicated to this special type of text including the Workshop on Noisy User-generated Text (WNUT) and Workshop on Making Sense of Microposts (#microposts) that hold annual shared tasks. Before that, most unedited text data (vs. well-edited such as news) is from web forums and blogs, while short message service (SMS) and email data are limited to rather small amounts due to privacy reasons (Baldwin et al., 2013). Interesting research falls into two camps: normalize lexical variants to standard form (Han and Baldwin, 2011; Xu et al., 2013) or develop domain adapted NLP systems (Ritter et al., 2011; Gimpel et al., 2011; Kong et al., 2014; Tabassum et al., 2016). The iconic opinion paper What to do about bad language on the Internet by Jacob Eisenstein (2013) highlighted this divide. There is a third point we have often missed. Besides the noisy hard-to-understand Internet language, many users also use rather standard language on social networks, formal or colloquial. Don’t forget that all the traditional news agencies also have Twitter accounts (Hu et al., 2013). Can we make the connections between the formal and colloquial languages as they are heavily mixed on social media? I think th"
W17-4901,P16-2052,0,0.0277885,"acter-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are also commonly studied for social science and strongly motivated by commercial usages of profiling users and personalized services. Leveraging user demographic factors also shows benefits on improving natural language processing applications such as sentiment analysis (Volkova et al., 2013) and sarcasm detection (Bamman and Smith, 2015). One particularly interesting challenge is how to handle the situation that stylistic differences (e.g. female users more likely use “wonderful” while male users use “superb”) are much more subtle than topical preferences (e.g. using"
W17-4901,D16-1030,1,0.895942,"Missing"
W17-4901,D15-1044,0,0.0199129,"the interactions between deletion and paraphrasing. Like simplification, previously, sentence compression also use human evaluation with Likert scale on grammaticality and meaning. However, it is shown to be problematic without controlling for compression ratio (Napoles et al., 2011). Now sentence compression systems are mostly compared at the same compression ratio. It is also worth noting that neural compression is similarly lacking in largescale parallel data (Toutanova et al., 2016) and currently relies on news headline data which results in headline-like outputs (Filippova et al., 2015; Rush et al., 2015). 2.2 network models (Lee et al., 2016; Misra and Artzi, 2016). The 1st Workshop on Language Grounding for Robotics (RoboNLP) will be held at ACL 2017. We shall expect research on instructional language become more and more fruitful in the near future. 2.3 Historical and Evolving The rise of digital humanities certainly helps to provide more digitized materials for leaning techniques. Historical documents are proven fun (in the other word, hard) to work with. Garrette and Alpert-Abrams (2016) used the following example to present the challenges of having multiple unknown fonts and inking on a"
W17-4901,D14-1121,0,0.0240547,"-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are also commonly studied for social science and strongly motivated by commercial usages of profiling users and personalized services. Leveraging user demographic factors also shows benefits on improving natural language processing applications such as sentiment analysis (Volkova et al., 2013) and sarcasm detection (Bamman and Smith, 2015). One particularly interesting challenge is how to handle the situation that stylistic differences (e.g. female users more likely use “wonderful”"
W17-4901,D16-1033,0,0.0158573,"uation designs. For the record, everything about sentence simplification is much harder than sentence compression2 primarily due to the interactions between deletion and paraphrasing. Like simplification, previously, sentence compression also use human evaluation with Likert scale on grammaticality and meaning. However, it is shown to be problematic without controlling for compression ratio (Napoles et al., 2011). Now sentence compression systems are mostly compared at the same compression ratio. It is also worth noting that neural compression is similarly lacking in largescale parallel data (Toutanova et al., 2016) and currently relies on news headline data which results in headline-like outputs (Filippova et al., 2015; Rush et al., 2015). 2.2 network models (Lee et al., 2016; Misra and Artzi, 2016). The 1st Workshop on Language Grounding for Robotics (RoboNLP) will be held at ACL 2017. We shall expect research on instructional language become more and more fruitful in the near future. 2.3 Historical and Evolving The rise of digital humanities certainly helps to provide more digitized materials for leaning techniques. Historical documents are proven fun (in the other word, hard) to work with. Garrette a"
W17-4901,N15-1177,0,0.0158045,"2013) and neural 2 which is closely related to, sometimes used interchangeably with, though different from, abstractive summarization, headline generation, sentence fusion. 2 tial paraphrase of “Aaaaaaaaand stephen curry is on fire” and “What a incredible performance from Stephen Curry”. Semantic equivalences, as formal as “fetuses” and “fetal tissue” (Lan et al., 2017) or as informal as “gets the boot from” and “has been sacked by” (Xu et al., 2014; Xu, 2014), can also be learned automatically from Twitter data. Not to mention that there are also studies that focus on multiword expressions (Schneider and Smith, 2015), idioms (Muzny and Zettlemoyer, 2013), and slang. (Plank, 2018). The most apparent case is outof-vocabulary (OOV) words (van der Wees et al., 2015; Seraj et al., 2015), especially new emerging named entities and newly coined words (e.g. “selfie”, “Brexiteers”). This problem will become more pressing and more feasible to study as more and more time-sensitive online text data is accumulating. Learning up-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausib"
W17-4901,W15-4304,0,0.0646792,"Missing"
W17-4901,L16-1258,0,0.0220601,"udy as more and more time-sensitive online text data is accumulating. Learning up-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to learn about the users authoring the texts. Much interesting research on gender difference3 in language styles appeared in the past few years. Besides gender (Verhoeven et al., 2016; Bamman et al., 2014), other user attributes such as age (Sap et al., 2014), race (Jørgensen et al., 2015) and personality (Schwartz et al., 2013; Ruan et al., 2016; Plank and Hovy, 2015) are also commonly studied for social science and strongly motivated by commercial usages of profiling users and personalized services. Leveraging user demographic factors also shows benefits on improving natural language processing applications such as sentiment analysis (Volkova et al., 2013) and sarcasm detection (Bamman and Smith, 2015). One particularly interesting challenge is how to handle the situatio"
W17-4901,D15-1163,0,0.0158587,"tial paraphrase of “Aaaaaaaaand stephen curry is on fire” and “What a incredible performance from Stephen Curry”. Semantic equivalences, as formal as “fetuses” and “fetal tissue” (Lan et al., 2017) or as informal as “gets the boot from” and “has been sacked by” (Xu et al., 2014; Xu, 2014), can also be learned automatically from Twitter data. Not to mention that there are also studies that focus on multiword expressions (Schneider and Smith, 2015), idioms (Muzny and Zettlemoyer, 2013), and slang. (Plank, 2018). The most apparent case is outof-vocabulary (OOV) words (van der Wees et al., 2015; Seraj et al., 2015), especially new emerging named entities and newly coined words (e.g. “selfie”, “Brexiteers”). This problem will become more pressing and more feasible to study as more and more time-sensitive online text data is accumulating. Learning up-to-date paraphrases (Lan et al., 2017), vector semantics (Cherry and Guo, 2015) and character-based neural models (Ling et al., 2015; Rei et al., 2016) from online data streams could be plausible solutions that connect unseen data with known expressions. 2.4 2.5 Gendered and Personalized One unique and exciting opportunity offered by social media data is to l"
W17-4901,W12-3204,0,0.022157,"ave Twitter accounts (Hu et al., 2013). Can we make the connections between the formal and colloquial languages as they are heavily mixed on social media? I think the answer is yes, and the twin research topics of paraphrasing and semantic similarity could be part of the solution as many language styles are heavily mixed on social media. For example, in the SemEval shared task PIT-2015 corpus (Xu et al., 2015a), the figurative meaning of the phrase “on fire” is captured by the senten3 Although unrelated to linguistic styles, the readers may find He Said, She Said: Gender in the ACL Anthology (Vogel and Jurafsky, 2012), a paper on gender-based statistics of NLP researchers, interesting. 3 dia Frames Corpus (Card et al., 2015)4 presents another encouraging opportunity to study framing. The legal domain, such as supreme court documents, is another common place for arguments (Sim et al., 2015) and would possibly be used for studying linguistic styles. Another subsequent challenge is how to transfer the subtle style differences into natural language generation and dialog systems. While we were able to transform contemporary texts into Shakespeare style (Xu et al., 2012b), we found gendered language style much h"
W17-4901,E14-1076,0,0.0136444,"simplification as a subtask can utilize or bypass ˇ the need of parallel data (Glavaˇs and Stajner, 2015; Paetzold and Specia, 2016; Pavlick and Callison-Burch, 2016). 1 Proceedings of the Workshop on Stylistic Variation, pages 1–9 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics unacceptable when deletion is involved, as it unfairly biases towards deletions over paraphrasing. There has been some progress on creating automatic evaluation metrics (Xu et al., 2016) and exploring new human evaluation methodologies (Xu et al., 2016; Nisioi et al., 2017; Siddharthan and Mandya, 2014). We are going to need more data, clever ideas and careful evaluation designs. For the record, everything about sentence simplification is much harder than sentence compression2 primarily due to the interactions between deletion and paraphrasing. Like simplification, previously, sentence compression also use human evaluation with Likert scale on grammaticality and meaning. However, it is shown to be problematic without controlling for compression ratio (Napoles et al., 2011). Now sentence compression systems are mostly compared at the same compression ratio. It is also worth noting that neural"
W17-4901,D13-1187,0,0.0516354,"Missing"
W17-4901,N12-1084,0,0.19819,"gh unsupervised learning. Unsupervised domain adaptation to historic text was also attempted by Yang and Eisenstein (2015) using feature embedding on the part-of-speech tagging task. Shakespeare plays in contrast are perfect for investigating a consistent writing style from a single author. Even with a relatively small amount of parallel training data, it is possible to learn paraphrase models which capture stylistic phenomena and can transform the line in the Star Wars “If you will not be turned, you will be destroyed!” to Shakespearean style “If you will not be turn’d, you will be undone!” (Xu et al., 2012b; Xu, 2014). One can image such stylistic paraphrasing, as it continues to improve, would possibly help preserve privacy and anonymity (Brennan et al., 2012). This is one thing about research on language styles, it often involves a sense of social justice and for social good (e.g. simplification for children, robotics for repetitive wet lab experiments). Being able to handle evolving language is crucial in natural language processing applications. As the most high-performance systems often utilize fully supervised or weakly supervised learning, the time elapsed from training data to new test"
W17-4901,S15-2001,1,0.918634,"e than seven language styles as there are more than seven wonders in the world. 2.1 Simple and Short Text simplification is one of the earliest topics in computational linguistics that directly deals with language styles, rewriting regular texts into simpler versions for people with limited reading capabilities. The major transition from rule-based to machine learning approach for automatic sentence simplification did not happen until 2010 after Simple English Wikipedia became available. It is worth noting that the Simple Wikipedia data has some issues on the quality and degree of simplicity (Xu et al., 2015b). The shortage of high quality data is becoming gradually alleviated as the Newsela corpus (Xu et al., 2015b) of professionally edited 1000+ articles is released, and as more and more attention and appreciation are given by the research community to data construction (Brunato et al., 2016; Hwang et al., 2015). Multiple studies have shown crowcourcing workers can produce high quality simplifications (Xu et al., 2016; Amancio and Specia, 2014; Pellow and Eskenazi, 2014), though it is costly to scale up. Data will remain a central problem1 as the data-hungry neural generation models (Nisioi et"
W17-4901,Q15-1021,1,0.91765,"e than seven language styles as there are more than seven wonders in the world. 2.1 Simple and Short Text simplification is one of the earliest topics in computational linguistics that directly deals with language styles, rewriting regular texts into simpler versions for people with limited reading capabilities. The major transition from rule-based to machine learning approach for automatic sentence simplification did not happen until 2010 after Simple English Wikipedia became available. It is worth noting that the Simple Wikipedia data has some issues on the quality and degree of simplicity (Xu et al., 2015b). The shortage of high quality data is becoming gradually alleviated as the Newsela corpus (Xu et al., 2015b) of professionally edited 1000+ articles is released, and as more and more attention and appreciation are given by the research community to data construction (Brunato et al., 2016; Hwang et al., 2015). Multiple studies have shown crowcourcing workers can produce high quality simplifications (Xu et al., 2016; Amancio and Specia, 2014; Pellow and Eskenazi, 2014), though it is costly to scale up. Data will remain a central problem1 as the data-hungry neural generation models (Nisioi et"
W17-4901,Q16-1029,1,0.914008,"d not happen until 2010 after Simple English Wikipedia became available. It is worth noting that the Simple Wikipedia data has some issues on the quality and degree of simplicity (Xu et al., 2015b). The shortage of high quality data is becoming gradually alleviated as the Newsela corpus (Xu et al., 2015b) of professionally edited 1000+ articles is released, and as more and more attention and appreciation are given by the research community to data construction (Brunato et al., 2016; Hwang et al., 2015). Multiple studies have shown crowcourcing workers can produce high quality simplifications (Xu et al., 2016; Amancio and Specia, 2014; Pellow and Eskenazi, 2014), though it is costly to scale up. Data will remain a central problem1 as the data-hungry neural generation models (Nisioi et al., 2017) are a promising direction for future work. Besides data, another severe problem is evaluation. In fact, one common human evaluation that uses a five point Likert scale on grammaticality, meaning and simplicity should be considered Top Three Problems The top three problems for studying language styles are data, data and data. More specifically, they are data shortage, data fusion, and data annotation proble"
W17-4901,C12-1177,1,0.855199,"gh unsupervised learning. Unsupervised domain adaptation to historic text was also attempted by Yang and Eisenstein (2015) using feature embedding on the part-of-speech tagging task. Shakespeare plays in contrast are perfect for investigating a consistent writing style from a single author. Even with a relatively small amount of parallel training data, it is possible to learn paraphrase models which capture stylistic phenomena and can transform the line in the Star Wars “If you will not be turned, you will be destroyed!” to Shakespearean style “If you will not be turn’d, you will be undone!” (Xu et al., 2012b; Xu, 2014). One can image such stylistic paraphrasing, as it continues to improve, would possibly help preserve privacy and anonymity (Brennan et al., 2012). This is one thing about research on language styles, it often involves a sense of social justice and for social good (e.g. simplification for children, robotics for repetitive wet lab experiments). Being able to handle evolving language is crucial in natural language processing applications. As the most high-performance systems often utilize fully supervised or weakly supervised learning, the time elapsed from training data to new test"
W17-4901,W13-2515,1,0.823647,"ere is a huge explosion on social media research. Multiple workshops are dedicated to this special type of text including the Workshop on Noisy User-generated Text (WNUT) and Workshop on Making Sense of Microposts (#microposts) that hold annual shared tasks. Before that, most unedited text data (vs. well-edited such as news) is from web forums and blogs, while short message service (SMS) and email data are limited to rather small amounts due to privacy reasons (Baldwin et al., 2013). Interesting research falls into two camps: normalize lexical variants to standard form (Han and Baldwin, 2011; Xu et al., 2013) or develop domain adapted NLP systems (Ritter et al., 2011; Gimpel et al., 2011; Kong et al., 2014; Tabassum et al., 2016). The iconic opinion paper What to do about bad language on the Internet by Jacob Eisenstein (2013) highlighted this divide. There is a third point we have often missed. Besides the noisy hard-to-understand Internet language, many users also use rather standard language on social networks, formal or colloquial. Don’t forget that all the traditional news agencies also have Twitter accounts (Hu et al., 2013). Can we make the connections between the formal and colloquial lang"
W17-4901,N15-1069,0,0.0134095,"word, hard) to work with. Garrette and Alpert-Abrams (2016) used the following example to present the challenges of having multiple unknown fonts and inking on a single page of a book in the Primeros Libros corpus: A series of work (Berg-Kirkpatrick et al., 2013; Berg-Kirkpatrick and Klein, 2014; Garrette et al., 2015) have been conducted on this and other corpora to develop historical document optical character recognition (OCR) better handle fonts, offsets, etc, together with language models through unsupervised learning. Unsupervised domain adaptation to historic text was also attempted by Yang and Eisenstein (2015) using feature embedding on the part-of-speech tagging task. Shakespeare plays in contrast are perfect for investigating a consistent writing style from a single author. Even with a relatively small amount of parallel training data, it is possible to learn paraphrase models which capture stylistic phenomena and can transform the line in the Star Wars “If you will not be turned, you will be destroyed!” to Shakespearean style “If you will not be turn’d, you will be undone!” (Xu et al., 2012b; Xu, 2014). One can image such stylistic paraphrasing, as it continues to improve, would possibly help pr"
W17-4901,D07-1071,0,0.0129513,"eakly supervised learning, the time elapsed from training data to new test data will cause performance deteriorating Instructional and Robotic Despite the fact that instructional language is important in our everyday lives, there have been relatively limited efforts to design automated algorithms that link language to action in real world applications. Largely because of the limited availability of annotated datasets which are much-needed for training and evaluating machine learning models, existing works are primarily on cooking recipes (Tasse and Smith, 2008), airline booking conversations (Zettlemoyer and Collins, 2007), software help documents (Branavan et al., 2009) and robot navigation commands (Chen and Mooney, 2011). In particular, cooking recipe has sprouted a rich line of research as a proxy to robotic instructions (Bollini et al., 2013; Jermsurawong and Habash, 2015; Kiddon et al., 2015). Recent efforts aim to study natural language instructions for biology lab experiments (Kulkarni et al., 2017). Two closely relevant research areas, semantic parsing and dialog, have also both made major advances in recent years to utilize largescale data via weak supervision (Cai and Yates, 2013; Artzi and Zettlemoy"
W17-4901,N13-1037,0,\N,Missing
W17-4901,P09-1010,0,\N,Missing
W17-4901,P13-1021,0,\N,Missing
W17-4901,D10-1124,0,\N,Missing
W17-4901,P11-1038,0,\N,Missing
W17-4901,Q14-1034,1,\N,Missing
W17-4901,P13-1025,0,\N,Missing
W17-4901,P11-2008,0,\N,Missing
W17-4901,P13-1042,0,\N,Missing
W17-4901,Q13-1005,0,\N,Missing
W17-4901,P15-2072,0,\N,Missing
W17-4901,D15-1042,0,\N,Missing
W17-4901,W15-2903,0,\N,Missing
W17-4901,D15-1090,0,\N,Missing
W17-4901,D15-1114,0,\N,Missing
W17-4901,P14-2020,0,\N,Missing
W17-4901,N15-1172,0,\N,Missing
W17-4901,N15-1109,0,\N,Missing
W17-4901,N15-1075,0,\N,Missing
W17-4901,N15-1022,0,\N,Missing
W17-4901,N16-1055,0,\N,Missing
W17-4901,C16-1065,0,\N,Missing
W17-4901,I13-1041,0,\N,Missing
Y07-1031,P04-1056,0,0.298182,"n natural language (including the NER task). How to utilize the non-local dependencies effectively is a key issue in NER task. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency. There are two methods to use non-local information. One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features. However, in the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005). Furthermore, high computational cost is spent for approximate inference. In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, Krishnan and Manning (2006) propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorpora"
Y07-1031,P05-1045,0,0.0467468,"ask. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency. There are two methods to use non-local information. One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local features. However, in the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al., 2005). Furthermore, high computational cost is spent for approximate inference. In order to establish the long dependencies easily and overcome the disadvantage of the approximate inference, Krishnan and Manning (2006) propose a two-stage approach using Conditional Random Fields (CRFs) with extract inference. They represent the non-locality with non-local features, and extract the nonlocal features from the output of the first stage CRF using local context alone; then they incorporate the non-local features into the second CRF. But the features in this approach are ∗ Copyright 2007 by Xinnian Mao,"
Y07-1031,P06-2060,0,0.104825,"the non-local features into the second CRF. But the features in this approach are ∗ Copyright 2007 by Xinnian Mao, Wei Xu, Yuan Dong, Saike He, and Haila Wang 303 only used to improve label consistency. To our best knowledge, up to now, non-local information has not been explored to improve NER recall in previous researches; on the other hand, NER is always impaired by its lower recall due to the imbalanced distribution where the NONE class dominates the entity classes. Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class (Kambhatla, 2006). In this paper, we employ non-local information to recall the missed entities. Similar to Krishnan and Manning (2006), we also encode non-local information with features and apply the simple two-stage architecture. Different from their work for improve label consistency, their features are activated on the recognized entities coming from the first CRF, the non-local features we design are used to recall more missed entities which are seen in the training data or unseen entities but some of their occurrences being recognized correctly in the first stage, our features are fired on the raw token"
Y07-1031,P06-1141,0,0.769301,"Non-local Feature, Conditional Random Field 1. Introduction Named entity recognition (NER) is a subtask of information extraction that seeks to locate and classify predefined entities, such as names of persons, locations, organizations, etc. in unstructured texts. It is the fundamental step to many natural language processing applications, like Information Extraction (IE), Information Retrieval (IR) and Question Answering (QA). Most empirical approaches currently employed in NER task make decision only on local context for extract inference, which is based on the data independent assumption (Krishnan and Manning, 2006). But often this assumption does not hold because non-local dependencies are prevalent in natural language (including the NER task). How to utilize the non-local dependencies effectively is a key issue in NER task. Unfortunately, few researches have been devoted to this issue, existing works mainly focus on using the non-local information for further improving NER label consistency. There are two methods to use non-local information. One is to add additional edges to graphical model structure to represent the distant dependencies and the other is to encode the non-locality with non-local featu"
Y07-1031,W06-0115,0,0.0682137,"Missing"
Y07-1031,W02-2018,0,0.0183184,"linear chain. Following Lafferty et al. (2001), the conditional probability of the state sequence (s1, s2…sn) given the input sequence (o1, o2…on) is computed as follows: PΛ (s |o) = T K ∏ exp( ∑ ∑ λk fk (st −1, st , o, t )) t =1 k=1 Zo c∈C(s,o) 1 (1) Where fk is an arbitrary feature function; and λk is the weight for the feature function; it can be optimized through iterative algorithms like GIS (Darroch and Ratcliff, 1972) and IIS (Della Pietra et al., 1997). However recent research y been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient (Byrd et al., 1994; Malouf, 2002; Sha and Pereira, 2003). 2.2. Local features 304 The fist stage CRF labels for token directly depends on the labels corresponding the previous and next token, namely C-2, C-1, C0, C-1, C2, C-2C-1, C-1C0, C0C-1, C1C2, and C-1C1, where C0 is the current character, C1 the next character, C2 the second character after C0, C-1 the character preceding C0, and C-2 the second character before C0. In addition, the first CRF used the tag bigram feature. Although these local features are simple, they give us state-of-the-art baseline using local information alone as described in Section 4. 2.3. Low reca"
Y07-1031,N03-1028,0,0.0347507,"the effectiveness of our approach of exploiting non-local features, we need to establish a baseline with state-of-the-art performance using local context alone. Similar to (Krishnan and Manning, 2006), we employ two-stage architecture under conditional random fields (CRFs) framework. In the first stage, we build the baseline with local features only, and then we build the second NER system with non-local features. We will introduce them step by step. 2.1. Conditional random fields We regard the NER task as a sequence labeling problem and apply Conditional Random Fields (Lafferty et al., 2001; Sha and Pereira, 2003) since it represents the state of the art in sequence modeling and has also been very effective at NER task. It is undirected graph established on G = (V, E), where V is the set of random variables Y = {Yi|1≤i≤ n} for each the n tokens in an input sequence and E = {(Yi−1, Yi) |1≤i≤n} is the set of (n − 1) edges forming a linear chain. Following Lafferty et al. (2001), the conditional probability of the state sequence (s1, s2…sn) given the input sequence (o1, o2…on) is computed as follows: PΛ (s |o) = T K ∏ exp( ∑ ∑ λk fk (st −1, st , o, t )) t =1 k=1 Zo c∈C(s,o) 1 (1) Where fk is an arbitrary"
