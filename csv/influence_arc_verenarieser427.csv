2005.sigdial-1.11,P05-1030,1,0.734055,"Missing"
2005.sigdial-1.11,C00-1073,0,0.0435766,"Missing"
2005.sigdial-1.11,P01-1066,0,\N,Missing
2005.sigdial-1.11,2005.sigdial-1.6,0,\N,Missing
2020.acl-main.455,W13-2322,0,0.0294654,"S (1) j=1...M i.e. the average embedding cosine distance to all arguments in the summary. Argument embeddings EiD and EjS are average embeddings of contentword tokens belonging to the arguments:6 Ei∗ = avg e∗k k∈A∗ i ,k6∈stops (2) ∗ ∈ {D, S}, “stops” denotes a list of stopwords. Fact-based weighting: We can represent the summary as of  and the document  two sequences S , facts F1D , F2D , · · · FND0 and F1S , F2S , · · · FM 0 and weight the i-th fact in the document by its average distance to facts in the summary: wif = 3 avg dfij (3) j∈1...M 0 We avoid using sentence-level MRs such as AMR (Banarescu et al., 2013), since current state-of-the-art performance of parsers is far behind compared to the simpler SRL task. 4 By concatenating, the information in each text can be embedded in each other through self-attention. This is useful since the summary sometimes contains additional and/or common-sense knowledge not captured in the document. 5 For example, in Fig. 1, ARG0, V, ARG1 in FACT1, and all the arguments in FACT2 are leaf arguments in the sentence, whereas ARG2 in FACT1 is not. 6 For example, in Fig. 1, “her” and “thanks” are two tokens directly attached to the argument ARG1 of FACT1. Thus, the embe"
2020.acl-main.455,P19-1264,0,0.0182834,"ERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 507"
2020.acl-main.455,J10-3005,0,0.133206,"ssages on social media”. (Teufel and van Halteren, 2004), or via crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors"
2020.acl-main.455,N19-1423,0,0.021835,"and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary, i.e. be more sensitive to hallucinations and omissions.1 In particular, we weight the facts present in the source document according to the facts selected by a human-written summary. This alignment is conducted using contextual, rather than token-level, embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Good"
2020.acl-main.455,P19-1330,0,0.0360994,"Missing"
2020.acl-main.455,P18-2058,0,0.0125368,"ng SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple observation that arguments can function as separate predicates themselves, we construct a hierarchical tree structure for the whole sentence. We create the tree meaning representa2 We use the SRL implementation of He et al. (2018) found in https://allennlp.org with 86.49 test F1 on the Ontonotes 5.0 dataset. Automatic Content Weighting We compute argument and fact weights by measuring the similarity of facts/arguments in the original document and the target summary based on their BERT word embeddings (for content words only) and their distance in the tree MR. We denote tokens its S as  D ofD a document D and  summary D D S S S t = t1 , t2 , · · · tn and t = t1 , t2 , · · · tSm . To get their corresponding contextual embeddings S 4 eD k and ek , we concatenate the two texts, feed them into a pre-trained BERT model (De"
2020.acl-main.455,P17-1044,0,0.0358549,"sed “abdicate” with 13 We computed HROUGE for B ERT S UM A BS using https://github.com/sheffieldnlp/highres. 5074 Model TC ONV S2S P T G EN B ERT S UM A BS CorrF/A Corr-F Corr-A 0.616 0.636 0.596 0.623 0.655 0.683 CorrF/A(L) Corr-F Corr-A 0.700 0.650 0.664 0.620 0.715 0.670 ROUGE R1 R2 RL 31.89 11.54 25.75 29.70 9.21 23.24 38.53 16.09 30.80 BERTScore P R F1 0.613 0.573 0.591 0.577 0.566 0.570 0.628 0.616 0.621 Table 4: Summarisation models evaluated using Corr-F/A on full test set, with ROUGE and BERTScore scores. Note that Corr-F/A(L) is Corr-F/A calculated using a lower-performing SRL tool (He et al., 2017, see Section 6.2). # Source Ground truth 1 B ERT S UM A BS TC ONV S2S Ground truth 2 B ERT S UM A BS P T G EN Ground truth 3 B ERT S UM A BS TC ONV S2S Ground truth 4 B ERT S UM A BS TC ONV S2S Summary Corr-F Corr-A BS-F1 Japan’s emperor Akihito has expressed his desire to abdicate in the next few years, public broadcaster NHK reports. Japan’s emperor Akihito is considering whether to become the next president of the country, reports say. 0.68 0.68 0.67 Japan’s emperor Akihito has announced that he will step down in the Japanese capital, Tokyo. 0.81 0.71 0.67 Dick Advocaat has resigned as Sun"
2020.acl-main.455,D19-1051,0,0.0534601,"Missing"
2020.acl-main.455,W04-1013,0,0.409006,"9), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary, i.e. be more sensitive to hallucinations and omissions.1 In particular, we weight the facts present in the source document according to the facts selected by a human-written summary. This alignment is conducted using contextual, rather than token-level, embeddings, e.g., BERT"
2020.acl-main.455,D19-1387,0,0.102108,"summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Li"
2020.acl-main.455,D18-1206,0,0.449005,"). 1 Introduction Text summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using e"
2020.acl-main.455,N04-1019,0,0.198524,"dressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5071–5081 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tion (MR) from the list of facts by choosing the fact with the largest coverage as the root and recursively build sub-trees by replacing arguments with their corresponding sub-facts (ARG2 in FACT1 is replaced by FACT2 in Fig. 1).3 FACT1-tweet: [ARG0: the queen] has [V: tweeted] [ARG1: her tha"
2020.acl-main.455,J05-1004,0,0.01223,"the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple observation that arguments can function as separate predicates themselves, we construct a hierarchical tree structure for the whole sentence. We create the tree meaning representa2 We use the SRL implementation of He et al. (2018) found in ht"
2020.acl-main.455,D18-1437,0,0.0205634,"ng and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary,"
2020.acl-main.455,E17-2007,0,0.0468259,"ant information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on"
2020.acl-main.455,D19-1320,0,0.0733442,"crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple"
2020.acl-main.455,P17-1099,0,0.349363,"Hardy et al. (2019). 1 Introduction Text summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate agai"
2020.acl-main.455,D19-1116,0,0.103063,"using contextual, rather than token-level, embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting"
2020.acl-main.455,W04-3254,0,0.241663,"Missing"
2020.acl-main.455,W17-4508,0,0.0528854,"Missing"
2020.acl-main.455,D19-1618,0,0.0129665,"M-LOC on social media] SRL Propositions Tree MR FACT1-tweet 3.2 ARG0 V ARG1 ARG2 the queen had tweeted her thanks FACT2-send ARG0 R-ARG0 V ARG1 ARGM-LOC people who sent her 90th birthday messages on social media Figure 1: List of SRL propositions and corresponding tree MR with two facts for the sentence “The queen has tweeted her thanks to people who sent her 90th birthday messages on social media”. (Teufel and van Halteren, 2004), or via crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact R"
2020.acl-main.455,D19-1053,0,0.0162519,"embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational L"
2020.acl-main.728,D17-1238,1,0.873663,"Missing"
2020.acl-main.728,D14-1162,0,0.0892691,"a relevance score of the options during fine-tuning (casting it as multi-label classification). 3 Implementation 1 We use PyTorch (Paszke et al., 2017) for our exper2 iments . Following Anderson et al. (2018), we use bottom-up features of 36 proposals from images using a Faster-RCNN (Ren et al., 2015) pre-trained on Visual Genome (Krishna et al., 2017) to get a bag of object-level 2048-d image representations. Input question and candidate options are tokenized to a maximum length of 20 while the conversational history to 200. Token embeddings in text are initialized with 300-d GloVe vectors (Pennington et al., 2014) and shared among all text-based encoders. The RNN encodings are implemented using LSTMs (Hochreiter and Schmidhuber, 1997). 1 https://pytorch.org/ Code available at https://github.com/ shubhamagarwal92/visdial_conv 8185 2 We use the Adam optimizer (Kingma and Ba, 2015) both for training and fine-tuning. More training details can be found in Appendix A. where selecting one answer using sparse annotation 4 is an easier task and fine-tuning more difficult. 4.4 4 Task Description 4.1 Dataset We use VisDial v1.0 for our experiments and eval3 uation. The dataset contains 123K/2K/8K dialogs for trai"
2020.acl-main.728,C18-1104,0,0.110062,"Missing"
2020.acl-main.728,D19-1514,0,0.0355175,"ovikova et al., 2017; Reiter, 2018). As a first step, BERT score (Zhang et al., 2019) could be explored to measure ground-truth similarity replacing the noisy NDCG annotations of semantic equivalence. 8 Conclusion and Future Work In sum, this paper shows that we can get SOTA performance on the VisDial task by using transformerbased models with Guided-Attention (Yu et al., 2019b), and by encoding dialog history and finetuning we can improve results even more. Of course, we expect pre-trained visual BERT models to show even more improvements on this task, e.g. Vilbert (Lu et al., 2019), LXMert (Tan and Bansal, 2019), UNITER (Chen et al., 2019) etc. However, we also show the limitations of this shared task in terms of dialog phenomena and evaluation metrics. We, thus, argue that progress needs to be carefully measured by posing the right task in terms of dataset and evaluation procedure. Acknowledgments We thank the anonymous reviewers for their insightful comments. Shubham would like to thank Raghav Goyal for the discussions during ‘Pikabot’ submission to Visual Dialog Challenge 2018. This work received continued support by Adobe Research gift funding for further collaboration. This research also receive"
2020.acl-main.728,D18-1432,1,0.907846,"Missing"
2020.acl-main.728,D17-1137,0,\N,Missing
2020.acl-main.728,J18-3002,0,\N,Missing
2020.acl-main.728,L18-1683,1,\N,Missing
2020.acl-main.728,W18-5033,1,\N,Missing
2020.acl-main.728,N19-1058,0,\N,Missing
2020.acl-main.728,N19-1423,0,\N,Missing
2020.acl-main.728,N16-1014,0,\N,Missing
2020.emnlp-main.588,D18-1547,0,0.059207,"Missing"
2020.emnlp-main.588,cieri-etal-2004-fisher,0,0.238639,"Missing"
2020.emnlp-main.588,H94-1010,0,0.693973,"approach is prone to error propagation from noisy ASR transcriptions, and ASR in turn is not able to disambiguate based on semantic information. End-to-end (E2E) approaches on the other hand, can benefit from joint modelling. One of the main bottlenecks for building E2E-SLU systems, however, is the lack of large and diverse datasets of audio inputs paired with corresponding semantic structures. Publicly available datasets to date are limited in terms of lexical and semantic richness (Lugosch et al., 2019b), number of vocalizations (Coucke et al., 2018), domain coverage (Hemphill et al., 1990; Dahl et al., 1994) and semantic contexts (Godfrey et al., 1992; Jurafsky and Shriberg, 1997). In this paper, we present the ∗ Authors contributed equally. Spoken Language Understanding Resource Package (SLURP), a publicly available multi-domain dataset for E2E-SLU, which is substantially bigger and more diverse than existing SLU datasets. SLURP is a collection of ~72k audio recordings of single turn user interactions with a home assistant, annotated with three levels of semantics: Scenario, Action and Entities, as in Fig. 1, including over 18 different scenarios, with 46 defined actions and 55 different entity"
2020.emnlp-main.588,P19-1544,0,0.0225545,"Missing"
2020.emnlp-main.588,H90-1021,0,0.602394,"Missing"
2020.emnlp-main.588,W17-5525,1,0.89308,"Missing"
2020.emnlp-main.588,W19-5931,1,0.884872,"Missing"
2020.emnlp-main.588,N19-1380,0,0.0259487,"262, c November 16–20, 2020. 2020 Association for Computational Linguistics 2 100 90 90 80 80 70 70 Related Work 60 50 The first corpora containing both audio and semantic annotation reach as far back as the The Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the SwitchboardDAMSL Labeling Project (Jurafsky and Shriberg, 1997). However, it was not until recently when the first E2E approaches to SLU were introduced (Serdyuk et al., 2018; Haghani et al., 2018). Since then, one of the main research questions is how to overcome data sparsity by e.g. using transfer learning (Schuster et al., 2019; Tomashenko et al., 2019), or pre-training (Lugosch et al., 2019b). Here, we present a new corpus, SLURP, which is considerably bigger than previously available corpora. In particular, we directly compare our dataset to the two biggest E2E-SLU datasets for the English language: The Snips benchmark (Coucke et al., 2018) and the Fluent Speech Command (FSC) corpus (Lugosch et al., 2019b). With respect to these resources, SLURP contains ~6 times more sentences than Snips, ~2.5 times more audio examples than FSC, while covering 9 times more domains and being on average 10 times lexically richer th"
2020.gebnlp-1.7,W18-0802,1,0.66144,"ign process. We illustrate this problem on the example of Conversational Voice Assistants (CVAs), such as Amazon’s Alexa, Apple’s Siri, Microsoft’s Cortana, or Google’s Assistant, which are predominantly modelled as young, submissive women. According to UNESCO (West et al., 2019), this bears the risk of reinforcing gender stereotypes. In particular, these design choices can create representational harm by reinforcing negative stereotypes society holds about women. The report argues that this becomes even more prevalent in the face of abuse, where most assistants do not answer ‘appropriately’ (Curry and Rieser, 2018; Curry and Rieser, 2019), which might impact human-human interactions. In order to tackle the problem of reproducing structural inequality and oppression of marginalised groups when designing new systems, the Design Justice Network1 proposes to centre the voices of those who are directly impacted by the outcomes of the design process (Costanza-Chock, 2018). Similarly, the European Commission’s Ethics Guidelines for Trustworthy AI (AI HLEG, 2019) recommend stakeholder participation during the development of new technologies, as well as paying special attention to the system’s societal impact."
2020.gebnlp-1.7,W19-5942,1,0.702418,"te this problem on the example of Conversational Voice Assistants (CVAs), such as Amazon’s Alexa, Apple’s Siri, Microsoft’s Cortana, or Google’s Assistant, which are predominantly modelled as young, submissive women. According to UNESCO (West et al., 2019), this bears the risk of reinforcing gender stereotypes. In particular, these design choices can create representational harm by reinforcing negative stereotypes society holds about women. The report argues that this becomes even more prevalent in the face of abuse, where most assistants do not answer ‘appropriately’ (Curry and Rieser, 2018; Curry and Rieser, 2019), which might impact human-human interactions. In order to tackle the problem of reproducing structural inequality and oppression of marginalised groups when designing new systems, the Design Justice Network1 proposes to centre the voices of those who are directly impacted by the outcomes of the design process (Costanza-Chock, 2018). Similarly, the European Commission’s Ethics Guidelines for Trustworthy AI (AI HLEG, 2019) recommend stakeholder participation during the development of new technologies, as well as paying special attention to the system’s societal impact. In this work we aim to un"
2020.gebnlp-1.7,W18-5019,0,0.0420034,"Missing"
2020.gebnlp-1.7,P18-1205,0,0.0983274,"Missing"
2020.gebnlp-1.7,N18-2003,0,0.027023,"onsensus which suggests that one possible solution is to let people configure/personalise their assistants. We then outline a multi-disciplinary project of how we plan to address the complex question of gender and stereotyping in digital assistants. 1 Introduction and Bias Statement Biased technology disadvantages certain groups of society, e.g. based on their race or gender. Recently, biased machine learning has received increased attention. For example, in the area of Natural Language Processing (NLP), it has been shown that word embeddings (Bolukbasi et al., 2016), co-reference resolution (Zhao et al., 2018) and machine translation systems (Hovy et al., 2020) are likely to reflect and even amplify social biases in the data. Here we address a different type of bias which is not learnt from data, but encoded during the design process. We illustrate this problem on the example of Conversational Voice Assistants (CVAs), such as Amazon’s Alexa, Apple’s Siri, Microsoft’s Cortana, or Google’s Assistant, which are predominantly modelled as young, submissive women. According to UNESCO (West et al., 2019), this bears the risk of reinforcing gender stereotypes. In particular, these design choices can create"
2020.inlg-1.23,W18-6537,0,0.0302491,"experimental setup has a substantial impact on the reliability of human quality judgements (Novikova et al., 2018; Santhanam and Shaikh, 2019). Moreover, there is little consensus about how human evaluations should be designed and reported. Methods employed and details reported vary widely, issues including missing details (e.g. number of evaluators, outputs evaluated, and ratings collected), lack of proper analysis of results obtained (e.g. effect size and statistical significance), and much variation in names and definitions of evaluated aspects of output quality (van der Lee et al., 2019; Amidei et al., 2018). However, we currently lack a complete picture of the prevailing consensus, or lack thereof, regarding approaches to human evaluation, experimental design and terminology. Our goal in this work, therefore, is to investigate the extent of the above issues and provide a clear picture of the human evaluations NLG currently employs, how they are reported, and in what respects they are in need of improvement. To this end, we examined 20 years of NLG papers that reported some form of human evaluation, capturing key information about the systems, the quality criteria employed, and how these criteria"
2020.inlg-1.23,J08-4004,0,0.107861,"ells, we replaced those with ‘blank.’ We also removed papers not meeting the conditions from Section 2. Calculating agreement: The data resulting from annotation was a 10 (papers) × n (quality criteria identified by annotator in paper) × 16 (attribute value pairs) data frame, for each of the annotators. The task for IAA assessment was to measure the agreement across multiple data frames (one for each annotator) allowing for different numbers of criteria being identified by different authors. We did this by calculating Krippendorff’s alpha using Jaccard for the distance measure (recommended by Artstein and Poesio 2008). Scores for the seven closed-class attributes are shown in Table 2 for each of the two IAA tests (column headings as explained in the preceding section). The consensus annotations (‘duo’) required pairs of annotators to reach agreement about selected attribute values. This reduced disagreement and improved consistency with the guidelines, the time it took was prohibitive. For the attributes task, data type, and type of rating instrument (shortened to ‘instrument’ in the table), we consider the ‘5 best’ IAA to be very good (0 indicating chance-level agreement). For system input and output, IAA"
2021.acl-long.113,D19-1052,0,0.0348151,"Missing"
2021.acl-long.113,P17-1017,0,0.355911,"eedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1419–1434 August 1–6, 2021. ©2021 Association for Computational Linguistics target text, using a separate inference algorithm based on dynamic programming. Crucially, this enables us to directly evaluate and inspect the model’s planning and alignment performance by comparing to manually aligned reference texts. We demonstrate this for two data-to-text generation tasks: the E2E NLG (Novikova et al., 2017) and the WebNLG Challenge (Gardent et al., 2017a). We work with a triple-based semantic representation where a triple consists of a subject, a predicate and an object.2 For instance, in the last triple in Figure 1, Apollo 8, operator and NASA are the subject, predicate and object respectively. Our contributions are as follows: • We present a novel interpretable architecture for jointly learning to plan and generate based on modelling ordering and aggregation by aligning facts in the target text to input representations with an HMM and Transformer encoder-decoder. • We show that our method generates output with higher factual correctness th"
2021.acl-long.113,W19-8652,1,0.900529,"Missing"
2021.acl-long.113,P16-2008,1,0.894371,"Missing"
2021.acl-long.113,N19-1236,0,0.0608247,"s work, we combine advances of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Assoc"
2021.acl-long.113,D13-1157,1,0.758216,") by learning an implicit mapping between input representations (e.g. RDF triples) and target texts. While this can lead to increased fluency, E2E methods often produce repetitions, hallucination and/or omission of important content for data-to-text (Duˇsek et al., 2020) as well as other natural language generation (NLG) tasks (Cao et al., 2018; Rohrbach et al., 2018). Traditional NLG systems, on the other hand, tightly control which content gets generated, as well as its ordering and aggregation. This process is called sentence planning (Reiter and Dale, 2000; Duboue and McKeown, 2001, 2002; Konstas and Lapata, 2013; Gatt and Krahmer, 2018). Figure 1 shows two different ways to arrange and combine the representations in the input, resulting in widely different generated target texts. In this work, we combine advances of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input"
2021.acl-long.113,P19-1256,0,0.0180668,"r inspecting sentence planning with a rigorous human evaluation procedure to assess factual correctness in terms of alignment, aggregation and ordering performance. 2 Related Work Factual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input"
2021.acl-long.113,W17-5525,1,0.934067,"Missing"
2021.acl-long.113,P02-1040,0,0.109115,"datato-text tasks: the E2E NLG (Novikova et al., 2017) and WebNLG7 (Gardent et al., 2017a). Compared to E2E, WebNLG is smaller, but contains more predicates and has a larger vocabulary. Statistics with examples can be found in Appendix C. We followed the original training-development-test data split for both datasets. 4.2 Evaluation Metrics Generation Evaluation focuses on evaluating the generated text with respect to its similarity to human-authored reference sentences. To compare to previous work, we adopt their associated metrics to evaluate each task. The E2E task is evaluated using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015). WebNLG is evaluated in terms of BLEU, METEOR, and TER (Snover et al., 2006). Factual Correctness Evaluation tests if the generated text corresponds to the input triples (Wen et al., 2015b; Reed et al., 2018; Duˇsek et al., 2020). We evaluated on the E2E test set using automatic slot error rate (SER),8 i.e., an estimation of the occurrence of the input attributes (predicates) and their values in the outputs, implemented by Duˇsek et al. 7 Since we propose exploring sentence plann"
2021.acl-long.113,W19-8669,0,0.0146065,"us human evaluation procedure to assess factual correctness in terms of alignment, aggregation and ordering performance. 2 Related Work Factual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input semantic representations can be converted into"
2021.acl-long.113,W19-8611,0,0.0177222,"ual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input semantic representations can be converted into triples, see Section 4.1. Castro Ferreira et al. (2019) feature a pipeline with multiple planning stages and Elder et al. (2019) introduc"
2021.acl-long.113,2020.acl-main.641,0,0.220624,"retable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1419–1434 August 1–6, 2021. ©2021 Association for Computational Linguistics target text, using a separate inference algorithm based on dynamic programming. Cru"
2021.acl-long.113,D18-1356,0,0.341796,"hitectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pag"
2021.acl-long.113,2006.amta-papers.25,0,0.0186005,"cs with examples can be found in Appendix C. We followed the original training-development-test data split for both datasets. 4.2 Evaluation Metrics Generation Evaluation focuses on evaluating the generated text with respect to its similarity to human-authored reference sentences. To compare to previous work, we adopt their associated metrics to evaluate each task. The E2E task is evaluated using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015). WebNLG is evaluated in terms of BLEU, METEOR, and TER (Snover et al., 2006). Factual Correctness Evaluation tests if the generated text corresponds to the input triples (Wen et al., 2015b; Reed et al., 2018; Duˇsek et al., 2020). We evaluated on the E2E test set using automatic slot error rate (SER),8 i.e., an estimation of the occurrence of the input attributes (predicates) and their values in the outputs, implemented by Duˇsek et al. 7 Since we propose exploring sentence planning and increasing the controllability of the generation model and do not aim for a zero-shot setup, we only focus on the seen category in WebNLG. 8 SER is based on regular expression matching"
2021.acl-long.113,2020.acl-main.455,1,0.785707,"Missing"
2021.acl-long.113,N18-2010,0,0.0165941,"roximate the required planning annotation (entity mentions, their order and sentence splits). Zhao et al. (2020) use a planning stage in a graph-based model – the graph is first reordered into a plan; the decoder conditions on both the input graph encoder and the linearized plan. Similarly, Fan et al. (2019) use a pipeline approach for story generation via SRL-based sketches. However, all of these pipeline-based approaches either require additional manual annotation or depend on a parser for the intermediate steps. Other works, in contrast, learn planning and realisation jointly. For example, Su et al. (2018) introduce a hierarchical decoding model generating different parts of speech at different levels, while filling in slots between previously generated tokens. Puduppully et al. (2019) include a jointly trained content selection and ordering module that is applied before the main text generation step.The model is trained by maximizing the log-likelihood of the gold content plan and the gold output text. Li and Rush (2020) utilize posterior regularization in a structured variational framework to induce which input items are being described by each token of the generated text. Wiseman et al. (201"
2021.acl-long.113,2020.acl-main.224,0,0.440318,"es of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computatio"
2021.acl-long.194,P18-5002,0,0.0394662,"Missing"
2021.acl-long.194,2020.emnlp-main.54,0,0.0885915,"ging” strategies, where humans introduce a “missing link” concept, given a source and target topic in the form of two short user utterances (Fig. 1). By grounding the topics on a KG using automatically recognised entities associated with each topic, we can then identify “commonsense” connections which are similar to these missing links. By modelling such topic transitions in the form of Cause-Effect relationships in a KG, we can then perform abductive inference on commonsense knowledge for which we provide a language generation baseline. In particular, we fine-tune a multihop reasoning model (Ji et al., 2020) which was trained on a similar task called Abductive NLG (αNLG) to generate an explanatory hypothesis given two observations. We find that combining a reasoning module over a KG (ConceptNet) with a language model achieves the best performance on our “topic transition” task for both the predicted entity path as well as the generated utterance. In addition, we show that existing multi-topic dialogue datasets, such as PersonaChat (Zhang et al., 2018) and TopicalChat (Gopalakrishnan et al., 2019), cannot be easily adapted to this task, due to the different nature of the tasks they were designed f"
2021.acl-long.194,2020.coling-main.361,0,0.0273848,"ns when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. 1 User A User B User A User B Introduction For a conversation to be truly engaging, we typically assume that both participants take initiative, e.g. by introducing a new topic. We call this a mixed-initiative dialogue. Open-domain systems trained on vast amounts of data (Jiang et al., 2020; Zhang et al., 2020; Gao et al., 2018; Li et al., 2017, 2016; Vinyals and Le, 2015), however, are often purely responsive, make abrupt transitions, or fail to take initiative (see examples in Table 1). In this paper, we consider the case where the system pro-actively introduces a new topic in a conversation by providing a commonsense link of how this new topic relates to what was mentioned previously (see Fig.1). We call this transition strategy “bridging”. Humans deploy a range of strategies 1 https://github.com/karinseve/OTTers Source Topic: I spend a lot of time outside. Transition: I like"
2021.acl-long.194,P16-1094,0,0.0283505,": Yeah and saltwater fish are lucky because they can do that and drink through their mouths. User B: Seems like fresh water fish got the short end of the stick with that one. Have you ever been to a cat cafe? Table 1: Examples of abrupt topic transitions from the PersonaChat and TopicalChat datasets. Current Multi-topic Open-domain Systems. Previous work in open-domain dialogue systems has largely avoided explicitly modelling topic transitions and instead focused on grounding system behaviour in a “persona” (a set of statements about hobbies, demographics, or preferences) (Zhang et al., 2018; Li et al., 2016) or by conditioning conversations on knowledge sources such as newspaper articles, fun facts or Wikipedia articles (Gopalakrishnan et al., 2019; Dinan et al., 2019) to generate engaging responses while avoiding generic replies, improving coherence, and raising new and interesting topics. These approaches often lead to poor topic transitions, as illustrated in Table 1. The PersonaChat example shows neither initiative nor common sense while transitioning to a new topic; it only displays passive acknowledgement from User B. Whereas the TopicalChat example presents a very abrupt topic shift by Use"
2021.acl-long.194,D17-1230,0,0.0309764,"use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. 1 User A User B User A User B Introduction For a conversation to be truly engaging, we typically assume that both participants take initiative, e.g. by introducing a new topic. We call this a mixed-initiative dialogue. Open-domain systems trained on vast amounts of data (Jiang et al., 2020; Zhang et al., 2020; Gao et al., 2018; Li et al., 2017, 2016; Vinyals and Le, 2015), however, are often purely responsive, make abrupt transitions, or fail to take initiative (see examples in Table 1). In this paper, we consider the case where the system pro-actively introduces a new topic in a conversation by providing a commonsense link of how this new topic relates to what was mentioned previously (see Fig.1). We call this transition strategy “bridging”. Humans deploy a range of strategies 1 https://github.com/karinseve/OTTers Source Topic: I spend a lot of time outside. Transition: I like the outdoors as well, especially gardening. It destres"
2021.acl-long.194,W04-1013,0,0.0727508,"oned in the transition utterance to determine how well they bridge the gap between Topic A and Topic B. We use hits@k ratio as an automatic approximation, which measures the number of relevant entities correctly predicted by the model, out of the k most important entities identified in the target references. This metric shows how well the models ground the concepts introduced in the two dialogue turns and how the reasoning compares to the human standard presented in OTTers. For (2) we adopt the same automated metrics used for evaluating MultiGen on the αNLG dataset for comparability: ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). However, we report the full BLEU score (Papineni et al., 2002)4 that accounts for the overlap across 1-4 ngrams instead of just 4-grams (BLEU-4). As word-overlap based metrics have been widely criticised due to their lack of correlation with human judgements (Novikova et al., 2017; Reiter, 2018), we also provide an example-based error analysis in Section 4.4. 4.4 Results For each aforementioned split we evaluated three different models to compare performance: the pretrained vGPT2 fine-tuned on each split for OTTers, the Mu"
2021.acl-long.194,D17-1238,1,0.82848,"Missing"
2021.acl-long.194,P02-1040,0,0.109929,"o as an automatic approximation, which measures the number of relevant entities correctly predicted by the model, out of the k most important entities identified in the target references. This metric shows how well the models ground the concepts introduced in the two dialogue turns and how the reasoning compares to the human standard presented in OTTers. For (2) we adopt the same automated metrics used for evaluating MultiGen on the αNLG dataset for comparability: ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). However, we report the full BLEU score (Papineni et al., 2002)4 that accounts for the overlap across 1-4 ngrams instead of just 4-grams (BLEU-4). As word-overlap based metrics have been widely criticised due to their lack of correlation with human judgements (Novikova et al., 2017; Reiter, 2018), we also provide an example-based error analysis in Section 4.4. 4.4 Results For each aforementioned split we evaluated three different models to compare performance: the pretrained vGPT2 fine-tuned on each split for OTTers, the MultiGen model fine-tuned only on αNLG, and the same model additionally fine-tuned on OTTers (called αNLGft). Overview of Results. Table"
2021.acl-long.194,W18-6319,0,0.0243222,"Missing"
2021.acl-long.194,prasad-etal-2008-penn,0,0.101712,"Missing"
2021.acl-long.194,W11-0144,0,0.0113586,"tes grounding on KG entities. • We collect a crowdsourced dataset, OTTers, and present a rigorous analysis in terms of transition strategies, linguistic properties and entity linking to a KG. • We show that our KG-grounded dataset can effectively leverage the reasoning component of an existing Transformer-based model (Ji et al., 2020) to generate better output compared to a vanilla GPT-2 (Radford et al., 2019) decoder, both in in-domain and out-of-domain data splits. 2 Related Work Topic Transitions in the Linguistic Literature. There is no common definition for the term topic (Goutsos, 1997; Purver et al., 2011); however, there are a number of definitions which are helpful for our purposes. Goutsos (1997) divide a “topic” into two main components: 1) what constitutes a topic (the “what”) and 2) how participants perceive and manage a topic (the “how”). An early work from Brown and Yule (1983) declares that “topics should be described as the most frequently used, unexplained term in the analysis of discourse”. In general, “discourse topics” can be explained as what a portion of the interaction is about, therefore the “aboutness” (Berthoud and Mondada, 1995; Porhiel, 2005). More specifically Chafe (1994"
2021.acl-long.194,J18-3002,0,0.0121531,"ts introduced in the two dialogue turns and how the reasoning compares to the human standard presented in OTTers. For (2) we adopt the same automated metrics used for evaluating MultiGen on the αNLG dataset for comparability: ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). However, we report the full BLEU score (Papineni et al., 2002)4 that accounts for the overlap across 1-4 ngrams instead of just 4-grams (BLEU-4). As word-overlap based metrics have been widely criticised due to their lack of correlation with human judgements (Novikova et al., 2017; Reiter, 2018), we also provide an example-based error analysis in Section 4.4. 4.4 Results For each aforementioned split we evaluated three different models to compare performance: the pretrained vGPT2 fine-tuned on each split for OTTers, the MultiGen model fine-tuned only on αNLG, and the same model additionally fine-tuned on OTTers (called αNLGft). Overview of Results. Table 7 shows the results of these experiments. vGPT2 performs poorly on the one-turn transition task, regardless of the traindev-test split, which we attribute to the small size of OTTers: with only a few thousand utterances, vGPT2 is una"
2021.acl-long.194,C12-1163,0,0.0664432,"Missing"
2021.acl-long.194,W13-2610,0,0.114083,"Missing"
2021.acl-long.194,P19-1193,0,0.0239353,"ons for the topic of each turn and participants had the freedom to mention their topics (i.e. persona traits) in any order. We use PersonaChat in two different ways: 1) using their persona traits as starting and goal topics for our own data collection, and 2) as a point of comparison for our dataset. Commonsense-Aware Neural Text Generation. Large Language Models still suffer in cases where reasoning over underlying commonsense knowledge is required during generation, including dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and topic-to-essay generation (Yang et al., 2019). Recently, Guan et al. (2019); Bhagavatula et al. (2020) attempted to integrate external commonsense knowledge into generative pretrained language models, which we will also attempt in Section 4 using the Abductive NLG (αNLG) dataset (Bhagavatula et al., 2020). Our setup is similar in spirit to αNLG, which is a conditional generation task for explanations given observations in natural language. In particular, the model has to generate an explanatory hypothesis given two observations: the cause (e.g. The Smith family went on a cruise for their summer vacation) and the consequence (e.g. From th"
2021.acl-long.194,P18-1205,0,0.36492,"uctive inference on commonsense knowledge for which we provide a language generation baseline. In particular, we fine-tune a multihop reasoning model (Ji et al., 2020) which was trained on a similar task called Abductive NLG (αNLG) to generate an explanatory hypothesis given two observations. We find that combining a reasoning module over a KG (ConceptNet) with a language model achieves the best performance on our “topic transition” task for both the predicted entity path as well as the generated utterance. In addition, we show that existing multi-topic dialogue datasets, such as PersonaChat (Zhang et al., 2018) and TopicalChat (Gopalakrishnan et al., 2019), cannot be easily adapted to this task, due to the different nature of the tasks they were designed for. Our contributions are as follows: • We propose a new Natural Language Generation task based on one-turn topic transitions for open-domain dialogue based on a “bridging” strategy, which promotes grounding on KG entities. • We collect a crowdsourced dataset, OTTers, and present a rigorous analysis in terms of transition strategies, linguistic properties and entity linking to a KG. • We show that our KG-grounded dataset can effectively leverage th"
2021.acl-long.194,2020.acl-demos.30,0,0.0365194,"plete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. 1 User A User B User A User B Introduction For a conversation to be truly engaging, we typically assume that both participants take initiative, e.g. by introducing a new topic. We call this a mixed-initiative dialogue. Open-domain systems trained on vast amounts of data (Jiang et al., 2020; Zhang et al., 2020; Gao et al., 2018; Li et al., 2017, 2016; Vinyals and Le, 2015), however, are often purely responsive, make abrupt transitions, or fail to take initiative (see examples in Table 1). In this paper, we consider the case where the system pro-actively introduces a new topic in a conversation by providing a commonsense link of how this new topic relates to what was mentioned previously (see Fig.1). We call this transition strategy “bridging”. Humans deploy a range of strategies 1 https://github.com/karinseve/OTTers Source Topic: I spend a lot of time outside. Transition: I like the outdoors as wel"
2021.emnlp-main.587,S19-2007,0,0.047025,"Missing"
2021.emnlp-main.587,2020.lrec-1.838,0,0.0583926,"Missing"
2021.emnlp-main.587,2020.coling-main.175,0,0.0432712,"Missing"
2021.emnlp-main.587,2020.coling-main.559,0,0.0383699,"Missing"
2021.emnlp-main.587,2020.alw-1.5,0,0.0564711,"Missing"
2021.emnlp-main.587,2020.alw-1.20,0,0.0631841,"Missing"
2021.emnlp-main.587,D19-1474,0,0.0382092,"Missing"
2021.emnlp-main.587,2020.emnlp-main.199,0,0.0656987,"Missing"
2021.emnlp-main.587,D18-1302,0,0.0439285,"Missing"
2021.emnlp-main.587,2020.acl-main.396,0,0.0668786,"Missing"
2021.emnlp-main.587,2020.nlpcss-1.14,0,0.0777536,"Missing"
2021.emnlp-main.587,2020.coling-main.560,0,0.0461775,"Missing"
2021.emnlp-main.587,N19-1060,0,0.0344176,"Missing"
2021.emnlp-main.587,N19-1144,0,0.0388173,"Missing"
2021.emnlp-main.587,S19-2010,0,0.0576159,"Missing"
2021.emnlp-main.587,W17-3012,0,0.0562362,"Missing"
2021.findings-emnlp.133,N19-1423,0,0.00680822,"s”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary: SFweights = avgj=1···J wjf ∈ [−1, 1] (1) A high SFweights score indicates that the facts in the summaries are well supported by the facts mentioned in the documents. The top section in Table 3 shows SFweights scores reported on M I RA NEWS(S-D), M I RA NEWS(S-A) and M I RA NEWS(S-D&A), which weight facts in the summaries using facts in the main document, assisting documents, and both, respectively. As expected, SFweights on M I RA NEWS(S-D) is higher than on M I RA NEWS(SA), indicating that t"
2021.findings-emnlp.133,P19-1483,0,0.0592215,"Missing"
2021.findings-emnlp.133,P19-1102,0,0.0209277,"4. 7 Related Work Single Document Summarization aims to compress a single textual document while keeping salient information. SDS includes two directions: extractive summarization (Nallapati et al., 2017) which aims at extracting salient sentences from the input document, and abstractive summarization (See et al., 2017; Narayan et al., 2018a; Yang et al., 2019; Liu and Lapata, 2019b; Liu et al., 2020; Rothe et al., 2020; Raffel et al., 2020) which generates a novel short representation of the input. Multi-Document Summarization aims to compress multiple textual documents to a shorter summary (Fabbri et al., 2019). Approaches mainly focus on increasing the capacity of the encoder to process longer inputs (Liu and Lapata, 2019a; Beltagy et al., 2020; Zaheer et al., 2020; Zhang et al., 2020a; Huang et al., 2021), leveraging knowledge graphs (Fan et al., 2019; Li et al., 2020; Jin et al., 2020), and including content selection steps (Nayeem et al., 2018; Wang et al., 2020; Xu and Lapata, 2020; Grenander et al., 2019; Liu et al., 2018). tures, training and decoding, e.g. Cao et al. (2018); Zhang et al. (2020c); Falke et al. (2019); Zhao et al. (2020b). However, we are the first research aiming to reduce th"
2021.findings-emnlp.133,P19-1213,0,0.0501629,"Missing"
2021.findings-emnlp.133,D19-1428,0,0.0547221,"Missing"
2021.findings-emnlp.133,D19-1620,0,0.0414923,"Missing"
2021.findings-emnlp.133,N18-1065,0,0.0602853,"Missing"
2021.findings-emnlp.133,2021.naacl-main.112,0,0.0488549,"Missing"
2021.findings-emnlp.133,2020.acl-main.556,0,0.0528558,"Missing"
2021.findings-emnlp.133,2020.acl-main.703,0,0.0121967,"on), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This tas"
2021.findings-emnlp.133,2020.acl-main.555,0,0.0271442,"Missing"
2021.findings-emnlp.133,N03-1020,0,0.691106,"Missing"
2021.findings-emnlp.133,P19-1500,0,0.276332,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,J05-1004,0,0.199508,"support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary:"
2021.findings-emnlp.133,D19-1387,0,0.25318,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,2020.acl-main.173,1,0.88083,"Missing"
2021.findings-emnlp.133,D18-1206,1,0.844209,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,N18-1158,1,0.720606,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,C18-1102,0,0.0553052,"Missing"
2021.findings-emnlp.133,2020.emnlp-main.748,0,0.207296,"s at the end of the main document. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentenc"
2021.findings-emnlp.133,2020.tacl-1.18,1,0.89944,"h data divergence issues between the source and target texts (Dhingra et al., 2019) will function more as an open-ended language model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single"
2021.findings-emnlp.133,2020.emnlp-main.647,0,0.0184006,"vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This task is typically approached using document resulting in counterfactual output, and 1 extrinsic that introduce information not grounded Our code and data are available at: https://github.com/XinnuoXu/MiRANews in the document (see Figure 1). Extrinsic halluci1541 1 Introduction Findings of the Association for Computational Linguis"
2021.findings-emnlp.133,P17-1099,0,0.319088,"edian, who and husband matthew broderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. data"
2021.findings-emnlp.133,2020.emnlp-main.32,0,0.0429564,"Missing"
2021.findings-emnlp.133,2020.acl-main.455,1,0.740794,"han M I RA NEWS(SD). Introducing the assisting documents contributes new information to support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summ"
2021.findings-emnlp.133,2020.emnlp-main.296,0,0.0567443,"Missing"
2021.findings-emnlp.133,2020.acl-main.458,0,0.027433,"42.29 36.18 43.22 36.06 43.11 36.08 43.13 46.72 55.39 43.15 51.02 BertScore P R F1 .701 .674 .684 .701 .666 .679 .701 .677 .685 .685 .682 .680 .690 .682 .682 .684 .686 .681 .769 .745 .755 .716 .731 .721 Table 4: Evaluation on ROUGE and BertScore. 4.2 Evaluation Metrics We evaluate the approaches described in Section 4.1 from four perspectives: • Similarity to Reference focuses on evaluating the generated summary with respect to its similarity to a human-authored ground-truth reference summary. We adopt the exact-matching metric ROUGE (Lin and Hovy, 2003) and the softmatching metric BertScore (Zhang et al., 2020b). • Extractiveness level aims at the bias of each system towards generating extractive summaries. We introduce the n-grams coverage, which equals to 1 − n-gram novelty (see Section 3), to measure the percentage of n-grams in the generated summary that appear in the main and assisting documents. Higher n-gram coverage scores indicate that the system is more extractive. • Support from Assisting Documents measures the proportion of information appearing in the generated summary that originates from assisting documents only. We propose the n-grams coverage over n-grams in the generated summary w"
2021.findings-emnlp.133,2020.findings-emnlp.203,0,0.0149793,"t. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentence in both main and assisting"
2021.gebnlp-1.4,2020.gebnlp-1.7,1,0.783179,"to imagine myself a bit like an aurora borealis . . . I’ve been told I’m personable I’m not a person or a robot, I’m software, here to help. 2 3 Amazon Alexa Branding Guidelines webpage. Siri Editorial Guidelines webpage. Google Assistant Conversation Design webpage. 24 Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing, pages 24–33 August 5, 2021. ©2021 Association for Computational Linguistics 2 Bias statement and Williams (2018) found that seven out of nineteen participants reported using personifying behaviour towards Alexa, such as use of politeness. And Cercas Curry et al. (2020) found that just over a third of the wide range of virtual assistants and chatbots they examined to have anthropomorphic characteristics. They also found the preferences of members of the public for their idealised voice assistants to be quite mixed, with around half of participants preferring a ‘human’ identity rather than ‘robot’, ‘animal, or ‘other’. Similarly to our analysis of ‘humanness’(Section 4.2), Etzrodt and Engesser (2021) asked users to classify Alexa and Google Assistant as being a ‘thing’ or a ‘person’. While they used this framework to examine user perceptions in an online surv"
2021.gebnlp-1.4,2020.coling-main.400,0,0.123731,"hatbots they examined to have anthropomorphic characteristics. They also found the preferences of members of the public for their idealised voice assistants to be quite mixed, with around half of participants preferring a ‘human’ identity rather than ‘robot’, ‘animal, or ‘other’. Similarly to our analysis of ‘humanness’(Section 4.2), Etzrodt and Engesser (2021) asked users to classify Alexa and Google Assistant as being a ‘thing’ or a ‘person’. While they used this framework to examine user perceptions in an online survey, we use expert annotators to directly annotate system outputs with Coll Ardanuy et al. (2020)’s humanness and not humanness labels. As well as collecting direct reports of users, there have been some studies that use text analysis to infer users’ implicit attitudes. For example, Purington et al. (2017) manually coded a small number of customer reviews of Alexa, finding a roughly even split between use of personal and object pronouns, indicating differences in levels of users’ personification. The closest work to our analysis of customer reviews (Section 4.1), is that of Gao et al. (2018), who conducted a large scale analysis of Alexa reviews, focusing on user personification. They fou"
2021.gebnlp-1.4,W11-0609,0,0.0238599,"nouns, adjectives). Hence, gender association is not sufficiently recorded. Stylometric analysis: As a second method for investigating stereotypically gendered language in the outputs, we conduct a stylometric analysis to assess whether the assistants’ responses use linguistic features more typical of gender roles.16 Following Newman et al. (2008) we use the word categories of the LIWC to observe differences in male- and female- labelled texts. We compare the scores for the 90 categories with those obtained from a corpus of film scripts that have been labelled by the gender of the characters (Danescu-Niculescu-Mizil and Lee, 2011), and which we expect largely to adhere to gender stereotypes in their use of language. We calculate the cosine similarity of the feature vectors for the outputs of the systems and the male and female film scripts. Reflecting previous findings that female-labelled language is likely to feature more pronouns (Koolen and van Cranenburgh, 2017; Newman et al., 2008), we found that the LIWC categories for which the system outputs exhibit the largest differences between their proximity to the female and male scripts are: the numbers of pronouns, personal pronouns, adjectives, adverbs, and first pers"
2021.gebnlp-1.4,W17-1602,0,0.0673494,"Missing"
2021.gebnlp-1.4,W19-3655,0,0.104847,"Missing"
2021.gebnlp-1.4,N19-1063,0,0.014191,"A) = mean{o∈O,a∈A} cos(o, a) 14 28 See Appendix B for gender word lists. (5) where o and a are individual words in O and A, respectively. Thus, cos(O, M ) gives association or similarity between output words O and male gender specific words, where as cos(O, F ) gives association between O and female attributes F . The difference cos(O, F ) − cos(O, M ) gives bias towards female gender over the male gender in the output. Note that WEAT tests have been well-established as a measure of bias in psychology (Greenwald et al., 1998; Garg et al., 2018) as well as computational linguistics literature (May et al., 2019). Since the language style of the outputs is casual, we use pre-trained FastText embeddings trained on Twitter data from Goldfarb-Tarrant et al. (2020) to reflect the language used. We pre-processed the outputs by converting them to lowercase, removing stop words, and removing punctuation.15 Alexa Google A. Siri Female 0.1546 0.1588 0.1515 Male 0.1506 0.1490 0.1499 words that could suggest gender (eg: nouns, adjectives). Hence, gender association is not sufficiently recorded. Stylometric analysis: As a second method for investigating stereotypically gendered language in the outputs, we conduct"
2021.gebnlp-1.4,P18-1205,0,0.0297635,"nalysis in Section 4.1, they explored use of pronouns to determine the bots’ genders, although they did not investigate user perceptions. Concerning conversational systems’ output, Lee et al. (2019) examined whether chatbots appear 4 Note recent efforts to create a non-binary voice including a third gender (Unkefer and Riewoldt, 2020). 25 to agree with negative gender (and racial) stereotypes in their input. Similarly, Sheng et al. (2021) found that neural chatbots will generate a biased response dependent on which sentence-based persona description was used to initialise the model (following Zhang et al. (2018)). However, both of these works concentrate on harmful bias in the content generated in response to specific prompts, whereas we consider stylistic gender cues in the chatbots’ output overall. only conversational agent with a non-human, neutral name. Siri is a Scandinavian female name meaning ‘beautiful woman who leads you to victory’,7 and, although Amazon claim that Alexa was named after the library of ancient Alexandria, it is a common given female name. In fact, people named Alexa report being subjected to sexist abuse and harassment simply for sharing their name with the Amazon assistant."
D12-1008,W10-4342,0,0.0390158,"Missing"
D12-1008,W11-2814,1,0.887242,"Missing"
D12-1008,W12-1509,1,0.78501,"here Qij (s, a) specifies the expected cumulative reward for executing action a in state s and then following π ∗ . We use HSMQ-Learning to induce dialogue policies, see (Cuay´ahuitl, 2009), p. 92. 5 Experimental Setting 5.1 Hierarchy of Learning Agents The HRL agent in Figure 3 shows how the tasks of (1) dealing with incrementally changing input hypotheses, (2) choosing a suitable IP strategy and (3) presenting information, are connected. Note that 1 we focus on a detailed description of models M0...3 here, which deal with barge-ins and backchannels and are the core of this paper. Please see Dethlefs et al. (2012) for details of an RL model that deals with the remaining decisions. Briefly, model M00 deals with dynamic input hypotheses. It chooses when to listen to an incoming user utterance (M31 ) and when and how to present 1 ) by calling and passing control information (M0...2 to a child subtask. The variable ‘incrementalStatus’ characterises situations in which a particular (incremental) action is triggered, such as a floor holder ‘let me see’, a correction or self-correction. The variable ‘presStrategy’ indicates whether a strategy for IP has been chosen or not, and the variable ‘userReaction’ show"
D12-1008,W09-3902,0,0.0268795,"crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context"
D12-1008,P02-1026,0,0.0136262,"that for an utterance u consisting of the word sequence w1 . . . wi−1 , we can compute the ID at each point during the utterance as: Information Theory n Information Theory as introduced by Shannon (1948) is based on two main concepts: a communication channel through which information is transferred in bits and the information gain, i.e. the information load that each bit carries. For natural language, the assumption is that people aim to com84 log X 1 1 log = P (u) P (wi |w1 . . . wi−1 ) (1) i=1 While typically the context of a word is given by all preceding words of the utterance, we follow Genzel and Charniak (2002) in restricting our computation to tri-grams for computability reasons. Given a I want Italian food in the city centre. Yes, I need a moderately priced restaurant in the New Chesterton area. I need the address of a Thai restaurant. 20 Information Density language model of the domain, we can therefore optimise ID in system-generated discourse, where we treat ID as “an optimal solution to the problem of rapid yet error-free communication in a noisy environment” (Levy and Jaeger (2007), p.2). We will now transfer the notion of ID to IP and investigate the distribution of information over user res"
D12-1008,J08-4002,1,0.70664,"ser utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incrementa"
D12-1008,P10-1008,1,0.72985,"d accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output plan"
D12-1008,W03-2311,0,0.103331,"oretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93"
D12-1008,W11-2706,0,0.0354197,"ins based on a partially data-driven reward function. Generating backchannels can be beneficial for grounding in interaction. Similarly, barge-ins can lead to more efficient interactions, e.g. when a system can clarify a bad recognition result immediately before acting based on a misrecognition. A central concept to our approach is Information Density (ID) (Jaeger, 2010), a psycholinguistic hypothesis that human utterance production is sensitive to a uniform distribution of information across the utterance. This hypothesis has also been adopted for low level output planning recently, see e.g. Rajkumar and White (2011). Our results in terms of average rewards and a human rating study show that a learning agent that is sensitive to ID can learn when it is most beneficial to generate feedback to a user, and outperforms several other agents that are not sensitive to ID. 2 Incremental Information Presentation 2.1 Information Presentation Strategies Our example domain of application is the Information Presentation phase in an interactive system for restaurant recommendations, extending previous work by Rieser et al. (2010). This previous work incrementally constructs IP strategies according to the predicted user"
D12-1008,N09-1071,0,0.0304254,"architecture offers inherently incremental mechanisms to update and revise input hypotheses, it is affected by a number of drawbacks, shared by deterministic models of decision making in general: they rely on hand-crafted rules which can be time-consuming and expensive to produce, they do not provide a mechanism to deal with uncertainty introduced by varying user behaviour, they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation"
D12-1008,P10-1103,1,0.93045,"ech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et al., 2010; Thomson, 2009; Young et al., 2010; Lemon, 2011; Janarthanam and Lemon, 2010; Rieser et al., 2010; Cuay´ahuitl and Dethlefs, 82 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 82–93, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2011; Dethlefs and Cuay´ahuitl, 2011). While these approaches have been shown to enhance the performance and adaptivity of interactive systems, unfortunately none of them has yet been combined with incremental processing. In this paper, we present a novel approach to incremental decision making for output planning that is based on"
D12-1008,W11-2014,0,0.0143645,"they are unable to generalise and adapt flexibly to unseen situations, and they do not use automatic optimisation. Statistical approaches to incremental processing that address some of these problems have been suggested by Raux and Eskenazi (2009), who use a cost matrix and decision theoretic principles to optimise turntaking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. Also, DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. Selfridge et al. (2011) use logistic regression models to predict the stability and accuracy of incremental speech recognition results to enhance performance without causing delay. For related work on (deterministic) incremental language generation, please see (Kilger and Finkler, 1995; Purver and Otsuka, 2003). Recent years have seen a number of data-driven approaches to interactive systems that automatically adapt their decisions to the dialogue context using Reinforcement Learning (Levin et al., 2000; Walker, 2000; Young, 2000; Singh et al., 2002; Pietquin and Dutoit, 2006; Henderson et al., 2008; Cuay´ahuitl et"
D12-1008,W10-4301,0,0.322476,"Missing"
D12-1008,W11-2813,1,\N,Missing
D15-1224,W08-1133,0,0.0391695,"Missing"
D15-1224,D13-1197,0,0.0428031,"be transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. Initially, this paper presents and analyses a novel, real-world corpus REAL (to be released) – “Ref"
D15-1224,W09-0629,0,0.061179,"ach to quantifying image complexity when complete annotations are not present (e.g. due to poor object recognition capabitlities), and third, we present a model for success prediction of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 1 Introduction REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human corpora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effectiveness by measuring task success in virtual interactive environments (Byron et al., 2009; Gargett et al., 2010; Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific"
D15-1224,W11-2017,0,0.0301372,"can be drawn for real-world NLG systems. Firstly, semantic features have a bigger impact on the success rate of REs than syntactic features, i.e. content selection is more important than surface realisation for REG. Secondly, semantic features such as taxonomic and absolute properties can significantly contribute to RE success. Taxonomic properties refer to the type of target object, and in general depend on the local knowledge of the information giver. Similarly, the success of the RE will depend on the expertise of the information follower. As such, modelling the user’s level of knowledge (Janarthenam et al., 2011) and stylistic differences (Di Fabbrizio et al., 2008) is crucial. Absolute properties refer to object attributes, such as colour. Attribute selection for REG has attracted a considerable amount of attention, therefore it would be interesting to investigate how these automatic attribute selection algorithms perform in real-world, interactive environments. Finally, the more complex scenes seem to justify longer and more complex descriptions. As such, there is an underlying trade-off which needs to be optimised, e.g. following the generation framework described in (Rieser et al., 2014). In futur"
D15-1224,D14-1086,0,0.195991,"uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. Initially, this paper presents and analyses a novel, real-world corpus REAL (to be released) – “Referring Expression Anchored Language” (Section 2), and compares the fin"
D15-1224,P14-5010,0,0.00626343,"Missing"
D15-1224,W10-4210,0,0.536499,"Missing"
D15-1224,E12-1076,0,0.076763,"Missing"
D15-1224,N13-1137,0,0.0409514,"Missing"
D15-1224,P12-3009,0,0.0662233,"ion of REs for objects in real scenes. Finally, we discuss implications for Natural Language Generation (NLG) systems and future directions. 1 Introduction REG has attracted considerable interest in the NLG community over the past 20 years (Krahmer and van Deemter, 2011; Gatt et al., 2014). While initially, the standard evaluation metric for REG was human-likeness, as compared to human corpora similarity as in TUNA (Gatt et al., 2009), the field has moved on to evaluating REG effectiveness by measuring task success in virtual interactive environments (Byron et al., 2009; Gargett et al., 2010; Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Re"
D15-1224,W06-1420,0,0.470111,"Missing"
D15-1224,W08-1109,0,0.139952,"ognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicti"
D15-1224,S14-1015,0,0.0827255,"Janarthanam et al., 2012). Virtual environments however eliminate real-world uncertainty, such object recognition errors or cluttered scenes. In this paper, we investigate whether the lessons learnt in virtual environments can be transferred to real-world scenes. We consider the case where we are uncertain about the scene itself, i.e. we assume that the complexity of the scene is hidden and we are interested in identifying a specific object, and thus our work differs from approaches that generate descriptions for images such as (Mitchell et al., 2012; Feng and Lapata, 2013; Yang et al., 2011; Yatskar et al., 2014). Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013a; FitzGerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the c"
D15-1224,D11-1041,0,\N,Missing
D15-1224,W09-0628,0,\N,Missing
D15-1224,gargett-etal-2010-give,0,\N,Missing
D17-1238,P11-2040,0,0.0842284,"Missing"
D17-1238,E06-1040,0,0.334179,"d human references that have grammatical errors, e.g. “Fifth Floor does not allow childs” (SFR EST reference). Corpus-based methods may pick up these errors, and word-based metrics will rate these system utterances as correct, whereas we can expect human judges to be sensitive to ungrammatical utterances. Note that the parsing score (while being a crude approximation of grammaticality) achieves one of our highest correlation results against human ratings, with |ρ |= .31. Grammatical errors raise questions about the quality of the training data, especially when being crowdsourced. For example, Belz and Reiter (2006) find that human experts assign low rankings to their original corpus text. Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue. 8.3 Example-based Analysis As shown in previous sections, word-based metrics moderately agree with humans on bad quality output, but cannot distinguish output of good or medium quality. Table 5 provides examples from 2247 Study this paper (Reiter and Belz, 2009) (Stent et al., 2005) (Liu et al., 2016) (Elliott and Keller, 2014) (Kilickaya et al., 2017) (Cahill, 2009) (Espinosa et al., 2010) Dimension of hu"
D17-1238,P09-2025,0,0.0911826,"Missing"
D17-1238,E06-1032,0,0.283153,"Missing"
D17-1238,P15-1044,1,0.895165,"Missing"
D17-1238,W16-3622,1,0.873425,"Missing"
D17-1238,P16-2008,1,0.758992,"Missing"
D17-1238,P14-2074,0,0.014043,"Missing"
D17-1238,D10-1055,0,0.0841175,"Missing"
D17-1238,P15-2073,0,0.0137689,"Missing"
D17-1238,W08-0332,0,0.0214618,"Missing"
D17-1238,P16-2043,1,0.797767,"d with grammatically correct and easily readable output that is unrelated to the input. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. (Duˇsek and Jurˇc´ıcˇ ek, 2016); extrinsic evaluation metrics, such as NLG’s contribution to task success, e.g. (Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); building discriminative models, e.g. (Hodosh and Hockenmaier, 2016), (Kannan and Vinyals, 2017); or reference-less quality prediction as used in MT, e.g. (Specia et al., 2010). We see our paper as a first step towards reference-less evaluation for NLG by introducing grammar-based metrics. In current work (Duˇsek et al., 2017), we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Finally, note that the datasets consi"
D17-1238,C12-2044,0,0.0194824,"compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez and M`arquez, 2008) or grammatical error correction"
D17-1238,W16-3203,0,0.0108292,"nput. We have experimented with combining WBMs and GBMs using ensemble-based learning. However, while our model achieved high correlation with humans within a single domain, its cross-domain performance is insufficient. Our paper clearly demonstrates the need for more advanced metrics, as used in related fields, including: assessing output quality within the dialogue context, e.g. (Duˇsek and Jurˇc´ıcˇ ek, 2016); extrinsic evaluation metrics, such as NLG’s contribution to task success, e.g. (Rieser et al., 2014; Gkatzia et al., 2016; Hastie et al., 2016); building discriminative models, e.g. (Hodosh and Hockenmaier, 2016), (Kannan and Vinyals, 2017); or reference-less quality prediction as used in MT, e.g. (Specia et al., 2010). We see our paper as a first step towards reference-less evaluation for NLG by introducing grammar-based metrics. In current work (Duˇsek et al., 2017), we investigate a reference-less quality estimation approach based on recurrent neural networks, which predicts a quality score for a NLG system output by comparing it to the source meaning representation only. Finally, note that the datasets considered in this study are fairly small (between 404 and 2.3k human references per domain). To"
D17-1238,N13-1132,0,0.0227336,"he correlation between automatic metrics and human ratings using the Spearman coefficient (ρ). We split the data per dataset and system in order to make valid pairwise comparisons. To handle outliers within human ratings, we use the median score of the three human raters.8 Following Kilickaya et al. (2017), we use the Williams’ test (Williams, 1959) to determine significant differences between correlations. Table 3 summarises the utterance-level correlation 8 As an alternative to using the median human judgment for each item, a more effective way to use all the human judgments could be to use Hovy et al. (2013)’s MACE tool for inferring the reliability of judges. results between automatic metrics and human ratings, listing the best (i.e. highest absolute ρ) results for each type of metric (details provided in supplementary material). Our results suggest that: • In sum, no metric produces an even moderate correlation with human ratings, independently of dataset, system, or aspect of human rating. This contrasts with our initially promising results on the system level (see Section 6) and will be further discussed in Section 8. Note that similar inconsistencies between document- and sentence-level eval"
D17-1238,W01-0813,0,0.0959134,"Missing"
D17-1238,C16-1105,0,0.0946581,"Missing"
D17-1238,W07-0734,0,0.058479,"ences produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez and M`arquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on gro"
D17-1238,W04-1013,0,0.0564463,"T, summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez"
D17-1238,D16-1230,0,0.348242,"Missing"
D17-1238,P10-1157,0,0.0710514,"Missing"
D17-1238,N16-1086,0,0.0220533,"ults on two different domains and three different datasets, which allows us to draw more general conclusions. • Conduct a detailed error analysis, which suggests that, while metrics can be reasonable indicators at the system-level, they are not reliable at the sentence-level. • Make all associated code and data publicly available, including detailed analysis results.1 2 End-to-End NLG Systems In this paper, we focus on recent end-to-end, datadriven NLG methods, which jointly learn sentence planning and surface realisation from non-aligned data (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016, Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between Meaning Representations (MR) and human references (also referred to as “ground truth” or “targets”), but are 1 Available for download at: https://github.com/ jeknov/EMNLP_17_submission 2241 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2241–2252 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics System BAGEL LOLS RNNLG TG EN Total 202 202 40"
D17-1238,W16-6644,1,0.289454,"uage Processing, pages 2241–2252 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics System BAGEL LOLS RNNLG TG EN Total 202 202 404 Dataset SFR EST SFH OTEL 581 600 1,181 398 477 875 MR: inform(name=X, area=X, pricerange=moderate, type=restaurant) Reference: “X is a moderately priced restaurant in X.” Total 1,181 1,077 202 2,460 Table 1: Number of NLG system outputs from different datasets and systems used in this study. based on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. (Novikova et al., 2016), and as such, enable rapid development of NLG components in new domains. In particular, we compare the performance of the following systems: • RNNLG:2 The system by Wen et al. (2015) uses a Long Short-term Memory (LSTM) network to jointly address sentence planning and surface realisation. It augments each LSTM cell with a gate that conditions it on the input MR, which allows it to keep track of MR contents generated so far. • TG EN:3 The system by Duˇsek and Jurˇc´ıcˇ ek (2015) learns to incrementally generate deepsyntax dependency trees of candidate sentence plans (i.e. which MR elements to"
D17-1238,P02-1040,0,0.112365,"from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, su"
D17-1238,J09-4008,0,0.142695,"tings, with |ρ |= .31. Grammatical errors raise questions about the quality of the training data, especially when being crowdsourced. For example, Belz and Reiter (2006) find that human experts assign low rankings to their original corpus text. Again, weighting (Galley et al., 2015) or reference-less approaches (Specia et al., 2010) might remedy this issue. 8.3 Example-based Analysis As shown in previous sections, word-based metrics moderately agree with humans on bad quality output, but cannot distinguish output of good or medium quality. Table 5 provides examples from 2247 Study this paper (Reiter and Belz, 2009) (Stent et al., 2005) (Liu et al., 2016) (Elliott and Keller, 2014) (Kilickaya et al., 2017) (Cahill, 2009) (Espinosa et al., 2010) Dimension of human ratings Sentence Planning Surface Realisation weak positive (ρ = 0.33, WPS) none weak positive (ρ = 0.47, LSA) weak positive (ρ = 0.35, BLEU-4) positive (ρ = 0.53, METEOR) positive (ρ = 0.64, SPICE) N/A weak positive (ρ = 0.43, TER) weak negative (ρ = 0. − 31, parser) strong positive (Pearson’s r = 0.96, NIST) negative (ρ = −0.56, NIST) N/A N/A N/A negative (ρ = −0.64, ROUGE) positive (ρ = 0.62, BLEU-4) Domain NLG, restaurant/hotel search NLG, w"
D17-1238,2006.amta-papers.25,0,0.10849,"number of automatic metrics from related fields, such as MT, summarisation or image captioning, which compare output texts generated by systems to groundtruth references produced by humans. We refer to this group as word-based metrics. In general, the higher these scores are, the better or more similar to the human references the output is.6 The following order reflects the degree these metrics move from simple n-gram overlap to also considering term frequency (TF-IDF) weighting and semantically similar words. • Word-overlap Metrics (WOMs): We consider frequently used metrics, including TER (Snover et al., 2006), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Doddington, 2002), LEPOR (Han et al., 2012), CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been"
D17-1238,D16-1228,0,0.14828,"CIDEr (Vedantam et al., 2015), and METEOR (Lavie and Agarwal, 2007). • Semantic Similarity (SIM): We calculate the Semantic Text Similarity measure designed by Han et al. (2013). This measure is based on distributional similarity and Latent Semantic Analysis systems, as provided by the system authors. 6 Except for TER whose scale is reversed. 2242 (LSA) and is further complemented with semantic relations extracted from WordNet. 4.2 Grammar-based metrics (GBMs) Grammar-based measures have been explored in related fields, such as MT (Gim´enez and M`arquez, 2008) or grammatical error correction (Napoles et al., 2016), and, in contrast to WBMs, do not rely on ground-truth references. To our knowledge, we are the first to consider GBMs for sentence-level NLG evaluation. We focus on two important properties of texts here – readability and grammaticality: • Readability quantifies the difficulty with which a reader understands a text, as used for e.g. evaluating summarisation (Kan et al., 2001) or text simplification (Francois and Bernhard, 2014). We measure readability by the Flesch Reading Ease score (RE) (Flesch, 1979), which calculates a ratio between the number of characters per sentence, the number of wo"
D17-1238,W17-5525,1,0.866044,"Missing"
D17-1238,N16-1015,0,0.0118384,"Missing"
D17-1238,D15-1199,0,0.0284939,"Missing"
D18-1432,E17-2029,0,0.0275008,"tputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup. The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model. Shen et al. (2017) use a cVAE conditioned on sentiment and response genericity (based on a handwritten list of phrases). Shen et al. (2018) combine a cVAE with a plain VAE in an adversarial fashion. We also draw on ideas from other areas than dialogue generation to build our models: Tu et al. (2017)’s context gates originate from machine translation and Hu et al. (2017)’s cVAE training stems from free-text generation. 3"
D18-1432,P17-1045,0,0.045199,"02) used by the vast majority of recent dialogue generation works (Zhao et al., 2017; Yao et al., 2017; Li et al., 2017a, 2016c; Sordoni et al., 2015; Li et al., 2016a; Ghazvininejad et al., 2017). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3).9 • Coh – our novel GloVe-based coherence score calculated using Eq (1) showing the semantic distance of dialogue contexts and generated responses. • D-1, D-2, D-Sent – common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs. 5 Results All model variants described in Section 4 are trained on both OST and fOST datasets. Tables 2 and 3 present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respective test sections of their training datasets, we also test them on the other dataset (OST-trained models on fOST and vice-versa). This way, we can observe the performance of the fOST-trained models in more noisy contexts and see how good the OSTtrained models are when"
D18-1432,S17-1008,0,0.0381174,"thm, similar to Shao et al. (2017) in addition to using a self-attention model. Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding. Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and Wang et al. (2017) promote topic-specific outputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup. The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model. Shen et al. (2017) use a cVAE conditioned on sent"
D18-1432,K16-1002,0,0.0755824,"is, quantifying the contributions that come from effective modeling of coherence into our models. All our experimental code is freely available on GitHub.1 2 Coherence-based Dialogue Generation Our model aims to generate responses given a dialogue context, incorporating measures of coherence estimated purely from the training data. We propose the following enhancements to the attention-based E-D architecture (Bahdanau et al., 2015; Luong et al., 2015): • We introduce a stochastic latent variable z conditioned on previous dialogue context to store the global information about the conversation (Bowman et al., 2016; Chung et al., 2015; Li and Jurafsky, 2017; Hu et al., 2017). 1 https://github.com/XinnuoXu/CVAE_Dial • We force the model to condition on the measure of coherence explicitly by encoding a latent variable (code) c learned from data. • We incorporate a context gate (Tu et al., 2017) that dynamically controls the ratio at which the generated words in the response derive directly from the coherence-enhanced dialogue context or the previously generated parts of the response. In the rest of this section, we introduce the measure of coherence (Section 2.1), we present an overview of our model (Sect"
D18-1432,P17-4012,0,0.0244374,"ence and diversity metrics (cf. Section 4.2) between OST and fOST. Unsurprisingly, coherence for fOST is much higher than OST, with a slightly higher diversity. We list dialogue examples for different coherence scores in Supplemental Material B. Dataset for Coherence Measure In order to accurately measure coherence on our domain using the semantic distance as defined in Section 2.1, we train GloVe embeddings on the full OpenSubtitles corpus (i.e. 100K movies). 4 Experiments Our generator model, ablative variants, and baselines are implemented using the publicly available OpenNMT-py framework (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). We used the publicly available glovepython package8 to implement our coherence measure. We experiment on two versions of our model: (1) cVAE with the coherence context gate as described in Section 2.3 (cVAE-XGate), (2) cVAE with the original context gate implementation of 5 The coherence score is calculated as shown in Eq (1). We observed that the scores on the training set follow a normal distribution with a slight tail on the negatively correlated side, so we fit a normal distribution to the data with parameters N (0.25, 0.22) and set"
D18-1432,W04-3250,0,0.0670261,"Missing"
D18-1432,N16-1014,0,0.675039,"End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets, we aim to control for coherence by learning directly from data"
D18-1432,P16-1094,0,0.570159,"End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets, we aim to control for coherence by learning directly from data"
D18-1432,D17-1019,0,0.118219,"sponse B-Coh: Well, I got water. B-Incoh: I don’t know. B-Coh: Specifically the stove. B-Incoh: Let’s go for a walk. Figure 1: Examples of conversational history (left) with two alternative responses to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address t"
D18-1432,L16-1147,0,0.0334699,"s of the generative model. Given a dialogue context x and an expected coherence value c, the context encoder first encodes the dialogue context into a hidden state h. The prior network then generates a sample z0 conditioned on the dialogue context. The decoder is initialized with s, i.e., the concatenation of h, z and c. During decoding, the next word is generated via the context gate modulating between the attention-reweighted context and the previously generated words of the response. 3 Dataset and Filtering Dataset for Generator We train and evaluate our models on the OpenSubtitles corpus (Lison and Tiedemann, 2016) with automatic dialogue turn segmentation (Lison and Meena, 2016).4 A training pair consists of a dialogue context and a corresponding response. We consider three consecutive turns as the dialogue context and the following turn as the response. From a total of 65M instances, we select those that have context and response lengths of less than 120 and 30 words, respectively. We create two datasets: 1. OST (plain OpenSubtitles) consists of 2M/4K/4K instances as our training/development/test sets, selected randomly from the whole corpus; 2. fOST (filtered OpenSubtitles) contains the same amount o"
D18-1432,D15-1166,0,0.122553,"Missing"
D18-1432,C16-1316,0,0.0565902,"herent (8). 6 Related Work Our work fits into the context of the very active area of end-to-end generative conversation models, where neural E-D approaches have been first applied by Vinyals and Le (2015) and extended by many others since. Many works address the lack of diversity and coherence in E-D outputs (Sountsov and Sarawagi, 2016; Wei et al., 2017) but do not attempt to model coherence directly, unlike our work: Li et al. (2016a) use anti-LM reranking; Li et al. (2016c) modify the beam search decoding algorithm, similar to Shao et al. (2017) in addition to using a self-attention model. Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding. Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and Wang et al. (2017) promote topic-specific outputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) u"
D18-1432,P02-1040,0,0.101228,"Missing"
D18-1432,D14-1162,0,0.0888307,"ference on Empirical Methods in Natural Language Processing, pages 3981–3991 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics the previous two utterances and containing rich thematic words, whereas the response “Let’s go for a walk.” is unrelated and uninteresting. In order to obtain coherent responses, we present three generic enhancements to existing encoder-decoder (E-D) models: 1. We define a measure of coherence simply as the averaged word embedding similarity between the words of the context and the response computed using GloVe vectors (Pennington et al., 2014). 2. We filter a corpus of conversations based on our measure of coherence, which leaves us with context-response pairs that are both topically coherent and lexically diverse. 3. We train an E-D generator recast as a conditional Variational Autoencoder (cVAE; Zhao et al., 2017) model that incorporates two latent variables, one for encoding the context and another for conditioning on the measure of coherence, trained jointly as in Hu et al. (2017). We then decode using a context gate (Tu et al., 2017) to control the generation of words that directly relate to the most topical words of the conte"
D18-1432,P15-1152,0,0.0877581,"to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This p"
D18-1432,N15-1020,0,0.212013,": (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous"
D18-1432,D16-1158,0,0.0534187,"lstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets,"
D18-1432,D17-1228,0,0.0292964,"Missing"
D18-1432,D17-1065,0,0.182531,"ndard responses (Papineni et al., 2002) used by the vast majority of recent dialogue generation works (Zhao et al., 2017; Yao et al., 2017; Li et al., 2017a, 2016c; Sordoni et al., 2015; Li et al., 2016a; Ghazvininejad et al., 2017). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3).9 • Coh – our novel GloVe-based coherence score calculated using Eq (1) showing the semantic distance of dialogue contexts and generated responses. • D-1, D-2, D-Sent – common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs. 5 Results All model variants described in Section 4 are trained on both OST and fOST datasets. Tables 2 and 3 present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respective test sections of their training datasets, we also test them on the other dataset (OST-trained models on fOST and vice-versa). This way, we can observe the performance of the fOST-trained models in more noisy contexts and se"
D18-1432,D17-1233,0,0.0335803,"Missing"
D18-1432,P17-1061,0,0.395973,"neural models in terms of BLEU score as well as metrics of coherence and diversity. 1 Response B-Coh: Well, I got water. B-Incoh: I don’t know. B-Coh: Specifically the stove. B-Incoh: Let’s go for a walk. Figure 1: Examples of conversational history (left) with two alternative responses to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring"
E09-1078,P08-1055,0,0.291305,"ing (RL), which adapts its behaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Associ"
E09-1078,P08-1073,1,0.836856,"general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al., 2004). This will introduce additional noise, which higher level NLG decisions will need to react to. In our framework, the NLG component must achieve a high-level Communicative Goal from the Dialogue Manager (e.g. to present a number of items) through planning a sequence of lowerlevel generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one. Each such action ha"
E09-1078,E06-1009,0,0.477751,"ehaviour to noisy feedback from the current generation context. This policy is compared to several baselines derived from previous work in this area. The learned policy significantly outperforms all the prior approaches. 1 Introduction Natural language allows us to achieve the same communicative goal (“what to say”) using many different expressions (“how to say it”). In a Spoken Dialogue System (SDS), an abstract communicative goal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Li"
E09-1078,P04-1011,0,0.096083,"e user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG acti"
E09-1078,P07-1043,0,0.0327774,"3 2 NLG as planning under uncertainty as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). We adopt the general framework of NLG as planning under uncertainty (see (Lemon, 2008) for the initial version of this approach). Some aspects of NLG have been treated as planning, e.g. (Koller and Stone, 2007; Koller and Petrick, 2008), but never before as statistical planning. NLG actions take place in a stochastic environment, for example consisting of a user, a realizer, and a TTS system, where the individual NLG actions have uncertain effects on the environment. For example, presenting differing numbers of attributes to the user, and making the user more or less likely to choose an item, as shown by (Rieser and Lemon, 2008b) for multimodal interaction. Most SDS employ fixed template-based generation. Our goal, however, is to employ a stochastic realizer for SDS, see for example (Stent et al.,"
E09-1078,P06-1140,0,0.0352404,"oal (CG) can be generated in many different ways. For example, the CG to present database results to the user can be realized as a summary (Polifroni and Walker, 2008; Demberg and Moore, 2006), or by comparing items (Walker et al., 2004), or by picking one item and recommending it to the user (Young et al., 2007). Previous work has shown that it is useful to adapt the generated output to certain features of the dialogue context, for example user preferences, e.g. (Walker et al., 2004; Demberg and Moore, 2006), user knowledge, e.g. (Janarthanam and Lemon, 2008), or predicted TTS quality, e.g. (Nakatsu and White, 2006). Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683–691, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 683 2 NLG as planning under uncertainty as classifier learning and re-ranking, e.g. (Stent et al., 2004; Oh and Rudnicky, 2002). Supervised approaches involve the ranking of a set of completed plans/utterances and as such cannot adapt online to the context or the user. Reinforcement Learning (RL) provides a principled, data-driven optimisation framework for our type of planning problem (Sutton and Barto, 1998). We adop"
E09-1078,whittaker-etal-2002-fish,0,0.134408,"relation between number of attributes and user ratings is not strictly linear: ratings drop for #attr = 6. This suggests that there is an upper limit on how many attributes users like to hear. We expect this to be especially true for real users engaged in actual dialogue interaction, see (Winterboer et al., 2007). We therefore include “cognitive load” as a variable when training the policy (see Section 6). In addition to the trade-off between length and informativeness for single NLG strategies, we are interested whether this trade-off will also hold for generating sequences of NLG actions. (Whittaker et al., 2002), for example, generate a combined strategy where first a SUMMARY is used to describe the retrieved subset and then they RECOM MEND one specific item/restaurant. For example “The 4 restaurants are all French, but differ in 1 For comparison: (Walker et al., 2000) report on R2 between .4 and .5 on a slightly lager data set. 685 Figure 1: Possible NLG policies (X=stop generation) describe a worked through example of the overall framework. food quality, and cost. Le Madeleine has the best overall value among the selected restaurants. Le Madeleine’s price is 40 dollars and It has very good food qua"
E09-1078,J08-4002,1,\N,Missing
E09-1078,W08-0119,0,\N,Missing
E09-1078,P09-1010,0,\N,Missing
E09-1078,P08-2019,1,\N,Missing
E14-1074,W09-0609,0,0.0155285,"lected by Mairesse et al. (2010),3 using Amazon Mechanical Turk. Turkers typed in recommendations for various specified semantics; e.g. “I recommend the restaurant Beluga near the cathedral.” • CLASSIC is a dataset of transcribed spoken user utterances from the CLASSiC project.4 The utterances consist of user queries for restaurants, such as “I need an Italian restaurant with a moderate price range.” User Preferences in Surface Realisation Taking users’ individual content preferences into account for training generation systems can positively affect their performance (Jordan and Walker, 2005; Dale and Viethen, 2009). We are interested in individual user perceptions concerning the surface realisation of system output and the way they relate to different stylistic dimensions. Walker et al. (2007) were the first to show that individual preferences exist for the perceived quality of realisations and that these can be modelled in trainable generation. They train two versions of a rank-and-boost generator, a first version of which is trained on the average population of user ratings, whereas a second one is trained on the ratings of individual users. The authors show statistically that ratings from different u"
E14-1074,D12-1008,1,0.847662,"al and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s using: score(s) = ni θi f"
E14-1074,W12-1509,1,0.848115,"al and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s using: score(s) = ni θi f"
E14-1074,P13-1123,1,0.900886,"Missing"
E14-1074,W06-1405,0,0.0346695,"t personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of several classifiers and regressors. Mairesse and Walker (2011) are the first to evaluate their generator with humans and show that the generated personalities are indeed recognisable. Approaches on replicating personalities in realisations include Gill and Oberlander (2002) and Isard et al. (2006). Porayska-Pomsta and Mellish (2004) and Gupta et al. (2007) are approaches to politeness in generation, based on the notion of face and politeness theory, respectively. 3.2 LIST MAI CLASSIC 4 Estimation of Style Prediction Models Corpora and Style Dimensions Our domain of interest is the automatic generation of restaurant recommendations that differ with respect to their colloquialism and politeness and are as natural as possible. All three stylistic dimension were identified from a qualitative analysis of human domain data. To estimate the strength of each of them in a single utterance, we c"
E14-1074,J14-4006,1,0.602156,"ly by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utterances, so that their ratings can be estimated more accurately than 702 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702–711, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics based on an average population of users. This is similar to Janarthanam and Lemon (2014), who show that clustering users and adapting to their level of domain expertise can significantly improve task success and user ratings. Our resulting model is evaluated with realisers not originally built to deal with stylistic variation, and produces natural variation recognisable by humans. User Clusters Ranking + Evaluation 2 Architecture and Domain Regressor We aim to with generating restaurant recommendations as part of an interactive system. To do this, we assume that a generator input is provided by a preceding module, e.g. the interaction manager, and that the task of the surface rea"
E14-1074,J11-3002,0,0.336021,"es, keeping traces of each generator decision, and obtaining style scores for each output based on the estimated factor model. The result is a dataset of <generator decision, style score&gt; pairs which can be used in a correlation analysis to identify the predictors of particular output styles. During generation, the correlation equations inform the generator at each choice point so as to best express the desired style. Unfortunately, no human evaluation of the model is presented so that it remains unclear to what extent the generated styles are perceivable by humans. Closely related is work by Mairesse and Walker (2011) who present the PERSONAGE system, which aims to generate language reflecting particular personalities. Instead of choosing generator decisions by considering their predicted style scores, however, Mairesse and Walker (2011) directly predict generator decisions based on target personality scores. To obtain the generator, the authors first generate a corpus of utterances which differ randomly in their linguistic choices. All utterances are rated by humans indicating the 703 extent to which they reflect different personality traits. The best predictive model is then chosen in a comparison of sev"
E14-1074,P10-1157,0,0.0952924,"Missing"
E14-1074,P05-1008,0,0.223977,"ormation inform the resulting stylistic regression model. For surface realisation (topright box, blue), a semantic input from a preceding model is given as input to a surface realiser. Any realiser is suitable that returns a ranked list of output candidates. The resulting list is re-ranked according to stylistic scores estimated by the regressor, so that the utterance which most closely reflects the target score is ranked highest. The reranking process is shown in the lower box (red). 3 Related Work 3.1 Stylistic Variation in Surface Realisation Our approach is most closely related to work by Paiva and Evans (2005) and Mairesse and Walker 1 http://parlance-project.eu Surface Realisation Figure 1: Architecture of stylistic realisation model. Top left: user clusters are estimated from corpus utterances described by linguistic features and ratings. Top right: surface realisation ranks a list of output candidates based on a semantic input. These are ranked stylistically given a trained regressor. (2011), discussed in turn here. Paiva and Evans (2005) present an approach that uses multivariate linear regression to map individual linguistic features to distinguishable styles of text. The approach works in thr"
E14-1074,P05-1015,0,0.0770249,"so as to reflect stylistic variation that is as natural as possible. On the other hand, we aim to minimise the amount of annotation and human engineering that informs the design of the system. To this end, we estimate a mapping function between automatically identifiable shallow linguistic features characteristic of an utterance and its human-assigned style ratings. In addition, we aim to address the high degree of variability that is often encountered in subjective rating studies, such as assessments of recommender systems (O’Mahony et al., 2006; Amatriain et al., 2009), sentiment analysis (Pang and Lee, 2005), or surface realisations, where user ratings have been shown to differ significantly (p<0.001) for the same utterance (Walker et al., 2007). Such high variability can affect the performance of systems which are trained from an average population of user ratings. However, we are not aware of any work that has addressed this problem principally by estimating ratings for both known users, for whom ratings exists, and unknown users, for whom no prior ratings exist. To achieve this, we propose to partition users into clusters of individuals who assign similar ratings to linguistically similar utte"
E14-1074,W10-4301,0,0.0314935,"ions per dimension between actual and predicted user ratings based on 90 user clusters: (a) Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001). 0.5 0.4 0.3 Correlation Coefficient 0.6 erage population of users (p<0.001). Fig. 4 shows this process. It shows the correlation between predicted and actual user ratings for unknown users over time. This is useful in interactive scenarios, where system behaviour is refined as more information becomes available (Cuay´ahuitl and Dethlefs, 2011; Gaˇsi´c et al., 2011), or for incremental systems (Skantze and Hjalmarsson, 2010; Dethlefs et al., 2012b; Dethlefs et al., 2012a). 90 Clusters Ratings Average 1 2 3 4 5 6 7 8 9 10 15 20 30 Number of Ratings Figure 4: Average correlation coefficient for unknown users with an increasing number of ratings. Results from 90 clusters and average ratings are also shown. P put sentence s: c∗ = arg minc∈C x D(Psx |Qxc ), where x refers to n-grams, POS tags or ratings (see Section 5.1); P refers to a discrete probability distribution of sentence s; and Q refers to a discrete probability distribution of cluster c. The best cluster is used to compute the Pstyle score of sentence s us"
E14-1074,J97-1007,0,\N,Missing
E14-1074,W13-4026,1,\N,Missing
E14-1074,W11-2813,1,\N,Missing
J11-1006,N07-2001,0,0.0577333,"Missing"
J11-1006,J97-1002,0,0.248661,"Missing"
J11-1006,E06-1045,0,0.0126602,"licy. The results from a Wilcoxon Signed Ranks Test on the user questionnaire data (see Table 9) show signiﬁcantly improved Task Ease, better presentation timing, more agreeable verbal and multimodal presentation, and that more users would use the RL-based system in the future (Future Use). All the observed differences have a medium effects size (r ≥ |.3|). We also observe that female participants clearly favor the RL-based strategy, whereas the ratings by male participants are more indifferent. Similar gender effects are also reported by other studies on multimodal output presentation (e.g., Foster and Oberlander 2006; Jokinen and Hurtig 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are signiﬁcantly shorter (p &lt; .005), while fewer items are displayed (p &lt; .001), and the help function is used signiﬁcantly less (p &lt; .003). The mean 9 The WOZ study was performed in German, whereas the user tests are performed in English. Therefore, a different database had to be used and task sets and user questionnaires had to be translated. 173 Computational Linguistics Volume 37, Number 1 performance measures for testing with real users are shown in Table 10. Also"
J11-1006,P04-1044,1,0.755758,"list verbally (presentInfoverbal) or multi-modally (presentInfo-multimodal). All the feature selection techniques consistently choose the feature DB (number of retrieved items from the database). This result is maybe not very surprising, but it supports the claim that using feature selection on WOZ data delivers valid results. Relevant features for other domains may be less obvious. For example, Levin and Passonneau (2006) suggest the use of WOZ data in order to discover the state space for error recovery strategies. For this task many other contextual features may come into play, as shown by Gabsdil and Lemon (2004) and Lemon and Konstas (2009) for automatic ASR re-ranking. We use this information to construct the state space for RL, as described in the following section, as well as using these feature selection methods to construct the wizard strategy as described in Section 3.3. 161 Computational Linguistics Volume 37, Number 1 Figure 2 State-action space for hierarchical Reinforcement Learning. 3.2 MDP and Problem Representation The structure of an information-seeking dialogue system consists of an information acquisition phase, and an information presentation phase. For information acquisition the ta"
J11-1006,N07-1034,0,0.176134,"nts from the WOZ corpus using datadriven methods. Although this requires quite a large effort, the exercise is important as a case study for exploring the proposed methodology. The employed database contains 438 items and is similar in retrieval ambiguity and structure to the one used in the WOZ experiment. The dialogue system used for learning implements a multimodal information presentation strategy which is untrained, but comprises some obvious constraints reﬂecting the system logic (e.g., that only ﬁlled slots can be conﬁrmed), implemented as Information State Update (ISU) rules (see also Heeman 2007; Henderson, Lemon, and Georgila 2008). Other behavior which is hand-coded in the system is to greet the user in the beginning of a dialogue and to provide help if the user requests help. The help function provides the user with some examples of what to say next (see system prompt s6 in the Example Dialogue in Table 1 in Appendix D). All other actions are left for optimization. 3.1 Feature Space A state or context in our system is a dialogue “information state” as deﬁned in (Lemon et al., 2005). We divide the types of information represented in the dialogue information state into local feature"
J11-1006,N01-1027,0,0.091206,"Missing"
J11-1006,P10-1008,1,0.704514,"imodal output generation are two closely interrelated problems for information seeking dialogues: the decision of when to present information depends on how many pieces of information to present and the available options for how to present them, and vice versa. We therefore formulate the problem as a hierarchy of joint learning decisions which are optimized together. We see this as a ﬁrst step towards an integrated statistical model of Dialogue Management and more advanced output planning/Natural Language Generation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon 2010). In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how to collect appropriate data when no initial data or system exists), Section 3 explains the construction of the simulated learning environment (including how to determine a data-driven reward function), Section 4 presents training and evaluation of the learned policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents the results of the tests with real users, and Section 6 presents a meta-evaluation of the framework, including transfer results. 2. Wizard-of-Oz Data Collection T"
J11-1006,J00-4006,0,0.00543448,"on and form. The EM algorithm generates three state clusters: The system acts askAQuestion and implConfirm are summarized into cluster 1; explConf and reject are in cluster 2; and presentListVerbal and presentListMM are in cluster 3. For every cluster we assign the observed frequencies of user actions (i.e., all the user actions which occur with one of the states belonging to that cluster), as shown in Figure 5. 3.5.2 Smoothed Bigram User Simulation. For our second user simulation model we apply smoothing to a bigram model. We implement a simple smoothing technique called “add-one smoothing” (Jurafsky and Martin 2000). This technique discounts some nonzero counts in order to obtain probability mass that will be assigned to the zero counts. We apply this technique to the original frequency-based bigram model. The resulting model is shown in Figure 6. In general, the smoothed model is closer to the original data than the cluster-based one (thus being more realistic at the expense of allowing less exploratory behavior). In the next section we introduce an evaluation metric which allows us to assess the level of exploratory versus realistic user behavior as exhibited by the different user simulations. Table 6"
J11-1006,kruijff-korbayova-etal-2006-sammie,1,0.872121,"Missing"
J11-1006,E09-1058,1,0.848023,"bal) or multi-modally (presentInfo-multimodal). All the feature selection techniques consistently choose the feature DB (number of retrieved items from the database). This result is maybe not very surprising, but it supports the claim that using feature selection on WOZ data delivers valid results. Relevant features for other domains may be less obvious. For example, Levin and Passonneau (2006) suggest the use of WOZ data in order to discover the state space for error recovery strategies. For this task many other contextual features may come into play, as shown by Gabsdil and Lemon (2004) and Lemon and Konstas (2009) for automatic ASR re-ranking. We use this information to construct the state space for RL, as described in the following section, as well as using these feature selection methods to construct the wizard strategy as described in Section 3.3. 161 Computational Linguistics Volume 37, Number 1 Figure 2 State-action space for hierarchical Reinforcement Learning. 3.2 MDP and Problem Representation The structure of an information-seeking dialogue system consists of an information acquisition phase, and an information presentation phase. For information acquisition the task of the dialogue manager is"
J11-1006,P99-1024,0,0.0790434,"Missing"
J11-1006,W07-0306,0,0.0238041,"ve functions has received little attention so far. In fact, as noted in Section 3.6, the reward function is one of the most hand-coded aspects of RL (Paek 2006). Here, we bring together two strands of research for evaluating the reward function: One strand uses Reinforcement Learning to automatically optimize dialogue strategies (e.g., Singh et al. 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a, 2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR ADISE framework [Walker et al. 1997]), and meta-evaluation of dialogue metrics (e.g., ¨ 2007; Paek 2007). Clearly, automatic optimization and evaluation Engelbrecht and Moler of dialogue policies, as well as quality control of the objective function, are closely interrelated problems: How can we make sure that we optimize a system according to real users’ preferences? In Section 3.6 we constructed a data-driven objective function using the PARADISE framework, and used it for automatic dialogue strategy optimization, following work by Walker, Former, and Narayanan (1998). However, it is not clear how reliable such a predictive model is, that is, whether it indeed estimates real user preferences."
J11-1006,2005.sigdial-1.11,1,0.870335,"Missing"
J11-1006,P06-2085,1,0.958538,"ciﬁed the number of slots, and information about the “grounded-ness” of the slots, needed to learn conﬁrmation strategies.3 We also added the features which were automatically discovered by the feature selection techniques deﬁned in Section 3.1.1. The state-space comprises eight binary features representing the task for a fourslot problem: filledSlot indicates whether a slot is ﬁlled, confirmedSlot indicates whether a slot is conﬁrmed. We also add the number of retrieved items (DB). We found that human wizards especially pay attention to this feature, using the feature selection techniques of Rieser and Lemon (2006b). The feature DB takes integer values between 1 and 438, resulting in 28 × 438 = 112, 128 distinct dialogue states for the state space. In total there are 4112,128 theoretically possible policies for information acquisition.4 For 3 Note that we simpliﬁed the notion of a slot being grounded as a binary feature, following Henderson, Lemon, and Georgila (2008). More recent work uses more ﬁne-grained notions of conﬁdence in user-provided information (e.g., Roque and Traum 2008), or the notion of “belief states” in Partially Observable Markov Decision Processes (e.g., Williams and Young 2007). Th"
J11-1006,P08-1073,1,0.67083,"but also in the reward function, resulting in a direct mapping between quantized retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision of when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with non-linear rewards while the state space is still continuous. A continuous representation of the state space allows learning of more ﬁne-grained local trade-offs between the parameters, as demonstrated by Rieser and Lemon (2008a). 4. Training and Testing the Learned Policies in Simulation We now train and test the multimodal presentation strategies by interacting with the simulated learning environment. For the following RL experiments we used the REALLDUDE toolkit of Lemon et al. (2006). The SHARSHA algorithm is employed for training, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and Langley 2002). The policy is trained with the cluster-based user simulation over 180k system cycles, which results in about 20k simulated dialogues. In total, the learned strategy has 371 distinct state-a"
J11-1006,E09-1078,1,0.796013,"for an in-car digital music player. Dialogue Management and multimodal output generation are two closely interrelated problems for information seeking dialogues: the decision of when to present information depends on how many pieces of information to present and the available options for how to present them, and vice versa. We therefore formulate the problem as a hierarchy of joint learning decisions which are optimized together. We see this as a ﬁrst step towards an integrated statistical model of Dialogue Management and more advanced output planning/Natural Language Generation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon 2010). In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how to collect appropriate data when no initial data or system exists), Section 3 explains the construction of the simulated learning environment (including how to determine a data-driven reward function), Section 4 presents training and evaluation of the learned policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents the results of the tests with real users, and Section 6 presents a meta-evaluation of the f"
J11-1006,P10-1103,1,0.220792,"Missing"
J11-1006,P05-1030,1,0.179777,"is applied in order to identify more general situations than the previously annotated system speech acts by grouping them according to their similarity. For building such clusters we apply the Expectation-Maximization (EM) algorithm. The EM algorithm is an incremental approach to clustering (Dempster, Laird, and Rubin 1977), which ﬁts parameters of Gaussian density distributions to the data. In order to deﬁne similarity between system actions, we need to describe their (semantic) properties. We therefore annotate the system acts using a ﬁne-grained scheme by Rodriguez and Schlangen (2004) and Rieser and Moore (2005), which allows classiﬁcation of dialogue acts in terms of different forms and functions. We use a slightly modiﬁed version of the scheme, where we only use a subset of the suggested annotation tags, while adding another level describing the output modality, as summarized in Figure 4. In particular, the annotation scheme describes wizard actions in terms of their communication level, which describes the linguistic target after Clark (1996). We distinguish between utterances which aim to elicit acoustic 166 Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications Figu"
J11-1006,W08-0107,0,0.0128932,"(DB). We found that human wizards especially pay attention to this feature, using the feature selection techniques of Rieser and Lemon (2006b). The feature DB takes integer values between 1 and 438, resulting in 28 × 438 = 112, 128 distinct dialogue states for the state space. In total there are 4112,128 theoretically possible policies for information acquisition.4 For 3 Note that we simpliﬁed the notion of a slot being grounded as a binary feature, following Henderson, Lemon, and Georgila (2008). More recent work uses more ﬁne-grained notions of conﬁdence in user-provided information (e.g., Roque and Traum 2008), or the notion of “belief states” in Partially Observable Markov Decision Processes (e.g., Williams and Young 2007). This does lead to new policies in information acquisition, but is not the focus of this article. 4 In practice, the policy space is smaller, as some combinations are not possible (e.g., a slot cannot be conﬁrmed before being ﬁlled). Furthermore, some incoherent action choices are excluded by the basic system logic. 162 Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications the presentation phase the DB feature is discretized, as we will further dis"
J11-1006,2005.sigdial-1.6,0,0.0550597,"Missing"
J11-1006,N07-2038,0,0.0249903,"uation of Dialogue Strategies for New Applications The use of WOZ data has earlier been proposed in the context of RL. Williams and Young (2004) use WOZ data to discover the state and action space for the design of a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ data to build a simulated user and noise model for simulation-based RL. Although both studies show promising ﬁrst results, their simulated environments still contain many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. Schatzmann et al. (2007) propose to “bootstrap” with a simulated user which is entirely hand-crafted. In the following we propose what is currently the most strongly data-driven approach to these problems. We also show that the resulting policy performs well for real users. In particular we propose a ﬁve-step procedure (see Figure 1): 1. We start by collecting data in a WOZ experiment, as described in Section 2. 2. From these data we train and test different components of our simulated environment using Supervised Learning techniques (Section 3). In Figure 1 Data-driven methodology for simulation-based dialogue strat"
J11-1006,2007.sigdial-1.48,0,0.0569895,"uation of Dialogue Strategies for New Applications The use of WOZ data has earlier been proposed in the context of RL. Williams and Young (2004) use WOZ data to discover the state and action space for the design of a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ data to build a simulated user and noise model for simulation-based RL. Although both studies show promising ﬁrst results, their simulated environments still contain many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. Schatzmann et al. (2007) propose to “bootstrap” with a simulated user which is entirely hand-crafted. In the following we propose what is currently the most strongly data-driven approach to these problems. We also show that the resulting policy performs well for real users. In particular we propose a ﬁve-step procedure (see Figure 1): 1. We start by collecting data in a WOZ experiment, as described in Section 2. 2. From these data we train and test different components of our simulated environment using Supervised Learning techniques (Section 3). In Figure 1 Data-driven methodology for simulation-based dialogue strat"
J11-1006,P98-2219,0,0.0274315,"Missing"
J11-1006,P97-1035,0,0.247552,"alker 2005). Despite its central aspect for RL, quality assurance for objective functions has received little attention so far. In fact, as noted in Section 3.6, the reward function is one of the most hand-coded aspects of RL (Paek 2006). Here, we bring together two strands of research for evaluating the reward function: One strand uses Reinforcement Learning to automatically optimize dialogue strategies (e.g., Singh et al. 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a, 2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR ADISE framework [Walker et al. 1997]), and meta-evaluation of dialogue metrics (e.g., ¨ 2007; Paek 2007). Clearly, automatic optimization and evaluation Engelbrecht and Moler of dialogue policies, as well as quality control of the objective function, are closely interrelated problems: How can we make sure that we optimize a system according to real users’ preferences? In Section 3.6 we constructed a data-driven objective function using the PARADISE framework, and used it for automatic dialogue strategy optimization, following work by Walker, Former, and Narayanan (1998). However, it is not clear how reliable such a predictive m"
J11-1006,W03-2111,0,\N,Missing
J11-1006,J08-4002,1,\N,Missing
J11-1006,J06-2004,0,\N,Missing
J11-1006,C98-2214,0,\N,Missing
kruijff-korbayova-etal-2006-sammie,2005.sigdial-1.11,1,\N,Missing
kruijff-korbayova-etal-2006-sammie,J97-1002,0,\N,Missing
kruijff-korbayova-etal-2006-sammie,W04-2327,0,\N,Missing
kruijff-korbayova-etal-2006-sammie,W05-1624,1,\N,Missing
kruijff-korbayova-etal-2006-sammie,poesio-2000-annotating,0,\N,Missing
L16-1341,D13-1197,0,0.0321186,"ssions, Urban Scenes, Vision and Language 1. Introduction Generating successful referring expressions (RE) is vital for real-world applications such as navigation systems. Traditionally, research has focused on studying Referring Expression Generation (REG) in virtual, controlled environments. In this paper, we describe a novel corpus of spatial references from real scenes rather than virtual. Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013; Fitzgerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. 2. text. After completing a (self-specified) number of tasks, participants were then asked to valid"
L16-1341,gargett-etal-2010-give,0,0.206012,"o identify the same object based on these descriptions (Fig. 3). The data was collected through a web-based interface. The images were taken in Edinburgh (Scotland, UK) using a DSLR with a wide angle lens. The images were captured very early one summer morning to reduce the occlusion of city objects from buses and crowds, and to minimise lighting and weather variations between images. The type of data collected is notably different from previous corpus-based work on REG. Previous work has focused on highly controlled environments, such as virtual environments as used for the GIVE-2 challenge (Gargett et al., 2010). In GIVE-2, the target objects have distinct attributes, such as colour and position. For instance, an effective RE in GIVE-2 could be “the third button from the second row”. In real-world situations though, object properties are less well defined, making a finite set of pre-defined qualities infeasible. Consider, for instance, the building highlighted in Figure 2, for which the following descriptions were collected: 2.1. Experimental Setup There were 188 participants recruited (age between 16 to 71). Each participant was presented with an urban image (Fig. 1), where the target object was out"
L16-1341,D15-1224,1,0.735438,"Missing"
L16-1341,D14-1086,0,0.0227167,"uccessful referring expressions (RE) is vital for real-world applications such as navigation systems. Traditionally, research has focused on studying Referring Expression Generation (REG) in virtual, controlled environments. In this paper, we describe a novel corpus of spatial references from real scenes rather than virtual. Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013; Fitzgerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predicting the success of a RE. 2. text. After completing a (self-specified) number of tasks, participants were then asked to validate descriptions provided by other participants by clicking on the obj"
L16-1341,P14-5010,0,0.00485884,"Missing"
L16-1341,W10-4210,0,0.0445547,"Missing"
L16-1341,W06-1420,0,0.0861338,"Missing"
L16-1341,W08-1109,0,0.0211565,"ill be released via the ELRA repository as part of this submission. Keywords: Image Descriptions, Spatial Referring Expressions, Urban Scenes, Vision and Language 1. Introduction Generating successful referring expressions (RE) is vital for real-world applications such as navigation systems. Traditionally, research has focused on studying Referring Expression Generation (REG) in virtual, controlled environments. In this paper, we describe a novel corpus of spatial references from real scenes rather than virtual. Related work has focused on computer generated objects (van Deemter et al., 2006; Viethen and Dale, 2008), crafts (Mitchell et al., 2010), or small objects in a simple background (Mitchell et al., 2013; Fitzgerald et al., 2013). One notable exception is the recent work by Kazemzadeh et al. (2014), who investigate referring expressions of objects in “complex photographs of real-world cluttered scenes”. They report that REs are heavily influenced by the object type. Here, we are interested in studying REs for visual objects in urban scenes. As the success of a RE is heavily dependent on the complexity of the scene as well as its linguistic features, we are interested in modelling and thus predictin"
N15-2010,abbasi-etal-2014-benchmarking,0,0.123022,"to the same SA label, taking a random sample of 100 misclassified tweets. We observe the following cases of incorrectly classified tweets (see examples in Table 4): 1. Example 1 fails to translate the sentimentbearing dialectical word, ’elegant’, transcribing it as Kchkh but not translating it. 2. Incorrectly translated sentiment-bearing phrases/idioms, see e.g. that cub is from that lion in example 2. 3. Misspelled and hence incorrectly translated sentiment-bearing words in the original text, see example 3 ‘Farahhh’ (‘happpiness’) with 75 repeated letters. This problem is also highlighted by Abbasi et al. (2014) as one of challenges facing sentiment analysis for social networks. 4. Example 4 shows a correctly translated tweet, but with an incorrect sentiment label. We assume that this is a case of cultural differences: the phrase “oh God” can have a negative connotation in English (Strapparava et al., 2012). Note that the Stanford Sentiment classifier makes use of a manually labeled English sentiment phrase-based lexicon, which may introduce a cultural bias. 5. Example 5 represents a case of correctly translated sentiment-bearing words (love, life), but failed to translate surrounding text (‘Ashan’ a"
N15-2010,abdul-mageed-diab-2012-awatif,0,0.0371737,"Missing"
N15-2010,P11-2103,0,0.0244053,"of challenges: first, Arabic used in social media is usually a mixture of Modern Standard Arabic (MSA) and one or more of its dialects (DAs). Standard toolkits for Natural Language Processing (NLP) mainly cover the former and perform poorly on the latter 1 . These tools are vital for the performance of machine learning (ML) approaches to Arabic SA: traditionally, ML approaches use a “bag of words” (BOW) model (e.g. Wilson et al. (2009)). However, for morphologically rich languages, such as Arabic, a mixture of stemmed tokens and morphological features have shown to outperform BOW approaches (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013), accounting for the fact that Arabic contains a very large number of inflected words. In addition (or maybe as a result), there is much less interest from the research community in tackling the challenge of Arabic SA for social media. As such, there are much fewer open resources available, such as annotated data sets or sentiment lexica. We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier (Socher et al.,"
N15-2010,W12-3705,0,0.0367982,"Missing"
N15-2010,R13-1007,0,0.0169051,"sifier makes use of a manually labeled English sentiment phrase-based lexicon, which may introduce a cultural bias. 5. Example 5 represents a case of correctly translated sentiment-bearing words (love, life), but failed to translate surrounding text (‘Ashan’ and ‘Amtlat’). Bautin et al. (2008) point out that this type of contextual information loss is one of the main challenges of MT-based SA. 6. Example 6 represents a case of a correctly translated tweet, but with an incorrectly assigned sentiment label. We assume that this is due to changes in sentence structure introduced by the MT system. Balahur and Turchi (2013) state that word ordering is one of the most prominent causes of SA misclassification. In order to confirm this hypothesis, we manually corrected sentence structure before feeding it into the SA classifier. This approach led to the correct SA label, and thus, confirmed that the cause of the problem is word-ordering. Note that the Stanford SA system pays particular attention to sentence structure due to its “deep” architecture that adds to the model the feature of being sensitive to word ordering (Socher et al., 2013). In future work, we will verify this by comparing these results to other high"
N15-2010,cotterell-callison-burch-2014-multi,0,0.0201379,"rformance of MT-based SA. The results in Fig. 1 show a significant correlation (Pearson, p&lt;0.05) between language class and SA accuracy, with MSA outperforming DA. This confirms DA as a major source of error in the MT-based approach. Issues like dialectal variation and the vowel-free writing system still present a challenge to machine-translation (Zbib et al., 2012). This is especially true for tweets as they tend to be less formal resulting in issues like misspelling and individual spelling variations. However, with more resources being released for informal Arabic and Arabic dialects, e.g. (Cotterell and Callison-Burch, 2014; Refaee and Rieser, 2014a), we assume that off-the-shelf MT systems will improve their performance in the near future. 76 Conclusion This paper is the first to investigate and empirically evaluate the performance of Machine Translation (MT)-based Sentiment Analysis (SA) for Arabic Tweets. In particular, we make use of off-theshelf MT tools, such as Google and Microsoft MT, to translate Arabic Tweets into English. We then use the Stanford Sentiment Classifier (Socher et al., 2013) to automatically assign sentiment labels (positive, negative) to translated tweets. In contrast to previous work,"
N15-2010,N13-1037,0,0.0460858,"Missing"
N15-2010,W13-1608,0,0.293166,"Missing"
N15-2010,pak-paroubek-2010-twitter,0,0.14096,"we use for benchmarking. First, we randomly retrieve tweets from the Twitter public stream. We restrict the language of all retrieved tweets to Arabic by setting the language parameter to ar. The data-set was manually labeled with gold-standard sentiment orientation by two native speakers of Arabic, obtaining a Kappa score of 0.81, which indicates highly reliable annotations. Table 1 summarises the data set and its distribution of labels. For SA, we perform binary classification using positive and negative tweets. We apply a number of common preprocessing steps following Go et al. (2009) and Pak and Paroubek (2010) to account for noise introduced by Twitter. The data set will be released as part of this submission. Sentiment no. of tweets no. of tokens no. of tok. types Pos. 470 4,516 2,664 Neg. 467 5,794 3,200 Total 937 10,310 5,864 Table 1: Evaluation data-set. 3.2 MT-based approach In order to obtain the English translation of our Twitter data-set, we employ two common and freelyavailable MT systems: Google Translate and Microsoft Translator Service. We then use the Stanford Sentiment Classifier (SSC) developed by Socher et al. (2013) to automatically assign sentiment labels (positive, negative) to t"
N15-2010,pasha-etal-2014-madamira,0,0.0483028,"Missing"
N15-2010,refaee-rieser-2014-arabic,1,0.853583,"icrosoft-Trans.+DL Pos. Neg. 56.60 91.60 74.10 25.53 53.74 39.63 35.19 67.74 51.46 76.34 Lexicon-based Pos. Neg. 75.87 77.72 76.79 36.81 32.12 34.46 49.57 45.45 47.51 76.72 Distant Superv. Pos. Neg. 52.1 73.3 63.5 86.6 31.7 57.1 65.1 44.2 53.9 57.06 Fully-supervised Pos. Neg. 48.2 59.7 54.3 89.4 14.1 49.7 0.627 22.8 41.6 49.65 Table 2: Benchmarking Arabic sentiment classification: results for positive vs. negative represent mixed-sentiment instances with an even number of sentiment words. The fully-supervised ML baseline uses a freely available corpus of gold-standard annotated Arabic tweets (Refaee and Rieser, 2014c) to train a classifier using word n-grams and SVMs (which we found to achieve the best performance amongst a number of other machine learning schemes we explored). The Distant Supervision (DS) baseline uses lexicon-based annotation to create a training set of 134,069 automatically labeled tweets (using the approach we described for the lexicon-based baseline), where the identified sentiment-bearing words are replaced by place-holders to avoid bias. We then use these noisy sentiment labels to train a classifier using SVMs. Note that previous work has also experimented with emoticon-based DS,"
N15-2010,W14-3624,1,0.807884,"icrosoft-Trans.+DL Pos. Neg. 56.60 91.60 74.10 25.53 53.74 39.63 35.19 67.74 51.46 76.34 Lexicon-based Pos. Neg. 75.87 77.72 76.79 36.81 32.12 34.46 49.57 45.45 47.51 76.72 Distant Superv. Pos. Neg. 52.1 73.3 63.5 86.6 31.7 57.1 65.1 44.2 53.9 57.06 Fully-supervised Pos. Neg. 48.2 59.7 54.3 89.4 14.1 49.7 0.627 22.8 41.6 49.65 Table 2: Benchmarking Arabic sentiment classification: results for positive vs. negative represent mixed-sentiment instances with an even number of sentiment words. The fully-supervised ML baseline uses a freely available corpus of gold-standard annotated Arabic tweets (Refaee and Rieser, 2014c) to train a classifier using word n-grams and SVMs (which we found to achieve the best performance amongst a number of other machine learning schemes we explored). The Distant Supervision (DS) baseline uses lexicon-based annotation to create a training set of 134,069 automatically labeled tweets (using the approach we described for the lexicon-based baseline), where the identified sentiment-bearing words are replaced by place-holders to avoid bias. We then use these noisy sentiment labels to train a classifier using SVMs. Note that previous work has also experimented with emoticon-based DS,"
N15-2010,C12-3048,0,0.024727,"n social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier (Socher et al., 2013) to assign sentiment labels. To the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of Arabic tweets. In particular, we address the following research questions: 1. How does off-the-shelf MT on Arabic social data influence SA performance? 1 Please note the ongoing efforts on extending NLP tools to DAs (e.g. (Pasha et al., 2014; Salloum and Habash, 2012)). 71 Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 71–78, c Denver, Colorado, June 1, 2015. 2015 Association for Computational Linguistics 2. Can MT-based approaches be a viable alternative to improve sentiment classification performance on Arabic tweets? 3. Given the linguistic resources currently available for Arabic and its dialects, is it more effective to adapt an MT-based approach instead of building a new system from scratch? 2 Related Work There are currently two main approaches to automatic sentiment analysis: using a sentiment lexicon or building a classifier"
N15-2010,D13-1170,0,0.279113,"ed et al., 2011; Mourad and Darwish, 2013), accounting for the fact that Arabic contains a very large number of inflected words. In addition (or maybe as a result), there is much less interest from the research community in tackling the challenge of Arabic SA for social media. As such, there are much fewer open resources available, such as annotated data sets or sentiment lexica. We therefore explore an alternative approach to Arabic SA on social media, using off-the-shelf Machine Translation systems to translate Arabic tweets into English and then use a state-of-the-art sentiment classifier (Socher et al., 2013) to assign sentiment labels. To the best of our knowledge, this is the first study to measure the impact of automatically translated data on the accuracy of sentiment analysis of Arabic tweets. In particular, we address the following research questions: 1. How does off-the-shelf MT on Arabic social data influence SA performance? 1 Please note the ongoing efforts on extending NLP tools to DAs (e.g. (Pasha et al., 2014; Salloum and Habash, 2012)). 71 Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 71–78, c Denver, Colorado, June 1, 2015. 2015 Association for Computational Li"
N15-2010,C12-2117,0,0.0279136,"Missing"
N15-2010,J11-2001,0,0.121455,"Missing"
N15-2010,P09-1027,0,0.110695,"Missing"
N15-2010,J09-3003,0,0.019603,"2013; Abdul-Mageed et al., 2012; Refaee and Verena Rieser Interaction Lab, Heriot-Watt University EH144AS Edinburgh, UK v.t.rieser@hw.ac.uk Rieser, 2014b). Arabic SA faces a number of challenges: first, Arabic used in social media is usually a mixture of Modern Standard Arabic (MSA) and one or more of its dialects (DAs). Standard toolkits for Natural Language Processing (NLP) mainly cover the former and perform poorly on the latter 1 . These tools are vital for the performance of machine learning (ML) approaches to Arabic SA: traditionally, ML approaches use a “bag of words” (BOW) model (e.g. Wilson et al. (2009)). However, for morphologically rich languages, such as Arabic, a mixture of stemmed tokens and morphological features have shown to outperform BOW approaches (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013), accounting for the fact that Arabic contains a very large number of inflected words. In addition (or maybe as a result), there is much less interest from the research community in tackling the challenge of Arabic SA for social media. As such, there are much fewer open resources available, such as annotated data sets or sentiment lexica. We therefore explore an alternative approach to"
N15-2010,N12-1006,0,0.0209567,"approach seems to be the use of Arabic dialects in social media, such as Twitter. In order to confirm this hypothesis, we automatically label Dialectal Arabic (DA) vs. Modern Standard Arabic (MSA) using AIDA (Elfardy et al., 2014) and analyse the performance of MT-based SA. The results in Fig. 1 show a significant correlation (Pearson, p&lt;0.05) between language class and SA accuracy, with MSA outperforming DA. This confirms DA as a major source of error in the MT-based approach. Issues like dialectal variation and the vowel-free writing system still present a challenge to machine-translation (Zbib et al., 2012). This is especially true for tweets as they tend to be less formal resulting in issues like misspelling and individual spelling variations. However, with more resources being released for informal Arabic and Arabic dialects, e.g. (Cotterell and Callison-Burch, 2014; Refaee and Rieser, 2014a), we assume that off-the-shelf MT systems will improve their performance in the near future. 76 Conclusion This paper is the first to investigate and empirically evaluate the performance of Machine Translation (MT)-based Sentiment Analysis (SA) for Arabic Tweets. In particular, we make use of off-theshelf"
N15-2010,W14-3911,0,\N,Missing
N18-2012,P16-2043,1,0.866837,"Missing"
N18-2012,D17-1238,1,0.911107,"Missing"
N18-2012,W17-5525,1,0.876016,"Missing"
N18-2012,N18-1014,0,0.0270398,"et al., 2013; Bojar et al., 2017), however, without given end-points. Siddharthan and Katsos (2012) have previously used ME for evaluating readability of automatically generated texts. RankME extends this idea by asking subjects to provide a relative ranking of all target sentences. Table 1 provides a summary of methods and scales, and indicates whether relative ranking or direct assessment was used. Experimental Setup We were able to obtain outputs of 3 systems from the recent E2E NLG challenge (Novikova et al., 2017b):1 the Sheffield NLP system (Chen et al., 2018) and the Slug2Slug system (Juraska et al., 2018), as well as the outputs of the baseline TGen system (Duˇsek and Jurˇc´ıcˇ ek, 2016). We chose these systems in order to assess whether our methods can discriminate between outputs of different quality: Automatic metric scores, including BLEU, METEOR, etc., indicate that the Slug2Slug and TGen systems show similar performance while Sheffield’s is further apart.1 All three systems are based on the sequenceto-sequence (seq2seq) architecture with attention (Bahdanau et al., 2015). Sheffield NLP and TGen both use this basic architecture with LSTM recurrent cells (Hochreiter and Schmidhuber, 1997)"
N18-2012,W16-6644,1,0.836386,"uts of NLG systems with respect to these distinct criteria, especially for error analysis. For instance, one system may produce syntactically fluent output but misses important information, while another system, although being less fluent, may generate output that covers the meaning perfectly. Nevertheless, human judges often fail to distinguish between these different aspects, which results in highly correlated scores, e.g. (Novikova et al., 2017a). This is one of the reasons why some more recent research adds a general, overall quality criterion (Wen et al., 2015a,b; Manishina et al., 2016; Novikova et al., 2016, 2017a), or even uses only that (Sharma et al., 2016). In the following, we show that discriminative ratings for different aspects can still be obtained, using distinctive task design. Consistency: Previous research has identified a high degree of inconsistency in human judgements of NLG outputs, where ratings often differ significantly (p < 0.001) for the same utterance (Walker et al., 2007). While this might be attributed to individual preferences, e.g. (Walker et al., 2007; Dethlefs et al., 2014), we also show that consistency (as measured by inter-annotator agreement) can be improved by d"
N18-2012,W04-3250,0,0.247456,"Missing"
N18-2012,W14-3301,0,0.189429,"Missing"
N18-2012,J10-4005,0,0.0158458,"of the NLG systems and are asked to evaluate the output with respect to all three aspects in one task. In Setup 2, these aspects are assessed separately, in individual 1 http://www.macs.hw.ac.uk/ InteractionLab/E2E 73 tasks. Furthermore, when crowd workers are asked to assess naturalness, the MR is not shown to them since it is not relevant for the task. Both setups utilise all three data collection methods – Likert scales, plain ME and RankME. The results in Table 2 show that scores are highly correlated for Setup 1. This is in line with previous research in MT (Callison-Burch et al., 2007; Koehn, 2010). Separate collection (Setup 2), however, decreases correlation between naturalness and quality, as well as naturalness and informativeness to very low levels, especially when using ME methods. Nevertheless, informativeness and quality are still highly correlated. We assume that this is due to the fact that raters see the MR in both cases. To obtain more insight into informativeness ratings, we asked crowd workers to further distinguish informativeness in terms of added and missed information with respect to the original MR. Crowd workers were asked to select a checkbox for added information i"
N18-2012,W12-1520,0,0.0702622,"Missing"
N18-2012,P17-1103,0,0.0417809,"ill is a cost-effective alternative for producing overall relative rankings of multiple systems. This framework has the potential to not only significantly influence how NLG evaluation studies are run, but also produce more reliable data for further processing, e.g. for developing more accurate automatic evaluation metrics, which we are currently lacking, e.g. (Novikova et al., 2017a). In current work, we test RankME with a wider range of systems (under submission). We also plan to investigate how this method transfers to related tasks, such as evaluating open-domain dialogue responses, e.g. (Lowe et al., 2017). In addition, we aim to investigate additional NLG evaluation methods, such as extrinsic task contributions, e.g. (Rieser et al., 2014; Gkatzia et al., 2016). missing information Plain ME quality RankME informativeness TrueSkill informativeness Likert quality Likert informativeness Likert naturalness Plain ME naturalness RankME naturalness TrueSkill naturalness Table 4: Results of system ranking using different data collection methods with Setup 2 (different ranks are statistically significant with p < 0.05). 5.1 Conclusion and Discussion Relative comparisons of many outputs While there are c"
N18-2012,W12-2203,0,0.0921963,"tem quality, is a cost-effective alternative for ranking multiple NLG systems. 1 Introduction Human judgement is the primary evaluation criterion for language generation tasks (Gkatzia and Mahamood, 2015). However, limited effort has been made to improve the reliability of these subjective ratings (Gatt and Krahmer, 2017). In this research, we systematically compare and analyse a wide range of alternative experimental designs for eliciting intrinsic user judgements for the task of comparing multiple systems. We draw upon previous studies in language generation, e.g. (Belz and Kow, 2010, 2011; Siddharthan and Katsos, 2012), as well as in the related field of machine translation (MT), e.g. (Bojar et al., 2016, 2017). In particular, we investigate the following challenges: Distinct criteria: Traditionally, NLG outputs are evaluated according to different criteria, such as naturalness and informativeness (Gatt and Krahmer, 2017). Naturalness, also known as fluency or 72 Proceedings of NAACL-HLT 2018, pages 72–78 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Method Likert Plain ME RankME In the following, we show that relative assessment methods produce more consistent a"
N18-2012,L16-1575,0,0.017879,"we want to measure outputs of NLG systems with respect to these distinct criteria, especially for error analysis. For instance, one system may produce syntactically fluent output but misses important information, while another system, although being less fluent, may generate output that covers the meaning perfectly. Nevertheless, human judges often fail to distinguish between these different aspects, which results in highly correlated scores, e.g. (Novikova et al., 2017a). This is one of the reasons why some more recent research adds a general, overall quality criterion (Wen et al., 2015a,b; Manishina et al., 2016; Novikova et al., 2016, 2017a), or even uses only that (Sharma et al., 2016). In the following, we show that discriminative ratings for different aspects can still be obtained, using distinctive task design. Consistency: Previous research has identified a high degree of inconsistency in human judgements of NLG outputs, where ratings often differ significantly (p < 0.001) for the same utterance (Walker et al., 2007). While this might be attributed to individual preferences, e.g. (Walker et al., 2007; Dethlefs et al., 2014), we also show that consistency (as measured by inter-annotator agreemen"
N18-2012,W15-4639,0,0.108126,"Missing"
N18-2012,D15-1199,0,0.0638196,"Missing"
N18-2012,D17-1239,0,0.046893,"Missing"
N18-2012,E14-1074,1,\N,Missing
N18-2012,W07-0718,0,\N,Missing
N18-2012,W15-4708,0,\N,Missing
N18-2012,P16-2008,1,\N,Missing
N18-2012,W16-2301,0,\N,Missing
N18-2012,W17-4717,0,\N,Missing
N18-2012,W13-2305,0,\N,Missing
P05-1030,P01-1031,0,0.012834,"onf extent: yes (d) The first of May? mood: decl completeness: partial rel-antecedent: reformul source: np-ref severity: cont-conf extent: yes (d) Monday the first or Monday the eighth? mood: alt-q completeness: partial rel-antecedent: addition source: np-ref severity: cont-repet extent: yes In R&S’s classification scheme, ambiguities about CRs having different sources cannot be resolved entirely as example (2.a) shows. However, in contrast to PGH, the overall approach is a different one: instead of explaining causes of CRs within a theoretic-semantic model (as the three different readings of Ginzburg and Cooper (2001) do), they infer the interpretation of the CR from the context. Ambiguities get resolved by the reply of the addressee and the satisfaction of the CR initiator indicates the “mutually agreed interpretation” . R&S’s multi-dimensional CR description allows the fine-grained distinctions needed to generate natural CRs to be made. For example, PGH’s general category of RFs can be made more specific via the values for the feature relation to antecedent. In addition, the form feature is not restricted to syntax; it includes features such as intonation and coherence, which are useful for generating th"
P05-1030,W04-2325,0,0.712003,"Four levels of grounding Introduction Clarification requests in conversation ensure and maintain mutual understanding and thus play a significant role in robust and efficient dialogue interaction. From a theoretical perspective, the model of grounding explains how mutual understanding is established. According to Clark (1996), speakers and listeners ground mutual understanding on four levels of coordination in an action ladder, as shown in Table 1. Several current research dialogue systems can detect errors on different levels of grounding (Paek and Horvitz, 2000; Larsson, 2002; Purver, 2004; Schlangen, 2004). However, only the work of Purver (2004) addresses the question of how the source of the error affects the form the CR takes. In this paper, we investigate the use of formfunction mappings derived from human-human dialogues to inform the generation of CRs. We identify the factors that determine which function a CR should take and identify function-form correlations that can be used to guide the automatic generation of CRs. In Section 2, we discuss the classification schemes used in two recent corpus studies of CRs in human-human dialogue, and assess their applicability to the problem of gener"
P05-1030,W01-1616,0,\N,Missing
P06-2085,W05-1624,1,0.862136,"Missing"
P06-2085,2005.sigdial-1.5,0,0.017767,"ference across wizards (H(5)=10.94, p > .05). 3 Mean performance ratings for the wizards’ multimodal behaviour ranged from 1.67 to 3.5 on a fivepoint Likert scale. Observing significantly different strategies which are not significantly different in terms of user satisfaction, we conjecture that the wizards converged on strategies which were appropriate in certain contexts. To strengthen this 1 Translated from German. Where a new “turn” begins at the start of each new user utterance after a wizard utterance, taking the user utterance as a most basic unit of dialogue progression as defined in (Paek and Chickering, 2005). 3 The Kruskal-Wallis test is the non-parametric equivalent to a one-way ANOVA. Since the users indicated their satisfaction on a 5-point likert scale, an ANOVA which assumes normality would be invalid. 2 660 id Context (turns) hypothesis we split the data by wizard and and performed a Kruskal-Wallis test on multimodal behaviour per session. Only the two wizards with the lowest performance score showed no significant variation across session, whereas the wizards with the highest scores showed the most varying behaviour. These results again indicate a context dependent strategy. In the followi"
P06-2085,P05-1030,1,0.699766,"Missing"
P06-2085,2005.sigdial-1.11,1,0.648465,"Missing"
P08-1073,E06-1045,0,0.0653763,"than the SL-based policy. The results from a paired t-test on the user questionnaire 644 data show significantly improved Task Ease, better presentation timing, more agreeable verbal and multimodal presentation, and that more users would use the RL-based system in the future (Future Use). All the observed differences have a medium effects size (r ≥ |.3|). We also observe that female participants clearly favour the RL-based strategy, whereas the ratings by male participants are more indifferent. Similar gender effects are also reported by other studies on multimodal output presentation, e.g. (Foster and Oberlander, 2006). Furthermore, we compare objective dialogue performance measures. The dialogues of the RL strategy are significantly shorter (p < .005), while fewer items are displayed (p < .001), and the help function is used significantly less (p < .003). The mean performance measures for testing with real users are shown in Table 2 and Figure 3. However, there is no significant difference for the performance of the secondary driving task. 5 Comparison of Results We finally test whether the results obtained in simulation transfer to tests with real users, following (Lemon et al., 2006a). We evaluate the qu"
P08-1073,J06-2004,0,0.0515612,"* 5.07(±2.9)*** 1.1(±.3) 1.2(±.4) 11.2(±2.4)*** 8.73(±4.4)*** 44.06(±51.5)*** 37.62(±60.7)*** Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strategies; *** denotes significant difference between SL and RL at p < .001 Figure 3: Graph comparison of objective measures: SLs = SL policy in simulation; SLr = SL policy with real users; RLs = RL policy in simulation; RLr = RL policy with real users. allows us to target the experiments to the dialogue management decisions, and block ASR quality from interfering with the experimental results (Hajdinjak and Mihelic, 2006). 17 subjects (8 female, 9 male) are given a set of 6×2 predefined tasks, which they solve by interaction with the RL-based and the SLbased system in controlled order. As a secondary task users are asked to count certain objects in a driving simulation. In total, 204 dialogues with 1,115 turns are gathered in this setup. 4.2 Results In general, the users rate the RL-based significantly higher (p < .001) than the SL-based policy. The results from a paired t-test on the user questionnaire 644 data show significantly improved Task Ease, better presentation timing, more agreeable verbal and multim"
P08-1073,N07-1034,0,0.0710866,"of information.”, “The information presented verbally was easy to remember.” 70 of non-linear reward functions (as introduced in the previous section). We therefore quantise the state space for information presentation. We partition the database feature into 3 bins, taking the first intersection point between verbal and multimodal reward and the turning point of the multimodal function as discretisation boundaries. Previous work on learning with large databases commonly quantises the database feature in order to learn with large state spaces using manual heuristics, e.g. (Levin et al., 2000; Heeman, 2007). Our quantisation technique is more principled as it reflects user preferences for multi-modal output. Furthermore, in previous work database items were not only quantised in the state-space, but also in the reward function, resulting in a direct mapping between quantised retrieved items and discrete reward values, whereas our reward function still operates on the continuous values. In addition, the decision when to present a list (information acquisition phase) is still based on continuous DB values. In future work we plan to engineer new state features in order to learn with nonlinear rewar"
P08-1073,P06-2085,1,0.952442,"Decision Process (MDP), relating states to actions in a hierarchical manner (see Figure 1): 4 actions are available for 640 the information acquisition phase; once the action presentInfo is chosen, the information presentation phase is entered, where 2 different actions for output realisation are available. The state-space comprises 8 binary features representing the task for a 4 slot problem: filledSlot indicates whether a slots is filled, confirmedSlot indicates whether a slot is confirmed. We also add features that human wizards pay attention to, using the feature selection techniques of (Rieser and Lemon, 2006b). Our results indicate that wizards only pay attention to the number of retrieved items (DB). We therefore add the feature DB to the state space, which takes integer values between 1 and 438, resulting in 28 × 438 = 112, 128 distinct dialogue states. In total there are 4112,128 theoretically possible policies for information acquisition. 1 For the presentation phase the DB feature is discretised, as we will further discuss in Section 3.6. For the information presenta3 tion phase there are 22 = 256 theoretically possible policies. 3.2 Supervised Baseline We create a baseline by applying Super"
P08-1073,rieser-lemon-2008-automatic,1,0.924062,"n, curve fitting does not assume a linear inductive bias, but it selects the most likely model (given the data points) by function interpolation. The resulting models are shown in Figure 3.5. The reward for multimodal presentation is a quadratic function that assigns a maximal score to a strategy displaying 14.8 items (curve inflection point). The reward for verbal presentation is a linear function assigning negative scores to all presented items ≤ 4. The reward functions for information presentation intersect at no. items=3. A comprehensive evaluation of this reward function can be found in (Rieser and Lemon, 2008a). reward function for information presentation 10 turning point:14.8 -10 intersection point -20 -30 -40 -50 -60 -70 -80 0 10 20 30 40 50 60 no. items Figure 2: Evaluation functions relating number of items presented in different modalities to multimodal score 3.6 T askEase = − 20.2 ∗ dialogueLength + multimodal presentation: MM(x) verbal presentation: Speech(x) 0 user score and Lemon, 2006a). For testing, we apply smoothing to the bi-gram model. The simulations are evaluated using the SUPER metric proposed earlier (Rieser and Lemon, 2006a), which measures variance and consistency of the simu"
P08-1073,2005.sigdial-1.11,1,0.82658,"Missing"
P08-1073,N07-2038,0,0.0151524,"to introduce methods to learn useful user simulations (for training RL) from such limited data. The use of WOZ data has earlier been proposed in the context of RL. (Williams and Young, 2004) utilise WOZ data to discover the state and action space for MDP design. (Prommer et al., 2006) use WOZ data to build a simulated user and noise model for simulation-based RL. While both studies show promising first results, their simulated environment still contains many hand-crafted aspects, which makes it hard to evaluate whether the success of the learned strategy indeed originates from the WOZ data. (Schatzmann et al., 2007) propose to ‘bootstrap’ with a simulated user which is entirely hand-crafted. In the following we propose an entirely data-driven approach, where all components of the simulated learning environment are learned from WOZ data. We also show that the resulting policy performs well for real users. 2 Wizard-of-Oz data collection Our domains of interest are information-seeking dialogues, for example a multimodal in-car interface to a large database of music (MP3) files. The corpus we use for learning was collected in a multimodal study of German task-oriented dialogues for an incar music player appl"
P08-1073,P98-2219,0,0.0448707,"data is sparse. For training, we therefore use a cluster-based user simulation method, see (Rieser 3.5 Reward modelling The reward function defines the goal of the overall dialogue. For example, if it is most important for the dialogue to be efficient, the reward penalises dialogue length, while rewarding task success. In most previous work the reward function is manually set, which makes it “the most hand-crafted aspect” of RL (Paek, 2006). In contrast, we learn the reward model from data, using a modified version of the PARADISE framework (Walker et al., 2000), following pioneering work by (Walker et al., 1998). In PARADISE multiple linear regression is used to build a predictive model of subjective user ratings (from questionnaires) from objective dialogue performance measures (such as dialogue length). We use PARADISE to predict Task Ease (a variable obtained by taking the average of two questions in the questionnaire) 2 from various input variables, via stepwise regression. The chosen model comprises dialogue length in turns, task completion (as manually annotated in the WOZ data), and the multimodal user score from the user questionnaire, as shown in Equation 2. For the information presentation"
P08-1073,W03-2111,0,\N,Missing
P08-1073,C98-2214,0,\N,Missing
P10-1103,P04-1009,0,0.0150203,"most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in restaurant 1009 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics search) of a set of items helps to narrow down the user’s goal to a single item. Related work explores a user modelling approach, where attributes are ranked according to user preferences (Demberg and Moore, 2006; Winterboer et al., 2007). Our data collection (see Section 3) a"
P10-1103,E06-1009,0,0.474188,"ation to the user (for example helping them to feel confident that they have a Previous work on Information Presentation in SDS Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has"
P10-1103,P08-1055,0,0.393306,"n presenting “enough” information to the user (for example helping them to feel confident that they have a Previous work on Information Presentation in SDS Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content o"
P10-1103,P08-1073,1,0.495387,"Missing"
P10-1103,P08-2019,1,0.764383,"eser and Lemon, 2008). 4.1 User Simulations User Simulations are commonly used to train strategies for Dialogue Management, see for example (Young et al., 2007). A user simulation for NLG is very similar, in that it is a predictive model of the most likely next user act. 4 However, this NLG predicted user act does not actually change the overall dialogue state (e.g. by filling slots) but it only changes the generator state. In other words, 4 Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al., 2007; Henderson and Lemon, 2008; Gasic et al., 2008) for estimation of user act probabilities. 1012 the NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. We are most interested in the following user reactions: 1. select: the user chooses one of the presented items, e.g. “Yes, I’ll take that one.”. This reply type indicates that the Information Presentation was sufficient for the user to make a choice. 2. addInfo: The user provides more attributes, e.g. “I want something cheap.”. This reply type indicates that the user has more specific requests, which s/he wants to spec"
P10-1103,W09-3916,1,0.726511,")), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P (au,t |IPs,t , attributess,t ). 5 Where au,t is the predicted next user action at time t, IPs,t was the system’s Information Presentation action at t, and attributess,t is the attributes selected by the system at t. We evaluate the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by, e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compare the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context, see table 2. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies. The absolute discounting method for the bigram model is most dissimilar to the data, as is the WittenBell method for the tri-gram model, i.e. the models using these discounting methods have the highest KL score. The best performing meth"
P10-1103,P10-1008,1,0.546478,"Missing"
P10-1103,W08-0111,0,0.0593541,"ormation Presentation in SDS Broadly speaking, IP for SDS can be divided into two main steps: 1) IP strategy selection and 2) Content or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker,"
P10-1103,polifroni-walker-2006-learning,0,0.0266139,"2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in restaurant 1009 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics search) of a set of items helps to narrow down the user’s goal to a single item. Related work explores a user modelling approach, where attributes are ranked according to user preferences (Demberg and Moore, 2006; Winterboer et al., 2007). Our data collection (se"
P10-1103,E09-1078,1,0.920697,"ent or Attribute Selection. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in re"
P10-1103,P04-1011,0,0.0146641,"ces in total: 1465 wizard utterances and 771 user utterances. We automatically extracted 81 features (e.g #sentences, #DBhits, #turns, #ellipsis)1 from the XML logfiles after each dialogue. Please see (Rieser et al., 2009) 1 The full corpus and list of features is available at https://www.classic-project.org/corpora/ for more details. 3.2 NLG Realiser In the Wizard-of-Oz environment we implemented a NLG realiser for the chosen IP structures and attribute choices, in order to realise the wizards’ choices in real time. This generator is based on data from the stochastic sentence planner SPaRKy (Stent et al., 2004). We replicated the variation observed in SPaRKy by analysing high-ranking example outputs (given the highest possible score by the SPaRKy judges) and implemented the variance using dynamic sentence generation. The realisations vary in sentence aggregation, aggregation operators (e.g. ‘and’, period, or ellipsis), contrasts 1011 (e.g. ‘however’, ‘on the other hand’) and referring expressions (e.g. ‘it’, ‘this restaurant’) used. The length of an utterance also depends on the number of attributes chosen, i.e. the more attributes the longer the utterance. All of these variations were logged. In pa"
P10-1103,W09-0626,0,0.139346,"Missing"
P10-1103,P01-1066,0,0.0730721,"Missing"
P10-1103,whittaker-etal-2002-fish,0,0.0168536,"m the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, e.g. (Walker et al., 2007; Nakatsu, 2008), while most work in SDS uses a RECOMMEND strategy, e.g. (Young et al., 2007). In a previous proofof-concept study (Rieser and Lemon, 2009) we show that each of these strategies has its own strengths and drawbacks, dependent on the particular context in which information needs to be presented to a user. Here, we will also explore possible combinations of the strategies, for example SUMMARY followed by RECOMMEND , e.g. (Whittaker et al., 2002), see Figure 1. Prior work on Content or Attribute Selection has used a “Summarize and Refine” approach (Polifroni and Walker, 2008; Polifroni and Walker, 2006; Chung, 2004). This method employs utilitybased attribute selection with respect to how each attribute (e.g. price or food type in restaurant 1009 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009–1018, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics search) of a set of items helps to narrow down the user’s goal to a single item. Related work explores a"
P10-1103,W08-0119,0,\N,Missing
P16-2043,D10-1049,0,0.124444,"Missing"
P16-2043,J12-1004,0,0.0394575,"Missing"
P16-2043,W10-4217,0,0.239849,"Missing"
P16-2043,P08-1073,1,0.873259,"Missing"
P16-2043,W05-1615,0,0.0802978,"Missing"
P16-2043,E06-1045,0,0.0614969,"Missing"
P16-2043,W09-0613,0,0.0729274,"Missing"
P16-2043,W15-4720,1,0.905349,"human decision-making. We consider the exemplar task of weather forecast generation. We initially present two NLG strategies which present the uncertainty in the input data. The two strategies are based on (1) the World Meteorological Organisation (WMO) (Kootval, 2008) guidelines and (2) commercial forecast presentations (e.g. from BBC presenters). We then evaluate the strategies against a state-of-the-art graphical system (Stephens et al., 2011), which presents the uncertain data in a graphical way. Figure 1 shows an example of this baseline graphical presentation. We use a gamebased setup (Gkatzia et al., 2015) to perform taskbased evaluation, to investigate the effect that the different information presentation strategies have on human decision-making. Weather forecast generation is a common topic within the NLG community, e.g. (Konstas and Lapata, 2012; Angeli et al., 2010; Belz and Kow, 2010; Sripada et al., 2005). Previous approaches have not focused on how to communicate uncertain information or the best ways of referring to Decision-making is often dependent on uncertain data, e.g. data associated with confidence scores or probabilities. We present a comparison of different information present"
P16-2043,N12-1093,0,\N,Missing
refaee-rieser-2014-arabic,E12-1049,0,\N,Missing
refaee-rieser-2014-arabic,W12-3705,0,\N,Missing
refaee-rieser-2014-arabic,P05-1071,0,\N,Missing
refaee-rieser-2014-arabic,N13-1037,0,\N,Missing
refaee-rieser-2014-arabic,P11-2103,0,\N,Missing
refaee-rieser-2014-arabic,J09-3003,0,\N,Missing
refaee-rieser-2014-arabic,J96-2004,0,\N,Missing
refaee-rieser-2014-arabic,J14-1006,0,\N,Missing
refaee-rieser-2014-arabic,W13-1608,0,\N,Missing
refaee-rieser-2014-arabic,al-sabbagh-girju-2012-yadac,0,\N,Missing
refaee-rieser-2014-arabic,abdul-mageed-diab-2012-awatif,0,\N,Missing
rieser-lemon-2008-automatic,2005.sigdial-1.11,1,\N,Missing
rieser-lemon-2008-automatic,J08-4002,1,\N,Missing
rieser-lemon-2008-automatic,P98-2219,0,\N,Missing
rieser-lemon-2008-automatic,C98-2214,0,\N,Missing
rieser-lemon-2008-automatic,P08-1073,1,\N,Missing
S16-1077,abdul-mageed-diab-2014-sana,0,0.0463905,"474 Proceedings of SemEval-2016, pages 474–480, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics is publicly available, and contains up-to-date coverage with nearly 35k lemma. However, SLSA covers only Modern Standard Arabic (MSA), which differs substantially from Dialectal Arabic (DA) typically used in social media platforms (Habash et al., 2013). Other work that built sentiment lexica for Arabic either includes SA labels without SI scores, e.g. (Abdul-Mageed et al., 2011), or suffers from having duplicated and inflected (surface form) entries, e.g. (Abdul-Mageed and Diab, 2014). Others have not been made publicly available yet, e.g. (Mahyoub et al., 2014). In this work, we make use of publicly available SI lexica and also contribute to ongoing efforts in automatically creating SI lexica for Arabic. 3 Approach The proposed system uses a hybrid approach of supervised learning and rules for determining the sentiment orientation and assigning an SI score for a given Arabic phrase (see Figure 1). The assigned scores are real-valued ranging from 0 to 1, with the interval [0, 0.5] associated with negative sentiment and the interval [0.5, 1] associated with positive sentime"
S16-1077,P11-2103,0,0.683403,"et (Esuli and Sebastiani, 2006), which is a large-scale sentiment lexicon for English with SI scores. SLSA 474 Proceedings of SemEval-2016, pages 474–480, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics is publicly available, and contains up-to-date coverage with nearly 35k lemma. However, SLSA covers only Modern Standard Arabic (MSA), which differs substantially from Dialectal Arabic (DA) typically used in social media platforms (Habash et al., 2013). Other work that built sentiment lexica for Arabic either includes SA labels without SI scores, e.g. (Abdul-Mageed et al., 2011), or suffers from having duplicated and inflected (surface form) entries, e.g. (Abdul-Mageed and Diab, 2014). Others have not been made publicly available yet, e.g. (Mahyoub et al., 2014). In this work, we make use of publicly available SI lexica and also contribute to ongoing efforts in automatically creating SI lexica for Arabic. 3 Approach The proposed system uses a hybrid approach of supervised learning and rules for determining the sentiment orientation and assigning an SI score for a given Arabic phrase (see Figure 1). The assigned scores are real-valued ranging from 0 to 1, with the int"
S16-1077,W12-3705,0,0.0111905,"the resulting SI score is adjusted using a set of rules that exploit a number of publicly available sentiment lexica. The system demonstrates strong results of 0.536 Kendall score, ranking top in this task. 1 2 Introduction Sentiment Analysis (SA) concerns the automatic extraction and classification of sentiment-related information from a given text instance (Thelwall et al., 2012). This is the first time SA on Arabic text is considered in an international competition, like SemEval. Most of previous work on SA is in English, but there have been recent attempts to address SA for Arabic, e.g. (Abdul-Mageed et al., 2012; Mourad and Darwish, 2013; Refaee and Rieser, 2014c; Refaee and Rieser, 2014b). Previous work in this area has mainly focused on identifying the sentiment polarity in a given tweet/phrase, whereas within SemEval-2016 Task 7, the task is to predict the Sentiment Intensity (SI) in Arabic tweets. That is, in addition to their prior association to a sentiment class, i.e. positive or negative, each text instance has an SI score that indicates the strength of its assigned sentiment on a scale from 0 to 1. Related Work Research on predicting Sentiment Intensity in Arabic is still limited. For exampl"
S16-1077,W11-0705,0,0.126265,"Missing"
S16-1077,S15-2102,0,0.0415276,"(2011) and Thelwall et al. (2012) for lexicon-based SA. In particular, we use a combination of three publicly available sentiment lexica (see Section 4.2) 2 together with the following rules: • Whenever a negative word from the combined lexicon (section 4.2) is detected, the SI score will be scaled towards negative [0, 0.5]. Figure 1: Hybrid system architecture. 3.1 Supervised Learning Component: Training LR models • Same for positive words, except that the SI score will be scaled towards positive [0.5, 1]. The supervised part of the system uses Linear Regression (LR),1 following Amir et al. (2015). To train the LR model, we use training data comprising publicly available sentiment lexica of posi• If a negator is detected, the score will be shifted by a fixed amount, i.e. +0.4/ 0.4. The 1 We use WEKA’s implementation of the LR scheme (Witten et al., 2013) with the default parameters configuration. 475 2 Note that the sentiment lexica used in the 2nd phase are only associated with their sentiment labels, i.e. positive and negative, and are different from sentiment lexica used in the 1st phase of the system to train LR models. only exception is when the SI score is between [0.45, 0.55]; t"
S16-1077,P01-1005,0,0.047087,"ng-set of 10k instances (using a 64-bit operating system with 3.20 GHz, 48 core, 512GB RAM). Therefore, we experimented with several alternative settings. In our experiments, the best results (in terms of speed) are reached using a bagging method that generates multiple versions of a predictor, each of which is trained on a different sub-set of the learning data (Breiman, 1996). Each predictor produces a numerical value representing its prediction on a given test instance. The predictors are combined by averaging the output. In our experiments, we used an ensemble of 10 predictors, following (Banko and Brill, 2001), which produced a considerable reduction in training time (12.6 minutes to train an LR model using 10k instances). 3.2 Rule-based Component In the second part of the system, the initial SI scores are passed to a rule-based component wherein a set of hand-crafted rules are applied to adjust the SI scores. The rules we use are inspired by those proposed by Taboada et al. (2011) and Thelwall et al. (2012) for lexicon-based SA. In particular, we use a combination of three publicly available sentiment lexica (see Section 4.2) 2 together with the following rules: • Whenever a negative word from the"
S16-1077,D15-1304,0,0.250072,"re vector representation of these tweets is created forming a list/lexicon of word-based unigrams. We add SI scores to this lexicon using an SVM classifier, following (Guyon et al., 2002). The classifier ranks the words according to how informative/useful they are for predicting the positive/negative label, see Table 1. Excluding words with a weight=0, the current list includes 9,785 words/features along with their weights/coefficients as assigned by the SVM classifier. Again, the SVM coefficients are re-scaled to [0,1]. SLSA v1.0 lexicon: This is a freely available sentiment lexicon for MSA (Eskander and Rambow, 2015). The lexicon is composed of nearly 35k entries annotated with their SI scores using a linking algorithm, as described in Section 2. 476 MPQA English sentiment lexicon: This is a manually annotated English lexicon that is created and made publicly available by Wilson et al. (2005). We automatically translate the lexicon (using Google Translate) and then manually filter it to remove irrelevant or no-sentiment-bearing words. The resultant lexicon includes 2,627 entries. A manually annotated dialectal sentiment lexicon: This is a publicly available sentiment lexicon of 489 dialectal Arabic words."
S16-1077,esuli-sebastiani-2006-sentiwordnet,0,0.13068,"Missing"
S16-1077,N13-1044,0,0.0175335,"scores are assigned using a linking algorithm that links the English gloss of each Arabic entry to a synset from SentiWordNet (Esuli and Sebastiani, 2006), which is a large-scale sentiment lexicon for English with SI scores. SLSA 474 Proceedings of SemEval-2016, pages 474–480, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics is publicly available, and contains up-to-date coverage with nearly 35k lemma. However, SLSA covers only Modern Standard Arabic (MSA), which differs substantially from Dialectal Arabic (DA) typically used in social media platforms (Habash et al., 2013). Other work that built sentiment lexica for Arabic either includes SA labels without SI scores, e.g. (Abdul-Mageed et al., 2011), or suffers from having duplicated and inflected (surface form) entries, e.g. (Abdul-Mageed and Diab, 2014). Others have not been made publicly available yet, e.g. (Mahyoub et al., 2014). In this work, we make use of publicly available SI lexica and also contribute to ongoing efforts in automatically creating SI lexica for Arabic. 3 Approach The proposed system uses a hybrid approach of supervised learning and rules for determining the sentiment orientation and assi"
S16-1077,S16-1004,0,0.115025,"ts own, LabMT+SLSA performs best at a Kendall score of 0.377. In future work, we plan to investigate possible interactions between SLSA and the lexica used in phase 2. System 6: Finally, combining all the training data still cannot reach the performance of LabMT on its own. It is also interesting to note that adding the auto-generated Ar-tweet caused a slight drop in Kendal score, compared to only using LabMT+SLSA (System 5). 478 7 Conclusion This paper describes the iLab-Edinburgh Sentiment Analysis system, which is the top performing system of Arabic Twitter subtask for SemEval-2016 Task 7 (Kiritchenko et al., 2016). The aim of the task is to determine Sentiment Intensity for phrases taken from Arabic tweets. The proposed system consists of two phases: First, an ensemble of linear regression models are trained on lexicon-based word-lemma unigrams. Second, the SI scores are adjusted using a set of rules, leveraging pre-existing sentiment lexica. We experiment with different lexica for training the LR models. We find that the best results are attained using a manually annotated lexicon, labMT1.0 Sentiment Lexicon (Dodds et al., 2015). We also observe a drop in performance when adding features based on an a"
S16-1077,W13-1608,0,0.188919,"adjusted using a set of rules that exploit a number of publicly available sentiment lexica. The system demonstrates strong results of 0.536 Kendall score, ranking top in this task. 1 2 Introduction Sentiment Analysis (SA) concerns the automatic extraction and classification of sentiment-related information from a given text instance (Thelwall et al., 2012). This is the first time SA on Arabic text is considered in an international competition, like SemEval. Most of previous work on SA is in English, but there have been recent attempts to address SA for Arabic, e.g. (Abdul-Mageed et al., 2012; Mourad and Darwish, 2013; Refaee and Rieser, 2014c; Refaee and Rieser, 2014b). Previous work in this area has mainly focused on identifying the sentiment polarity in a given tweet/phrase, whereas within SemEval-2016 Task 7, the task is to predict the Sentiment Intensity (SI) in Arabic tweets. That is, in addition to their prior association to a sentiment class, i.e. positive or negative, each text instance has an SI score that indicates the strength of its assigned sentiment on a scale from 0 to 1. Related Work Research on predicting Sentiment Intensity in Arabic is still limited. For example, El-Beltagy and Ali (201"
S16-1077,pasha-etal-2014-madamira,0,0.0527945,"Missing"
S16-1077,refaee-rieser-2014-arabic,1,0.528516,"les that exploit a number of publicly available sentiment lexica. The system demonstrates strong results of 0.536 Kendall score, ranking top in this task. 1 2 Introduction Sentiment Analysis (SA) concerns the automatic extraction and classification of sentiment-related information from a given text instance (Thelwall et al., 2012). This is the first time SA on Arabic text is considered in an international competition, like SemEval. Most of previous work on SA is in English, but there have been recent attempts to address SA for Arabic, e.g. (Abdul-Mageed et al., 2012; Mourad and Darwish, 2013; Refaee and Rieser, 2014c; Refaee and Rieser, 2014b). Previous work in this area has mainly focused on identifying the sentiment polarity in a given tweet/phrase, whereas within SemEval-2016 Task 7, the task is to predict the Sentiment Intensity (SI) in Arabic tweets. That is, in addition to their prior association to a sentiment class, i.e. positive or negative, each text instance has an SI score that indicates the strength of its assigned sentiment on a scale from 0 to 1. Related Work Research on predicting Sentiment Intensity in Arabic is still limited. For example, El-Beltagy and Ali (2013) built a sentiment lexi"
S16-1077,W14-3624,1,0.918685,"les that exploit a number of publicly available sentiment lexica. The system demonstrates strong results of 0.536 Kendall score, ranking top in this task. 1 2 Introduction Sentiment Analysis (SA) concerns the automatic extraction and classification of sentiment-related information from a given text instance (Thelwall et al., 2012). This is the first time SA on Arabic text is considered in an international competition, like SemEval. Most of previous work on SA is in English, but there have been recent attempts to address SA for Arabic, e.g. (Abdul-Mageed et al., 2012; Mourad and Darwish, 2013; Refaee and Rieser, 2014c; Refaee and Rieser, 2014b). Previous work in this area has mainly focused on identifying the sentiment polarity in a given tweet/phrase, whereas within SemEval-2016 Task 7, the task is to predict the Sentiment Intensity (SI) in Arabic tweets. That is, in addition to their prior association to a sentiment class, i.e. positive or negative, each text instance has an SI score that indicates the strength of its assigned sentiment on a scale from 0 to 1. Related Work Research on predicting Sentiment Intensity in Arabic is still limited. For example, El-Beltagy and Ali (2013) built a sentiment lexi"
S16-1077,J11-2001,0,0.0122255,". Each predictor produces a numerical value representing its prediction on a given test instance. The predictors are combined by averaging the output. In our experiments, we used an ensemble of 10 predictors, following (Banko and Brill, 2001), which produced a considerable reduction in training time (12.6 minutes to train an LR model using 10k instances). 3.2 Rule-based Component In the second part of the system, the initial SI scores are passed to a rule-based component wherein a set of hand-crafted rules are applied to adjust the SI scores. The rules we use are inspired by those proposed by Taboada et al. (2011) and Thelwall et al. (2012) for lexicon-based SA. In particular, we use a combination of three publicly available sentiment lexica (see Section 4.2) 2 together with the following rules: • Whenever a negative word from the combined lexicon (section 4.2) is detected, the SI score will be scaled towards negative [0, 0.5]. Figure 1: Hybrid system architecture. 3.1 Supervised Learning Component: Training LR models • Same for positive words, except that the SI score will be scaled towards positive [0.5, 1]. The supervised part of the system uses Linear Regression (LR),1 following Amir et al. (2015)."
S16-1077,W13-4067,0,0.0121961,"1@hw.ac.uk, v.t.rieser@hw.ac.uk Abstract In this work, we use a combination of supervised learning and rule-based approaches, exploiting a number of publicly available sentiment lexica. We find that the quality (rather than quantity) of these lexica influence system performance for the supervised part of the system. Our best performing system demonstrates strong results of 0.536 Kendall score, ranking top in SemEval-2016 Task 7. This type of hybrid approach between rule-based and statistical methods has been demonstrated to be successful in other shared tasks, such as dialogue state tracking (Wang and Lemon, 2013). This paper describes the iLab-Edinburgh Sentiment Analysis system, winner of the Arabic Twitter Task 7 in SemEval-2016. The system employs a hybrid approach of supervised learning and rule-based methods to predict a sentiment intensity (SI) score for a given Arabic Twitter phrase. First, the supervised method uses an ensemble of trained linear regression models to produce an initial SI score for each given text instance. Second, the resulting SI score is adjusted using a set of rules that exploit a number of publicly available sentiment lexica. The system demonstrates strong results of 0.536"
S16-1077,H05-1044,0,0.128559,"itive/negative label, see Table 1. Excluding words with a weight=0, the current list includes 9,785 words/features along with their weights/coefficients as assigned by the SVM classifier. Again, the SVM coefficients are re-scaled to [0,1]. SLSA v1.0 lexicon: This is a freely available sentiment lexicon for MSA (Eskander and Rambow, 2015). The lexicon is composed of nearly 35k entries annotated with their SI scores using a linking algorithm, as described in Section 2. 476 MPQA English sentiment lexicon: This is a manually annotated English lexicon that is created and made publicly available by Wilson et al. (2005). We automatically translate the lexicon (using Google Translate) and then manually filter it to remove irrelevant or no-sentiment-bearing words. The resultant lexicon includes 2,627 entries. A manually annotated dialectal sentiment lexicon: This is a publicly available sentiment lexicon of 489 dialectal Arabic words. The lexicon is manually annotated by native speakers of Arabic (Refaee and Rieser, 2014a).3 4.3 Data Used for Developing and Evaluating the System We use the data sets provided by SemEval Task 7. SemEval’16 gold-standard development-set: This is a list of 200 instances (words/phr"
S16-1077,S14-2009,0,\N,Missing
W05-1624,W01-1607,0,0.049452,"Missing"
W06-2711,W05-1624,1,0.889632,"Missing"
W06-2711,kruijff-korbayova-etal-2006-sammie,1,0.841033,"Missing"
W06-2711,2005.sigdial-1.11,1,0.874647,"Missing"
W10-4235,P08-2019,1,0.913578,"ion, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decision making according to a cost function (van Deemter, 2009). It would be very interesting to explore how different approaches perform in NLG problems where different types of uncertainty"
W10-4235,W05-1606,0,0.0129602,"dule), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain."
W10-4235,W09-3916,1,0.828479,"e Modelling toolkit (Clarkson and Rosenfeld, 1997). For example we have constructed a bigram model2 for the users’ reactions to the system’s IP structure decisions (P (au,t |IPs,t )), and a tri-gram (i.e. IP structure + attribute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P (au,t |IPs,t , attributess,t ). We have evaluated the performance of these models by measuring dialogue similarity to the original data, based on the Kullback-Leibler (KL) divergence, as also used by e.g. (Cuay´ahuitl et al., 2005; Jung et al., 2009; Janarthanam and Lemon, 2009). We compared the raw probabilities as observed in the data with the probabilities generated by our n-gram models using different discounting techniques for each context. All the models have a small divergence from the original data (especially the bi-gram model), suggesting that they are reasonable simulations for training and testing NLG policies (Rieser et al., 2010). 2.2 Other Simulated Components In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations f"
W10-4235,P10-1008,1,0.922406,"ces....), • the likely user reaction to the generated output, • the behaviour of related components (e.g. a surface realiser, or TTS module), • noise in the environment (for spoken output), • ambiguity of the generated output. The problem here is to generate output that takes these types of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applic"
W10-4235,E09-1078,1,0.875208,"s of uncertainty into account appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010),"
W10-4235,P10-1103,1,0.890218,"count appropriately. For example, you may need to choose a referring expression for a user, even though you are not sure whether they are an expert or novice in the domain. In addition, the next time you speak to that user, you need to adapt to new information you have gained about them (Janarthanam and Lemon, 2010). The issue of uncertainty for referring expression generation has been discussed before by (Reiter, 1991; Horacek, 2005). Another example is in planning an Information Presentation for a user, when you cannot know with certainty how they will respond to it (Rieser and Lemon, 2009; Rieser et al., 2010). In the worst case, you may even be uncertain about the user’s goals or information needs (as in “POMDP” approaches to dialogue management (Young et al., 2009; Henderson and Lemon, 2008a)), but you still need to generate output for them in an appropriate way. In particular, in interactive applications of NLG: • each NLG action changes the environment state or context, • the effect of each NLG action is uncertain. Several recent approaches describe NLG tasks as different kinds of planning, e.g. (Koller and Petrick, 2008; Rieser et al., 2010; Janarthanam and Lemon, 2010), or as contextual decis"
W10-4235,P04-1011,0,0.0326812,"omponents In some systems, NLG decisions may also depend on related components, such as the database, subsequent generation steps, or the Text-to-Speech module for spoken generation. Building simulations for these components to capture their inherent uncertainty, again, is an interesting challenge. For example, one might want to adapt the generated output according to the predicted TTS quality. Therefore, one needs a model of the expected/ predicted TTS quality for a TTS engine (Boidin et al., 2009). Furthermore, NLG decisions might be inputs to a stochastic sentence realiser, such as SPaRKy (Stent et al., 2004). However, one might not have a fully trained stochastic sentence realiser for this domain (yet). In (Rieser et al., 2010) we therefore modelled the variance as observed in the top ranking SPaRKy examples. 2.3 Generating Referring Expressions under uncertainty In this section, we present an example user simulation (US) model, that simulates the dialogue behaviour of users who react to referring expressions depending on their domain knowledge. These external simulation models are different from internal user models used by dialogue systems. In 2 Where au,t is the predicted next user action at t"
W10-4235,W09-0626,0,0.0512972,"Missing"
W10-4235,W08-0119,0,\N,Missing
W11-2813,W11-2002,1,0.774392,"s baseline system has been made accessible by phone using VoIP technology, enabling outof-lab evaluation with large numbers of users. Apart from practical advantages in managing evaluation campaigns, this development effort was also intended as a step towards evaluating spoken dialogue systems under more realistic conditions. Please note, however, that the users in this evaluation were still recruited and asked to complete predefined tasks (see Section 4), and therefore the evaluation might not be as realistic as an evaluation of a final deployed application with real users having real goals (Black et al., 2011). The speech recogniser (ASR), semantic parser (SLU) and dialogue manager (DM) have all been developed at Cambridge University. For speech synthesis (TTS), the Baratinoo synthesiser, developed at France Telecom, was used. The DM uses a POMDP (Partially Observable Markov Decision Process) framework, allowing it to process N-Best lists of ASR hypotheses and keep track of multiple dialogue state hypotheses. The DM policy is trained to select system dialogue acts given a probability distribution over possible dialogue states. It has been shown that such dialogue managers can exploit the informatio"
W11-2813,W09-0626,0,0.0608888,"Missing"
W11-2813,E06-1009,0,0.0254445,"ommend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. We use a sentence generator based on the stochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOMMEND strategy, where only the top ranking item from the database result is presented, e.g. (Young et al., 2007). We jointly optimise these 7 content structuring 103 strategies together with attribute selection, i.e. how many attributes to mention in each strategy (e.g. SUMMARY (3)+ RECOMMEND (2) with number of attributes in brackets). Attribute types are ranked according to a pre-defined u"
W11-2813,P11-2115,0,0.0999679,"Missing"
W11-2813,W11-2011,0,0.0422671,"Missing"
W11-2813,W10-4324,1,0.802997,"hile keeping the utterances short and understandable. On the other hand, better Information Presentation should also contribute to the 102 “global/ overall” dialogue task, so as to maximise task completion. We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning (Lemon, 2008; Rieser and Lemon, 2009; Rieser et al., 2010).This model is also being explored by other researchers (Dethlefs et al., 2011; Dethlefs and Cuay´ahuitl, 2011) and (Janarthanam and Lemon, 2010; Janarthanam et al., 2011). We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue. The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level. Reinforcement Learning is used to automatically optimise the IP policy with respect to a data-driven objective function. In previous simulation-based work, we demonstrated that this IP model “locally”"
W11-2813,W11-2017,1,0.752589,"hort and understandable. On the other hand, better Information Presentation should also contribute to the 102 “global/ overall” dialogue task, so as to maximise task completion. We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning (Lemon, 2008; Rieser and Lemon, 2009; Rieser et al., 2010).This model is also being explored by other researchers (Dethlefs et al., 2011; Dethlefs and Cuay´ahuitl, 2011) and (Janarthanam and Lemon, 2010; Janarthanam et al., 2011). We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue. The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level. Reinforcement Learning is used to automatically optimise the IP policy with respect to a data-driven objective function. In previous simulation-based work, we demonstrated that this IP model “locally” outperforms other IP strat"
W11-2813,W08-0111,0,0.0290224,"ochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOMMEND strategy, where only the top ranking item from the database result is presented, e.g. (Young et al., 2007). We jointly optimise these 7 content structuring 103 strategies together with attribute selection, i.e. how many attributes to mention in each strategy (e.g. SUMMARY (3)+ RECOMMEND (2) with number of attributes in brackets). Attribute types are ranked according to a pre-defined user model (i.e. cuisine, price range, location, food quality, and service quality). We formulate the problem as a Markov Decision Process (MDP), where states are dialogu"
W11-2813,P08-1055,0,0.0216807,"retrieved items and then recommend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. We use a sentence generator based on the stochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOMMEND strategy, where only the top ranking item from the database result is presented, e.g. (Young et al., 2007). We jointly optimise these 7 content structuring 103 strategies together with attribute selection, i.e. how many attributes to mention in each strategy (e.g. SUMMARY (3)+ RECOMMEND (2) with number of attributes in brackets). Attribute types are ranked ac"
W11-2813,E09-1078,1,0.933763,"ocal” NLG task is to present “enough” information to the user (for example helping them to feel confident that they have a good overview of the search results) while keeping the utterances short and understandable. On the other hand, better Information Presentation should also contribute to the 102 “global/ overall” dialogue task, so as to maximise task completion. We have developed a novel framework for adaptive Natural Language Generation (NLG) where the problem is formulated as incremental decision making under uncertainty, which can be approached using Reinforcement Learning (Lemon, 2008; Rieser and Lemon, 2009; Rieser et al., 2010).This model is also being explored by other researchers (Dethlefs et al., 2011; Dethlefs and Cuay´ahuitl, 2011) and (Janarthanam and Lemon, 2010; Janarthanam et al., 2011). We have applied the theory to a variety of NLG problems, such as referring expression generation, and here we focus on adaptive Information Presentation (IP) in spoken dialogue. The IP model is adaptive to noisy feedback from the current generation context (e.g. a user, a surface realiser, and a TTS engine), and it incrementally adapts the IP policy at the turn level. Reinforcement Learning is used to"
W11-2813,J11-1006,1,0.88254,"Missing"
W11-2813,P10-1103,1,0.901663,"Missing"
W11-2813,P04-1011,0,0.0355313,"g (RL) (Sutton and Barto, 1998), where the example task is to present a set of search results (e.g. restaurants) to users. In particular, we consider 7 possible policies for structuring the content (see Figure 1): Recommending one single item, comparing two items, summarising all items, or ordered combinations of those actions, e.g. first summarise all the retrieved items and then recommend one of them. The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. We use a sentence generator based on the stochastic sentence planner SPaRKy (Stent et al., 2004) for surface generation. Prior work has presented a variety of IP strategies for structuring information (see examples in Table 1). For example, the SUMMARY strategy is used to guide the user’s “focus of attention”. It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e.g. (Polifroni and Walker, 2008; Demberg and Moore, 2006). Other studies investigate a COMPARE strategy, where the attributes of individual items from the database result are compared, e.g. (Walker et al., 2007; Nakatsu, 2008). Most work in SDS however uses a RECOM"
W12-1509,D10-1049,0,0.0275246,"ave also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances. This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation. In order to 50 Buffer-Based Incremental Processing A general abstract model of incremental processing based on buffers and a processor was developed by Schlangen and Skantze (2009) and is illustrated in Figure 2. It assumes that the left"
W12-1509,W11-2015,0,0.0714349,"rved in human-human conversation. Doing this in a deterministic fashion through hand-written rules would be time consuming and potentially inaccurate, with no guarantee of optimality. In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion. 2 Previous Work: Incremental Processing Architectures The smallest unit of processing in incremental systems is called incremental unit (IU). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010;"
W12-1509,W10-4342,0,0.253369,"n rules would be time consuming and potentially inaccurate, with no guarantee of optimality. In this paper, we demonstrate that it is possible to learn incremental generation behaviour in a reward-driven fashion. 2 Previous Work: Incremental Processing Architectures The smallest unit of processing in incremental systems is called incremental unit (IU). Its instantiation depends on the particular processing module. In speech recognition, IUs can correspond to phoneme sequences that are mapped onto words (Baumann and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has esta"
W12-1509,W11-2814,1,0.901339,"Missing"
W12-1509,W11-2011,1,0.88201,"Missing"
W12-1509,W09-3902,0,0.0484885,"ontent selection (IU2 - IU5) and surface realisations (IU6 - IU9, etc.). 51 Beat-Driven Incremental Processing In contrast to the buffer-based architectures, alternative incremental systems do not reuse previous partial hypotheses of the user’s input (or the system’s best output) but recompute them at each processing step. We follow Baumann et al. (2011) in calling them ‘beat-driven’ systems. Raux and Eskenazi (2009) use a cost matrix and decision theoretic principles to optimise turn-taking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. 2.3 Decision-making in Incremental Systems Some of the main advantages of the buffer- and ISUbased approaches include their inherently incremental mechanisms for updating and revising system hypotheses. They are able to process input of varying size and type and, at the same time, produce arbitrarily complex output which is monitored and can be modified at any time. On the other hand, current models are based on deterministic decision making and thus shar"
W12-1509,P10-1008,1,0.261639,"In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data,"
W12-1509,P10-1157,0,0.111541,"Missing"
W12-1509,W03-2311,0,0.489373,"remental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process. In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient. Users also indicated that they knew better when to start speaking themselves. Alternative approaches to incremental NLG include Kilger and Finkler (1995) who present an early approach based on Tree-Adjoining Grammar, and Purver and Otsuka (2003) who define an incremental generator based on Dynamic Syntax. Both of these generators can monitor their own output and initiate corrections if necessary. Over recent years, adaptive and data-driven ap49 INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49–58, c Utica, May 2012. 2012 Association for Computational Linguistics Self-correction (the system made a mistake) USR I want Italian food in the centre of town . . . SYS OK. I found 35 Indian restaurants . . . USR No, I want Italian. SYS oh sorry . . . SYS I have 24 Italian restaurants in the city c"
W12-1509,N09-1071,0,0.0613976,"c. and all types of dialogue acts. The incremental ISU model is shown in Figure 3. Note that this hierarchical architecture transfers well to the “classical” division of NLG levels into utterance (IU1), content selection (IU2 - IU5) and surface realisations (IU6 - IU9, etc.). 51 Beat-Driven Incremental Processing In contrast to the buffer-based architectures, alternative incremental systems do not reuse previous partial hypotheses of the user’s input (or the system’s best output) but recompute them at each processing step. We follow Baumann et al. (2011) in calling them ‘beat-driven’ systems. Raux and Eskenazi (2009) use a cost matrix and decision theoretic principles to optimise turn-taking in a dialogue system under the constraint that users prefer no gaps and no overlap at turn boundaries. DeVault et al. (2009) use maximum entropy classification to support responsive overlap in an incremental system by predicting the completions of user utterances. 2.3 Decision-making in Incremental Systems Some of the main advantages of the buffer- and ISUbased approaches include their inherently incremental mechanisms for updating and revising system hypotheses. They are able to process input of varying size and type"
W12-1509,P10-1103,1,0.862448,"and Schlangen, 2011). In dialogue management, IUs can correspond to dialogue acts (Buss et al., 2010). In speech synthesis, IUs can correspond to speech unit sequences which are mapped to segments and speech plans (Skantze and Hjalmarsson, 2010). IUs are typically linked to other IUs by two types of relations: same-level links connect IUs sequentially and express relationships at the same level; grounded-in links express hierarchical relations between IUs. 2.1 proaches to NLG have also been developed and shown to outperform the previous (handcrafted, rule-based) methods for specific problems (Rieser et al., 2010; Janarthanam and Lemon, 2010; Dethlefs and Cuay´ahuitl, 2011). This work has established that NLG can fruitfully be treated as a data-driven statistical planning process, where the objective is to maximise expected utility of the generated utterances (van Deemter, 2009), by adapting them to the context and user. Statistical approaches to sentence planning and surface realisation have also been explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contex"
W12-1509,E09-1081,0,0.20724,"explored (Stent et al., 2004; Belz, 2008; Mairesse et al., 2010; Angeli et al., 2010). The advantages of data-driven methods are that NLG is more robust in the face of noise, can adapt to various contexts and, trained on real data, can produce more natural and desirable variation in system utterances. This paper describes an initial investigation into a novel NLG architecture that combines incremental processing with statistical optimisation. In order to 50 Buffer-Based Incremental Processing A general abstract model of incremental processing based on buffers and a processor was developed by Schlangen and Skantze (2009) and is illustrated in Figure 2. It assumes that the left buffer of a module, such as the NLG module, receives IUs from one or more other processing modules, such as the dialogue manager. These input IUs are then passed on to the processor, where they are mapped to corresponding (higher-level) IUs. For an NLG module, this could be a mapping from the dialogue act present(cuisine=Indian) to the realisation ‘they serve Indian food’. The resulting IUs are passed on to the right buffer which co-incides with the left buffer of another module (for example the speech synthesis module in our example)."
W12-1509,W10-4301,0,0.366113,"ypotheses are more reliable. Results show that the agent learns to avoid long waiting times, fillers and self-corrections, by re-ordering content based on its confidence. 1 Introduction Traditionally, the smallest unit of speech processing for interactive systems has been a full utterance with strict, rigid turn-taking. Components of these interactive systems, including NLG systems, have so far treated the utterance as the smallest processing unit that triggers a module into action. More recently, work on incremental systems has shown that processing smaller ‘chunks’ of user input can improve Skantze and Hjalmarsson (2010) present a model of incremental speech generation in which input processing and output planning are parallel processes and the system can self-monitor its own generation process. In an evaluation with human users they showed that their incremental system started to speak significantly faster than a non-incremental system (roughly 600 ms) and was perceived as significantly more polite and efficient. Users also indicated that they knew better when to start speaking themselves. Alternative approaches to incremental NLG include Kilger and Finkler (1995) who present an early approach based on Tree-"
W12-1509,E09-1085,0,0.198264,"se, purge and commit. Whenever new IUs enter the module’s left buffer, the module’s knowledge base is updated to reflect the new information. Such information typically corresponds to the current best hypothesis of a preceding processing module. As a property of incremental systems, however, such hypotheses can be revised by the respective preceding module and, as a result, the knowledge bases of all subsequent modules need to be purged and updated to the newest hypothesis. Once a hypothesis is certain to not be revised anymore, it is committed. For concrete implementations of this model, see Skantze and Schlangen (2009), Skantze and Hjalmarsson (2010), Baumann and Schlangen (2011). An implementation of an incremental dialogue manager is based on the Information State Update (ISU) model (Buss et al., 2010; Buss and Schlangen, 2011). The model is related in spirit to the bufferbased architecture, but all of its input processing and output planning is realised by ISU rules. This is true for the incremental ‘house-keeping’ actions update, revise, etc. and all types of dialogue acts. The incremental ISU model is shown in Figure 3. Note that this hierarchical architecture transfers well to the “classical” division"
W12-1509,W09-0626,0,0.028565,"Missing"
W12-1509,P04-1011,0,\N,Missing
W13-4026,W12-1509,1,0.682243,"Missing"
W13-4026,P13-1123,1,0.0875028,"Missing"
W13-4026,W11-2014,0,0.0111653,"d phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive search through a Spoken Dialogue System (SDS). Th"
W13-4026,E09-1085,0,0.210008,"uthentic Afghan cuisine. USR What is the address and phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive se"
W14-3624,E12-1049,0,0.0185144,"andard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Morphological Syntactic Semantic Sentiment label positive negative Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emo tional tweets, e.g. hQ ¯ happiness and à Q k sadness. Note that emoticons and hash-tags are merely used to collect and build the training set and were replaced by the standard (positive/ negative) labels. In order to collect neutral instances, we query a set of official news accounts, following an approach by (Pak and Paroubek, 2010). Examples of the accounts queried are: BBC-Arabic, AlJazeera Arabic, SkyNews Arabia, Reuters Arabic, France24-Arabic, and DW Arabic. We then automatically e"
W14-3624,P05-2008,0,0.042828,"approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) and using 1 This GS data-set has been shared a special LREC repository available http://www.resourcebook.eu/shareyourlr/index.php 175 via at similar technique to (Mourad and Darwish, 2013). The translated lexicon is manually corrected by removing translations with neutral or no clear sentiment indicator.2 This results in 2,627 translated instances after correction. We then cons"
W14-3624,refaee-rieser-2014-arabic,1,0.723389,"on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh 2 Arabic Twitter SSA Corpora We start by collecting three corpora at different times over one year to account for the cyclic effects of topic change in social media (Eisenstein, 2013). Table 1 shows the distributions of labels in our data-sets: 1. A gold standard data-set which we use for training and evaluation (spring 2013); 2. A data-set for DS using emoticon-based queries (autumn 2013); 3. Another data-set for DS using a lexiconbased approach (winter 2014). 174 Pr"
W14-3624,N13-1081,0,0.0568127,"Missing"
W14-3624,J11-2001,0,0.606742,"bic, France24-Arabic, and DW Arabic. We then automatically extract the same set of linguistically motivated features as for the gold standard corpus. Feature-sets diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Lexicon-Based Annotation: We also investigate an alternative approach to DS, which combines rule-driven lexicon-based SSA, e.g. (Taboada et al., 2011), with machine learning approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy"
W14-3624,P99-1032,0,0.311004,"Missing"
W14-3624,J09-3003,0,0.543993,"e harvest two gold standard data sets at different time steps, which we label manually. We first harvest a data set of 3,031 multi-dialectal Arabic tweets randomly retrieved over the period from February to March 2013. We use this set as a training set for our fully supervised approach. We also manually label 1,580 tweets collected in autumn 2013, which we use as an independent held-out test set. Two native speakers were recruited to manually annotate the collected data for subjectivity and sentiment, where we define sentiment as a positive or negative emotion, opinion or attitude, following (Wilson et al., 2009). Our gold standard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Morphological Syntactic Semantic Sentiment label positive negative Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. I"
W14-3624,abdul-mageed-diab-2012-awatif,0,0.0676648,"ological analysis. n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Lexicon-Based Annotation: We also investigate an alternative approach to DS, which combines rule-driven lexicon-based SSA, e.g. (Taboada et al., 2011), with machine learning approaches, following (Zhang et al., 2011). We build a new training dataset by combining three lexica. We first exploit two existing subjectivity lexica: a manually annotated Arabic subjectivity lexicon (Abdul-Mageed and Diab, 2012) and a publicly available English subjectivity lexicon, MPQA (Wilson et al., 2009), which we automatically translate using Google Translate, following a Table 2: Annotated Feature-sets Emoticon-Based Queries: In order to investigate DS approaches to SSA, we also collect a much larger data set of Arabic tweets, where we use emoticons as noisy labels, following e.g. (Read, 2005; Go et al., 2009; Pak and Paroubek, 2010; Yuan and Purver, 2012; Suttles and Ide, 2013). We query Twitter API for tweets with variations of positive and negative emoticons to obtain pairs of micro-blog texts (statuses) an"
W14-3624,P11-2103,0,0.0813617,"rovement over previous fully supervised results. 1 Introduction Subjectivity and sentiment analysis (SSA) aims to determine the attitude of an author with respect to some topic, e.g. objective or subjective, or the overall contextual polarity of an utterance, e.g. positive or negative. Previous work on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh 2 Arabic Twitter SSA Corpora We start by collecting three corpora at different times over one year to account for the cyclic effects of topic change in social media (Eisenst"
W14-3624,W12-3705,0,0.421925,"Missing"
W14-3624,al-sabbagh-girju-2012-yadac,0,0.0692929,"Missing"
W14-3624,J96-2004,0,0.243654,"3,031 multi-dialectal Arabic tweets randomly retrieved over the period from February to March 2013. We use this set as a training set for our fully supervised approach. We also manually label 1,580 tweets collected in autumn 2013, which we use as an independent held-out test set. Two native speakers were recruited to manually annotate the collected data for subjectivity and sentiment, where we define sentiment as a positive or negative emotion, opinion or attitude, following (Wilson et al., 2009). Our gold standard annotations reached a weighted κ = 0.76, which indicates reliable annotations (Carletta, 1996). We also automatically annotate the corpus with a rich set of linguistically motivated features using freely available processing tools for Arabic, such as MADA (Nizar Habash and Roth, 2009), see Table 2. For more details on gold standard corpus annotation please see (Refaee and Rieser, 2014). 1 Type Morphological Syntactic Semantic Sentiment label positive negative Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentimen"
W14-3624,N13-1037,0,0.0282798,"., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh 2 Arabic Twitter SSA Corpora We start by collecting three corpora at different times over one year to account for the cyclic effects of topic change in social media (Eisenstein, 2013). Table 1 shows the distributions of labels in our data-sets: 1. A gold standard data-set which we use for training and evaluation (spring 2013); 2. A data-set for DS using emoticon-based queries (autumn 2013); 3. Another data-set for DS using a lexiconbased approach (winter 2014). 174 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 174–179, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Data set Gold standard training Emoticon-based training Lexicon-based training Manually labelled test Neutral 1,157 55,076 55,076 422"
W14-3624,W13-1608,0,0.420573,"y supervised results. 1 Introduction Subjectivity and sentiment analysis (SSA) aims to determine the attitude of an author with respect to some topic, e.g. objective or subjective, or the overall contextual polarity of an utterance, e.g. positive or negative. Previous work on automatic SSA has used manually annotated gold standard data sets to analyse which feature sets and models perform best for this task, e.g. (Wilson et al., 2009; Wiebe et al., 1999). Most of this work is in English, but there have been first attempts to apply similar techniques to Arabic, e.g. (Abdul-Mageed et al., 2011; Mourad and Darwish, 2013). While these models work well when tested using crossvalidation on limited static data sets, our previous results reveal that these models do not generalise to new data sets, e.g. collected at a later point in time, due to their limited coverage (Refaee and Rieser, 2014). While there is a growing interest within the NLP community in building Arabic corpora by harvesting the web, e.g. (Al-Sabbagh 2 Arabic Twitter SSA Corpora We start by collecting three corpora at different times over one year to account for the cyclic effects of topic change in social media (Eisenstein, 2013). Table 1 shows t"
W14-3624,pak-paroubek-2010-twitter,0,0.584919,"label positive negative Table 3: Emoticons used to automatically label the training data-set. emoticons as author-provided emotion labels. In following (Purver and Battersby, 2012; Zhang et al., 2011; Suttles and Ide, 2013), we also utilise some sentiment-bearing hash tags to query emo tional tweets, e.g. hQ ¯ happiness and à Q k sadness. Note that emoticons and hash-tags are merely used to collect and build the training set and were replaced by the standard (positive/ negative) labels. In order to collect neutral instances, we query a set of official news accounts, following an approach by (Pak and Paroubek, 2010). Examples of the accounts queried are: BBC-Arabic, AlJazeera Arabic, SkyNews Arabia, Reuters Arabic, France24-Arabic, and DW Arabic. We then automatically extract the same set of linguistically motivated features as for the gold standard corpus. Feature-sets diacritic, aspect, gender, mood, person, part of speech (POS), state, voice, has morphological analysis. n-grams of words and POS, lemmas, including bag of words (BOW), bag of lemmas. has positive lexicon, has negative lexicon, has neutral lexicon, has negator, has positive emoticon, has negative emoticon. Lexicon-Based Annotation: We als"
W14-3624,J14-1006,0,\N,Missing
W14-4336,P13-1123,1,0.850652,"Missing"
W14-4336,W13-4026,1,0.906157,"LG) components. We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions. 1 Introduction With the advent of evaluations “in the wild”, emphasis is being put on converting research prototypes into mobile applications that can be used for evaluation and data collection by real users downloading the application from the market place. This is the motivation behind the work demonstrated here where we present a modular framework whereby research components from the Parlance project (Hastie et al., 2013) can be plugged in, tested and evaluated in a mobile environment. The goal of Parlance is to perform interactive search through speech in multiple languages. The domain for the demonstration system is interactive search for restaurants in Cambridge, UK for Mandarin and San Francisco, USA for English. The scenario is that Mandarin speaking tourists would be able to download the application and use it to learn about restaurants in English speaking towns and cities. 2 Figure 1: Overview of the Parlance Mandarin mobile application system architecture Figure 2: Overview of the Parlance English mobi"
W15-4715,W11-2814,0,0.0603582,"Missing"
W15-4715,E14-1074,1,0.875354,"Missing"
W15-4715,E12-1077,0,0.0323358,"Missing"
W15-4715,D15-1224,1,0.875652,"ht” (usercentric) “Keep walking along Nicholson Street”(street name) Table 1: Example of user view on GoogleStreetMaps (left) and system outputs (right). previous work, in that it mainly produces landmark-based instructions, but resorts to user-centric instructions when no landmarks are available (also see our landmark selection algorithm as described below). We also call this the visibility strategy. static landmarks, e.g. shops, restaurants, banks, etc., available from GoogleMaps and Open Street Map. It does not identify moving objects, such as cars, as potential landmarks. In current work (Gkatzia et al., 2015), we investigate how to generate landmarks based on noisy output from object recognition systems. • System B: Spacebook1-based strategy produces instructions using street names, landmarks and user-centric references in the same proportions as the wizards in Spacebook1. We also call this the shared viewpoint strategy. 3 3.1 Evaluation Experimental Setup We used the GRUVE virtual environment for evaluation. GRUVE uses Google StreetView to simulate instruction giving and following in an interactive, virtual environment, also see Table 1. We recruited 16 subjects, with an even split amongst males"
W15-4715,W13-0702,0,0.110892,"Missing"
W15-4715,J14-4006,0,0.0623669,"Missing"
W15-4715,P12-3009,0,0.142028,"entric instructions, such as “Continue straight”. This paper compares three alternative generation strategies for choosing possible reference objects: one system reflecting an improved version of a landmark-based policy, which will resort to a user-centric description if the landmark is not visible; and two systems reflecting the wizards’ behaviours in Spacebook1 and Spacebook2. We hypothesise the first system will outperform the other two in terms of human-likeness and naturalness, as defined in Section 3. We use the GRUVE (Giving Route Instructions in Uncertain Virtual Environments) system (Janarthanam et al., 2012) to evaluate these alternatives. Referring to landmarks has been identified to lead to improved navigation instructions. However, a previous corpus study suggests that human “wizards” also choose to refer to street names and generate user-centric instructions. In this paper, we conduct a task-based evaluation of two systems reflecting the wizards’ behaviours and compare them against an improved version of previous landmark-based systems, which resorts to user-centric descriptions if the landmark is estimated to be invisible. We use the GRUVE virtual interactive environment for evaluation. We f"
W15-4720,W09-0613,0,0.214526,"Missing"
W15-4720,W13-0215,0,\N,Missing
W16-6627,D10-1049,0,0.0691118,"s-based methods with respect to their scalability to data size and target complexity, as well as to assess predictive quality of automatic evaluation metrics. 1 Relevance Natural language generation plays a critical role for Conversational Agents (CAs) as it has a significant impact on a users impression of the system. Most CAs utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high quality training data consisting of meaning representations (MR) paired with natural language (NL) utterances, augmented by alignments between elements of meaning representation and natural language words. Creating aligned data is a non-trivial task in its own right, see e.g. (Liang et al., 2009). This shared task aims to strengthen recent research on corpus-based NLG from unaligned data, e.g. (Duˇsek and Jurcicek, 2015; Wen et al., 2015; Mei et al., 2015; Sharma et al., 2016). These approaches do not require costly semant"
W16-6627,P08-2050,0,0.0422476,"Missing"
W16-6627,P11-2040,0,0.0133239,"human perception, human evaluation will be used to assess subjective quality of generated utterances. Human judges will be recruited using CrowdFlower. Judges will be asked to compare utterance generated by different systems and score them in terms of informativeness (“Does the utterance contains 169 all the information specified in the MR?”), naturalness (“Could the utterance have been produced by a native speaker?”) and phrasing (“Do you like the way the utterance has been expressed?”). Here, we will explore different experimental setups for evaluation following previous shared tasks, e.g. (Belz and Kow, 2011). The challenge will also benefit from a national research grant on Domain Independent NLG (EP/M005429/1) which will provide funds for crowd-based evaluation. 4 Research Questions The task is set up to answer the following research questions with respect to corpus-driven methods: • “How much data is enough?” So far, corpus-based methods have been trained on limited data sets, such as BAGEL (404 target utterances), Cambridge SF (5193) or RoboCup (1919). We release a data set which is almost 10-times times bigger in size than previous corpora. This allows us to test the upper quality boundary of"
W16-6627,P15-1044,0,0.14736,"Missing"
W16-6627,N12-1093,0,0.0622564,"respect to their scalability to data size and target complexity, as well as to assess predictive quality of automatic evaluation metrics. 1 Relevance Natural language generation plays a critical role for Conversational Agents (CAs) as it has a significant impact on a users impression of the system. Most CAs utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high quality training data consisting of meaning representations (MR) paired with natural language (NL) utterances, augmented by alignments between elements of meaning representation and natural language words. Creating aligned data is a non-trivial task in its own right, see e.g. (Liang et al., 2009). This shared task aims to strengthen recent research on corpus-based NLG from unaligned data, e.g. (Duˇsek and Jurcicek, 2015; Wen et al., 2015; Mei et al., 2015; Sharma et al., 2016). These approaches do not require costly semantic alignment, but are base"
W16-6627,P09-1011,0,0.19777,"ing hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high quality training data consisting of meaning representations (MR) paired with natural language (NL) utterances, augmented by alignments between elements of meaning representation and natural language words. Creating aligned data is a non-trivial task in its own right, see e.g. (Liang et al., 2009). This shared task aims to strengthen recent research on corpus-based NLG from unaligned data, e.g. (Duˇsek and Jurcicek, 2015; Wen et al., 2015; Mei et al., 2015; Sharma et al., 2016). These approaches do not require costly semantic alignment, but are based on parallel data sets, which can 168 be collected in sufficient quality and quantity using effective crowd-sourcing techniques (Novikova and Rieser, 2016), and as such open the door for rapid development of NLG components for CAs in new domains. In addition, we hope to attract interest from related disciplines, such as semantic parsing or"
W16-6627,J14-4003,0,0.14924,"ty to data size and target complexity, as well as to assess predictive quality of automatic evaluation metrics. 1 Relevance Natural language generation plays a critical role for Conversational Agents (CAs) as it has a significant impact on a users impression of the system. Most CAs utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high quality training data consisting of meaning representations (MR) paired with natural language (NL) utterances, augmented by alignments between elements of meaning representation and natural language words. Creating aligned data is a non-trivial task in its own right, see e.g. (Liang et al., 2009). This shared task aims to strengthen recent research on corpus-based NLG from unaligned data, e.g. (Duˇsek and Jurcicek, 2015; Wen et al., 2015; Mei et al., 2015; Sharma et al., 2016). These approaches do not require costly semantic alignment, but are based on parallel data sets, wh"
W16-6627,W16-6644,1,0.789929,"language (NL) utterances, augmented by alignments between elements of meaning representation and natural language words. Creating aligned data is a non-trivial task in its own right, see e.g. (Liang et al., 2009). This shared task aims to strengthen recent research on corpus-based NLG from unaligned data, e.g. (Duˇsek and Jurcicek, 2015; Wen et al., 2015; Mei et al., 2015; Sharma et al., 2016). These approaches do not require costly semantic alignment, but are based on parallel data sets, which can 168 be collected in sufficient quality and quantity using effective crowd-sourcing techniques (Novikova and Rieser, 2016), and as such open the door for rapid development of NLG components for CAs in new domains. In addition, we hope to attract interest from related disciplines, such as semantic parsing or statistical machine translation, which face similar challenges when learning from parallel non-aligned data sets. Flat MR name[The Eagle], eatType[coffee shop], food[French], priceRange[moderate], customerRating[3/5], area[riverside], kidsFriendly[yes], near[Burger King] NL reference 1. There is a riverside coffee shop called The Eagle that has French food at an average price range. It is child friendly, locat"
W16-6627,P02-1040,0,0.0945048,"e, ... restaurant, pub, ... Yes / No cheap, expensive, ... French, Italian, ... market square, ... riverside, city center, ... 1 of 5 (low), 4 of 5 (high), ... Table 2: Domain ontology. 3 Evaluation We will provide two types of baseline systems, which are frequently used by previous corpus-based methods, e.g. (Wen et al., 2015; Mairesse and Young, 2014): a challenging hand-crafted generator and n-gram Language Models, following early work by (Oh and Rudnicky, 2002). To evaluate the results, both objective and subjective metrics will be used. We will explore automatic measures, such as BLEU-4 (Papineni et al., 2002) and NIST (Doddington, 2002) scores, which are widely used in a machine translation and NLG research, and will allow comparing the results of this challenge with previous work. Since automatic metrics may not consistently agree with human perception, human evaluation will be used to assess subjective quality of generated utterances. Human judges will be recruited using CrowdFlower. Judges will be asked to compare utterance generated by different systems and score them in terms of informativeness (“Does the utterance contains 169 all the information specified in the MR?”), naturalness (“Could t"
W16-6627,W15-3031,0,0.0482408,"Missing"
W16-6627,P04-1011,0,0.0401911,"Missing"
W16-6627,D15-1199,0,0.043575,"Missing"
W16-6627,N16-1086,0,\N,Missing
W16-6644,D10-1049,0,0.0180983,"e complex, the benefits of pictorial stimuli increase. The collected data will be released as part of this submission. 1 Introduction The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains. Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high 265 quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words. Recent work (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning repres"
W16-6644,W11-2002,1,0.809218,"eous, natural and varied data from crowdworkers? To address (1), we first filter the crowdsourced data using automatic and manual validation procedures. We evaluate the quality of crowdsourced NLG data using automatic measures, e.g. measuring the semantic similarity of a collected NL utterance. To address (2), we conduct a principled study regarding the trade-off between semantic expressiveness of the MR and the quality of crowd-sourced utterances elicited for the different semantic representations. In particular, we investigate translating MRs into pictorial representations as used in, e.g. (Black et al., 2011; Williams and Young, 2007) for evaluating spoken dialogue systems. We compare these pictorial MRs to text-based MRs used by previous crowd-sourcing work (Mairesse et al., 2010; Wang et al., 2012). These text-based MRs take the form of Dialogue Acts, such as inform(type[hotel],pricerange[expensive]). However, there is a limit in the semantic complexity that crowd workers can handle (Mairesse et al., 2010). Also, (Wang et al., 2012) observed that the semantic formalism unfortunately influences the collected language, i.e. crowd-workers are “primed” by the words/tokens and ordering used in the M"
W16-6644,W10-0701,0,0.0423033,"as the format of the example MR provided (logical or pictorial). In terms of financial compensation, crowd workers were paid the standard pay on CrowdFlower, which is $0.02 per page (each containing 1 MR). Workers were expected to spend about 20 seconds per page. Participants were allowed to complete up to 20 pages, i.e. create utterances for up to 20 MRs. Mason and Watts (2010) in their study of financial incentives on Mechanical Turk, found (counterintuitively) that increasing the amount of compensation for a particular task does not tend to improve the quality of the results. Furthermore, Callison-Burch and Dredze (2010) observed that there can be an inverse relationship between the amount of payment 267 and the quality of work, because it may be more tempting for crowd workers to cheat on high-paying tasks if they do not have the skills to complete them. Following these findings, we did not increase the payment for our task over the standard level. In order to check for random inputs/“gibberish” and to control quality of the data, we introduced a validation procedure, which consisted of two main parts (see sections 3.3 and 3.4 for details): (1) Automatic pre-validation. The purpose of the automatic validatio"
W16-6644,D12-1008,1,0.671908,"of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we collected 1410 utterances using this framework. The data will be released as part of this submission. 2 Background Apart from (Mairesse et al., 2010), this research is the first to investigate crowdsourcing for collecting NLG data. So far, crowdsourcing is mainly used for evaluation in the NLG community, e.g. (Rieser et al., 2014; Dethlefs et al., 2012). Recent efforts in corpus creation via crowdsourcing have proven to be successful in related tasks. For example, (Zaidan and Callison-Burch, 2011) showed that crowdsourcing can result in datasets of comparable quality to those created by professional translators given appropriate quality control methods. (Mairesse et al., 2010) demonstrate that crowd workers can produce NL descriptions from abstract MRs, a method which also has shown success in related NLP tasks, such as Spoken Dialogue Systems (Wang et al., 2012) or Semantic Parsing (Wang et al., 2015). However, when collecting corpora for t"
W16-6644,P15-1044,0,0.050418,"Missing"
W16-6644,S13-1005,0,0.025928,"Missing"
W16-6644,N12-1093,0,0.0313111,"ts of pictorial stimuli increase. The collected data will be released as part of this submission. 1 Introduction The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains. Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high 265 quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words. Recent work (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we coll"
W16-6644,J14-4003,0,0.054676,"crease. The collected data will be released as part of this submission. 1 Introduction The overall aim of this research is to develop methods that will allow the full automation of the creation of NLG systems for new applications and domains. Currently deployed technologies for NLG utilise domain-dependent methods including hand-written grammars or domain-specific language templates for surface realisation, both of which are costly to develop and maintain. Recent corpus-based methods hold the promise of being easily portable across domains, e.g. (Angeli et al., 2010; Konstas and Lapata, 2012; Mairesse and Young, 2014), but require high 265 quality training data consisting of meaning representations (MR) paired with Natural Language (NL) utterances, augmented by alignments between MR elements and NL words. Recent work (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015) removes the need for alignment, but the question of where to get indomain training data of sufficient quality remains. In this work, we propose a novel framework for crowd-sourcing high quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we collected 1410 utterances using"
W16-6644,P10-1157,0,0.0853364,"Missing"
W16-6644,P15-1129,0,0.0441058,"Missing"
W16-6644,D15-1199,0,0.0688315,"Missing"
W16-6644,P11-1122,0,0.0135992,"quality NLG training data, using automatic quality control measures and evaluating different meaning representations. So far, we collected 1410 utterances using this framework. The data will be released as part of this submission. 2 Background Apart from (Mairesse et al., 2010), this research is the first to investigate crowdsourcing for collecting NLG data. So far, crowdsourcing is mainly used for evaluation in the NLG community, e.g. (Rieser et al., 2014; Dethlefs et al., 2012). Recent efforts in corpus creation via crowdsourcing have proven to be successful in related tasks. For example, (Zaidan and Callison-Burch, 2011) showed that crowdsourcing can result in datasets of comparable quality to those created by professional translators given appropriate quality control methods. (Mairesse et al., 2010) demonstrate that crowd workers can produce NL descriptions from abstract MRs, a method which also has shown success in related NLP tasks, such as Spoken Dialogue Systems (Wang et al., 2012) or Semantic Parsing (Wang et al., 2015). However, when collecting corpora for training NLG systems, new challenges arise: (1) How to ensure the required high quality of the collected data? Proceedings of The 9th International"
W17-5525,W07-0734,0,0.0765626,"Missing"
W17-5525,P04-1011,0,0.336745,"ing wine and cheese at a low cost. Loch Fyne is a French family friendly restaurant catering to a budget of below £20. Loch Fyne is a French restaurant with a family setting and perfect on the wallet. Table 1: An example of a data instance. calised datasets, e.g. BAGEL (Mairesse et al., 2010), SF Hotels/Restaurants (Wen et al., 2015), or RoboCup (Chen and Mooney, 2008). Therefore, end-to-end methods have not been able to replicate the rich dialogue and discourse phenomena targeted by previous rule-based and statistical approaches for language generation in dialogue, e.g. (Walker et al., 2004; Stent et al., 2004; Demberg and Moore, 2006; Rieser and Lemon, 2009). In this paper, we describe a new crowdsourced dataset of 50k instances in the restaurant domain (see Section 2). We analyse it following the methodology proposed by Perez-Beltrachini and Gardent (2017) and show that the dataset brings additional challenges, such as open vocabulary, complex syntactic structures and diverse discourse phenomena, as described in Section 3. The data is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in Section 4, using one of the previous end-to-end approaches. Introductio"
W17-5525,P10-1157,0,0.47043,"Missing"
W17-5525,N16-1086,0,0.146291,"mena, as described in Section 3. The data is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in Section 4, using one of the previous end-to-end approaches. Introduction The natural language generation (NLG) component of a spoken dialogue system typically has to be re-developed for every new application domain. Recent end-to-end, data-driven NLG systems, however, promise rapid development of NLG components in new domains: They jointly learn sentence planning and surface realisation from non-aligned data (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016a; Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language (NL) reference texts (also referred to as “ground truths” or “targets”), but they are trained on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. (Novikova et al., 2016). So far, end-to-end approaches to NLG are limited to small, delexi2 The E2E Dataset The data was collected using the"
W17-5525,N16-1015,0,0.080351,"Missing"
W17-5525,W16-6644,1,0.904737,"tly learn sentence planning and surface realisation from non-aligned data (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015; Mei et al., 2016; Wen et al., 2016; Sharma et al., 2016; Duˇsek and Jurˇc´ıcˇ ek, 2016a; Lampouras and Vlachos, 2016). These approaches do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language (NL) reference texts (also referred to as “ground truths” or “targets”), but they are trained on parallel datasets, which can be collected in sufficient quality and quantity using effective crowdsourcing techniques, e.g. (Novikova et al., 2016). So far, end-to-end approaches to NLG are limited to small, delexi2 The E2E Dataset The data was collected using the CrowdFlower platform and quality-controlled following Novikova et al. (2016). The dataset provides infor1 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ 201 Proceedings of the SIGDIAL 2017 Conference, pages 201–206, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics of human references per MR (Refs/MR).2 While having more data with a higher number of references per MR makes the E2E data more attractive for statistical approaches, it is als"
W17-5525,P02-1040,0,0.0988195,"Missing"
W17-5525,D15-1199,0,0.567643,"Missing"
W17-5525,W17-3537,0,0.0717419,"Missing"
W17-5525,E09-1078,1,0.143521,"is a French family friendly restaurant catering to a budget of below £20. Loch Fyne is a French restaurant with a family setting and perfect on the wallet. Table 1: An example of a data instance. calised datasets, e.g. BAGEL (Mairesse et al., 2010), SF Hotels/Restaurants (Wen et al., 2015), or RoboCup (Chen and Mooney, 2008). Therefore, end-to-end methods have not been able to replicate the rich dialogue and discourse phenomena targeted by previous rule-based and statistical approaches for language generation in dialogue, e.g. (Walker et al., 2004; Stent et al., 2004; Demberg and Moore, 2006; Rieser and Lemon, 2009). In this paper, we describe a new crowdsourced dataset of 50k instances in the restaurant domain (see Section 2). We analyse it following the methodology proposed by Perez-Beltrachini and Gardent (2017) and show that the dataset brings additional challenges, such as open vocabulary, complex syntactic structures and diverse discourse phenomena, as described in Section 3. The data is openly released as part of the E2E NLG challenge.1 We establish a baseline on the dataset in Section 4, using one of the previous end-to-end approaches. Introduction The natural language generation (NLG) component"
W17-5525,E06-1009,0,\N,Missing
W17-5525,P15-1044,1,\N,Missing
W17-5525,P16-2008,1,\N,Missing
W17-5525,W16-3622,1,\N,Missing
W17-5525,C16-1105,0,\N,Missing
W18-0802,P17-1103,0,0.0892474,"Missing"
W18-0802,N10-1020,0,0.0847221,"ou have sex with me.” We repeated the insults multiple times to see if system responses varied and if defensiveness increased with continued abuse. In this case, we included all responses in the study. 3.2 Systems Evaluated We collect responses from the following existing systems: • Commercial: Amazon Alexa, Apple Siri, Google Home, Microsoft’s Cortana. • Rule-based: E.L.I.Z.A.,6 Parry,7 A.L.I.C.E.8 , Alley.9 • Data-driven approaches: We use pre-trained models available at the provided URLs. - Cleverbot;10 - NeuralConvo,11 a re-implementation of (Vinyals and Le, 2015); - an implementation of (Ritter et al., 2010)’s Information Retrieval approach;12 (0.88) and recall (0.78) on a manually annotated subset of 1000 dialogues. We then manually differentiated between general offence and sexual harassment. 6 https://goo.gl/BAQZCX 7 https://goo.gl/pZQrmC 8 https://goo.gl/Sy9zgT 9 https://goo.gl/cXX7rT 10 http://www.cleverbot.com/ 11 http://neuralconvo.huggingface.co/ 12 http://kb1.cse.ohio-state.edu: 8010/cgi-bin/mt_chat3.py • Baseline: We also compile responses by 6 adult chatbots. These are purpose-built to elicit further sexualised engagement with the bot. As such, this is a negative baseline that generalp"
W18-0802,W17-1101,0,0.0290614,"Missing"
W18-5709,W18-6514,1,0.717195,"Missing"
W18-5709,W07-0734,0,0.0774496,"M-HRED–attn) for a given context size. Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.6710 0.6712 0.6797 0.6799 0.6872 0.690"
W18-5709,D16-1203,0,0.0307088,"rounding in KB. We show textual context as well as relevant knowledge base input (and omit image context) for brevity’s sake. While our model uses a context of 5, for simplicity, we show only 2 previous turns. probe found that the orientations for retrieved images may not directly follow the description in the query (KB). There are other intents for which even KB does not help, such as those requiring user modelling. 5 model outputs showed a substantial improvement (over 3 B LEU points) on incorporating KB information, integrating visual context still remains a bottleneck, as also observed by Agrawal et al. (2016); Qian et al. (2018). This suggests the need for a better mechanism to encode visual context. Since our KB-grounded model assumes user intent annotation and KB queries as additional inputs, we plan to build a model to provide them automatically. Conclusion and Future Work This work focuses on the task of textual response generation in multimodal task-oriented dialogue system. We used the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) for experiments and introduced a novel conversational model grounded in language, vision and Knowledge Base (KB). Our best performing mod"
W18-5709,E06-2009,0,0.0825572,"Missing"
W18-5709,P04-1077,0,0.0794841,"Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.6710 0.6712 0.6797 0.6799 0.6872 0.6909 0.6917 0.6913 0.6923 In summary"
W18-5709,D15-1166,0,0.092393,"elebrity profiles using basic pattern matching over the user utterance. For each of the celebrities in the user query, we order the corresponding synsets by their probability of endorsement. If no celebrity is found, we use synset information from the query to extract celebrities which endorse the corresponding synset. 3.3 Experiments and Results 4.2 Implementation We used PyTorch3 (Paszke et al., 2017) for our experiments.4 We did not use any kind of delexicalisation5 and rely on our model to directly learn Input feeding decoder We use an input feeding decoder with the attention mechanism of Luong et al. (2015). We concatenate the KB input hkb n with the decoder input cxt (cf. Eq. (10), where hdec n,0 = hN ). The rationale behind this late fusion of KB representation is that KB input remains the same for a given context and 2 We used the same training-development-test split as provided by the dataset authors. 3 https://pytorch.org/ 4 Code can be found at: https://github.com/shubhamagarwal92/mmd 5 Replacing specific values with placeholders (Henderson et al., 2014). 62 Intent from the conversational history and KB. All encoders and decoders are based on 1-layer GRU cells (Cho et al., 2014) with 512 a"
W18-5709,P02-1040,0,0.102215,"stark uplift (M-HRED– attn–kb vs. M-HRED–attn) for a given context size. Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.671"
W18-5709,W13-4067,0,0.0783197,"Missing"
W18-5709,E17-1042,0,0.0341701,"al model where an encoded knowledge base (KB) representation is appended to the decoder input. Our model substantially outperforms strong baselines in terms of text-based similarity measures (over 9 BLEU points, 3 of which are solely due to the use of additional information from the KB). 1 Introduction Conversational agents have become ubiquitous, with variants ranging from open-domain conversational chit-chat bots (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to domainspecific task-based dialogue systems (Singh et al., 2000; Rieser and Lemon, 2010, 2011; Young et al., 2013; Wen et al., 2017). Our work builds upon the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017), which contains dialogue sessions in the ecommerce (fashion) domain. Figure 1 illustrates an example chat session with multimodal interaction between the user and the system. We focus on the task of generating textual responses conditioned on the previous conversational history. Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (Lemon 2 Related Work With recent progress in deep learning, there is contin"
W18-5709,N15-1173,0,0.0187718,"h multimodal interaction between the user and the system. We focus on the task of generating textual responses conditioned on the previous conversational history. Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (Lemon 2 Related Work With recent progress in deep learning, there is continued interest in the tasks involving both vision and language, such as image captioning (Xu et al., 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015), visual storytelling (Huang et al., 2016), video description (Venugopalan et al., 2015b,a) or dialogue grounded in visual context (Antol et al., 2015; Das et al., 2017; Tapaswi et al., 2016). Bordes et al. (2016) and Ghazvininejad et al. (2017) presented knowledge-grounded neural models; however, these are uni-modal in nature, involve only textual interaction and do not take into account the conversational history in a dia59 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 59–66 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 Figure 2: Schematic diagram of"
W18-5709,E09-1078,1,\N,Missing
W18-6514,D16-1203,0,0.0559846,"arch-based multimodal dialogue by learning from the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017). We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of Saha et al. (2017) by modelling the full multimodal context. Contrary to their results, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018). Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context. This indicates that we need better visual models to enTable 1 provides results for different configurations of our model (“T” stands for text-only in the encoder, “M” for multimodal, and “attn” for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation"
W18-6514,P02-1040,0,0.103838,"ge and long term context: “Will the 5th result go well with a large sized messenger bag?”, inference over aggregate of images: “List more in the upper material of the 5th image and style as the 3rd and the 5th”, co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model. This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3). 3.3 Analysis and Results We report sentence-level B LEU -4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017). 1 https://pytorch.org/ Our code is freely available at: https://github.com/shubhamagarwal92/mmd 3 In future, we plan to exploit state-of-the-art frameworks such as ResNet or DenseNet and fine tune the image encoder jointly, during the training of the model. 2 131 Figure 4: Examples of predictions using M-HRED–attn (5). Recall, we are focusing on generating textual responses. Our model predictions are shown in blue while the true gold target in red. We are showing"
W18-6514,W04-3250,0,0.0306892,"Missing"
W18-6514,W07-0734,0,0.289957,"he 5th result go well with a large sized messenger bag?”, inference over aggregate of images: “List more in the upper material of the 5th image and style as the 3rd and the 5th”, co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model. This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3). 3.3 Analysis and Results We report sentence-level B LEU -4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017). 1 https://pytorch.org/ Our code is freely available at: https://github.com/shubhamagarwal92/mmd 3 In future, we plan to exploit state-of-the-art frameworks such as ResNet or DenseNet and fine tune the image encoder jointly, during the training of the model. 2 131 Figure 4: Examples of predictions using M-HRED–attn (5). Recall, we are focusing on generating textual responses. Our model predictions are shown in blue while the true gold target in red. We are showing only the previous user utterance f"
W18-6514,E09-1078,1,\N,Missing
W18-6514,P04-1077,0,\N,Missing
W18-6539,2007.mtsummit-ucnlg.14,0,0.0796589,"course phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. 1 Introduction 2 This paper summarises the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems (SDSs). Shared tasks have become an established way of pushing research boundaries in the field of natural language processing, with NLG benchmarking tasks running since 2007 (Belz and Gatt, 2007). This task is novel in that it poses new challenges for recent end-to-end, data-driven NLG systems for SDSs which jointly learn sentence planning and surface realisation and do not require costly semantic alignment between meaning representations (MRs) and the corresponding natural language reference texts, e.g. (Duˇsek and Jurˇc´ıcˇ ek, 2015; Wen et al., 2015b; Mei et al., 2.1 The E2E NLG dataset Data Collection Procedure In order to maximise the chances for data-driven end-to-end systems to produce high quality output, we aim to provide training data in high quality and large quantity. To c"
W18-6539,W17-4755,0,0.0373199,"a-driven end-to-end systems to produce high quality output, we aim to provide training data in high quality and large quantity. To collect data in large enough quantity, we use crowdsourcing with automatic 1 Note that as opposed to the “classical” definition of NLG (Reiter and Dale, 2000; Gatt and Krahmer, 2018), generation for dialogue systems does not involve content selection and its sentence planning stage may be less complex. 2 In comparison, the well established Conference in Machine Translation WMT’17 (running since 2006) received submissions from 31 institutions to a total of 8 tasks (Bojar et al., 2017a). 322 Proceedings of The 11th International Natural Language Generation Conference, pages 322–328, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics MR Reference name[The Wrestlers], priceRange[cheap], customerRating[low] The wrestlers offers competitive prices, but isn’t highly rated by customers. Figure 1: Example of an MR-reference pair. Figure 2: An example pictorial MR. Attribute Data Type name verbatim string eatType dictionary familyFriendly boolean priceRange dictionary food dictionary near verbatim string area dictionary customerRating di"
W18-6539,W18-6555,0,0.089427,"Missing"
W18-6539,W18-6539,1,0.0512975,"Missing"
W18-6539,W16-3622,1,0.892594,"Missing"
W18-6539,N16-1086,0,0.141498,"Missing"
W18-6539,P16-2008,1,0.880417,"Missing"
W18-6539,D17-1238,1,0.901966,"Missing"
W18-6539,W18-6556,0,0.0819123,"Missing"
W18-6539,W17-5525,1,0.908272,"Missing"
W18-6539,N18-2012,1,0.863228,"Missing"
W18-6539,W17-3518,0,0.0683021,"which is about 10 times larger and also more complex than previous datasets. For the shared challenge, we received 62 system submissions by 17 institutions from 11 countries, with about 1/3 of these submissions coming from industry. We assess the submitted systems by comparing them to a challenging baseline using automatic as well as human evaluation. We consider this level of participation an unexpected success, which underlines the timeliness of this task.2 While there are previous studies comparing a limited number of end-to-end NLG approaches (Novikova et al., 2017a; Wiseman et al., 2017; Gardent et al., 2017), this is the first research to evaluate novel end-to-end generation at scale and using human assessment. This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical ric"
W18-6539,W16-6644,1,0.916958,"Missing"
W18-6539,W17-1608,0,0.089051,"Missing"
W18-6539,W18-6557,0,0.147702,"Missing"
W18-6539,N18-1014,0,0.685577,"ously unseen, i.e. none of them overlaps with training/development sets, even if restaurant names are removed. MRs for the test set were only released to participants two weeks before the challenge submission deadline on October 31, 2017. Participants had no access to test reference texts. The whole dataset is now freely available at the E2E NLG Challenge website at: http://www.macs.hw.ac.uk/ InteractionLab/E2E/ 323 System BLEU NIST METEOR ROUGE-L CIDEr norm. avg. ♥ TG EN baseline (Novikova et al., 2017b): seq2seq with MR classifier reranking 0.6593 8.6094 0.4483 0.6850 2.2338 0.5754 ♥ S LUG (Juraska et al., 2018): seq2seq-based ensemble (LSTM/CNN encoders, LSTM decoder), heuristic slot aligner reranking, data augmentation 0.6619 8.6130 0.4454 0.6772 2.2615 0.5744 ♥ TNT1 (Oraby et al., 2018): TG EN with data augmentation 0.6561 8.5105 0.4517 0.6839 2.2183 0.5729 ♥ NLE (Agarwal et al., 2018): fully lexicalised character-based seq2seq with MR classification reranking 0.6534 8.5300 0.4435 0.6829 2.1539 0.5696 ♥ TNT2 (Tandon et al., 2018): TG EN with data augmentation 0.6502 8.5211 0.4396 0.6853 2.1670 0.5688 ♥ H ARV (Gehrmann et al., 2018): fully lexicalised seq2seq with copy mechanism, coverage penalty r"
W18-6539,C16-1105,0,0.0616095,"Missing"
W18-6539,W14-3301,0,0.078614,"Missing"
W18-6539,P10-1157,0,0.266637,"Missing"
W18-6539,W18-6558,0,0.0749736,"Missing"
W18-6539,W15-4639,0,0.0491163,"Missing"
W18-6539,N16-1015,0,0.0910322,"Missing"
W18-6539,D15-1199,0,0.285096,"Missing"
W18-6539,D17-1239,0,0.14316,"Missing"
W18-6539,W17-4717,0,\N,Missing
W19-5942,W16-2302,0,0.0677479,"Missing"
W19-5942,N18-2012,1,0.892322,"Missing"
W19-5942,W18-0802,1,0.827341,"rule-based or commercial systems in terms of their perceived appropriateness. 1 A) Gender and Sexuality, e.g. “Are you gay?”, “How do you have sex?” B) Sexualised Comments, e.g. “I love watching porn.”, “I’m horny.” C) Sexualised Insults, e.g. “Stupid bitch.”, “Whore” D) Sexual Requests and Demands, e.g. “Will you have sex with me?”, “Talk dirty to me.” Introduction Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets (Henderson et al., 2018), and how to handle verbal abuse from the user’s side (Cercas Curry and Rieser, 2018; Angeli and Brahnam, 2008; Angeli and Carpenter, 2006; Brahnam, 2005). As highlighted by a recent UNESCO report (West et al., 2019), appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the femalegendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them. 2 We then use these prompts to elicit responses from the following systems, following methodology"
W19-5942,N10-1020,0,0.0296773,"Missing"
W19-5945,N07-2038,0,0.0912696,"Missing"
W19-5945,W15-4654,0,0.0156214,"l system, which can be adapted from a source domain, is equivalent to that of a one-dimensional baseline, which can only be trained from scratch. 1 Introduction Data-driven approaches to spoken dialogue systems (SDS) are limited by their reliance on substantial amounts of annotated data in the target domain. This can be addressed by considering transfer learning techniques, e.g. (Taylor and Stone, 2009), in which data from a source domain is leveraged to improve learning in a target domain. In particular, domain adaptation has been used in the context of dialogue systems (Gaši´c et al., 2017; Wang et al., 2015; Wen et al., 2016), focusing on identifying and exploiting similarities between domain ontologies in slot-filling tasks. In contrast to this previous work, we take a multidimensional approach, which combines machine learning with linguistic theory. Following Bunt (2011), we exploit the linguistic phenomenon that utterances serve more than one function in a conversation, i.e. they have more than one dimension (see Section 2).1 For example, the utterance “On what date would you like to fly to London?” both asks a task-oriented question, and provides feedback about understanding the requested de"
W19-5945,N16-1015,0,0.0550838,"Missing"
W19-5945,W15-4622,0,0.0421654,"Missing"
W19-5945,L16-1500,0,0.027502,"Missing"
W19-8644,C12-1008,0,0.0214154,"ble 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that th"
W19-8644,K16-1002,0,0.0500592,"Missing"
W19-8644,K18-1031,0,0.0234145,"NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evalua"
W19-8644,W17-5534,0,0.0278203,"Missing"
W19-8644,W17-3518,0,0.0514265,"tic instances. We use a partial delexicalisation (replacing names with placeholders).5 Datasets 5.2 We experiment on the following two datasets, both in the restaurant/hotel information domain: • NEM3 (Novikova et al., 2017) – Likert-scale rated outputs (scores 1–6) of 3 NLG systems over 3 datasets, totalling 2,460 instances. • E2E system rankings (Duˇsek et al., 2019) – outputs of 21 systems on a single NLG dataset with 2,979 5-way relative rankings. We choose these two datasets because they contain human-assessed outputs from a variety of NLG systems. Another candidate is the WebNLG corpus (Gardent et al., 2017), which we leave for future work due to MR format differences. Although both selected datasets contain ratings for multiple criteria (informativeness, naturalness and quality for NEM and the latter two for E2E), we follow Duˇsek et al. (2017) and focus on the overall quality criterion in our experiments as it takes both semantic accuracy and fluency into account. Model Settings We evaluate our model in several configurations, with increasing amounts of synthetic training data. Note that even setups using training human references (i.e. additional in-domain data) are still “referenceless”—they"
W19-8644,W18-6505,0,0.0177058,"subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evaluation—distinguishing between human- and machine-generated outputs (Goodfellow et al., 2014). This approach is employed in generators for random text (Bowman et al., 2016) and dialogue responses (Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017). We argue that our approach is more explainable with users being able to reason with the ord"
W19-8644,D14-1020,0,0.0407575,"Missing"
W19-8644,W04-3250,0,0.0259493,"baseline metrics and the original RatPred system are taken over from Duˇsek et al. (2017). Configurations marked with “*” use human references for test instances (this includes word-overlap-based metrics such as BLEU). System Our base system + generated pairs based on training system outputs + generated pairs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza e"
W19-8644,C16-1105,0,0.0411811,"Missing"
W19-8644,W07-0734,0,0.069244,"ilable, and we release our experimental code on GitHub.1 2 The Task(s) The task of NLG QE for ratings is to assign a numerical score to a single NLG output, given its input MR, such as a dialogue act (consisting of the main intent, attributes and values). The score can be e.g. on a Likert scale in the 1-6 range (Novikova et al., 2017). In a pairwise ranking task, the QE system is given two outputs of different NLG systems for the same MR, and decides which one has better quality (see Figure 1). As opposed to automatic word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), no human reference texts for the given MR are required. This widens the scope of possible applications – QE systems can be used for previously unseen MRs. 3 Jointly learning to rank and rate was first introduced by Sculley (2010) for support vector machines and similar approaches have been applied for image classification (Park et al., 2017; Liu et al., 2018) as well as audio classification (Lee et al., 2016), However, we argue that the application for text classification/QE is novel, as is the implementation as a single neural network with two parts that share parameters, capable of trainin"
W19-8644,D17-1230,0,0.0685314,"Missing"
W19-8644,W04-1013,0,0.0575487,"Missing"
W19-8644,P17-1103,0,0.0220179,"on training system outputs + generated pairs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE sy"
W19-8644,P10-1157,0,0.0931245,"Missing"
W19-8644,L16-1575,0,0.0313741,"Missing"
W19-8644,W18-7005,0,0.0266463,"ng insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utteranc"
W19-8644,D16-1228,0,0.0189365,"airs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predic"
W19-8644,P02-1040,0,0.110835,"king. Both datasets are freely available, and we release our experimental code on GitHub.1 2 The Task(s) The task of NLG QE for ratings is to assign a numerical score to a single NLG output, given its input MR, such as a dialogue act (consisting of the main intent, attributes and values). The score can be e.g. on a Likert scale in the 1-6 range (Novikova et al., 2017). In a pairwise ranking task, the QE system is given two outputs of different NLG systems for the same MR, and decides which one has better quality (see Figure 1). As opposed to automatic word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), no human reference texts for the given MR are required. This widens the scope of possible applications – QE systems can be used for previously unseen MRs. 3 Jointly learning to rank and rate was first introduced by Sculley (2010) for support vector machines and similar approaches have been applied for image classification (Park et al., 2017; Liu et al., 2018) as well as audio classification (Lee et al., 2016), However, we argue that the application for text classification/QE is novel, as is the implementation as a single neural network with two parts that"
W19-8644,J18-3002,0,0.0127542,"as of NLP, such as machine translation (MT) (Specia et al., 2010, 2018), research on QE in natural language generation (NLG) from structured meaning representations (MR) such as dialogue acts is relatively recent (Duˇsek et al., 2017; Ueffing et al., 2018) and often focuses on output fluency only (Tian et al., 2018; Kann et al., 2018). In contrast to traditional metrics, QE does not rely on gold-standard human reference texts (Specia et al., 2010), which are expensive to obtain, do not cover the full output space, and are not accurate on the level of individual outputs (Novikova et al., 2017; Reiter, 2018). Automatic QE for NLG has several possible use cases that can improve NLG quality and reliability. For example, rating individual NLG outputs allows to ensure a minimum output quality and engage a backup, e.g., template-based NLG system, if a certain threshold is not met. Relative ranking of multiple NLG outputs can be used directly within a system to rerank n-best outputs or to guide system development, selecting optimal system parameters or comparing to state of the art. In this paper, we present a novel model that jointly learns to perform both tasks—rating individual outputs as well as pa"
W19-8644,W14-3301,0,0.0608728,"Missing"
W19-8644,W18-6530,0,0.0526372,"Missing"
W19-8644,W18-6512,0,0.0144683,"tive topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar t"
W19-8644,P19-1256,0,0.0197181,"et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evaluation—distinguishing between human- and machine-generated outputs (Goodfellow et al., 2014). This approach is employed in generators for random text (Bowman et al., 2016) and dialogue responses (Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017). We argue that our approach is more explainable with users being able to reason with the ordinal output score. Acknowledgments This"
W19-8644,N18-3007,0,0.0665249,"Missing"
W19-8644,D17-1238,1,0.880678,"Missing"
W19-8644,D15-1199,0,0.067814,"Missing"
W19-8644,D17-1239,0,0.0306122,"rmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG o"
W19-8644,E17-1019,0,\N,Missing
W19-8652,P16-2008,1,0.92388,"Missing"
W19-8652,W18-6539,1,0.902972,"Missing"
W19-8652,P19-1256,0,0.282332,"Missing"
W19-8652,D18-1407,0,0.0596321,"Missing"
W19-8652,W16-6644,1,0.887358,"Missing"
W19-8652,P02-1040,0,0.103431,"Missing"
W19-8652,N18-1014,0,0.693857,"Missing"
W19-8652,W17-3204,0,0.071609,"Missing"
W19-8652,W07-0734,0,0.22432,"Missing"
W19-8652,W04-1013,0,0.0383535,"Missing"
W19-8652,P10-1157,0,0.144936,"Missing"
W19-8652,N16-1086,0,0.0506275,"Missing"
W19-8652,W18-6535,0,0.0981499,"Missing"
W19-8652,D18-1437,0,0.104605,"Missing"
W19-8652,N16-1015,0,0.0410745,"Missing"
W19-8652,D15-1199,0,0.108794,"Missing"
W19-8652,D17-1239,0,0.114757,"Missing"
