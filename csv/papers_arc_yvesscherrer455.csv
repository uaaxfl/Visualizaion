2021.wnut-1.52,Sesame Street to Mount Sinai: {BERT}-constrained character-level {M}oses models for multilingual lexical normalization,2021,-1,-1,1,1,263,yves scherrer,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"This paper describes the HEL-LJU submissions to the MultiLexNorm shared task on multilingual lexical normalization. Our system is based on a BERT token classification preprocessing step, where for each token the type of the necessary transformation is predicted (none, uppercase, lowercase, capitalize, modify), and a character-level SMT step where the text is translated from original to normalized given the BERT-predicted transformation constraints. For some languages, depending on the results on development data, the training data was extended by back-translating OpenSubtitles data. In the final ordering of the ten participating teams, the HEL-LJU team has taken the second place, scoring better than the previous state-of-the-art."
2021.vardial-1.1,Findings of the {V}ar{D}ial Evaluation Campaign 2021,2021,-1,-1,12,0,613,bharathi chakravarthi,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"This paper describes the results of the shared tasks organized as part of the VarDial Evaluation Campaign 2021. The campaign was part of the eighth workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with EACL 2021. Four separate shared tasks were included this year: Dravidian Language Identification (DLI), Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). DLI was organized for the first time and the other three continued a series of tasks from previous evaluation campaigns."
2021.vardial-1.16,Social Media Variety Geolocation with geo{BERT},2021,-1,-1,1,1,263,yves scherrer,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",0,"This paper describes the Helsinki{--}Ljubljana contribution to the VarDial 2021 shared task on social media variety geolocation. Following our successful participation at VarDial 2020, we again propose constrained and unconstrained systems based on the BERT architecture. In this paper, we report experiments with different tokenization settings and different pre-trained models, and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at VarDial 2020. Both the code and the best-performing pre-trained models are made freely available."
2021.nodalida-main.37,Boosting Neural Machine Translation from {F}innish to {N}orthern {S}{\\'a}mi with Rule-Based Backtranslation,2021,-1,-1,3,0,2697,mikko aulamo,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"We consider a low-resource translation task from Finnish into Northern S{\'a}mi. Collecting all available parallel data between the languages, we obtain around 30,000 sentence pairs. However, there exists a significantly larger monolingual Northern S{\'a}mi corpus, as well as a rule-based machine translation (RBMT) system between the languages. To make the best use of the monolingual data in a neural machine translation (NMT) system, we use the backtranslation approach to create synthetic parallel data from it using both NMT and RBMT systems. Evaluating the results on an in-domain test set and a small out-of-domain set, we find that the RBMT backtranslation outperforms NMT backtranslation clearly for the out-of-domain test set, but also slightly for the in-domain data, for which the NMT backtranslation model provided clearly better BLEU scores than the RBMT. In addition, combining both backtranslated data sets improves the RBMT approach only for the in-domain test set. This suggests that the RBMT system provides general-domain knowledge that cannot be found from the relative small parallel training data."
2021.konvens-1.25,Towards a balanced annotated Low {S}axon dataset for diachronic investigation of dialectal variation,2021,-1,-1,2,0,5583,janine siewert,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.americasnlp-1.29,The {H}elsinki submission to the {A}mericas{NLP} shared task,2021,-1,-1,2,0.283688,9977,raul vazquez,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,0,"The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail."
2020.wmt-1.40,The {MUCOW} word sense disambiguation test suite at {WMT} 2020,2020,-1,-1,1,1,263,yves scherrer,Proceedings of the Fifth Conference on Machine Translation,0,"This paper reports on our participation with the MUCOW test suite at the WMT 2020 news translation task. We introduced MUCOW at WMT 2019 to measure the ability of MT systems to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. MUCOW is created automatically using existing resources, and the evaluation process is also entirely automated. We evaluate all participating systems of the language pairs English -{\textgreater} Czech, English -{\textgreater} German, and English -{\textgreater} Russian and compare the results with those obtained at WMT 2019. While current NMT systems are fairly good at handling ambiguous source words, we could not identify any substantial progress - at least to the extent that it is measurable by the MUCOW method - in that area over the last year."
2020.wmt-1.134,The {U}niversity of {H}elsinki and Aalto University submissions to the {WMT} 2020 news and low-resource translation tasks,2020,-1,-1,1,1,263,yves scherrer,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the joint participation of University of Helsinki and Aalto University to two shared tasks of WMT 2020: the news translation between Inuktitut and English and the low-resource translation between German and Upper Sorbian. For both tasks, our efforts concentrate on efficient use of monolingual and related bilingual corpora with scheduled multi-task learning as well as an optimized subword segmentation with sampling. Our submission obtained the highest score for Upper Sorbian -{\textgreater} German and was ranked second for German -{\textgreater} Upper Sorbian according to BLEU scores. For English{--}Inuktitut, we reached ranks 8 and 10 out of 11 according to BLEU scores."
2020.vardial-1.1,A Report on the {V}ar{D}ial Evaluation Campaign 2020,2020,-1,-1,10,0,14229,mihaela gaman,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"This paper presents the results of the VarDial Evaluation Campaign 2020 organized as part of the seventh workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with COLING 2020. The campaign included three shared tasks each focusing on a different challenge of language and dialect identification: Romanian Dialect Identification (RDI), Social Media Variety Geolocation (SMG), and Uralic Language Identification (ULI). The campaign attracted 30 teams who enrolled to participate in one or multiple shared tasks and 14 of them submitted runs across the three shared tasks. Finally, 11 papers describing participating systems are published in the VarDial proceedings and referred to in this report."
2020.vardial-1.3,{LSDC} - A comprehensive dataset for Low {S}axon Dialect Classification,2020,-1,-1,2,0,5583,janine siewert,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"We present a new comprehensive dataset for the unstandardised West-Germanic language Low Saxon covering the last two centuries, the majority of modern dialects and various genres, which will be made openly available in connection with the final version of this paper. Since so far no such comprehensive dataset of contemporary Low Saxon exists, this provides a great contribution to NLP research on this language. We also test the use of this dataset for dialect classification by training a few baseline models comparing statistical and neural approaches. The performance of these models shows that in spite of an imbalance in the amount of data per dialect, enough features can be learned for a relatively high classification accuracy."
2020.vardial-1.19,{H}e{L}ju@{V}ar{D}ial 2020: Social Media Variety Geolocation with {BERT} Models,2020,-1,-1,1,1,263,yves scherrer,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",0,"This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our models reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional models by far, and that improvements obtained by pre-training models on large quantities of (mostly standard) text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a (mis)prediction: named entities and dialectal features, both of which are handled well by our models."
2020.lrec-1.224,Paraphrase Generation and Evaluation on Colloquial-Style Sentences,2020,-1,-1,3,0,204,eetu sjoblom,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we investigate paraphrase generation in the colloquial domain. We use state-of-the-art neural machine translation models trained on the Opusparcus corpus to generate paraphrases in six languages: German, English, Finnish, French, Russian, and Swedish. We perform experiments to understand how data selection and filtering for diverse paraphrase pairs affects the generated paraphrases. We compare two different model architectures, an RNN and a Transformer model, and find that the Transformer does not generally outperform the RNN. We also conduct human evaluation on five of the six languages and compare the results to the automatic evaluation metrics BLEU and the recently proposed BERTScore. The results advance our understanding of the trade-offs between the quality and novelty of generated paraphrases, affected by the data selection method. In addition, our comparison of the evaluation methods shows that while BLEU correlates well with human judgments at the corpus level, BERTScore outperforms BLEU in both corpus and sentence-level evaluation."
2020.lrec-1.452,An Evaluation Benchmark for Testing the Word Sense Disambiguation Capabilities of Machine Translation Systems,2020,-1,-1,2,0.784314,4093,alessandro raganato,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Lexical ambiguity is one of the many challenging linguistic phenomena involved in translation, i.e., translating an ambiguous word with its correct sense. In this respect, previous work has shown that the translation quality of neural machine translation systems can be improved by explicitly modeling the senses of ambiguous words. Recently, several evaluation test sets have been proposed to measure the word sense disambiguation (WSD) capability of machine translation systems. However, to date, these evaluation test sets do not include any training data that would provide a fair setup measuring the sense distributions present within the training data itself. In this paper, we present an evaluation benchmark on WSD for machine translation for 10 language pairs, comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at http://github.com/Helsinki-NLP/MuCoW."
2020.lrec-1.848,{T}a{P}a{C}o: A Corpus of Sentential Paraphrases for 73 Languages,2020,-1,-1,1,1,263,yves scherrer,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper presents TaPaCo, a freely available paraphrase corpus for 73 languages extracted from the Tatoeba database. Tatoeba is a crowdsourcing project mainly geared towards language learners. Its aim is to provide example sentences and translations for particular linguistic constructions and words. The paraphrase corpus is created by populating a graph with Tatoeba sentences and equivalence links between sentences {``}meaning the same thing{''}. This graph is then traversed to extract sets of paraphrases. Several language-independent filters and pruning steps are applied to remove uninteresting sentences. A manual evaluation performed on three languages shows that between half and three quarters of inferred paraphrases are correct and that most remaining ones are either correct but trivial, or near-paraphrases that neutralize a morphological distinction. The corpus contains a total of 1.9 million sentences, with 200 - 250 000 sentences per language. It covers a range of languages for which, to our knowledge, no other paraphrase dataset exists. The dataset is available at https://doi.org/10.5281/zenodo.3707949."
2020.findings-emnlp.49,Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation,2020,60,1,2,0.784314,4093,alessandro raganato,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed {--} non-learnable {--} attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios."
W19-5432,The {U}niversity of {H}elsinki Submissions to the {WMT}19 Similar Language Translation Task,2019,22,0,1,1,263,yves scherrer,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",0,"This paper describes the University of Helsinki Language Technology group{'}s participation in the WMT 2019 similar language translation task. We trained neural machine translation models for the language pairs Czech {\textless}-{\textgreater} Polish and Spanish {\textless}-{\textgreater} Portuguese. Our experiments focused on different subword segmentation methods, and in particular on the comparison of a cognate-aware segmentation method, Cognate Morfessor, with character segmentation and unsupervised segmentation methods for which the data from different languages were simply concatenated. We did not observe major benefits from cognate-aware segmentation methods, but further research may be needed to explore larger parts of the parameter space. Character-level models proved to be competitive for translation between Spanish and Portuguese, but they are slower in training and decoding."
W19-5347,The {U}niversity of {H}elsinki Submissions to the {WMT}19 News Translation Task,2019,22,0,4,0,2672,aarne talman,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"In this paper we present the University of Helsinki submissions to the WMT 2019 shared news translation task in three language pairs: English-German, English-Finnish and Finnish-English. This year we focused first on cleaning and filtering the training data using multiple data-filtering approaches, resulting in much smaller and cleaner training sets. For English-German we trained both sentence-level transformer models as well as compared different document-level translation approaches. For Finnish-English and English-Finnish we focused on different segmentation approaches and we also included a rule-based system for English-Finnish."
W19-5354,The {M}u{C}o{W} Test Suite at {WMT} 2019: Automatically Harvested Multilingual Contrastive Word Sense Disambiguation Test Sets for Machine Translation,2019,0,2,2,0.784314,4093,alessandro raganato,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs. One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types. We present MuCoW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 thousand contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet. We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using NMT pretrained models. The MuCoW test suite is available at http://github.com/Helsinki-NLP/MuCoW."
W19-2005,Measuring Semantic Abstraction of Multilingual {NMT} with Paraphrase Recognition and Generation Tasks,2019,0,1,2,0,2675,jorg tiedemann,Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {NLP},0,"In this paper, we investigate whether multilingual neural translation models learn stronger semantic abstractions of sentences than bilingual ones. We test this hypotheses by measuring the perplexity of such models when applied to paraphrases of the source language. The intuition is that an encoder produces better representations if a decoder is capable of recognizing synonymous sentences in the same language even though the model is never trained for that task. In our setup, we add 16 different auxiliary languages to a bidirectional bilingual baseline model (English-French) and test it with in-domain and out-of-domain paraphrases in English. The results show that the perplexity is significantly reduced in each of the cases, indicating that meaning can be grounded in translation. This is further supported by a study on paraphrase generation that we also include at the end of the paper."
W19-1401,A Report on the Third {V}ar{D}ial Evaluation Campaign,2019,-1,-1,3,0.0997324,622,marcos zampieri,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"In this paper, we present the findings of the Third VarDial Evaluation Campaign organized as part of the sixth edition of the workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with NAACL 2019. This year, the campaign included five shared tasks, including one task re-run {--} German Dialect Identification (GDI) {--} and four new tasks {--} Cross-lingual Morphological Analysis (CMA), Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT), Moldavian vs. Romanian Cross-dialect Topic identification (MRC), and Cuneiform Language Identification (CLI). A total of 22 teams submitted runs across the five shared tasks. After the end of the competition, we received 14 system description papers, which are published in the VarDial workshop proceedings and referred to in this report."
D19-6506,Analysing concatenation approaches to document-level {NMT} in two different domains,2019,0,1,1,1,263,yves scherrer,Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019),0,"In this paper, we investigate how different aspects of discourse context affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the models are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores."
W18-6425,The {U}niversity of {H}elsinki submissions to the {WMT}18 news task,2018,0,4,2,0.784314,4093,alessandro raganato,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the University of Helsinki{'}s submissions to the WMT18 shared news translation task for English-Finnish and English-Estonian, in both directions. This year, our main submissions employ a novel neural architecture, the Transformer, using the open-source OpenNMT framework. Our experiments couple domain labeling and fine tuned multilingual models with shared vocabularies between the source and target language, using the provided parallel data of the shared task and additional back-translations. Finally, we compare, for the English-to-Finnish case, the effectiveness of different machine translation architectures, starting from a rule-based approach to our best neural model, analyzing the output and highlighting future research."
W18-6433,"The {WMT}{'}18 Morpheval test suites for {E}nglish-{C}zech, {E}nglish-{G}erman, {E}nglish-{F}innish and {T}urkish-{E}nglish",2018,-1,-1,2,0,23863,franck burlot,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"Progress in the quality of machine translation output calls for new automatic evaluation procedures and metrics. In this paper, we extend the Morpheval protocol introduced by Burlot and Yvon (2017) for the English-to-Czech and English-to-Latvian translation directions to three additional language pairs, and report its use to analyze the results of WMT 2018{'}s participants for these language pairs. Considering additional, typologically varied source and target languages also enables us to draw some generalizations regarding this morphology-oriented evaluation procedure."
W18-3901,Language Identification and Morphosyntactic Tagging: The Second {V}ar{D}ial Evaluation Campaign,2018,0,13,7,0.133248,622,marcos zampieri,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"We present the results and the findings of the Second VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects. The campaign was organized as part of the fifth edition of the VarDial workshop, collocated with COLING{'}2018. This year, the campaign included five shared tasks, including two task re-runs {--} Arabic Dialect Identification (ADI) and German Dialect Identification (GDI) {--}, and three new tasks {--} Morphosyntactic Tagging of Tweets (MTT), Discriminating between Dutch and Flemish in Subtitles (DFS), and Indo-Aryan Language Identification (ILI). A total of 24 teams submitted runs across the five shared tasks, and contributed 22 system description papers, which were included in the VarDial workshop proceedings and are referred to in this report."
L18-1527,Crowdsourcing Regional Variation Data and Automatic Geolocalisation of Speakers of {E}uropean {F}rench,2018,0,0,2,0.714286,29758,jeanphilippe goldman,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present the crowdsourcing platform Donnez Votre FranAxc2xa7ais Axc2xa0 la Science (DFS, or xc3xa2xe2x82xacxc5x93Give your French to Sciencexc3xa2xe2x82xacxc2x9d), which aims to collect linguistic data and document language use, with a special focus on regional variation in European French. The activities not only gather data that is useful for scientific studies, but they also provide feedback to the general public; this is important in order to reward participants, to encourage them to follow future surveys, and to foster interaction with the scientific community. The two main activities described here are 1) a linguistic survey on lexical variation with immediate feedback and 2) a speaker geolocalisation system; i.e., a quiz that guesses the linguistic origin of the participant by comparing their answers with previously gathered linguistic data. For the geolocalisation activity, we set up a simulation framework to optimise predictions. Three classification algorithms are compared: the first one uses clustering and shibboleth detection, whereas the other two rely on feature elimination techniques with support Vector Machines and Maximum Entropy models as underlying base classifiers. The best-performing system uses a selection of 17 questions and reaches a localisation accuracy of 66%, extending the prediction from the one-best area (one among 109 base areas) to its first-order and second-order neighbouring areas."
W17-4811,Neural Machine Translation with Extended Context,2017,18,0,2,0,2675,jorg tiedemann,Proceedings of the Third Workshop on Discourse in Machine Translation,0,"We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases."
W17-4733,The {H}elsinki Neural Machine Translation System,2017,0,6,2,0,655,robert ostling,Proceedings of the Second Conference on Machine Translation,0,None
W17-1405,Lexicon Induction for Spoken {R}usyn {--} Challenges and Results,2017,0,1,2,0,32047,achim rabus,Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing,0,"This paper reports on challenges and results in developing NLP resources for spoken Rusyn. Being a Slavic minority language, Rusyn does not have any resources to make use of. We propose to build a morphosyntactic dictionary for Rusyn, combining existing resources from the etymologically close Slavic languages Russian, Ukrainian, Slovak, and Polish. We adapt these resources to Rusyn by using vowel-sensitive Levenshtein distance, hand-written language-specific transformation rules, and combinations of the two. Compared to an exact match baseline, we increase the coverage of the resulting morphological dictionary by up to 77.4{\%} relative (42.9{\%} absolute), which results in a tagging recall increased by 11.6{\%} relative (9.1{\%} absolute). Our research confirms and expands the results of previous studies showing the efficiency of using NLP resources from neighboring languages for low-resourced languages."
W17-1201,Findings of the {V}ar{D}ial Evaluation Campaign 2017,2017,0,26,7,0.133248,622,marcos zampieri,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"We present the results of the VarDial Evaluation Campaign on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects, which we organized as part of the fourth edition of the VarDial workshop at EACL{'}2017. This year, we included four shared tasks: Discriminating between Similar Languages (DSL), Arabic Dialect Identification (ADI), German Dialect Identification (GDI), and Cross-lingual Dependency Parsing (CLP). A total of 19 teams submitted runs across the four tasks, and 15 of them wrote system description papers."
W17-1210,Multi-source morphosyntactic tagging for spoken {R}usyn,2017,0,1,1,1,263,yves scherrer,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)",0,"This paper deals with the development of morphosyntactic taggers for spoken varieties of the Slavic minority language Rusyn. As neither annotated corpora nor parallel corpora are electronically available for Rusyn, we propose to combine existing resources from the etymologically close Slavic languages Russian, Ukrainian, Slovak, and Polish and adapt them to Rusyn. Using MarMoT as tagging toolkit, we show that a tagger trained on a balanced set of the four source languages outperforms single language taggers by about 9{\%}, and that additional automatically induced morphosyntactic lexicons lead to further improvements. The best observed accuracies for Rusyn are 82.4{\%} for part-of-speech tagging and 75.5{\%} for full morphological tagging."
L16-1641,{A}rchi{M}ob - A Corpus of Spoken {S}wiss {G}erman,2016,8,10,2,0,17323,tanja samardvzic,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Swiss dialects of German are, unlike most dialects of well standardised languages, widely used in everyday communication. Despite this fact, automatic processing of Swiss German is still a considerable challenge due to the fact that it is mostly a spoken variety rarely recorded and that it is subject to considerable regional variation. This paper presents a freely available general-purpose corpus of spoken Swiss German suitable for linguistic research, but also for training automatic tools. The corpus is a result of a long design process, intensive manual work and specially adapted computational processing. We first describe how the documents were transcribed, segmented and aligned with the sound source, and how inconsistent transcriptions were unified through an additional normalisation layer. We then present a bootstrapping approach to automatic normalisation using different machine-translation-inspired methods. Furthermore, we evaluate the performance of part-of-speech taggers on our data and show how the same bootstrapping approach improves part-of-speech tagging by 10{\%} over four rounds. Finally, we present the modalities of access of the corpus as well as the data format."
C16-2019,On-line Multilingual Linguistic Services,2016,8,0,2,0,18785,eric wehrli,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"In this demo, we present our free on-line multilingual linguistic services which allow to analyze sentences or to extract collocations from a corpus directly on-line, or by uploading a corpus. They are available for 8 European languages (English, French, German, Greek, Italian, Portuguese, Romanian, Spanish) and can also be accessed as web services by programs. While several open systems are available for POS-tagging and dependency parsing or terminology extraction, their integration into an application requires some computational competence. Furthermore, none of the parsers/taggers handles MWEs very satisfactorily, in particular when the two terms of the collocation are distant from each other or in reverse order. Our tools, on the other hand, are specifically designed for users with no particular computational literacy. They do not require from the user any download, installation or adaptation if used on-line, and their integration in an application, using one the scripts described below is quite easy. Furthermore, by default, the parser handles collocations and other MWEs, as well as anaphora resolution (limited to 3rd person personal pronouns). When used in the tagger mode, it can be set to display grammatical functions and collocations."
2016.jeptalnrecital-jep.14,Cartopho : un site web de cartographie de variantes de prononciation en fran{\\c{c}}ais (Cartopho: a website for mapping pronunciation variants in {F}rench),2016,-1,-1,4,0,14683,philippe mareuil,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"Le pr{\'e}sent travail se propose de renouveler les traditionnels atlas dialectologiques pour cartographier les variantes de prononciation en fran{\c{c}}ais, {\`a} travers un site internet. La toile est utilis{\'e}e non seulement pour collecter des donn{\'e}es, mais encore pour diss{\'e}miner les r{\'e}sultats aupr{\`e}s des chercheurs et du grand public. La m{\'e}thodologie utilis{\'e}e, {\`a} base de crowdsourcing (ou Â« production participative Â»), nous a permis de recueillir des informations aupr{\`e}s de 2500 francophones d{'}Europe (France, Belgique, Suisse). Une plateforme dynamique {\`a} l{'}interface conviviale a ensuite {\'e}t{\'e} d{\'e}velopp{\'e}e pour cartographier la prononciation de 70 mots dans les diff{\'e}rentes r{\'e}gions des pays concern{\'e}s (des mots notamment {\`a} voyelle moyenne ou dont la consonne finale peut {\^e}tre prononc{\'e}e ou non). Les options de visualisation par d{\'e}partement/canton/province ou par r{\'e}gion, combinant plusieurs traits de prononciation et ensembles de mots, sous forme de pastilles color{\'e}es, de hachures, etc. sont pr{\'e}sent{\'e}es dans cet article. On peut ainsi observer imm{\'e}diatement un /E/ plus ferm{\'e} (ainsi qu{'}un /O/ plus ouvert) dans le Nord-Pas-de-Calais et le sud de la France, pour des mots comme parfait ou rose, un /{\OE}/ plus ferm{\'e} en Suisse pour un mot comme gueule, par exemple."
W14-5304,Unsupervised adaptation of supervised part-of-speech taggers for closely related languages,2014,21,3,1,1,263,yves scherrer,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"When developing NLP tools for low-resource languages, one is often confronted with the lack of annotated data. We propose to circumvent this bottleneck by training a supervised HMM tagger on a closely related language for which annotated data are available, and translating the words in the tagger parameter files into the low-resource language. The translation dictionaries are created with unsupervised lexicon induction techniques that rely only on raw textual data. We obtain a tagging accuracy of up to 89.08% using a Spanish tagger adapted to Catalan, which is 30.66% above the performance of an unadapted Spanish tagger, and 8.88% below the performance of a supervised tagger trained on annotated Catalan data. Furthermore, we evaluate our model on several Romance, Germanic and Slavic languages and obtain tagging accuracies of up to 92%."
scherrer-etal-2014-swissadmin,{S}wiss{A}dmin: A multilingual tagged parallel corpus of press releases,2014,11,2,1,1,263,yves scherrer,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"SwissAdmin is a new multilingual corpus of press releases from the Swiss Federal Administration, available in German, French, Italian and English. We provide SwissAdmin in three versions: (i) plain texts of approximately 6 to 8 million words per language; (ii) sentence-aligned bilingual texts for each language pair; (iii) a part-of-speech-tagged version consisting of annotations in both the Universal tagset and the richer Fips tagset, along with grammatical functions, verb valencies and collocations. The SwissAdmin corpus is freely available at www.latl.unige.ch/swissadmin."
scherrer-sagot-2014-language,A language-independent and fully unsupervised approach to lexicon induction and part-of-speech tagging for closely related languages,2014,17,2,1,1,263,yves scherrer,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe our generic approach for transferring part-of-speech annotations from a resourced language towards an etymologically closely related non-resourced language, without using any bilingual (i.e., parallel) data. We first induce a translation lexicon from monolingual corpora, based on cognate detection followed by cross-lingual contextual similarity. Second, POS information is transferred from the resourced language along translation pairs to the non-resourced language and used for tagging the corpus. We evaluate our methods on three language families, consisting of five Romance languages, three Germanic languages and five Slavic languages. We obtain tagging accuracies of up to 91.6{\%}."
W13-5306,Lexicon induction and part-of-speech tagging of non-resourced languages without any bilingual resources,2013,19,5,1,1,263,yves scherrer,Proceedings of the Workshop on Adaptation of Language Resources and Tools for Closely Related Languages and Language Variants,0,"We introduce a generic approach for transferring part-of-speech annotations from a resourced language to a non-resourced but etymologically close language. We first infer a bilingual lexicon between the two languages with methods based on character similarity, frequency similarity and context similarity. We then assign part-of-speech tags to these bilingual lexicon entries and annotate the remaining words on the basis of suffix analogy. We evaluate our approach on five language pairs of the Iberic peninsula, reaching up to 95% of precision on the lexicon induction task and up to 85% of tagging accuracy."
W13-2409,Modernizing historical {S}lovene words with character-based {SMT},2013,17,17,1,1,263,yves scherrer,Proceedings of the 4th Biennial International Workshop on {B}alto-{S}lavic Natural Language Processing,0,"We propose a language-independent word normalization method exemplified on modernizing historical Slovene words. Our method relies on character-based statistical machine translation and uses only shallow knowledge. We present the relevant lexicons and two experiments. In one, we use a lexicon of historical wordxe2x80x90 contemporary word pairs and a list of contemporary words; in the other, we only use a list of historical words and one of contemporary ones. We show that both methods produce significantly better results than the baseline."
W12-0210,Recovering dialect geography from an unaligned comparable corpus,2012,17,6,1,1,263,yves scherrer,Proceedings of the {EACL} 2012 Joint Workshop of {LINGVIS} {\\&} {UNCLH},0,"This paper proposes a simple metric of dialect distance, based on the ratio between identical word pairs and cognate word pairs occurring in two texts. Different variations of this metric are tested on a corpus containing comparable texts from different Swiss German dialects and evaluated on the basis of spatial autocorrelation measures. The visualization of the results as cluster dendrograms shows that closely related dialects are reliably clustered together, while multidimensional scaling produces graphs that show high agreement with the geographic localization of the original texts."
scherrer-cartoni-2012-trilingual,The Trilingual {ALLEGRA} Corpus: Presentation and Possible Use for Lexicon Induction,2012,13,3,1,1,263,yves scherrer,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we present a trilingual parallel corpus for German, Italian and Romansh, a Swiss minority language spoken in the canton of Grisons. The corpus called ALLEGRA contains press releases automatically gathered from the website of the cantonal administration of Grisons. Texts have been preprocessed and aligned with a current state-of-the-art sentence aligner. The corpus is one of the first of its kind, and can be of great interest, particularly for the creation of natural language processing resources and tools for Romansh. We illustrate the use of such a trilingual resource for automatic induction of bilingual lexicons, which is a real challenge for under-represented languages. We induce a bilingual lexicon for German-Romansh by phrase alignment and evaluate the resulting entries with the help of a reference lexicon. We then show that the use of the third language of the corpus â Italian â as a pivot language can improve the precision of the induced lexicon, without loss in terms of quality of the extracted pairs."
W11-2604,Syntactic transformations for {S}wiss {G}erman dialects,2011,14,4,1,1,263,yves scherrer,Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties,0,"While most dialectological research so far focuses on phonetic and lexical phenomena, we use recent fieldwork in the domain of dialect syntax to guide the development of multidialectal natural language processing tools. In particular, we develop a set of rules that transform Standard German sentence structures into syntactically valid Swiss German sentence structures. These rules are sensitive to the dialect area, so that the dialects of more than 300 towns are covered. We evaluate the transformation rules on a Standard German treebank and obtain accuracy figures of 85% and above for most rules. We analyze the most frequent errors and discuss the benefit of these transformations for various natural language processing tasks."
2011.jeptalnrecital-court.43,"{\\'E}tude inter-langues de la distribution et des ambigu{\\\\\i}t{\\'e}s syntaxiques des pronoms (A study of cross-language distribution and syntactic ambiguities of pronouns)""",2011,-1,-1,2,0,39858,lorenza russo,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Ce travail d{\'e}crit la distribution des pronoms selon le style de texte (litt{\'e}raire ou journalistique) et selon la langue (fran{\c{c}}ais, anglais, allemand et italien). Sur la base d{'}un {\'e}tiquetage morpho-syntaxique effectu{\'e} automatiquement puis v{\'e}rifi{\'e} manuellement, nous pouvons constater que la proportion des diff{\'e}rents types de pronoms varie selon le type de texte et selon la langue. Nous discutons les cat{\'e}gories les plus ambigu{\""e}s de mani{\`e}re d{\'e}taill{\'e}e. Comme nous avons utilis{\'e} l{'}analyseur syntaxique Fips pour l{'}{\'e}tiquetage des pronoms, nous l{'}avons {\'e}galement {\'e}valu{\'e} et obtenu une pr{\'e}cision moyenne de plus de 95{\%}."
2011.jeptalnrecital-court.44,La traduction automatique des pronoms. Probl{\\`e}mes et perspectives (Automatic translation of pronouns. Problems and perspectives),2011,-1,-1,1,1,263,yves scherrer,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cette {\'e}tude, notre syst{\`e}me de traduction automatique, Its-2, a fait l{'}objet d{'}une {\'e}valuation manuelle de la traduction des pronoms pour cinq paires de langues et sur deux corpus : un corpus litt{\'e}raire et un corpus de communiqu{\'e}s de presse. Les r{\'e}sultats montrent que les pourcentages d{'}erreurs peuvent atteindre 60{\%} selon la paire de langues et le corpus. Nous discutons ainsi deux pistes de recherche pour l{'}am{\'e}lioration des performances de Its-2 : la r{\'e}solution des ambigu{\""\i}t{\'e}s d{'}analyse et la r{\'e}solution des anaphores pronominales."
D10-1112,Word-Based Dialect Identification with Georeferenced Rules,2010,11,7,1,1,263,yves scherrer,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel approach for (written) dialect identification based on the discriminative potential of entire words. We generate Swiss German dialect words from a Standard German lexicon with the help of hand-crafted phonetic/graphemic rules that are associated with occurrence maps extracted from a linguistic atlas created through extensive empirical fieldwork. In comparison with a character-n-gram approach to dialect identification, our model is more robust to individual spelling differences, which are frequently encountered in non-standardized dialect writing. Moreover, it covers the whole Swiss German dialect continuum, which trained models struggle to achieve due to sparsity of training data."
2010.jeptalnrecital-demonstration.9,Des cartes dialectologiques num{\\'e}ris{\\'e}es pour le {TALN},2010,-1,-1,1,1,263,yves scherrer,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,"Cette d{\'e}monstration pr{\'e}sente une interface web pour des donn{\'e}es num{\'e}ris{\'e}es de l{'}atlas linguistique de la Suisse allemande. Nous pr{\'e}sentons d{'}abord l{'}int{\'e}gration des donn{\'e}es brutes et des donn{\'e}es interpol{\'e}es de l{'}atlas dans une interface bas{\'e}e sur Google Maps. Ensuite, nous montrons des prototypes de syst{\`e}mes de traduction automatique et d{'}identification de dialectes qui s{'}appuient sur ces donn{\'e}es dialectologiques num{\'e}ris{\'e}es."
W09-0415,Deep Linguistic Multilingual Translation and Bilingual Dictionaries,2009,13,27,3,0,18785,eric wehrli,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper describes the MulTra project, aiming at the development of an efficient multilingual translation technology based on an abstract and generic linguistic model as well as on object-oriented software design. In particular, we will address the issue of the rapid growth both of the transfer modules and of the bilingual databases. For the latter, we will show that a significant part of bilingual lexical databases can be derived automatically through transitivity, with corpus validation."
2009.jeptalnrecital-court.44,Un syst{\\`e}me de traduction automatique param{\\'e}tr{\\'e} par des atlas dialectologiques,2009,-1,-1,1,1,263,yves scherrer,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Contrairement {\`a} la plupart des syst{\`e}mes de traitement du langage, qui s{'}appliquent {\`a} des langues {\'e}crites et standardis{\'e}es, nous pr{\'e}sentons ici un syst{\`e}me de traduction automatique qui prend en compte les sp{\'e}cificit{\'e}s des dialectes. En g{\'e}n{\'e}ral, les dialectes se caract{\'e}risent par une variation continue et un manque de donn{\'e}es textuelles en qualit{\'e} et quantit{\'e} suffisantes. En m{\^e}me temps, du moins en Europe, les dialectologues ont {\'e}tudi{\'e} en d{\'e}tail les caract{\'e}ristiques linguistiques des dialectes. Nous soutenons que des donn{\'e}es provenant d{'}atlas dialectologiques peuvent {\^e}tre utilis{\'e}es pour param{\'e}trer un syst{\`e}me de traduction automatique. Nous illustrons cette id{\'e}e avec le prototype d{'}un syst{\`e}me de traduction bas{\'e} sur des r{\`e}gles, qui traduit de l{'}allemand standard vers les diff{\'e}rents dialectes de Suisse allemande. Quelques exemples linguistiquement motiv{\'e}s serviront {\`a} exposer l{'}architecture de ce syst{\`e}me."
W08-1003,Part-of-Speech Tagging with a Symbolic Full Parser: Using the {TIGER} Treebank to Evaluate Fips,2008,9,1,1,1,263,yves scherrer,Proceedings of the Workshop on Parsing {G}erman,0,"In this paper, we introduce the German version of the multilingual Fips parsing system. We focus on the evaluation of its part-of-speech tagging component with the help of the TIGER treebank. We explain how Fips can be adapted to the tagset used by TIGER and report first results of this study: currently, 87% of words are tagged correctly. We also discuss some common errors and explore a possible extension of this study to parsing."
2008.jeptalnrecital-recital.8,Transducteurs {\\`a} fen{\\^e}tre glissante pour l{'}induction lexicale,2008,-1,-1,1,1,263,yves scherrer,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues,0,"Nous appliquons diff{\'e}rents mod{\`e}les de similarit{\'e} graphique {\`a} la t{\^a}che de l{'}induction de lexiques bilingues entre un dialecte de Suisse allemande et l{'}allemand standard. Nous comparons des transducteurs stochastiques utilisant des fen{\^e}tres glissantes de 1 {\`a} 3 caract{\`e}res, entra{\^\i}n{\'e}s {\`a} l{'}aide de l{'}algorithme de maximisation de l{'}esp{\'e}rance avec des corpus d{'}entra{\^\i}nement de tailles diff{\'e}rentes. Si les transducteurs {\`a} unigrammes donnent des r{\'e}sultats satisfaisants avec des corpus tr{\`e}s petits, nous montrons que les transducteurs {\`a} bigrammes les d{\'e}passent {\`a} partir de 750 paires de mots d{'}entra{\^\i}nement. En g{\'e}n{\'e}ral, les mod{\`e}les entra{\^\i}n{\'e}s nous ont permis d{'}am{\'e}liorer la F-mesure de 7{\%} {\`a} 15{\%} par rapport {\`a} la distance de Levenshtein."
P07-3010,Adaptive String Distance Measures for Bilingual Dialect Lexicon Induction,2007,9,13,1,1,263,yves scherrer,Proceedings of the {ACL} 2007 Student Research Workshop,0,This paper compares different measures of graphemic similarity applied to the task of bilingual lexicon induction between a Swiss German dialect and Standard German. The measures have been adapted to this particular language pair by training stochastic transducers with the Expectation-Maximisation algorithm or by using handmade transduction rules. These adaptive metrics show up to 11% F-measure improvement over a static metric like Levenshtein distance.
