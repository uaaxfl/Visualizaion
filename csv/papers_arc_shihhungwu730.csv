2021.rocling-1.48,{CYUT} at {ROCLING}-2021 Shared Task: Based on {BERT} and {M}ac{BERT},2021,-1,-1,2,0,2435,xiesheng hong,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper present a description for the ROCLING 2021 shared task in dimensional sentiment analysis for educational texts. We submitted two runs in the final test. Both runs use the standard regression model. The Run1 uses Chinese version of BERT as the base, and in Run2 we use the early version of MacBERT that Chinese version of RoBERTa-like BERT model, RoBERTa-wwm-ext. Using powerful pre-training model of BERT for text embedding to help train the model."
2020.nlptea-1.12,{CYUT} Team {C}hinese Grammatical Error Diagnosis System Report in {NLPTEA}-2020 {CGED} Shared Task,2020,-1,-1,1,1,2436,shihhung wu,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,0,"This paper reports our Chinese Grammatical Error Diagnosis system in the NLPTEA-2020 CGED shared task. In 2020, we sent two runs with two approaches. The first one is a combination of conditional random fields (CRF) and a BERT model deep-learning approach. The second one is a BERT model deep-learning approach. The official results shows that our run1 achieved the highest precision rate 0.9875 with the lowest false positive rate 0.0163 on detection, while run2 gives a more balanced performance."
2020.lrec-1.198,Learning the Human Judgment for the Automatic Evaluation of Chatbot,2020,-1,-1,1,1,2436,shihhung wu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"It is hard to evaluate the quality of the generated text by a generative dialogue system. Currently, dialogue evaluation relies on human judges to label the quality of the generated text. It is not a reusable mechanism that can give consistent evaluation for system developers. We believe that it is easier to get consistent results on comparing two generated dialogue by two systems and it is hard to give a consistent quality score on only one system at a time. In this paper, we propose a machine learning approach to reduce the effort of human evaluation by learning the human judgment on comparing two dialogue systems. Training from the human labeling result, the evaluation model learns which generative models is better in each dialog context. Thus, it can be used for system developers to compare the fine-tuned models over and over again without the human labor. In our experiment we find the agreement between the learned model and human judge is 70{\%}. The experiment is conducted on comparing two attention based GRU-RNN generative models."
2019.rocling-1.33,åºæ¼{S}eq2{S}eqæ¨¡åçä¸­æææ³é¯èª¤è¨ºæ·ç³»çµ±(A {C}hinese Grammatical Error Diagnosis System Based on {S}eq2{S}eq Model),2019,-1,-1,4,0,16015,junwei wang,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
W18-3718,A Short Answer Grading System in {C}hinese by Support Vector Approach,2018,0,1,1,1,2436,shihhung wu,Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,0,"In this paper, we report a short answer grading system in Chinese. We build a system based on standard machine learning approaches and test it with translated corpus from two publicly available corpus in English. The experiment results show similar results on two different corpus as in English."
W18-3729,{CYUT}-{III} Team {C}hinese Grammatical Error Diagnosis System Report in {NLPTEA}-2018 {CGED} Shared Task,2018,0,0,1,1,2436,shihhung wu,Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,0,"This paper reports how we build a Chinese Grammatical Error Diagnosis system in the NLPTEA-2018 CGED shared task. In 2018, we sent three runs with three different approaches. The first one is a pattern-based approach by frequent error pattern matching. The second one is a sequential labelling approach by conditional random fields (CRF). The third one is a rewriting approach by sequence to sequence (seq2seq) model. The three approaches have different properties that aim to optimize different performance metrics and the formal run results show the differences as we expected."
I17-4022,{CYUT} at {IJCNLP}-2017 Task 3: System Report for Review Opinion Diversification,2017,0,0,1,1,2436,shihhung wu,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"Review Opinion Diversification (RevOpiD) 2017 is a shared task which is held in International Joint Conference on Natural Language Processing (IJCNLP). The shared task aims at selecting top-k reviews, as a summary, from a set of re-views. There are three subtasks in RevOpiD: helpfulness ranking, rep-resentativeness ranking, and ex-haustive coverage ranking. This year, our team submitted runs by three models. We focus on ranking reviews based on the helpfulness of the reviews. In the first two models, we use linear regression with two different loss functions. First one is least squares, and second one is cross entropy. The third run is a random baseline. For both k=5 and k=10, our second model gets the best scores in the official evaluation metrics."
W16-4909,{CYUT}-{III} System at {C}hinese Grammatical Error Diagnosis Task,2016,0,2,2,1,33547,polin chen,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"This paper describe the CYUT-III system on grammar error detection in the 2016 NLP-TEA Chinese Grammar Error Detection shared task CGED. In this task a system has to detect four types of errors, in-cluding redundant word error, missing word error, word selection error and word ordering error. Based on the conditional random fields (CRF) model, our system is a linear tagger that can detect the errors in learners{'} essays. Since the system performance depends on the features heavily, in this paper, we are going to report how to integrate the collocation feature into the CRF model. Our system presents the best detection accuracy and Identification accuracy on the TOCFL dataset, which is in traditional Chi-nese. The same system also works well on the simplified Chinese HSK dataset."
O16-1011,"ä»¥èªè¨æ¨¡åè©ä¼°å­¸ç¿è\
æå¥ä¿®æ¹åå¾ä¹æµæ¢åº¦(Using language model to assess the fluency of learners sentences edited by teachers)[In {C}hinese]",2016,0,0,3,0,34579,guanying pu,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
W15-4402,{C}hinese Grammatical Error Diagnosis by Conditional Random Fields,2015,6,3,2,1,33547,polin chen,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"This paper reports how to build a Chinese Grammatical Error Diagnosis system based on the conditional random fields (CRF). The system can find four types of grammatical errors in learnersxe2x80x99 essays. The four types or errors are redundant words, missing words, bad word selection, and disorder words. Our system presents the best false positive rate in 2015 NLP-TEA-2 CGED shared task, and also the best precision rate in three diagnosis levels."
O15-1021,"ä»¥èªè¨æ¨¡åå¤æ·å­¸ç¿è\
æå¥æµæ¢åº¦(Analyzing Learners {`}Writing Fluency Based on Language Model)[In {C}hinese]",2015,0,0,2,1,33547,polin chen,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O14-3002,Modeling the Helpful Opinion Mining of Online Consumer Reviews as a Classification Problem,2014,0,2,3,1,39223,yiching zeng,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 2, June 2014",0,None
W13-4406,{C}hinese Spelling Check Evaluation at {SIGHAN} Bake-off 2013,2013,6,34,1,1,2436,shihhung wu,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper introduces an overview of Chinese Spelling Check task at SIGHAN Bake-off 2013. We describe all aspects of the task for Chinese spelling check, consisting of task description, data preparation, performance metrics, and evaluation results. This bake-off contains two subtasks, i.e., error detection and error correction. We evaluate the systems that can automatically point out the spelling errors and provide the corresponding corrections in studentsxe2x80x99 essays, summarize the performance of all participantsxe2x80x99 submitted results, and discuss some advanced issues. The hope is that through such evaluation campaigns, more advanced Chinese spelling check techniques will be emerged."
W13-4205,Modeling the Helpful Opinion Mining of Online Consumer Reviews as a Classification Problem,2013,10,6,2,1,39223,yiching zeng,Proceedings of the {IJCNLP} 2013 Workshop on Natural Language Processing for Social Media ({S}ocial{NLP}),0,"The paper addresses an opinion mining problem: how to find the helpful reviews from online consumer reviews via the quality of the content. Since there are too many reviews, efficiently identifying the helpful ones earlier can benefit both consumers and companies. Consumers can read only the helpful opinions from helpful reviews before they purchase a product, while companies can acquire the true reasons a product is liked or hated. A system is built to assess the difficulty of the problem. The experimental results show that helpful reviews can be distinguished from unhelpful ones with high precision."
O13-5001,èæ¶µå¥ååææ¼æ¹é²ä¸­ææå­èæ¶µè­å¥ç³»çµ± (Entailment Analysis for Improving {C}hinese Recognizing Textual Entailment System) [In {C}hinese],2013,16,2,2,1,41520,shanshun yang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 18, Number 4, {D}ecember 2013-Special Issue on Selected Papers from {ROCLING} {XXV}",0,"Recognizing Textual Entailment (RTE) is a new research issue in natural language processing (NLP) research area. RTE can be a useful component in many NLP applications. In this paper, we introduce our finding on the entailment analysis of the NTCIR-10 RITE-2 dataset, and use the observation to improve our system. In the previous works, all the input pairs are treated equally in a standard classification architecture. We find that is not suitable for some special cases. We believe that by isolating the special cases and building separated classifiers, a RTE system can perform better. After implementing modules for four special cases into our system, the result is significantly improved from 67.86% to 72.92% on the binary class classification task."
O13-1012,èæ¶µå¥ååææ¼æ¹é²ä¸­ææå­èæ¶µè­å¥ç³»çµ± (Entailment Analysis for Improving {C}hinese Recognizing Textual Entailment System) [In {C}hinese],2013,16,2,2,1,41520,shanshun yang,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,"Recognizing Textual Entailment (RTE) is a new research issue in natural language processing (NLP) research area. RTE can be a useful component in many NLP applications. In this paper, we introduce our finding on the entailment analysis of the NTCIR-10 RITE-2 dataset, and use the observation to improve our system. In the previous works, all the input pairs are treated equally in a standard classification architecture. We find that is not suitable for some special cases. We believe that by isolating the special cases and building separated classifiers, a RTE system can perform better. After implementing modules for four special cases into our system, the result is significantly improved from 67.86% to 72.92% on the binary class classification task."
W12-6339,Sentence Parsing with Double Sequential Labeling in Traditional {C}hinese Parsing Task,2012,7,1,1,1,2436,shihhung wu,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"In this paper, we propose a new sequential labeling scheme, double sequential labeling, that we apply it on Chinese parsing. The parser is built with conditional random field (CRF) sequential labeling models. One focuses on the beginning of a phrase and the phrase type, while the other focuses on the end of a phrase. Our system, CYUT, attended 2012 the second CIPS-SGHAN conference Bake-off Task4, traditional Chinese parsing task, and got promising result on the sentence parsing task."
O12-1034,åºæ¼å®èªè¨æ©å¨ç¿»è­¯æè¡æ¹é²ä¸­ææå­èæ¶µ (Improving {C}hinese Textural Entailment by Monolingual Machine Translation Technology) [In {C}hinese],2012,0,0,2,1,41520,shanshun yang,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,None
O11-3004,ä»¥ç± çµ±æ¥è©¢è©ä¼°æ¥è©¢æ´å±æ¹æ³èç·ä¸æå°å¼æä¹è³è¨æª¢ç´¢æè½ (Evaluating the Information Retrieval Performance of Query Expansion Method and On-line Search Engine on General Query) [In {C}hinese],2011,0,0,2,0,44705,chihchuan hsu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 16, Number 1-2, March/June 2011",0,None
O11-2008,ä¸­ææå­èæ¶µç³»çµ±ä¹ç¹å¾µåæ (Feature Analysis of {C}hinese Textual Entailment System) [In {C}hinese],2011,29,0,2,0,44714,wanchi huang,{ROCLING} 2011 Poster Papers,0,None
W10-4107,Reducing the False Alarm Rate of {C}hinese Character Error Detection and Correction,2010,9,23,1,1,2436,shihhung wu,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"The main drawback of previous Chinese character error detection systems is the high false alarm rate. To solve this problem, we propose a system that combines a statistic method and template matching to detect Chinese character errors. Error types include pronunciationrelated errors and form-related errors. Possible errors of a character can be collected to form a confusion set. Our system automatically generates templates with the help of a dictionary and confusion sets. The templates can be used to detect and correct errors in essays. In this paper, we compare three methods proposed in previous works. The experiment results show that our system can reduce the false alarm significantly and give the best performance on fscore."
O10-4003,Improving the Template Generation for {C}hinese Character Error Detection with Confusion Sets,2010,7,1,2,1,45141,yongzhi chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 15, Number 2, June 2010",0,"In this paper, we propose a system that automatically generates templates for detecting Chinese character errors. We first collect the confusion sets for each high-frequency Chinese character. Error types include pronunciation-related errors and radical-related errors. With the help of the confusion sets, our system generates possible error patterns in context, which will be used as detection templates. Combined with a word segmentation module, our system generates more accurate templates. The experimental results show the precision of performance approaches 95%. Such a system should not only help teachers grade and check student essays, but also effectively help students learn how to write."
O10-3002,"åºæ¼å°ç\
§è¡¨ä»¥åèªè¨æ¨¡åä¹ç°¡ç¹å­é«è½æ ({C}hinese Characters Conversion System based on Lookup Table and Language Model) [In {C}hinese]",2010,0,9,2,0,45734,minhsiang li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 15, Number 1, March 2010",0,None
O10-1008,"åºæ¼å°ç\
§è¡¨ä»¥åèªè¨æ¨¡åä¹ç°¡ç¹å­é«è½æ ({C}hinese Characters Conversion System based on Lookup Table and Language Model) [In {C}hinese]",2010,0,1,2,0,45734,minhsiang li,Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2010),0,None
W09-3412,Phonological and Logographic Influences on Errors in Written {C}hinese Words,2009,7,14,5,0,15624,chaolin liu,Proceedings of the 7th Workshop on {A}sian Language Resources ({ALR}7),0,"We analyze a collection of 3208 reported errors of Chinese words. Among these errors, 7.2% involved rarely used character, and 98.4% were assigned common classifications of their causes by human subjects. In particular, 80% of the errors observed in the writings of middle school students were related to the pronunciations and 30% were related to the logographs of the words. We conducted experiments that shed light on using the Web-based statistics to correct the errors, and we designed a software environment for preparing test items whose authors intentionally replace correct characters with wrong ones. Experimental results show that using Web-based statistics can help us correct only about 75% of these errors. In contrast, Web-based statistics are useful for recommending incorrect characters for composing test items for incorrect character identification tests about 93% of the time."
P09-2007,Capturing Errors in Written {C}hinese Words,2009,5,7,5,0,15624,chaolin liu,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"A collection of 3208 reported errors of Chinese words were analyzed. Among which, 7.2% involved rarely used character, and 98.4% were assigned common classifications of their causes by human subjects. In particular, 80% of the errors observed in writings of middle school students were related to the pronunciations and 30% were related to the compositions of words. Experimental results show that using intuitive Web-based statistics helped us capture only about 75% of these errors. In a related task, the Web-based statistics are useful for recommending incorrect characters for composing test items for incorrect character identification tests about 93% of the time."
O09-2007,ä¸­ææ··æ·å­éæç¨æ¼å¥å­åµé¯æ¨¡æ¿èªåç¢ç ({C}hinese Confusion Word Set for Automatic Generation of Spelling Error Detecting Template) [In {C}hinese],2009,0,4,2,1,45141,yongzhi chen,{ROCLING} 2009 Poster Papers,0,None
O08-6004,Automatic Wikibook Prototyping via Mining {W}ikipedia,2008,17,0,2,0,47933,jenliang chou,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 4, {D}ecember 2008",0,"Wikipedia is the world's largest collaboratively edited source of encyclopedic knowledge. Wikibook is a sub-project of Wikipedia that is intended to create a book that can be edited by various contributors, similar to how Wikipedia is composed and edited. Editing a book, however, requires more effort than editing separate articles. Therefore, methods of quickly prototyping a book is a new research issue. In this paper, we investigate how to automatically extract content from Wikipedia and generate a prototype of a Wikibook as a start point for further editing. Applying search technology, our system can retrieve relevant articles from Wikipedia. A table of contents is built automatically and is based on a two-stage searching method. Our experiments show that, given a keyword as the title of a book, our system can generate a table of contents, which can be treated as a prototype of a Wikibook. Such a system can help free textbook editing. We propose an evaluation method based on the comparison of system results to a traditional textbook and show the coverage of our system."
O06-1023,An Evaluation of Adopting Language Model as the Checker of Preposition Usage,2006,12,4,1,1,2436,shihhung wu,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,"Many grammar checkers in rule-based approach do not handle errors that come from various usages, for example, the usages of prepositions. To study the behavior of prepositions, we introduce the language model into a grammarchecking task. A language model is trained from a large training corpus, which contains many short phrases. It can be used for detecting and correcting certain types of grammar errors, where local information is sufficient to make decision. We conduct several experiments on finding the correct English prepositions. The experiment results show that the accuracy of open test is 71% and the accuracy of closed test is 89%. The accuracy is 70% on TOEFL-level tests."
O05-1017,Applying Maximum Entropy to Robust {C}hinese Shallow Parsing,2005,19,6,1,1,2436,shihhung wu,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,"Recently, shallow parsing has been applied to various information processing systems, such as information retrieval, information extraction, question answering, and automatic document summarization. A shallow parser is suitable for online applications, because it is much more efficient and less demanding than a full parser. In this research, we formulate shallow parsing as a sequential tagging problem and use a supervised machine learning technique, Maximum Entropy (ME), to build a Chinese shallow parser. The major features of the ME-based shallow parser are POSs and the context words in a sentence. We adopt the shallow parsing results of Sinica Treebank as our standard, and select 30,000 and 10,000 sentences from Sinica Treebank as the training set and test set respectively. We then test the robustness of the shallow parser with noisy data. The experiment results show that the proposed shallow parser is quite robust for sentences with unknown proper nouns."
O04-2004,{M}encius: A {C}hinese Named Entity Recognizer Using the Maximum Entropy-based Hybrid Model,2004,16,28,2,1,37579,tzonghan tsai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 9, Number 1, {F}ebruary 2004: Special Issue on Selected Papers from {ROCLING} {XV}",0,"This paper presents a Chinese named entity recognizer (NER): Mencius. It aims to address Chinese NER problems by combining the advantages of rule-based and machine learning (ML) based NER systems. Rule-based NER systems can explicitly encode human comprehension and can be tuned conveniently, while ML-based systems are robust, portable and inexpensive to develop. Our hybrid system incorporates a rule-based knowledge representation and template-matching tool, called InfoMap [Wu et al. 2002], into a maximum entropy (ME) framework. Named entities are represented in InfoMap as templates, which serve as ME features in Mencius. These features are edited manually, and their weights are estimated by the ME framework according to the training data. To understand how word segmentation might influence Chinese NER and the differences between a pure template-based method and our hybrid method, we configure Mencius using four distinct settings. The F-Measures of person names (PER), location names (LOC) and organization names (ORO) of the best configuration in our experiment were respectively 94.3%, 77.8% and 75.3%. From comparing the experiment results obtained using these configurations reveals that hybrid NER Systems always perform better performance in identifying person names. On the other hand, they have a little difficulty identifying location and organization names. Furthermore, using a word segmentation module improves the performance of pure Template-based NER Systems, but, it has little effect on hybrid NER systems."
O04-1032,The Construction of a {C}hinese Named Entity Tagged Corpus: {CNEC}1.0,2004,8,4,3,0,40686,chengwei shih,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,"In order to build an automatic named entity recognition (NER) system for machine learning, a large tagged corpus is necessary. This paper describes the manual construction of a Chinese named entity tagged corpus (CNEC 1.0) that can be used to improve NER performance. In this project, we define five named entity tags: PER (person name), LOC (location name), ORG (organization name), LAO (location as organization), and OAL (organization as location) for named entity categories. In addition, we propose a special tag, DIFF (Difficulty), to annotate ambiguous cases during corpus construction. A, corpus-annotating procedure, a tagging tool, and an original corpus are also introduced. Finally, we demonstrate a part of our manual-tagged corpus."
W03-1118,Text Categorization Using Automatically Acquired Domain Ontology,2003,11,23,1,1,2436,shihhung wu,Proceedings of the Sixth International Workshop on Information Retrieval with {A}sian Languages,0,"In this paper, we describe ontology-based text categorization in which the domain ontologies are automatically acquired through morphological rules and statistical methods. The ontology-based approach is a promising way for general information retrieval applications such as knowledge management or knowledge discovery. As a way to evaluate the quality of domain ontologies, we test our method through several experiments. Automatically acquired domain ontologies, with or without manual editing, have been used for text categorization. The results are quite satisfactory. Furthermore, we have developed an automatic method to evaluate the quality of our domain ontology."
O03-1012,{M}encius: A {C}hinese Named Entity Recognizer Using Hybrid Model,2003,16,0,2,1,37579,tzonghan tsai,Proceedings of Research on Computational Linguistics Conference {XV},0,"This paper presents a maximum entropy based Chinese named entity recognizer (NER): Mencius. It aims to address Chinese NER problems by combining the advantages of rule-based and machine learning (ML) based NER systems. Rule-based NER systems can explicitly encode human comprehension and can be tuned conveniently, while ML-based systems are robust, portable and inexpensive to develop. Our hybrid system incorporates a rule-based knowledge representation and template-matching tool, InfoMap [1], into a maximum entropy (ME) framework. Named entities are represented in InfoMap as templates, which serve as ME features in Mencius. These features are edited manually and their weights are estimated by the ME framework according to the training data. To avoid the errors caused by word segmentation, we model the NER problem as a character-based tagging problem. In our experiments, Mencius outperforms both pure rule-based and pure ME-based NER systems. The F-Measures of person names (PER), location names (LOC) and organization names (ORG) in the experiment are respectively 92.4%, 73.7% and 75.3%."
C02-2013,{SOAT}: A Semi-Automatic Domain Ontology Acquisition Tool from {C}hinese Corpus,2002,14,32,1,1,2436,shihhung wu,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"In this paper, we focus on the domain ontology acquisition from Chinese corpus by extracting rules designed for Chinese phrases. These rules are noun sequences with part-of-speech tags. Experiments show that this process can construct domain ontology prototypes efficiently and effectively."
O98-4003,An Assessment of Character-based {C}hinese News Filtering Using Latent Semantic Indexing,1998,0,2,1,1,2436,shihhung wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 2, August 1998",0,"We assess the Latent Semantic Indexing (LSI) approach to Chinese information filtering. In particular, the approach is for Chinese news filtering agents that use a character-based and hierarchical filtering scheme. The traditional vector space model is employed as an information filtering model, and each document is converted into a vector of weights of terms. Instead of using words as terms in the JR nominating tradition, terms refer to Chinese characters. LSI captures the semantic relationship between documents and Chinese characters. We use the Sin-gular-value Decomposition (SVD) technique to compress the term space into a lower dimension which achieves latent association between documents and terms. The results of experiments show that the recall and precision rates of Chinese news filtering using the character-based ap-proach incorporating the LSI technique are satisfactory."
O97-1014,An Assessment on Character-based {C}hinese News Filtering Using Latent Semantic Indexing,1997,0,1,1,1,2436,shihhung wu,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
