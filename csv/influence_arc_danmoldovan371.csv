1991.mtsummit-papers.15,P90-1006,0,0.0637758,"Missing"
1991.mtsummit-papers.15,W89-0201,0,0.0251679,"Missing"
1991.mtsummit-papers.15,1998.amta-tutorials.6,0,0.0721947,"Missing"
1991.mtsummit-papers.15,E89-1010,1,0.886847,"Missing"
1991.mtsummit-papers.15,C90-3044,0,0.0387073,"Missing"
1991.mtsummit-papers.15,P91-1024,0,\N,Missing
1991.mtsummit-papers.15,P87-1022,0,\N,Missing
2020.coling-main.30,W18-0701,0,0.0268224,"Missing"
2020.coling-main.30,W17-2408,0,0.0225322,"ed corpus. It also explores the addition of features and experiments to evaluate the same. Section 4 presents the baseline models, experiments carried out, and the results obtained. Error analysis of the results is covered in Section 5, followed by a discussion on the problem of missing context in Section 6. Section 7 concludes the work done in this paper. 2 Related Work Email processing has been an active research topic with the earlier works focusing on email classification (Cohen and others, 1996; Whittaker and Sidner, 1996; Brutlag and Meek, 2000; Manco et al., 2002; Klimt and Yang, 2004; Alkhereyf and Rambow, 2017). This was later followed by work on intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006; Minkov et al., 2008), clustering (Huang and Mitchell, 2008), and summarization (Muresan et al., 2001; Lam, 2002; Newman and Blitzer, 2003; Nenkova and Bagga, 2004; Corston-Oliver et al., 2004; Rambow et al., 2004; Carenini et al., 2007; Ulrich et al., 2008). One of the earliest works highlighting the challenges of the thread-like nature of email conversations was carried out by Lewis and Knowles (1997). Murakoshi et. al (2000) proposed the creation of extended contribution trees t"
2020.coling-main.30,W16-3612,0,0.0190939,"oration. Similarly, the Avocado Research Email Collection (Douglas Oard, 2015) consists of emails from 282 accounts of a now-defunct IT company. The task of coreference resolution, specifically entity resolution, has received attention in the natural language research community since the 1960s with noun-phrase and pronomial resolution being the early forms of the task. Although multiple corpora released over the years contain a small fraction of telephonic speech text, only a few corpora have focused on the study of the task in a purely conversational setting. Character Identification Corpus (Chen and Choi, 2016) was the first corpus to focus on the entity-linking task in this setting. It was constructed using TV show transcripts with annotations for speakers in a multi-party conversation. Aktas¸ et. al (2018) used a Twitter corpus to study the performance of Stanford statistical coreference system (Clark and Manning, 2015). They evaluated a corpus with 185 threads containing 278 coreference chains and reported a mediocre performance by the model. 3 Corpus 3.1 Seed Corpus Dakle et al. (2020) in their study on entity resolution released a small manually annotated corpus containing 46 email threads from"
2020.coling-main.30,P15-1136,0,0.021651,"nd pronomial resolution being the early forms of the task. Although multiple corpora released over the years contain a small fraction of telephonic speech text, only a few corpora have focused on the study of the task in a purely conversational setting. Character Identification Corpus (Chen and Choi, 2016) was the first corpus to focus on the entity-linking task in this setting. It was constructed using TV show transcripts with annotations for speakers in a multi-party conversation. Aktas¸ et. al (2018) used a Twitter corpus to study the performance of Stanford statistical coreference system (Clark and Manning, 2015). They evaluated a corpus with 185 threads containing 278 coreference chains and reported a mediocre performance by the model. 3 Corpus 3.1 Seed Corpus Dakle et al. (2020) in their study on entity resolution released a small manually annotated corpus containing 46 email threads from the Enron Email Corpus2 (Klimt and Yang, 2004). The Enron Email Corpus is a multi-lingual corpus with the majority of email threads in English. The corpus consists of email threads organized in a directory structure for each user. The annotated corpus consists of 245 email messages with 866 coreference chains conta"
2020.coling-main.30,W04-3240,0,0.0409087,"me. Section 4 presents the baseline models, experiments carried out, and the results obtained. Error analysis of the results is covered in Section 5, followed by a discussion on the problem of missing context in Section 6. Section 7 concludes the work done in this paper. 2 Related Work Email processing has been an active research topic with the earlier works focusing on email classification (Cohen and others, 1996; Whittaker and Sidner, 1996; Brutlag and Meek, 2000; Manco et al., 2002; Klimt and Yang, 2004; Alkhereyf and Rambow, 2017). This was later followed by work on intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006; Minkov et al., 2008), clustering (Huang and Mitchell, 2008), and summarization (Muresan et al., 2001; Lam, 2002; Newman and Blitzer, 2003; Nenkova and Bagga, 2004; Corston-Oliver et al., 2004; Rambow et al., 2004; Carenini et al., 2007; Ulrich et al., 2008). One of the earliest works highlighting the challenges of the thread-like nature of email conversations was carried out by Lewis and Knowles (1997). Murakoshi et. al (2000) proposed the creation of extended contribution trees to better understand the conversation structure of email threads. The impact of"
2020.coling-main.30,W04-1008,0,0.143685,"6. Section 7 concludes the work done in this paper. 2 Related Work Email processing has been an active research topic with the earlier works focusing on email classification (Cohen and others, 1996; Whittaker and Sidner, 1996; Brutlag and Meek, 2000; Manco et al., 2002; Klimt and Yang, 2004; Alkhereyf and Rambow, 2017). This was later followed by work on intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006; Minkov et al., 2008), clustering (Huang and Mitchell, 2008), and summarization (Muresan et al., 2001; Lam, 2002; Newman and Blitzer, 2003; Nenkova and Bagga, 2004; Corston-Oliver et al., 2004; Rambow et al., 2004; Carenini et al., 2007; Ulrich et al., 2008). One of the earliest works highlighting the challenges of the thread-like nature of email conversations was carried out by Lewis and Knowles (1997). Murakoshi et. al (2000) proposed the creation of extended contribution trees to better understand the conversation structure of email threads. The impact of coreference resolution on conversations in a threaded format was first studied by Hendrickx and Hoste (2009) using a corpus of blogs and commented news for opinion mining. Although the impact was negative, it was attributed to"
2020.coling-main.30,2020.lrec-1.8,1,0.885516,"n object or a group of objects in the real world and a span of text referring to an entity is called a Mention. When all mentions in a text which refer to the same real-world entity are linked together, they form a coreference chain. Example 1. Example of entity resolution task in email conversations Date: Mon, 17 Dec 2001 14:28:03 -0800 (PST) From: g..barkowsky@enron.com(1) To: theresa.staab@enron.com(2) Subject: RE: Final Statements and Invoices for November X-From: Barkowsky, Gloria G.(1) X-To: Staab, Theresa(2) yes, I(1) ’ll do this. Do you(2) have anything for Crestone and Lost Creek(3)? Dakle et al. (2020) first studied entity resolution in email conversations using a small annotated corpus. Following the same task definition, this paper builds on their work and makes the following key contributions: 1. A large corpus for entity resolution in email conversations (CEREC), weakly annotated for mentions and coreference chains, is presented. Detailed corpus statistics are also discussed. The corpus will be released along with the paper1 . This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://github.com/paragdakle/emailc"
2020.coling-main.30,C96-1079,0,0.728358,"ep process with minimal manual effort. Experiments are carried out for evaluating different features and performance of four baselines on the created corpus. For the task of mention identification and coreference resolution, a best performance of 54.1 F1 is reported, highlighting the room for improvement. An in-depth qualitative and quantitative error analysis is presented to understand the limitations of the baselines considered. 1 Introduction Entity resolution is defined as linking referring spans of text that point to the same discourse entity by CoNLL 2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996) shared tasks. The corpora used for this task primarily consist of text from news (Pradhan et al., 2012; Cybulska and Vossen, 2014; Recasens et al., 2010; Grishman and Sundheim, 1996), web-logs and transcripted dialogs. This research focusses on the entity resolution task for email conversations. Example 1 shows a sample email message and the corresponding entities. The boldfaced tokens represent entities and the numbers beside them represent coreference chain identifiers. An Entity is defined as an object or a group of objects in the real world and a span of text referring to an entity is cal"
2020.coling-main.30,D19-1588,0,0.0722563,"nts Non-English content Seed corpus overlap Accepted Email Threads Total 2,867 564 75 54 20 6,144 9,724 Table 1: Distribution of email threads per filtering category 3 A digital entity is a media or pointer to a media which is present on some form of digital storage (Dakle et al., 2020) Only 43 email threads out of the 46 have been used in this work as 3 email threads were discarded due to their overlap with the other email threads in SC 4 341 3.3 Annotation The annotation procedure is divided into two parts: mention annotation and coreference annotation. For both parts, pre-trained SpanBERT (Joshi et al., 2019a) variant of the model proposed by Joshi et. al (2019b)5 is used6 . Henceforth, we will refer to this model as VanillaSpanBERT (for additional description of the model see 4.1). 3.3.1 Mention Annotation Given an email thread, correctly identifying spans of text which refer to an entity is the task of mention identification. Here, mention identification task is framed as identifying a single coreference chain which consists of all spans of text referring to a valid entity. A valid entity is an entity of the type PERSON, ORGANIZATION, LOCATION or DIGITAL. Consider Example 1, here the single cor"
2020.coling-main.30,N18-2108,0,0.0132601,"nder or an alias of the sender is one of the recipients of the email 344 c2f-coref (C2F): The model proposed by Lee et. al (2018) is used for this baseline9 . This was the first end-to-end neural coreference resolution model. It uses highway LSTMs to generate embeddings for each span and then with a span-ranking model decides which of the previous spans is a suitable antecedent (if any). The inputs to the LSTMs are embedding representations from a language model (Peters et al., 2018). VanillaSpanBERT (SBERT): Joshi et. al (2019b) proposed a BERT (Devlin et al., 2018) version of the C2F model (Lee et al., 2018). Joshi et. al (2019b) introduced BERT to obtain all input embedding representations. For this baseline, the SpanBERT (Joshi et al., 2019a) variant of the model is used as the baseline owing to its performance gains. 4.2 Experimental Setup The training set for these experiments is CEREC containing 6001 email threads and the validation set contains 34 email threads, the one used for coreference annotation. The SC containing 43 email threads is used as the test set. Mention detection and coreference resolution are the two tasks evaluated in these experiments. The following three experiments are"
2020.coling-main.30,P16-1060,0,0.0188379,"ax span width, max training sentences and epochs are set to 20, 30 and 10 respectively. This is done to make training tractable on the environment. For the SBERT baseline, the spanbert base model is used with a maximum segment length of 256, and training is carried out on an NVIDIA GeForce GTX 1080 Ti GPU with 8 12gb cores. 4.3 Evaluation Metrics This work follows the standard experimental setup used in the CoNLL 2012 Shared task. Primary evaluation is done using the unweighted average of MUC, B 3 , and CEAFE metrics (Pradhan et al., 2012)10 . In addition to this, scores using the LEA metric (Moosavi and Strube, 2016) are also reported. 4.4 Results Table 6 shows results of Exp1 and Exp2 for all baselines. First, it can be seen that how first-person plural pronouns are resolved in the header baselines does not have a significant impact on the average F1 score. Second, the average F1 score of SBERT is just 0.23 F1 points higher than the C2F baseline. This shows that increasing the maximum sentence length and maximum training sentences do not help C2F in outperforming SBERT. Both models perform equally well. Compared to the results reported by Dakle et. al (2020), the SBERT baseline performs slightly better."
2020.coling-main.30,W01-0719,0,0.0633998,"in Section 5, followed by a discussion on the problem of missing context in Section 6. Section 7 concludes the work done in this paper. 2 Related Work Email processing has been an active research topic with the earlier works focusing on email classification (Cohen and others, 1996; Whittaker and Sidner, 1996; Brutlag and Meek, 2000; Manco et al., 2002; Klimt and Yang, 2004; Alkhereyf and Rambow, 2017). This was later followed by work on intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006; Minkov et al., 2008), clustering (Huang and Mitchell, 2008), and summarization (Muresan et al., 2001; Lam, 2002; Newman and Blitzer, 2003; Nenkova and Bagga, 2004; Corston-Oliver et al., 2004; Rambow et al., 2004; Carenini et al., 2007; Ulrich et al., 2008). One of the earliest works highlighting the challenges of the thread-like nature of email conversations was carried out by Lewis and Knowles (1997). Murakoshi et. al (2000) proposed the creation of extended contribution trees to better understand the conversation structure of email threads. The impact of coreference resolution on conversations in a threaded format was first studied by Hendrickx and Hoste (2009) using a corpus of blogs and"
2020.coling-main.30,N18-1202,0,0.00822488,"tification or company advertisement. All privacy notifications have been ignored in this work. 8 This excludes the cases when the sender or an alias of the sender is one of the recipients of the email 344 c2f-coref (C2F): The model proposed by Lee et. al (2018) is used for this baseline9 . This was the first end-to-end neural coreference resolution model. It uses highway LSTMs to generate embeddings for each span and then with a span-ranking model decides which of the previous spans is a suitable antecedent (if any). The inputs to the LSTMs are embedding representations from a language model (Peters et al., 2018). VanillaSpanBERT (SBERT): Joshi et. al (2019b) proposed a BERT (Devlin et al., 2018) version of the C2F model (Lee et al., 2018). Joshi et. al (2019b) introduced BERT to obtain all input embedding representations. For this baseline, the SpanBERT (Joshi et al., 2019a) variant of the model is used as the baseline owing to its performance gains. 4.2 Experimental Setup The training set for these experiments is CEREC containing 6001 email threads and the validation set contains 34 email threads, the one used for coreference annotation. The SC containing 43 email threads is used as the test set. Me"
2020.coling-main.30,W12-4501,0,0.169241,"tion is carried out as a two-step process with minimal manual effort. Experiments are carried out for evaluating different features and performance of four baselines on the created corpus. For the task of mention identification and coreference resolution, a best performance of 54.1 F1 is reported, highlighting the room for improvement. An in-depth qualitative and quantitative error analysis is presented to understand the limitations of the baselines considered. 1 Introduction Entity resolution is defined as linking referring spans of text that point to the same discourse entity by CoNLL 2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996) shared tasks. The corpora used for this task primarily consist of text from news (Pradhan et al., 2012; Cybulska and Vossen, 2014; Recasens et al., 2010; Grishman and Sundheim, 1996), web-logs and transcripted dialogs. This research focusses on the entity resolution task for email conversations. Example 1 shows a sample email message and the corresponding entities. The boldfaced tokens represent entities and the numbers beside them represent coreference chain identifiers. An Entity is defined as an object or a group of objects in the real world and a span"
2020.coling-main.30,N04-4027,0,0.102791,"ork done in this paper. 2 Related Work Email processing has been an active research topic with the earlier works focusing on email classification (Cohen and others, 1996; Whittaker and Sidner, 1996; Brutlag and Meek, 2000; Manco et al., 2002; Klimt and Yang, 2004; Alkhereyf and Rambow, 2017). This was later followed by work on intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006; Minkov et al., 2008), clustering (Huang and Mitchell, 2008), and summarization (Muresan et al., 2001; Lam, 2002; Newman and Blitzer, 2003; Nenkova and Bagga, 2004; Corston-Oliver et al., 2004; Rambow et al., 2004; Carenini et al., 2007; Ulrich et al., 2008). One of the earliest works highlighting the challenges of the thread-like nature of email conversations was carried out by Lewis and Knowles (1997). Murakoshi et. al (2000) proposed the creation of extended contribution trees to better understand the conversation structure of email threads. The impact of coreference resolution on conversations in a threaded format was first studied by Hendrickx and Hoste (2009) using a corpus of blogs and commented news for opinion mining. Although the impact was negative, it was attributed to the poor performance"
2020.coling-main.30,S10-1001,0,0.0888751,"Missing"
2020.lrec-1.135,W12-1623,0,0.0518499,"76 the impact of casting the problem in an alternate fashion and of the syntactic features considered, we analyze errors made by the model on the eng.rst.rstdt dataset. 4.1. Comparison with Existing Models We first compare the results obtained for the eng.rst.rstdt corpus with previous work done. To ensure fair comparison, we report results for discourse segmentation at the sentence-level and not at the document-level (Braud et al., 2017). Table 3 reports the performance of our model and other competing systems. Model Soricut and Marcu (2003) Subba and Di Eugenio (2007) Hernault et al. (2010) Bach et al. (2012) Feng and Hirst (2014) Wang et al. (2018) Lin et al. (2019) Muller et al. (2019) Our Model Human P 84.1 85.5 91.0 91.5 92.8 92.9 94.1 95.3 96.3 98.5 R 85.4 86.6 87.2 90.4 92.3 95.7 96.6 96.8 97.0 98.2 4.2. Sentence length v/s Number of errors We compare the proportion of errors made by the models with respect to the length of the sentence i.e. the number of tokens in the sentence. For the purpose of evaluation, we consider a sentence to be incorrectly segmented regardless of whether the type of error (Bach et al., 2012) is over (i.e. a sentence is segmented when it should not be) or miss (a se"
2020.lrec-1.135,D17-1258,0,0.266157,"m the RST-DT corpus (Carlson et al., 2003) and the tree was constructed using the tool provided by Gessler et al. (2019). imal and this makes training of data-hungry models like neural networks difficult. To assuage the problem of data insufficiency, syntax-free models that leverage pre-trained representations (Wang et al., 2018; Muller et al., 2019) from sentence encoders like ElMo and BERT were proposed: these models achieved very good results for the segmentation task. Likewise, the use of syntactic features such as part-of-speech tags and parse tree features helped achieve better results (Braud et al., 2017; Lin et al., 2019). In fact, the latter achieved state-of-the-art results using pointer networks (Vinyals et al., 2015) with parse tree features. In this paper, we propose a few changes that leverage BERT’s (Devlin et al., 2019) structure in performing segmentation. Our main contributions are: 1. We cast discourse segmentation as a token classification problem, as opposed to sequence tagging. This allows BERT to attend to one token at a time and not the entire sequence, thereby making better decisions. 2. We suggest a simple multi-task learning approach that uses the intermediate layers of BE"
2020.lrec-1.135,N19-1423,0,0.549139,"of data insufficiency, syntax-free models that leverage pre-trained representations (Wang et al., 2018; Muller et al., 2019) from sentence encoders like ElMo and BERT were proposed: these models achieved very good results for the segmentation task. Likewise, the use of syntactic features such as part-of-speech tags and parse tree features helped achieve better results (Braud et al., 2017; Lin et al., 2019). In fact, the latter achieved state-of-the-art results using pointer networks (Vinyals et al., 2015) with parse tree features. In this paper, we propose a few changes that leverage BERT’s (Devlin et al., 2019) structure in performing segmentation. Our main contributions are: 1. We cast discourse segmentation as a token classification problem, as opposed to sequence tagging. This allows BERT to attend to one token at a time and not the entire sequence, thereby making better decisions. 2. We suggest a simple multi-task learning approach that uses the intermediate layers of BERT to carry out part-of-speech tag prediction, and dependency relation classification. This improves model performance, particularly for languages other than English. 3. Experiments are performed for different languages to demons"
2020.lrec-1.135,W19-2708,0,0.0284944,"oint chunks of text called Elementary Discourse Units (EDUs). In the context of Rhetorical Structure Theory (Mann and Thompson, 1988) or RST, EDUs form the nodes of a discourse tree; while relations between EDUs form arcs or edges between nodes. As a motivating example, consider the discourse tree given in Figure 1. EDUs labeled 1, 2 and 3 form nodes of the tree; and arcs are labeled with ATTRIBUTION (used to indicate instances of reported speech) and PURPOSE relations. This example was taken from the RST-DT corpus (Carlson et al., 2003) and the tree was constructed using the tool provided by Gessler et al. (2019). imal and this makes training of data-hungry models like neural networks difficult. To assuage the problem of data insufficiency, syntax-free models that leverage pre-trained representations (Wang et al., 2018; Muller et al., 2019) from sentence encoders like ElMo and BERT were proposed: these models achieved very good results for the segmentation task. Likewise, the use of syntactic features such as part-of-speech tags and parse tree features helped achieve better results (Braud et al., 2017; Lin et al., 2019). In fact, the latter achieved state-of-the-art results using pointer networks (Vin"
2020.lrec-1.135,P19-1410,0,0.3793,"(Carlson et al., 2003) and the tree was constructed using the tool provided by Gessler et al. (2019). imal and this makes training of data-hungry models like neural networks difficult. To assuage the problem of data insufficiency, syntax-free models that leverage pre-trained representations (Wang et al., 2018; Muller et al., 2019) from sentence encoders like ElMo and BERT were proposed: these models achieved very good results for the segmentation task. Likewise, the use of syntactic features such as part-of-speech tags and parse tree features helped achieve better results (Braud et al., 2017; Lin et al., 2019). In fact, the latter achieved state-of-the-art results using pointer networks (Vinyals et al., 2015) with parse tree features. In this paper, we propose a few changes that leverage BERT’s (Devlin et al., 2019) structure in performing segmentation. Our main contributions are: 1. We cast discourse segmentation as a token classification problem, as opposed to sequence tagging. This allows BERT to attend to one token at a time and not the entire sequence, thereby making better decisions. 2. We suggest a simple multi-task learning approach that uses the intermediate layers of BERT to carry out par"
2020.lrec-1.135,moldovan-blanco-2012-polaris,1,0.729203,"erformed show how injecting syntax into the model helped achieve better results. In particular, the joint learning of syntactic features allowed the model uncover complex syntactic patterns that could not be captured by simply fine-tuning BERT. Further, the use of syntactic features helped achieve solid gains for languages such as German and Dutch; highlighting both the importance of syntax; and also certain limitations of multilingual BERT. Several complex cases of discourse segmentation could be effectively captured by our model. We believe that having knowledge of sentence-level semantics (Moldovan and Blanco, 2012) may help identify such nuanced patterns even better. This was in fact empirically proven by Lin et al. (2019) who jointly carried out discourse segmentation and coherence relation classification, observing an incremental improvement in model performance. A potential drawback of our system is that the time taken to tag a full sequence is quite large as the model performs sentence segmentation in O(n) time while other models take O(S) time, n being the number of tokens in the document, and S << n being the number of sentences in the document. However, with a sufficiently large batch size; and t"
2020.lrec-1.135,W19-2715,0,0.590354,"tween nodes. As a motivating example, consider the discourse tree given in Figure 1. EDUs labeled 1, 2 and 3 form nodes of the tree; and arcs are labeled with ATTRIBUTION (used to indicate instances of reported speech) and PURPOSE relations. This example was taken from the RST-DT corpus (Carlson et al., 2003) and the tree was constructed using the tool provided by Gessler et al. (2019). imal and this makes training of data-hungry models like neural networks difficult. To assuage the problem of data insufficiency, syntax-free models that leverage pre-trained representations (Wang et al., 2018; Muller et al., 2019) from sentence encoders like ElMo and BERT were proposed: these models achieved very good results for the segmentation task. Likewise, the use of syntactic features such as part-of-speech tags and parse tree features helped achieve better results (Braud et al., 2017; Lin et al., 2019). In fact, the latter achieved state-of-the-art results using pointer networks (Vinyals et al., 2015) with parse tree features. In this paper, we propose a few changes that leverage BERT’s (Devlin et al., 2019) structure in performing segmentation. Our main contributions are: 1. We cast discourse segmentation as a"
2020.lrec-1.135,P19-1493,0,0.0144391,"e pre-trained BERT models on downstream tasks, especially when training data is minimal. This allows us to get away with the data insufficiency problem since the size of training corpora is small. To work with languages other than English, Devlin et al. (2019) released multilingual BERT: a single language model pre-trained from multi-lingual corpora in 104 languages; using a shared multi-lingual vocabulary. Despite being a shared language model, which could raise concerns about its cross-lingual effectiveness, multilingual BERT has given surprisingly good results for different language tasks (Pires et al., 2019; Wu and Dredze, 2019). 2.2. Problem Formulation Deep learning frameworks (Braud et al., 2017; Wang et al., 2018) cast discourse segmentation as a BI tagging problem, where B indicates the beginning of a new span and I indi1074 root ccomp xcomp nummod nsubj nsubj aux obj det obj compound Pepsi said it will spend $ 10 million advertising the promotion NNS VBD PRP MD VB $ CD CD VBG DT NN Figure 3: An example showing the features captured for each token in the sentence: we consider the gold part-of-speech tags and dependency relations with respect to the masked token. cates the continuation of a"
2020.lrec-1.135,redeker-etal-2012-multi,0,0.482159,"Missing"
2020.lrec-1.135,N03-1030,0,0.608871,"nge in formulation alone helped achieve impressive results. To understand 1076 the impact of casting the problem in an alternate fashion and of the syntactic features considered, we analyze errors made by the model on the eng.rst.rstdt dataset. 4.1. Comparison with Existing Models We first compare the results obtained for the eng.rst.rstdt corpus with previous work done. To ensure fair comparison, we report results for discourse segmentation at the sentence-level and not at the document-level (Braud et al., 2017). Table 3 reports the performance of our model and other competing systems. Model Soricut and Marcu (2003) Subba and Di Eugenio (2007) Hernault et al. (2010) Bach et al. (2012) Feng and Hirst (2014) Wang et al. (2018) Lin et al. (2019) Muller et al. (2019) Our Model Human P 84.1 85.5 91.0 91.5 92.8 92.9 94.1 95.3 96.3 98.5 R 85.4 86.6 87.2 90.4 92.3 95.7 96.6 96.8 97.0 98.2 4.2. Sentence length v/s Number of errors We compare the proportion of errors made by the models with respect to the length of the sentence i.e. the number of tokens in the sentence. For the purpose of evaluation, we consider a sentence to be incorrectly segmented regardless of whether the type of error (Bach et al., 2012) is o"
2020.lrec-1.135,stede-neumann-2014-potsdam,0,0.260238,"and ablation study: As is evident from the obtained results, casting the problem as a token classification (Token) problem led to significant improvement. Likewise, training for POS tags (Post), dependency relations (Depend) or both (Both) improved model performance across all languages. layer’s output to a probability distribution (over two classes i.e. B and I). 3. 3.1. Experimental Results Data To test the performance of our model, we experimented with datasets in 5 different languages: 1. The RST-DT corpus (Carlson et al., 2003) in English (eng.rst.rstdt) 2. The Potsdam Commentary corpus (Stede and Neumann, 2014) in German (deu.rst.pcc) 3. The Dutch Discourse Treebank (Redeker et al., 2012) (nld.rst.rstdt) 4. The Cross-document Structure Theory News Corpus (Cardoso et al., 2011) in Portugeuse Brazilian (por.rst.cstn) 5. Basque Discourse Treebank (Iruskieta et al., 2013) (eus.rst.rstdt) Some statistics on the data are included in Table 1. 3.2. Setup We implemented our tool in PyTorch 3 and used the API provided by researchers at HuggingFace4 to fine-tune pretrained BERT. All experiments were performed using 8 NVIDIA-GTX 1080 Ti GPUs in parallel. For training, we constructed batches of size 16. We used"
2020.lrec-1.135,D18-1548,0,0.0288557,"hat segmentation is a local problem and demarcating a segment requires only on a small window of neighbouring tokens. Trying to tag the full sequence may introduce unnecessary noise and lead to errors. Additionally, we define a positional vector p (Zhang et al., 2017) for T = t1 , t2 ...tj relative to the masked token ti as:   i − j pj = 0   j−i if j <i if j = i if j >i (1) Following Shi and Lin (2019), we embed the positional vector and concatenate it to the encoder representations. 2.3. Joint Learning of Syntactic Features Syntactic features are very helpful in learning language tasks. Strubell et al. (2018) particularly observed that An example is provided in Figure 3. We extract partof-speech tags of all tokens; and the dependency parent (and corresponding relation), child(ren) and siblings of the masked token. For example, if the token ‘spend’ is masked, we train the classifier to learn CCOMP relation between ‘said’ and ‘spend’; CHILD relations with respect to the tokens ‘it’, ‘will’, ‘$’, and ‘advertising’; SIBLING relation with respect to the token ‘Pepsi’ and NOREL with respect to every other token. As shown in Figure 2, the first p layers of BERT learn the part-of-speech tags of the words"
2020.lrec-1.135,D18-1116,0,0.328259,"rm arcs or edges between nodes. As a motivating example, consider the discourse tree given in Figure 1. EDUs labeled 1, 2 and 3 form nodes of the tree; and arcs are labeled with ATTRIBUTION (used to indicate instances of reported speech) and PURPOSE relations. This example was taken from the RST-DT corpus (Carlson et al., 2003) and the tree was constructed using the tool provided by Gessler et al. (2019). imal and this makes training of data-hungry models like neural networks difficult. To assuage the problem of data insufficiency, syntax-free models that leverage pre-trained representations (Wang et al., 2018; Muller et al., 2019) from sentence encoders like ElMo and BERT were proposed: these models achieved very good results for the segmentation task. Likewise, the use of syntactic features such as part-of-speech tags and parse tree features helped achieve better results (Braud et al., 2017; Lin et al., 2019). In fact, the latter achieved state-of-the-art results using pointer networks (Vinyals et al., 2015) with parse tree features. In this paper, we propose a few changes that leverage BERT’s (Devlin et al., 2019) structure in performing segmentation. Our main contributions are: 1. We cast disco"
2020.lrec-1.135,D19-1077,0,0.0206231,"odels on downstream tasks, especially when training data is minimal. This allows us to get away with the data insufficiency problem since the size of training corpora is small. To work with languages other than English, Devlin et al. (2019) released multilingual BERT: a single language model pre-trained from multi-lingual corpora in 104 languages; using a shared multi-lingual vocabulary. Despite being a shared language model, which could raise concerns about its cross-lingual effectiveness, multilingual BERT has given surprisingly good results for different language tasks (Pires et al., 2019; Wu and Dredze, 2019). 2.2. Problem Formulation Deep learning frameworks (Braud et al., 2017; Wang et al., 2018) cast discourse segmentation as a BI tagging problem, where B indicates the beginning of a new span and I indi1074 root ccomp xcomp nummod nsubj nsubj aux obj det obj compound Pepsi said it will spend $ 10 million advertising the promotion NNS VBD PRP MD VB $ CD CD VBG DT NN Figure 3: An example showing the features captured for each token in the sentence: we consider the gold part-of-speech tags and dependency relations with respect to the masked token. cates the continuation of a previous span. As oppo"
2020.lrec-1.135,W19-2701,0,0.02534,"Missing"
2020.lrec-1.135,D17-1004,0,0.070838,"Missing"
2020.lrec-1.188,S18-1037,0,0.037286,"weets (Mohammad et al., 2018) presents an array of subtasks where participating systems need to automatically determine the (intensity of) emotions and (intensity of) sentiments from the corpora provided by the organizers. Meanwhile, the organizers also summarized the methods and resources used by the participating teams. From their summarization, we observed that most of the participants chose to solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerable manual effort in featu"
2020.lrec-1.188,N19-1423,0,0.17865,"and system design. Transfer learning is a machine learning strategy where a model trained on several related tasks is re-used as the starting point for a new task, so that the model can take advantage of the pre-learned knowledge from the previous tasks to make predictions for the new tasks. In recent years, transfer-learning has dominated a wide range of NLP tasks including Question Answer (Lan et al., 2019), Information Retrieval (Nogueira et al., 2019) and Text Understanding (Raffel et al., 2019). A lot of pre-learned structures have been proposed, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019). In this paper, we designed and implemented a transfer learning system based on BERT and applied it to solving four subtasks related to the affect analysis of tweets from SemEval-2018 Task 1. Meanwhile, we compared its performance with that of other top performers that used traditional feature-based machine learning or deep learning methods. We aim to show that by leveraging the prelearned knowledge and a very limited amount of fine-tuning effort, transfer learning models can achieve competitive to state-of-the-art results compared with the tradition models with"
2020.lrec-1.188,S18-1002,0,0.0161625,"o automatically determine the (intensity of) emotions and (intensity of) sentiments from the corpora provided by the organizers. Meanwhile, the organizers also summarized the methods and resources used by the participating teams. From their summarization, we observed that most of the participants chose to solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerable manual effort in feature engineering and system design. Transfer learning is a machine learning strategy where a mo"
2020.lrec-1.188,D17-1169,0,0.0275641,"sentiments from the corpora provided by the organizers. Meanwhile, the organizers also summarized the methods and resources used by the participating teams. From their summarization, we observed that most of the participants chose to solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerable manual effort in feature engineering and system design. Transfer learning is a machine learning strategy where a model trained on several related tasks is re-used as the starting point"
2020.lrec-1.188,S18-1056,0,0.0517764,"Missing"
2020.lrec-1.188,S18-1043,0,0.0213578,"2018) presents an array of subtasks where participating systems need to automatically determine the (intensity of) emotions and (intensity of) sentiments from the corpora provided by the organizers. Meanwhile, the organizers also summarized the methods and resources used by the participating teams. From their summarization, we observed that most of the participants chose to solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerable manual effort in feature engineering and system"
2020.lrec-1.188,W10-0204,0,0.0340758,"machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerable manual effort in feature engineering and system design. Transfer learning is a machine learning strategy where a model trained on several related tasks is re-used as the starting point for a new task, so that the model can take advantage of the pre-learned knowledge from the previous tasks to make predictions for the new tasks. In recent years, transfer-learning has dominated a wide range of NLP tasks including Question Answer (Lan et al., 2019), Information R"
2020.lrec-1.188,S18-1001,0,0.108839,"s and workshops (Rosenthal et al., 2015) (Nakov et al., 2016) (Rosenthal et al., 2017) (Barbieri et al., 2018) (Van Hee et al., 2018) have been organized. An essential part towards analyzing Twitter is to detect the emotions and the intensity of these emotions that are contained or can be inferred from tweets. For example, from the following two tweets, (1) I’m in tears. This is so heartbreaking, (2) You don’t know how to love me when you’re in sober. we should know that both tweets convey sadness, and the second tweet implies sadness to a lesser extent. SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018) presents an array of subtasks where participating systems need to automatically determine the (intensity of) emotions and (intensity of) sentiments from the corpora provided by the organizers. Meanwhile, the organizers also summarized the methods and resources used by the participating teams. From their summarization, we observed that most of the participants chose to solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meish"
2020.lrec-1.188,L18-1027,0,0.0274821,"solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerable manual effort in feature engineering and system design. Transfer learning is a machine learning strategy where a model trained on several related tasks is re-used as the starting point for a new task, so that the model can take advantage of the pre-learned knowledge from the previous tasks to make predictions for the new tasks. In recent years, transfer-learning has dominated a wide range of NLP tasks including Q"
2020.lrec-1.188,S16-1001,0,0.0270441,"earned knowledge, transfer learning models can achieve competitive results in the affectual content analysis of tweets, compared to the traditional models. As shown by the experiments on SemEval-2018 Task 1: Affect in Tweets, our model ranking 2nd , 4th and 6th place in four of its subtasks proves the effectiveness of our idea. Keywords: Natural Language Processing, Deep Learning, Transfer Learning 1. Introduction In recent years, the interest in analyzing Twitter has grown exponentially among the NLP community and a substantial amount of related events and workshops (Rosenthal et al., 2015) (Nakov et al., 2016) (Rosenthal et al., 2017) (Barbieri et al., 2018) (Van Hee et al., 2018) have been organized. An essential part towards analyzing Twitter is to detect the emotions and the intensity of these emotions that are contained or can be inferred from tweets. For example, from the following two tweets, (1) I’m in tears. This is so heartbreaking, (2) You don’t know how to love me when you’re in sober. we should know that both tweets convey sadness, and the second tweet implies sadness to a lesser extent. SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018) presents an array of subtasks where pa"
2020.lrec-1.188,S18-1039,0,0.0178964,"Task 1: Affect in Tweets (Mohammad et al., 2018) presents an array of subtasks where participating systems need to automatically determine the (intensity of) emotions and (intensity of) sentiments from the corpora provided by the organizers. Meanwhile, the organizers also summarized the methods and resources used by the participating teams. From their summarization, we observed that most of the participants chose to solve the problems with feature-based machine learning algorithms, which implement systems with a tremendous amount of linguistic features, pre-learned vectors and extra corpora (Park et al., 2018) (Baziotis et al., 2018) (Meisheri and Dey, 2018). For instance, the top one performer SeerNet (Duppada et al., 2018) implemented its system with pre-trained DeepMoji (Felbo et al., 2017) vectors, Skip-Thought vectors (Kiros et al., 2015) and Sentiment Neuron vectors (Radford et al., 2017), as well as a substantial amount of linguistic features, such as AFINN (Nielsen, 2011), NRC Affect Intensities (Mohammad, 2018) and Emotion Lexicon (Mohammad and Turney, 2010). Such a task specific architecture is not only difficult to be well adapted to different tasks or domains, but also needs considerabl"
2020.lrec-1.188,N18-1202,0,0.0541151,"fort in feature engineering and system design. Transfer learning is a machine learning strategy where a model trained on several related tasks is re-used as the starting point for a new task, so that the model can take advantage of the pre-learned knowledge from the previous tasks to make predictions for the new tasks. In recent years, transfer-learning has dominated a wide range of NLP tasks including Question Answer (Lan et al., 2019), Information Retrieval (Nogueira et al., 2019) and Text Understanding (Raffel et al., 2019). A lot of pre-learned structures have been proposed, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and XLNet (Yang et al., 2019). In this paper, we designed and implemented a transfer learning system based on BERT and applied it to solving four subtasks related to the affect analysis of tweets from SemEval-2018 Task 1. Meanwhile, we compared its performance with that of other top performers that used traditional feature-based machine learning or deep learning methods. We aim to show that by leveraging the prelearned knowledge and a very limited amount of fine-tuning effort, transfer learning models can achieve competitive to state-of-the-art results compared wi"
2020.lrec-1.188,S15-2078,0,0.0309238,"t by leveraging the pre-learned knowledge, transfer learning models can achieve competitive results in the affectual content analysis of tweets, compared to the traditional models. As shown by the experiments on SemEval-2018 Task 1: Affect in Tweets, our model ranking 2nd , 4th and 6th place in four of its subtasks proves the effectiveness of our idea. Keywords: Natural Language Processing, Deep Learning, Transfer Learning 1. Introduction In recent years, the interest in analyzing Twitter has grown exponentially among the NLP community and a substantial amount of related events and workshops (Rosenthal et al., 2015) (Nakov et al., 2016) (Rosenthal et al., 2017) (Barbieri et al., 2018) (Van Hee et al., 2018) have been organized. An essential part towards analyzing Twitter is to detect the emotions and the intensity of these emotions that are contained or can be inferred from tweets. For example, from the following two tweets, (1) I’m in tears. This is so heartbreaking, (2) You don’t know how to love me when you’re in sober. we should know that both tweets convey sadness, and the second tweet implies sadness to a lesser extent. SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018) presents an array"
2020.lrec-1.188,S17-2088,0,0.0234086,"nsfer learning models can achieve competitive results in the affectual content analysis of tweets, compared to the traditional models. As shown by the experiments on SemEval-2018 Task 1: Affect in Tweets, our model ranking 2nd , 4th and 6th place in four of its subtasks proves the effectiveness of our idea. Keywords: Natural Language Processing, Deep Learning, Transfer Learning 1. Introduction In recent years, the interest in analyzing Twitter has grown exponentially among the NLP community and a substantial amount of related events and workshops (Rosenthal et al., 2015) (Nakov et al., 2016) (Rosenthal et al., 2017) (Barbieri et al., 2018) (Van Hee et al., 2018) have been organized. An essential part towards analyzing Twitter is to detect the emotions and the intensity of these emotions that are contained or can be inferred from tweets. For example, from the following two tweets, (1) I’m in tears. This is so heartbreaking, (2) You don’t know how to love me when you’re in sober. we should know that both tweets convey sadness, and the second tweet implies sadness to a lesser extent. SemEval-2018 Task 1: Affect in Tweets (Mohammad et al., 2018) presents an array of subtasks where participating systems need"
2020.lrec-1.188,S18-1033,0,0.0446765,"Missing"
2020.lrec-1.188,S18-1005,0,0.0442438,"Missing"
2020.lrec-1.8,P19-1409,0,0.183506,"/emailcoref 65 No. of Email Messages 1 2-3 4-7 8-10 11-15 16-20 21-30 41 each email in an email thread represents a single document. EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014) are the most widely used corpora for this task setting. However, as seen with most corpora for within document entity resolution task, the documents in both corpus are taken from Google News search and hence are restricted to the news domain. Additionally, this corpus is primarily event-centric as it is based on the EventCorefBank (Bejan and Harabagiu, 2010). As per our knowledge, the model proposed by Barhom et al. (2019) is the current state-of-the-art on the ECB+ corpus. Barhom et al. (2019) model the event and entity coreference problem jointly and to show the differences between the tasks. This work also evaluates modeling coreference resolution for email conversations in a cross-document setting. 3. Table 1: Email thread count distribution in terms of email messages • The thread must consist of more than three email messages. The objective of this research is to address both intra-email and inter-email entity resolution problems. Entity resolution in single email messages is also an unexplored problem, ho"
2020.lrec-1.8,P10-1143,0,0.0247111,"6 describes the experiments 1 Related Work https://github.com/paragdakle/emailcoref 65 No. of Email Messages 1 2-3 4-7 8-10 11-15 16-20 21-30 41 each email in an email thread represents a single document. EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014) are the most widely used corpora for this task setting. However, as seen with most corpora for within document entity resolution task, the documents in both corpus are taken from Google News search and hence are restricted to the news domain. Additionally, this corpus is primarily event-centric as it is based on the EventCorefBank (Bejan and Harabagiu, 2010). As per our knowledge, the model proposed by Barhom et al. (2019) is the current state-of-the-art on the ECB+ corpus. Barhom et al. (2019) model the event and entity coreference problem jointly and to show the differences between the tasks. This work also evaluates modeling coreference resolution for email conversations in a cross-document setting. 3. Table 1: Email thread count distribution in terms of email messages • The thread must consist of more than three email messages. The objective of this research is to address both intra-email and inter-email entity resolution problems. Entity res"
2020.lrec-1.8,W16-3612,0,0.0712994,"ve been released for coreference resolution, with MUC-6 (Grishman and Sundheim, 1996), MUC-7 (Chinchor, 1998), ACE (Doddington et al., 2004) and OntoNotes being the popular ones. OntoNotes 2.0 and OntoNotes 5.0 were used in Task-1 of SemEval 2010 (Recasens et al., 2010) and CoNLL 2012 shared task (Pradhan et al., 2012) respectively. However, each of these corpora either fully or mainly comprise of news articles. Conversational texts in the form of telephonic speech have been a part of many corpora. The first corpus to entirely focus on conversational texts was Character Identification Corpus (Chen and Choi, 2016). This was constructed using TV show transcripts and labeled speakers in a multi-party conversation. It introduced the task of character-linking in a multi-party conversation. Zhou and Choi (2018) further expanded this corpus by annotating additional text and adding plural mentions. The Manually Annotated Sub-Corpus (MASC) (Ide et al., 2008) project is one of the first corpora to consider annotating coreference chains for emails. The corpus includes 45 emails from the Enron Email Corpus (Klimt and Yang, 2004), 96 spam emails and 35 w3c email digests. However, no coreference annotations were re"
2020.lrec-1.8,M98-1001,0,0.535259,"on (Aktas¸ et al., 2018) respectively. Email corpora have been widely used for numerous tasks like text classification (Klimt and Yang, 2004), intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006) and summarization (Ulrich et al., 2008). This paper addresses the task of entity resolution in email conversations, which to the best of our knowledge has not been examined in any study so far. This research makes the following key contributions: 2. Over the years, numerous corpora have been released for coreference resolution, with MUC-6 (Grishman and Sundheim, 1996), MUC-7 (Chinchor, 1998), ACE (Doddington et al., 2004) and OntoNotes being the popular ones. OntoNotes 2.0 and OntoNotes 5.0 were used in Task-1 of SemEval 2010 (Recasens et al., 2010) and CoNLL 2012 shared task (Pradhan et al., 2012) respectively. However, each of these corpora either fully or mainly comprise of news articles. Conversational texts in the form of telephonic speech have been a part of many corpora. The first corpus to entirely focus on conversational texts was Character Identification Corpus (Chen and Choi, 2016). This was constructed using TV show transcripts and labeled speakers in a multi-party co"
2020.lrec-1.8,P15-1136,0,0.0599675,"f the corpus. Furthermore, the emails considered from the Enron Corpus were single messages as compared to our work which focuses on email threads. Hendrickx and Hoste (2009) studied coreference resolution in conversations present in a threaded format. A corpus consisting of blogs and commented news was used to highlight a significant performance drop when dealing with coreference resolution in unedited text. Aktas¸ et al. (2018), in their work on Twitter conversations, describe steps to create a corpus for anaphora resolution, obtain predictions using Stanford statistical coreference system (Clark and Manning, 2015) and present analysis on the mediocre performance of the system on the Twitter corpus. Entity resolution for email threads can also be cast as a multi-document or cross-document (CD) problem, where 1. For the first time, the entity resolution task for emails is analyzed. Characteristics of emails that make this problem difficult are identified. 2. A human-annotated seed corpus containing email threads is presented for the entity resolution task. This annotated seed corpus will be released as a part of the paper1 . 3. An evaluation of the current state-of-the-art model for within document (WD)"
2020.lrec-1.8,W04-3240,0,0.558931,"Missing"
2020.lrec-1.8,cybulska-vossen-2014-using,0,0.0513459,"is organized as follows: Section 2 presents an overview of the work done on the entity resolution task. Section 3 describes the entity resolution task for emails, which is followed by a description of the corpus creation process in Section 4. Challenges observed in the seed corpus as well as email conversations in general are elaborated in Section 5. Section 6 describes the experiments 1 Related Work https://github.com/paragdakle/emailcoref 65 No. of Email Messages 1 2-3 4-7 8-10 11-15 16-20 21-30 41 each email in an email thread represents a single document. EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014) are the most widely used corpora for this task setting. However, as seen with most corpora for within document entity resolution task, the documents in both corpus are taken from Google News search and hence are restricted to the news domain. Additionally, this corpus is primarily event-centric as it is based on the EventCorefBank (Bejan and Harabagiu, 2010). As per our knowledge, the model proposed by Barhom et al. (2019) is the current state-of-the-art on the ECB+ corpus. Barhom et al. (2019) model the event and entity coreference problem jointly and to show the differences between the task"
2020.lrec-1.8,doddington-etal-2004-automatic,0,0.0157839,"8) respectively. Email corpora have been widely used for numerous tasks like text classification (Klimt and Yang, 2004), intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006) and summarization (Ulrich et al., 2008). This paper addresses the task of entity resolution in email conversations, which to the best of our knowledge has not been examined in any study so far. This research makes the following key contributions: 2. Over the years, numerous corpora have been released for coreference resolution, with MUC-6 (Grishman and Sundheim, 1996), MUC-7 (Chinchor, 1998), ACE (Doddington et al., 2004) and OntoNotes being the popular ones. OntoNotes 2.0 and OntoNotes 5.0 were used in Task-1 of SemEval 2010 (Recasens et al., 2010) and CoNLL 2012 shared task (Pradhan et al., 2012) respectively. However, each of these corpora either fully or mainly comprise of news articles. Conversational texts in the form of telephonic speech have been a part of many corpora. The first corpus to entirely focus on conversational texts was Character Identification Corpus (Chen and Choi, 2016). This was constructed using TV show transcripts and labeled speakers in a multi-party conversation. It introduced the t"
2020.lrec-1.8,C96-1079,0,0.326017,"e explained. Finally, performance of the current state-of-the-art deep learning models on the seed corpus is evaluated and qualitative error analysis on the predictions obtained is presented. Keywords: entity resolution, email threads, coreference resolution, enron, email conversations 1. Introduction performed on the seed corpus. Error analysis on the predictions is covered in Section 7 and a conclusion is provided in Section 8. Entity resolution has been an active topic in the Natural Language Processing domain since the 1960s. Shared tasks such as CoNLL 2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996) define it as linking referring spans of text that point to the same discourse entity. Previous works highlight that good performance on these tasks does not necessarily result in a similar performance on downstream or similar tasks like machine translation (Guillou, 2012) or anaphora resolution (Aktas¸ et al., 2018) respectively. Email corpora have been widely used for numerous tasks like text classification (Klimt and Yang, 2004), intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006) and summarization (Ulrich et al., 2008). This paper addresses the task of entity reso"
2020.lrec-1.8,E12-3001,0,0.0232957,"1. Introduction performed on the seed corpus. Error analysis on the predictions is covered in Section 7 and a conclusion is provided in Section 8. Entity resolution has been an active topic in the Natural Language Processing domain since the 1960s. Shared tasks such as CoNLL 2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996) define it as linking referring spans of text that point to the same discourse entity. Previous works highlight that good performance on these tasks does not necessarily result in a similar performance on downstream or similar tasks like machine translation (Guillou, 2012) or anaphora resolution (Aktas¸ et al., 2018) respectively. Email corpora have been widely used for numerous tasks like text classification (Klimt and Yang, 2004), intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006) and summarization (Ulrich et al., 2008). This paper addresses the task of entity resolution in email conversations, which to the best of our knowledge has not been examined in any study so far. This research makes the following key contributions: 2. Over the years, numerous corpora have been released for coreference resolution, with MUC-6 (Grishman and Sun"
2020.lrec-1.8,W09-2411,0,0.101622,"Missing"
2020.lrec-1.8,H93-1012,0,0.0509659,"Missing"
2020.lrec-1.8,Q18-1042,0,0.0325832,". (2019b) is used. This is the current state-of-the-art for the CoNLL 2012 shared task. Joshi et al. (2019b) take the c2f-coref model (Lee et al., 2018) as the base model. The proposed model uses BERT to obtain the span embedding replacing the original LSTM-encoder in the c2f-coref model. For each mention embedding, a mention score is computed and for each valid antecedent:mention pair, an antecedent score is computed. Eventually, using these scores, the probability of an antecedent belonging to a chain is computed. The model was fine-tuned and evaluated on the CoNLL 2012 shared task and GAP (Webster et al., 2018) corpus. As compared to using simple BERT as a model component, Joshi et al. (2019a) show that replacing BERT with SpanBERT leads to better performance on the CoNLL task. For readability, this model has been termed as OntoSpanBERT and the SpanBERT model not fine-tuned on the OntoNotes 5.0 corpus as VanillaSpanBERT respectively. For the CD formulation of the task, the model proposed by Barhom et al. (2019) is used. The model was trained jointly on ECB+ corpus for both event and entity 5 https://github.com/mandarjoshi90/coref Zhou and Choi (2018) propose variations of B3 and CEAFE which may be m"
2020.lrec-1.8,C18-1003,0,0.27608,"OntoNotes 5.0 were used in Task-1 of SemEval 2010 (Recasens et al., 2010) and CoNLL 2012 shared task (Pradhan et al., 2012) respectively. However, each of these corpora either fully or mainly comprise of news articles. Conversational texts in the form of telephonic speech have been a part of many corpora. The first corpus to entirely focus on conversational texts was Character Identification Corpus (Chen and Choi, 2016). This was constructed using TV show transcripts and labeled speakers in a multi-party conversation. It introduced the task of character-linking in a multi-party conversation. Zhou and Choi (2018) further expanded this corpus by annotating additional text and adding plural mentions. The Manually Annotated Sub-Corpus (MASC) (Ide et al., 2008) project is one of the first corpora to consider annotating coreference chains for emails. The corpus includes 45 emails from the Enron Email Corpus (Klimt and Yang, 2004), 96 spam emails and 35 w3c email digests. However, no coreference annotations were released as a part of the corpus. Furthermore, the emails considered from the Enron Corpus were single messages as compared to our work which focuses on email threads. Hendrickx and Hoste (2009) stu"
2020.lrec-1.8,ide-etal-2008-masc,0,0.0215246,"ch of these corpora either fully or mainly comprise of news articles. Conversational texts in the form of telephonic speech have been a part of many corpora. The first corpus to entirely focus on conversational texts was Character Identification Corpus (Chen and Choi, 2016). This was constructed using TV show transcripts and labeled speakers in a multi-party conversation. It introduced the task of character-linking in a multi-party conversation. Zhou and Choi (2018) further expanded this corpus by annotating additional text and adding plural mentions. The Manually Annotated Sub-Corpus (MASC) (Ide et al., 2008) project is one of the first corpora to consider annotating coreference chains for emails. The corpus includes 45 emails from the Enron Email Corpus (Klimt and Yang, 2004), 96 spam emails and 35 w3c email digests. However, no coreference annotations were released as a part of the corpus. Furthermore, the emails considered from the Enron Corpus were single messages as compared to our work which focuses on email threads. Hendrickx and Hoste (2009) studied coreference resolution in conversations present in a threaded format. A corpus consisting of blogs and commented news was used to highlight a"
2020.lrec-1.8,D19-1588,0,0.221191,"ators were used. Inter-annotator agreement on the Fleiss et al. (2003) Kappa statistic was κ = 0.87. A high κ value is due to a large number of email and name occurrences in the seed corpus which are unambiguous. All cases where no agreement was reached were resolved by discussion. Table 2 gives details on the size of annotations in the seed corpus. The distribution of mentions per entity type is given in Table 3. Note that the seed corpus also contains speaker annotations. Before manually annotating the seed corpus, an evaluation of weakly annotating email threads using the model proposed by Joshi et al. (2019b) with few manually annotated samples was carried out. Poor performance on this evaluation led to manually annotating the seed corpus. 5.1. Email addresses An email address is a unique identifier for every user having an email account and hence can be considered as a mention representing a Person or an Organization entity. An email message in its entirety, that is header and body, most certainly contains the email addresses of the sender and recipient(s). However, it may or may not contain the names of the sender and/or recipient(s). Thus, it is crucial to identify and link an email address t"
2020.lrec-1.8,D12-1045,0,0.0336849,"del is presented. The paper is organized as follows: Section 2 presents an overview of the work done on the entity resolution task. Section 3 describes the entity resolution task for emails, which is followed by a description of the corpus creation process in Section 4. Challenges observed in the seed corpus as well as email conversations in general are elaborated in Section 5. Section 6 describes the experiments 1 Related Work https://github.com/paragdakle/emailcoref 65 No. of Email Messages 1 2-3 4-7 8-10 11-15 16-20 21-30 41 each email in an email thread represents a single document. EECB (Lee et al., 2012) and ECB+ (Cybulska and Vossen, 2014) are the most widely used corpora for this task setting. However, as seen with most corpora for within document entity resolution task, the documents in both corpus are taken from Google News search and hence are restricted to the news domain. Additionally, this corpus is primarily event-centric as it is based on the EventCorefBank (Bejan and Harabagiu, 2010). As per our knowledge, the model proposed by Barhom et al. (2019) is the current state-of-the-art on the ECB+ corpus. Barhom et al. (2019) model the event and entity coreference problem jointly and to"
2020.lrec-1.8,N18-2108,0,0.0446122,"arger group or an organization. These cases add ambiguity to the resolution of first-person plural pronouns. Consider the pronoun ‘we’, it can resolve to both the sender and recipient together or the entity the sender is representing. 6.1. Setting Experiments Models Entity resolution for the seed corpus is evaluated by considering both within document (WD) and crossdocument (CD) formulations of the task. For the WD formulation, the model proposed by Joshi et al. (2019b) is used. This is the current state-of-the-art for the CoNLL 2012 shared task. Joshi et al. (2019b) take the c2f-coref model (Lee et al., 2018) as the base model. The proposed model uses BERT to obtain the span embedding replacing the original LSTM-encoder in the c2f-coref model. For each mention embedding, a mention score is computed and for each valid antecedent:mention pair, an antecedent score is computed. Eventually, using these scores, the probability of an antecedent belonging to a chain is computed. The model was fine-tuned and evaluated on the CoNLL 2012 shared task and GAP (Webster et al., 2018) corpus. As compared to using simple BERT as a model component, Joshi et al. (2019a) show that replacing BERT with SpanBERT leads t"
2020.lrec-1.8,P09-1113,0,0.21669,"Missing"
2020.lrec-1.8,P16-1060,0,0.154557,"re. 2. Speaker references: Email conversations are multiuser conversations by nature. Due to this, third-person pronouns are used very frequently. Aktas¸ et al. (2018) is the only work in our knowledge considering this phenomenon in a conversational setting. Although an email thread can be viewed as a turn-based sequential conversation over time, the time sequencing may not align with the flow of the conversation, thereby adding to the complexity of the task. 6.3. 6. Evaluation The majority of the recent work done on entity coreference use the MUC, B3 and CEAFE metrics (Pradhan et al., 2012). Moosavi and Strube (2016) show the shortcomings of each of these metrics and propose the Link-based Entity Aware (LEA) metric6 . The results using all four metrics, for comparability as well as correctness, have been reported. The official scorer 7 provided by CoNLL 2012 shared task is used. The official scorer raises a non-crashing duplicate reference error when a single-token mention belongs to more than one chain. This error is also observed on the OntoNotes corpus and hence this paper reports the scores ignoring the errors. Both models deal with more entity types than those defined here like Facility, Event, Produ"
2020.lrec-1.8,W12-4501,0,0.680731,"corpus and annotation steps are explained. Finally, performance of the current state-of-the-art deep learning models on the seed corpus is evaluated and qualitative error analysis on the predictions obtained is presented. Keywords: entity resolution, email threads, coreference resolution, enron, email conversations 1. Introduction performed on the seed corpus. Error analysis on the predictions is covered in Section 7 and a conclusion is provided in Section 8. Entity resolution has been an active topic in the Natural Language Processing domain since the 1960s. Shared tasks such as CoNLL 2012 (Pradhan et al., 2012) and MUC (Grishman and Sundheim, 1996) define it as linking referring spans of text that point to the same discourse entity. Previous works highlight that good performance on these tasks does not necessarily result in a similar performance on downstream or similar tasks like machine translation (Guillou, 2012) or anaphora resolution (Aktas¸ et al., 2018) respectively. Email corpora have been widely used for numerous tasks like text classification (Klimt and Yang, 2004), intent classification (Cohen et al., 2004), searching (Soboroff et al., 2006) and summarization (Ulrich et al., 2008). This p"
A00-1037,P98-2180,0,0.066364,"Missing"
A00-1037,C98-2175,0,\N,Missing
balakrishna-etal-2010-semi,P98-1013,0,\N,Missing
balakrishna-etal-2010-semi,C98-1013,0,\N,Missing
blanco-etal-2008-causal,A00-2031,0,\N,Missing
blanco-etal-2008-causal,W04-2609,1,\N,Missing
blanco-etal-2008-causal,W04-2610,1,\N,Missing
blanco-etal-2008-causal,P98-1013,0,\N,Missing
blanco-etal-2008-causal,C98-1013,0,\N,Missing
blanco-etal-2008-causal,P05-2006,0,\N,Missing
blanco-etal-2008-causal,J06-1005,1,\N,Missing
blanco-etal-2008-causal,P06-1015,0,\N,Missing
blanco-etal-2008-causal,W03-1210,0,\N,Missing
blanco-etal-2008-causal,P00-1043,0,\N,Missing
blanco-etal-2008-causal,J02-3001,0,\N,Missing
blanco-etal-2008-causal,P02-1047,0,\N,Missing
blanco-etal-2008-causal,N03-1011,1,\N,Missing
C02-1167,J91-1002,0,\N,Missing
C02-1167,W99-0501,1,\N,Missing
C02-1169,C00-1043,1,\N,Missing
C02-1169,P01-1037,1,\N,Missing
C02-1169,P98-1035,0,\N,Missing
C02-1169,C98-1035,0,\N,Missing
C02-1169,P96-1025,0,\N,Missing
C10-2009,S07-1045,0,0.0336058,"Missing"
C10-2009,P07-2040,0,0.184953,"Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underlying connections between concepts. For example, the noun phrase the car engine encodes a PART- WHOLE relation: the engine is a part of the car. Semantic relations are the building blocks for creating a semantic structure of a sentence. There is a growing interest"
C10-2009,P98-1013,0,0.421017,"t and the motivation for this work. This paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed. A set of 26 relations is introduced, with their arguments defined on an ontology of sorts. A semantic parser is used to extract these relations from noun phrases and verb argument structures. The method was successfully used in two applications: rapid customization of semantic relations to arbitrary domains and recognizing entailments. 2 Related Work In Computational Linguistics, WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) are probably the most used semantic resources. Like our approach and unlike PropBank, FrameNet annotates semantics between concepts regardless of their position in a parse tree. Unlike us, they use a predefined set of frames to be filled. PropBank adds semantic annotation on top of the Penn TreeBank and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Appr"
C10-2009,bethard-etal-2008-building,0,0.0182228,"d n new relations, resulting in a richer semantic representation (Figure 2). (2) REA−1 (x, y ) ◦ GOA(y, z ) → PRP(x, z ): events have as their purpose the effects of their goals. This is a strong relation. For example: Since they have a better view, they can see the mountain range. They cut the tree to have a better view. Therefore, they cut the tree to see the mountain range. P P The axioms have been evaluated using manually annotated data. PropBank CAU and PNC are used as reason and goal. Reason annotation is further collected from a corpus which adds causal annotation to the Penn TreeBank (Bethard et al., 2008). A total of 5 and 29 instances for axioms 3 and 4 were found. For all of them, the axioms yield a valid inference. For example, Buick [approached]y American express about [a joint promotion]x because [its card holders generally have a good credit history]z . PropBank annotation states GOA(x, y ) and REA−1 (y, z ), axiom 4 makes the implicit relation IFL−1 (x, z ) explicit. 6 Case Study: Reason and Goal P GOA (get there faster, crossed carelessly ) REA(crossed carelessly, got run over ) IFL (get there faster, got run over ) −1 REA (see the mountain range, better view ) GOA (better view, cut th"
C10-2009,J08-2001,0,0.108476,"Missing"
C10-2009,W04-2609,1,0.832612,"ations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underlying connections between concepts. For example, the noun phrase the car engine encodes a PART- WHOLE relation: the engine is a part of the car. Semantic relations are the building blocks for creating a semantic structure of a"
C10-2009,P01-1019,0,0.0435144,"The work reported in this paper aims at extracting as many semantic relations from text as possi72 Coling 2010: Poster Volume, pages 72–80, Beijing, August 2010 posals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005) and PART- WHOLE (Girju et al., 2006). Researchers have also worked on combining semantic relations. Harabagiu and Moldovan (1998) combine WordNet relations and Helbig (2005) transforms chains of relations into theoretical axioms. Some use logic as the underlying formalism (Lakoff, 1970; S´anchez Valencia, 1991), more ideas can be found in (Copestake et al., 2001). MAIN(R ); and (iii) R ANGE(R ). Stating restrictions for D OMAIN and R ANGE has several advantages: it (i) helps distinguishing between relations, e.g., [tall]ql and [John]aco can be linked through VALUE, but not POSSESSION; (ii) helps discarding potential relations that do not hold, e.g., abstract objects do not have INTENT; and (iii) helps combining semantic relations (Section 5). Ontology of Sorts In order to define D OMAIN(R) and R ANGE(R), we use a customized ontology of sorts (Figure 1) modified from (Helbig, 2005). The root corresponds to entities, which refers to all things about whi"
C10-2009,J05-1004,0,0.285104,". This paper presents a framework for combining semantic relations extracted from text to reveal even more semantics that otherwise would be missed. A set of 26 relations is introduced, with their arguments defined on an ontology of sorts. A semantic parser is used to extract these relations from noun phrases and verb argument structures. The method was successfully used in two applications: rapid customization of semantic relations to arbitrary domains and recognizing entailments. 2 Related Work In Computational Linguistics, WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) are probably the most used semantic resources. Like our approach and unlike PropBank, FrameNet annotates semantics between concepts regardless of their position in a parse tree. Unlike us, they use a predefined set of frames to be filled. PropBank adds semantic annotation on top of the Penn TreeBank and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations"
C10-2009,P08-1027,0,0.0123445,"nk and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underlying connections between concepts. For example, the noun phrase the car engine encodes a PART- WHOLE relation: the engine is a part of the car. Semantic relations are the building blocks for creating a"
C10-2009,P04-1055,0,0.03674,"ine, car ) MAK(cars, BMW ) POS(Ford F-150, John ) MNR(quick, delivery ) RCP(Mary, gave ) SYN (a dozen, twelve ) AT- L (party, John’s house ) AT- T (party, last Saturday ) PRO(height, John ) QNT(a dozen, eggs ) Table 1: The set of 26 relations clustered and classified with their properties (reflexive, symmetric, transitive) and examples. An asterisk indicates that the property holds under certain conditions. 4.1 includes relations present in WordNet (Miller, 1995), such as IS - A , PART- WHOLE and CAUSE. Szpakowicz et al. (1995) proposed a set of nine relations and Turney (2006) a set of five. Rosario and Hearst (2004) proposed a set of 38 relations including standard case roles and a set of specific relations for medical domain. Helbig (2005) proposed a set of 89 relations, including ANTONYMY and several TEMPORAL relations, e.g. SUCCES SION , EXTENSION , END . Our set clusters some of the previous proposals (e.g. we only consider AT- TIME ) and discards relations proposed elsewhere when they did not occur frequently enough in our experiments. For example, even though ANTONYMY and ENTAIL MENT are semantically grounded, they are very infrequent and we do not deal with them. Our pragmatic goal is to capture a"
C10-2009,W07-1401,0,0.0113818,"infer IS - EMPLOYER, IS - COWORKER, IS PARAMOUR, IS - INTERPRETER, WAS - ASSASSIN , ATTENDS - SCHOOL - AT , JAILED - AT, COHABITS WITH , AFFILIATED - TO , MARRIED - TO , RENTED BY, KIDNAPPED- BY and the relations in Table 4 were defined. Note that a relation can be inferred by several axioms. This customization effort to add 37 new specialized relations took a person only a few days and without modifying the SP. 7.2 Textual Entailment Problem An application of CSR is recognizing entailments. Given text T and hypothesis H, the task consists on determining whether or not H can be inferred by T (Giampiccolo et al., 2007). CSR axioms Several examples of the RTE3 challenge can be solved by applying CSR (Table 5). The rest of this section depicts the axioms involved in detecting entailment for each pair. Pair 113 is a simple one. A perfect match for H in T can be obtained by an axiom reading all concepts inherit the semantic relations of their hypernyms. Formally, ISA(x, y ) ◦ THM(y, z ) → THM(x, z ), T 2 and T 4 are the premises and the conclusion matches H2. T 1 matches H1. Pair 429 can be solved by an axiom reading agents are values for their themes. Formally, AGT(x, y ) ◦ THM−1 (y, z ) → VAL(x, z ); T 1 and"
C10-2009,J06-1005,1,0.880656,"ctured text into structured knowledge. More and more enterprises and academic organizations have adopted the World Wide Web Consortium (W3C) Resource Description Framework (RDF) specification as a standard representation of text knowledge. This is based on semantic triples, which can be used to represent semantic relations. The work reported in this paper aims at extracting as many semantic relations from text as possi72 Coling 2010: Poster Volume, pages 72–80, Beijing, August 2010 posals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005) and PART- WHOLE (Girju et al., 2006). Researchers have also worked on combining semantic relations. Harabagiu and Moldovan (1998) combine WordNet relations and Helbig (2005) transforms chains of relations into theoretical axioms. Some use logic as the underlying formalism (Lakoff, 1970; S´anchez Valencia, 1991), more ideas can be found in (Copestake et al., 2001). MAIN(R ); and (iii) R ANGE(R ). Stating restrictions for D OMAIN and R ANGE has several advantages: it (i) helps distinguishing between relations, e.g., [tall]ql and [John]aco can be linked through VALUE, but not POSSESSION; (ii) helps discarding potential relations th"
C10-2009,W07-1404,1,0.898013,"Missing"
C10-2009,S07-1003,0,0.236658,"Missing"
C10-2009,P05-2006,0,0.146997,"at aim at transforming unstructured text into structured knowledge. More and more enterprises and academic organizations have adopted the World Wide Web Consortium (W3C) Resource Description Framework (RDF) specification as a standard representation of text knowledge. This is based on semantic triples, which can be used to represent semantic relations. The work reported in this paper aims at extracting as many semantic relations from text as possi72 Coling 2010: Poster Volume, pages 72–80, Beijing, August 2010 posals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005) and PART- WHOLE (Girju et al., 2006). Researchers have also worked on combining semantic relations. Harabagiu and Moldovan (1998) combine WordNet relations and Helbig (2005) transforms chains of relations into theoretical axioms. Some use logic as the underlying formalism (Lakoff, 1970; S´anchez Valencia, 1991), more ideas can be found in (Copestake et al., 2001). MAIN(R ); and (iii) R ANGE(R ). Stating restrictions for D OMAIN and R ANGE has several advantages: it (i) helps distinguishing between relations, e.g., [tall]ql and [John]aco can be linked through VALUE, but not POSSESSION; (ii) he"
C10-2009,P06-1040,0,0.526264,"tics between concepts regardless of their position in a parse tree. Unlike us, they use a predefined set of frames to be filled. PropBank adds semantic annotation on top of the Penn TreeBank and it contains only annotations between a verb and its arguments. Moreover, the semantics of a given label depends on the verb. For example, ARG 2 is used for INSTRUMENT and VALUE. Copious work has been done lately on semantic roles (M`arquez et al., 2008). Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns or kind of arguments. There are both unsupervised (Turney, 2006) and supervised approaches. The SemEval2007 Task 4 (Girju et al., 2007) focused on relations between nominals. Work has been done on detecting relations between noun phrases (Davidov and Rappoport, 2008; Moldovan et al., 2004), named entities (Hirano et al., 2007), and clauses (Szpakowicz et al., 1995). There have been pro1 Introduction Semantic representation of text facilitates inferences, reasoning, and greatly improves the performance of Question Answering, Information Extraction, Machine Translation and other NLP applications. Broadly speaking, semantic relations are unidirectional underl"
C10-2009,C98-1013,0,\N,Missing
C92-2121,P91-1002,0,0.0261313,"Missing"
C92-2121,C90-3052,0,0.0377275,"Missing"
C92-2121,E89-1010,1,0.89437,"Missing"
C92-2121,P91-1024,0,\N,Missing
C92-2121,P89-1001,0,\N,Missing
C92-2121,W89-0201,0,\N,Missing
D10-1031,P98-1013,0,0.102676,"Missing"
D10-1031,C10-2009,1,0.818952,"ctic parsers achieve a good performance, they make mistakes and the less our models rely on them, the better. 9 Composing MANNER with PURPOSE MANNER can combine with other semantic relations in order to reveal implicit relations that otherwise would be missed. The basic idea is to compose MANNER with other relations in order to infer another MANNER. A necessary condition for combining MANNER with another relation R is the compatibility of R ANGE(MNR) with D OMAIN(R) or R ANGE(R) with D OMAIN(MNR). The extended definition (Section 3) allows to quickly determine if two relations are compatible (Blanco et al., 2010). The new MANNER is automatically inferred by humans when reading, but computers need an explicit representation. Consider the following example: [. . . ] the traders [place]y orders [via computers]MNR [to buy the basket of stocks . . . ]PRP (wsj 0118, 48). PropBank states the basic annotation between brackets: via computers is the MANNER and to buy the basket [. . . ] the PURPOSE of the place orders event. We propose to combine these two relations in order to come up with the new relation MNR(via computers, buy the basket [. . . ] ). This relation is obvious when reading the sentence, so it i"
D10-1031,W05-0620,0,0.114188,"Missing"
D10-1031,P08-1079,0,0.0394071,"Missing"
D10-1031,P07-1030,0,0.0225366,"growing very slowly if at all and He started the company on his own. Consider the following example: The company said Mr. Stronach will personally direct the restructuring assisted by Manfred Gingl, [. . . ]1 . There are two MANNER relations in this sentence: the underlined chunks of text encode the way in which Mr. Stronach will direct the restructuring. 2 Previous Work The extraction of semantic relations in general has caught the attention of several researchers. Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns. There are both unsupervised (Davidov et al., 2007; Turney, 2006) and supervised approaches. The SemEval2007 Task 04 (Girju et al., 2007) aimed at relations between nominals. Work has been done on detecting relations within noun phrases (Nulty, 2007), 1 Penn TreeBank, file wsj 0027, sentence 10. 315 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315–324, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics named entities (Hirano et al., 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). There have been propos"
D10-1031,W03-1207,1,0.313621,"Missing"
D10-1031,J06-1005,1,0.898066,"Missing"
D10-1031,S07-1003,0,0.0678697,"Missing"
D10-1031,P06-1117,0,0.0431454,"Missing"
D10-1031,C92-2082,0,0.298859,"Missing"
D10-1031,P07-2040,0,0.0627118,"Missing"
D10-1031,J08-2001,0,0.0272255,"Missing"
D10-1031,P07-3014,0,0.0131917,". There are two MANNER relations in this sentence: the underlined chunks of text encode the way in which Mr. Stronach will direct the restructuring. 2 Previous Work The extraction of semantic relations in general has caught the attention of several researchers. Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns. There are both unsupervised (Davidov et al., 2007; Turney, 2006) and supervised approaches. The SemEval2007 Task 04 (Girju et al., 2007) aimed at relations between nominals. Work has been done on detecting relations within noun phrases (Nulty, 2007), 1 Penn TreeBank, file wsj 0027, sentence 10. 315 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315–324, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics named entities (Hirano et al., 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). There have been proposals to detect a particular relation, e.g., CAUSE (Chang and Choi, 2006), INTENT (Tatu, 2005), PART- WHOLE (Girju et al., 2006) and IS - A (Hearst, 1992). MANNER is a frequent relation, but besides the"
D10-1031,J05-1004,0,0.0148818,"Missing"
D10-1031,P08-1117,0,0.035175,"Missing"
D10-1031,P05-2006,0,0.0742562,"Missing"
D10-1031,P06-1040,0,0.0215597,"at all and He started the company on his own. Consider the following example: The company said Mr. Stronach will personally direct the restructuring assisted by Manfred Gingl, [. . . ]1 . There are two MANNER relations in this sentence: the underlined chunks of text encode the way in which Mr. Stronach will direct the restructuring. 2 Previous Work The extraction of semantic relations in general has caught the attention of several researchers. Approaches to detect semantic relations usually focus on particular lexical and syntactic patterns. There are both unsupervised (Davidov et al., 2007; Turney, 2006) and supervised approaches. The SemEval2007 Task 04 (Girju et al., 2007) aimed at relations between nominals. Work has been done on detecting relations within noun phrases (Nulty, 2007), 1 Penn TreeBank, file wsj 0027, sentence 10. 315 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 315–324, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics named entities (Hirano et al., 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). There have been proposals to detect a"
D10-1031,C98-1013,0,\N,Missing
D10-1031,J02-3001,0,\N,Missing
D13-1123,S12-1051,0,0.164379,"ity Eduardo Blanco and Dan Moldovan Lymba Corporation Richardson, TX 75080 USA {eduardo,moldovan}@lymba.com Abstract man a This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping"
D13-1123,P98-1013,0,0.0248229,"he only required modification would be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; MSRpar (36/35) [750/750] MSRvid (13/13), [750/750] SMTeuroparl (56/21), [734/459] surprise.OnWN (–/16), [–/750] surprise.SMTnews (–/24), [–/399] Score 2.600 0.000 4.250 1.500 3.000 Sentence Pair The unions also staged a five-day strike in March that forced all but one of Yale’s dining halls to close. The unions also staged a five-day strike in March; strikes have preceded eight of the last 10 contracts. A woman is swimming underwater. A man is slicing some carrots. Then perhaps we coul"
D13-1123,S12-1094,0,0.551478,"‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained models"
D13-1123,S12-1059,0,0.0968318,"Missing"
D13-1123,C04-1180,0,0.0261863,"to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based approaches to recognize textual entailment are similar to the work presented here. Bos and Markert (2006) extract semantic representations with Boxer (Bos et al., 2004) and incorporate background knowledge from external re1 2 http://www.wiktionary.org/ A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step an"
D13-1123,I05-5002,0,0.0288994,"text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art systems to determine textual simˇ c et al., 2012; Banea ilarity (B¨ar et al., 2012; Sari´ et al., 2012) do not ac"
D13-1123,W07-1401,0,0.0113594,"t man a This paper presents a novel approach to determine textual similarity. A layered methodology to transform text into logic forms is proposed, and semantic features are derived from a logic prover. Experimental results show that incorporating the semantic structure of sentences is beneficial. When training data is unavailable, scores obtained from the logic prover in an unsupervised manner outperform supervised methods. 1 Introduction The task of Semantic Textual Similarity (Agirre et al., 2012) measures the degree of semantic equivalence between two sentences. Unlike textual entailment (Giampiccolo et al., 2007), textual similarity is symmetric, and unlike both textual entailment and paraphrasing (Dolan and Brockett, 2005), textual similarity is modeled using a graded score rather than a binary decision. For example, sentence pair (1) below is very similar [5 out of 5], (2) is somewhat similar [3 out of 5] and (3) is not similar at all [0 out of 5]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. A cat is playing with a watermelon on a floor. A man is pouring oil into a pan. State-of-the-art"
D13-1123,S07-1003,0,0.0437229,"Missing"
D13-1123,S12-1079,0,0.105039,"al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1 , ˇ c et which contains over 3 million entries; and Sari´ al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 2012; Rios et al., 2012)2 did not perform at the top. Out of 88 runs, they were ranked 16, 36 and 64. We believe this is because they use semantic relations to calculate some ad-hoc similarity score. In contrast, our approach derives features from semantic representations encoded using logic, and combine these features using machine learning. Moreover, we use three logic form transformations capturing different levels of knowledge, from only content words to semantic structure. In turn, this allows us to boost performance by relying on semantics when simpler shallow methods fail. A few logic-based"
D13-1123,W99-0625,0,0.0653822,"y the same meaning. Sentence 2(c) A woman is applying cosmetics to her face and 2(d) A woman is putting on makeup are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity"
D13-1123,S10-1006,0,0.0259352,"Missing"
D13-1123,N06-2015,0,0.0348913,"ber of instances) in the train and test splits. Pustejovsky and Verhagen, 2009; Hendrickx et al., 2010) and in-house annotations. 4.1 Corpora We use the corpora released by SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity5 (Agirre et al., 2012). These corpora consist of pairs of sentences labeled with their semantic similarity score, ranging from 0.0 to 5.0. Sentence pairs come from five sources: (1) MSRpar, a corpus of paraphrases; (2) MSRvid, short video descriptions; (3) SMTeuroparl, output of machine translation systems and reference translations; (4) surprise.OnWN, OntoNotes (Hovy et al., 2006) and WordNet (Miller, 1995) glosses; and (5) surprise.SMTnews, output of machine translation systems in the news domain and gold translations. Examples can be found in Table 4, for more details refer to the aforementioned citation. 4.2 Results and Error Analysis Results are reported using the same train and test splits provided by the organization of SemEval 2012 Task 6. For surprise.OnWn and surprise.SMTnews, only test data is available and supervised machine learning is not an option. Table 5 shows results obtained with the test split not dropping and dropping unbound predicates. For compari"
D13-1123,O97-1002,0,0.187296,"Missing"
D13-1123,P12-3029,0,0.0230766,"Missing"
D13-1123,J10-3003,0,0.025096,"smetics to her face and 2(d) A woman is putting on makeup are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system t"
D13-1123,P11-1076,0,0.0131218,"up are highly similar even though the latter specifies neither the LOCATION where the ‘makeup’ is applied nor the fact that a PART of the ‘woman’ is her ‘face’. Similarly, sentences 2(e) A woman is dancing in the rain and 2(f) A woman dances in the rain outside are semantically equivalent since ‘rain’ always has LOCATION ‘outside’: missing this information does not carry loss of meaning. 2 Related Work Determining similarity between text snippets is relevant to information retrieval (Hatzivassiloglou et al., 1999), paraphrase recognition (Madnani and Dorr, 2010), grading answers to questions (Mohler et al., 2011) and many others. We focus on recent work and emphasize the differences from our approach. The SemEval 2012 Task 6: A Pilot on Semantic Textual Similarity (Agirre et al., 2012) brought together 35 teams that competed against each other. ˇ c et The top 3 performers (B¨ar et al., 2012; Sari´ al., 2012; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system to determine textual similarity. chine learning appro"
D13-1123,moldovan-blanco-2012-polaris,1,0.896368,"Missing"
D13-1123,J05-1004,0,0.00755777,"ould be the LFT component (Figure 3) so that it accounts for the subtleties of the representation of choice. The named entity recognizer extracts 35 finegrained types organized in a taxonomy (date, language, city, instrument, etc.) and was first developed for a question answering system (Moldovan et al., 2002). The implementation uses publicly available gazetteers as well as machine learning. Semantic relations are extracted with Polaris (Moldovan and Blanco, 2012), a semantic parser that given text extracts semantic relations. Polaris is trained using FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), several SemEval corpora (Girju et al., 2007; MSRpar (36/35) [750/750] MSRvid (13/13), [750/750] SMTeuroparl (56/21), [734/459] surprise.OnWN (–/16), [–/750] surprise.SMTnews (–/24), [–/399] Score 2.600 0.000 4.250 1.500 3.000 Sentence Pair The unions also staged a five-day strike in March that forced all but one of Yale’s dining halls to close. The unions also staged a five-day strike in March; strikes have preceded eight of the last 10 contracts. A woman is swimming underwater. A man is slicing some carrots. Then perhaps we could have avoided a catastrophe. We"
D13-1123,D09-1001,0,0.0303832,"SemRels Full woman N(x1 ) & dance V(x2 ) & rain N(x3 ) & outside M(x4 ) woman N(x1 ) & dance V(x2 ) & A G E N T SR(x2 , x1 ) & rain N(x3 ) & L O C A T I O N SR(x2 , x3 ) woman N(x1 ) & dance V(x2 ) & A G E N T SR(x2 , x1 ) & rain N(x3 ) & L O C A T I O N SR(x2 , x3 ) & outside M(x4 ) Table 1: Examples of logic from transformation using modes Basic, SemRels and Full. 3.1 Logic Form Transformation The logic form transformation (LFT) of a sentence is derived from the concepts in it, the semantic relations linking them and named entities. Unlike other LFT proposals (Zettlemoyer and Collins, 2005; Poon and Domingos, 2009), transforming sentences into logic forms is a straightforward step, the quality of the logic forms is determined by the output of standard NLP tools. We distinguish six types of predicates: • N for nouns, e.g., woman: woman N(x1 ). • V for verbs, e.g., dances: dance V(x2 ). • M for adjectives and adverbs, e.g., outside: outside M(x3 ). • O for concepts encoded by other POS tags. • NE for named entities, e.g., guitar: guitar N(x4 ) & instrument NE(x4 ). • SR for semantic relations, e.g., A woman dances: woman N(x1 ) & dance V(x2 ) & A G E N T SR(x2 , x1 ). In order to overcome semantic relatio"
D13-1123,W09-2418,0,0.0267092,"Missing"
D13-1123,S12-1100,0,0.0312894,"12; Banea et al., 2012), followed a maSentences Logic Form Transformation Pairwise Similarity Measures LFT-based scores logic forms Logic Prover features Machine Learning score pairwise word similarity scores Figure 3: Main components of our system to determine textual similarity. chine learning approach with features that do not take into account the semantic structure of sentences, e.g., n-grams, word overlap, evaluation measures for machine translation, pairwise word similarities, syntactic dependencies. All three used WordNet, Wikipedia and other large corpora. In particular, Banea et al. (2012) obtained models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B¨ar et al. (2012) used Wiktionary1 , ˇ c et which contains over 3 million entries; and Sari´ al. (2012) used The New York Times Annotated Corpus (Sandhaus, 2008), which contains over 1.8 million news articles, and Google n-grams (Lin et al., 2012), which consists of approximately 24GB of compressed text files. Our approach only uses WordNet, by far the smallest external resource with less than 120,000 synsets. Participants that incorporated information about the semantic structure of sentences (Glinos, 20"
D13-1123,H05-1047,1,0.763697,"nal re1 2 http://www.wiktionary.org/ A third team, spirin2, submitted results but a description paper could not be found in the ACL anthology. sources. They use a standard theorem prover and extract 8 features that are later combined using machine learning. Raina et al. (2005) use a logic form transformation derived from dependency parses and named entities. They use abductive reasoning and define an assumption cost model to account for partial entailments. Unlike them, we define three logic from transformations, use a modified resolution step and extract hundreds of features from the proofs. Tatu and Moldovan (2005) use a modified logic prover that drops predicates when a proof cannot be found. Unlike us, they do not drop unbound predicates and use a single logic form transformation. Another key difference is that they assign fixed weights to predicates a priori instead of using machine learning to determine them. 3 Approach Our approach to determine textual similarity (Figure 3) is grounded on using semantic features derived from a logic prover that are later combined in a standard supervised machine learning framework. First, sentences are transformed into logic forms (lft1 , lft2 ). Then, a modified l"
D13-1123,S12-1060,0,0.106804,"Missing"
D13-1123,C98-1013,0,\N,Missing
D13-1123,P94-1019,0,\N,Missing
D13-1123,meyers-etal-2004-annotating,0,\N,Missing
davis-moldovan-2010-feasibility,J93-2003,0,\N,Missing
davis-moldovan-2010-feasibility,E99-1010,0,\N,Missing
davis-moldovan-2010-feasibility,N03-1017,0,\N,Missing
davis-moldovan-2010-feasibility,W06-3121,1,\N,Missing
davis-moldovan-2010-feasibility,erjavec-2004-multext,0,\N,Missing
davis-moldovan-2010-feasibility,P00-1056,0,\N,Missing
E14-1016,J08-4004,0,0.0364456,"Missing"
E14-1016,P98-1013,0,0.477901,"Missing"
E14-1016,J12-4003,0,0.186285,"Missing"
E14-1016,bethard-etal-2008-building,0,0.064119,"Missing"
E14-1016,P11-1146,1,0.716211,"Missing"
E14-1016,W05-0620,0,0.651863,"Missing"
E14-1016,N10-1030,0,0.0696558,"Missing"
E14-1016,S10-1059,0,0.0276925,"Missing"
E14-1016,P10-1160,0,0.533142,"Missing"
E14-1016,S07-1003,0,0.0769354,"Missing"
E14-1016,W09-2415,0,0.0157265,"les of ‘introduced ’ indicate that NP1 is the THEME and ‘[in the 1930s]PP ’ the TIME. In this case, there is no connection beTable 3: Counts of selected PropBank semantic roles. Total number of predicates is 112,917. verb by verb basis in each frameset. For example, ARG2 is used to indicate “created-from, thing changed” with verb make and “entity exempted from” with verb exempt (Table 1). Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2). Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009). For example, AM - NEG and AM - MOD signal the presence of negation and modals, e.g., [wo]AM - MOD [n’t]AM - NEG [go]v . For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an3 http://verbs.colorado.edu/˜mpalmer/projects/ace/ PBguidelines.pdf 147 rs = {TIME , LOCATION, MANNER, CAUSE , PURPOSE }; foreach semantic role R′ (x′ , y) such that R′ ∈ rs do foreach verb x in the same sentence do generate potential implicit"
E14-1016,D08-1008,0,0.0224012,"Missing"
E14-1016,W05-0625,0,0.0391924,"Missing"
E14-1016,P13-1116,0,0.810702,"Missing"
E14-1016,W13-0114,0,0.0227615,"Missing"
E14-1016,meyers-etal-2004-annotating,0,0.130215,"Missing"
E14-1016,moldovan-blanco-2012-polaris,1,0.846444,"lapping sem rel ARG1 feature 51, overlapping head resigned feature 52, overlapping direct true Table 8: PropBank roles and values for features (50–52) when predicting potential implicit relation R (said, to pursue other interests ), labeled N . ARG 1 Hibben false LIBSVM (Chang and Lin, 2011). Parameters α and γ were tuned by grid search using 10-fold cross validation over training instances. Results are reported using features extracted from gold and automatic annotations. Gold annotations are taken directly from the Penn TreeBank and PropBank. Automatic annotations are obtained with Polaris (Moldovan and Blanco, 2012), a semantic parser that among others is trained with PropBank. Results using gold (automatic) annotations are obtained with a model trained with gold (automatic) annotations. Table 7: Feature values when deciding if R (succeeds, last summer ) can be inferred from the verb-argument structures in Figure 1. these features, detailed descriptions and examples are provided by Gildea and Jurafsky (2002). Features (17–52) are derived from the predicate structures of x and y and specially defined to infer implicit semantic relations. Features (17–31, 35– 49) are flags indicating the presence of semant"
E14-1016,P86-1004,0,0.340548,"Missing"
E14-1016,J05-1004,0,0.847625,"USE of the man undergoing some ‘change ’. A question answering system would benefit from detecting this relation when answering Why did he change? Extracting all semantic relations from text is a monumental task and is at the core of language understanding. In recent years, approaches that aim at extracting a subset of all relations have achieved great success. In particular, previous research (Carreras and M`arquez, 2005; Punyakanok et al., 2008; Che et al., 2010; Zapirain et al., 2010) focused on verb-argument structures, i.e., relations between a verb and its syntactic arguments. PropBank (Palmer et al., 2005) is the corpus of reference for verb-argument relations. However, relations between a verb and its syntactic arguments are only a fraction of the relations present in texts. Consider the statement [Mr. Brown]NP1 succeeds [Joseph W. Hibben, who retired last August]NP2 and its parse tree (Figure 1). Verbargument relations encode that NP1 is the AGENT and NP2 is the THEME of verb ‘succeeds ’ (PropBank uses labels ARG0 and ARG1 ). Any semantic relation between ‘succeeds ’ and concepts dominated in the parse tree by one of its syntactic arguments NP1 or NP2 , e.g., ‘succeeds ’ occurred after ‘last"
E14-1016,J08-2005,0,0.0529507,"Missing"
E14-1016,S10-1065,0,0.0259311,"Missing"
E14-1016,P10-1070,0,0.0263912,"d S-ADV the MANNER; roles of ‘introduced ’ indicate that NP1 is the THEME and ‘[in the 1930s]PP ’ the TIME. In this case, there is no connection beTable 3: Counts of selected PropBank semantic roles. Total number of predicates is 112,917. verb by verb basis in each frameset. For example, ARG2 is used to indicate “created-from, thing changed” with verb make and “entity exempted from” with verb exempt (Table 1). Unlike numbered arguments, modifiers share a common meaning across verbs (Table 2). Some modifiers are arguably not a semantic relation and are not present in most relation inventories (Tratz and Hovy, 2010; Hendrickx et al., 2009). For example, AM - NEG and AM - MOD signal the presence of negation and modals, e.g., [wo]AM - MOD [n’t]AM - NEG [go]v . For more information about PropBank annotations and examples, refer to the annotation guidelines.3 Inspecting PropBank annotations one can easily conclude that numbered arguments dominate the annotations and only a few modifiers are an3 http://verbs.colorado.edu/˜mpalmer/projects/ace/ PBguidelines.pdf 147 rs = {TIME , LOCATION, MANNER, CAUSE , PURPOSE }; foreach semantic role R′ (x′ , y) such that R′ ∈ rs do foreach verb x in the same sentence do ge"
E14-1016,S13-2001,0,0.0646688,"Missing"
E14-1016,P91-1003,0,0.836088,"Missing"
E14-1016,N10-1058,0,0.0256065,"Missing"
E14-1016,W09-2417,0,0.529564,"Missing"
E14-1016,S12-1001,0,0.116343,"Missing"
E14-1016,J93-2004,0,\N,Missing
E14-1016,H86-1011,0,\N,Missing
E14-1016,S10-1008,0,\N,Missing
E14-1016,C98-1013,0,\N,Missing
E14-1016,J02-3001,0,\N,Missing
E14-1016,S10-1006,0,\N,Missing
erekhinskaya-etal-2014-multilingual,C92-2082,0,\N,Missing
erekhinskaya-etal-2014-multilingual,P06-2037,0,\N,Missing
erekhinskaya-etal-2014-multilingual,moldovan-blanco-2012-polaris,1,\N,Missing
erekhinskaya-etal-2014-multilingual,fernando-stevenson-2012-mapping,0,\N,Missing
H05-1035,W99-0606,0,0.111116,"Missing"
H05-1035,W99-0628,0,0.638068,"Missing"
H05-1035,P98-1013,0,0.0340557,"s head noun, the prepositional phrase (np2), its preposition and its head noun (the second most important word in the PP). We have adopted the notation from (Collins and Brooks, 1995), where v is the verb, n1 is the head noun of object phrase, p is the preposition and n2 is the head noun of the prepositional phrase. Compared to our datasets, Ratnaparkhi’s dataset (Ratnaparkhi et al., 1994) contains only the lexical heads v, n1 , p and n2 . Thus, our methodology cannot be applied to Ratnaparkhi’s dataset (RRR). In our experiments we used two datasets: 274 • FN – extracted from FrameNet II 1.1 (Baker et al., 1998) • TB2 – extracted from Penn Treebank-II Table 1 presents the datasets1 . The creation of the datasets is described in details in (Olteanu, 2004). 4 Features The experiments described in this paper use a set of discrete (alphanumeric) and continuous (numeric) features. All features are fully deterministic, except the features count-ratio and pp-count that are based on information provided by an external resource - Google search engine (http://www.google. com). In describing the features, we will use the Penn Treebank-II parse tree associated with the sentence “The Lorillard spokeswoman said as"
H05-1035,C94-2195,0,0.410084,"Missing"
H05-1035,W95-0103,0,0.668252,"o choose between two classes: V when the prepositional phrase is attached to the verb and N when the prepositional phrase is attached to the preceding head noun. 3 Data To be able to extract the required features from a dataset instance, one must identify the verb, the phrase identifying the object of the verb that precedes the prepositional phrase in question (np1) which usually is part of the predicate-argument structure of the verb, its head noun, the prepositional phrase (np2), its preposition and its head noun (the second most important word in the PP). We have adopted the notation from (Collins and Brooks, 1995), where v is the verb, n1 is the head noun of object phrase, p is the preposition and n2 is the head noun of the prepositional phrase. Compared to our datasets, Ratnaparkhi’s dataset (Ratnaparkhi et al., 1994) contains only the lexical heads v, n1 , p and n2 . Thus, our methodology cannot be applied to Ratnaparkhi’s dataset (RRR). In our experiments we used two datasets: 274 • FN – extracted from FrameNet II 1.1 (Baker et al., 1998) • TB2 – extracted from Penn Treebank-II Table 1 presents the datasets1 . The creation of the datasets is described in details in (Olteanu, 2004). 4 Features The ex"
H05-1035,J02-3001,0,0.264118,"tion, Information Retrieval, Automatic Speech Recognition. Since parsing occurs early in the chain of NLP processing steps it has a large impact on the overall system performance. 2 Approach Our approach to solve the PP-attachment ambiguity is based on a Support Vector Machines learner (Cortes and Vapnik, 1995). The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions. Some of these features were proven efficient for semantic information labeling (Gildea and Jurafsky, 2002). The feature set also includes unsupervised information obtained from a very large corpus (World Wide Web). Features containing manually annotated semantic information about the verb and about the objects of the verb have also been used. We adopted the standard approach to distinguish between verb and noun attachment; thus the classifier has to choose between two classes: V when the prepositional phrase is attached to the verb and N when the prepositional phrase is attached to the preceding head noun. 3 Data To be able to extract the required features from a dataset instance, one must identif"
H05-1035,J93-1005,0,0.713299,"Missing"
H05-1035,J93-2004,0,0.0315929,"Missing"
H05-1035,P00-1014,0,0.830623,"Missing"
H05-1035,H94-1048,0,0.743818,"res from a dataset instance, one must identify the verb, the phrase identifying the object of the verb that precedes the prepositional phrase in question (np1) which usually is part of the predicate-argument structure of the verb, its head noun, the prepositional phrase (np2), its preposition and its head noun (the second most important word in the PP). We have adopted the notation from (Collins and Brooks, 1995), where v is the verb, n1 is the head noun of object phrase, p is the preposition and n2 is the head noun of the prepositional phrase. Compared to our datasets, Ratnaparkhi’s dataset (Ratnaparkhi et al., 1994) contains only the lexical heads v, n1 , p and n2 . Thus, our methodology cannot be applied to Ratnaparkhi’s dataset (RRR). In our experiments we used two datasets: 274 • FN – extracted from FrameNet II 1.1 (Baker et al., 1998) • TB2 – extracted from Penn Treebank-II Table 1 presents the datasets1 . The creation of the datasets is described in details in (Olteanu, 2004). 4 Features The experiments described in this paper use a set of discrete (alphanumeric) and continuous (numeric) features. All features are fully deterministic, except the features count-ratio and pp-count that are based on in"
H05-1035,P98-2177,0,0.545317,"Missing"
H05-1035,W97-0109,0,0.836631,"Missing"
H05-1035,W97-1016,0,0.478229,"ly for each P Most likely for each P Most likely for each P Average human, headwords (Ratnaparkhi et al., 1994) Average human, whole sentence (Ratnaparkhi et al., 1994) Maximum Likelihood-based (Hindle and Rooth, 1993) Maximum entropy, words (Ratnaparkhi et al., 1994) Maximum entropy, words & classes (Ratnaparkhi et al., 1994) Decision trees (Ratnaparkhi et al., 1994) Transformation-Based Learning (Brill and Resnik, 1994) Maximum-Likelihood based (Collins and Brooks, 1995) Maximum-Likelihood based (Collins and Brooks, 1995) Decision trees & WSD (Stetina and Nagao, 1997) Memory-based Learning (Zavrel et al., 1997) Maximum entropy, unsupervised (Ratnaparkhi, 1998) Maximum entropy, supervised (Ratnaparkhi, 1998) Neural Nets (Alegre et al., 1999) Boosting (Abney et al., 1999) Semi-probabilistic (Pantel and Lin, 2000) Maximum entropy, ensemble (McLauchlan, 2001) SVM (Vanschoenwinkel and Manderick, 2003) Nearest-neighbor (Zhao and Lin, 2004) FN dataset, w/o semantic features (FN-best-no-sem) FN dataset, w/ semantic features (FN-best-sem) TB2 dataset, best feature set (TB2-best) Accuracy 55.0 72.19 72.30 81.73 88.2 93.2 79.7 77.7 81.6 77.7 81.8 84.5 86.1 88.1 84.4 81.9 83.7 86.0 84.4 84.31 85.5 84.8 86.5 91."
H05-1035,A00-2018,0,\N,Missing
H05-1035,C98-1013,0,\N,Missing
H05-1035,C98-2172,0,\N,Missing
H05-1047,P98-1013,0,0.0640144,"such as PART- WHOLE , ISA , LOCATION , ATTRIBUTE , or AGENT . We listed several example rules in Table 3. The 64 axioms can be applied independent of the concepts involved in the semantic composition. We have also identified rules that can be applied only if the concepts that participate satisfy a certain condition or if the relations are of a certain type. For example, LOC ◦ LOC = LOC only if the LOC relation shows inclusion (John is in the car in the garage → LOC(John, garage). John is near the car behind the garage 6→ LOC(John, garage)). 4 FrameNet Can Help The Berkeley FrameNet project9 (Baker et al., 1998) is a lexicon-building effort based on the theory of frame semantics which defines the meanings of lexical units with respect to larger conceptual structures, called frames. Individual lexical units point to specific frames and establish a binding pattern to specific elements within the frame. FrameNet describes the underlying frames for different lexical units and examines sentences related to the frames using the BNC corpus. The result is an XML database that 9 http://framenet.icsi.berkeley.edu 375 contains a set of frames, a set of frame elements for each frame, and a set of frame annotated"
H05-1047,C02-1167,1,0.746102,"e identify some of the semantic relations encoded in the analyzed texts. We note that state-of-the-art semantic parsers cannot discover all the semantic relations conveyed implicitly or explicitly by the text. This problem compromises our system’s performance. To obtain the complete set of semantic relations that represents the meaning of the given texts, we introduce a new step in our algorithm. 1 After all, the entailment, inference, and equivalence terms originated from logic. 372 3. We add semantic axioms to the already created set of world knowledge, NLP, and WordNet-based lexical chain (Moldovan and Novischi, 2002) axioms that assist the logic prover in its search for proofs. We developed semantic axioms that show how two semantic relations can be combined. This will allow the logic prover to combine, whenever possible, semantic instances in order to infer new semantic relationships. The instances of relations that participate in semantic combinations can be either provided by the text or annotated between WordNet synsets. We also exploit other sources of semantic information from the text. For example, the frames encoded in the text sentence provide information which complements the meaning given by th"
H05-1047,P01-1052,1,0.811957,"n of SYNONYMY, HYPERNYMY or morphological derivation relations in WordNet. Because the set of semantic elements identified by a semantic parser does not necessarily convey the complete meaning of a sentence, we shall use a set of semantic axioms to infer the missing pieces of information. By combining two semantic relations or by using the FrameNet’s frame elements identified in a given text, we derive new semantic information. In order to show if T entails H, we analyze their meanings. Our approach to semantic entailment involves the following stages: 1. We convert each text into logic form (Moldovan and Rus, 2001). This conversion includes part-ofspeech tagging, parse tree generation, and name entity recognition. 2. Using our semantic parser, we identify some of the semantic relations encoded in the analyzed texts. We note that state-of-the-art semantic parsers cannot discover all the semantic relations conveyed implicitly or explicitly by the text. This problem compromises our system’s performance. To obtain the complete set of semantic relations that represents the meaning of the given texts, we introduce a new step in our algorithm. 1 After all, the entailment, inference, and equivalence terms origi"
H05-1047,N03-1022,1,0.816395,"ch complements the meaning given by the semantic relations. Our second type of axioms derive semantic relations between the frame elements of a given FrameNet frame. We claim that the process of applying the semantic axioms, given the semantic relations detected by a semantic parser, will capture the complete semantic information expressed by a text fragment. In this paper, we show the usefulness of this procedure for the RTE task, but we are convinced that it can be used by any system which plans to extract the entire semantic information from a given text. 4. We load the COGEX logic prover (Moldovan et al., 2003) which operates by “reductio ad absurdum” with H’s negated form and T ’s predicates. These clauses are weighted in the order in which they should be chosen to participate in the search. To ensure that H will be the last clause to participate, we assign it the largest value. The logic prover searches for new inferences that can be made using the smallest weight clauses. It also assigns a value to each inference based on the axiom it used to derive it. This process continues until the set of clauses is empty. If a refutation is found, the proof is complete. If a contradiction cannot be found, th"
H05-1047,W04-2609,1,0.773601,"axioms that combine two semantic relations. 373 Figure 1: TSemantics and HSemantics . The solid arrows represent the relations identified by the semantic parser. The dotted arrows symbolize the lexical chains between concepts in T and their analogous concepts in H (U ST and AmericaH belong to the same WordNet synset). The dash arrows denote the relations inferred by combining two semantic relations. The long dash arrows indicate the relations between frame elements. 3 Semantic Calculus 3.1 Semantic relations For this study, we adopt a revised version of the semantic relation set proposed by (Moldovan et al., 2004). Table 2 enumerates the semantic relations that we consider2 . 2 See (Moldovan et al., 2004) for definitions and examples. POSSESSION ( POS ) KINSHIP ( KIN ) PROPERTY- ATTRIBUTE ( PAH ) AGENT ( AGT ) TEMPORAL ( TMP ) DEPICTION ( DPC ) PART- WHOLE ( PW ) HYPERNYMY ( ISA ) ENTAIL ( ENT ) CAUSE ( CAU ) MAKE - PRODUCE ( MAK ) INSTRUMENT ( INS ) LOCATION - SPACE ( LOC ) PURPOSE ( PRP ) SOURCE - FROM ( SRC ) TOPIC ( TPC ) MANNER ( MNR ) MEANS ( MNS ) ACCOMPANIMENT ( ACC ) EXPERIENCER ( EXP ) RECIPIENT ( REC ) FREQUENCY ( FRQ ) INFLUENCE ( IFL ) ASSOCIATED WITH ( OTH ) MEASURE ( MEA ) SYNONYMY- NAME"
H05-1047,C04-1051,0,\N,Missing
H05-1047,C98-1013,0,\N,Missing
H05-1047,W07-1401,0,\N,Missing
H05-1112,N03-1011,1,0.92423,"Missing"
H05-1112,J02-3001,0,0.222247,"Missing"
H05-1112,W97-0808,0,0.0659645,"Missing"
H05-1112,W04-2609,1,0.900399,"Missing"
H05-1112,W04-0848,1,0.811604,"Missing"
H05-1112,W01-0511,0,0.468778,"Missing"
H05-1112,P99-1008,0,0.109492,"Missing"
H05-1112,J98-2002,0,\N,Missing
J06-1005,P99-1008,0,0.938571,"from them. For example, oasis–desert and Guadalupe Mountains National Park–Texas are PLACE – AREA relations. In this paper we use the Winston, Chaffin, and Hermann classification as a criterion for building the training corpus to provide a wide coverage of such subtypes of part– whole relations. 86 Girju, Badulescu, and Moldovan Automatic Discovery of Part–Whole Relations In computational linguistics, although a considerable amount of work has been done on semantic relation detection,7 the work most similar to the task of identifying part–whole semantic relations is that of Hearst (1992) and Berland and Charniak (1999). Hearst developed a method for the automatic acquisition of hypernymy relations by identifying a set of frequently used and mostly unambiguous lexico-syntactic patterns. For example, countries, such as England indicates a hypernymy relation between the words countries and England. In her paper, she mentions that she tried applying the same method to meronymy, but without much success, as the patterns detected also expressed other semantic relations. This is consistent with our study of part–whole lexico-syntactic patterns presented in this paper. In 1999, Berland and Charniak applied statisti"
J06-1005,J95-4004,0,0.0738293,"Missing"
J06-1005,A00-2018,0,0.00966401,"my girl does not). The ambiguity of these patterns explains our rationale for choosing an approach based on a machine learning method to discover discriminating rules automatically. 3. Lexico-Syntactic Patterns that Express Meronymy The automatic discovery of any semantic relation must start with a thorough understanding of the lexical and syntactic forms used to express that relation. Since there are many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express a meronymy semantic relation. 7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and Manning 2003), considerable interest has been shown in the automatic interpretation of various noun phrase-level constructions, such as noun compounds. The focus here is to determine the semantic relations that link the two noun constituents. The best-performing noun compound interpretation systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques (Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc, domain-specific, hand-coded semantic taxonomies, or on statistical"
J06-1005,J93-1003,0,0.121771,"Missing"
J06-1005,J02-3001,0,0.107462,"t). The ambiguity of these patterns explains our rationale for choosing an approach based on a machine learning method to discover discriminating rules automatically. 3. Lexico-Syntactic Patterns that Express Meronymy The automatic discovery of any semantic relation must start with a thorough understanding of the lexical and syntactic forms used to express that relation. Since there are many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express a meronymy semantic relation. 7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and Manning 2003), considerable interest has been shown in the automatic interpretation of various noun phrase-level constructions, such as noun compounds. The focus here is to determine the semantic relations that link the two noun constituents. The best-performing noun compound interpretation systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques (Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc, domain-specific, hand-coded semantic taxonomies, or on statistical patterns in a large corpu"
J06-1005,N03-1011,1,0.888805,"Missing"
J06-1005,C92-2082,0,0.741029,"Missing"
J06-1005,J93-2004,0,0.0283044,"Missing"
J06-1005,H05-1112,1,0.735372,"the verb to have has the sense of to possess, and only in some particular contexts refers to meronymy. Table 5 presents a summary of some of the most frequent part–whole lexicosyntactic patterns we observed, classified based on their ambiguity. Below we discuss further the ambiguities encountered in the patterns of the first three clusters. The Semantic Ambiguity of Genitive Constructions In English there are two kinds of genitives: the s-genitive and the of-genitive. A characteristic of the genitives is that they are very ambiguous, as the constructions can be given various interpretations (Moldovan and Badulescu 2005). For instance, genitives can encode relations such as PART– WHOLE (Mary’s hand), POSSESSION (Mary’s car), KINSHIP (Mary’s sister), PROPERTY / ATTRIBUTE HOLDER (Mary’s beauty), DEPICTION – DEPICTED (Mary’s painting — if it depicts her), SOURCE - FROM (Mary’s birth city), or Table 5 Examples of meronymic expressions based on their ambiguity. Types of Part–Whole Expressions Positive Examples (part–whole) Unambiguous The parts of an airplane include the engine, .. The substance consists of three ingredients. One of the air’s constituents is oxygen. The cloud was made of dust. Iceland is a member"
J06-1005,W04-2609,1,0.888751,"Missing"
J06-1005,W04-2607,0,0.00916935,"f word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns. 1. Introduction The identification of semantic relations in text is at the core of Natural Language Processing and many of its applications. Detecting semantic relations between various text segments, such as phrases, sentences, and discourse spans, is important for automatic text understanding (Rosario, Hearst, and Fillmore 2002; Lapata 2002; Morris and Hirst 2004). Furthermore, semantic relations represent the core elements in the organization of lexical semantic knowledge bases intended for inference purposes. Recently, there has been a renewed interest in text semantics as evidenced by the international ∗ Computer Science Department, University of Illinois at Urbana-Champaign, Urbana, IL 61801, E-mail: girju@uiuc.edu. † Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail: adriana@languagecomputer.com. ‡ Language Computer Corporation, 1701 N. Collins Blvd. Suite 2000, Richardson, TX 75080, E-mail: moldovan@lan"
J06-1005,W04-0848,1,0.743602,"Missing"
J06-1005,J93-2005,0,0.0427892,"Missing"
J06-1005,W93-0307,0,0.0402405,"Missing"
J06-1005,W01-0511,0,0.154961,"Missing"
J06-1005,P02-1032,0,0.0130828,"Missing"
J06-1005,H05-1047,1,0.642971,"Missing"
J06-1005,C94-2125,0,0.0109779,"many ways in which something can be part of something else, there is a variety of lexico-syntactic structures that can express a meronymy semantic relation. 7 Besides the work on semantic roles (Charniak 2000; Gildea and Jurafsky 2002; Thompson, Levy, and Manning 2003), considerable interest has been shown in the automatic interpretation of various noun phrase-level constructions, such as noun compounds. The focus here is to determine the semantic relations that link the two noun constituents. The best-performing noun compound interpretation systems have employed either symbolic (Finin 1980; Vanderwende 1994) or statistical techniques (Pustejovsky, Bergler, and Anick 1993; Lauer and Dras 1994; Lapata 2002) relying on rather ad hoc, domain-specific, hand-coded semantic taxonomies, or on statistical patterns in a large corpus of examples, respectively. 8 The North American News Corpus (NANC) of 1 million words. 87 Computational Linguistics Volume 32, Number 1 There are unambiguous lexical expressions that always convey a part–whole relation. For example: (2) The substance consists of three ingredients. (3) The cloud was made of dust. (4) Iceland is a member of NATO. In these cases the simple detecti"
J06-1005,J02-3004,0,\N,Missing
L18-1077,H05-1091,0,0.142472,"selected, the errors will be added up until the end. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 19"
L18-1077,I05-2023,0,0.0623371,"corpus, the training details and the tools used. Finally, we compare our work with previous work in the literature, and show the effectiveness of different features on the final results. 2. Related Work Research on Chinese relation extraction is quite limited compared with the progress with English. This may be due to two reasons. First, the Chinese language makes less use of function words and morphology (Levy and Manning, 2003), which makes it harder to extract syntactic information from it. Second, the lack of relevant corpora and tools also slows down the progress of research in Chinese. (Che et al., 2005) (Kebin et al., 2007) (Huang et al., 2008) (Yu et al., 2010), and (Dandan et al., 2012) proposed different kernel based approach to Chinese relation extraction. (Li et al., 2008) (Zhang et al., 2008),(Zhang et al., 2011) published a series of papers on feature-based approaches. In particular, they designed nine positional structures of enti489 ties and focused on the effectiveness of the positional feature on Chinese relation extraction. (Chen et al., 2014) proposed a novel Omni-word feature which takes advantage of Chinese sub-phrases, together with soft constraints for Chinese relation extra"
L18-1077,D14-1082,0,0.106079,"Missing"
L18-1077,P14-1054,0,0.0392107,"Missing"
L18-1077,D14-1179,0,0.0140362,"Missing"
L18-1077,P04-1054,0,0.179655,". If the features are not well selected, the errors will be added up until the end. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Ho"
L18-1077,N15-1133,0,0.0187373,"t. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so they could be used to model sequential data naturally. Recently they have been used frequently in many NLP tasks (Cho et al., 2014). Shortest dependency paths (SDP) have proven to be highly useful to relation extraction (Ebrahimi and Dou, 2015). They can to the utmost avoid involving irrelevant words in the path from one entity to another. (Xu et al., 2"
L18-1077,P05-1053,0,0.110354,"resident” is the employee of “Russia”, EMPLOYMENT(Russia, President). In ACE corpus 2005, six general relations such as ORG - AFF and PART- WHOLE, and eighteen subtype relations such as OWNERSHIP , LOCATED , and EMPLOYMENT are defined. The feature-based, kernel-based and deep learning-based methods are the most popular models for relation extraction in the literature. The basic idea of the feature-based approach is to treat relation extraction as a classification problem. Different kinds of features are extracted from text and then fed into a classifier. Such work includes (Kambhatla, 2004), (GuoDong et al., 2005) and (Moldovan and Blanco, 2012). However, these approaches suffer from error propagation problems. If the features are not well selected, the errors will be added up until the end. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel functio"
L18-1077,N07-1015,0,0.0448955,"D , and EMPLOYMENT are defined. The feature-based, kernel-based and deep learning-based methods are the most popular models for relation extraction in the literature. The basic idea of the feature-based approach is to treat relation extraction as a classification problem. Different kinds of features are extracted from text and then fed into a classifier. Such work includes (Kambhatla, 2004), (GuoDong et al., 2005) and (Moldovan and Blanco, 2012). However, these approaches suffer from error propagation problems. If the features are not well selected, the errors will be added up until the end. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang,"
L18-1077,P03-1056,0,0.0477535,"describe the system in detail, including the SDP and features for Chinese, and explain how to apply them to the LSTM Model. After that we present the details of our experiment, such as the corpus, the training details and the tools used. Finally, we compare our work with previous work in the literature, and show the effectiveness of different features on the final results. 2. Related Work Research on Chinese relation extraction is quite limited compared with the progress with English. This may be due to two reasons. First, the Chinese language makes less use of function words and morphology (Levy and Manning, 2003), which makes it harder to extract syntactic information from it. Second, the lack of relevant corpora and tools also slows down the progress of research in Chinese. (Che et al., 2005) (Kebin et al., 2007) (Huang et al., 2008) (Yu et al., 2010), and (Dandan et al., 2012) proposed different kernel based approach to Chinese relation extraction. (Li et al., 2008) (Zhang et al., 2008),(Zhang et al., 2011) published a series of papers on feature-based approaches. In particular, they designed nine positional structures of enti489 ties and focused on the effectiveness of the positional feature on Chi"
L18-1077,P08-2023,0,0.0630344,"Missing"
L18-1077,P14-5010,0,0.00774316,"atures into dense vectors, and feed them as input of LSTM models. 3.1. The Shortest Dependency Path Dependency parsing captures the dependence relations between words, and when compared with constituency paths, dependency paths have a better ability to encode information. In dependency parsing, the dependency relations and words will form a dependency graph. The edges are the dependency relations and the vertices are the words. Finding the shortest dependency path between words may be mapped into finding the shortest path between two vertices in the dependency graph. We used Stanford CoreNLP (Manning et al., 2014) for dependency parsing, and NetworkX (Hagberg et al., 2008) to get the shortest path in the dependency graph. Figure 2 and 3 show an example to map from the dependency graph to shortest dependency path of sentence “We went home after comforting her sister ( ).”. (p_Þ¶ 3.2. Figure 1: The complete representation of a long short term memory unit. The mathematical formulation of LSTM units is as follows: The input gate is to decide if the input xt is worth being preserved based on the input word xt and the past hidden state ht−1 . it = σ(W (i) xt + U (i) ht−1 ) The forget gate ft makes an a"
L18-1077,moldovan-blanco-2012-polaris,1,0.823074,"f “Russia”, EMPLOYMENT(Russia, President). In ACE corpus 2005, six general relations such as ORG - AFF and PART- WHOLE, and eighteen subtype relations such as OWNERSHIP , LOCATED , and EMPLOYMENT are defined. The feature-based, kernel-based and deep learning-based methods are the most popular models for relation extraction in the literature. The basic idea of the feature-based approach is to treat relation extraction as a classification problem. Different kinds of features are extracted from text and then fed into a classifier. Such work includes (Kambhatla, 2004), (GuoDong et al., 2005) and (Moldovan and Blanco, 2012). However, these approaches suffer from error propagation problems. If the features are not well selected, the errors will be added up until the end. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity betw"
L18-1077,W15-1506,0,0.0185393,"s (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so they could be used to model sequential data naturally. Recently they have been used frequently in many NLP tasks (Cho et al., 2014). Shortest dependency paths (SDP) have proven to be highly useful to relation extraction (Ebrahimi and Dou, 2015). They can to the utmost avoid involving irrelevant words in the path from one entity to another. (Xu et al., 2015) combined LSTMs and SDPs together and proposed a SDP-LSTM model for an English relation classification task on the SemEval-2012 d"
L18-1077,D14-1162,0,0.0817818,"Missing"
L18-1077,P13-1147,0,0.0206198,"lly analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so they could be used to model seque"
L18-1077,P15-1061,0,0.0246017,"objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so they could be used to model sequential data naturally. Recently they have been used frequently in many NLP tasks (Cho et al., 2014). Shortest dependency paths (SDP) have proven to be highly useful to relation extraction (Ebrahimi and Dou, 2015). They can to the utmost avoid involving irrelevant words in the path from one entity to another. (Xu et al., 2015) combined LSTMs and SDPs together and proposed a SDP-LSTM model for an English relation classifi"
L18-1077,D12-1110,0,0.0320155,"not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so they could be used to model sequential data naturally. Recently they have been used frequently in many NLP tasks (Cho et al., 2014). Shortest dependency paths (SDP) have proven to be highly useful to relation extraction (Ebrahimi and Dou, 2015). They can to the utmost avoid involving irrelevant words in the path from one entity"
L18-1077,I08-2119,0,0.030124,"2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so t"
L18-1077,D15-1206,0,0.0263447,"d Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in sequences, so they could be used to model sequential data naturally. Recently they have been used frequently in many NLP tasks (Cho et al., 2014). Shortest dependency paths (SDP) have proven to be highly useful to relation extraction (Ebrahimi and Dou, 2015). They can to the utmost avoid involving irrelevant words in the path from one entity to another. (Xu et al., 2015) combined LSTMs and SDPs together and proposed a SDP-LSTM model for an English relation classification task on the SemEval-2012 dataset. Intrigued by this idea, we built a simplified SDP-LSTM model for Chinese relation classification on the ACE 2005 corpus and obtained state-of-the-art results. To the best of our knowledge, we are the first to implement an LSTM network for Chinese relation classification. This paper is organized as follows: We first present related work, then describe the system in detail, including the SDP and features for Chinese, and explain how to apply them to the LSTM Mo"
L18-1077,C14-1220,0,0.0912262,"Missing"
L18-1077,P06-1104,0,0.0510858,"e added up until the end. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-t"
L18-1077,zhang-etal-2008-exploiting,0,0.0610012,"Missing"
L18-1077,D07-1076,0,0.0476796,"nd. (Jiang and Zhai, 2007) systematically analyzed the effectiveness of different features for relation extraction on a large feature space and concluded that just using the basic unit features from each feature space (sequence, syntactic and dependency relation) can achieve reasonably good performance, and adding more complex features may not benefit the result. The kernel-based approach is to compute a kernel function to measure the similarity between two data objects. Such work includes (Zelenko et al., 2003), (Culotta and Sorensen, 2004), (Bunescu and Mooney, 2005), (Zhang et al., 2006), (Zhou et al., 2007), (Wang, 2008) and (Plank and Moschitti, 2013). The key issue of the kernel-based approach is the slow training and prediction time, so it is not good enough to process big data. With the development of deep learning, a series of neural network-based models are proposed, such as recursive neural network-based approaches (Socher et al., 2012),(Ebrahimi and Dou, 2015), ÄW¯;ß and convolutional neural network-based approaches (Zeng et al., 2014), (Santos et al., 2015), and (Nguyen and Grishman, 2015). Long short-term memory (Hochreiter and Schmidhuber, 1997) can capture long-term dependencies in s"
M93-1025,M92-1001,0,\N,Missing
moldovan-blanco-2012-polaris,S07-1017,0,\N,Missing
moldovan-blanco-2012-polaris,S07-1008,0,\N,Missing
moldovan-blanco-2012-polaris,W08-2222,0,\N,Missing
moldovan-blanco-2012-polaris,H05-1112,1,\N,Missing
moldovan-blanco-2012-polaris,W08-2227,0,\N,Missing
moldovan-blanco-2012-polaris,H05-1047,1,\N,Missing
moldovan-blanco-2012-polaris,W09-2415,0,\N,Missing
moldovan-blanco-2012-polaris,W05-0620,0,\N,Missing
moldovan-blanco-2012-polaris,W03-1210,0,\N,Missing
moldovan-blanco-2012-polaris,D09-1001,0,\N,Missing
moldovan-blanco-2012-polaris,J02-3001,0,\N,Missing
moldovan-blanco-2012-polaris,S07-1003,0,\N,Missing
moldovan-blanco-2012-polaris,P10-1070,0,\N,Missing
moldovan-blanco-2012-polaris,P11-1146,1,\N,Missing
moldovan-blanco-2012-polaris,W09-2417,0,\N,Missing
moldovan-blanco-2012-polaris,balakrishna-etal-2010-semi,1,\N,Missing
N03-1011,P99-1008,0,0.84155,"Missing"
N03-1011,A00-2018,0,0.00477532,"cept, we manually annotate it with its corresponding sense in WordNet, for example carving knife 1 means sense number 1. 3.3 Building the Training Corpus and the Test Corpus In order to learn the constraints, we used the SemCor 1.7 and TREC 9 text collections. From the first two sets of the SemCor collection, 19,000 sentences were selected. Another 100,000 sentences were extracted from the LA Times articles of TREC 9. A corpus “A” was thus created from the selected sentences of each text collection. Each sentence in this corpus was then parsed using the syntactic parser developed by Charniak (Charniak, 2000). Focusing only on the sentences containing relations indicated by the three patterns considered, we manually annotated all the noun phrases in the 53,944 relationships matched by these patterns with their corresponding senses in WordNet (with the exception of those from SemCor). 6,973 of these relationships were part-whole relations, while 46,971 were not meronymic relations. We used for training a corpus of 34,609 positive examples (6,973 pairs of NPs in a part-whole relation extracted from the corpus “A” and 27,636 extracted from WordNet as selected pairs) and 46,971 negative examples (the"
N03-1022,P02-1005,1,0.505389,"Missing"
N03-1022,P01-1052,1,0.244805,"h in terms of high failure rate due to insufficient input axioms, as well as long processing time. Our solution is to integrate the prover into the QA system and rely on reasoning methods only to augment other previously implemented answer extraction techniques. 2 Integration of Logic Prover into a QA System The QA system includes traditional modules such as question processing, document retrieval, answer extraction, built in ontologies, as well as many tools such as syntactic parser, name entity recognizer, word sense disambiguation (Moldovan and Noviscki 2002), logic representation of text (Moldovan and Rus 2001) and others. The Logic Prover is integrated in this rich NLP environment and augments the QA system operation. As shown in Figure 1, the inputs to COGEX consist of logic representations of questions, potential answer paragraphs, world knowledge and lexical information. The term Answer Logic Form (ALF) refers to the candidate answers in logic form. Candidate answers returned by the Answer Extraction module are classified as open text due to the unpredictable nature of their grammatical structure. The term Question Logic Form (QLF) refers to the questions posed to the Question Answering system r"
N03-1022,C02-1167,1,\N,Missing
N12-1050,P98-1013,0,0.0858955,"g corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresponds to the focus of negation. Simply put, they propose that all roles but the one corresponding to the focus are actually positive. Their approach, however, has a major drawback: selecting the whole role often yields too coarse of a focus and the positive implicit meaning is not fully specified (Section 3.1). Focus-Sensitive Phenomena. The literature uses the term foc"
N12-1050,P11-1059,1,0.8915,"uistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresponds to the focus of negation. Simply put, they propose that all roles but the one corresponding to the focus are actually positive. Their approach, however, has a major drawback: selecting the whole role often yields too coarse of a focus and the positive implicit meaning is not fully specified (Section 3.1). Focus-Sensitive Phenomena. The literature uses the term focus for widely distinct phenomena; space permits only a curso"
N12-1050,W10-3001,0,0.0842809,"Missing"
N12-1050,D09-1103,0,0.0505812,"Missing"
N12-1050,W00-0730,0,0.038542,"hin coarse-grained focus {affected, go, . . . } {VB, VBN, . . . } predicate text predicate POS tag 19 pred-word 20 pred-POS 21 sem-role 22 coarse-chunk (SBAR) {ARG1, MLOC, . . . } {B-CFG, I-CFG, O} semantic role this token belongs to wrt coarse-verb coarse-grained annotation using BIO Table 6: Feature set used to predict fine-grained focus of negation. If a feature is especially useful for a particular syntactic node, we indicate so between parenthesis in the right hand side of column 1 (otherwise it is useful for all). 5.2 Experiments and Results We have carried our experiments using Yamcha (Kudoh and Matsumoto, 2000), a generic, customizable, and open source text chunker1 implemented using TinySVM2 . Following Yamcha’s design, we distinguish between static and dynamic features. Static features are the ones depicted in Table 6 for a fixed size window. Dynamic features are the predicted classes for a fixed set of previous instances. Whereas values for static features are considered correct, values for dynamic features are predictions of previous instances and therefore may contain errors. Varying window size effectively varies the number of features considered, the larger the window the more local context i"
N12-1050,C10-1076,0,0.0785113,"Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently,"
N12-1050,P11-1060,0,0.0273526,"knesses, shallow representations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is actually negating the TEMPORAL context linked to ship and not the verb per se; more specifically, n’t is negating exclusively the preposition until. Only doing so one can aim at representing the actual meaning of (1): The company will ship the new product to the United States during next year. Note that the verb ship, and its AGENT, THEME and LOCATION (i.e., The company, the new product and to the United States) are positive, as well as the temporal an"
N12-1050,W09-1304,0,0.0273848,"3-8, 2012. 2012 Association for Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes"
N12-1050,J05-1004,0,0.0368834,"statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresponds to the focus of negation. Simply put, they propose that all roles but the one corresponding to the focus are actually positive. Their approach, however, has a major drawback: selecting the whole role often yields too coarse of a focus and the positive implicit meaning is not fully specified (Section 3"
N12-1050,D09-1001,0,0.0122931,"taphor. Despite these weaknesses, shallow representations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is actually negating the TEMPORAL context linked to ship and not the verb per se; more specifically, n’t is negating exclusively the preposition until. Only doing so one can aim at representing the actual meaning of (1): The company will ship the new product to the United States during next year. Note that the verb ship, and its AGENT, THEME and LOCATION (i.e., The company, the new product and to the United States) are positive, as wel"
N12-1050,P11-1145,0,0.0178955,"resentations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is actually negating the TEMPORAL context linked to ship and not the verb per se; more specifically, n’t is negating exclusively the preposition until. Only doing so one can aim at representing the actual meaning of (1): The company will ship the new product to the United States during next year. Note that the verb ship, and its AGENT, THEME and LOCATION (i.e., The company, the new product and to the United States) are positive, as well as the temporal anchor next year. Regardless of"
N12-1050,W00-0726,0,0.160365,"Missing"
N12-1050,W08-0606,0,0.126036,"orleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations such as the reactions in NK3.3 cells are not always identical (Vincze et al., 2008), which carry the kind of positive meaning we aim at extracting (the reactions in NK3.3 cells are sometimes identical). Recently, Morante et al. (2011) present scope annotation in two Conan Doyle works, but they dismiss focus and positive meaning extraction. As stated before, PropBank (Palmer et al., 2005) treats negation superficially and FrameNet (Baker et al., 1998) regrettably disregards negation. Blanco and Moldovan (2011) introduce a semantic representation of negation using focus detection. They target verbal negation and work on top of PropBank, selecting as focus the role that corresp"
N12-1050,W10-3111,0,0.0177985,"of negative ele456 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 456–465, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely disregards negations"
N12-1050,J09-3003,0,0.0474498,"the position and form of negative ele456 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 456–465, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ments and negative concords; concepts such as intra and inter-domain negation and strength of negation (Ladusaw, 1996), syntactic and semantic negation (L¨obner, 2000) have been discussed in the extensive literature, although we do not use them. In computational linguistics, negation has mainly drawn attention in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, two events (Morante and Sporleder, 2010; Farkas et al., 2010) targeted negation mostly on those subfields. Among many others, Morante and Daelemans (2009) and Li et al. (2010) propose scope detectors using the BioScope corpus. Considering scope is indeed a step forward, but focus must also be taken into account to represent negated statements and detect their positive implicit meanings. Regarding corpora, BioScope annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purpose"
N12-1050,N09-2004,0,0.0145766,"ng using focus of negation are presented. The concept of granularity of focus is introduced and justified. New annotation and features to detect fine-grained focus are discussed and results reported. 1 Introduction Semantic representation of text is an important step towards text understanding. Current approaches are based on relatively shallow representations and ignore pervasive linguistic phenomena such as negation and metaphor. Despite these weaknesses, shallow representations have been proven useful for several tasks, e.g., coreference resolution (Kong et al., 2009), machine translation (Wu and Fung, 2009). Consider statement (1) The company won’t ship the new product to the United States until next year. Existing approaches to represent the meaning of (1) either indicate that the verb ship is negated or disregard the negation altogether. Semantic role labelers trained over PropBank would link n’t to ship with MNEG (i.e., negate the verb); any system based on FrameNet and more recent unsupervised proposals (Poon and Domingos, 2009; Liang et al., 2011; Titov and Klementiev, 2011) ignore negation. In order to represent the meaning of (1), one must first ascertain that the negation mark n’t is act"
N12-1050,J03-4003,0,\N,Missing
N12-1050,C98-1013,0,\N,Missing
P01-1037,A00-1023,0,\N,Missing
P01-1037,C00-1043,1,\N,Missing
P01-1037,H94-1052,0,\N,Missing
P01-1037,P00-1071,1,\N,Missing
P01-1037,A00-1025,0,\N,Missing
P01-1037,P96-1025,0,\N,Missing
P01-1037,A00-1021,0,\N,Missing
P01-1037,P95-1037,0,\N,Missing
P01-1037,A00-1041,0,\N,Missing
P01-1052,J96-3009,0,\N,Missing
P01-1052,C92-4189,0,\N,Missing
P01-1052,A88-1032,0,\N,Missing
P01-1052,P98-2180,0,\N,Missing
P01-1052,C98-2175,0,\N,Missing
P01-1052,P85-1037,0,\N,Missing
P01-1052,P98-2181,0,\N,Missing
P01-1052,C98-2176,0,\N,Missing
P01-1052,W99-0501,1,\N,Missing
P01-1052,H86-1003,0,\N,Missing
P02-1005,A00-1023,0,\N,Missing
P02-1005,C00-1043,1,\N,Missing
P02-1005,N01-1005,0,\N,Missing
P02-1005,A00-1025,0,\N,Missing
P02-1005,W01-1201,0,\N,Missing
P02-1005,H01-1069,0,\N,Missing
P02-1005,A00-1041,0,\N,Missing
P05-1026,C04-1084,1,0.885908,"gether to define a topic structure that will govern future interactions with the Q/A system. In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. 207 The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature. Later work by (Harabagiu, 2004) demonstrated that topic signatures can be further enhanced by discovering the most relevant relations that exist between pairs of concepts. However, both of these types of topic representations are limited by the fact that they require the identification of topic-relevant documents prior to the discovery of the topic signatures. In our experiments, we were only presented with a set of documents relevant to a particular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopi"
P05-1026,P94-1002,0,0.0328955,"ular scenario; no further relevance information was provided for individual subject areas or sub-topics. In order to solve the problem of finding relevant documents for each subtopic, we considered four different approaches: Approach 1: All documents in the CNS collection were initially clustered using K-Nearest Neighbor (KNN) clustering (Dudani, 1976). Each cluster that contained at least one keyword that described the sub-topic was deemed relevant to the topic. Approach 2: Since individual documents may contain discourse segments pertaining to different sub-topics, we first used TextTiling (Hearst, 1994) to automatically segment all of the documents in the CNS collection into individual text tiles. These individual discourse segments then served as input to the KNN clustering algorithm described in Approach 1. Approach 3: In this approach, relevant documents were discovered simultaneously with the discovery of topic signatures. First, we associated a binary seed relation  for each each  sub-topic  . (Seed relations were created both by hand and using the method presented in (Harabagiu, 2004).) Since seed relations are by definition relevant to a particular subtopic, they can be used to de"
P05-1026,W97-1307,0,0.0336162,"n relations were identified by patterns used for processing definition questions. Extraction relations are discovered by processing documents in order to identify three types of relations, including: (1) syntactic attachment relations (including subject-verb, object-verb, and verb-PP relations), (2) predicate-argument relations, and (3) salience-based relations that can be used to encode long-distance dependencies between topic-relevant concepts. (Salience-based relations are discovered using a technique first reported in (Harabagiu, 2004) which approximates a Centering Theory-style approach (Kameyama, 1997) to the resolution of coreference.) Subtopic: Egypt’s production of toxins and BW agents Topic Signature: produce − phosphorous trichloride (TOXIN) house − ORGANIZATION cultivate − non−pathogenic Bacilus Subtilis (TOXIN) produce − mycotoxins (TOXIN) acquire − FACILITY Subtopic: Egypt’s allies and partners Topic Signature: cooperate − COUNTRY provide − COUNTRY train − PERSON cultivate − COUNTRY supply − know−how supply − precursors Figure 3: Example of two topic signatures acquired for the scenario illustrated in Figure 2. We made the extraction relations associated with each topic signature mo"
P05-1026,C00-1072,0,0.143199,"esearch) and audience (i.e. the organization receiving the information), as well as a series of evidence conditions which specify how much verification information must be subject to before it can be accepted as fact. We assume the set of sub-topics mentioned in the general background and the scenario can be used together to define a topic structure that will govern future interactions with the Q/A system. In order to model this structure, the topic representation that we create considers separate topic signatures for each sub-topic. 207 The notion of topic signatures was first introduced in (Lin and Hovy, 2000). For each subtopic in a scenario, given (a) documents relevant to the sub-topic and (b) documents not relevant to the subtopic, a statistical method based on the likelihood ratio is used to discover a weighted list of the most topic-specific concepts, known as the topic signature. Later work by (Harabagiu, 2004) demonstrated that topic signatures can be further enhanced by discovering the most relevant relations that exist between pairs of concepts. However, both of these types of topic representations are limited by the fact that they require the identification of topic-relevant documents pr"
P05-1026,C04-1100,1,0.30758,"Missing"
P05-1026,P03-1044,0,0.0112707,"Missing"
P05-1026,C00-2136,0,\N,Missing
P05-1026,P03-1002,1,\N,Missing
P06-1113,kingsbury-palmer-2002-treebank,0,\N,Missing
P06-1113,C02-1167,1,\N,Missing
P06-1113,N03-1022,1,\N,Missing
P06-1113,P98-1013,0,\N,Missing
P06-1113,C98-1013,0,\N,Missing
P06-1113,P01-1052,1,\N,Missing
P06-1113,W07-1401,0,\N,Missing
P06-1113,P98-2127,0,\N,Missing
P06-1113,C98-2122,0,\N,Missing
P06-1113,W00-2021,0,\N,Missing
P06-2038,P03-1004,0,0.0479641,"Missing"
P06-2038,W00-2027,0,0.0739375,"Missing"
P06-2038,W00-0726,0,0.0608673,"Missing"
P06-2038,W01-0708,0,0.0652177,"Missing"
P06-2038,A00-2018,0,0.4885,"Missing"
P06-2038,P96-1025,0,0.0183897,"+ 1 do end ← start + span − 1 if ∀x ∈ S(x 6∼ (start, end)) then complete(start, end) end if end for end for X ← edge in chart[1,n,TOP] with highest probability return X (2,4) (2,3) (1,1) (2,2) The red (2,5) (3,5) (3,4) (3,3) (4,5) (4,4) (5,5) balloon flew away Chunk Span Figure 2: The same CYK example as in Figure 1. Blacked out box spans will not be calculated, while half toned box spans do not have to calculate as many possibilities because they depend on an uncalculated span. 3 Experiments & Results 3.1 Our parser with chunks Our parser uses a simplified version of the model presented in (Collins, 1996). For this experiment,we tested four versions of our internal parser: For example, given the chunk parse: [The red balloon]N P [flew]V P [away]ADV P , S = {(1, 3)} because there is only one chunk with a length greater than 1. Suppose we are analyzing the span (3, 4) on the example sentence above. This span will be rejected, as it overlaps with the chunk (1, 3); the leaf nodes “balloon” and “flew” are not going to be children of the same parsetree parent node. Thus, this method does not compute the generative rules for all the splits of the spans {(2, 4), (2, 5), (3, 4), (3, 5)}. This will also"
P06-2038,J93-2004,0,\N,Missing
P06-2038,J03-4003,0,\N,Missing
P06-2038,N01-1025,0,\N,Missing
P06-2105,H05-1079,0,0.108694,"Missing"
P06-2105,P97-1003,0,0.0758674,"has been confirmed, the verb’s predicate is negated (case NN(x1) & -confirm VB(e2,x15,x1)). prepositions and conjunctions are also added to link the text’s constituents. This syntactic layer of the logic representation is, automatically, derived from a full parse tree and acknowledges syntaxbased relationships such as: syntactic subjects, syntactic objects, prepositional attachments, complex nominals, and adjectival/adverbial adjuncts. In order to objectively evaluate our representation, we derived it from two different sources: constituency parse trees (generated with our implementation of (Collins, 1997)) and dependency parse trees (created using Minipar (Lin, 1998))1 . The two logic forms are slightly different. The dependency representation captures more accurately the syntactic dependencies between the concepts, but lacks the semantic information that our semantic parser extracts from the constituency parse trees. For instance, the sentence Gilda Flores was kidnapped on the 13th of January 19902 is “constituency” represented as Gilda NN(x1) & Flores NN(x2) & nn NNC(x3,x1,x2) & human NE(x3) & kidnap VB(e1,x9,x3) & on IN(e1,x8) & 13th NN(x4) & of NN(x5) & January (x6) & 1990 NN(x7) & nn NNC("
P06-2105,C02-1167,1,0.568855,"ce of the lexical chains by limiting the path length to three relations and the set of WordNet relations used to create the chains by discarding the paths that contain certain relations in a particular order. For example, the automatic axiom generation module does not consider chains with an IS - A relation followed by a )+*(,+-/.0*213*  !#&quot; eXtended WordNet lexical chains For the semantic entailment task, the ability to recognize two semantically-related words is an important requirement. Therefore, we automatically construct lexical chains of WordNet relations from ’s constituents to ’s (Moldovan and Novischi, 2002). In order to avoid errors introduced by a Word Sense Disambiguation system, we used the first senses for each word 5 unless the source and the target of the chain are synonyms. If a chain exists6 , the system generates, on demand, an axiom with the predicates of the source (from ) and the target (from ).         5 Because WordNet senses are ranked based on their frequency, the correct sense is most likely among the first . In our experiments,  . 6 Each lexical chain is assigned a weight based on its properties: shorter chains are better than longer ones, the relations are not equal"
P06-2105,P01-1052,1,0.407795,"ilment (Some people read Some smart people read). But, if both and are universally quantified, then the groups mentioned in must be a subset of the ones from (All people read All smart people read and All smart people read All people read). Thus, the scoring mod  Architecture 3 Knowledge Representation For the textual entailment task, our logic prover uses a two-layered logical representation which captures the syntactic and semantic propositions encoded in a text fragment. 3.1 Logic Form Transformation  In the first stage of our representation process, COGEX converts and into logic forms (Moldovan and Rus, 2001). More specifically, a predicate is created for each noun, verb, adjective and adverb. The nouns that form a noun compound are gathered under a nn NNC predicate. Each named entity class of a noun has a corresponding predicate which shares its argument with the noun predicate it modifies. Predicates for      820 ( ,  ) All people read Some smart people read All smart people read Some people read Add the dropped points for ’s modifiers Table 1: The quantification of  and (  , ) Some people read All smart people read Some smart people read All people read Subtract points for modif"
P06-2105,N03-1022,1,0.580308,"swers. In Multi-document Summarization, the redundant information should be recognized and omitted from the summary. Trying to boost research in textual inferences, the PASCAL Network proposed the Recognizing Textual Entailment (RTE) challenges (Dagan et al., 2005; Bar-Haim et al., 2006). For a pair of two text fragments, the task is to determine if the meaning of one text (the entailed hypothesis denoted by ) can be inferred from the meaning of the other text (the entailing text or ). In this paper, we propose a model to represent    2 Cogex - A Logic Prover for NLP Our system uses COGEX (Moldovan et al., 2003), a natural language prover originating from OTTER (McCune, 1994). Once its set of support is loaded with and the negated hypothesis ( ) and its usable list with the axioms needed to gener  819 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 819–826, c Sydney, July 2006. 2006 Association for Computational Linguistics  Figure 1: COGEX’s ule adds back the points for the modifiers dropped from and subtracts points for ’s modifiers not present in . The remaining two cases are summarized in Table 1. Because pairs with longer sentences can potentially drop more predicate"
P06-2105,H05-1047,1,\N,Missing
P06-2105,W07-1401,0,\N,Missing
P11-1059,P98-1013,0,0.0225897,"s (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does not consider negation and FactBank (Saur´ı and Pustejovsky, 2009) only annotates degrees of factuality for events. None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor extracting implicit positive meaning from them. 3 Negation in Natural Language Simply put, negation is a process that turns a statement into its opposite. Unlike affirmative statements, negation is marked by words (e.g., not, no, never) or affixes (e.g., -n’t, un-). Negation can interact wit"
P11-1059,H05-1079,0,0.0100444,"Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does"
P11-1059,choi-etal-2010-propbank-instance,0,0.0207324,"Missing"
P11-1059,W10-3110,0,0.171327,"th (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meani"
P11-1059,W10-3001,0,0.0781501,"Missing"
P11-1059,W09-1304,0,0.0240154,"ve concords. Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain eleme"
P11-1059,J05-1004,0,0.409408,"cations deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does not consider negation and FactBank (Saur´ı and Pustejovsky, 2009) only annotates degrees of factuality for events. None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor extracting implicit positive meaning from them. 3 Negation in Natural Language Simply put, negation is a process that turns a statement into"
P11-1059,W03-0210,0,0.0127702,"inly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the"
P11-1059,C88-2090,0,0.641311,"Missing"
P11-1059,W08-0606,0,0.360703,"ctors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005). Regarding corpora, the BioScope corpus annotates negation marks and linguistic scopes exclusively on biomedical texts. It does not annotate focus and it purposely ignores negations such as (talk582 ing about the reaction of certain elements) in NK3.3 cells is not always identical (Vincze et al., 2008), which carry the kind of positive meaning this work aims at extracting (in NK3.3 cells is often identical). PropBank (Palmer et al., 2005) only indicates the verb to which a negation mark attaches; it does not provide any information about the scope or focus. FrameNet (Baker et al., 1998) does not consider negation and FactBank (Saur´ı and Pustejovsky, 2009) only annotates degrees of factuality for events. None of the above references aim at detecting or annotating the focus of negation in natural language. Neither do they aim at carefully representing the meaning of negated statements nor ex"
P11-1059,W10-3111,0,0.104716,"over 60 pages to it. Negation interacts with quantifiers and anaphora (Hintikka, 2002), and influences reasoning (Dowty, 1994; S´anchez Valencia, 1991). Zeijlstra (2007) analyzes the position and form of negative elements and negative concords. Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments (Bos and Markert, 2005)"
P11-1059,J09-3003,0,0.022372,"llum (2002) dedicate over 60 pages to it. Negation interacts with quantifiers and anaphora (Hintikka, 2002), and influences reasoning (Dowty, 1994; S´anchez Valencia, 1991). Zeijlstra (2007) analyzes the position and form of negative elements and negative concords. Rooth (1985) presented a theory of focus in his dissertation and posterior publications (e.g., Rooth (1992)). In this paper, we follow the insights on scope and focus of negation by Huddleston and Pullum (2002) rather than Rooth’s (1985). Within natural language processing, negation has drawn attention mainly in sentiment analysis (Wilson et al., 2009; Wiegand et al., 2010) and the biomedical domain. Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al., 2010) targeted negation mostly on those subfields. Morante and Daelemans (2009) and ¨ ur and Radev (2009) propose scope detectors Ozg¨ using the BioScope corpus. Councill et al. (2010) present a supervised scope detector using their own annotation. Some NLP applications deal indirectly with negation, e.g., machine translation (van Munster, 1988), text classification (Rose et al., 2003) and recognizing entailments"
P11-1059,D09-1145,0,\N,Missing
P11-1059,C98-1013,0,\N,Missing
P11-1146,P08-2045,0,0.0608037,"R(x’, z ) is already known. We use the following heuristic in order to improve accuracy: do not instantiate an axiom R1 (x, y) ◦ R2 (y, z) → R3 (x, z) if a relation of the form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an alr"
P11-1146,W11-0106,1,0.783078,"of y id. id. op. id. id. id. id. op. op. [1] [1] [2] [3] [3] - Table 1: List of semantic primitives. In the fourth column, [1] stands for (Winston et al., 1987), [2] for (Cohen and Losielle, 1988) and [3] for (Huhns and Stephens, 1989). lations. The conclusion of an axiom is identified using an algebra for composing semantic primitives. We name this framework Composition of Semantic Relations (CSR). The extended definition, set of primitives, algebra to compose primitives and CSR algorithm are independent of any particular set of relations. We first presented CSR and used it over PropBank in (Blanco and Moldovan, 2011). In this paper, we extend that work using a different set of primitives and relations. Seventy eight inference axioms are obtained and an empirical evaluation shows that inferred relations have high accuracies. 2 Semantic Relations Semantic relations are underlying relations between concepts. In general, they are defined by a textual definition accompanied by a few examples. For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess::review and a"
P11-1146,W05-0620,0,0.713667,"Missing"
P11-1146,W04-3205,0,0.0467592,"finition, set of primitives, algebra to compose primitives and CSR algorithm are independent of any particular set of relations. We first presented CSR and used it over PropBank in (Blanco and Moldovan, 2011). In this paper, we extend that work using a different set of primitives and relations. Seventy eight inference axioms are obtained and an empirical evaluation shows that inferred relations have high accuracies. 2 Semantic Relations Semantic relations are underlying relations between concepts. In general, they are defined by a textual definition accompanied by a few examples. For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess::review and accomplish::complete. We find this widespread kind of definition weak and prone to confusion. Following (Helbig, 2005), we propose an extended definition for semantic relations, including semantic restrictions for its arguments. For example, AGENT(x, y ) holds between an animate concrete object x and a situation y. Moreover, we propose to characterize relations by semantic primitives. Primitives indicate whether a pr"
P11-1146,P01-1019,0,0.0435576,"entailments associated with certain predicates and arguments (Dowty, 2001). There has not been much work on composing relations in the field of computational linguistics. The term compositional semantics is used in conjunction with the principle of compositionality, i.e., the meaning of a complex expression is determined from the meanings of its parts, and the way in which those parts are combined. These approaches are usually formal and use a potentially infinite set of predicates to represent semantics. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al. (2001) develops algebras for semantic construction within grammars. Logic approaches include (Lakoff, 1970; S´anchez Valencia, 1991; MacCartney and Manning, 2009). Composition of Semantic Relations is complimentary to Compositional Semantics. Previous research has manually extracted plausible inference axioms for WordNet relations (Harabagiu and Moldovan, 1998) and transformed chains of relations into theoretical axioms (Helbig, 2005). The CSR algorithm proposed here automatically obtains inference axioms. Composing relations has been proposed before within knowledge bases. Cohen and Losielle (1988)"
P11-1146,P09-1069,0,0.0287744,"mes to perform linguistic analysis. Dowty (2006) studies compositionality and identifies entailments associated with certain predicates and arguments (Dowty, 2001). There has not been much work on composing relations in the field of computational linguistics. The term compositional semantics is used in conjunction with the principle of compositionality, i.e., the meaning of a complex expression is determined from the meanings of its parts, and the way in which those parts are combined. These approaches are usually formal and use a potentially infinite set of predicates to represent semantics. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al. (2001) develops algebras for semantic construction within grammars. Logic approaches include (Lakoff, 1970; S´anchez Valencia, 1991; MacCartney and Manning, 2009). Composition of Semantic Relations is complimentary to Compositional Semantics. Previous research has manually extracted plausible inference axioms for WordNet relations (Harabagiu and Moldovan, 1998) and transformed chains of relations into theoretical axioms (Helbig, 2005). The CSR algorithm proposed here automatically obtains inference axioms. Com"
P11-1146,P10-1160,0,0.128766,"uctivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation inventory, require annotation and do not reason or manipulate relations. In contrast to all the above references and the state of the art, the proposed framework obtains axioms that take as input semantic relations produce"
P11-1146,S07-1003,0,0.241537,"Missing"
P11-1146,W09-2415,0,0.143166,"Missing"
P11-1146,P07-2040,0,0.0291223,"do not instantiate an axiom R1 (x, y) ◦ R2 (y, z) → R3 (x, z) if a relation of the form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation in"
P11-1146,W04-0803,0,0.0163488,"e form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation inventory, require annotation and do not reason or manipulate relations. In co"
P11-1146,W09-3714,0,0.0218556,"tational linguistics. The term compositional semantics is used in conjunction with the principle of compositionality, i.e., the meaning of a complex expression is determined from the meanings of its parts, and the way in which those parts are combined. These approaches are usually formal and use a potentially infinite set of predicates to represent semantics. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al. (2001) develops algebras for semantic construction within grammars. Logic approaches include (Lakoff, 1970; S´anchez Valencia, 1991; MacCartney and Manning, 2009). Composition of Semantic Relations is complimentary to Compositional Semantics. Previous research has manually extracted plausible inference axioms for WordNet relations (Harabagiu and Moldovan, 1998) and transformed chains of relations into theoretical axioms (Helbig, 2005). The CSR algorithm proposed here automatically obtains inference axioms. Composing relations has been proposed before within knowledge bases. Cohen and Losielle (1988) combines a set of nine fairly specific relations (e.g., 1463 FOCUS - OF, PRODUCT- OF, SETTING - OF). The key to determine plausibility is the transitivity"
P11-1146,P07-3014,0,0.0290883,"in order to improve accuracy: do not instantiate an axiom R1 (x, y) ◦ R2 (y, z) → R3 (x, z) if a relation of the form R3 (x’, z) is already known. This simple heuristic has increased the accuracy of the inferences at the cost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR,"
P11-1146,J05-1004,0,0.172045,"in the box) or temporal (tmp, e.g., yesterday, last month) context of an entity. This simplified ontology does not aim at defining domains and ranges for any relation set; it is a simplification to fit the eight relations we work with. 5 Evaluation An evaluation was performed to estimate the validity of the 78 axioms. Because the number of axioms is large we have focused on a subset of them (Table 6). The 31 axioms having SYN as premise are intuitively correct: since synonymous concepts are interchangeable, given veracious annotation they perform valid inferences. We use PropBank annotation (Palmer et al., 2005) to instantiate the premises of each axiom. First, all instantiations of axiom PRP ◦ MNR−1 → MNR−1 were manually checked. This axiom yields 237 new MANNER, 189 of which are valid (Accuracy 0.80). Second, we evaluated axioms 1–7 (Table 6). Since PropBank is a large corpus, we restricted this phase to the first 1,000 sentences in which there is an instantiation of any axiom. These sentences contain 1,412 instantiations and are found in the first 31,450 sentences of PropBank. Table 6 depicts the total number of instantiations for each axiom and its accuracy (columns 3 and 4). Accuracies range fro"
P11-1146,W09-2417,0,0.286512,"ost of lowering their productivity. The last three columns in Table 6 show results when using the heuristic. 1462 6 Comparison with Previous Work There have been many proposals to detect semantic relations from text without composition. Researches have targeted particular relations (e.g., CAUSE (Chang and Choi, 2006; Bethard and Martin, 2008)), relations within noun phrases (Nulty, 2007), named entities (Hirano et al., 2007) or clauses (Szpakowicz et al., 1995). Competitions include (Litkowski, 2004; Carreras and M`arquez, 2005; Girju et al., 2007; Hendrickx et al., 2009). Two recent efforts (Ruppenhofer et al., 2009; Gerber and Chai, 2010) are similar to CSR in their goal (i.e., extract meaning ignored by current semantic parsers), but completely differ in their means. Their merit relies on annotating and extracting semantic connections not originally contemplated (e.g., between concepts from two different sentences) using an already known and fixed relation set. Unlike CSR, they are dependent on the relation inventory, require annotation and do not reason or manipulate relations. In contrast to all the above references and the state of the art, the proposed framework obtains axioms that take as input se"
P11-1146,S10-1006,0,\N,Missing
P99-1020,P94-1020,0,0.0605572,"1 Introduction Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al., 1992), (Miller et al., 1994), (Agirre and Rigau, 1995), (Li et al., 1995), (McRoy, 1992); There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. A possible solution for automatic acquisition of sense tagged corpora has been presented in (Mihalcea and Moldovan, 1999), but the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the sta"
P99-1020,H92-1046,0,0.0370666,"Missing"
P99-1020,H92-1045,0,0.0109888,"he sentence context. The words are paired and an attempt is made to disambiguate one word within the context of the other word. This is done by searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al., 1992), (Ng and Lee, 1996); 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). 152 processed and the senses axe ranked. We use the ranking of senses to curb the computational complexity in the step that follows. Only the most promising senses are kept. The next step is to refine the ordering of senses by using a completely different method, namely the semantic density. This is measured by the number of common words that are within a semantic distance of two or more words. The closer the semantic relationship between two words the hi"
P99-1020,J92-1001,0,0.0850231,"Missing"
P99-1020,H94-1046,0,0.00891521,"Missing"
P99-1020,P96-1006,0,0.458527,"e Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al., 1992), (Miller et al., 1994), (Agirre and Rigau, 1995), (Li et al., 1995), (McRoy, 1992); There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. A possible solution for automatic acquisition of sense tagged corpora has been presented in (Mihalcea and Moldovan, 1999), but the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the statistical methods di"
P99-1020,W97-0213,0,0.0121105,"the use of glosses is that they are not part-of-speech tagged, like some corpora are (i.e. Treebank). For this reason, when determining the nouns from the verb glosses, an error rate is introduced, as some verbs (like make, have, go, do) are lexically ambiguous having a noun representation in WordNet as well. We believe that future work on part-of-speech tagging the glosses of WordNet will improve our results. 2. The determination of senses in SemCor was done of course within a larger context, the context of sentence and discourse. By working 156 Comparison with other methods As indicated in (Resnik and Yarowsky, 1997), it is difficult to compare the WSD methods, as long as distinctions reside in the approach considered (MRD based methods, supervised or unsupervised statistical methods), and in the words that are disambiguated. A method that disambiguates unrestricted nouns, verbs, adverbs and adjectives in texts is presented in (Stetina et al., 1998); it attempts to exploit sentential and discourse contexts and is based on the idea of semantic distance between words, and lexical relations. It uses WordNet and it was tested on SemCor. Table 4 presents the accuracy obtained by other WSD methods. The baseline"
P99-1020,W97-0209,0,0.0270838,"searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al., 1992), (Ng and Lee, 1996); 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). 152 processed and the senses axe ranked. We use the ranking of senses to curb the computational complexity in the step that follows. Only the most promising senses are kept. The next step is to refine the ordering of senses by using a completely different method, namely the semantic density. This is measured by the number of common words that are within a semantic distance of two or more words. The closer the semantic relationship between two words the higher the semantic density between them. We introduce the semantic density because it is relatively easy to measure it on a MRD like WordNet"
P99-1020,P97-1007,0,0.0160918,"SD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et al., 1992), (Miller et al., 1994), (Agirre and Rigau, 1995), (Li et al., 1995), (McRoy, 1992); There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. A possible solution for automatic acquisition of sense tagged corpora has been presented in (Mihalcea and Moldovan, 1999), but the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the statistical methods disambiguate adjectives"
P99-1020,W98-0701,0,0.0221786,"ut the corpora acquired with this method has not been yet tested for statistical disambiguation of words. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. None of the statistical methods disambiguate adjectives or adverbs so far. In this paper, we introduce a method that attempts to disambiguate all the nouns, verbs, adjectives and adverbs in a text, using the senses provided in WordNet (Fellbaum, 1998). To our knowledge, there is only one other method, recently reported, that disambiguates unrestricted words in texts (Stetina et al., 1998). 2 A word-word approach dependency The method presented here takes advantage of the sentence context. The words are paired and an attempt is made to disambiguate one word within the context of the other word. This is done by searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpu"
P99-1020,P95-1026,0,0.0407558,"This is done by searching on Internet with queries formed using different senses of one word, while keeping the other word fixed. The senses are ranked simply by the order provided by the number of hits. A good accuracy is obtained, perhaps because the number of texts on the Internet is so large. In this way, all the words are 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al., 1992), (Ng and Lee, 1996); 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). 152 processed and the senses axe ranked. We use the ranking of senses to curb the computational complexity in the step that follows. Only the most promising senses are kept. The next step is to refine the ordering of senses by using a completely different method, namely the semantic density. This is measured by the number of common words that are within a semantic distance of two or more words. The closer the semantic relationship between two words the higher the semantic density between them. We introduce the semantic density because it is relatively easy to measure it on a M"
S01-1031,J95-4004,0,0.0137324,"ning from available sense tagged corpora and dictionary definitions and instance based learning with active feature selection. The two modules are preceded by a preprocessing phase which includes compound concept identification, and followed by a default phase that assigns the most frequent sense as a last resort, when no other previous methods could be applied. The shaded areas in Figure 1 are specific for the case when larger training data sets are available. During the preprocessing stage, SGML tags are eliminated, the text is tokenized, part of speech tags are assigned using Brill tagger (Brill, 1995), and Named Entities (NE) are identified with an in-house implementation of an NE recognizer. To identify collocations, we determine sequences of words that form compound concept,s defined in WordNet. In the second step, patterns 3 are learned from WordNet, SemCor and GenCor, which is a large 2 I.e. in addition to the publicly available sense tagged corpora 3 We alternatively call them rules as they basically specify the sense triggered by a given local context, using rules like ""if the word before is X then sense is Y"" FREE UNTAGGED TEXT ,'~REPROCESSING , '' - eliminate SGMLtags - tokenizatio"
tatu-moldovan-2012-tool,N10-1020,0,\N,Missing
tatu-moldovan-2012-tool,W07-1404,1,\N,Missing
tatu-moldovan-2012-tool,P06-2105,1,\N,Missing
tatu-moldovan-2012-tool,P94-1009,0,\N,Missing
W00-1104,P97-1010,0,0.0493456,"Missing"
W00-1104,H92-1022,0,0.0172453,"Missing"
W00-1104,H93-1061,0,0.0411975,"peech and Offset is the offset of the WordNet synset in which this word occurs. In the case when no sense is assigned by the WSD module or if the word cannot be found in WordNet, the last field is left empty. 2. I n d e x i n g module, which indexes the documents, after they are processed by the WSD module. From the new format of a word, as returned by the WSD function, the Stem and, separately, the Offset{POS are added to the index. This 38 be part of these pairs. Then, we extract all the occurrences of these pairs found within the semantic tagged corpus formed with the 179 texts from SemCor(Miller et al., 1993). If, in all the occurrences, the word Wi has only one sense # k , and the number of occurrences of this sense is larger t h a n 3, then mark the word Wi as having sense # k . Example. Consider t h e word a p p r o v a l in the text fragment ' ' c o m m i t t e e a p p r o v a l o f ' '. The pairs formed are ' ~cown-ittee a p p r o v a l ' ' and ' ~a p p r o v a l o f ' '. No occurrences of the first pair are found in the corpus. Instead, there are four occurrences of the second pair, and in all these occurrences the sense of a p p r o v a l is sense # 1 . Thus, a p p r o v a l is marked with"
W00-1104,H93-1052,0,0.0946179,"Missing"
W03-1207,P02-1031,0,0.0240159,"Missing"
W03-1207,A00-2031,0,\N,Missing
W03-1207,P00-1065,0,\N,Missing
W03-1207,H94-1020,0,\N,Missing
W04-0840,P01-1052,1,0.80227,"this way, heads of phrases are marked. Next comes the identification of dependent arguments (those which are derived from other independent and/or dependent arguments). These may include arguments for modifiers (adjectives, adverbs), secondary verb slots (all except the first) and secondary coordinating conjunction slots (all except the resulting argument), or linking words (prepositions, subordinating conjunctions, etc). The derivation of these arguments follows from a slot-filling approach and is based on the interpretation of the parse tree structure and the associated transformation rule (Moldovan and Rus, 2001). This is a rule that says how a particular parse tree structure must be handled, for instance, 'S -&gt; NP VP' says that the subject of the main/action verb of VP is the head of phrase of NP. 3 Dealing with Ambiguous Structures Named-entity tags are helpful when parsing certain ambiguous structures. Take, for instance, the following two sentences: (i) They gave the visiting team a heavy loss. (ii) They played football every evening. The grammar rule for the verb phrase in both sentences is 'VP -&gt; VB NP NP'. Whereas, in sentence (i), the first NP is the indirect object and the second one the dire"
W04-0840,N03-1022,1,0.907114,"Missing"
W04-2609,P98-1013,0,0.0729414,"Missing"
W04-2609,A00-2031,0,0.521413,"ous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on sta"
W04-2609,P01-1017,0,0.0737792,"Missing"
W04-2609,J02-3001,0,0.102668,"a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer a"
W04-2609,P02-1031,0,0.0148143,"ns, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsk"
W04-2609,N03-1011,1,0.590975,"Missing"
W04-2609,W04-2610,1,0.739062,"vels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others. These errors could be substantially decreased with more research effort. A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations. The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure. Details about this approach are provided in (Girju et al. 2004)). 3 Applications Semantic relations occur with high frequency in open text, and thus, their discovery is paramount for many applications. One important application is Question Answering. A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs. Here is an example. Q. Where have nuclear incidents occurred? From the question stem word where, we know the question asks for a LOCATION which is found in the"
W04-2609,J93-2005,0,0.0217005,"lmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993). Another popular approach focuses on the interpretation of the underlying semantics. Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Vanderwende 1994). More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text. (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques. 2 Approach 2.1 Basic Approach We approach the problem top-down, namely identify and"
W04-2609,W01-0511,0,0.811371,"d by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds. Several approaches have been proposed for empirical noun-compound"
W04-2609,P02-1032,0,0.624644,"Missing"
W04-2609,C94-2125,0,0.825094,"ome references. Most of the time, the semantic relations are encoded by lexico-syntactic patterns that are highly ambiguous. One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge. Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct. This is called semantic blend (Quirk et al.1985). For example, the expression “Texas city” contains both a LOCATION as well as a PART- WHOLE relation. Other researchers have identified other sets of semantic relations (Levi 1979), (Vanderwende 1994), (Sowa 1994), (Baker, Fillmore, and Lowe 1998), (Rosario and Hearst 2001), (Kingsbury, et al. 2002), (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002). Our list contains the most frequently used semantic relations we have observed on a large corpus. Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals. The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these com"
W04-2609,C98-1013,0,\N,Missing
W04-2609,J02-3004,0,\N,Missing
W04-2610,P98-1013,0,0.07349,"Missing"
W04-2610,A00-2031,0,0.075496,"and (5) Adjective clauses where the head noun is modified by a relative clause (eg the man who was driving the car - an AGENT relation between man and driving). 1.2 Previous work on the discovery of semantic relations The development of large semantically annotated corpora, such as Penn Treebank2 and, more recently, PropBank (Kingsbury, et al. 2002), as well as semantic knowledge bases, such as FrameNet (Baker, Fillmore, and Lowe 1998), have stimulated a high interest in the automatic acquisition of semantic relations, and especially of semantic roles. In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. These statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in a learning algorithm. While these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominalized noun phrases and present a method for their automatic detection in op"
W04-2610,kingsbury-palmer-2002-treebank,0,0.0614616,"Missing"
W04-2610,P01-1017,0,0.0819073,"Missing"
W04-2610,W04-2609,1,0.796026,"Missing"
W04-2610,J02-3001,0,0.0537888,"re the head noun is modified by a relative clause (eg the man who was driving the car - an AGENT relation between man and driving). 1.2 Previous work on the discovery of semantic relations The development of large semantically annotated corpora, such as Penn Treebank2 and, more recently, PropBank (Kingsbury, et al. 2002), as well as semantic knowledge bases, such as FrameNet (Baker, Fillmore, and Lowe 1998), have stimulated a high interest in the automatic acquisition of semantic relations, and especially of semantic roles. In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. These statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in a learning algorithm. While these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominalized noun phrases and present a method for their automatic detection in open-text. 1.3 Approach We app"
W04-2610,P02-1031,0,0.117825,"by a relative clause (eg the man who was driving the car - an AGENT relation between man and driving). 1.2 Previous work on the discovery of semantic relations The development of large semantically annotated corpora, such as Penn Treebank2 and, more recently, PropBank (Kingsbury, et al. 2002), as well as semantic knowledge bases, such as FrameNet (Baker, Fillmore, and Lowe 1998), have stimulated a high interest in the automatic acquisition of semantic relations, and especially of semantic roles. In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. These statistical techniques operate on the output of probabilistic parsers and take advantage of the characteristic features of the semantic roles that are then employed in a learning algorithm. While these systems focus on verb-argument semantic relations, called semantic roles, in this paper we investigate predicate-argument semantic relations in nominalized noun phrases and present a method for their automatic detection in open-text. 1.3 Approach We approach the problem top-down"
W04-2610,C94-1042,0,0.0576763,"Missing"
W04-2610,C98-1013,0,\N,Missing
W06-3121,P05-1066,0,0.042557,"le 1: Vocabulary size change due to forced alignment the vocabulary size is shown in Table 1. To simplify the process, we limited the replacement of tokens to one-to-one (one real token to one special token), so that the word alignment file can be directly used together with the original parallel corpus to extract phrases required for the generation of the translation table. Table 2 shows an example of the output. The second enhancement tries to improve the quality of the translation by rearranging the words in the source sentence to better match the correct word order in the target language (Collins et al., 2005). We focused on a very specific pattern – based on the part-of-speech tags, changing the order of NN-ADJ phrases in the non-English sentences. This process was also applied to the input dev/test files, when the target language was English. Figure 1 shows the reordering process and its effect on the alignment. The expected benefits are: • More phrases extracted from the word aligned corpus. Monotone alignment tends to generate more phrases than a random alignment. • Higher mixture weight for the monotone distortion model because of fewer reordering constraints during MERT, thus the value of the"
W06-3121,W05-0820,0,0.169454,"Missing"
W06-3121,N03-1017,0,0.0152725,"a system for participation in the WMT 2006 shared task based on Phramer and other tools. We participated in 5 subtasks: DE→EN, FR→EN, ES→EN, EN→FR and EN→ES. 3.1 Baseline system 3.1.1 Translation table generation To generate a translation table for each pair of languages starting from a sentence-aligned parallel corpus, we used a modified version of the Pharaoh training software 2 . The software also required GIZA++ word alignment tool(Och and Ney, 2003). We generated for each phrase pair in the translation table 5 features: phrase translation probability (both directions), lexical weighting (Koehn et al., 2003) (both directions) and phrase penalty (constant value). 3.1.2 Decoder The Phramer decoder was used to translate the devtest2006 and test2006 files. We accelerated the decoding process by using the distributed decoding tool. 3.1.3 Minimum Error Rate Training We determined the weights to combine the models using the MERT component in Phramer. Because of the time constrains for the shared task submission3 , we used Pharaoh + Carmel4 as the de2 http://www.iccs.inf.ed.ac.uk/∼pkoehn/training.tgz After the shared task submission, we optimized a lot our decoder. Before the optimizations (LM optimizati"
W06-3121,koen-2004-pharaoh,0,0.103311,"ranslation (SMT) is very active, there isn’t an abundance of open-source tools available to the community. In this paper, we present Phramer, an open-source system that embeds a phrase-based decoder, a minimum error rate training (Och, 2003) module and various tools related to Machine Translation (MT). The software is released under BSD license and it is available at http://www.phramer.org/. We also describe our Phramer-based system that we build for the WMT06 shared task. 2 Phramer Phramer is a phrase-based SMT system written in Java. It includes: • A decoder that is compatible with Pharaoh (Koehn, 2004), • A minimum error rate training (MERT) module, compatible with Phramer’s decoder, with The decoder is fully compatible with Pharaoh 1.2 in the algorithms that are implemented, input files (configuration file, translation table, language models) and command line. Some of the advantages of Phramer over Pharaoh are: (1) source code availability and its permissive license; (2) it is very fast (1.5–3 times faster for most of the configurations); (3) it can work with various storage layers for the translation table (TT) and the language models (LMs): memory, remote (access through TCP/IP), disk (u"
W06-3121,J03-1002,0,0.00836043,"Missing"
W06-3121,P03-1021,0,0.0672894,"urce Phrase-Based Statistical Machine Translation Decoder - Phramer. The paper also presents the UTD (HLTRI) system build for the WMT06 shared task. Our goal was to improve the translation quality by enhancing the translation table and by preprocessing the source language text • various tools. 1 Introduction Despite the fact that the research in Statistical Machine Translation (SMT) is very active, there isn’t an abundance of open-source tools available to the community. In this paper, we present Phramer, an open-source system that embeds a phrase-based decoder, a minimum error rate training (Och, 2003) module and various tools related to Machine Translation (MT). The software is released under BSD license and it is available at http://www.phramer.org/. We also describe our Phramer-based system that we build for the WMT06 shared task. 2 Phramer Phramer is a phrase-based SMT system written in Java. It includes: • A decoder that is compatible with Pharaoh (Koehn, 2004), • A minimum error rate training (MERT) module, compatible with Phramer’s decoder, with The decoder is fully compatible with Pharaoh 1.2 in the algorithms that are implemented, input files (configuration file, translation table,"
W06-3121,P02-1040,0,0.0730724,"Lite databases1 ). Extensions for other storage layers can be very easily implemented; (4) it is more configurable; (5) it accepts compressed data files (TTs and LMs); (6) it is very easy to extend; an example is provided in the package – part-of-speech decoding on either source language, target language or both; support for POS-based language models; (7) it can internally generate n-best lists. Thus no external tools are required. The MERT module is a highly modular, efficient and customizable implementation of the algorithm described in (Och, 2003). The release has implementations for BLEU (Papineni et al., 2002), WER and PER error criteria and it has decoding interfaces for Phramer and Pharaoh. It can be used to search parameters over more than one million variables. It offers features as resume search, reuse hypotheses from previous runs and various strategies to search for optimal λ weight vectors. 1 http://www.sqlite.org/ 146 Proceedings of the Workshop on Statistical Machine Translation, pages 146–149, c New York City, June 2006. 2006 Association for Computational Linguistics The package contains a set of tools that include: coder for the MERT algorithm. • Distributed decoding (compatible with bo"
W06-3122,P01-1017,0,0.211089,"Phramer and the configuration described in Section 3, the 3-gram LM was queried 27 million times (3 million distinct queries). We developed for the WMT 2006 shared task a system that is trained on a (a) word-aligned bilingual corpus, (b) a large monolingual (English) corpus and (c) an English treebank and it is capable of translating from a source language (German, Spanish and French) into English. Our system embeds Phramer2 (used for minimum error rate training, decoding, decoding tools), Pharaoh (Koehn, 2004) (decoding), Carmel 3 (helper for Pharaoh in n-best generation), Charniak’s parser (Charniak, 2001) (language model) and SRILM4 (n-gram LM construction). 2.1 Translation table construction We developed a component that builds a translation table from a word-aligned parallel corpus. The component generates the translation table according to the process described in the Pharaoh training manual5 . It generates a vector of 5 numeric values for each phrase pair: • phrase translation probability: φ(f¯|¯ e) = count(f¯, e¯) count(f¯, e¯) , φ(¯ e|f¯) = count(¯ e) count(f¯) 2 http://www.phramer.org/ – Java-based open-source phrase based SMT system 3 http://www.isi.edu/licensed-sw/carmel/ 4 http://www"
W06-3122,N03-1017,0,0.0220113,"Missing"
W06-3122,koen-2004-pharaoh,0,0.0363877,"LMs. For 1 During the translation of the first 10 sentences of the devtest2006.de dataset using Phramer and the configuration described in Section 3, the 3-gram LM was queried 27 million times (3 million distinct queries). We developed for the WMT 2006 shared task a system that is trained on a (a) word-aligned bilingual corpus, (b) a large monolingual (English) corpus and (c) an English treebank and it is capable of translating from a source language (German, Spanish and French) into English. Our system embeds Phramer2 (used for minimum error rate training, decoding, decoding tools), Pharaoh (Koehn, 2004) (decoding), Carmel 3 (helper for Pharaoh in n-best generation), Charniak’s parser (Charniak, 2001) (language model) and SRILM4 (n-gram LM construction). 2.1 Translation table construction We developed a component that builds a translation table from a word-aligned parallel corpus. The component generates the translation table according to the process described in the Pharaoh training manual5 . It generates a vector of 5 numeric values for each phrase pair: • phrase translation probability: φ(f¯|¯ e) = count(f¯, e¯) count(f¯, e¯) , φ(¯ e|f¯) = count(¯ e) count(f¯) 2 http://www.phramer.org/ – J"
W06-3122,J93-2004,0,0.0261182,"otheses with different length. The number of n-grams in each of the two cases is presented in Table 2. 6 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp? catalogId=LDC2005T12 1-grams 2-grams 3-grams 4-grams sentence probability model 310 K 45 M 123 M 235 M n-gram hit/miss model 310 K 45 M 283 M 675 M Table 2: Number of n-gram entries in the EGW LM 2.4.2 Charniak parsing We used Charniak’s parser as an additional LM (Charniak, 2001) in reranking. The parser provides one feature for our model – the log-grammarprobability of the sentence. We retrained the parser on lowercased Penn Treebank II (Marcus et al., 1993), to match the lowercased output of the MT decoder. Considering the huge number of hypotheses that needed to be parsed for this task, we set it to parse very fast (using the command-line parameter -T107 ). 2.5 Reranking and voting A λ weights vector trained over the 8 basic features (λ1 ) is used to decode a n-best list. Then, a λ vector trained over all 21 features (λ2 ) is used to rerank the n-best list, potentially generating a new first-best hypothesis. To improve the results, we generated during training a set of distinct λ2 weight vectors (4-10 different weight vectors). Each λ2 picks a"
W06-3122,P03-1021,0,0.00746888,".uk/∼pkoehn/training.tgz 150 Proceedings of the Workshop on Statistical Machine Translation, pages 150–153, c New York City, June 2006. 2006 Association for Computational Linguistics No. of sentences No. of tokens Vocabulary size Distinct grams • lexical weighting (Koehn et al., 2003): lex(f¯|¯ e, a) = n Y i=1 lex(¯ e|f¯, a) = m Y j=1 1 |{j|(i, j) ∈ a}| 1 |{i|(i, j) ∈ a}| X w(fi |ej ) ∀(i,j)∈a X Table 1: English Gigaword LM statistics w(ej |fi ) ∀(i,j)∈a 2.4 • phrase penalty: τ (f¯|¯ e) = e; log(τ (f¯|¯ e)) = 1 2.2 Decoding We used the Pharaoh decoder for both the Minimum Error Rate Training (Och, 2003) and test dataset decoding. Although Phramer provides decoding functionality equivalent to Pharaoh’s, we preferred to use Pharaoh for this task because it is much faster than Phramer – between 2 and 15 times faster, depending on the configuration – and preliminary tests showed that there is no noticeable difference between the output of these two in terms of BLEU (Papineni et al., 2002) score. The log-linear model uses 8 features: one distortion feature, one basic LM feature, 5 features from the translation table and one sentence length feature. 2.3 96.7 M 2.3 B 1.6 M 1B Minimum Error Rate Tra"
W06-3122,P02-1040,0,0.0731699,"w(fi |ej ) ∀(i,j)∈a X Table 1: English Gigaword LM statistics w(ej |fi ) ∀(i,j)∈a 2.4 • phrase penalty: τ (f¯|¯ e) = e; log(τ (f¯|¯ e)) = 1 2.2 Decoding We used the Pharaoh decoder for both the Minimum Error Rate Training (Och, 2003) and test dataset decoding. Although Phramer provides decoding functionality equivalent to Pharaoh’s, we preferred to use Pharaoh for this task because it is much faster than Phramer – between 2 and 15 times faster, depending on the configuration – and preliminary tests showed that there is no noticeable difference between the output of these two in terms of BLEU (Papineni et al., 2002) score. The log-linear model uses 8 features: one distortion feature, one basic LM feature, 5 features from the translation table and one sentence length feature. 2.3 96.7 M 2.3 B 1.6 M 1B Minimum Error Rate Training To determine the best coefficients of the log-linear model (λ) for both the initial stage decoding and the second stage reranking, we used the unsmoothed Minimum Error Rate Training (MERT) component present in the Phramer package. The MERT component is highly efficient; the time required to search a set of 200,000 hypotheses is less than 30 seconds per iteration (search from a pre"
W07-1404,J94-4002,0,0.019262,"icipate VB(e1,x1,x3) & which is closer to the meaning of the English text snippet. For Run #1 (with TARSQI output), we only used the polarity information attached to the identified events and negated the event’s predicate. in IN(e1,x2) & AGT SR(x1,e1))) 2.5 Coreference Resolution In order to cope with the long text pairs, we added in our processing pipeline a dedicated pronominal coreference resolution module which replaced the inter-sentential resolution processing we used until now. The new tool combines Hobbs algorithm (Hobbs, 1978) and the Resolution of Anaphora Procedure (RAP) algorithm (Lappin and Leass, 1994). For the RTE task, it is very important to have tight connections between the predicates of Semantic Relation ISA DERIVATION CAUSE AGENT PERTAIN Axiom Templates n1(x1) -> n2(x1); v1(e1,x1,x2) -> v2(e1,x1,x2) n(x1) -> v(e1,x1,x2) & AGENT SR(x1,e1); n(e1) -> v(e1,x1,x2) v(e1,x1,x2) -> n(x1); v(e1,x1,x2) -> n(e1) v1(e1,x1,x2) -> v2(e2,x2,x3) & CAUSE SR(e1,e2) n1(x1) -> n2(x2) & AGENT SR(x1,x2) a(x1,x2) -> n(x1) Table 2: Semantic Relation - Axiom Template mapping long texts. For example, for QA Dev pair 409, resolving the pronoun he to George H.W. Bush is a step needed to correctly label the pair"
W07-1404,P06-1014,0,0.007419,"to prove the entailment for IE Dev pair 196. We also changed the subset of senses considered when lexical chains are built. Previously, this subset contained the first k (k = 3) senses for each content word. For this year’s challenge, we changed the 24 sense selection mechanism and we used the cluster of WordNet senses to which the fine-grained sense assigned by the Word Sense Disambiguation system corresponds. We used the coarse-grained sense inventory for WordNet 2.1 released for Task #7 in SemEval-20074 . This clustering was created automatically with the aid of a methodology described in (Navigli, 2006). For example, the 10 WordNet senses for the noun bank are mapped into 3 clusters. 3.2 NLP Axioms In addition to the syntactic re-writing rules which break down complex syntactic structures, including complex nominals and coordinating conjunctions, we added a new type of NLP axioms which links a named entity to its set of aliases. For IE Dev pair 35, the link between the Central Intelligence Agency mentioned in T and H’s CIA is very important. We also added a deeper analysis of multi-word human named entities which marks last names (Hawking), first (male/female) names (Stephen), titles (Prime"
W07-1404,P05-3021,0,0.0209511,"d WordNet Knowledge Base (XWN-KB) is the result of our ongoing research which captures and stores the rich world knowledge encoded in WordNet’s glosses into a knowledge base. In XWNKB, the glosses have been transformed into a set of semantic relations using a semantic parser whose output has been verified by human annotators. Fig. 2 displays the semantic relations derived for Nobel laureate’s definition. Our system used this representation for QA Dev pair 579 and QA Test pair 582 2 . 2.2 TARSQI Toolkit The TARSQI project (Temporal Awareness and Reasoning Systems for Question Interpretation)3 (Verhagen et al., 2005) builds a modular system which detects, resolves and normalizes time expressions (both absolute and relative times) - GUTime tagger; marks events and their grammatical features 2 3 Table 6 lists the pairs referenced throughout the paper. http://www.timeml.org/site/tarsqi 22 Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 22–27, c Prague, June 2007. 2007 Association for Computational Linguistics Knowledge Representation Text T Hypothesis H Named Entity Recognition Part−of−Speech Tagging Axioms on demand XWN Lexical NLP Chains Axioms TLF −HLF Syntactic Parsing Coreferen"
W11-0106,P08-2045,0,0.0667426,", all inferences are correct. 6 Comparison with Previous Work There have been abundant proposals to detect semantic relations without taking into account composition of relations. All these approaches, regardless of their particular details, take as their input text and output the relations found in it. In contrast, the framework proposed in this article obtains axioms that take as their input relations found in text and output more relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define"
W11-0106,W05-0620,0,0.458193,"Missing"
W11-0106,J06-1005,1,0.815317,"re have been abundant proposals to detect semantic relations without taking into account composition of relations. All these approaches, regardless of their particular details, take as their input text and output the relations found in it. In contrast, the framework proposed in this article obtains axioms that take as their input relations found in text and output more relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among"
W11-0106,S07-1003,0,0.435452,"uistics for decades. They are unidirectional underlying connections between concepts. For example, the sentence The construction slowed down the traffic encodes a CAUSE and detecting it would help answer the question Why is traffic slower? In Computational Linguistics, there have been several proposals to detect semantic relations. Current approaches focus on a particular set of relations and given a text they output relations. There have been competitions aiming at detecting semantic roles (i.e., relations between a verb and its arguments) (Carreras and M`arquez, 2005), and between nominals (Girju et al., 2007; Hendrickx et al., 2009). In this paper, we propose a model to compose semantic relations to extract previously ignored relations. The model allows us to automatically obtain inference axioms given a set of relations and is not coupled to any particular set. Axioms take as their input semantic relations and yield a new semantic relation as their conclusion. Consider the sentence John went to the shop to buy flowers. Figure 1 shows semantic role annotation with solid arrows. By composing this basic annotation with inference axioms, one can obtain the relations shown with discontinuous arrows:"
W11-0106,J08-2001,0,0.0220263,"Missing"
W11-0106,W04-2609,1,0.767188,"n text and output more relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among others, the literature uses relation elements, deep structure, aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory, and differentiate relations by relation elements. The authors describe a set of 31"
W11-0106,P07-3014,0,0.185972,"relations previously ignored. Generally, efforts to extract semantic relations have concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among others, the literature uses relation elements, deep structure, aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory, and differentiate relations by relation elements. The authors describe a set of 31 relations clu"
W11-0106,J05-1004,0,0.0554931,"N(Rj ) = ∅, break 2. Primitives composition Using the algebra for composing semantic primitives, calculate PRi ◦ PRj 3. Conclusion match Repeat for R3 ∈ R If D OMAIN(R3 ) ∩ D OMAIN(Ri ) 6= ∅ and R ANGE(R3 ) ∩ R ANGE(Rj ) 6= ∅ and consistent(PR3 , PRi ◦ PRj ), then inf erence axioms += Ri (x, y) ◦ Rj (y, z) → R3 (x, z) The method consistent(P1 , P2 ) is a simple procedure that compares the values assigned to each primitive one by one. Two values for the same primitive are compatible unless they have different opposites or either value is ‘×’ (i.e., prohibited). 5 Case Study: PropBank PropBank (Palmer et al., 2005) adds a layer of predicate-argument information, or semantic role labels, on top of the syntactic trees provided by the Penn TreeBank. Along with FrameNet, it is the resource most widely used for semantic role annotation. PropBank uses a series of numeric core roles (ARG 0 - ARG 5) and a set of more general roles, ARGMs (e.g. MTMP, MLOC, MMNR). The interpretation of the numeric roles is determined by a verb-specific framesets, although ARG 0 and ARG 1 usually correspond to the prototypical AGENT and THEME. On the other hand, the meaning of AGRMs generalize across verbs. An example of PropBank"
W11-0106,P08-1117,0,0.0185363,"concentrated on particular sets of relations or a single relation, e.g. CAUSE (Bethard and Martin, 2008; Chang and Choi, 2006) and PART- WHOLE (Girju et al., 2006). Automatic detection of semantic roles has received a lot of attention lately (M`arquez et al., 2008; Carreras and M`arquez, 2005). The SemEval-2007 Task 04 (Girju et al., 2007) and SemEval-2010 Task 08 (Hendrickx et al., 2009) aimed at relations between nominals. There has been work on detecting relations within noun phrases (Moldovan et al., 2004; Nulty, 2007), clauses (Szpakowicz et al., 1995) and syntax-based comma resolution (Srikumar et al., 2008). Previous research has exploited the idea of using semantic primitives to define and classify semantic relations under different names. Among others, the literature uses relation elements, deep structure, aspects and primitives. To the best of our knowledge, the first effort on describing semantic relations 52 using primitives was made by Chaffin and Herrmann (1987). They introduce Relation Element Theory, and differentiate relations by relation elements. The authors describe a set of 31 relations clustered in five groups (CONTRAST, SIMILARS, CLASS INCLUSION, CASE - RELATIONS, PART- WHOLE), a"
W11-0106,W09-2415,0,\N,Missing
W18-3701,P17-1123,0,0.0147692,"therefore adept at gauging comprehension skills. The scope of several QG tasks has been severely restricted to restructuring declarative sentences into specific level questions. For example, consider the given text and the questions that follow. 1 Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 1–10 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 2.1 Related Work (2014)) to automatically learn the transformation rules that enable translation from one language to another. Yin et al. (2015) and Du et al. (2017) argue that similar models can be used to automatically translate narrative sentences into interrogative ones. Previous QG systems Previous research work done in QG has primarily focused on transforming declarations into interrogative sentences, or on using shallow semantic parsers to create factoid questions. Mitkov and Ha (2003) made use of term extraction and shallow parsing to create questions from simple sentences. Heilman and Smith (2010) suggested a system that over-generates questions from a sentence. Firstly, the sentence is simplified by discarding leading conjunctions, sentencelevel"
W18-3701,C16-1107,0,0.0640688,"entences and thereby identifying answer phrases (Becker et al. (2012)); or using semantics-based templates (Lindberg et al. (2013); Mazidi and Nielsen (2014)). A common drawback associated with these systems is that they create factoid questions from single sentences and focus on grammatical and/or semantic correctness, not question difficulty. The generation of complex questions from multiple sentences or paragraphs was explored by Mannem et al. (2010). Discourse connectives such as ‘because’, ‘since’ and ‘as a result’ signal explicit coherence and can be used to generate Why-type questions. Araki et al. (2016) created an event-centric information network where each node represents an event and each edge represents an event-event relation. Using this network, multiple choice questions and a corresponding set of distractor choices are generated. Olney et al. (2012) suggested the use of concept maps to create inter-sentential questions where knowledge in a book chapter is represented as a concept map to generate relevant exam questions. Likewise, Papasalouros et al. (2008) and Stasaski and Hearst (2017) created questions utilizing information-rich ontologies. Of late, several encoder-decoder models ha"
W18-3701,N10-1086,0,0.233287,"guistics 2 2.1 Related Work (2014)) to automatically learn the transformation rules that enable translation from one language to another. Yin et al. (2015) and Du et al. (2017) argue that similar models can be used to automatically translate narrative sentences into interrogative ones. Previous QG systems Previous research work done in QG has primarily focused on transforming declarations into interrogative sentences, or on using shallow semantic parsers to create factoid questions. Mitkov and Ha (2003) made use of term extraction and shallow parsing to create questions from simple sentences. Heilman and Smith (2010) suggested a system that over-generates questions from a sentence. Firstly, the sentence is simplified by discarding leading conjunctions, sentencelevel modifying phrases, and appositives. It is then transformed into a set of candidate questions by carrying out a sequence of well-defined syntactic and lexical transformations. Then, these questions are evaluated and ranked using a classifier to identify the most suitable one. Similar approaches have been suggested over time to generate questions, like using a recursive algorithm to explore parse trees of sentences in a top-down fashion (Curto e"
W18-3701,N12-1092,0,0.0227526,"ns, sentencelevel modifying phrases, and appositives. It is then transformed into a set of candidate questions by carrying out a sequence of well-defined syntactic and lexical transformations. Then, these questions are evaluated and ranked using a classifier to identify the most suitable one. Similar approaches have been suggested over time to generate questions, like using a recursive algorithm to explore parse trees of sentences in a top-down fashion (Curto et al. (2012)), creating fill-in-the-blank type questions by analyzing parse trees of sentences and thereby identifying answer phrases (Becker et al. (2012)); or using semantics-based templates (Lindberg et al. (2013); Mazidi and Nielsen (2014)). A common drawback associated with these systems is that they create factoid questions from single sentences and focus on grammatical and/or semantic correctness, not question difficulty. The generation of complex questions from multiple sentences or paragraphs was explored by Mannem et al. (2010). Discourse connectives such as ‘because’, ‘since’ and ‘as a result’ signal explicit coherence and can be used to generate Why-type questions. Araki et al. (2016) created an event-centric information network wher"
W18-3701,D14-1220,0,0.0788898,"Missing"
W18-3701,D08-1031,0,0.0327605,"e, both the question and answer are derived from text spans belonging to different sentences. Thus the score assigned will be 1. Why was the author’s brother killed by the mafia three years ago? The student should be able to correctly resolve the pronoun ‘my’ to ‘the author’ and know that ‘killed’ is a synonym of ‘murdered’. Thus two semantic concepts, paraphrase detection and entity co-reference resolution, are tested here. Table 4: Examples for metric evaluation Xue (2014); Li et al. (2014)), such models make several simplifying assumptions about the input. Likewise, coreference resolution (Bengtson and Roth (2008); Wiseman et al. (2016)) is also an uphill task in discourse parsing. 4.3 All the metrics use a two-point scale: a score of 1 indicates the question successfully passed the metric, a score of 0 indicates otherwise. • Grammatic correctness of questions: This metric checks whether the question generated is only syntactically correct. We do not take into account the semantics of the question. Evaluation Criteria To evaluate the quality of generated questions, we used a set of criteria that are defined below. We considered and designed metrics that measure both the correctness and difficulty of th"
W18-3701,W13-2114,0,0.0256041,"then transformed into a set of candidate questions by carrying out a sequence of well-defined syntactic and lexical transformations. Then, these questions are evaluated and ranked using a classifier to identify the most suitable one. Similar approaches have been suggested over time to generate questions, like using a recursive algorithm to explore parse trees of sentences in a top-down fashion (Curto et al. (2012)), creating fill-in-the-blank type questions by analyzing parse trees of sentences and thereby identifying answer phrases (Becker et al. (2012)); or using semantics-based templates (Lindberg et al. (2013); Mazidi and Nielsen (2014)). A common drawback associated with these systems is that they create factoid questions from single sentences and focus on grammatical and/or semantic correctness, not question difficulty. The generation of complex questions from multiple sentences or paragraphs was explored by Mannem et al. (2010). Discourse connectives such as ‘because’, ‘since’ and ‘as a result’ signal explicit coherence and can be used to generate Why-type questions. Araki et al. (2016) created an event-centric information network where each node represents an event and each edge represents an e"
W18-3701,P06-4018,0,0.0257033,"the entity it is referencing i.e. ‘the offices of El Especatador’, to obtain the question stem: ‘destroyed a major part of the installations and equipment of the offices of El Especatador’. We use the template for the cause relation for Type 2 to obtain the question: “What destroyed the installations and equipment of the offices of El Especatador?”. Similar examples have also been provided in Table 4. 4 4.1 4.2 Implementation Part-of-Speech tagging and Dependency parsing were performed using Stanford’s Part-of-Speech tagger (Toutanova et al. (2003)) and Dependency Parser (Nivre et al. (2016); Bird (2006)) respectively. We used the powerful linguistics library provided by NodeBox (Bleser et al. (2002)) to convert between verb forms. We have used a heavily annotated corpus and made several amendments ourselves, by performing coreference resolution and paraphrasing. This is due to the inability of modern discourse parsers to perform these tasks with high accuracy. While advances have been made in discourse parsing (Rutherford and Experimental Results Data For the purpose of experimentation, we used the RST-DT corpus (Carlson et al. (2003)) that contains annotated Wall Street Journal articles. Ea"
W18-3701,N03-1033,0,0.0261865,"d to past tense form ‘destroyed’ and the pronoun ‘it’ is replaced by the entity it is referencing i.e. ‘the offices of El Especatador’, to obtain the question stem: ‘destroyed a major part of the installations and equipment of the offices of El Especatador’. We use the template for the cause relation for Type 2 to obtain the question: “What destroyed the installations and equipment of the offices of El Especatador?”. Similar examples have also been provided in Table 4. 4 4.1 4.2 Implementation Part-of-Speech tagging and Dependency parsing were performed using Stanford’s Part-of-Speech tagger (Toutanova et al. (2003)) and Dependency Parser (Nivre et al. (2016); Bird (2006)) respectively. We used the powerful linguistics library provided by NodeBox (Bleser et al. (2002)) to convert between verb forms. We have used a heavily annotated corpus and made several amendments ourselves, by performing coreference resolution and paraphrasing. This is due to the inability of modern discourse parsers to perform these tasks with high accuracy. While advances have been made in discourse parsing (Rutherford and Experimental Results Data For the purpose of experimentation, we used the RST-DT corpus (Carlson et al. (2003))"
W18-3701,P14-2053,0,0.0179071,"set of candidate questions by carrying out a sequence of well-defined syntactic and lexical transformations. Then, these questions are evaluated and ranked using a classifier to identify the most suitable one. Similar approaches have been suggested over time to generate questions, like using a recursive algorithm to explore parse trees of sentences in a top-down fashion (Curto et al. (2012)), creating fill-in-the-blank type questions by analyzing parse trees of sentences and thereby identifying answer phrases (Becker et al. (2012)); or using semantics-based templates (Lindberg et al. (2013); Mazidi and Nielsen (2014)). A common drawback associated with these systems is that they create factoid questions from single sentences and focus on grammatical and/or semantic correctness, not question difficulty. The generation of complex questions from multiple sentences or paragraphs was explored by Mannem et al. (2010). Discourse connectives such as ‘because’, ‘since’ and ‘as a result’ signal explicit coherence and can be used to generate Why-type questions. Araki et al. (2016) created an event-centric information network where each node represents an event and each edge represents an event-event relation. Using"
W18-3701,W03-0203,0,0.117744,"ucational Applications, pages 1–10 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics 2 2.1 Related Work (2014)) to automatically learn the transformation rules that enable translation from one language to another. Yin et al. (2015) and Du et al. (2017) argue that similar models can be used to automatically translate narrative sentences into interrogative ones. Previous QG systems Previous research work done in QG has primarily focused on transforming declarations into interrogative sentences, or on using shallow semantic parsers to create factoid questions. Mitkov and Ha (2003) made use of term extraction and shallow parsing to create questions from simple sentences. Heilman and Smith (2010) suggested a system that over-generates questions from a sentence. Firstly, the sentence is simplified by discarding leading conjunctions, sentencelevel modifying phrases, and appositives. It is then transformed into a set of candidate questions by carrying out a sequence of well-defined syntactic and lexical transformations. Then, these questions are evaluated and ranked using a classifier to identify the most suitable one. Similar approaches have been suggested over time to gen"
W18-3701,N16-1114,0,0.022531,"nswer are derived from text spans belonging to different sentences. Thus the score assigned will be 1. Why was the author’s brother killed by the mafia three years ago? The student should be able to correctly resolve the pronoun ‘my’ to ‘the author’ and know that ‘killed’ is a synonym of ‘murdered’. Thus two semantic concepts, paraphrase detection and entity co-reference resolution, are tested here. Table 4: Examples for metric evaluation Xue (2014); Li et al. (2014)), such models make several simplifying assumptions about the input. Likewise, coreference resolution (Bengtson and Roth (2008); Wiseman et al. (2016)) is also an uphill task in discourse parsing. 4.3 All the metrics use a two-point scale: a score of 1 indicates the question successfully passed the metric, a score of 0 indicates otherwise. • Grammatic correctness of questions: This metric checks whether the question generated is only syntactically correct. We do not take into account the semantics of the question. Evaluation Criteria To evaluate the quality of generated questions, we used a set of criteria that are defined below. We considered and designed metrics that measure both the correctness and difficulty of the question. • Semantic"
W18-3701,W00-1434,0,0.0489187,"Missing"
W18-3701,E14-1068,0,0.0432685,"Missing"
W18-3701,W17-5034,0,0.0229364,"ause’, ‘since’ and ‘as a result’ signal explicit coherence and can be used to generate Why-type questions. Araki et al. (2016) created an event-centric information network where each node represents an event and each edge represents an event-event relation. Using this network, multiple choice questions and a corresponding set of distractor choices are generated. Olney et al. (2012) suggested the use of concept maps to create inter-sentential questions where knowledge in a book chapter is represented as a concept map to generate relevant exam questions. Likewise, Papasalouros et al. (2008) and Stasaski and Hearst (2017) created questions utilizing information-rich ontologies. Of late, several encoder-decoder models have been used in Machine Translation (Cho et al. 2.2 Rhetorical Structure Theory In an attempt to study the functional organization of information in a discourse, a framework called Rhetorical Structure Theory (RST) was proposed by Thompson and Mann (1987). The framework describes how short texts written in English are structured by defining a set of coherence relations that can exist between text spans. Typically, relations in RST are characterized by three parameters: the nucleus, the satellite"
W18-3803,S12-1051,0,0.243203,"ve use of linguistic features and resources. In this paper, we present an end-to-end attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive, feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against the SemEval 2012 benchmark, we make an assessment on the limitations and strengths of neural net systems as opposed to rule-based systems on STS. 1 Introduction Semantic Textual Similarity (STS) is the task of determining the resemblance of the meanings between two sentences (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016; Cer et al., 2017). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval"
W18-3803,S13-1004,0,0.0148015,"features and resources. In this paper, we present an end-to-end attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive, feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against the SemEval 2012 benchmark, we make an assessment on the limitations and strengths of neural net systems as opposed to rule-based systems on STS. 1 Introduction Semantic Textual Similarity (STS) is the task of determining the resemblance of the meanings between two sentences (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016; Cer et al., 2017). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot"
W18-3803,S14-2010,0,0.0142879,"s. In this paper, we present an end-to-end attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive, feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against the SemEval 2012 benchmark, we make an assessment on the limitations and strengths of neural net systems as opposed to rule-based systems on STS. 1 Introduction Semantic Textual Similarity (STS) is the task of determining the resemblance of the meanings between two sentences (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016; Cer et al., 2017). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual"
W18-3803,S15-2045,0,0.0167899,"present an end-to-end attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive, feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against the SemEval 2012 benchmark, we make an assessment on the limitations and strengths of neural net systems as opposed to rule-based systems on STS. 1 Introduction Semantic Textual Similarity (STS) is the task of determining the resemblance of the meanings between two sentences (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016; Cer et al., 2017). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et"
W18-3803,S16-1081,0,0.0204073,"attention-based Bi-LSTM neural network system that solely takes word-level features, without expensive, feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against the SemEval 2012 benchmark, we make an assessment on the limitations and strengths of neural net systems as opposed to rule-based systems on STS. 1 Introduction Semantic Textual Similarity (STS) is the task of determining the resemblance of the meanings between two sentences (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016; Cer et al., 2017). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et al., 2012), the top"
W18-3803,S12-1094,0,0.344459,"ale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et al., 2012), the top three performers (Šarić et al., 2012; B r et al., 2012; Banea et al., 2012) all derived knowledge from WordNet, Wikipedia and other large corpora. In particular, Banea et al. built the models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B r et al. used Wiktionary, which contains over 3 million entries; and Šarić et al. used The New York Times Annotated Corpus that contains over 1.8 million news articles. Blanco and Moldovan (2013) proposed a model with semantic representation of sentences, which was considered to use the smallest external resources and features in 2015. However, their model still required WordNet with approximately 120,00"
W18-3803,S12-1059,0,0.448376,"s below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et al., 2012), the top three performers (Šarić et al., 2012; B r et al., 2012; Banea et al., 2012) all derived knowledge from WordNet, Wikipedia and other large corpora. In particular, Banea et al. built the models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B r et al. used Wiktionary, which contains over 3 million entries; and Šarić et al. used The New York Times Annotated Corpus that contains over 1.8 million news articles. Blanco and Moldovan (2013) proposed a model with semantic representation of sentences, which was considered to use the smallest external resources and features in 2015. However, their model still required WordNet with"
W18-3803,D13-1123,1,0.838501,"e of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et al., 2012), the top three performers (Šarić et al., 2012; B r et al., 2012; Banea et al., 2012) all derived knowledge from WordNet, Wikipedia and other large corpora. In particular, Banea et al. built the models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B r et al. used Wiktionary, which contains over 3 million entries; and Šarić et al. used The New York Times Annotated Corpus that contains over 1.8 million news articles. Blanco and Moldovan (2013) proposed a model with semantic representation of sentences, which was considered to use the smallest external resources and features in 2015. However, their model still required WordNet with approximately 120,000 synsets and a semantic parser. Complex neural network architectures are being increasingly used for learning to compute the semantic resemblances among natural language texts. To this date, there are two end-to-end neural network models proposed for STS tasks (Shao, 2017; Prijatelj et al., 2017), and both of them followed a standard sentence pair modeling neural network architecture"
W18-3803,S17-2001,0,0.011506,"STM neural network system that solely takes word-level features, without expensive, feature engineering work or the usage of external resources. By comparing its performance with traditional rule-based systems against the SemEval 2012 benchmark, we make an assessment on the limitations and strengths of neural net systems as opposed to rule-based systems on STS. 1 Introduction Semantic Textual Similarity (STS) is the task of determining the resemblance of the meanings between two sentences (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016; Cer et al., 2017). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et al., 2012), the top three performers (Š"
W18-3803,D14-1080,0,0.0310806,"gs by adding a true/false (1/0) flag to them if the corresponding word appears in both sentences. Lastly, we unified the length of the inputs by padding the sentences. Figure 1: The structure of attention-based Bi-LSTM network for Semantic Textual Similarity. 3.2 Attention-based LSTM Since sentences are sequences of words, and the order of the words matters, it is natural to use LSTMs (Hochreiter and Schmidhuber, 1997) to encode sentences into vectors. However, sometimes the backward sequence contains useful information as well, especially for long and unstructured sentences. Because of this, Irsoy and Cardie (2014) proposed Deep Bidirectional RNNs that can make predictions based on future words by having the RNN model read through the sentence backwards. In this section, we will first introduce a regular LSTM network and then extend it into a Bi-LSTM. At the end of this section, we will apply attention mechanisms to improve the performance of the system. The traditional LSTM unit is defined by 5 components: an input gate, a forget gate, an output gate, a new memory generation cell and a final memory cell. The input gate is to decide if the input xt is worth being preserved based on the input word xt and"
W18-3803,W17-7556,0,0.0698123,"used The New York Times Annotated Corpus that contains over 1.8 million news articles. Blanco and Moldovan (2013) proposed a model with semantic representation of sentences, which was considered to use the smallest external resources and features in 2015. However, their model still required WordNet with approximately 120,000 synsets and a semantic parser. Complex neural network architectures are being increasingly used for learning to compute the semantic resemblances among natural language texts. To this date, there are two end-to-end neural network models proposed for STS tasks (Shao, 2017; Prijatelj et al., 2017), and both of them followed a standard sentence pair modeling neural network architecture that contains three components: a word embedding This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 12 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 12–17 Santa Fe, New Mexico, USA, August 20, 2018. component that transforms words into word vectors, a sentence embedding component that takes word vectors as input and encodes the sentence into a single vector th"
W18-3803,S12-1060,0,0.429995,"). For the sentence pairs below, on a scale from 0 to 5, (1) is very similar [5.0], (2) is somewhat similar [3.0] and (3) is not similar [0.2]: 1. Someone is removing the scales from the fish. A person is descaling a fish. 2. A woman is chopping an herb. A man is finely chopping a green substance. 3. The woman is smoking. The man is walking. In STS tasks, the performance of traditional models relies highly on the usage of linguistic resources and hand-crafted features. For example, in SemEval 2012 Task 06: A Pilot on Semantic Textual Similarity (Agirre et al., 2012), the top three performers (Šarić et al., 2012; B r et al., 2012; Banea et al., 2012) all derived knowledge from WordNet, Wikipedia and other large corpora. In particular, Banea et al. built the models from 6 million Wikipedia articles and more than 9.5 million hyperlinks; B r et al. used Wiktionary, which contains over 3 million entries; and Šarić et al. used The New York Times Annotated Corpus that contains over 1.8 million news articles. Blanco and Moldovan (2013) proposed a model with semantic representation of sentences, which was considered to use the smallest external resources and features in 2015. However, their model still r"
W18-3803,S17-2016,0,0.0608136,"arić et al. used The New York Times Annotated Corpus that contains over 1.8 million news articles. Blanco and Moldovan (2013) proposed a model with semantic representation of sentences, which was considered to use the smallest external resources and features in 2015. However, their model still required WordNet with approximately 120,000 synsets and a semantic parser. Complex neural network architectures are being increasingly used for learning to compute the semantic resemblances among natural language texts. To this date, there are two end-to-end neural network models proposed for STS tasks (Shao, 2017; Prijatelj et al., 2017), and both of them followed a standard sentence pair modeling neural network architecture that contains three components: a word embedding This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 12 Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing, pages 12–17 Santa Fe, New Mexico, USA, August 20, 2018. component that transforms words into word vectors, a sentence embedding component that takes word vectors as input and encodes the sentenc"
W18-3803,N16-1174,0,0.0116618,"ings of the original sentence, and a comparator component that evaluates the similarity between sentence vectors and generates a similarity score. In this paper, we modified and improved the modals proposed by Prijatelj et al. (2017) and Shao (2017), and proposed a Bi-LSTM neural network model as the representative of the neural net approach and evaluated it on the SemEval 2012 dataset. In the experimental section, we compared our system with the top three performers in SemEval 2012 using traditional rule-based models. Because neither Shao nor Prijatelj et al. considered attention mechanisms (Yang et al., 2016) in their systems, we specifically applied attention mechanisms to improve the performance of our system. The goal of the paper is to illustrate that with well-designed neural network models, we can achieve competitive results (compared to traditional rule-based models) without expensive feature engineering work and external resources. We also make an assessment on the limitations of neural net systems as opposed to rule-based systems on STS. 2 Related Work Determining textual similarity is relatively new as a stand-alone task since SemEval-2012, but it is often a component of NLP applications"
W98-0703,H92-1046,0,0.0314179,"Missing"
W98-0703,H93-1061,0,0.231041,"Missing"
W98-0703,P96-1006,0,0.059827,"Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing. Its solution impacts other tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods"
W98-0703,W97-0209,0,0.0851351,"as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. To our knowledge, none of the statistical methods disambigu"
W98-0703,W97-0213,0,0.16071,"Missing"
W98-0703,P97-1007,0,0.0214995,"Missing"
W98-0703,P95-1026,0,0.12922,"ther tasks such as discourse, reference resolution, coherence, inference and others. WSD methods can be broadly classified into three types: 1. WSD that make use of the information provided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. To our knowledge, none of the statistical met"
W98-0703,P94-1020,0,0.0277235,"rovided by machine readable dictionaries (Cowie et a1.1992), (Miller et a1.1994), (Agirre and Rigau, 1995), (Li et a1.1995), (McRoy, 1992); 2. WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale, Church et al., 1992), (Ng and Lee, 1996}; 3. WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau, Asterias et al., 1997). Statistical methods produce high accuracy results for small number of preselected words. A lack of widely available semantically tagged corpora almost excludes supervised learning methods. On the other hand, the disambiguation using unsupervised methods has the disadvantage that the senses are not well defined. To our knowledge, none of the statistical methods disambiguate adjectives or adverbs so far. One approach to WSD is to determine the conceptual distance between words, that is to measure the semantic closeness of the words within a sem"
W98-0703,W95-0105,0,0.0417132,"Missing"
W98-0703,W95-0100,0,0.232285,"Missing"
W98-0703,C90-2067,0,0.0943322,"Missing"
W98-0703,C92-2070,0,0.268935,"Missing"
W98-0703,H94-1046,0,\N,Missing
