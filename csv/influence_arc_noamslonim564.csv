2020.acl-main.371,D19-1290,0,0.407054,"qually contributed to this work. 2017; Stab et al., 2018a,b; Ein-Dor et al., 2020). Policy makers in governments or businesses may also conduct surveys to collect from large audiences arguments supporting or contesting some proposal. Each of the above methods may result in hundreds or thousands of arguments per topic, making it impossible for the decision maker to read and digest such large amounts of information. Several works aimed to alleviate this problem by clustering together related arguments, based on different notions of relatedness, such as similarity (Reimers et al., 2019), frames (Ajjour et al., 2019), and argument facets (Misra et al., 2016). These works, however, did not attempt to create a concise textual summary from the resulting clusters. In this work we propose to summarize the arguments supporting each side of the debate by mapping them to a short list of talking points, termed key points. The salience of each key point can be represented by the number of its matching arguments. An example for such summary is shown in Table 1. Key points may be viewed as highlevel arguments. They should be general enough to match a significant portion of the arguments, yet informative enough to mak"
2020.acl-main.371,E17-1024,1,0.821243,"argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al., 2020). Stance classification of extracted arguments can be performed as a separate step (Bar-Haim et al., 2017) or jointly with argument detection, as a three-way classification (pro argument/con argument/none), as done by Stab et al. (2018b). 4030 2.2 Argument Clustering and Summarization Several works have focused on identifying pairs of similar arguments, or clustering similar arguments together. Ajjour et al. (2019) addressed the task of splitting a set of arguments into a set of nonoverlapping frames such as Economics, Environment and Politics. Reimers et al. (2019) classified argument pairs as similar/dissimilar. Misra et al. (2016) aimed to detect argument pairs that are assumed to share the sam"
2020.acl-main.371,W14-2107,0,0.0621531,"Missing"
2020.acl-main.371,D15-1075,0,0.0275066,"key point pairs. We added a linear fully connected layer of size 1 followed by a sigmoid layer to the special [CLS] token in the BERT model, and trained it for three epochs with a learning rate of 2e-5 and a binary cross entropy loss. NLI Transfer Learning. We also experimented with transfer learning from NLI to our task of argument-to-key point match classification. This was motivated by the similarity between these tasks (as discussed in Section 2.2), as well as the availability of large-scale NLI labeled datasets. We con4034 sidered the Stanford (SNLI) and the Multi-Genre (MNLI) datasets (Bowman et al., 2015; Williams et al., 2018), each comprising hundreds of thousands of labeled premise-hypothesis pairs. Pairs labeled as E NTAILMENT were considered positive instances, while the rest of the pairs, labeled as N EUTRAL or C ONTRADICTION were considered negative. We trained BERT-base and BERT-large models on each of these datasets, following the procedure described above. 4.1.2 Match Classification In the match classification step we select the matching key points for each argument, based on their respective matching scores. The classification can be done locally, treating each pair individually, o"
2020.acl-main.371,D18-2029,0,0.0129842,".82 (“almost perfect agreement”), validating the high quality of the dataset. 4 weighted word vectors and use their cosine similarity as the match score. • Word Embedding. We examined averaged word embeddings using GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019). GloVe is a context independent model that computes a single embedding for each word. BERT is a contextualized embedding model that takes the entire sentence into account. We also experimented with other embedding methods that under-performed BERT and thus their results are not reported here: Universal Sentence Encoder (Cer et al., 2018) and InferSent (Conneau et al., 2017). Again, we use cosine similarity to compute the match score. Experiments 4.1 Experimental Setup We perform the task of matching arguments to key points in two steps. In the Match Scoring step (Section 4.1.1), we generate a score for each argument and key point. Then, in the Match Classification step (Section 4.1.2), we use these scores to classify the pairs as matching or non-matching. We perform 4-fold cross-validation over the ArgKP dataset. Each fold comprises 7 test topics, 17 train topics and 4 development topics. 4.1.1 Match Scoring We experimented w"
2020.acl-main.371,D17-1070,0,0.0220874,"validating the high quality of the dataset. 4 weighted word vectors and use their cosine similarity as the match score. • Word Embedding. We examined averaged word embeddings using GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019). GloVe is a context independent model that computes a single embedding for each word. BERT is a contextualized embedding model that takes the entire sentence into account. We also experimented with other embedding methods that under-performed BERT and thus their results are not reported here: Universal Sentence Encoder (Cer et al., 2018) and InferSent (Conneau et al., 2017). Again, we use cosine similarity to compute the match score. Experiments 4.1 Experimental Setup We perform the task of matching arguments to key points in two steps. In the Match Scoring step (Section 4.1.1), we generate a score for each argument and key point. Then, in the Match Classification step (Section 4.1.2), we use these scores to classify the pairs as matching or non-matching. We perform 4-fold cross-validation over the ArgKP dataset. Each fold comprises 7 test topics, 17 train topics and 4 development topics. 4.1.1 Match Scoring We experimented with both unsupervised and supervised"
2020.acl-main.371,N19-1423,0,0.0307107,"cross-validation over the ArgKP dataset. Each fold comprises 7 test topics, 17 train topics and 4 development topics. 4.1.1 Match Scoring We experimented with both unsupervised and supervised methods for computing a match score for a given (argument, key point) pair. We also explored transfer learning from the related task of natural language inference (NLI). Unsupervised Methods • Tf-Idf. In order to assess the role of lexical overlap in the matching task, we represent each argument and key point as tf-idf Supervised Methods. We fine tuned the BERTbase-uncased and BERT-large-uncased models (Devlin et al., 2019) to predict matches between argument and key point pairs. We added a linear fully connected layer of size 1 followed by a sigmoid layer to the special [CLS] token in the BERT model, and trained it for three epochs with a learning rate of 2e-5 and a binary cross entropy loss. NLI Transfer Learning. We also experimented with transfer learning from NLI to our task of argument-to-key point match classification. This was motivated by the similarity between these tasks (as discussed in Section 2.2), as well as the availability of large-scale NLI labeled datasets. We con4034 sidered the Stanford (SNL"
2020.acl-main.371,W16-2816,0,0.183359,". (2019) addressed the task of splitting a set of arguments into a set of nonoverlapping frames such as Economics, Environment and Politics. Reimers et al. (2019) classified argument pairs as similar/dissimilar. Misra et al. (2016) aimed to detect argument pairs that are assumed to share the same argument facet, which is similar to our notion of key points. However, they did not attempt to explicitly identify or generate these facets, which remained implicit, but rather focused on detecting similarity between argument pairs. In contrast to these works, we directly map arguments to key points. Egan et al. (2016) proposed to summarize argumentative discussions through the extraction of salient “points”, where each point is a verb and its syntactic arguments. Applying their unsupervised method to online political debates showed significant improvement over a baseline extractive summarizer, according to human evaluation. While the current work also aims to summarize argumentative content via concise points, our goal is not to extract these points but to accurately map arguments to given points. Our main challenge is to identify the various ways in which the meaning of a point is conveyed in different ar"
2020.acl-main.371,D14-1083,0,0.223504,"ing arguments. In addition, it can be used for novelty detection - identifying unexpected arguments that do not match presupposed key points. We develop the ArgKP dataset for the argumentto-keypoint mapping task, comprising about 24,000 (argument, key point) pairs labeled as matching/non matching.1 To the best of our knowledge, this is the first dataset for this task. As discussed in the next section in more detail, our dataset is also much larger and far more comprehensive than datasets developed for related tasks such as mapping posts or comments in online debates to reasons or arguˇ ments (Hasan and Ng, 2014; Boltuˇzi´c and Snajder, 2014). We report empirical results for an extensive set of supervised and unsupervised configurations, achieving promising results. The main contributions of this work are: 1. We demonstrate, through extensive data annotation and analysis over a variety of topics, the feasibility and effectiveness of summarizing a large set of arguments collected from a large audience by mapping them to a small set of key points. 1 The dataset is available at https://www.research. ibm.com/haifa/dept/vst/debating_data. shtml 2. We develop the first large-scale dataset for the task of a"
2020.acl-main.371,C14-1141,1,0.785334,"ection of pro and con arguments for a given topic. As previously mentioned, these arguments may be collected from a large audience by conducting a survey, or mined automatically from text. Some of the previous work on argument mining focused on specific domains such as legal documents (Moens et al., 2007; Wyner et al., 2010), student essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014). Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al., 2020). Stance classification of extracted arguments can be performed as a separate step (Bar-Haim et al., 2017) or jointly with argument detection, as a three-way classification (pro argument/con argument/none), as done by Stab"
2020.acl-main.371,C18-1176,1,0.924057,"Missing"
2020.acl-main.371,W17-5110,1,0.952788,"the pros and cons of the proposal under consideration. We may then summarize the collected information as a short list of the main arguments for each side. Lastly, we aim to weigh the pro and con arguments against each other to make the final decision. Where can we find relevant arguments for a given topic? In recent years, significant progress was made in the field of argument mining, automatic identification and extraction of argumentative structures in text (Lawrence and Reed, 2020). Specifically, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., ∗ All authors equally contributed to this work. 2017; Stab et al., 2018a,b; Ein-Dor et al., 2020). Policy makers in governments or businesses may also conduct surveys to collect from large audiences arguments supporting or contesting some proposal. Each of the above methods may result in hundreds or thousands of arguments per topic, making it impossible for the decision maker to read and digest such large amounts of information. Several works aimed to alleviate this problem by clustering together related arguments, based on different notions of relatedness, such as si"
2020.acl-main.371,W16-3636,0,0.170046,"et al., 2018a,b; Ein-Dor et al., 2020). Policy makers in governments or businesses may also conduct surveys to collect from large audiences arguments supporting or contesting some proposal. Each of the above methods may result in hundreds or thousands of arguments per topic, making it impossible for the decision maker to read and digest such large amounts of information. Several works aimed to alleviate this problem by clustering together related arguments, based on different notions of relatedness, such as similarity (Reimers et al., 2019), frames (Ajjour et al., 2019), and argument facets (Misra et al., 2016). These works, however, did not attempt to create a concise textual summary from the resulting clusters. In this work we propose to summarize the arguments supporting each side of the debate by mapping them to a short list of talking points, termed key points. The salience of each key point can be represented by the number of its matching arguments. An example for such summary is shown in Table 1. Key points may be viewed as highlevel arguments. They should be general enough to match a significant portion of the arguments, yet informative enough to make a useful summary. The proposed method ra"
2020.acl-main.371,W14-2105,0,0.0294832,"aluation and analysis of a variety of classification methods for the above task. 2 2.1 Related Work Argument Mining The starting point for the current work is a collection of pro and con arguments for a given topic. As previously mentioned, these arguments may be collected from a large audience by conducting a survey, or mined automatically from text. Some of the previous work on argument mining focused on specific domains such as legal documents (Moens et al., 2007; Wyner et al., 2010), student essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014). Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al., 2020). Stance classification of extracted arguments can be performe"
2020.acl-main.371,D14-1162,0,0.082657,"Missing"
2020.acl-main.371,W17-5106,0,0.0805057,"tudent essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014). Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al., 2020). Stance classification of extracted arguments can be performed as a separate step (Bar-Haim et al., 2017) or jointly with argument detection, as a three-way classification (pro argument/con argument/none), as done by Stab et al. (2018b). 4030 2.2 Argument Clustering and Summarization Several works have focused on identifying pairs of similar arguments, or clustering similar arguments together. Ajjour et al. (2019) addressed the task of splitting a set of arguments into a set of nonoverlapping frames such as Economics, Environment and Politics. Reime"
2020.acl-main.371,N16-1164,0,0.0201677,"task of argument-to-key point mapping. 3. We perform empirical evaluation and analysis of a variety of classification methods for the above task. 2 2.1 Related Work Argument Mining The starting point for the current work is a collection of pro and con arguments for a given topic. As previously mentioned, these arguments may be collected from a large audience by conducting a survey, or mined automatically from text. Some of the previous work on argument mining focused on specific domains such as legal documents (Moens et al., 2007; Wyner et al., 2010), student essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014). Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al.,"
2020.acl-main.371,P19-1054,0,0.3445,"chsmuth et al., ∗ All authors equally contributed to this work. 2017; Stab et al., 2018a,b; Ein-Dor et al., 2020). Policy makers in governments or businesses may also conduct surveys to collect from large audiences arguments supporting or contesting some proposal. Each of the above methods may result in hundreds or thousands of arguments per topic, making it impossible for the decision maker to read and digest such large amounts of information. Several works aimed to alleviate this problem by clustering together related arguments, based on different notions of relatedness, such as similarity (Reimers et al., 2019), frames (Ajjour et al., 2019), and argument facets (Misra et al., 2016). These works, however, did not attempt to create a concise textual summary from the resulting clusters. In this work we propose to summarize the arguments supporting each side of the debate by mapping them to a short list of talking points, termed key points. The salience of each key point can be represented by the number of its matching arguments. An example for such summary is shown in Table 1. Key points may be viewed as highlevel arguments. They should be general enough to match a significant portion of the arguments,"
2020.acl-main.371,D15-1050,1,0.853028,"ge audience by conducting a survey, or mined automatically from text. Some of the previous work on argument mining focused on specific domains such as legal documents (Moens et al., 2007; Wyner et al., 2010), student essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014). Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2018a,b; EinDor et al., 2020). Stance classification of extracted arguments can be performed as a separate step (Bar-Haim et al., 2017) or jointly with argument detection, as a three-way classification (pro argument/con argument/none), as done by Stab et al. (2018b). 4030 2.2 Argument Clustering and Summarization Several works have focused on identifying pairs of similar"
2020.acl-main.371,N18-1101,0,0.0182151,"added a linear fully connected layer of size 1 followed by a sigmoid layer to the special [CLS] token in the BERT model, and trained it for three epochs with a learning rate of 2e-5 and a binary cross entropy loss. NLI Transfer Learning. We also experimented with transfer learning from NLI to our task of argument-to-key point match classification. This was motivated by the similarity between these tasks (as discussed in Section 2.2), as well as the availability of large-scale NLI labeled datasets. We con4034 sidered the Stanford (SNLI) and the Multi-Genre (MNLI) datasets (Bowman et al., 2015; Williams et al., 2018), each comprising hundreds of thousands of labeled premise-hypothesis pairs. Pairs labeled as E NTAILMENT were considered positive instances, while the rest of the pairs, labeled as N EUTRAL or C ONTRADICTION were considered negative. We trained BERT-base and BERT-large models on each of these datasets, following the procedure described above. 4.1.2 Match Classification In the match classification step we select the matching key points for each argument, based on their respective matching scores. The classification can be done locally, treating each pair individually, or globally, by examining"
2020.acl-main.371,N18-5005,0,0.17408,"ation as a short list of the main arguments for each side. Lastly, we aim to weigh the pro and con arguments against each other to make the final decision. Where can we find relevant arguments for a given topic? In recent years, significant progress was made in the field of argument mining, automatic identification and extraction of argumentative structures in text (Lawrence and Reed, 2020). Specifically, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., ∗ All authors equally contributed to this work. 2017; Stab et al., 2018a,b; Ein-Dor et al., 2020). Policy makers in governments or businesses may also conduct surveys to collect from large audiences arguments supporting or contesting some proposal. Each of the above methods may result in hundreds or thousands of arguments per topic, making it impossible for the decision maker to read and digest such large amounts of information. Several works aimed to alleviate this problem by clustering together related arguments, based on different notions of relatedness, such as similarity (Reimers et al., 2019), frames (Ajjour et al., 2019), and argument facets (Misra et al.,"
2020.acl-main.371,J17-3005,0,0.0304092,"rge-scale dataset for the task of argument-to-key point mapping. 3. We perform empirical evaluation and analysis of a variety of classification methods for the above task. 2 2.1 Related Work Argument Mining The starting point for the current work is a collection of pro and con arguments for a given topic. As previously mentioned, these arguments may be collected from a large audience by conducting a survey, or mined automatically from text. Some of the previous work on argument mining focused on specific domains such as legal documents (Moens et al., 2007; Wyner et al., 2010), student essays (Stab and Gurevych, 2017; Persing and Ng, 2016), and user comments on proposed regulations (Park and Cardie, 2014). Mining arguments and argument components for a given topic (also known as context) has been a prominent line of research in argument mining. Levy et al. (2014) introduced the task of context-dependent claim detection in a collection of Wikipedia articles, and Rinott et al. (2015) did the same for context-dependent evidence detection. More recently, several works focused on topic-related argument mining from the Web or other massive corpora (Levy et al., 2017, 2018; Wachsmuth et al., 2017; Stab et al., 2"
2020.acl-main.371,D18-1402,0,0.0284775,"Missing"
2020.acl-main.633,P19-1255,0,0.0154524,"018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical framework for engagement (Toulmin, 2003; Govier, 1991; Dung, 1995; Damer, 2009; Walton, 2009). The rise of deliberate disinformation, such as fake news, highlights the erosion in the credibility of consumed content (Lazer et al., 2018), and situations where one is exposed only to opinions that agree with their own, as captured by the notion of echo chambers, are becoming more prevalent (Garimella et al., 2018; Duseja and Jhamtani, 2019). The task proposed in this work seems timely in this context. 3 Data We now detail the process of collecting the speeches, the"
2020.acl-main.633,P18-1021,0,0.0895955,"in Mirkin et al. (2018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical framework for engagement (Toulmin, 2003; Govier, 1991; Dung, 1995; Damer, 2009; Walton, 2009). The rise of deliberate disinformation, such as fake news, highlights the erosion in the credibility of consumed content (Lazer et al., 2018), and situations where one is exposed only to opinions that agree with their own, as captured by the notion of echo chambers, are becoming more prevalent (Garimella et al., 2018; Duseja and Jhamtani, 2019). The task proposed in this work seems timely in this context. 3 Data We now detail the process of collectin"
2020.acl-main.633,D19-5905,1,0.900671,"- a useful tool for these worthy goals, but not a complete solution. To pursue the aforementioned task, one needs a corresponding benchmark data, that would serve for training and evaluating the performance of an automatic system. For example, one may start with an opinion article, find a set of opinion articles on the same topic with an opposing stance, and aim to detect those that most effectively counter the arguments raised in the opinion article we started with. This path represents a formidable challenge; for example, reliable annotation of long texts is notoriously difficult to obtain (Lavee et al., 2019a), to name just one reason out of many. To overcome this issue, here we focus on a unique debate setup, in which the goal of one expert debater is to generate a coherent speech that counters the arguments raised in another speech by a fellow debater. Specifically, as part of Project Debater3 , we collected more than 3,600 debate speeches, each around four minutes long, recorded by professional debaters, on a wide variety of controversial topics, posed as debate motions (e.g. we should ban gambling). With this paper, we make this data available to the community at large. Each motion has a set"
2020.acl-main.633,W19-4507,1,0.891349,"Missing"
2020.acl-main.633,L18-1037,1,0.846561,"Missing"
2020.acl-main.633,D18-1078,1,0.873612,"Missing"
2020.acl-main.633,D19-1561,1,0.811988,"ion. Other than the different setup, our task also handles a more complex premise – speeches which are substantially longer than any single argumentative unit, and include multiple such units. An alternative to our approach is breaking the problem into three stages: (1) identifying specific arguments made in each debate speech; (2) establishing counterargument relations between such arguments found in different speeches; (3) choosing the best response speech based on these argument-level relations. The first subproblem has been recently explored in Mirkin et al. (2018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical"
2020.acl-main.633,P15-1012,0,0.0249489,"n different speeches; (3) choosing the best response speech based on these argument-level relations. The first subproblem has been recently explored in Mirkin et al. (2018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical framework for engagement (Toulmin, 2003; Govier, 1991; Dung, 1995; Damer, 2009; Walton, 2009). The rise of deliberate disinformation, such as fake news, highlights the erosion in the credibility of consumed content (Lazer et al., 2018), and situations where one is exposed only to opinions that agree with their own, as captured by the notion of echo chambers, are becoming more prevalent (Garimella et"
2020.acl-main.633,P18-1023,0,0.433364,"-framed debate setup, in which one should identify the response speech(es) that rebuts a given supporting speech. (3) Sharing a large collection of more than 3,600 recorded debate speeches, that allow to train and evaluate automatic methods in our debate-setup task. (4) Providing empirical results for a variety of contemporary NLP models in this task. (5) Establishing the performance of humans in this task, conveying that expert humans currently outperform automatic methods. 2 Related Work Most similar to our work is the task of retrieving the best counter argument to a single given argument (Wachsmuth et al., 2018), also within the debate domain. However, in that setting counterarguments may discuss different motions, or have the same stance towards one motion. In our setting, identifying speeches discussing the same motion can be done using existing NLP methods, and being of opposing stances may be explored with various sentiment analysis techniques. Our focus is on identifying the response to a supporting speech within a set of opposing speeches, all discussing the same motion. Other than the different setup, our task also handles a more complex premise – speeches which are substantially longer than a"
2020.acl-main.633,walker-etal-2012-corpus,0,0.0408159,"se speech based on these argument-level relations. The first subproblem has been recently explored in Mirkin et al. (2018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical framework for engagement (Toulmin, 2003; Govier, 1991; Dung, 1995; Damer, 2009; Walton, 2009). The rise of deliberate disinformation, such as fake news, highlights the erosion in the credibility of consumed content (Lazer et al., 2018), and situations where one is exposed only to opinions that agree with their own, as captured by the notion of echo chambers, are becoming more prevalent (Garimella et al., 2018; Duseja and Jhamtani, 2019). The task"
2020.acl-main.633,W18-5446,0,0.0827497,"Missing"
2020.acl-main.633,W15-0513,0,0.224779,"each debate speech; (2) establishing counterargument relations between such arguments found in different speeches; (3) choosing the best response speech based on these argument-level relations. The first subproblem has been recently explored in Mirkin et al. (2018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical framework for engagement (Toulmin, 2003; Govier, 1991; Dung, 1995; Damer, 2009; Walton, 2009). The rise of deliberate disinformation, such as fake news, highlights the erosion in the credibility of consumed content (Lazer et al., 2018), and situations where one is exposed only to opinions that agree with their o"
2020.acl-main.633,W15-4625,0,0.0267568,"g specific arguments made in each debate speech; (2) establishing counterargument relations between such arguments found in different speeches; (3) choosing the best response speech based on these argument-level relations. The first subproblem has been recently explored in Mirkin et al. (2018b); Lavee et al. (2019b); Orbach et al. (2019). The second is related to a major research area within computational argumentation 7074 (see recent surveys by Cabrio and Villata (2018); Lawrence and Reed (2019)). Such research includes detecting attack relations between arguments (Cabrio and Villata, 2012; Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Cocarascu and Toni, 2017; Wachsmuth et al., 2018), modeling them (Sridhar et al., 2015), depicting these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), generating counter-arguments (Hua and Wang, 2018; Hua et al., 2019), and establishing a theoretical framework for engagement (Toulmin, 2003; Govier, 1991; Dung, 1995; Damer, 2009; Walton, 2009). The rise of deliberate disinformation, such as fake news, highlights the erosion in the credibility of consumed content (Lazer et al., 2018), and situations where one is exposed only to opinio"
2020.acl-main.633,P05-1018,0,\N,Missing
2020.acl-main.633,P12-2041,0,\N,Missing
2020.acl-main.633,D17-1144,0,\N,Missing
2020.emnlp-main.3,2020.acl-main.371,1,0.542168,"he following selection policies defined by Bar-Haim et al. A selection policy defines how to match an argument to one or more key points, based on the classifier’s match score for each key point (kp), and a given threshold t: Matching Comments to Key Points The goal of key point analysis is to extract key points and to match comments to these key points. As mentioned in the previous section (and will be further detailed in the next section), our key point selection algorithm is also based on matching comments to key points, making it a critical component in our system. We build on the work of Bar-Haim et al. (2020), who developed a large-scale labeled dataset for the task of matching arguments to key points. The dataset, termed ArgKP, contains about 24K (argument, key point) pairs, for 28 controversial topics. Each of the pairs is labeled as matching/nonmatching. Given a set of key points for a topic, an argument could be matched to one or more key points, or to none of them. The arguments in this dataset are a subset of a larger dataset, the IBMArgQ-Rank-30kArgs dataset, which contains 71 topics, with stance and argument quality annotations for each argument (Gretz et al., 2020). Bar-Haim et al. only e"
2020.emnlp-main.3,W14-2107,0,0.0748753,"Missing"
2020.emnlp-main.3,N13-1136,0,0.0819045,"Missing"
2020.emnlp-main.3,2021.ccl-1.108,0,0.135003,"Missing"
2020.emnlp-main.3,N19-1423,0,0.0102761,"-scale labeled dataset for the task of matching arguments to key points. The dataset, termed ArgKP, contains about 24K (argument, key point) pairs, for 28 controversial topics. Each of the pairs is labeled as matching/nonmatching. Given a set of key points for a topic, an argument could be matched to one or more key points, or to none of them. The arguments in this dataset are a subset of a larger dataset, the IBMArgQ-Rank-30kArgs dataset, which contains 71 topics, with stance and argument quality annotations for each argument (Gretz et al., 2020). Bar-Haim et al. only experimented with BERT (Devlin et al., 2019) as a supervised model for argument matching, which they trained on the ArgKP dataset. We aimed to improve their results by testing several more recent transformer-based pretrained models that were shown to substantially outperform BERT on various tasks (Wang et al., 2018), and in particular on the related task of Recognising Textual Entailment (RTE). We used the HuggingFace transformers framework and fine-tuned four different models: bert-large-uncased (Devlin • The threshold (TH) policy matches the argument to all the kps with match score > t. • The best match (BM) policy matches the argumen"
2020.emnlp-main.3,W16-2816,0,0.227853,"be successfully applied cross-domain, making it unnecessary to collect domain specific labeled data for each target domain. In future work, we would like to improve comment matching, e.g., by making it stance-aware. We also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments. In computational argumentation, several works have focused on pairwise argument similarity and clustering (Ajjour et al., 2019; Reimers et al., 2019; Misra et al., 2016). These works, however, did not attempt to create textual summaries from the resulting clusters. Egan et al. (2016) summarized argumentative discussions through the extraction of salient “points”, where each point is a verb and its syntactic arguments. The current work also extracts points from argumentative data, but our goal is to go beyond textual summaries, by matching each key point to its corresponding sentences in the input data. Similar to Egan et al., we also experimented with extracting syntactic subtrees as key points, but found that this often results in incomplete sentences or omission of important information. Selecting short, high quality sentences as key points was found to perform better i"
2020.emnlp-main.3,W16-3636,0,0.164201,"at the necessary knowledge for key point analysis, once acquired by supervised learning from argumentation data, can be successfully applied cross-domain, making it unnecessary to collect domain specific labeled data for each target domain. In future work, we would like to improve comment matching, e.g., by making it stance-aware. We also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments. In computational argumentation, several works have focused on pairwise argument similarity and clustering (Ajjour et al., 2019; Reimers et al., 2019; Misra et al., 2016). These works, however, did not attempt to create textual summaries from the resulting clusters. Egan et al. (2016) summarized argumentative discussions through the extraction of salient “points”, where each point is a verb and its syntactic arguments. The current work also extracts points from argumentative data, but our goal is to go beyond textual summaries, by matching each key point to its corresponding sentences in the input data. Similar to Egan et al., we also experimented with extracting syntactic subtrees as key points, but found that this often results in incomplete sentences or omi"
2020.emnlp-main.3,C10-1039,0,0.113374,"extraction, and (ii) Demonstrating the applicability of key point analysis to additional domains besides argumentation, including surveys and user reviews. Furthermore, we were able to achieve promising results on these domains using models that were only trained on argumentation data. Many of the works on Opinion Summarization take an alternative, sentiment-based approach. These works aim to identify the main aspects discussed in user reviews, and quantify the sentiment towards each of these aspects (Hu and Liu, 2004; Snyder and Barzilay, 2007; Titov and McDonald, 2008). However, as noted by Ganesan et al. (2010), it is still hard for a user to understand why an aspect received a particular rating. As demonstrated in Table 6, key points can address this limitation by providing a more informative summary of user reviews. However, the detection of the stance (or sentiment) of each key point with respect to the topic was left out of the scope of the current work, and we plan to address it in future work. 6 Conclusion Key Point Analysis is a novel framework for summarizing arguments, opinions and views. It provides both textual and quantitative view of the main points in the summarized data, and allows th"
2020.emnlp-main.3,P19-1054,0,0.0520784,"urthermore, we show that the necessary knowledge for key point analysis, once acquired by supervised learning from argumentation data, can be successfully applied cross-domain, making it unnecessary to collect domain specific labeled data for each target domain. In future work, we would like to improve comment matching, e.g., by making it stance-aware. We also plan to experiment with sequence-tosequence neural models for generating key point candidates from comments. In computational argumentation, several works have focused on pairwise argument similarity and clustering (Ajjour et al., 2019; Reimers et al., 2019; Misra et al., 2016). These works, however, did not attempt to create textual summaries from the resulting clusters. Egan et al. (2016) summarized argumentative discussions through the extraction of salient “points”, where each point is a verb and its syntactic arguments. The current work also extracts points from argumentative data, but our goal is to go beyond textual summaries, by matching each key point to its corresponding sentences in the input data. Similar to Egan et al., we also experimented with extracting syntactic subtrees as key points, but found that this often results in incomp"
2020.emnlp-main.3,D14-1083,0,0.065603,"Missing"
2020.emnlp-main.3,D08-1027,0,0.0690325,"Missing"
2020.emnlp-main.3,P08-1036,0,0.209413,"point analysis, enabled by automatic key point extraction, and (ii) Demonstrating the applicability of key point analysis to additional domains besides argumentation, including surveys and user reviews. Furthermore, we were able to achieve promising results on these domains using models that were only trained on argumentation data. Many of the works on Opinion Summarization take an alternative, sentiment-based approach. These works aim to identify the main aspects discussed in user reviews, and quantify the sentiment towards each of these aspects (Hu and Liu, 2004; Snyder and Barzilay, 2007; Titov and McDonald, 2008). However, as noted by Ganesan et al. (2010), it is still hard for a user to understand why an aspect received a particular rating. As demonstrated in Table 6, key points can address this limitation by providing a more informative summary of user reviews. However, the detection of the stance (or sentiment) of each key point with respect to the topic was left out of the scope of the current work, and we plan to address it in future work. 6 Conclusion Key Point Analysis is a novel framework for summarizing arguments, opinions and views. It provides both textual and quantitative view of the main"
2020.emnlp-main.3,W18-5446,0,0.0238154,"gument could be matched to one or more key points, or to none of them. The arguments in this dataset are a subset of a larger dataset, the IBMArgQ-Rank-30kArgs dataset, which contains 71 topics, with stance and argument quality annotations for each argument (Gretz et al., 2020). Bar-Haim et al. only experimented with BERT (Devlin et al., 2019) as a supervised model for argument matching, which they trained on the ArgKP dataset. We aimed to improve their results by testing several more recent transformer-based pretrained models that were shown to substantially outperform BERT on various tasks (Wang et al., 2018), and in particular on the related task of Recognising Textual Entailment (RTE). We used the HuggingFace transformers framework and fine-tuned four different models: bert-large-uncased (Devlin • The threshold (TH) policy matches the argument to all the kps with match score > t. • The best match (BM) policy matches the argument to the kp with the highest match score. • The best match+threshold (BM+TH) policy matches the argument to the kp with the highest match score, if the match score > t. For each fold, we selected the threshold t that maximizes the F1 score over the dev-set. The model that"
2020.emnlp-main.638,C02-1150,0,0.687844,"Missing"
2020.emnlp-main.638,P04-1035,0,0.0390063,"Missing"
2020.emnlp-main.638,P05-1015,0,0.260531,"Missing"
2020.emnlp-main.638,D19-1417,0,0.0597444,"et al., 2020), which have been shown to substantially improve state-of-the-art results in numerous NLP tasks. A prominent example is the BERT model (Devlin et al., 2018), which has received massive attention from the NLP research community since its inception. However, the use of AL with deep pre-trained models for text classification – and BERT in particular – has so far received surprisingly little consideration. Thus, while recent papers have demonstrated the value of AL for various deep-learning text classification schemes (Shen et al., 2017; Zhang et al., 2017; Siddhant and Lipton, 2018; Prabhu et al., 2019), the potential of AL combined with BERT is yet to be explored. First, given the unique properties of pre-trained models, 7949 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7949–7962, c November 16–20, 2020. 2020 Association for Computational Linguistics and the expectation that such models will yield adequate performance even with small amounts of training data, it is unclear a priori whether – and to what extent – established AL paradigms can further enhance their classification performance. Moreover, more recent Deep AL strategies, such as Cor"
2020.emnlp-main.638,C08-1143,0,0.29124,"(2019), we define the Diversity of a set B as:  D(B) =  −1 1 min d(xi , xj ) |U |x ∈U xj ∈B X (1) i where xi denotes the representation of the [CLS] token of example i obtained by the model which was trained using L, and d(xi , xj ) denotes the Euclidean distance between xi and xj . Representativeness: A known issue with AL strategies, especially the uncertainty-based ones, is their tendency to select outlier examples that do not properly represent the overall data distribution. We thus examine the representativeness of the selected batches. We rely on the KNN-density measure proposed by Zhu et al. (2008), in which the density of an example is quantified by the average distance between the example in question and its K most similar examples (i.e., K nearest neighbors) within U , based on the [CLS] representations as above. An example with high density degree is less likely to be an outlier. We define the representativeness of a batch as one over the average KNN-density of its instances using the Euclidean distance with K = 10. The diversity and representativeness of the different strategies are depicted in Figure 2, where for 7955 Figure 2: Diversity (left) and Representativeness (right) of th"
2020.findings-emnlp.243,C92-2082,0,0.270579,"data. In statistics, the field of analyzing new datasets is called Exploratory Data Analysis (Yu, 1977; Fekete and Primet, 2016). In NLP, such work is less common and characteristics of each dataset, task or domain are extracted independently (Choshen and Abend, 2018; Koptient et al., 2019). This has the benefit of gaining a deep understanding of each task. For instance, the work on translation divergences (Dorr, 1994; Nidhi and Singh, 2018) that aims to better explain translation to support system development later on. Research about patterns and expert crafted rules was popular in the past (Hearst, 1992; Kukich, 1992; Ravichandran and Hovy, 2002) and is still found useful nowadays; for enhancing embeddings (Schwartz, 2017), filtering noise in crawled data (Grundkiewicz and Junczys-Dowmunt, 2014; Koehn et al., 2019), as a component within large pipelines (Ein-Dor et al., 2019) or by itself in textrich domains (Padillo et al., 2019). Using domain expertise to categorize and understand a new domain is often the first practical step to apply in other fields too, which may devise rules for that purpose (Brandes and Dover, 2018; Choshen-Hillel et al., 2019; Li et al., 2019; Nguyen et al., 2010). W"
2020.findings-emnlp.243,W19-5404,0,0.0237428,"Missing"
2020.findings-emnlp.243,W19-5033,0,0.0228477,"’s prediction, when compared to words only. We leave for future work the interesting topic of how one can use GrASP lite as a surrogate model over black-box models, as well as how an expert may utilize the rules offered by GrASP lite to efficiently build rule-based models. 6 Related Work Our work provides a method to explore new data. In statistics, the field of analyzing new datasets is called Exploratory Data Analysis (Yu, 1977; Fekete and Primet, 2016). In NLP, such work is less common and characteristics of each dataset, task or domain are extracted independently (Choshen and Abend, 2018; Koptient et al., 2019). This has the benefit of gaining a deep understanding of each task. For instance, the work on translation divergences (Dorr, 1994; Nidhi and Singh, 2018) that aims to better explain translation to support system development later on. Research about patterns and expert crafted rules was popular in the past (Hearst, 1992; Kukich, 1992; Ravichandran and Hovy, 2002) and is still found useful nowadays; for enhancing embeddings (Schwartz, 2017), filtering noise in crawled data (Grundkiewicz and Junczys-Dowmunt, 2014; Koehn et al., 2019), as a component within large pipelines (Ein-Dor et al., 2019)"
2020.findings-emnlp.243,D19-1523,0,0.0437358,"Missing"
2020.findings-emnlp.243,D18-1078,1,0.734939,"nds. Each sentence in it was labeled with a single emotion (out of joy, fear, anger, sadness, disgust, shame, and guilt). HOLJ (Grover et al., 2004) A corpus of judgments of the U.K. House of Lords: legal documents containing legal terms, references and citations from rules, decisions, and more, given as free speech. Categorized into six rhetorical roles. Wiki attack (Wulczyn et al., 2017) A corpus of discussion comments from English Wikipedia talk pages that were annotated for attack; personal, general aggression, or toxicity.1 ASRD Spoken debate speeches transcribed by an ASR system, as in (Mirkin et al., 2018a,b). We believe ASR well exemplifies a commonly used domain with scarce annotated data (especially if one considers the varieties due to different systems). As this dataset comes with no sentence-level annotation, we created a test set by annotating 700 sentences to whether they contain an argument for a given topic. These sentences cover 20 topics with no intersection with the texts and topics from which rules were discovered. Annotations details are given in Appendix B, and the annotated dataset is available on the IBM Project Debater datasets webpage.2 . Essays (Stab and Gurevych, 2017) Wr"
2020.findings-emnlp.243,P04-1035,0,0.0289474,"he rest of the clusters are used as the background. 3 Datasets To demonstrate that GrASP lite rules are useful across domains, we evaluate them on 10 datasets and 26 target categories. The list of datasets, detailed next, contains both written and spoken language, from SMS messages with informal abbreviations, through posts of movie reviews, to formal protocols and legal documents written by professionals. In addition, both clean text and noisy automatic speech recognition (ASR) output are being used. The datasets’ categories, sizes and download links are provided in Appendix A. Subjectivity (Pang and Lee, 2004) Subjective and objective movie reviews automatically obtained from Rotten Tomatoes and IMDb. Polarity (Pang and Lee, 2005) Positive and negative automatically derived movie reviews. AG’s News A large-scale corpus of categorized news articles. We used the description field of the version released by Zhang et al. (2015). SMS spam (Almeida et al., 2011) SMS messages tagged for ham (legitimate) or spam. ToS (Lippi et al., 2019) Terms of Service legal documents of 50 major internet sites, in which sentences were annotated for one category - whether they belong to an unfair clause. ISEAR The Intern"
2020.findings-emnlp.243,P05-1015,0,0.0547072,"ns, we evaluate them on 10 datasets and 26 target categories. The list of datasets, detailed next, contains both written and spoken language, from SMS messages with informal abbreviations, through posts of movie reviews, to formal protocols and legal documents written by professionals. In addition, both clean text and noisy automatic speech recognition (ASR) output are being used. The datasets’ categories, sizes and download links are provided in Appendix A. Subjectivity (Pang and Lee, 2004) Subjective and objective movie reviews automatically obtained from Rotten Tomatoes and IMDb. Polarity (Pang and Lee, 2005) Positive and negative automatically derived movie reviews. AG’s News A large-scale corpus of categorized news articles. We used the description field of the version released by Zhang et al. (2015). SMS spam (Almeida et al., 2011) SMS messages tagged for ham (legitimate) or spam. ToS (Lippi et al., 2019) Terms of Service legal documents of 50 major internet sites, in which sentences were annotated for one category - whether they belong to an unfair clause. ISEAR The International Survey on Emotion Antecedents and Reactions (ISEAR) (Shao et al., 2015) is a collection of personal reports on emot"
2020.findings-emnlp.243,P02-1006,0,0.135931,"ld of analyzing new datasets is called Exploratory Data Analysis (Yu, 1977; Fekete and Primet, 2016). In NLP, such work is less common and characteristics of each dataset, task or domain are extracted independently (Choshen and Abend, 2018; Koptient et al., 2019). This has the benefit of gaining a deep understanding of each task. For instance, the work on translation divergences (Dorr, 1994; Nidhi and Singh, 2018) that aims to better explain translation to support system development later on. Research about patterns and expert crafted rules was popular in the past (Hearst, 1992; Kukich, 1992; Ravichandran and Hovy, 2002) and is still found useful nowadays; for enhancing embeddings (Schwartz, 2017), filtering noise in crawled data (Grundkiewicz and Junczys-Dowmunt, 2014; Koehn et al., 2019), as a component within large pipelines (Ein-Dor et al., 2019) or by itself in textrich domains (Padillo et al., 2019). Using domain expertise to categorize and understand a new domain is often the first practical step to apply in other fields too, which may devise rules for that purpose (Brandes and Dover, 2018; Choshen-Hillel et al., 2019; Li et al., 2019; Nguyen et al., 2010). With the increasing use of AI, a new field is"
2020.findings-emnlp.243,N16-3020,0,0.0159014,"led data (Grundkiewicz and Junczys-Dowmunt, 2014; Koehn et al., 2019), as a component within large pipelines (Ein-Dor et al., 2019) or by itself in textrich domains (Padillo et al., 2019). Using domain expertise to categorize and understand a new domain is often the first practical step to apply in other fields too, which may devise rules for that purpose (Brandes and Dover, 2018; Choshen-Hillel et al., 2019; Li et al., 2019; Nguyen et al., 2010). With the increasing use of AI, a new field is emerging – Explainable AI (XAI). It is concerned with how to understand models’ inner workings. LIME (Ribeiro et al., 2016) attempts to explain predictions by perturbing the input and understanding how the predictions change. Other works use attention as a mechanism to interpret a model’s prediction (see e.g., Ghaeini et al., 2018, who propose to interpret the intermediate layers of DNN models by visualizing the saliency of attention and 2685 LSTM gating signals). A survey of the XAI field for NLP does not exist but see (Gilpin et al., 2018; Arrieta et al., 2019) for surveys of the XAI field in general. We show in this paper that GrASP lite is interpretable by human users and is thus interesting for the XAI commun"
2020.findings-emnlp.243,D15-1050,1,0.887284,"Missing"
2020.findings-emnlp.29,D15-1050,1,0.806004,"of the Natural Language Inference (NLI) problem, where one is given two sentences, and the objective is to determine whether one entails the other, contradicts it or is neutral. This task had been researched extensively, with Conneau et al. (2018) providing a 15-language benchmark for the multilingual setting. Another earlier work on a related task is addressing the support/attack relation prediction of two arguments in Italian (Basile et al., 2016). Evidence detection is the task of determining, given some text and a topic, whether the text can serve as evidence in the context of the topic (Rinott et al., 2015). We follow Ein-Dor et al. (2020), in defining evidence as a single sentence that clearly supports or contests the topic, yet is not merely a belief or a claim. Rather, it provides an indication for whether a relevant belief or a claim is true, and, since we use their datasets, we restrict our analysis to evidence of type Study and Expert. In a multilingual setting, a similar task, that of premise detection, was considered in Eger et al. (2018a) for German, French, Spanish, and Chinese; in Fishcheva and Kotelnikov (2019) for Russian; in Eger et al. (2018b) for French and German; and in Aker an"
2020.findings-emnlp.29,D19-1564,1,0.898532,"Missing"
2020.findings-emnlp.29,P17-2039,0,0.0769029,"ask, that of premise detection, was considered in Eger et al. (2018a) for German, French, Spanish, and Chinese; in Fishcheva and Kotelnikov (2019) for Russian; in Eger et al. (2018b) for French and German; and in Aker and Zhang (2017) for Chinese. Argument quality prediction is the task of evaluating the quality of an argument, either on an objective scale - the input is an argument and the output is a quality score; or in a relative manner the input is a pair of arguments and the output is which of them is of higher quality. While there are many, arguably independent, dimensions for quality (Wachsmuth et al., 2017b), it seems that people - and, consequently, algorithms - can usually perform this task in a consistent manner (Habernal and Gurevych, 2016; Wachsmuth et al., 2017a; Toledo et al., 2019; Gretz et al., 2020). To the best of our knowledge, this task was not previously considered in a multilingual setting. 1 https://www.research.ibm.com/ haifa/dept/vst/debating_data.shtml# MultilingualArgumentMining 304 In contrast with previous multilingual research on argument mining, in this work we address three different problems, of varying complexity, over a relatively large number of languages. This allo"
2020.findings-emnlp.29,E17-1017,1,0.872483,"ask, that of premise detection, was considered in Eger et al. (2018a) for German, French, Spanish, and Chinese; in Fishcheva and Kotelnikov (2019) for Russian; in Eger et al. (2018b) for French and German; and in Aker and Zhang (2017) for Chinese. Argument quality prediction is the task of evaluating the quality of an argument, either on an objective scale - the input is an argument and the output is a quality score; or in a relative manner the input is a pair of arguments and the output is which of them is of higher quality. While there are many, arguably independent, dimensions for quality (Wachsmuth et al., 2017b), it seems that people - and, consequently, algorithms - can usually perform this task in a consistent manner (Habernal and Gurevych, 2016; Wachsmuth et al., 2017a; Toledo et al., 2019; Gretz et al., 2020). To the best of our knowledge, this task was not previously considered in a multilingual setting. 1 https://www.research.ibm.com/ haifa/dept/vst/debating_data.shtml# MultilingualArgumentMining 304 In contrast with previous multilingual research on argument mining, in this work we address three different problems, of varying complexity, over a relatively large number of languages. This allo"
2020.findings-emnlp.29,2020.lrec-1.171,0,0.103803,"Missing"
2020.findings-emnlp.47,P19-1097,1,0.84376,"et al., 2018), occupies a parallel venue, where the output should satisfy stylistic and rhetorical constraints – yet no well-defined semantic goal – with much room and desire for innovation. Approaches to argument generation have included traditional NLG architectures (Zukerman et al., 1998; Carenini and Moore, 2006); assembling arguments from given, smaller argumentative units (Walton and Gordon, 2012; Reisert et al., 2015; Wachsmuth et al., 2018; El Baff et al., 2019); welding the topic of the debate to appropriate predicates (Bilu and Slonim, 2016); and using predefined argument templates (Bilu et al., 2019). Of particular interest is the generation of counter arguments, for which solutions include an encoderdecoder architecture (Hidey and McKeown, 2019), which may be augmented by a retrieval system (Hua et al., 2019; Hua and Wang, 2018), or alternatively offering “general purpose” rebuttal based on similarity to predefined claims (Orbach et al., 2019). Concurrent with our work, and most similar, is Schiller et al. (2020), who frame the AspectControlled Argument Generation problem as follows - given a topic, a stance and an aspect, generate an argument with the given stance towards the topic, whi"
2020.findings-emnlp.47,P16-2085,1,0.934944,"the input. Argument Generation, alongside Story Generation (Fan et al., 2018), occupies a parallel venue, where the output should satisfy stylistic and rhetorical constraints – yet no well-defined semantic goal – with much room and desire for innovation. Approaches to argument generation have included traditional NLG architectures (Zukerman et al., 1998; Carenini and Moore, 2006); assembling arguments from given, smaller argumentative units (Walton and Gordon, 2012; Reisert et al., 2015; Wachsmuth et al., 2018; El Baff et al., 2019); welding the topic of the debate to appropriate predicates (Bilu and Slonim, 2016); and using predefined argument templates (Bilu et al., 2019). Of particular interest is the generation of counter arguments, for which solutions include an encoderdecoder architecture (Hidey and McKeown, 2019), which may be augmented by a retrieval system (Hua et al., 2019; Hua and Wang, 2018), or alternatively offering “general purpose” rebuttal based on similarity to predefined claims (Orbach et al., 2019). Concurrent with our work, and most similar, is Schiller et al. (2020), who frame the AspectControlled Argument Generation problem as follows - given a topic, a stance and an aspect, gene"
2020.findings-emnlp.47,W14-2107,0,0.0569939,"Missing"
2020.findings-emnlp.47,W19-8607,0,0.328816,"Missing"
2020.findings-emnlp.47,P18-1082,0,0.154866,"mpt. 1 Given a topic of interest, Claim Retrieval is the task of retrieving relevant claims from a corpus; Claim Detection is the task of determining whether a given text is a relevant claim. 528 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 528–544 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Related Work In classical Natural Language Generation (NLG) tasks – Machine Translation, Summarization, and Question Answering – the semantic content of the output strongly depends on the input. Argument Generation, alongside Story Generation (Fan et al., 2018), occupies a parallel venue, where the output should satisfy stylistic and rhetorical constraints – yet no well-defined semantic goal – with much room and desire for innovation. Approaches to argument generation have included traditional NLG architectures (Zukerman et al., 1998; Carenini and Moore, 2006); assembling arguments from given, smaller argumentative units (Walton and Gordon, 2012; Reisert et al., 2015; Wachsmuth et al., 2018; El Baff et al., 2019); welding the topic of the debate to appropriate predicates (Bilu and Slonim, 2016); and using predefined argument templates (Bilu et al.,"
2020.findings-emnlp.47,D18-1402,0,0.0185485,"emed generic, suggesting that such GTs are prevalent, but by no means the only types of texts being generated. 533 7 7.1 The Complete Pipeline Ranking Generated Claims So far we have assessed the overall ability of the models to generate relevant claims. A natural question is whether one can efficiently rank the obtained GTs, retaining only the most attractive ones for downstream tasks. This could be considered somewhat analogous to Claim Retrieval tasks, where first a large amount of argument candidates is retrieved, and are then ranked according to their relevance (e.g., Levy et al. (2014); Stab et al. (2018); Ein-Dor et al. (2020)). We considered three existing models for ranking GTs - the argument quality and stance models described in §3.3, and a Claim Detection (CD) proprietary service, obtained by training a BERT model on LN55k. The data for training the model is augmented with negative samples from the same corpus – sub-sentential fragments which were labeled as non-claims. The objective of the model is to differentiate between claims and non-claims, and is similar to that described in Ein-Dor et al. (2020) for Evidence detection. For evaluation we considered GTs generated on the dev set by"
2020.findings-emnlp.47,D19-1561,1,0.604257,"en, smaller argumentative units (Walton and Gordon, 2012; Reisert et al., 2015; Wachsmuth et al., 2018; El Baff et al., 2019); welding the topic of the debate to appropriate predicates (Bilu and Slonim, 2016); and using predefined argument templates (Bilu et al., 2019). Of particular interest is the generation of counter arguments, for which solutions include an encoderdecoder architecture (Hidey and McKeown, 2019), which may be augmented by a retrieval system (Hua et al., 2019; Hua and Wang, 2018), or alternatively offering “general purpose” rebuttal based on similarity to predefined claims (Orbach et al., 2019). Concurrent with our work, and most similar, is Schiller et al. (2020), who frame the AspectControlled Argument Generation problem as follows - given a topic, a stance and an aspect, generate an argument with the given stance towards the topic, which discusses the given aspect. They finetune CTRL (Keskar et al., 2019) over claims from 8 controversial topics, and mostly use automatic measures to assess claim generation over the same 8 topics. By contrast, here we are interested in a less restricted setting and explore the properties of the generated claims. Specifically, we fine-tune GPT-2 on"
2020.findings-emnlp.47,P17-2039,0,0.0122122,"framing the topic, and filtered using Claim Detection tools. Results on a diverse set of 96 new topics demonstrate the merit of our approach. As expected, fine tuning on a larger dataset of claims leads to more accurate generation. Yet, the coherency of the dataset also matters; simple merging of datasets of different flavors does not improve generation, and may even hamper it. To evaluate the generation models we examined several measures, which roughly estimate how “good” the generated text is. But since they do so from different perspectives, they are often not consistent with one another (Wachsmuth et al., 2017). Here they were combined heuristically, but future work should explore this more rigorously. Our work highlights some of the relations between Claim Generation, Claim Retrieval, and Claim Detection. In our pipeline, Claim Detection is used to weed out poorly-generated claims. Further, we show that Claim Retrieval is a sufficient basis – alongside a powerful language model – for building a claim generation pipeline; and that Ethical note Argument generation has the potential of being misused (Solaiman et al., 2019), as it can potentially allow to automatically generate a variety of false asser"
2020.findings-emnlp.47,C18-1318,0,0.387293,"Missing"
2020.findings-emnlp.47,W15-0507,0,0.0234845,"NLG) tasks – Machine Translation, Summarization, and Question Answering – the semantic content of the output strongly depends on the input. Argument Generation, alongside Story Generation (Fan et al., 2018), occupies a parallel venue, where the output should satisfy stylistic and rhetorical constraints – yet no well-defined semantic goal – with much room and desire for innovation. Approaches to argument generation have included traditional NLG architectures (Zukerman et al., 1998; Carenini and Moore, 2006); assembling arguments from given, smaller argumentative units (Walton and Gordon, 2012; Reisert et al., 2015; Wachsmuth et al., 2018; El Baff et al., 2019); welding the topic of the debate to appropriate predicates (Bilu and Slonim, 2016); and using predefined argument templates (Bilu et al., 2019). Of particular interest is the generation of counter arguments, for which solutions include an encoderdecoder architecture (Hidey and McKeown, 2019), which may be augmented by a retrieval system (Hua et al., 2019; Hua and Wang, 2018), or alternatively offering “general purpose” rebuttal based on similarity to predefined claims (Orbach et al., 2019). Concurrent with our work, and most similar, is Schiller"
2020.findings-emnlp.47,D15-1050,1,0.796316,"hey are the latter, do they tend to be factually true? 3 3.1 Experimental Details Data We compare the performance of fine-tuning GPT-2 on three argument datasets, two publicly available and one proprietary. Rank-30k. This dataset includes 30k arguments for 71 topics, labeled for their quality (Gretz et al., 2020). For fine-tuning GPT-2 we consider all arguments with quality score (denoted there as WAscore) > 0.9, resulting in 10,669 arguments. These arguments are typically 1-2 sentences long. CE2.3k. This dataset consists of 2.3k manually curated claims extracted from Wikipedia for 58 topics (Rinott et al., 2015). These claims are usually sub-sentence, concise phrases. We exclude claims for topics which are part of our dev set (see below). Further, we “wikify” each topic, i.e., automatically map each topic to a corresponding Wikipedia title (Shnayderman et al., 2019), and remove topics for which no such mapping is found. After this filtering, we remain with 1,489 claims for 29 topics. LN55k. This proprietary dataset consists of 55,024 manually curated claims for the 192 topics in the train set of Ein-Dor et al. (2020). These claims were extracted from a corpus of some 400 million newspaper articles pr"
2020.findings-emnlp.47,D19-1339,0,0.0162216,"ion, Claim Retrieval, and Claim Detection. In our pipeline, Claim Detection is used to weed out poorly-generated claims. Further, we show that Claim Retrieval is a sufficient basis – alongside a powerful language model – for building a claim generation pipeline; and that Ethical note Argument generation has the potential of being misused (Solaiman et al., 2019), as it can potentially allow to automatically generate a variety of false assertions regarding a topic of interest. In addition, GPT-2 text generations have been shown to exhibit different levels of bias towards different demographics (Sheng et al., 2019). Nonetheless, the way to address these dangers is for the community to recognize and better understand the properties of such generated texts, and we hope this work provides a step forward in this direction. As, to the best of our knowledge, this is the first work leveraging GPT-2 in the context of argumentation, such work can be used to advance research in the argument generation community, by surfacing issues of such systems. Furthermore, in our setting we allow for arguments to be generated on both sides of the topic, thus if one side is misrepresented, it would be easily uncovered. 536 Re"
2021.acl-long.262,2020.acl-main.371,1,0.47855,"While such summaries provide more detail, they lack a quantitative view of the data. The salience of each element in the summary is not indicated, making it difficult to evaluate their relative significance. This is particularly important for the common case of conflicting opinions. In order to fully capture the controversy, the summary should ideally indicate the proportion of favorable vs. unfavorable reviews for the controversial aspect. Recently, Key Point Analysis (KPA) has been proposed as a novel extractive summarization framework that addresses the limitations of the above approaches (Bar-Haim et al., 2020a,b). KPA extracts the main points discussed in a collection of texts, and matches the input sentences to these key points (KPs). The salience of each KP corresponds to the number of its matching sentences. The set of key points is selected out of a set of candidates short input sentences with high argumentative quality, so that together they achieve high coverage, while aiming to avoid redundancy. The resulting summary provides both textual and quantitative 3376 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference"
2021.acl-long.262,2020.emnlp-main.3,1,0.904052,"While such summaries provide more detail, they lack a quantitative view of the data. The salience of each element in the summary is not indicated, making it difficult to evaluate their relative significance. This is particularly important for the common case of conflicting opinions. In order to fully capture the controversy, the summary should ideally indicate the proportion of favorable vs. unfavorable reviews for the controversial aspect. Recently, Key Point Analysis (KPA) has been proposed as a novel extractive summarization framework that addresses the limitations of the above approaches (Bar-Haim et al., 2020a,b). KPA extracts the main points discussed in a collection of texts, and matches the input sentences to these key points (KPs). The salience of each KP corresponds to the number of its matching sentences. The set of key points is selected out of a set of candidates short input sentences with high argumentative quality, so that together they achieve high coverage, while aiming to avoid redundancy. The resulting summary provides both textual and quantitative 3376 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference"
2021.acl-long.262,2020.emnlp-main.337,0,0.0606267,"Missing"
2021.acl-long.262,2020.acl-main.461,0,0.09952,"Missing"
2021.acl-long.262,E06-1039,0,0.118876,"., 2005; Snyder and Barzilay, 2007; Blair-goldensohn et al., 2008; Titov and McDonald, 2008) aimed to extract, aggregate and quantify the sentiment toward the main aspects or features of the reviewed entity (e.g., food, price, service, and ambience for restaurants). Such aspectbased sentiment summaries provide a high-level, quantitative view of the summarized opinions, but lack explanations and justifications for the assigned scores (Ganesan et al., 2010). An alternative line of work casts this problem as multi-document summarization, aiming to create a textual summary from the input reviews (Carenini et al., 2006; Ganesan et al., 2010; Chu and Liu, 2019; Braˇzinskas et al., 2020b). While such summaries provide more detail, they lack a quantitative view of the data. The salience of each element in the summary is not indicated, making it difficult to evaluate their relative significance. This is particularly important for the common case of conflicting opinions. In order to fully capture the controversy, the summary should ideally indicate the proportion of favorable vs. unfavorable reviews for the controversial aspect. Recently, Key Point Analysis (KPA) has been proposed as a novel extractive summariza"
2021.acl-long.262,N19-1423,0,0.0074092,"n Table 1. Table 2 shows a few examples of matching sentences to KPs. 4 Classification Models Our system employs several classification models: in addition to the matching and argument quality models discussed in Section 2, in this work we add a sentiment classification model and a KP quality model, to be discussed in the next sections. All four classifiers were trained by fine-tuning a RoBERTa-large model (Liu et al., 2019). Prior to the fine-tuning of each classifier, we adapted the model to the business reviews domain, by pretraining on the Yelp dataset. We performed Masked LM pertraining (Devlin et al., 2019; Liu et al., 2019) on 1.5 million sentences sampled from the train set with a length filter of 20-150 characters per sentence. The following parameters were used: learning rate - 1e-5; 2 epochs. Training took two days on a single v100 GPU. The matching model was then obtained by fine3378 tuning the pre-trained model on the ArgKP dataset, with the parameters specified by Bar-Haim et al. (2020b). The quality model was fine-tuned following the procedure described by Gretz et al. (2020), except for using RoBERTa-large instead of BERTbase, with learning rate of 1e-5. 5 Incorporating Sentiment into"
2021.acl-long.262,C10-1039,0,0.248061,"n summarization is a long-standing challenge, which has attracted a lot of research interest over the past two decades. Early works (Hu and Liu, 2004; Gamon et al., 2005; Snyder and Barzilay, 2007; Blair-goldensohn et al., 2008; Titov and McDonald, 2008) aimed to extract, aggregate and quantify the sentiment toward the main aspects or features of the reviewed entity (e.g., food, price, service, and ambience for restaurants). Such aspectbased sentiment summaries provide a high-level, quantitative view of the summarized opinions, but lack explanations and justifications for the assigned scores (Ganesan et al., 2010). An alternative line of work casts this problem as multi-document summarization, aiming to create a textual summary from the input reviews (Carenini et al., 2006; Ganesan et al., 2010; Chu and Liu, 2019; Braˇzinskas et al., 2020b). While such summaries provide more detail, they lack a quantitative view of the data. The salience of each element in the summary is not indicated, making it difficult to evaluate their relative significance. This is particularly important for the common case of conflicting opinions. In order to fully capture the controversy, the summary should ideally indicate the"
2021.acl-long.262,2021.ccl-1.108,0,0.0438873,"Missing"
2021.acl-long.262,P19-1344,0,0.0170998,"an aggregated sentiment score or rating to the main aspects of the reviewed entity (Hu and Liu, 2004; Gamon et al., 2005; Snyder and Barzilay, 2007; Blair-goldensohn et al., 2008; Titov and McDonald, 2008). Aspects typically comprise 1-2 words (e.g., service, picture quality), and are either predefined or extracted automatically. A core 3383 sub-task in this approach is Aspect-Based Sentiment Analysis: identification of aspect mentions in the text, which may be further classified into highlevel aspect categories, and classification of the sentiment towards these mentions. Recent examples are (Ma et al., 2019; Miao et al., 2020; Karimi et al., 2020). The main shortcoming of such summaries is the lack of detail, which makes it difficult for a user to understand why an aspect received a particular rating (Ganesan et al., 2010). Although some of these summaries include for each aspect a few supporting text snippets as “evidence”, these examples may be considered anecdotal rather than representative. introduced in this work may improve KPA performance in other domains as well. In future work we would like to generate richer summaries by combining domain level key points with “local” key points, indivi"
2021.acl-long.262,N07-1038,0,0.424008,"ur next vacation. However, this abundance is often overwhelming: reading hundreds or thousands of reviews on a certain business or product is impractical, and users typically have to rely on aggregated numeric ratings, complemented by reading a small sample of reviews, which may not be representative. The vast majority of available information is left unexploited. ∗ First three authors equally contributed to this work. Opinion summarization is a long-standing challenge, which has attracted a lot of research interest over the past two decades. Early works (Hu and Liu, 2004; Gamon et al., 2005; Snyder and Barzilay, 2007; Blair-goldensohn et al., 2008; Titov and McDonald, 2008) aimed to extract, aggregate and quantify the sentiment toward the main aspects or features of the reviewed entity (e.g., food, price, service, and ambience for restaurants). Such aspectbased sentiment summaries provide a high-level, quantitative view of the summarized opinions, but lack explanations and justifications for the assigned scores (Ganesan et al., 2010). An alternative line of work casts this problem as multi-document summarization, aiming to create a textual summary from the input reviews (Carenini et al., 2006; Ganesan et"
2021.acl-long.262,2020.acl-main.513,0,0.0226988,"ur use of the Yelp dataset has been reviewed and approved by both the data acquisition authority in our organization and the Yelp team. • We do not store or use any user information from the Yelp dataset. Multi-document opinion summarization. This approach aims to create a fluent textual summary from the input reviews. A major challenge here is the limited amount of human-written summaries available for training. Recently, several abstractive neural summarization methods have shown promising results. These models require no summaries for training (Chu and Liu, 2019; Braˇzinskas et al., 2020b; Suhara et al., 2020), or only a handful of them (Braˇzinskas et al., 2020a). As discussed in the previous section, textual summaries provide more detail than aspect-based sentiment summaries, but lack a quantitative dimension. In addition, the assessment of such summaries is known to be difficult. As demonstrated in this work, KPA can be evaluated using straightforward measures such as precision and coverage. 10 Conclusion • We ensured fair compensation for crowd annotators as follows: we set a fair hourly rate according to our organization’s standards, and derived the payment per task from the hourly rate by est"
2021.acl-long.262,P08-1036,0,0.383595,"lming: reading hundreds or thousands of reviews on a certain business or product is impractical, and users typically have to rely on aggregated numeric ratings, complemented by reading a small sample of reviews, which may not be representative. The vast majority of available information is left unexploited. ∗ First three authors equally contributed to this work. Opinion summarization is a long-standing challenge, which has attracted a lot of research interest over the past two decades. Early works (Hu and Liu, 2004; Gamon et al., 2005; Snyder and Barzilay, 2007; Blair-goldensohn et al., 2008; Titov and McDonald, 2008) aimed to extract, aggregate and quantify the sentiment toward the main aspects or features of the reviewed entity (e.g., food, price, service, and ambience for restaurants). Such aspectbased sentiment summaries provide a high-level, quantitative view of the summarized opinions, but lack explanations and justifications for the assigned scores (Ganesan et al., 2010). An alternative line of work casts this problem as multi-document summarization, aiming to create a textual summary from the input reviews (Carenini et al., 2006; Ganesan et al., 2010; Chu and Liu, 2019; Braˇzinskas et al., 2020b)."
2021.acl-long.262,D19-1564,1,0.889482,"Missing"
2021.acl-tutorials.1,E17-1024,1,0.844099,"ation 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018) 3 Mirkin et al. 6 Tutorial Presenters has been leading research activities within Project Debater on tasks such as semantic similarity and argumentation mining. She has a diverse background in machine learning, having worked on a variety of domains including computational linguistics, computational biology, fraud detection and theoretical physics. She has publications in all these fields. • Roy Bar-Haim, IBM Research"
2021.acl-tutorials.1,D18-1078,1,0.884888,"Missing"
2021.acl-tutorials.1,P19-1097,1,0.795699,"ebater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018) 3 Mirkin et al. 6 Tutorial Presenters has been leading research activities within Project Debater on tasks such as semantic similarity and argumentation mining. She has a diverse background in machine learning, having worked on a variety of domains including computational linguistics, computational biology, fraud detection and theoretical physics. She has publications in all these fields. • Roy Bar-Haim, IBM Research AI roybar@il.ibm.com https://researcher.watson.ibm.com/ researcher/view.php?person=il-ROYB"
2021.acl-tutorials.1,D15-1050,1,0.81724,"ocument-level vs. sentence level approach • Usage examples • Corpus-wide argument mining 4 • Debate topic expansion The tutorial will be self-contained. We assume basic knowledge of NLP and machine learning, at the level of introductory courses in these areas. • Token-level argument mining Part 3: Argument Evaluation and Analysis (25 min) 5 Prerequisites Reading List • Argument stance classification 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018) 3 Mirkin et al. 6 Tuto"
2021.acl-tutorials.1,P19-4008,0,0.0235813,"This contrasts with today’s conversational agents, which aim at understanding a single functional command from short inputs. 1.3 Relevance to the Computational Linguistics Community The tutorial is relevant to a broad audience of NLP researchers and practitioners, working on problems related to argumentation mining, stance classification, discourse analysis, text summarization, NLG, dialogue systems, and more. 2 Tutorial Type This is a cutting-edge tutorial. The main difference between this tutorial and previous tutorials on computational argumentation or argument mining (Slonim et al., 2016; Budzynska and Reed, 2019) is that we focus on the science behind debating systems — systems that can engage in a live debate with humans. Accordingly, a large portion of the tutorial’s topics, e.g., listening comprehension, rebuttal, narrative generation and modeling Core NLP capabilities. This section describes several core NLP capabilities developed as part of Project Debater, including thematic clustering, highly scalable Wikification and semantic similarity for phrases and Wikipedia concepts. 2 https://early-access-program.debater. res.ibm.com/academic_use 2 human dilemma, was not covered in previous tutorials. So"
2021.acl-tutorials.1,P16-5002,1,0.604157,"everal minutes long. This contrasts with today’s conversational agents, which aim at understanding a single functional command from short inputs. 1.3 Relevance to the Computational Linguistics Community The tutorial is relevant to a broad audience of NLP researchers and practitioners, working on problems related to argumentation mining, stance classification, discourse analysis, text summarization, NLG, dialogue systems, and more. 2 Tutorial Type This is a cutting-edge tutorial. The main difference between this tutorial and previous tutorials on computational argumentation or argument mining (Slonim et al., 2016; Budzynska and Reed, 2019) is that we focus on the science behind debating systems — systems that can engage in a live debate with humans. Accordingly, a large portion of the tutorial’s topics, e.g., listening comprehension, rebuttal, narrative generation and modeling Core NLP capabilities. This section describes several core NLP capabilities developed as part of Project Debater, including thematic clustering, highly scalable Wikification and semantic similarity for phrases and Wikipedia concepts. 2 https://early-access-program.debater. res.ibm.com/academic_use 2 human dilemma, was not covere"
2021.acl-tutorials.1,P16-1150,0,0.0240088,"ng Part 3: Argument Evaluation and Analysis (25 min) 5 Prerequisites Reading List • Argument stance classification 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018) 3 Mirkin et al. 6 Tutorial Presenters has been leading research activities within Project Debater on tasks such as semantic similarity and argumentation mining. She has a diverse background in machine learning, having worked on a variety of domains including computational linguistics, computational biology, fraud det"
2021.acl-tutorials.1,D18-1402,0,0.0173224,"ed. We assume basic knowledge of NLP and machine learning, at the level of introductory courses in these areas. • Token-level argument mining Part 3: Argument Evaluation and Analysis (25 min) 5 Prerequisites Reading List • Argument stance classification 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018) 3 Mirkin et al. 6 Tutorial Presenters has been leading research activities within Project Debater on tasks such as semantic similarity and argumentation mining. She has"
2021.acl-tutorials.1,J19-4006,0,0.0232478,"- Using Debating Technologies in Your Application (30 min) • What is argument mining? • Identification of argument components • Overview of Project Debater APIs • Document-level vs. sentence level approach • Usage examples • Corpus-wide argument mining 4 • Debate topic expansion The tutorial will be self-contained. We assume basic knowledge of NLP and machine learning, at the level of introductory courses in these areas. • Token-level argument mining Part 3: Argument Evaluation and Analysis (25 min) 5 Prerequisites Reading List • Argument stance classification 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation sys"
2021.acl-tutorials.1,C14-1141,1,0.786512,"ect Debater APIs • Document-level vs. sentence level approach • Usage examples • Corpus-wide argument mining 4 • Debate topic expansion The tutorial will be self-contained. We assume basic knowledge of NLP and machine learning, at the level of introductory courses in these areas. • Token-level argument mining Part 3: Argument Evaluation and Analysis (25 min) 5 Prerequisites Reading List • Argument stance classification 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018)"
2021.acl-tutorials.1,E17-1017,0,0.0174497,"Token-level argument mining Part 3: Argument Evaluation and Analysis (25 min) 5 Prerequisites Reading List • Argument stance classification 1. A survey on argument mining: Lawrence and Reed (2019) • Argument quality 2. Project Debater: Slonim et al. (2021) 3. Identification of argument components within an article: Levy et al. (2014), Rinott et al. (2015), Lippi and Torroni (2015) Part 4: Modeling Human Dilemma (15 min) • Common principled arguments 4. Corpus-wide argument mining: Stab et al. (2018), Ein-Dor et al. (2020) • When do principled arguments apply? Coffee break 5. Argument quality: Wachsmuth et al. (2017), Habernal and Gurevych (2016) Part 5: Listening Comprehension and Rebuttal (25 min) 6. Stance classification: Bar-Haim et al. (2017) • Debate vs. classical conversation systems 7. Modeling human dilemma: Bilu et al. (2019) • Understanding the gist of long, spontaneous speech 8. Listening Comprehension: (2018) 3 Mirkin et al. 6 Tutorial Presenters has been leading research activities within Project Debater on tasks such as semantic similarity and argumentation mining. She has a diverse background in machine learning, having worked on a variety of domains including computational linguistics, co"
2021.argmining-1.16,2020.emnlp-main.3,1,0.89056,"being considered for evaluation on both tracks are crowd sourced arguments on three debatable topics. The nature of the KPA task requires to consider various evaluation measures to estimate the quality of the results across different dimensions. In this paper we report and analyze the results of 22 models submitted by 17 teams to the KPA-2021 shared task across these different evaluation measures. We also discuss lessons learned from these multiple submissions, that can guide future development of new KPA algorithms and associated evaluation methods. 2 Related Work Our shared task is based on Bar-Haim et al. (2020a,b), which introduced the problem of KPA and proposed an extractive approach to address the task. In particular, we focus on benchmarking two sub tasks: (1) Key point matching; and (2) Key point generation and matching. For the matching task Bar-Haim et al. (2020b) uses RoBERTa (Liu et al., 2019) based classifier, trained on argument-KP pairs. For the generation task (Bar-Haim et al., 2020b) employs the follow154 Proceedings of The 8th Workshop on Argument Mining, pages 154–164 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics ing steps: Fir"
2021.argmining-1.16,2021.eacl-main.141,0,0.0110599,"non-redundant sentences that are most relevant to the query to include in the summary (Xu and Lapata, 2020). A few studies (Liu et al., 2018; Shapira et al., 2021) have explored generating the most salient queries the users might be interested in. However, it is still unclear how these salient queries/points are represented in the source documents. KPA addresses this problem by reporting the prevalence of each KP in the input data. Opinion and argument summarization. Recently there has been an increasing interest in summarizing opinions expressed in various reviews (Amplayo and Lapata, 2021; Elsahar et al., 2021) or argumentative text (Wang and Ling, 2016; Chen et al., 2019; Syed et al., 2020). KPA contributes to this line of work by adding a quantitative dimension which reflects the distribution of opinions in the examined data. For a detailed discussion of the relation between KPA and argument clustering and summarization see (Bar-Haim et al., 2020a,b). 3 3.1 Task Details Task Description is given as part of the input, requiring models to only predict a match confidence score for each argument in the input, towards each of the provided KPs. In both tracks, the confidence score expresses the model’s"
2021.argmining-1.16,D19-1387,0,0.0558908,"Missing"
2021.argmining-1.16,2020.emnlp-main.648,0,0.0214265,"f high quality arguments that serve as KP candidates. Next, a match confidence score is calculated between all KP candidate-argument pairs, as well as between all the KP candidates to themselves. Finally, several filtering and ordering strategies are employed to maintain the most salient, yet non-overlapping KP candidates to serve as KPs. Below we briefly review the work closely related to KPA in a few fields. Multi-document summarization (MDS). Most prior work on MDS has been focused on the news, Wikipedia, and scientific literature domains (Dang, 2005; Baumel et al., 2016; Liu et al., 2018; Lu et al., 2020). MDS techniques are typically querybased and extractive, where models select a set of non-redundant sentences that are most relevant to the query to include in the summary (Xu and Lapata, 2020). A few studies (Liu et al., 2018; Shapira et al., 2021) have explored generating the most salient queries the users might be interested in. However, it is still unclear how these salient queries/points are represented in the source documents. KPA addresses this problem by reporting the prevalence of each KP in the input data. Opinion and argument summarization. Recently there has been an increasing int"
2021.argmining-1.16,D19-1564,1,0.888638,"Missing"
2021.argmining-1.16,N16-1007,0,0.015059,"nt to the query to include in the summary (Xu and Lapata, 2020). A few studies (Liu et al., 2018; Shapira et al., 2021) have explored generating the most salient queries the users might be interested in. However, it is still unclear how these salient queries/points are represented in the source documents. KPA addresses this problem by reporting the prevalence of each KP in the input data. Opinion and argument summarization. Recently there has been an increasing interest in summarizing opinions expressed in various reviews (Amplayo and Lapata, 2021; Elsahar et al., 2021) or argumentative text (Wang and Ling, 2016; Chen et al., 2019; Syed et al., 2020). KPA contributes to this line of work by adding a quantitative dimension which reflects the distribution of opinions in the examined data. For a detailed discussion of the relation between KPA and argument clustering and summarization see (Bar-Haim et al., 2020a,b). 3 3.1 Task Details Task Description is given as part of the input, requiring models to only predict a match confidence score for each argument in the input, towards each of the provided KPs. In both tracks, the confidence score expresses the model’s confidence that the KP represents the essen"
2021.argmining-1.16,2020.emnlp-main.296,0,0.0121287,"lves. Finally, several filtering and ordering strategies are employed to maintain the most salient, yet non-overlapping KP candidates to serve as KPs. Below we briefly review the work closely related to KPA in a few fields. Multi-document summarization (MDS). Most prior work on MDS has been focused on the news, Wikipedia, and scientific literature domains (Dang, 2005; Baumel et al., 2016; Liu et al., 2018; Lu et al., 2020). MDS techniques are typically querybased and extractive, where models select a set of non-redundant sentences that are most relevant to the query to include in the summary (Xu and Lapata, 2020). A few studies (Liu et al., 2018; Shapira et al., 2021) have explored generating the most salient queries the users might be interested in. However, it is still unclear how these salient queries/points are represented in the source documents. KPA addresses this problem by reporting the prevalence of each KP in the input data. Opinion and argument summarization. Recently there has been an increasing interest in summarizing opinions expressed in various reviews (Amplayo and Lapata, 2021; Elsahar et al., 2021) or argumentative text (Wang and Ling, 2016; Chen et al., 2019; Syed et al., 2020). KPA"
2021.argmining-1.16,2021.argmining-1.17,0,0.0694742,"Missing"
2021.argmining-1.16,2021.argmining-1.18,0,0.0768215,"Missing"
2021.argmining-1.16,2021.naacl-main.54,0,0.0578228,"Missing"
2021.argmining-1.16,2020.coling-main.470,0,0.0990805,"Missing"
2021.emnlp-demo.31,E17-1024,1,0.843941,"on. The service is based on a BERT model, which was fine-tuned on 52K crowd-annotated examples mined from the Lexis-Nexis corpus. Evidence Detection. Similar to the Claim Detection service, this service gets a sentence and a topic and identifies whether the sentence is an Evidence supporting or contesting the topic. In our context, an Evidence is an argument that contains research results or an expert opinion. This is a BERT based service which was fine-tuned using 200K annotated examples from Lexis-Nexis corpus. This model is based on the work of Ein-Dor et al. (2020). Pro-Con. This service (Bar-Haim et al., 2017; Toledo-Ronen et al., 2020), gets an argument and a topic and predicts whether the argument supports or contests the topic. This service is a BERTbased classifier, which was trained on 400K stancelabeled examples. It includes arguments extracted from the Lexis-Nexis corpus, as well as arguments collected via crowsourcing. The set of training arguments was automatically expanded by replacing the original debate concept with consistent and contrastive expansions, based on the work of Bar-Haim et al. (2019). 2.3 Content Summarization This group contains two high-level services that create differ"
2021.emnlp-demo.31,2020.acl-main.371,1,0.906546,"a BERTbased classifier, which was trained on 400K stancelabeled examples. It includes arguments extracted from the Lexis-Nexis corpus, as well as arguments collected via crowsourcing. The set of training arguments was automatically expanded by replacing the original debate concept with consistent and contrastive expansions, based on the work of Bar-Haim et al. (2019). 2.3 Content Summarization This group contains two high-level services that create different types of summaries. Key Point Analysis. This service summarizes a collection of comments on a given topic as a small set of key points (Bar-Haim et al., 2020a,b). The salience of each key point is given by the number of its matching sentences in the given comments. The input for the service is a collection of textual comments, which are split into sentences. The output is a short list of key points and their salience, along with a list of matching sentences per key point. A key point matches a sentence if it captures the gist of the sentence, or is directly supported by a point made in the sentence. The service selects key points from a subset of concise, high-quality sentences (according to the quality service described above), aiming to achieve"
2021.emnlp-demo.31,2021.acl-long.262,1,0.733321,"ubset. IBM Employee Engagement Survey. Key Point Analysis has also been applied to analyze the 2020 IBM employee engagement survey. Over 300K employees wrote more than 550K sentences in total. These sentences were automatically classified into positive and negative, and we ran KPA on each set separately to extract positive and negative key points. The HR team reported that these analyses enable them to extract actionable and valuable insights with significantly less effort. Business Reviews. Similar to surveys, KPA can also be used for effectively summarizing user reviews. In our recent work (Bar-Haim et al., 2021) we demonstrate its application to the Yelp dataset of business reviews. 5.2 Online Debates low quality arguments, the KPA service to summarize the data into key points and the narrative generation to create a coherent speech. That’s Debateable. “That’s Debatable” is a TV show presented by Bloomberg Media and Intelligence Squared. In each episode, a panel of experts debates a controversial topic, such as “It’s time to redistribute the wealth”. Using the above pipeline, we were able to summarize thousands of arguments submitted online by the audience, and the resulting pro and con key points an"
2021.emnlp-demo.31,2020.emnlp-main.3,1,0.895539,"a BERTbased classifier, which was trained on 400K stancelabeled examples. It includes arguments extracted from the Lexis-Nexis corpus, as well as arguments collected via crowsourcing. The set of training arguments was automatically expanded by replacing the original debate concept with consistent and contrastive expansions, based on the work of Bar-Haim et al. (2019). 2.3 Content Summarization This group contains two high-level services that create different types of summaries. Key Point Analysis. This service summarizes a collection of comments on a given topic as a small set of key points (Bar-Haim et al., 2020a,b). The salience of each key point is given by the number of its matching sentences in the given comments. The input for the service is a collection of textual comments, which are split into sentences. The output is a short list of key points and their salience, along with a list of matching sentences per key point. A key point matches a sentence if it captures the gist of the sentence, or is directly supported by a point made in the sentence. The service selects key points from a subset of concise, high-quality sentences (according to the quality service described above), aiming to achieve"
2021.emnlp-demo.31,P19-3031,0,0.0123698,"rative Generation constructs a well-structured speech that supports or contests a given topic, according to the specified polarity. Key Point Analysis summarizes a collection of comments as a small set of automatically extracted, human-readable key points, each assigned with a numeric measure of its prominence in the input. These tools may serve data scientists analyzing opinionated texts such as user reviews, survey responses, social media, customer feedback, etc. Several demonstrations of argument mining capabilities have been previously published (Stab et al., 2018; Wachsmuth et al., 2017; Chernodub et al., 2019), some of which also provide access to their capabilities via APIs. However, Project Debater APIs offer a much broader set of services, trained on unique large-scale, high quality datasets, which have been developed over many years of research. The next sections describe each of the APIs and their performance assessment, and how they can be accessed and used via the Debater Early Access Program. We then describe several examples of using and combining these APIs in practical applications. 2 Services Overview In this section we provide a short description for each service, and point to its rela"
2021.emnlp-demo.31,N19-1423,0,0.00906453,"since existing tools were far too slow to be applied to the Lexis-Nexis corpus we used for argument mining, which contains about 10 billion sentences. We developed a simple rule-based method, which relies on matching the mentions to the Wikipedia title, as well as on Wikipedia redirects. This approach enables very fast Wikification, about 20 times faster than the commonly-used TagMe Wikifier (Ferragina and Scaiella, 2010), while achieving competitive accuracy. Concept relatedness. This service measures the semantic relatedness between a pair of Wikipedia concepts. We trained a BERT regressor (Devlin et al., 2019) on the WORD dataset (Ein Dor et al., 2018), which includes 13K pairs of Wikipedia concepts manually annotated to determine their level of relatedness. The input to the regressor is the first sentence in the Wikipedia article of each concept. Text clustering. Our Text clustering service is based the Sequential Information Bottleneck (sIB) algorithm (Slonim et al., 2002). This unsupervised algorithm has been shown to achieve strong results on standard benchmarks. However, sIB has not been as popular as other clustering algorithms such as K-Means, since its run time was significantly higher. Our"
2021.emnlp-demo.31,L18-1408,1,0.912034,"pplied to the Lexis-Nexis corpus we used for argument mining, which contains about 10 billion sentences. We developed a simple rule-based method, which relies on matching the mentions to the Wikipedia title, as well as on Wikipedia redirects. This approach enables very fast Wikification, about 20 times faster than the commonly-used TagMe Wikifier (Ferragina and Scaiella, 2010), while achieving competitive accuracy. Concept relatedness. This service measures the semantic relatedness between a pair of Wikipedia concepts. We trained a BERT regressor (Devlin et al., 2019) on the WORD dataset (Ein Dor et al., 2018), which includes 13K pairs of Wikipedia concepts manually annotated to determine their level of relatedness. The input to the regressor is the first sentence in the Wikipedia article of each concept. Text clustering. Our Text clustering service is based the Sequential Information Bottleneck (sIB) algorithm (Slonim et al., 2002). This unsupervised algorithm has been shown to achieve strong results on standard benchmarks. However, sIB has not been as popular as other clustering algorithms such as K-Means, since its run time was significantly higher. Our implementation of sIB is highly optimized,"
2021.emnlp-demo.31,C14-1141,1,0.766978,"nnotators were asked whether the sentence, as is, may fit in a speech supporting or contesting the given topic. High quality scores typically indicate arguments that are grammatically valid, use proper language, make a clear and concise argument, have a clear stance towards the topic, etc. Argument Mining and Analysis This group includes classifiers and regressors that aim to identify arguments in input texts, determine their stance, and assess their quality. Claim Detection. This service identifies whether a sentence contains a claim with respect to a given topic. This task was introduced by Levy et al. (2014). They define a Claim as “a general, concise statement that directly supports or contests the given Topic”. The claim detection model is a BERT-based classifier, trained on 90K positive and negative labeled examples from the Lexis-Nexis corpus. The model is similar to the one described in (Ein-Dor et al., 2020). Claim Boundaries. Given an input sentence that is assumed to contain a claim, this service returns the boundaries of the claim within the sentence (Levy et al., 2014). The Claim Boundaries service may be used to refine the results of the Claim Detection service, which provides sentence"
2021.emnlp-demo.31,2021.ccl-1.108,0,0.0319921,"Missing"
2021.emnlp-demo.31,W17-5106,0,0.0288361,"highlevel services: Narrative Generation constructs a well-structured speech that supports or contests a given topic, according to the specified polarity. Key Point Analysis summarizes a collection of comments as a small set of automatically extracted, human-readable key points, each assigned with a numeric measure of its prominence in the input. These tools may serve data scientists analyzing opinionated texts such as user reviews, survey responses, social media, customer feedback, etc. Several demonstrations of argument mining capabilities have been previously published (Stab et al., 2018; Wachsmuth et al., 2017; Chernodub et al., 2019), some of which also provide access to their capabilities via APIs. However, Project Debater APIs offer a much broader set of services, trained on unique large-scale, high quality datasets, which have been developed over many years of research. The next sections describe each of the APIs and their performance assessment, and how they can be accessed and used via the Debater Early Access Program. We then describe several examples of using and combining these APIs in practical applications. 2 Services Overview In this section we provide a short description for each servi"
2021.emnlp-demo.31,N18-5005,0,0.0376234,"Missing"
2021.emnlp-demo.31,2020.findings-emnlp.29,1,0.713401,"d on a BERT model, which was fine-tuned on 52K crowd-annotated examples mined from the Lexis-Nexis corpus. Evidence Detection. Similar to the Claim Detection service, this service gets a sentence and a topic and identifies whether the sentence is an Evidence supporting or contesting the topic. In our context, an Evidence is an argument that contains research results or an expert opinion. This is a BERT based service which was fine-tuned using 200K annotated examples from Lexis-Nexis corpus. This model is based on the work of Ein-Dor et al. (2020). Pro-Con. This service (Bar-Haim et al., 2017; Toledo-Ronen et al., 2020), gets an argument and a topic and predicts whether the argument supports or contests the topic. This service is a BERTbased classifier, which was trained on 400K stancelabeled examples. It includes arguments extracted from the Lexis-Nexis corpus, as well as arguments collected via crowsourcing. The set of training arguments was automatically expanded by replacing the original debate concept with consistent and contrastive expansions, based on the work of Bar-Haim et al. (2019). 2.3 Content Summarization This group contains two high-level services that create different types of summaries. Key"
2021.emnlp-main.721,C18-1096,0,0.0261082,"s (Pontiki et al., 2016), Czech (Steinberger et al., 2014) and German (Klinger and Cimiano, 2014). Annotation Scheme Our annotation scheme is reminiscent of two-phase data collection efforts in other tasks. These typically include an initial phase where annotation candidates are detected, followed by a verification phase that further labels each candidate by multiple annotators. Some examples include the annotation of claims (Levy et al., 2014), evidence (Rinott et al., 2015) or mentions (Mass et al., 2018). works (Tang et al., 2016a,b; Ruder et al., 2016; Ma et al., 2018; Huang et al., 2018; He et al., 2018) have utilized pre-transformer models (see surveys by Schouten and Frasincar (2015); Zhang et al. (2018)). Recently, focus has shifted to using pretrained language models (Sun et al., 2019; Song et al., 2019; Zeng et al., 2019; Phan and Ogunbona, 2020). Generalization to unseen domains has also been explored with pre-training that includes domain-specific data (Xu et al., 2019; Rietzler et al., 2020), adds sentiment-related objectives (Tian et al., 2020), or combines instance-based domain adaptation (Gong et al., 2020). 3 Input Data The input data for the annotation was sampled from the follow"
2021.emnlp-main.721,P19-1048,0,0.0150616,"nt targets in a given text; and sentiment classification (SC), of determining the sentiment towards a specific candidate target in a given text. TSA systems are either pipelined systems running a TE model followed by an SC model (e.g., 5 Karimi et al. (2020)), or end-to-end (sometimes www.yelp.com/dataset 6 registry.opendata.aws/ called joint) systems using a single model for the whole task, which is typically regarded as a se- amazon-reviews-ml 7 nlp.stanford.edu/sentiment 8 quence labeling problem (Li and Lu, 2019; Li et al., github.com/kavgan/ opinosis-summarization 2019a; Hu et al., 2019; He et al., 2019). Earlier 9156 tences from Tripadvisor (hotels), Edmunds (cars), and Amazon (electronics) (Ganesan et al., 2010). Each sentence discusses a topic comprised of a product name and an aspect of the product (e.g. ""performance of Toyota Camry”). At least 10 sentences were randomly sampled from each of the 51 topics in the dataset, yielding 512 sentences. Overall, the input data includes reviews from many domains not previously annotated for TSA, such as books, cars, pet products, kitchens, movies or drugstores. Further examples are detailed in Appendix A. The annotation input also included 200 rand"
2021.emnlp-main.721,P19-1051,0,0.0197555,"fying all sentiment targets in a given text; and sentiment classification (SC), of determining the sentiment towards a specific candidate target in a given text. TSA systems are either pipelined systems running a TE model followed by an SC model (e.g., 5 Karimi et al. (2020)), or end-to-end (sometimes www.yelp.com/dataset 6 registry.opendata.aws/ called joint) systems using a single model for the whole task, which is typically regarded as a se- amazon-reviews-ml 7 nlp.stanford.edu/sentiment 8 quence labeling problem (Li and Lu, 2019; Li et al., github.com/kavgan/ opinosis-summarization 2019a; Hu et al., 2019; He et al., 2019). Earlier 9156 tences from Tripadvisor (hotels), Edmunds (cars), and Amazon (electronics) (Ganesan et al., 2010). Each sentence discusses a topic comprised of a product name and an aspect of the product (e.g. ""performance of Toyota Camry”). At least 10 sentences were randomly sampled from each of the 51 topics in the dataset, yielding 512 sentences. Overall, the input data includes reviews from many domains not previously annotated for TSA, such as books, cars, pet products, kitchens, movies or drugstores. Further examples are detailed in Appendix A. The annotation input also"
2021.emnlp-main.721,2020.emnlp-main.369,0,0.243196,"on or Yelp that host reviews from dozens or even hundreds of domains.1 Existing English TSA datasets do not facilitate such a broad evaluation, as they typically include reviews from a small number of domains. For example, the popular S EM E VAL (SE) datasets created by Pontiki et al. (2014, 2015, 2016) (henceforth SE14, SE15, and SE16, respectively), contain English reviews of restaurants, laptops and hotels (see §2 for a discussion of other existing datasets). To address this gap, we present YASO,2 a new TSA dataset collected over user reviews taken from four sources: the Y ELP and A MAZON (Keung et al., 2020) datasets of reviews from those two sites; the Stanford Sentiment Treebank (SST) movie reviews corpus (Socher et al., 2013); and the O PINOSIS dataset of reviews from over 50 topics (Ganesan et al., 2010). To the best of our knowledge, while these resources have been previously used for sentiment analysis research, they were not annotated and used for targeted sentiment analysis. The new YASO evaluation dataset contains 2215 annotated sentences, on par with the size of existing test sets (e.g., one of the largest is the SE14 test set, with 1,600 sentences). The annotation of open-domain review"
2021.emnlp-main.721,klinger-cimiano-2014-usage,0,0.0176324,"Twitter data include targets that are either celebrities, products, or companies (Dong et al., 2014), and a multi-target corpus on UK elections data (Wang et al., 2017a). Lastly, Hamborg et al. (2021) annotated named entities for their sentiment within the news domain. Multilingual Other datasets exist for various languages, such as: Norwegian (Øvrelid et al., 2020), Catalan and Basque (Barnes et al., 2018), Chinese (Yang et al., 2018), Hungarian (Szabó et al., 2016), Hindi (Akhtar et al., 2016), SE16 with multiple languages (Pontiki et al., 2016), Czech (Steinberger et al., 2014) and German (Klinger and Cimiano, 2014). Annotation Scheme Our annotation scheme is reminiscent of two-phase data collection efforts in other tasks. These typically include an initial phase where annotation candidates are detected, followed by a verification phase that further labels each candidate by multiple annotators. Some examples include the annotation of claims (Levy et al., 2014), evidence (Rinott et al., 2015) or mentions (Mass et al., 2018). works (Tang et al., 2016a,b; Ruder et al., 2016; Ma et al., 2018; Huang et al., 2018; He et al., 2018) have utilized pre-transformer models (see surveys by Schouten and Frasincar (201"
2021.emnlp-main.721,C14-1141,1,0.784319,"l., 2020), Catalan and Basque (Barnes et al., 2018), Chinese (Yang et al., 2018), Hungarian (Szabó et al., 2016), Hindi (Akhtar et al., 2016), SE16 with multiple languages (Pontiki et al., 2016), Czech (Steinberger et al., 2014) and German (Klinger and Cimiano, 2014). Annotation Scheme Our annotation scheme is reminiscent of two-phase data collection efforts in other tasks. These typically include an initial phase where annotation candidates are detected, followed by a verification phase that further labels each candidate by multiple annotators. Some examples include the annotation of claims (Levy et al., 2014), evidence (Rinott et al., 2015) or mentions (Mass et al., 2018). works (Tang et al., 2016a,b; Ruder et al., 2016; Ma et al., 2018; Huang et al., 2018; He et al., 2018) have utilized pre-transformer models (see surveys by Schouten and Frasincar (2015); Zhang et al. (2018)). Recently, focus has shifted to using pretrained language models (Sun et al., 2019; Song et al., 2019; Zeng et al., 2019; Phan and Ogunbona, 2020). Generalization to unseen domains has also been explored with pre-training that includes domain-specific data (Xu et al., 2019; Rietzler et al., 2020), adds sentiment-related obje"
2021.emnlp-main.721,D19-5505,0,0.017883,"he sentiment of food is unclear (and exclusive SE14 annotations manually identified as errors. 11 labeled as positive in SE14). From the other 12 github.com/IMPLabUniPr/BERT-for-ABSA 9160 with domain-specific language models (Xu et al., 2019) augmented with adversarial data. – LCF:12 (Yang et al., 2020): An end-to-end model based on Song et al. (2019), with domain adaptation and a local context focus mechanism. – RACL:13 (Chen and Qian, 2020): An end-to-end multi-task learning and relation propagation system. We used the RACL-GloVe variant, based on pre-trained word embeddings. – BERT-E2E:14 (Li et al., 2019b): A BERT-based end-to-end sequence labeling system. We used the BERT+Linear architecture, which computes pertoken labels using a linear classification layer. – HAST+MCRF: A pipeline of (i) HAST,15 a TE system based on capturing aspect detection history and opinion summary (Li et al., 2018); and (ii) MCRF-SA,16 an SC system utilizing multiple CRF-based structured attention models (Xu et al., 2020a). Evaluation Metrics As a pre-processing step, any predicted target with a span equal to the span of a target candidate annotated with low-confidence was excluded from the evaluation, since it is un"
2021.emnlp-main.721,D13-1171,0,0.0845452,"Missing"
2021.emnlp-main.721,2020.lrec-1.618,0,0.032244,"entified by a few annotators may be incorrect (see §5 for further analysis). Therefore, marked candidates are passed through a second sentiment annotation (bottom) phase, which separately collects their sentiments. named entities. Other TSA datasets on Twitter data include targets that are either celebrities, products, or companies (Dong et al., 2014), and a multi-target corpus on UK elections data (Wang et al., 2017a). Lastly, Hamborg et al. (2021) annotated named entities for their sentiment within the news domain. Multilingual Other datasets exist for various languages, such as: Norwegian (Øvrelid et al., 2020), Catalan and Basque (Barnes et al., 2018), Chinese (Yang et al., 2018), Hungarian (Szabó et al., 2016), Hindi (Akhtar et al., 2016), SE16 with multiple languages (Pontiki et al., 2016), Czech (Steinberger et al., 2014) and German (Klinger and Cimiano, 2014). Annotation Scheme Our annotation scheme is reminiscent of two-phase data collection efforts in other tasks. These typically include an initial phase where annotation candidates are detected, followed by a verification phase that further labels each candidate by multiple annotators. Some examples include the annotation of claims (Levy et a"
2021.emnlp-main.721,D15-1050,1,0.865877,"Missing"
2021.emnlp-main.721,D16-1103,0,0.0288965,"Hindi (Akhtar et al., 2016), SE16 with multiple languages (Pontiki et al., 2016), Czech (Steinberger et al., 2014) and German (Klinger and Cimiano, 2014). Annotation Scheme Our annotation scheme is reminiscent of two-phase data collection efforts in other tasks. These typically include an initial phase where annotation candidates are detected, followed by a verification phase that further labels each candidate by multiple annotators. Some examples include the annotation of claims (Levy et al., 2014), evidence (Rinott et al., 2015) or mentions (Mass et al., 2018). works (Tang et al., 2016a,b; Ruder et al., 2016; Ma et al., 2018; Huang et al., 2018; He et al., 2018) have utilized pre-transformer models (see surveys by Schouten and Frasincar (2015); Zhang et al. (2018)). Recently, focus has shifted to using pretrained language models (Sun et al., 2019; Song et al., 2019; Zeng et al., 2019; Phan and Ogunbona, 2020). Generalization to unseen domains has also been explored with pre-training that includes domain-specific data (Xu et al., 2019; Rietzler et al., 2020), adds sentiment-related objectives (Tian et al., 2020), or combines instance-based domain adaptation (Gong et al., 2020). 3 Input Data The in"
2021.emnlp-main.721,C16-1146,0,0.0198933,"aset for several SOTA baseline systems. All collected data are available online.4 2 Related work Review datasets The Darmstadt Review Corpora (Toprak et al., 2010) contains annotations of user reviews in two domains – online universities and online services. Later on, SE14 annotated laptop and restaurant reviews (henceforth SE14L and SE14-R). In SE15 a third domain (hotels) was added, and SE16 expanded the English data for the two original domains (restaurants and laptops). Jiang et al. (2019) created a challenge dataset with multiple targets per-sentence, again within the restaurants domain. Saeidi et al. (2016) annotated opinions from discussions on urban neighbourhoods. Clearly, the diversity of the reviews in these datasets is limited, even when taken together. Non-review datasets The Multi-Purpose Question Answering dataset (Wiebe et al., 2005) was the first opinion mining corpus with a detailed annotation scheme applied to sentences from news documents. Mitchell et al. (2013) annotated opendomain tweets using an annotation scheme similar to ours, where target candidates were annotated for their sentiment by crowd-workers, yet the annotated terms were limited to automatically detected 3 Mixed: a"
C14-1141,W14-2109,1,0.377525,"ion that perhaps does not have a strict truth value associated to it (Table 1, S5). More generally, TE is typically focused on declarative statements. However, persuasion and argumentation often have an emotional aspect and thus may involve additional sentence types. Correspondingly, in our framework it is quite natural that the Topic, or the associated CDCs, will correspond to imperative sentences, or even to exclamatory sentences. 3 Data Our supervised learning approach relies on labeled data that were collected as described below. A detailed description of the labeling process is given in (Aharoni et al., 2014). Due to the high complexity of the labeling task, we worked with in-house labelers which were provided with detailed guidelines, and went through rigorous training. At the core of the labeling guidelines, we outlined the definition of a CDC as a general, concise statement that directly supports or contests the given Topic. In practice, the labelers were asked to label a text fragment as a CDC if and only if it complies with all the following five criteria: • Strength – Strong content that directly supports/contests the Topic. • Generality – General content that deals with a relatively broad i"
C14-1141,P12-2041,0,0.0732848,"uire that the detected CDCs are reasonably well phrased, so that they can be instantly and naturally used in a discussion about the given Topic.This task, which we term Context Dependent Claim Detection (CDCD), can be of great practical importance in decision support and persuasion enhancement, in various domains where relevant massive corpora are available for mining. CDCD can be seen as a sub-task in the emerging wider field of argumentation mining that involves identifying argumentative structures within a document, as well as their potential relations 1490 (Mochales Palau and Moens, 2009; Cabrio and Villata, 2012; Wyner et al., 2012). However, CDCD has several distinctive key features. Most importantly, as implied by its name, a CDC is defined with respect to a given context – the input Topic. Thus, identifying general characteristics of a claim-like statement as done in (Mochales Palau and Moens, 2009) is not sufficient, since one should further identify the relevance of the candidate claim to the Topic. In addition, we do not restrict ourselves to a particular domain nor to structured data (Mochales Palau and Moens, 2009), but rather consider free-text Wikipedia articles in a diverse range of subjec"
C14-1141,P05-1045,0,0.0191032,"s a mix of these two types, that was proven essential to our performance, and relied on an extension of the Sequential Pattern Matching (SPM) algorithm (Srikant and Agrawal, 1996). Specifically, for this SequentialPatternMatch feature, each sentence token was encoded as a tuple describing several attributes for that token – e.g., the token’s text, the token’s POS tag, and various binary indicators, indicating if the token is a sentiment word, if it is mentioned in the given Topic, if it is included in an automatically learned lexicon of “claim words”, and if it is identified by a NER utility (Finkel et al., 2005). A variant of the SPM algorithm (Srikant and Agrawal, 1996) was then used to detect patterns that characterize CDCsentences, and these patterns were added to the features examined by the LR classifier. Specifically, each of these feature values was set to 1 if a candidate sentence had a match with the relevant pattern, and to 0 otherwise. For example, in Table 2, the word ”that” is encoded as [that,IN,CDC] implying it is included in the “claim words” lexicon with POS tag IN; the word ”games” is encoded as [games,NNS,Topic] implying it is mentioned in the Topic with POS tag NNS; and the word ”"
C14-1141,D11-1025,0,0.0115425,"ns, 2009) is not sufficient, since one should further identify the relevance of the candidate claim to the Topic. In addition, we do not restrict ourselves to a particular domain nor to structured data (Mochales Palau and Moens, 2009), but rather consider free-text Wikipedia articles in a diverse range of subject matters. Moreover, in CDCD we require pinpointing the exact claim boundaries, which do not necessarily match a whole sentence or even a clause in the original text, thus adding a significant burden to the task, compared to classical tasks that are focused on sentence classifications (Guo et al., 2011). CDCD also shares some relations with Argumentative Zoning (Teufel, 1999; Guo et al., 2011). There, the aim is to divide the text of a scientific article into “zones”, each characterized by the rhetorical nature of its content. However, our work is not limited to scientific literature that often has a more objective and less persuasive style. Further, as mentioned, we go beyond sentence classification, aiming to detect the exact claim boundaries, and require detecting only claims relevant to a given Topic, rather than just any claim mentioned in a given article. Finally, another important lin"
C14-1141,P13-1045,0,0.00801797,"at is the probability of having a grammatically correct sentence, given that the observed token is in its observed position. ModifierSeparation: The ESG parser (McCord et al., 2012) describes the modifiers of its parsed tokens, such as the object of a verb. Typically, a token and its modifier should either be jointly included in the CDC, or not included in it. This notion gave rise to several corresponding features. Parse Sentence Match: These are binary features that indicate whether the examined boundaries correspond to a sub-tree whose root is labeled ”S” (sentence) by the Stanford parser (Socher et al., 2013) or by the ESG parser (McCord et al., 2012), while parsing the entire surrounding sentence. ”that-conj” matches CDC: A binary feature indicating whether in the ESG parsing we have a subordinator ”that” token, whose corresponding covered text matches the examined boundaries. DigitCount: Counts the number of digits appearing in the sentence – before, within, and after the examined boundaries. UnbalancedQuotesOrParenthesis: Binary features, indicating whether there is an odd number of quote marks, or unbalanced parenthesis, within the examined boundaries. 4.3 Ranking Component The Ranking Compone"
C14-2002,W14-2109,1,0.791505,"Missing"
C14-2002,C14-1141,1,0.840609,"cussed in more detail elsewhere. Given a Topic, the Topic Analysis engine starts with initial semantic analysis of the Topic, aiming to identify the main concepts mentioned in this Topic and the sentiment towards each concept. Next, the CDC Oriented Article Retrieval engine employs IR and opinion mining techniques in order to retrieve Wikipedia articles that with high probability contain CDCs. Next, the CDC Detection engine relies on a combination of NLP and ML techniques to zoom-in within the retrieved articles and detect candidate CDCs. A detailed description of this engine can be found in (Levy et al 2014). Next, the CDC Pro/Con engine aims to automatically determine the polarity of the candidate CDC with respect to the given Topic by analyzing and contrasting the sentiment towards key concepts mentioned in the Topic and within the candidate CDC. Next, the CDC Equivalence engine uses techniques reminiscent of automatic paraphrase detection to identify whether two candidate CDCs are semantically equivalent, so to avoid redundancy in the generated output. Finally, the CDC Refinement engine aims to improve the precision of the generated output, based on the results collected thus far; e.g., using"
C18-1176,W14-2109,1,0.879467,"training DNNs to obtain significantly greater performance. Several works used DNN to tackle a variety of computational argumentation tasks, such as argument mining (Eger et al., 2017), predicting argument convincingness (Habernal and Gurevych, 2016), detecting context dependent claims and evidence (Laha and Raykar, 2016) and attack and support relations between arguments (Cocarascu and Toni, 2017). However, these works used the fully–supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult (Aharoni et al., 2014). In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals’ data, to develop a classifier for argumentative texts stored in these portals. To the best of our knowledge, the present work is the first to demonstrate the value of DNN trained solely with weak supervision (Hearst, 1992) in this challenging field. For a good exposition on the field of argument mining refer to (Lippi and Torroni, 2016). Some notable works include (Palau and Moens, 2009) who first suggested the argument mining task, (Levy et al., 2014; Rinott et al., 2015) who focused on min"
C18-1176,N16-1165,0,0.239192,"Missing"
C18-1176,D17-1144,0,0.0225426,"tion (ignoring the document context). The only work we are aware of that tackles corpus wide claim detection, is the work by (Levy et al., 2017). Here, we demonstrate how this work can be leveraged to define weak signals for training DNNs to obtain significantly greater performance. Several works used DNN to tackle a variety of computational argumentation tasks, such as argument mining (Eger et al., 2017), predicting argument convincingness (Habernal and Gurevych, 2016), detecting context dependent claims and evidence (Laha and Raykar, 2016) and attack and support relations between arguments (Cocarascu and Toni, 2017). However, these works used the fully–supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult (Aharoni et al., 2014). In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals’ data, to develop a classifier for argumentative texts stored in these portals. To the best of our knowledge, the present work is the first to demonstrate the value of DNN trained solely with weak supervision (Hearst, 1992) in this challenging field. For a good exposition on t"
C18-1176,P17-1002,0,0.0551209,"the retrieved documents via crowd-sourcing and to use these labels in order to build a supervised learning system that finds arguments in the given documents. Similar to our work, sentences are treated in isolation (ignoring the document context). The only work we are aware of that tackles corpus wide claim detection, is the work by (Levy et al., 2017). Here, we demonstrate how this work can be leveraged to define weak signals for training DNNs to obtain significantly greater performance. Several works used DNN to tackle a variety of computational argumentation tasks, such as argument mining (Eger et al., 2017), predicting argument convincingness (Habernal and Gurevych, 2016), detecting context dependent claims and evidence (Laha and Raykar, 2016) and attack and support relations between arguments (Cocarascu and Toni, 2017). However, these works used the fully–supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult (Aharoni et al., 2014). In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals’ data, to develop a classifier for argumentative texts store"
C18-1176,P11-1099,0,0.0341497,"llenging field. For a good exposition on the field of argument mining refer to (Lippi and Torroni, 2016). Some notable works include (Palau and Moens, 2009) who first suggested the argument mining task, (Levy et al., 2014; Rinott et al., 2015) who focused on mining claims/evidence in the context of a user given controversial topic and several works related to specific text genres such as student essays (Stab and Gurevych, 2014), legal documents (Wyner et al., 2010; Moens et al., 2007; Grabmair et al., 2015), user comments on proposed regulations (Park and Cardie, 2014) and newspaper articles (Feng and Hirst, 2011). 2068 3 Method 3.1 Setup and pre-processing We follow the setup and pre-processing described in (Levy et al., 2017) – see appendix for details. We consider the same train 3 and test sets, consisting of 100 and 50 topics respectively. Next, we prepared a sentence–level index from the Wikipedia May 2017 dump, and used a simple Wikification tool (to be described in a separate publication)4 to focus our attention on sentences that mention the MC. Filtering out sentences that mention a location/person named entity using Stanford NER (Finkel et al., 2005), after the MC, results in an average of ≈ 1"
C18-1176,P05-1045,0,0.225558,"and Cardie, 2014) and newspaper articles (Feng and Hirst, 2011). 2068 3 Method 3.1 Setup and pre-processing We follow the setup and pre-processing described in (Levy et al., 2017) – see appendix for details. We consider the same train 3 and test sets, consisting of 100 and 50 topics respectively. Next, we prepared a sentence–level index from the Wikipedia May 2017 dump, and used a simple Wikification tool (to be described in a separate publication)4 to focus our attention on sentences that mention the MC. Filtering out sentences that mention a location/person named entity using Stanford NER (Finkel et al., 2005), after the MC, results in an average of ≈ 10K sentences per MC. 3.2 Claim sentence queries and weak labels The basic query we start with, denoted qM C , only requires that the MC will appear in the sentence. For the 150 topics of this study, we retrieve a total of ≈ 1.5M sentences matching qM C (Table 4), which we release as a data set to enhance future research. Next, we consider the query qthat , which retrieves all sentences in which the token ‘that’ precedes the MC (cf. S1 and S2 in table 2). There are ≈ 1, 100 such sentences per topic (Table 4). Aiming to increase the prior of CS in the"
C18-1176,P16-1150,0,0.0636417,"ese labels in order to build a supervised learning system that finds arguments in the given documents. Similar to our work, sentences are treated in isolation (ignoring the document context). The only work we are aware of that tackles corpus wide claim detection, is the work by (Levy et al., 2017). Here, we demonstrate how this work can be leveraged to define weak signals for training DNNs to obtain significantly greater performance. Several works used DNN to tackle a variety of computational argumentation tasks, such as argument mining (Eger et al., 2017), predicting argument convincingness (Habernal and Gurevych, 2016), detecting context dependent claims and evidence (Laha and Raykar, 2016) and attack and support relations between arguments (Cocarascu and Toni, 2017). However, these works used the fully–supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult (Aharoni et al., 2014). In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals’ data, to develop a classifier for argumentative texts stored in these portals. To the best of our knowledge, the present work"
C18-1176,C92-2082,0,0.337315,"and support relations between arguments (Cocarascu and Toni, 2017). However, these works used the fully–supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult (Aharoni et al., 2014). In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals’ data, to develop a classifier for argumentative texts stored in these portals. To the best of our knowledge, the present work is the first to demonstrate the value of DNN trained solely with weak supervision (Hearst, 1992) in this challenging field. For a good exposition on the field of argument mining refer to (Lippi and Torroni, 2016). Some notable works include (Palau and Moens, 2009) who first suggested the argument mining task, (Levy et al., 2014; Rinott et al., 2015) who focused on mining claims/evidence in the context of a user given controversial topic and several works related to specific text genres such as student essays (Stab and Gurevych, 2014), legal documents (Wyner et al., 2010; Moens et al., 2007; Grabmair et al., 2015), user comments on proposed regulations (Park and Cardie, 2014) and newspape"
C18-1176,N13-1132,0,0.0784923,"Missing"
C18-1176,C16-1260,0,0.0557491,"in the given documents. Similar to our work, sentences are treated in isolation (ignoring the document context). The only work we are aware of that tackles corpus wide claim detection, is the work by (Levy et al., 2017). Here, we demonstrate how this work can be leveraged to define weak signals for training DNNs to obtain significantly greater performance. Several works used DNN to tackle a variety of computational argumentation tasks, such as argument mining (Eger et al., 2017), predicting argument convincingness (Habernal and Gurevych, 2016), detecting context dependent claims and evidence (Laha and Raykar, 2016) and attack and support relations between arguments (Cocarascu and Toni, 2017). However, these works used the fully–supervised learning paradigm, which is inherently demanding, especially in the context of argument mining where obtaining labeled data is notoriously difficult (Aharoni et al., 2014). In addition, Al-Khatib et al. (2016) used a distant supervision approach trained over debate portals’ data, to develop a classifier for argumentative texts stored in these portals. To the best of our knowledge, the present work is the first to demonstrate the value of DNN trained solely with weak su"
C18-1176,C14-1141,1,0.95524,"results in terms of both precision and coverage. Finally, we adapt our system to solve a recent argument mining task of identifying argumentative sentences in Web texts retrieved from heterogeneous sources, and obtain F1 scores comparable to the supervised baseline. 1 Introduction The arguments raised during a decision making process, will often determine its outcome. A common component in all argument models (e.g., (Toulmin, 2003)) is the claim, i.e. the assertion the argument aims to prove. The problem of automatically detecting claims supporting or contesting a given controversial topic 1 (Levy et al., 2014) is considered a fundamental task in the emerging field of computational argumentation (Lippi and Torroni, 2016; Palau and Moens, 2009). We refer to their definition of a Topic and a Claim; Topic - a short phrase that frames the discussion and Context Dependent Claim - a general, concise statement that directly supports or contests the given Topic (we henceforth use the term claim instead of Context Dependent Claim). Previous works have focused on detecting claims within a small set of documents related to the topic (Levy et al., 2014), or within documents enriched with argumentative content ("
C18-1176,W17-5110,1,0.178514,"ds an argumentative content search engine using weak supervision Ran Levy,∗ Ben Bogin∗, Shai Gretz∗, Ranit Aharonov, Noam Slonim IBM Research {ranl,boginb,avishaig,ranita,noams}@il.ibm.com Abstract Searching for sentences containing claims in a large text corpus is a key component in developing an argumentative content search engine. Previous works focused on detecting claims in a small set of documents or within documents enriched with argumentative content. However, pinpointing relevant claims in massive unstructured corpora, received little attention. A step in this direction was taken in (Levy et al., 2017), where the authors suggested using a weak signal to develop a relatively strict query for claim–sentence detection. Here, we leverage this work to define weak signals for training DNNs to obtain significantly greater performance. This approach allows to relax the query and increase the potential coverage. Our results clearly indicate that the system is able to successfully generalize from the weak signal, outperforming previously reported results in terms of both precision and coverage. Finally, we adapt our system to solve a recent argument mining task of identifying argumentative sentences"
C18-1176,W14-2105,0,0.0287045,"ith weak supervision (Hearst, 1992) in this challenging field. For a good exposition on the field of argument mining refer to (Lippi and Torroni, 2016). Some notable works include (Palau and Moens, 2009) who first suggested the argument mining task, (Levy et al., 2014; Rinott et al., 2015) who focused on mining claims/evidence in the context of a user given controversial topic and several works related to specific text genres such as student essays (Stab and Gurevych, 2014), legal documents (Wyner et al., 2010; Moens et al., 2007; Grabmair et al., 2015), user comments on proposed regulations (Park and Cardie, 2014) and newspaper articles (Feng and Hirst, 2011). 2068 3 Method 3.1 Setup and pre-processing We follow the setup and pre-processing described in (Levy et al., 2017) – see appendix for details. We consider the same train 3 and test sets, consisting of 100 and 50 topics respectively. Next, we prepared a sentence–level index from the Wikipedia May 2017 dump, and used a simple Wikification tool (to be described in a separate publication)4 to focus our attention on sentences that mention the MC. Filtering out sentences that mention a location/person named entity using Stanford NER (Finkel et al., 200"
C18-1176,D14-1162,0,0.0891036,"g ‘that’ following the MC 11,624 DN Npref MC → CL MC without a following CL token preceding the MC 132,856 Table 6: Characteristics of the two datasets used to train the networks. Note that the data for the suffix network is much smaller because of the restriction to sentences in which the token ‘that’ immediately precedes the MC. trained with a dropout of 0.15, using a single dropout mask across all time-steps as proposed by (Gal and Ghahramani, 2016), one LSTM layer with a cell size of 128, and an attention layer of size 100. Words are represented using the 300 dimensional GloVe embeddings (Pennington et al., 2014). Inference is performed for any qM C sentence by averaging the DN Nsuf f score of its suffix with the DN Npref score of its prefix. We used the heldout set to determine early stopping and to optimize the following hyper–parameters (each parameter was optimized independently): Number of layers (1/2), LSTM cell size (64/128/256/512), attention FF size (50/100/200) and dropout rate (0/0.05/0.1/0.15/0.2/0.25/0.3/0.35). 4 Data for evaluation We labeled via crowd the top 50 predicted sentences for each of the 50 test-set topics, taking the majority vote of at least 10 workers. The guidelines are pr"
C18-1176,D15-1050,1,0.935646,"Missing"
C18-1176,D14-1006,0,0.0903351,"is considered a fundamental task in the emerging field of computational argumentation (Lippi and Torroni, 2016; Palau and Moens, 2009). We refer to their definition of a Topic and a Claim; Topic - a short phrase that frames the discussion and Context Dependent Claim - a general, concise statement that directly supports or contests the given Topic (we henceforth use the term claim instead of Context Dependent Claim). Previous works have focused on detecting claims within a small set of documents related to the topic (Levy et al., 2014), or within documents enriched with argumentative content (Stab and Gurevych, 2014). However, pinpointing relevant claims within massive unstructured corpora, received relatively little attention. While this problem is obviously more challenging, its potential value is also much higher. For a widely discussed topic, one should expect many relevant claims to be mentioned across a widespread set of articles in the given corpus. The remaining issue is to develop a technology to swiftly detect these claims and present the results to potential users, similarly to search engines that retrieve information in response to a query. A step in this direction was taken in (Levy et al., 2"
C18-1176,W17-5106,0,0.101849,"relaxed query that only requires the MC to be mentioned in the sentence. Our results clearly indicate that both DNNs were able to generalize and obtain promising precision results, that are further improved when their scores are averaged. That is, combining the predictions of a DNN trained over prefixes of sentences enriched with claims, with those by a DNN trained over suffixes of such sentences, results in a pincer–movement like approach, that successfully pinpoints a wide range of CS in a massive unstructured corpus, while using only weak supervision for training. 2 Related Work Recently, Wachsmuth et al. (2017) suggested an argument search framework and a corresponding search engine prototype. However, the proposed system relies on arguments crawled from dedicated resources that suggest pre–written arguments for various topics, and hence, is only relevant for topics covered in these resources, and cannot be used directly over unstructured textual data. Stab et al. (2018) tackled the argument mining task in heterogeneous texts retrieved by Google search when queried with a controversial topic. They show that it is feasible to annotate the retrieved documents via crowd-sourcing and to use these labels"
C18-1176,N16-1174,0,0.0402432,"ional requirement of not having a CL token after the MC. Again, the corresponding DNN, denoted by DN Npref is trained only on the sentence prefixes. Table 5 lists examples of sentences in the weak–positive and weak–negative sets used to train the networks. The part “seen” by the relevant network appears in bold, where by an anecdotal examination it is indeed possible to identify a signal in the positive sets. Table 6 summarizes the characteristics of the two datasets used to train the networks. 3.3 DNN System For both DN Nsuf f and DN Npref , we use a Bi-LSTM architecture with self-attention (Yang et al., 2016). The networks were trained on sentences retrieved for 70 of the 100 train–set topics, where sentences retrieved from the other 30 train–set topics (heldout set) were used to optimize hyper-parameters. We used Adam optimizer (Kingma and Ba, 2014) over the cross-entropy loss. The best model was 3 The set of 100 topics was termed dev set in their work because there was no training involved. A Wikification tool allows retrieving sentences that mention the topic explicitly, as well as sentences which use a different surface form, as in S2 in Table 2 (wind energy surface-form linked to the wind pow"
C18-1189,S15-2102,0,0.0311678,"sition. 1 Available at http://www.research.ibm.com/haifa/dept/vst/debating_data.shtml 2231 3 Method Our method for learning sentiment composition lexicons comprises the following steps: 1. Train an n-gram sentiment classifier on a given sentiment lexicon for unigrams. 2. Use the sentiment classifier to automatically generate large sentiment lexicons of bigrams and unigrams. 3. Extract sentiment composition lexicons based on statistics from the bigram and unigram sentiment lexicons. The rest of the section describes each of the above steps. 3.1 The Sentiment Classifier Following previous work (Amir et al., 2015; Rothe et al., 2016; Bar-Haim et al., 2017b), we train a sentiment classifier on a sentiment lexicon, where the word’s polarity is taken to be the label, and the word embedding is the feature vector. We use the publicly-available sentiment lexicon of Hu and Liu (2004) (hereafter, HL). After removing 224 multi-word expressions, the lexicon contains 6,565 words. The above previous work focused on learning sentiment of unigrams, and optionally of multi-word expressions (conflated into single tokens), and used word2vec embeddings (Mikolov et al., 2013). In contrast, we are interested in learning"
C18-1189,E17-1024,1,0.886904,"rch.ibm.com/haifa/dept/vst/debating_data.shtml 2231 3 Method Our method for learning sentiment composition lexicons comprises the following steps: 1. Train an n-gram sentiment classifier on a given sentiment lexicon for unigrams. 2. Use the sentiment classifier to automatically generate large sentiment lexicons of bigrams and unigrams. 3. Extract sentiment composition lexicons based on statistics from the bigram and unigram sentiment lexicons. The rest of the section describes each of the above steps. 3.1 The Sentiment Classifier Following previous work (Amir et al., 2015; Rothe et al., 2016; Bar-Haim et al., 2017b), we train a sentiment classifier on a sentiment lexicon, where the word’s polarity is taken to be the label, and the word embedding is the feature vector. We use the publicly-available sentiment lexicon of Hu and Liu (2004) (hereafter, HL). After removing 224 multi-word expressions, the lexicon contains 6,565 words. The above previous work focused on learning sentiment of unigrams, and optionally of multi-word expressions (conflated into single tokens), and used word2vec embeddings (Mikolov et al., 2013). In contrast, we are interested in learning sentiment composition from bigrams, and the"
C18-1189,W17-5104,1,0.878681,"rch.ibm.com/haifa/dept/vst/debating_data.shtml 2231 3 Method Our method for learning sentiment composition lexicons comprises the following steps: 1. Train an n-gram sentiment classifier on a given sentiment lexicon for unigrams. 2. Use the sentiment classifier to automatically generate large sentiment lexicons of bigrams and unigrams. 3. Extract sentiment composition lexicons based on statistics from the bigram and unigram sentiment lexicons. The rest of the section describes each of the above steps. 3.1 The Sentiment Classifier Following previous work (Amir et al., 2015; Rothe et al., 2016; Bar-Haim et al., 2017b), we train a sentiment classifier on a sentiment lexicon, where the word’s polarity is taken to be the label, and the word embedding is the feature vector. We use the publicly-available sentiment lexicon of Hu and Liu (2004) (hereafter, HL). After removing 224 multi-word expressions, the lexicon contains 6,565 words. The above previous work focused on learning sentiment of unigrams, and optionally of multi-word expressions (conflated into single tokens), and used word2vec embeddings (Mikolov et al., 2013). In contrast, we are interested in learning sentiment composition from bigrams, and the"
C18-1189,D08-1083,0,0.0416319,"based on manual lists of negators, intensifiers, etc. (Wilson et al., 2005; Kennedy and Inkpen, 2006; Taboada et al., 2011). Moilanen and Pulman (2007) apply manually-composed syntactic rules for sentiment composition over the syntactic parse. Neviarouskaya et al. (2010) manually created a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not r"
C18-1189,J15-2004,0,0.0193741,"iment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for multiword phrases as well as their constituen"
C18-1189,I08-1039,0,0.0151699,"ap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for multiword phrases as well as their constituent words can be very useful in studying sentiment composition”. They manually developed such a lexicon for a few hundreds of bigrams and trigrams with opposing sentiments (each phra"
C18-1189,N16-1128,0,0.0604771,"fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for multiword phrases as well as their constituent words can be very useful in studying sentiment composition”. They manually developed such a lexicon for a few hundreds of bigrams and trigrams with opposing sentiments (each phrase has both negative and positive words), and experimented with both supervised and unsupervised methods for classifying and ranking the sentiment of these phrases. Their work has shown that rules based on part-of-speech patterns and unigram sentiment strengths are not sufficient for determining the sentiment of"
C18-1189,Q15-1016,0,0.13133,"sources. The size of this corpus is in the order of 1011 tokens. While word2vec is known as a scalable method for deriving word embeddings, applying it to a corpus of this size is still computationally expensive. We therefore opted for a more lightweight method for computing sentiment-oriented embeddings. Our method is inspired by the classical work of Turney and Littman (2003), who learned word sentiments based on their pointwise mutual information (PMI) with seed sentiment words. Our method represents each n-gram (unigram or bigram) as a 6,565 dimensional vector of its Positive PMI (PPMI) (Levy et al., 2015) with all the words in the HL lexicon. PPMI is defined for phrases u,v as P P M I(u, v) = max(0, log f (u, v) · N ), f (u) · f (v) (1) where f (u) and f (v) are the number of sentences that contain u and v in the corpus, respectively, f (u, v) is the number of sentences in which u and v co-occur, and N is the number of sentences in the corpus. u and v are said to co-occur if they are found in the same sentence, within a maximum distance of 10 tokens from each other, and have no overlap. We define P P M I(u, u) = 0 for any u. We convert these sparse word vectors into dense embeddings as follows"
C18-1189,N10-1120,0,0.034316,"e syntactic parse. Neviarouskaya et al. (2010) manually created a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016)"
C18-1189,C10-1091,0,0.0182608,"utomatically-generated sentiment lexicons for investigating sentiment composition phenomena. We made both these sentiment lexicons and the resulting composition lexicons publicly available, to facilitate further research on sentiment composition.1 2 Related Work Many of the previous works on sentiment analysis include some treatment of valence shifters, based on manual lists of negators, intensifiers, etc. (Wilson et al., 2005; Kennedy and Inkpen, 2006; Taboada et al., 2011). Moilanen and Pulman (2007) apply manually-composed syntactic rules for sentiment composition over the syntactic parse. Neviarouskaya et al. (2010) manually created a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-s"
C18-1189,L16-1431,0,0.0259199,"n of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for multiword phrases as well as their constituent words can be very useful in studying sentiment composition”. They manually developed such a lexicon for a few hundreds of bigrams and trigrams with opposing sentiments (each phrase has both negative and positiv"
C18-1189,N16-1091,0,0.0541874,"Missing"
C18-1189,I17-1063,0,0.0146749,"e, to facilitate further research on sentiment composition.1 2 Related Work Many of the previous works on sentiment analysis include some treatment of valence shifters, based on manual lists of negators, intensifiers, etc. (Wilson et al., 2005; Kennedy and Inkpen, 2006; Taboada et al., 2011). Moilanen and Pulman (2007) apply manually-composed syntactic rules for sentiment composition over the syntactic parse. Neviarouskaya et al. (2010) manually created a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed t"
C18-1189,D13-1170,0,0.0254717,"on which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for multiword phrases as well as their constituent words can be very useful in studying sentiment co"
C18-1189,J11-2001,0,0.141069,"essment. We also show their contribution to both phrase-level and sentence level sentiment analysis. Our results illustrate the value of our automatically-generated sentiment lexicons for investigating sentiment composition phenomena. We made both these sentiment lexicons and the resulting composition lexicons publicly available, to facilitate further research on sentiment composition.1 2 Related Work Many of the previous works on sentiment analysis include some treatment of valence shifters, based on manual lists of negators, intensifiers, etc. (Wilson et al., 2005; Kennedy and Inkpen, 2006; Taboada et al., 2011). Moilanen and Pulman (2007) apply manually-composed syntactic rules for sentiment composition over the syntactic parse. Neviarouskaya et al. (2010) manually created a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment com"
C18-1189,P15-1150,0,0.0399528,"conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for multiword phrases as well as their constituent words can be very useful in studying sentiment composition”. They ma"
C18-1189,H05-1044,0,0.097556,"omposition lexicons is confirmed via manual assessment. We also show their contribution to both phrase-level and sentence level sentiment analysis. Our results illustrate the value of our automatically-generated sentiment lexicons for investigating sentiment composition phenomena. We made both these sentiment lexicons and the resulting composition lexicons publicly available, to facilitate further research on sentiment composition.1 2 Related Work Many of the previous works on sentiment analysis include some treatment of valence shifters, based on manual lists of negators, intensifiers, etc. (Wilson et al., 2005; Kennedy and Inkpen, 2006; Taboada et al., 2011). Moilanen and Pulman (2007) apply manually-composed syntactic rules for sentiment composition over the syntactic parse. Neviarouskaya et al. (2010) manually created a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine lea"
C18-1189,D11-1016,0,0.0226843,"ated a lexicon with fine-grained categories for sentiment composition such as propagation and domination which may resolve conflicting sentiments. Schulder et al. (2017) applied an SVM classifier with linguistic features to bootstrap manual construction of a verbal polarity shifters lexicon. Some researchers, like Choi and Cardie (2008), combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields (Nakagawa et al., 2010), compositional matrix-space models (Yessenalina and Cardie, 2011), statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015). Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g. Ikeda et al. (2008), Noferesti and Shamsfard (2016). In contrast to previous methods, our method does not require sentiment-labeled texts or feature engineering. It also does not rely on manually-composed lexicons of negators, propagators or dominators, but rather learns such lexicons automatically. Kiritchenko and Mohammad (2016) wrote that “lexicons that include sentiment associations for mult"
D15-1050,W14-2107,0,0.0693973,"Missing"
D15-1050,P12-2041,0,0.15339,"ext columns indicate the number of articles in which at least one CDE was found; the total number of CDE detected for each type; the average percent of claims for which at least one CDE was found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in identifying arguments. In contrast, here, we focus on detecting an essen"
D15-1050,P05-1045,0,0.0257917,"ed a lexicon of words characterizing this type by looking at examples from the held-out data. This resulted with high–precision / low–recall lexicons. For example, for type Expert we used a lexicon of words describing persons and organizations that may have some relevant expertise, such as: economist, philosopher, court. In addition, we used the held-out data to automatically learn wider lexicons of words that are significantly associated with each type. All the in-house lexicons are described in detail in the supplementary material. • Named Entity Recognition (NER). We used the Stanford NER (Finkel et al., 2005) to extract named entities such as person and organization, and an in-house NER (Lally et al., 2012) to extract more fine grained categories such as ”educational organization” and ”leader”. • Patterns. We used regular expressions to represent features like: does that candidate contain a quote; does it contain a citation; does it contain numeric quantitative results. In addition, we generated complex regular expressions which combine the above lexicons with NER results to capture patterns indicative of different types. For example, the pattern [Person/organization, 0 to 10 wildcard words, an op"
D15-1050,W14-2106,0,0.0436803,"umber of articles in which at least one CDE was found; the total number of CDE detected for each type; the average percent of claims for which at least one CDE was found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in identifying arguments. In contrast, here, we focus on detecting an essential constituent of a"
D15-1050,W14-2105,0,0.23984,"luation of each type, topics that had less than three CDE of that type. This leaves a total of 30, 37, and 22 topics for types Expert, Study, and Anecdotal, respectively. The current work is the first to report results over these CDE data, which are more than 4 times larger compared to the data released in (Aharoni et al., 2014). These data are now freely available for research purposes 4 . 4 of the relevant Evidence type; and finally, of course, it should support the claim. In addition to these observations, we note that a priori, we do not expect all claims to be supported by all CDE types (Park and Cardie, 2014). For example, opinion claims like claim B in Table 1 are expected to be less supported by Study evidence compared to factual claims, like claim A in Table 1. Moreover, as evident from Table 2, many claims do not have any associated CDE in the same article. Thus, the system performance may naturally improve if it will propose candidate CDE of a particular type, only to an automatically identified subset of the input claims. Based on these observations, we are led to suggest an architecture which approaches CDED via a pipeline of modular components. Each of these components relies upon the resu"
D15-1050,W14-2112,0,0.0117282,"etermines the number of claims considered for each type. The next columns indicate the number of articles in which at least one CDE was found; the total number of CDE detected for each type; the average percent of claims for which at least one CDE was found; and for these claims, the average number of CDE found. Note that the total number of CDE is not a simple sum of the CDE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in i"
D15-1050,C14-1141,1,0.877203,"earlier defined in (Aharoni et al., 2014) and we use the same definitions here. Topic: a short phrase that frames the discussion. Claim: a general, concise statement that directly supports or contests the topic. Context Dependent Evidence (CDE): a text segment that directly supports a claim in the context of the topic. The first three rows of Table 1 show examples of a topic, a claim and CDE. For the purpose of this work, we assume that we are given a concrete topic, a relevant claim, and potentially relevant documents, provided either manually or by automatic methods (Cartright et al., 2011; Levy et al., 2014). Our task, which we term Context Dependent Evidence Detection (CDED), is to automatically pinpoint CDE within these documents. We further require that a detected CDE is reasonably well phrased, and easily understandable in the given context, so that it can be instantly and naturally used to support the claim in a discussion. Table 1 gives examples of valid CDE (V) and non-valid CDE (X) according to the definition mentioned above. It is well recognized that one can support a claim using different types of evidence (Rieke and Sillars, 2001; Seech, 2008). Furthermore, for different use cases, di"
D15-1050,P10-2062,0,0.0143555,"DE per type, as CDE can be assigned with more than one type. Standard deviations of distribution across topics are given in parenthesis where relevant. guments (Mochales Palau and Moens, 2009), analyzing argument structure (Peldszus, 2014), and identifying relations between arguments (Cabrio and Villata, 2012; Ghosh et al., 2014). Other works focused on specific domains such as evidence-based legal documents (Mochales Palau and Moens, 2011; Ashley and Walker, 2013), online debates (Cabrio and Villata, ˇ 2012; Boltuˇzi´c and Snajder, 2014), and product reviews (Villalba and Saint-Dizier, 2012; Yessenalina et al., 2010). In addition, some works based on machinelearning techniques, used the same topic in training and testing (Rosenfeld and Kraus, 2015; Boltuˇzi´c and ˇ Snajder, 2014), relying on features from the topic itself in identifying arguments. In contrast, here, we focus on detecting an essential constituent of an argument – the evidence – rather then detecting whole arguments, or detecting other argument parts like claims (Levy et al., 2014; Lippi and Torroni, 2015). In addition, we do not limit ourselves to a particular domain, nor assume that the topic of the discussion is known in advance. Finally"
D15-1050,W14-2109,1,\N,Missing
D17-1140,W14-2109,1,0.801017,"stimonies. For task (a) we performed two ablation tests, each of them yielded a decrease in performance: (i) not limiting the match of a pattern in a window (a decrease of 10.3 for P@5 and 2.8 for P@20), and (ii) not enforcing the order defined by the pattern (a decrease of 7.6 for P@5 and 2.8 for P@20). 4.2 Indirect Evaluation In this evaluation we add GrASP patterns as additional features to the full claim detection system of Levy et al. (2014) to inspect their contribution. This evaluation is performed on a second claim detection benchmark (on which they have reported results), released by Aharoni et al. (2014) (1,387 annotated claims associated with 33 topics). The system of Levy et al. (2014) is comprised of a cascade of three components; (i) detecting sentences which contain claims, (ii) identifying the exact boundaries of the claim part within the sentence, and (iii) ranking the claim candidates. Each of these components applies a classifier over dedicated features. Results were reported for the full cascade and for the first component, which is our task (a). For an idea on how to adapt GrASP for the claim boundaries detection task, see Section 5. Table 3 presents measures reported in Levy et al"
D17-1140,W03-1014,0,0.0778606,", and (ii) consider all attributes when searching for patterns. GrASP provides a framework to integrate information from different layers, choosing the best combination to produce highly expressive patterns. The alphabet of Hearst (1992) patterns is mainly stop words and noun-phrase tags, while Snow et al. (2004) add syntactic relations. Yangarber et al. (2000) consider a larger set of attributes (e.g., named entities, numeric expressions), however they commit to one generalization of each term. In contrast, we do not limit our alphabet and systematically consider all attributes of each term. Riloff and Wiebe (2003) start with a small set of syntactic templates, composed of a single syntactic relation and a single POS tag, to learn a variety of lexicalized patterns that match these templates. RAPIER (Califf and Mooney, 2003) constraints are similar to our attributes, but are basic (surface form, POS tag, and hypernyms only), and expanding them will exponentially increase its complexity. In contrast, adding attributes to GrASP only increment runtime linearly (see Section 3.2). To summarize, prior works usually have a basic alphabet and commit to one rule to generalize each term. Commonly, they do not allo"
D17-1140,D15-1050,1,0.83986,"grawal and Srikant, 1995) which are mainly used for data mining and rarely for unstructured text (Jindal and Liu, 2006). GrASP also has these characteristics, and in addition it can learn negative patterns, indicating that the examined text does not contain the target phenomenon. The phenomena we target are from the area of Argumentation Mining (see Lippi and Torroni (2016) for a survey). We focus on open-domain argument extraction. In this context, Levy et al. (2014) detect claims relevant to a debatable topic, Lippi and Torroni (2015) defined the contextindependent claim detection task, and Rinott et al. (2015) introduced the context dependent evidence detection task (which is further split into different types of evidence, e.g., a study that supports a claim or a relevant expert testimony). These tasks aim to capture a subtle and rare linguistic phenomenon within large corpora, hence are suitable for demonstrating the potential of GrASP. 3 The GrASP Algorithm The algorithm depicted in Algorithm 1. Its input is a set of positive and negative examples for the target phenomenon. The output is a ranked list of patterns, aiming to indicate the presence – or absence – of this phenomenon. In the following"
D17-1140,C92-2082,0,0.257109,"fferent argumentation mining tasks, we show that GrASP outperforms classical techniques, and boosts full argumentation mining systems when added to them. 2 Background While some aspects of GrASP were considered in the past, to the best of our knowledge, no prior work has presented a framework that allows users to: (i) easily add any type of attribute to the pattern alphabet, and (ii) consider all attributes when searching for patterns. GrASP provides a framework to integrate information from different layers, choosing the best combination to produce highly expressive patterns. The alphabet of Hearst (1992) patterns is mainly stop words and noun-phrase tags, while Snow et al. (2004) add syntactic relations. Yangarber et al. (2000) consider a larger set of attributes (e.g., named entities, numeric expressions), however they commit to one generalization of each term. In contrast, we do not limit our alphabet and systematically consider all attributes of each term. Riloff and Wiebe (2003) start with a small set of syntactic templates, composed of a single syntactic relation and a single POS tag, to learn a variety of lexicalized patterns that match these templates. RAPIER (Califf and Mooney, 2003)"
D17-1140,P82-1020,0,0.827901,"Missing"
D17-1140,A00-1039,0,0.171753,"on mining systems when added to them. 2 Background While some aspects of GrASP were considered in the past, to the best of our knowledge, no prior work has presented a framework that allows users to: (i) easily add any type of attribute to the pattern alphabet, and (ii) consider all attributes when searching for patterns. GrASP provides a framework to integrate information from different layers, choosing the best combination to produce highly expressive patterns. The alphabet of Hearst (1992) patterns is mainly stop words and noun-phrase tags, while Snow et al. (2004) add syntactic relations. Yangarber et al. (2000) consider a larger set of attributes (e.g., named entities, numeric expressions), however they commit to one generalization of each term. In contrast, we do not limit our alphabet and systematically consider all attributes of each term. Riloff and Wiebe (2003) start with a small set of syntactic templates, composed of a single syntactic relation and a single POS tag, to learn a variety of lexicalized patterns that match these templates. RAPIER (Califf and Mooney, 2003) constraints are similar to our attributes, but are basic (surface form, POS tag, and hypernyms only), and expanding them will"
D17-1140,D14-1181,0,0.002572,"terns: a baseline that reflects common practices in the literature where a pattern is a consecutive ordered list of stop words or POS tags. We add a symbol for topic match (for the context-dependent tasks). A brute force process generates all possible patterns up to size maxLen and selects top k by the same procedure as GrASP. 5 We did not examine the Anecdotal type due to the small size of the available benchmark data. 6 for more details see IBM Debating Technologies. For each task we report the best results obtained with k ∈ {50, 100, .., 400}. Convolutional Neural Network (CNN): following (Kim, 2014; Vinyals and Le, 2015) we used CNN whose input is a concatenation of the topic and the candidate.7 The final state vector is fed to a LR soft-max layer. Cross-entropy loss function was used for training. The embedding layer was initialized using word2vec vectors (Mikolov et al., 2013). Hyper-parameters were tuned on the same portion of the dataset as used by GrASP for tuning. For these baselines, we are not aware of available methods to incorporate GrASP multi-layered representation. GrASP alphabet: a simplified version of GrASP which uses the chosen alphabet, or “patterns” of length 1. This"
D17-1140,C14-1141,1,0.931497,"a 1 Get GrASP cloud service at http://ibm.biz/graspULP single element of the pattern. Such characteristics are presented in sequential patterns (Agrawal and Srikant, 1995) which are mainly used for data mining and rarely for unstructured text (Jindal and Liu, 2006). GrASP also has these characteristics, and in addition it can learn negative patterns, indicating that the examined text does not contain the target phenomenon. The phenomena we target are from the area of Argumentation Mining (see Lippi and Torroni (2016) for a survey). We focus on open-domain argument extraction. In this context, Levy et al. (2014) detect claims relevant to a debatable topic, Lippi and Torroni (2015) defined the contextindependent claim detection task, and Rinott et al. (2015) introduced the context dependent evidence detection task (which is further split into different types of evidence, e.g., a study that supports a claim or a relevant expert testimony). These tasks aim to capture a subtle and rare linguistic phenomenon within large corpora, hence are suitable for demonstrating the potential of GrASP. 3 The GrASP Algorithm The algorithm depicted in Algorithm 1. Its input is a set of positive and negative examples for"
D18-1078,W14-2107,0,0.120664,"Missing"
D18-1078,S18-2023,0,0.0404066,"Missing"
D18-1078,D16-1264,0,0.0396942,"debate speeches rather than by reading transcripts as done in prior work. The auditory modality is richer than written text in terms of the signal available to listeners, e.g., prosody. Similarly, machine comprehension can make use of the extra-lexical signal. The dataset we construct and release enables utilizing such signals, as done for instance in (Lippi and Torroni, 2016) for detecting claims in debates. Most often, in both reading and listening comprehension tasks, the answer is explicitly mentioned in the text; frequently, the answer is even an actual segment of the text, as in SQuAD (Rajpurkar et al., 2016), one of the most popular MRC datasets. Conversely, in argumentation, presuppositions are fundamental (Habernal et al., 2018), inferences are more subtle and the answer may rely on common knowledge. Going beyond the factoid level, Tseng et al. (2016) presented a listening comprehension task over TOEFL listening tests.1 In comparison, our data consists of spontaneous speech and is not adapted for non-native speakers. We use data from iDebate2 , a high-quality, curated database of controversial topics – referred to as “motions”, as in formal parliamentary proposals – with a list of arguments for"
D18-1078,N18-2017,0,0.0181061,"s refer to titles stems from entailment transitivity: under the observation that in our data an argument text typically entails its title, if an argument text is mentioned in the speech (i.e. is entailed by it), its title is also entailed by the speech (that is, given that typically arg ⇒ title, if speech ⇒ arg then speech ⇒ title.) Analysis of potential bias Recent works showed undesired artifacts in natural language inference datasets, namely that in some datasets for inferring relations between two texts, inference can in fact be done by only considering one of them (Schwartz et al., 2017; Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018). To explore this issue in our dataset, we assessed the correlation between several features of the argument title and the label. Specifically, we computed the Spearman’s correlation (Spearman, 1904) between the label and the title’s length, occurrences of named entities or negation words in the title, and the correlation between the labels and the titles’ 100 most frequent words, stopwords excluded. This resulted in low correlation coefficients, summarized in Table 1. This preliminary analysis suggests that naive features extracted from the title are not"
D18-1078,D13-1020,0,0.0169219,"nce of potential arguments in the speech. Labels were collected by listening to the speech and marking which arguments were mentioned by the speaker. We applied baseline methods addressing the task, to be used as a benchmark for future work over this dataset. All data used in this work is freely available for research. 1 Introduction Machine reading comprehension (MRC) is the NLP task equivalent to reading comprehension tests that assess the understanding of written texts by humans. MRC is usually realized as a question answering (QA) task through multiple-choice questions or as a cloze test (Richardson et al., 2013; Hermann et al., 2015; Hill et al., 2015). With the abundance of multimedia content nowadays, this line of work has been extended to speech, by applying QA methods to speech transcripts, i.e. the output of automatic speech recognition (ASR). In such works, the task is consequently referred to as ‘spoken question answering’ (Li et al., 2018), ‘question answering over speech transcripts’ (Turmo et al., 2007; Lamel et al., 2008) or machine listening comprehension (MLC) (Chung and Glass, 2018). We continue this line of work, and present a listening comprehension task and associated benchmark data"
D18-1078,N18-1175,0,0.0242049,"n terms of the signal available to listeners, e.g., prosody. Similarly, machine comprehension can make use of the extra-lexical signal. The dataset we construct and release enables utilizing such signals, as done for instance in (Lippi and Torroni, 2016) for detecting claims in debates. Most often, in both reading and listening comprehension tasks, the answer is explicitly mentioned in the text; frequently, the answer is even an actual segment of the text, as in SQuAD (Rajpurkar et al., 2016), one of the most popular MRC datasets. Conversely, in argumentation, presuppositions are fundamental (Habernal et al., 2018), inferences are more subtle and the answer may rely on common knowledge. Going beyond the factoid level, Tseng et al. (2016) presented a listening comprehension task over TOEFL listening tests.1 In comparison, our data consists of spontaneous speech and is not adapted for non-native speakers. We use data from iDebate2 , a high-quality, curated database of controversial topics – referred to as “motions”, as in formal parliamentary proposals – with a list of arguments for and against each motion. We selected 50 motions, and recorded experienced debaters making four speeches for each of them (tw"
D18-1078,K17-1004,0,0.01449,"the speech while labels refer to titles stems from entailment transitivity: under the observation that in our data an argument text typically entails its title, if an argument text is mentioned in the speech (i.e. is entailed by it), its title is also entailed by the speech (that is, given that typically arg ⇒ title, if speech ⇒ arg then speech ⇒ title.) Analysis of potential bias Recent works showed undesired artifacts in natural language inference datasets, namely that in some datasets for inferring relations between two texts, inference can in fact be done by only considering one of them (Schwartz et al., 2017; Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018). To explore this issue in our dataset, we assessed the correlation between several features of the argument title and the label. Specifically, we computed the Spearman’s correlation (Spearman, 1904) between the label and the title’s length, occurrences of named entities or negation words in the title, and the correlation between the labels and the titles’ 100 most frequent words, stopwords excluded. This resulted in low correlation coefficients, summarized in Table 1. This preliminary analysis suggests that naive features extracte"
D18-1078,L18-1239,0,0.029179,"rom entailment transitivity: under the observation that in our data an argument text typically entails its title, if an argument text is mentioned in the speech (i.e. is entailed by it), its title is also entailed by the speech (that is, given that typically arg ⇒ title, if speech ⇒ arg then speech ⇒ title.) Analysis of potential bias Recent works showed undesired artifacts in natural language inference datasets, namely that in some datasets for inferring relations between two texts, inference can in fact be done by only considering one of them (Schwartz et al., 2017; Gururangan et al., 2018; Tsuchiya, 2018; Poliak et al., 2018). To explore this issue in our dataset, we assessed the correlation between several features of the argument title and the label. Specifically, we computed the Spearman’s correlation (Spearman, 1904) between the label and the title’s length, occurrences of named entities or negation words in the title, and the correlation between the labels and the titles’ 100 most frequent words, stopwords excluded. This resulted in low correlation coefficients, summarized in Table 1. This preliminary analysis suggests that naive features extracted from the title are not sufficient for p"
D18-1078,L18-1037,1,0.506226,"seline methods applied to it as a benchmark dataset for the MLC task. The dataset includes 200 speeches for 50 motions, in English. For each speech we include the following: (i) the audio version of the speech; (ii) manual and automatic transcripts; (iii) a labeled listening comprehension question, consisting of a set of arguments from iDebate that potentially appear in the speech. Producing debate speeches We recorded argumentative speeches for each motion. First, two speeches supporting each motion were recorded by two experienced debaters. In doing so, we followed the process described in (Mirkin et al., 2018), where a speaker is presented with a motion and its description and is instructed to record a few minutes speech that supports it, with 10 minutes to prepare, but without checking any online materials. Given a speech supporting the motion, we asked another debater to listen to it and then record a speech rebutting it, and in consequence – opposing the motion. These response speeches are of different nature than the initial speeches beyond the opposite stance, as they often contain references and rebuttal to arguments mentioned in this initial speech. Through this process, we produced in total"
D18-1522,P14-2118,0,0.0600068,"Missing"
D18-1522,S12-1066,0,0.0296151,"Missing"
D18-1522,C16-1157,0,0.0280612,"Missing"
D18-1522,N16-1050,0,0.0357582,"Missing"
D18-1522,D14-1162,0,0.0812251,"cation models We hypothesize that words that share similar degree of abstractness tend to share certain similarities in their contextual usage; that, in contrast to concepts that exhibit opposite abstractness rate. Indeed, a statistical significance test applied to the (weak) positive and negative training data (Section 3.2) reveals markers such as {parish, movement, century, spiritual, life, doctrine, nature, regime} sharing excessive frequency in sentences containing abstract concepts. The very essence of this phenomenon is captured by distributed word representations (Mikolov et al., 2013; Pennington et al., 2014), a.k.a. word embeddings, learned based on the contextual usage of words. We therefore trained three classifiers, each exploiting different language properties, as described below. Naive Bayes (NB) Using solely word counts in textual data, we used a simple probabilistic Naive Bayes classifier, with a bag-of-words feature set extracted from the 800K sentences containing positive and negative training concepts. Given a sentence containing a test concept, its degree of abstractness was defined as the posterior probability assigned by the classifier. Aiming at robust classification, we retrieved 5"
D18-1522,N16-1091,0,0.14139,"Missing"
D18-1522,W13-0906,0,0.0559042,"Missing"
D18-1522,D11-1063,0,0.191578,"Missing"
D19-1561,P19-1097,1,0.666283,"c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics buttal responses. Given an argumentative text, the system would identify which claims from the GPR-KB are made in the text (explicitly or implicitly), and produce a rebuttal using the available counter-arguments. Clearly, many of the claims made in the text would not appear in the GPR-KB. The objective is therefore not to identify and rebut all arguments, but rather to identify and rebut some arguments, and construct a GPR-KB that facilitates that. Such a system (based on the more elaborate CoPA modeling of Bilu et al., 2019) was indeed implemented as a key element in IBM’s Project Debater rebuttal mechanism, and demonstrated during the live debate held between it and debating champion Harish Natarajan2 . However, a rebuttal system of this nature may be of interest beyond the realm of debating technologies. For example, such a system may be instrumental in making media consumption a more critical process, by automatically challenging the consumer with counterarguments. Similarly, it can be applicable in the education domain, stimulating critical thinking by prompting students with counter-arguments in response to"
D19-1561,P19-1255,0,0.0598121,"e datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based generative approach, and experiment with user-written posts. Our task differs in that the input is longer text, potentially containing multiple arguments. 3 3.1 Data Motions and Speeches The speeches analyzed in this work are the 200 speeches provided by Mirkin et al. (2018). Each speech debates one of 50 motions originating from iDebate. In this data, the phrasing of the motions is often simplified to include an explicit topic and action. For example, the iDebate motion This House would introduce goal line technology in football is simplified to We should"
D19-1561,P18-1021,0,0.101229,"s. Such works include datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based generative approach, and experiment with user-written posts. Our task differs in that the input is longer text, potentially containing multiple arguments. 3 3.1 Data Motions and Speeches The speeches analyzed in this work are the 200 speeches provided by Mirkin et al. (2018). Each speech debates one of 50 motions originating from iDebate. In this data, the phrasing of the motions is often simplified to include an explicit topic and action. For example, the iDebate motion This House would introduce goal line technology in football is simpl"
D19-1561,D18-1078,1,0.572019,"debaters need to combine specific knowledge about the topic at hand, with general arguments that arise from the underlying principles of the debate. Their ability to use such general arguments for different topics lays the basis for using a GPR-KB as the one described above. Accordingly, we asked an expert debater to create the initial GPR-KB by suggesting common claims and preparing matching rebuttals. The full process is detailed in §3.2. To assess the usefulness of the suggested claims and rebuttals in the real world, we performed several steps of labeling on the dataset we constructed in Mirkin et al. (2018), containing spoken argu2 Video of the debate at https://www.youtube. com/watch?v=m3u-1yttrVw mentative content discussing controversial topics. Details of this process, along with an analysis showing the high coverage obtained by our knowledge base, are described in §4. Another major challenge is the development of automatic methods for identifying whether knowledge-base claims are mentioned by speakers. We break this problem into a three-stage funnel – identifying whether: (i) a claim is relevant to the topic; (ii) the claim’s stance aligns with that of the speaker; (iii) the claim was made"
D19-1561,W15-0513,0,0.233128,"ying in between the two: on the one hand it allows for a response to speeches on a variety of topics; on the other, the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rather than taking turns, and the goal is to respond to some of the claims - but not necessarily all. In the context of computational argumentation much attention has been given to mapping rebuttal or disagreement among arguments. Such works include datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based generative approach, and experim"
D19-1561,D11-1054,0,0.0477381,"results. Second, while Mirkin et al. (2018) mention that the iDebate counter points can in principle be used for rebuttal, and Lavee et al. (2019) suggest mining opposing arguments from their corpus to counter arguments mentioned in speeches, pursuing both ideas is left for future work. We pick up the baton (in the context of the GPR-KB suggested here), and annotate the validity of the counter arguments as rebuttal to the ideas 5592 3 https://idebate.org/debatabase expressed in a matching speech. Response generation has been the subject of much research, using a wide variety of methods (e.g. Ritter et al., 2011; Sugiyama et al., 2013; Shin et al., 2015; Yan et al., 2016; Xing et al., 2017). In the context of dialog systems (see recent survey in Chen et al., 2017), there is usually a distinction between task-oriented systems (Wen et al., 2016) and open-domain ones (Mazar´e et al., 2018; Weizenbaum, 1966). The task here can be seen as lying in between the two: on the one hand it allows for a response to speeches on a variety of topics; on the other, the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of"
D19-1561,W15-4625,0,0.0746972,"the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rather than taking turns, and the goal is to respond to some of the claims - but not necessarily all. In the context of computational argumentation much attention has been given to mapping rebuttal or disagreement among arguments. Such works include datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based generative approach, and experiment with user-written posts. Our task differs in that the input is longer text, potentially containing multiple argume"
D19-1561,N15-1150,0,0.0178338,"mention that the iDebate counter points can in principle be used for rebuttal, and Lavee et al. (2019) suggest mining opposing arguments from their corpus to counter arguments mentioned in speeches, pursuing both ideas is left for future work. We pick up the baton (in the context of the GPR-KB suggested here), and annotate the validity of the counter arguments as rebuttal to the ideas 5592 3 https://idebate.org/debatabase expressed in a matching speech. Response generation has been the subject of much research, using a wide variety of methods (e.g. Ritter et al., 2011; Sugiyama et al., 2013; Shin et al., 2015; Yan et al., 2016; Xing et al., 2017). In the context of dialog systems (see recent survey in Chen et al., 2017), there is usually a distinction between task-oriented systems (Wen et al., 2016) and open-domain ones (Mazar´e et al., 2018; Weizenbaum, 1966). The task here can be seen as lying in between the two: on the one hand it allows for a response to speeches on a variety of topics; on the other, the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rather than taking tur"
D19-1561,P15-1012,0,0.0635176,"nse to speeches on a variety of topics; on the other, the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rather than taking turns, and the goal is to respond to some of the claims - but not necessarily all. In the context of computational argumentation much attention has been given to mapping rebuttal or disagreement among arguments. Such works include datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based generative approach, and experiment with user-written posts. Our task differs in that the i"
D19-1561,D18-1298,0,0.0689493,"Missing"
D19-1561,W13-4051,0,0.0198068,"le Mirkin et al. (2018) mention that the iDebate counter points can in principle be used for rebuttal, and Lavee et al. (2019) suggest mining opposing arguments from their corpus to counter arguments mentioned in speeches, pursuing both ideas is left for future work. We pick up the baton (in the context of the GPR-KB suggested here), and annotate the validity of the counter arguments as rebuttal to the ideas 5592 3 https://idebate.org/debatabase expressed in a matching speech. Response generation has been the subject of much research, using a wide variety of methods (e.g. Ritter et al., 2011; Sugiyama et al., 2013; Shin et al., 2015; Yan et al., 2016; Xing et al., 2017). In the context of dialog systems (see recent survey in Chen et al., 2017), there is usually a distinction between task-oriented systems (Wen et al., 2016) and open-domain ones (Mazar´e et al., 2018; Weizenbaum, 1966). The task here can be seen as lying in between the two: on the one hand it allows for a response to speeches on a variety of topics; on the other, the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rat"
D19-1561,P18-1023,0,0.0979617,"made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rather than taking turns, and the goal is to respond to some of the claims - but not necessarily all. In the context of computational argumentation much attention has been given to mapping rebuttal or disagreement among arguments. Such works include datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based generative approach, and experiment with user-written posts. Our task differs in that the input is longer text, potentially containing multiple arguments. 3 3.1 Data Motions and Speeches The speeches an"
D19-1561,walker-etal-2012-corpus,0,0.0356214,"here can be seen as lying in between the two: on the one hand it allows for a response to speeches on a variety of topics; on the other, the response is restricted to be a rebuttal of a claim made in the speech. A major difference from dialog systems is that in this task the analysis is of a complete speech - rather than taking turns, and the goal is to respond to some of the claims - but not necessarily all. In the context of computational argumentation much attention has been given to mapping rebuttal or disagreement among arguments. Such works include datasets exemplifying these relations (Walker et al., 2012; Peldszus and Stede, 2015a; Musi et al., 2017), modeling them (Sridhar et al., 2015) and explicitly detecting them (Rosenthal and McKeown, 2015; Peldszus and Stede, 2015b; Wachsmuth et al., 2018). The GPR-KB in this work is reminiscent of argument datasets that depict rebuttal relations, but the arguments are of a different type, being manually authored as general and applicable to a wide range of topics. Most similar to our work is the task of generating an argument from an opposing stance for a given statement (Hua and Wang, 2018; Hua et al., 2019). These works present a neural-based genera"
D19-1564,E17-1024,1,0.842345,"uggest neural methods based on a recently released language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods. 1 Introduction Computational argumentation has been receiving growing interest in the NLP community in recent years (Reed, 2016). With this field rapidly expanding, various methods have been developed for subtasks such as argument detection (Lippi and Torroni, 2016; Levy et al., 2014; Rinott et al., 2015), stance detection (Bar-Haim et al., 2017) and argument clustering (Reimers et al., 2019). Recently, IBM introduced Project Debater, the first AI system able to debate humans on complex topics. The system participated in a live debate against a world champion debater, and was able to mine arguments, use them for composing a speech supporting its side of the debate, and also rebut its human competitor.1 The underlying technology is intended to enhance decision-making. ∗ These authors equally contributed to this work. For more details: https://www.research. ibm.com/artificial-intelligence/ project-debater/live/ More recently, IBM also i"
D19-1564,P19-1093,1,0.816058,"convincingness – a primary dimension of quality – and determine it by comparing pairs of arguments with similar stance. In this view, the convincingness of an individual argument is a derivative of its relative convincingness: arguments that are judged as more convincing when compared to others are attributed higher scores. These works explore the labeling and automatic assessment of argument convincingness using two datasets introduced by Habernal and Gurevych (2016b): UKPConvArgRank (henceforth, UKPRank) and UKPConvArgAll, which contain 1k and 16k arguments and argumentpairs, respectively. Gleize et al. (2019) also take a relative approach to argument quality, focusing on ranking convincingness of evidence. Their solution is based on a Siamese neural network, which outperforms the results achieved in Simpson and Gurevych (2018) on the UKP datasets, as well as several baselines on their own dataset, IBM-ConvEnv.4 Here, we extend earlier work in several ways: (1) introducing a large dataset of actively collected arguments, carefully annotated for quality; (2) suggesting a method for argument-pair classification, which outperforms state-of-the-art accuracy on available datasets; (3) suggesting a metho"
D19-1564,D16-1129,0,0.409164,"ibutions. Assessing argument quality has driven practitioners in a plethora of fields for centuries from philosophers (Aristotle et al., 1991), through academic debaters, to argumentation scholars (Walton et al., 2008). An inherent difficulty in this domain is the presumably subjective nature of the task. Wachsmuth et al. (2017a) proposed a taxonomy of quantifiable dimensions of argument quality, comprised of high-level dimensions such as cogency and effectiveness, and sub-dimensions such as relevance and clarity, that together enable the assignment of a holistic quality score to an argument. Habernal and Gurevych (2016b) and Simpson 1 2 3 https://decide.madrid.es https://zencity.io 5625 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5625–5635, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and Gurevych (2018) take a relative approach and treat the problem as relation classification. They focus on convincingness – a primary dimension of quality – and determine it by comparing pairs of arguments with similar stance. In this view, the convincingness of"
D19-1564,P16-1150,0,0.469658,"ibutions. Assessing argument quality has driven practitioners in a plethora of fields for centuries from philosophers (Aristotle et al., 1991), through academic debaters, to argumentation scholars (Walton et al., 2008). An inherent difficulty in this domain is the presumably subjective nature of the task. Wachsmuth et al. (2017a) proposed a taxonomy of quantifiable dimensions of argument quality, comprised of high-level dimensions such as cogency and effectiveness, and sub-dimensions such as relevance and clarity, that together enable the assignment of a holistic quality score to an argument. Habernal and Gurevych (2016b) and Simpson 1 2 3 https://decide.madrid.es https://zencity.io 5625 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5625–5635, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and Gurevych (2018) take a relative approach and treat the problem as relation classification. They focus on convincingness – a primary dimension of quality – and determine it by comparing pairs of arguments with similar stance. In this view, the convincingness of"
D19-1564,N13-1132,0,0.0590221,"Missing"
D19-1564,C14-1141,1,0.754336,"se the labeled datasets to the community. Furthermore, we suggest neural methods based on a recently released language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods. 1 Introduction Computational argumentation has been receiving growing interest in the NLP community in recent years (Reed, 2016). With this field rapidly expanding, various methods have been developed for subtasks such as argument detection (Lippi and Torroni, 2016; Levy et al., 2014; Rinott et al., 2015), stance detection (Bar-Haim et al., 2017) and argument clustering (Reimers et al., 2019). Recently, IBM introduced Project Debater, the first AI system able to debate humans on complex topics. The system participated in a live debate against a world champion debater, and was able to mine arguments, use them for composing a speech supporting its side of the debate, and also rebut its human competitor.1 The underlying technology is intended to enhance decision-making. ∗ These authors equally contributed to this work. For more details: https://www.research. ibm.com/artifici"
D19-1564,D14-1162,0,0.0827148,"Missing"
D19-1564,P18-2124,0,0.0643158,"Missing"
D19-1564,D16-1264,0,0.0510485,"0 10 20 30 40 Argument Length in UKPRank 30 25 8.2 20 15 10 5 0 0 10 20 30 40 50 60 70 80 90 100 110 Figure 2: Histograms of argument length in IBMRank and UKPRank. X-axis: length (token count). Y-axis: the number of arguments at that length. classification of arguments. We devise two methods corresponding to the two newly introduced datasets. Our methods are based upon a powerful language representational model named Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) which achieves state-of-the-art results on a wide range of tasks in NLP (Wang et al. (2018), Rajpurkar et al. (2016, 2018)). BERT has been extensively trained over large corpora to perform two tasks: (1) Masked Language Model - randomly replace words with a predefined token, [MASK], and predict the missing word. (2) Next Sentence Prediction - given a pair of sentences A and B, predict whether sentence B follows sentence A. Due to its bidirectional nature, BERT achieves remarkable results when fine-tuned to different tasks without the need for specific modifications per task. For further details refer to Devlin et al. (2018). 8.1 Argument-Pair Classification We fine-tune BERT’s Base Uncased English pretrain"
D19-1564,W16-2800,0,0.077627,"rgument in each pair. In spite of the inherent subjective nature of the task, both annotation schemes led to surprisingly consistent results. We release the labeled datasets to the community. Furthermore, we suggest neural methods based on a recently released language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods. 1 Introduction Computational argumentation has been receiving growing interest in the NLP community in recent years (Reed, 2016). With this field rapidly expanding, various methods have been developed for subtasks such as argument detection (Lippi and Torroni, 2016; Levy et al., 2014; Rinott et al., 2015), stance detection (Bar-Haim et al., 2017) and argument clustering (Reimers et al., 2019). Recently, IBM introduced Project Debater, the first AI system able to debate humans on complex topics. The system participated in a live debate against a world champion debater, and was able to mine arguments, use them for composing a speech supporting its side of the debate, and also rebut its human competitor.1 The underlying t"
D19-1564,P19-1054,0,0.0247654,"ed language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods. 1 Introduction Computational argumentation has been receiving growing interest in the NLP community in recent years (Reed, 2016). With this field rapidly expanding, various methods have been developed for subtasks such as argument detection (Lippi and Torroni, 2016; Levy et al., 2014; Rinott et al., 2015), stance detection (Bar-Haim et al., 2017) and argument clustering (Reimers et al., 2019). Recently, IBM introduced Project Debater, the first AI system able to debate humans on complex topics. The system participated in a live debate against a world champion debater, and was able to mine arguments, use them for composing a speech supporting its side of the debate, and also rebut its human competitor.1 The underlying technology is intended to enhance decision-making. ∗ These authors equally contributed to this work. For more details: https://www.research. ibm.com/artificial-intelligence/ project-debater/live/ More recently, IBM also introduced Speech by Crowd, a service which supp"
D19-1564,D15-1050,1,0.876134,"sets to the community. Furthermore, we suggest neural methods based on a recently released language model, for argument ranking as well as for argument-pair classification. In the former task, our results are comparable to state-of-the-art; in the latter task our results significantly outperform earlier methods. 1 Introduction Computational argumentation has been receiving growing interest in the NLP community in recent years (Reed, 2016). With this field rapidly expanding, various methods have been developed for subtasks such as argument detection (Lippi and Torroni, 2016; Levy et al., 2014; Rinott et al., 2015), stance detection (Bar-Haim et al., 2017) and argument clustering (Reimers et al., 2019). Recently, IBM introduced Project Debater, the first AI system able to debate humans on complex topics. The system participated in a live debate against a world champion debater, and was able to mine arguments, use them for composing a speech supporting its side of the debate, and also rebut its human competitor.1 The underlying technology is intended to enhance decision-making. ∗ These authors equally contributed to this work. For more details: https://www.research. ibm.com/artificial-intelligence/ proje"
D19-1564,Q18-1026,0,0.356161,"vincingness: arguments that are judged as more convincing when compared to others are attributed higher scores. These works explore the labeling and automatic assessment of argument convincingness using two datasets introduced by Habernal and Gurevych (2016b): UKPConvArgRank (henceforth, UKPRank) and UKPConvArgAll, which contain 1k and 16k arguments and argumentpairs, respectively. Gleize et al. (2019) also take a relative approach to argument quality, focusing on ranking convincingness of evidence. Their solution is based on a Siamese neural network, which outperforms the results achieved in Simpson and Gurevych (2018) on the UKP datasets, as well as several baselines on their own dataset, IBM-ConvEnv.4 Here, we extend earlier work in several ways: (1) introducing a large dataset of actively collected arguments, carefully annotated for quality; (2) suggesting a method for argument-pair classification, which outperforms state-of-the-art accuracy on available datasets; (3) suggesting a method for individual argument ranking, which achieves results comparable to the state of the art. Our data was collected actively, via a dedicated user interface. This is in contrast to previous datasets, which were sampled fr"
D19-1564,C14-1142,0,0.0523637,"hance decision making on various topics. There are already several public organizations and commercial companies in this domain, e.g., Decide Madrid2 and Zencity.3 As part of the development of Speech by Crowd, 6.3k arguments were collected from contributors of various levels, and are released as part of this work. An important sub-task of such a service is the automatic assessment of argument quality, which has already shown its importance for prospective applications such as automated decision making (Bench-Capon et al., 2009), argument search (Wachsmuth et al., 2017b), and writing support (Stab and Gurevych, 2014). Identifying argument quality in the context of Speech by Crowd allows for the top-quality arguments to surface out of many contributions. Assessing argument quality has driven practitioners in a plethora of fields for centuries from philosophers (Aristotle et al., 1991), through academic debaters, to argumentation scholars (Walton et al., 2008). An inherent difficulty in this domain is the presumably subjective nature of the task. Wachsmuth et al. (2017a) proposed a taxonomy of quantifiable dimensions of argument quality, comprised of high-level dimensions such as cogency and effectiveness,"
D19-1564,E17-1017,0,0.0780182,"aim is to exploit the wisdom of the crowd to enhance decision making on various topics. There are already several public organizations and commercial companies in this domain, e.g., Decide Madrid2 and Zencity.3 As part of the development of Speech by Crowd, 6.3k arguments were collected from contributors of various levels, and are released as part of this work. An important sub-task of such a service is the automatic assessment of argument quality, which has already shown its importance for prospective applications such as automated decision making (Bench-Capon et al., 2009), argument search (Wachsmuth et al., 2017b), and writing support (Stab and Gurevych, 2014). Identifying argument quality in the context of Speech by Crowd allows for the top-quality arguments to surface out of many contributions. Assessing argument quality has driven practitioners in a plethora of fields for centuries from philosophers (Aristotle et al., 1991), through academic debaters, to argumentation scholars (Walton et al., 2008). An inherent difficulty in this domain is the presumably subjective nature of the task. Wachsmuth et al. (2017a) proposed a taxonomy of quantifiable dimensions of argument quality, comprised of high-lev"
D19-1564,W17-5106,0,0.0859178,"aim is to exploit the wisdom of the crowd to enhance decision making on various topics. There are already several public organizations and commercial companies in this domain, e.g., Decide Madrid2 and Zencity.3 As part of the development of Speech by Crowd, 6.3k arguments were collected from contributors of various levels, and are released as part of this work. An important sub-task of such a service is the automatic assessment of argument quality, which has already shown its importance for prospective applications such as automated decision making (Bench-Capon et al., 2009), argument search (Wachsmuth et al., 2017b), and writing support (Stab and Gurevych, 2014). Identifying argument quality in the context of Speech by Crowd allows for the top-quality arguments to surface out of many contributions. Assessing argument quality has driven practitioners in a plethora of fields for centuries from philosophers (Aristotle et al., 1991), through academic debaters, to argumentation scholars (Walton et al., 2008). An inherent difficulty in this domain is the presumably subjective nature of the task. Wachsmuth et al. (2017a) proposed a taxonomy of quantifiable dimensions of argument quality, comprised of high-lev"
D19-1564,W18-5446,0,0.0336968,"Missing"
D19-5102,D14-1148,0,0.160299,"events of interest; automatically classifying these events into types is probably of secondary importance to an expert in the field. Thus, our focus here is on a binary classification problem that is not type-based. This presents an interesting challenge, since the aim is not capturing the characteristics of predefined event types, but rather capturing general properties of relevant events. The common NLP approach for economic event extraction has mostly made use of hand-crafted rules and patterns (Feldman et al., 2011; Arendarenko and Kakkonen, 2012; Xie et al., 2013; Hogenboom et al., 2013; Ding et al., 2014, 2015; Du et al., 2016). However, creating and maintaining such rules is time consuming, and further seems less suitable for our scenario, where no set Extraction of financial and economic events from text has previously been done mostly using rule-based methods, with more recent works employing machine learning techniques. This work is in line with this latter approach, leveraging relevant Wikipedia sections to extract weak labels for sentences describing economic events. Whereas previous weakly supervised approaches required a knowledgebase of such events, or corresponding financial figures"
D19-5102,W18-3101,0,0.389105,"v Katz and Noam Slonim IBM Research, Haifa, Israel Abstract lighting meaningful company-related news events that are likely to deserve the analyst’s attention. Work on economic event extraction often defines an ad-hoc taxonomy of events, and what constitutes an ’important event’ for one might not be considered as such for another. For instance, the CoProE event ontology (Kakkonen and Mufti, 2011) includes events such as patent issuance and delayed filing of company reports, which are not considered by Du et al. (2016); similarly, while CoProE consider earnings estimates by analysts as events, Jacobs et al. (2018) examine instead analyst buy ratings and recommendations. Outlining a comprehensive list of event types seems futile. For example, if a company’s databases are hacked, this is certainly an influential event; but compiling an explicit and exhaustive event taxonomy that is sufficiently fine-grained to include all events such as this one is doomed to fail. At the same time, a formal event hierarchy is not necessarily required from an analyst’s perspective. The strength of an automated system comes from the ability to process a large volume of news data and detect events of interest; automatically"
D19-5102,P13-1086,0,0.566354,"ss a large volume of news data and detect events of interest; automatically classifying these events into types is probably of secondary importance to an expert in the field. Thus, our focus here is on a binary classification problem that is not type-based. This presents an interesting challenge, since the aim is not capturing the characteristics of predefined event types, but rather capturing general properties of relevant events. The common NLP approach for economic event extraction has mostly made use of hand-crafted rules and patterns (Feldman et al., 2011; Arendarenko and Kakkonen, 2012; Xie et al., 2013; Hogenboom et al., 2013; Ding et al., 2014, 2015; Du et al., 2016). However, creating and maintaining such rules is time consuming, and further seems less suitable for our scenario, where no set Extraction of financial and economic events from text has previously been done mostly using rule-based methods, with more recent works employing machine learning techniques. This work is in line with this latter approach, leveraging relevant Wikipedia sections to extract weak labels for sentences describing economic events. Whereas previous weakly supervised approaches required a knowledgebase of such"
D19-5905,W10-0713,0,0.121386,"Missing"
D19-5905,P16-1150,0,0.0618145,"Missing"
D19-5905,N10-1045,0,0.0334021,"ale of crowdsourcing platforms today has enabled the collection of large scale labeled datasets (Negri et al., 2011; Sabou et al., 2014; Rajpurkar et al., 2016, 2018; Choi et al., 2018). These datasets facilitate the use of advanced machine learning methods, which leverage such vast volumes of labeled data to achieve state-of-the-art performance on various tasks. Crowd annotation tasks are typically simple, short, and easy to explain, making them well-suited to the typically untrained temporary workforce. Some examples include named entity recognition (Finin et al., 2010), textual entailment (Mehdad et al., 2010) or generating facts ∗ 1 Demonstrated at Think 2019; https://www. youtube.com/watch?v=m3u-1yttrVw Current affiliation: Intuition Robotics 29 Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP, pages 29–38 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Topic: We should end water fluoridation Argumentative speech: We should continue fluoridating public water. Three arguments for this. The first is about why putting fluoride in the water is a public good. So recognize that tooth deca"
D19-5905,D18-1078,1,0.834398,"for NLU tasks, focuses only on annotations of single sentences or pairs of sentences, which tend to be simpler than those required in longer texts. However, task decomposition is not always feasible. As we discuss below, while a relevant decomposition scheme can be defined for our task, it does not allow performing the task in an effective and comprehensive way. We describe the adaptation of a complex labeling task to the crowd: identifying claims in spoken argumentative content (for an example, see Figure 1). This work extends our previous study, in which annotation was performed by experts (Mirkin et al., 2018). Obtaining such labeled data facilitates the development of language understanding systems which listen to speeches and identify claims therein. This, in turn, can serve as the basic building block for generating arguments rebutting these claims, or summarizing an argumentative text into the main claims made therein. Indeed, this annotation was made in the context of Project Debater, a system that can hold a debate with humans1 , where rebuttal was based on Argument Mining (Lavee et al., 2019) and general-purpose claims (Orbach et al., 2019). At first glance, simplifying such a task could see"
D19-5905,D11-1062,0,0.0260056,"that sentence-by-sentence annotation does not scale and that labeling only a subset of sentences is insufficient. Instead, we propose a scheme for effectively performing the full, complex task with crowd annotators, allowing the collection of large scale annotated datasets. We believe that the encountered challenges and pitfalls, as well as lessons learned, are relevant in general when collecting data for large scale natural language understanding (NLU) tasks. 1 Introduction The availability and scale of crowdsourcing platforms today has enabled the collection of large scale labeled datasets (Negri et al., 2011; Sabou et al., 2014; Rajpurkar et al., 2016, 2018; Choi et al., 2018). These datasets facilitate the use of advanced machine learning methods, which leverage such vast volumes of labeled data to achieve state-of-the-art performance on various tasks. Crowd annotation tasks are typically simple, short, and easy to explain, making them well-suited to the typically untrained temporary workforce. Some examples include named entity recognition (Finin et al., 2010), textual entailment (Mehdad et al., 2010) or generating facts ∗ 1 Demonstrated at Think 2019; https://www. youtube.com/watch?v=m3u-1yttr"
D19-5905,P12-2031,0,0.044229,"Missing"
D19-5905,D19-1561,1,0.532321,"tudy, in which annotation was performed by experts (Mirkin et al., 2018). Obtaining such labeled data facilitates the development of language understanding systems which listen to speeches and identify claims therein. This, in turn, can serve as the basic building block for generating arguments rebutting these claims, or summarizing an argumentative text into the main claims made therein. Indeed, this annotation was made in the context of Project Debater, a system that can hold a debate with humans1 , where rebuttal was based on Argument Mining (Lavee et al., 2019) and general-purpose claims (Orbach et al., 2019). At first glance, simplifying such a task could seem straightforward. By segmenting speeches Recent advancements in machine reading and listening comprehension involve the annotation of long texts. Such tasks are typically time consuming, making crowd-annotations an attractive solution, yet their complexity often makes such a solution unfeasible. In particular, a major concern is that crowd annotators may be tempted to skim through long texts, and answer questions without reading thoroughly. We present a case study of adapting this type of task to the crowd. The task is to identify claims in"
D19-5905,Q14-1025,0,0.0784247,"Missing"
D19-5905,P18-2124,0,0.0697817,"Missing"
D19-5905,D16-1264,0,0.0468087,"s not scale and that labeling only a subset of sentences is insufficient. Instead, we propose a scheme for effectively performing the full, complex task with crowd annotators, allowing the collection of large scale annotated datasets. We believe that the encountered challenges and pitfalls, as well as lessons learned, are relevant in general when collecting data for large scale natural language understanding (NLU) tasks. 1 Introduction The availability and scale of crowdsourcing platforms today has enabled the collection of large scale labeled datasets (Negri et al., 2011; Sabou et al., 2014; Rajpurkar et al., 2016, 2018; Choi et al., 2018). These datasets facilitate the use of advanced machine learning methods, which leverage such vast volumes of labeled data to achieve state-of-the-art performance on various tasks. Crowd annotation tasks are typically simple, short, and easy to explain, making them well-suited to the typically untrained temporary workforce. Some examples include named entity recognition (Finin et al., 2010), textual entailment (Mehdad et al., 2010) or generating facts ∗ 1 Demonstrated at Think 2019; https://www. youtube.com/watch?v=m3u-1yttrVw Current affiliation: Intuition Robotics 2"
D19-5905,sabou-etal-2014-corpus,0,0.0174305,"tence annotation does not scale and that labeling only a subset of sentences is insufficient. Instead, we propose a scheme for effectively performing the full, complex task with crowd annotators, allowing the collection of large scale annotated datasets. We believe that the encountered challenges and pitfalls, as well as lessons learned, are relevant in general when collecting data for large scale natural language understanding (NLU) tasks. 1 Introduction The availability and scale of crowdsourcing platforms today has enabled the collection of large scale labeled datasets (Negri et al., 2011; Sabou et al., 2014; Rajpurkar et al., 2016, 2018; Choi et al., 2018). These datasets facilitate the use of advanced machine learning methods, which leverage such vast volumes of labeled data to achieve state-of-the-art performance on various tasks. Crowd annotation tasks are typically simple, short, and easy to explain, making them well-suited to the typically untrained temporary workforce. Some examples include named entity recognition (Finin et al., 2010), textual entailment (Mehdad et al., 2010) or generating facts ∗ 1 Demonstrated at Think 2019; https://www. youtube.com/watch?v=m3u-1yttrVw Current affiliati"
D19-5905,W17-0803,0,0.0612054,"Missing"
D19-5905,W18-5446,0,0.228955,"of argumentative content annotation Tamar Lavee, Lili Kotlerman∗, Matan Orbach, Yonatan Bilu, Michal Jacovi, Ranit Aharonov and Noam Slonim IBM Research Abstract from text (Wang and Callison-Burch, 2010). Complex tasks are typically broken into smaller, simpler chunks to suit these requirements (Wang et al., 2013). For example, Zeichner et al. (2012) break up their evaluation of inference rules into three simpler sub-tasks, and Scholman and Demberg (2017) simplify their discourse relation annotation task by casting it as a selection of a connecting phrase from a predefined list. Indeed, GLUE (Wang et al., 2018), a popular benchmark for NLU tasks, focuses only on annotations of single sentences or pairs of sentences, which tend to be simpler than those required in longer texts. However, task decomposition is not always feasible. As we discuss below, while a relevant decomposition scheme can be defined for our task, it does not allow performing the task in an effective and comprehensive way. We describe the adaptation of a complex labeling task to the crowd: identifying claims in spoken argumentative content (for an example, see Figure 1). This work extends our previous study, in which annotation was"
D19-5905,W10-0725,0,0.0762826,"Missing"
E17-1024,I13-1191,0,0.112258,"e pros and cons of a given proposal are presented to the user. A notable research effort in this area is the IBM Debater® project whose goal is “to develop technologies that can assist humans to debate and ∗ 1 http://researcher.ibm.com/researcher/ view_group.php?id=5443 Present affiliation - Amazon. 251 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251–261, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013; Walker et al., 2012a; Sridhar et al., 2014), congressional floor debates (Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011), public comments on proposed regulations (Kwon et al., 2007), and student essays (Faulkner, 2014). Most of these works relied on both generic features such as sentiment, and topic-specific features learned from labeled data for a closed set of topics. Simple classifiers with unigram or ngram features are known to be hard to beat for these tasks (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). In addition to content-based feat"
E17-1024,D13-1171,0,0.0259598,"Missing"
E17-1024,J13-3004,0,0.0653959,"Missing"
E17-1024,S16-1003,0,0.243277,"Missing"
E17-1024,C14-1141,1,0.714104,"everal baselines, which represent the common practice of applying a single, monolithic classifier for stance classification. 1 (1) The sale of violent video games to minors should be banned , their system extracts, from corpora such as Wikipedia, Context-Dependent Claims (CDCs), defined as “general, concise statements that directly support or contest the given Topic”. A claim forms the basis of an argument, being the assertion that the argument aims to establish, and therefore claim detection may be viewed as a first step in automated argument construction. Recent research on claim detection (Levy et al., 2014; Lippi and Torroni, 2015) was facilitated by the IBM argumentative structure dataset (Aharoni et al., 2014), which contains manually collected claims for a variety of topics, as well as supporting evidence. In this work we introduce the related task of Claim Stance Classification: given a topic, and a set of claims extracted for it, determine for each claim whether it supports or contests the topic. Sorting extracted claims into Pro and Con would clearly improve the usability of both debating and decision support systems. We introduce the first benchmark for this task, by adding Pro/Con annot"
E17-1024,P15-2069,1,0.822785,"R(xc , xt ) = 1), stance is then predicted as sc × st , where st is the given topic Generating anchor candidates: Candidate anchors for measures (i)-(iv) are all single tokens. For our method, we additionally considered phrases as anchors. Candidates were generated from diverse sources, including the output of the ESG syntactic parser (McCord, 1990; McCord et al., 2012), the TagMe Wikifier (Ferragina and Scaiella, 2010), named entities recognized with the Stanford NER (Finkel et al., 2005) and multiword expressions in WordNet. Candidates subsumed by larger candidates were discarded. Following Levy et al. (2015), we kept only dominant terms with respect to the topic, by applying a statistical significance test (Hyper-geometric test with Bonferroni correction). Overall, our method detects many consistent and contrastive pairs missed by previous methods. 257 Configuration Baselines Unigrams SVM Unigrams+Sentiment SVM Our System Sentiment Score +Targeted Sentiment +Contrast Detection Our System+Unigrams SVM Accuracy@Coverage 0.4 0.5 0.6 0.7 0.1 0.2 0.3 0.688 0.717 0.688 0.717 0.659 0.717 0.612 0.709 0.587 0.693 0.563 0.691 0.752 0.770 0.849 0.784 0.720 0.770 0.847 0.758 0.720 0.770 0.836 0.749 0.720 0.7"
E17-1024,H05-1043,0,0.021089,"We manually composed a small lexicon of about 160 sentiment shifters. The scope was defined as the k tokens following the shifter word.9 Sentiment weighting and score computation: Following Ding et al., sentiment term weight decays based on its distance from the claim target. We used a weight of d−0.5 , where d is the distance in tokens between the sentiment term and the target. Let p and n be the weighted sums of positive Claim Target Identification Previous work on targeted/aspect-based sentiment analysis focused on detecting in user reviews sentiment towards products and their components (Popescu and Etzioni, 2005; Hu and Liu, 2004b), or considered only named entities as targets (Mitchell et al., 2013). Here we address a more general problem of open domain, generic target identification. Table 1 illustrates the diversity and complexity of claim targets. We set up the problem of claim target identification as a supervised learning problem, using an L2-regularized logistic regression classifier. Target candidates are the noun phrases in the claim, obtained from its syntactic parse6 . We create one training example from each such candidate phrase x and claim c in our training set. The feature set is summa"
E17-1024,D10-1102,0,0.0205644,"bater® project whose goal is “to develop technologies that can assist humans to debate and ∗ 1 http://researcher.ibm.com/researcher/ view_group.php?id=5443 Present affiliation - Amazon. 251 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251–261, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013; Walker et al., 2012a; Sridhar et al., 2014), congressional floor debates (Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011), public comments on proposed regulations (Kwon et al., 2007), and student essays (Faulkner, 2014). Most of these works relied on both generic features such as sentiment, and topic-specific features learned from labeled data for a closed set of topics. Simple classifiers with unigram or ngram features are known to be hard to beat for these tasks (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). In addition to content-based features, previous work also made use of various types of contextual information, such as agreement/disagreement between post"
E17-1024,D13-1170,0,0.00164134,"idate phrase x and claim c in our training set. The feature set is summarized in Table 2. Candidate phrases that exactly match the true target or overlap significantly with it are considered positive training examples, while the other candidates are considered negative examples. We measured overlap using the Jaccard similarity coefficient, defined as the ratio between the number of tokens in the intersection and the union of the two phrases, and considered an over7 Determined empirically based on the training set. Our sentiment analyzer was found to outperform the Stanford sentiment analyzer (Socher et al., 2013) on claims. 9 We experimentally set k = 8 based on the training data. 8 6 We used the ESG parser (McCord, 1990; McCord et al., 2012). 255 ally developed a small lexicon of stance flipping words, which largely overlaps with our sentiment shifters lexicon. We employ several relatedness measures, described in the next subsection, and the contrast scores obtained for these measures are used as features in the contrast classifier, implemented as a random forest classifier. The above approach can be extended to find the top-K anchor pairs for complex targets. We use K = 3 in our experiments. When co"
E17-1024,D12-1111,0,0.0410586,"Missing"
E17-1024,P09-1026,0,0.0162275,"subtasks. Most notably, we present a novel method for the challenging task of contrast detection. Empirical evaluation confirms that our modular approach outperforms several strong baselines that employ a single, monolithic classier. 2 Related Work Previous work on stance classification focused on analyzing debating forums (Somasundaran and 2 252 Termed same/alternative in their paper. used them in conjunction with discourse relations to improve the prediction of opinion polarity. However, these targets and relations were not automatically identified, but rather taken from a labeled dataset. Somasundaran and Wiebe (2009) considered debates comparing two products, such as Windows and Mac. In comparison, topics in our setting are not limited to product names, and the scope of contrast we address is far more general. Cabrio and Villata (2013) employ textual entailment to detect support/attack relations between arguments. However, as illustrated in Table 1, claims typically refer to the pros and cons of the topic target, but do not entail or contradict the topic. A recent related task is the SemEval 2016 tweets stance classification (Mohammad et al., 2016). In particular, in its weakly supervised subtask (Task B)"
E17-1024,W10-0214,0,0.198476,"a topic of interest, and decision support, where the pros and cons of a given proposal are presented to the user. A notable research effort in this area is the IBM Debater® project whose goal is “to develop technologies that can assist humans to debate and ∗ 1 http://researcher.ibm.com/researcher/ view_group.php?id=5443 Present affiliation - Amazon. 251 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251–261, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013; Walker et al., 2012a; Sridhar et al., 2014), congressional floor debates (Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011), public comments on proposed regulations (Kwon et al., 2007), and student essays (Faulkner, 2014). Most of these works relied on both generic features such as sentiment, and topic-specific features learned from labeled data for a closed set of topics. Simple classifiers with unigram or ngram features are known to be hard to beat for these tasks (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al."
E17-1024,D09-1018,0,0.0269135,"ctions is important. We explore this aspect in our evaluation. Consequently, our approach relies on generic sentiment analysis, rather than on topic or domainspecific features. We focus on precise semantic analysis of the debate topic and the claim, including target identification, and contrast detection between the claim and the topic targets. While sentiment analysis is a well-studied task, open-domain target identification and open-domain contrast detection between two given phrases have received little attention in previous work. Consistent/contrastive targets were previously discussed by Somasundaran et al. (2009) 2 , who speech. A Pro claim may support it by arguing in favor of free discussion, or alternatively by criticizing censorship. We say that freedom of speech and free discussion are consistent targets, while freedom of speech and censorship are contrastive. Accordingly, we suggest that claim stance classification can be reduced to simpler, more tractable sub-problems: 1. Identify the targets of the given topic and claim. 2. Identify the polarity (sentiment) towards each of the targets. 3. Determine whether the targets are consistent or contrastive. While our model seems intuitive, it was not c"
E17-1024,W14-2715,0,0.501487,"esented to the user. A notable research effort in this area is the IBM Debater® project whose goal is “to develop technologies that can assist humans to debate and ∗ 1 http://researcher.ibm.com/researcher/ view_group.php?id=5443 Present affiliation - Amazon. 251 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251–261, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013; Walker et al., 2012a; Sridhar et al., 2014), congressional floor debates (Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011), public comments on proposed regulations (Kwon et al., 2007), and student essays (Faulkner, 2014). Most of these works relied on both generic features such as sentiment, and topic-specific features learned from labeled data for a closed set of topics. Simple classifiers with unigram or ngram features are known to be hard to beat for these tasks (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). In addition to content-based features, previous work also made use of various"
E17-1024,W06-1639,0,0.0164881,"is area is the IBM Debater® project whose goal is “to develop technologies that can assist humans to debate and ∗ 1 http://researcher.ibm.com/researcher/ view_group.php?id=5443 Present affiliation - Amazon. 251 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251–261, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013; Walker et al., 2012a; Sridhar et al., 2014), congressional floor debates (Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011), public comments on proposed regulations (Kwon et al., 2007), and student essays (Faulkner, 2014). Most of these works relied on both generic features such as sentiment, and topic-specific features learned from labeled data for a closed set of topics. Simple classifiers with unigram or ngram features are known to be hard to beat for these tasks (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). In addition to content-based features, previous work also made use of various types of contextual information, such as agreement"
E17-1024,N12-1072,0,0.229741,"sion support, where the pros and cons of a given proposal are presented to the user. A notable research effort in this area is the IBM Debater® project whose goal is “to develop technologies that can assist humans to debate and ∗ 1 http://researcher.ibm.com/researcher/ view_group.php?id=5443 Present affiliation - Amazon. 251 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 251–261, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Wiebe, 2009; Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013; Walker et al., 2012a; Sridhar et al., 2014), congressional floor debates (Thomas et al., 2006; Yessenalina et al., 2010; Burfoot et al., 2011), public comments on proposed regulations (Kwon et al., 2007), and student essays (Faulkner, 2014). Most of these works relied on both generic features such as sentiment, and topic-specific features learned from labeled data for a closed set of topics. Simple classifiers with unigram or ngram features are known to be hard to beat for these tasks (Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Mohammad et al., 2016). In addition"
E17-1024,H05-2017,0,\N,Missing
E17-1024,P05-1045,0,\N,Missing
L18-1037,W17-5108,0,0.0364903,"Missing"
L18-1037,W17-5104,1,0.835409,"ng, automatic speech recognition 1. Introduction Computational argumentation and debating technologies aim to automate the extraction, understanding and generation of argumentative discourse. This field has seen a surge in research in recent years, and involves a variety of tasks, over various domains, including legal, scientific writing and education. Much of the focus is on argumentation mining, the detection of arguments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni ("
L18-1037,P16-2085,1,0.850648,"argumentation and debating technologies aim to automate the extraction, understanding and generation of argumentative discourse. This field has seen a surge in research in recent years, and involves a variety of tasks, over various domains, including legal, scientific writing and education. Much of the focus is on argumentation mining, the detection of arguments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth"
L18-1037,D15-1050,1,0.918299,"ce classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth et al. (2017b) have released an argument search engine over multiple debating websites, and Aker and Zhang (2017) have initiated the projection of some datasets to languages other than English, such as Chinese. All of the above are based on written texts, while datasets of spoken debates, outside of the political domain, are scarce. A spoken debate differs from a written essay or discussion not only in structure and content, but also in style as in any other case of spoken vs. written la"
L18-1037,W15-0509,0,0.0264885,"on, argumentation mining, automatic speech recognition 1. Introduction Computational argumentation and debating technologies aim to automate the extraction, understanding and generation of argumentative discourse. This field has seen a surge in research in recent years, and involves a variety of tasks, over various domains, including legal, scientific writing and education. Much of the focus is on argumentation mining, the detection of arguments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 20"
L18-1037,P17-2039,0,0.0117036,"argumentative discourse. This field has seen a surge in research in recent years, and involves a variety of tasks, over various domains, including legal, scientific writing and education. Much of the focus is on argumentation mining, the detection of arguments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth et al. (2017b) have released an argument search engine over multiple debating websites, and Aker and Z"
L18-1037,W17-5106,0,0.0198843,"argumentative discourse. This field has seen a surge in research in recent years, and involves a variety of tasks, over various domains, including legal, scientific writing and education. Much of the focus is on argumentation mining, the detection of arguments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth et al. (2017b) have released an argument search engine over multiple debating websites, and Aker and Z"
L18-1037,P16-2032,0,0.0263501,"action, understanding and generation of argumentative discourse. This field has seen a surge in research in recent years, and involves a variety of tasks, over various domains, including legal, scientific writing and education. Much of the focus is on argumentation mining, the detection of arguments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth et al. (2017b) have released an argument search engine ove"
L18-1037,N16-1017,0,0.01489,"i et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth et al. (2017b) have released an argument search engine over multiple debating websites, and Aker and Zhang (2017) have initiated the projection of some datasets to languages other than English, such as Chinese. All of the above are based on written texts, while datasets of spoken debates, outside of the political domain, are scarce. A spoken debate differs from a written essay or discussion not only in structure and content, but also in style as in any other case of spoken vs. written language. Zhang et al. (2016) made available transcripts from the Intelligence Squared1 debating television show2 . The transcripts of the show are available on the show’s site, and while they are of high quality, they do not match the audio recordings preci1 http://www.intelligencesquaredus.org http://www.cs.cornell.edu/˜cristian/ debates/ 2 sely, requiring substantial additional effort, if one wishes, for example, to use them as ASR training data. With this paper we release a dataset of 60 audio speeches, recorded specifically for debating research purposes. We describe in detail the process of producing these speeches"
L18-1037,P17-1144,0,0.0250504,"uments in text and their classification (Palau and Moens, 2009), but many other tasks are being addressed as well, including argument stance classification (Sobhani et al., 2015; Bar-Haim et al., 2017), the automatic generation of arguments (Bilu and Slonim, 2016), identification of persuasive arguments (Wei et al., 2016), quality assessment (Wachsmuth et al., 2017a) and more. Multiple datasets are available for such research, mostly in English, such as the Internet Argument Corpus (Walker et al., 2012), that consists of numerous annotated political discussions in internet forums, ArgRewrite (Zhang et al., 2017), a corpus of argumentative essay revisions, and the datasets released by IBM Research as part of the Debater Project (Rinott et al., 2015; Aharoni et al., 2014). Lippi and Torroni (2016) list several additional such datasets. Further, Wachsmuth et al. (2017b) have released an argument search engine over multiple debating websites, and Aker and Zhang (2017) have initiated the projection of some datasets to languages other than English, such as Chinese. All of the above are based on written texts, while datasets of spoken debates, outside of the political domain, are scarce. A spoken debate dif"
L18-1379,baccianella-etal-2010-sentiwordnet,0,0.112237,"Missing"
L18-1379,balahur-etal-2010-sentiment,0,0.20504,"ments, often with some treatment of compositional phenomena such as valence shifters (Polanyi and Zaenen, 2004) and mixed sentiment (Kiritchenko and Mohammad, 2016). Other approaches are based on bottom-up sentiment composition, starting at the word level and computing the sentiment of each phrase based on the semantics and sentiment of its daughter phrases, according to the syntactic structure of the sentence (Moilanen and Pulman, 2007; Nakagawa et al., 2010; Socher et al., 2013). Due to their non-compositionality, idioms are often not handled correctly by current sentiment analysis systems (Balahur et al., 2010). Word-level sentiment analysis would miss the positive sentiment in two thumbs up, and on the other hand, we might incorrectly assign positive sentiment to as well as, because of the positive sentiment of well. Similarly, we would like to know that it is not good if something bites the dust, while we would be happy to hear that our handing of idioms was dead on. Ignoring idioms overlooks an important signal of the sentiment of the text, as figurative and idiomatic language often directs sentence polarity (Rentoumi et al., 2012). For the above reasons, idioms have begun to receive some attenti"
L18-1379,esuli-sebastiani-2006-sentiwordnet,0,0.00996565,"g et al., 1994)). Here we focus on work related to building sentiment lexicons and the importance of handling idioms in sentiment classification. Available sentiment lexicons do not handle idiomatic expressions and focus almost entirely on unigrams. Manually curated lexicons such as the Harvard General Inquirer (Stone et al., 1966) or MPQA (Wilson et al., 2005) have hyphenated words but no idioms or MWEs. The lexicons created by early automatic approaches (Turney and Littman, 2003; Hu and Liu, 2004) deal with words but not longer n-grams. Approaches using WordNet (Miller, 1995), like those of Esuli and Sebastiani (2006) or Blair-Goldensohn et al. (2008), will include MWEs but WordNet has low coverage of idioms in our lexicon. Other graph-based approaches using distributionally similar n-grams (Velikovich et al., 2010) can return sentiment for MWEs, but the approach is sensitive to parameter tuning and there has been 2390 no evaluation of the quality of the MWE sentiment. Recently, Williams et al. (2015) released a sentiment lexicon with 580 idioms, but the selection of idioms focused on emotional idioms, some of which are not very frequent (e.g., they showed that more than a quarter were not found in the Bri"
L18-1379,N16-1128,0,0.0200794,"rocessing (Sag et al., 2002). Among MWEs, idioms are often defined as non-compositional multiword expressions, the meaning of which cannot be deduced from the literal meaning of constituent words (Nunberg et al., 1994). Sentiment analysis systems typically consider words as the basic sentiment units. Word sentiments are either learned from the training data or looked up in a sentiment lexicon. Text sentiment is then derived by means of aggregation over word sentiments, often with some treatment of compositional phenomena such as valence shifters (Polanyi and Zaenen, 2004) and mixed sentiment (Kiritchenko and Mohammad, 2016). Other approaches are based on bottom-up sentiment composition, starting at the word level and computing the sentiment of each phrase based on the semantics and sentiment of its daughter phrases, according to the syntactic structure of the sentence (Moilanen and Pulman, 2007; Nakagawa et al., 2010; Socher et al., 2013). Due to their non-compositionality, idioms are often not handled correctly by current sentiment analysis systems (Balahur et al., 2010). Word-level sentiment analysis would miss the positive sentiment in two thumbs up, and on the other hand, we might incorrectly assign positive"
L18-1379,D17-1124,0,0.0478159,"Missing"
L18-1379,N10-1120,0,0.0243055,"ntiments are either learned from the training data or looked up in a sentiment lexicon. Text sentiment is then derived by means of aggregation over word sentiments, often with some treatment of compositional phenomena such as valence shifters (Polanyi and Zaenen, 2004) and mixed sentiment (Kiritchenko and Mohammad, 2016). Other approaches are based on bottom-up sentiment composition, starting at the word level and computing the sentiment of each phrase based on the semantics and sentiment of its daughter phrases, according to the syntactic structure of the sentence (Moilanen and Pulman, 2007; Nakagawa et al., 2010; Socher et al., 2013). Due to their non-compositionality, idioms are often not handled correctly by current sentiment analysis systems (Balahur et al., 2010). Word-level sentiment analysis would miss the positive sentiment in two thumbs up, and on the other hand, we might incorrectly assign positive sentiment to as well as, because of the positive sentiment of well. Similarly, we would like to know that it is not good if something bites the dust, while we would be happy to hear that our handing of idioms was dead on. Ignoring idioms overlooks an important signal of the sentiment of the text,"
L18-1379,D08-1027,0,0.03099,"Missing"
L18-1379,D13-1170,0,0.0427615,"rned from the training data or looked up in a sentiment lexicon. Text sentiment is then derived by means of aggregation over word sentiments, often with some treatment of compositional phenomena such as valence shifters (Polanyi and Zaenen, 2004) and mixed sentiment (Kiritchenko and Mohammad, 2016). Other approaches are based on bottom-up sentiment composition, starting at the word level and computing the sentiment of each phrase based on the semantics and sentiment of its daughter phrases, according to the syntactic structure of the sentence (Moilanen and Pulman, 2007; Nakagawa et al., 2010; Socher et al., 2013). Due to their non-compositionality, idioms are often not handled correctly by current sentiment analysis systems (Balahur et al., 2010). Word-level sentiment analysis would miss the positive sentiment in two thumbs up, and on the other hand, we might incorrectly assign positive sentiment to as well as, because of the positive sentiment of well. Similarly, we would like to know that it is not good if something bites the dust, while we would be happy to hear that our handing of idioms was dead on. Ignoring idioms overlooks an important signal of the sentiment of the text, as figurative and idio"
L18-1379,N10-1119,0,0.032383,"sions and focus almost entirely on unigrams. Manually curated lexicons such as the Harvard General Inquirer (Stone et al., 1966) or MPQA (Wilson et al., 2005) have hyphenated words but no idioms or MWEs. The lexicons created by early automatic approaches (Turney and Littman, 2003; Hu and Liu, 2004) deal with words but not longer n-grams. Approaches using WordNet (Miller, 1995), like those of Esuli and Sebastiani (2006) or Blair-Goldensohn et al. (2008), will include MWEs but WordNet has low coverage of idioms in our lexicon. Other graph-based approaches using distributionally similar n-grams (Velikovich et al., 2010) can return sentiment for MWEs, but the approach is sensitive to parameter tuning and there has been 2390 no evaluation of the quality of the MWE sentiment. Recently, Williams et al. (2015) released a sentiment lexicon with 580 idioms, but the selection of idioms focused on emotional idioms, some of which are not very frequent (e.g., they showed that more than a quarter were not found in the British National Corpus). To address the lack of large-scale idiom sentiment lexicon, we manually annotated 5,000 of the most frequently used idioms, which is still feasible using crowdsourcing and avoids"
L18-1379,H05-1044,0,0.18396,"ring out idioms with swear words and was rarely used or needed. 7 Wiktionary: “ready; in a state of preparation or waiting; in position or anticipation.” 5. Related Work There is a wealth of literature on sentiment analysis ((Liu, 2012)) and idioms ((Nunberg et al., 1994)). Here we focus on work related to building sentiment lexicons and the importance of handling idioms in sentiment classification. Available sentiment lexicons do not handle idiomatic expressions and focus almost entirely on unigrams. Manually curated lexicons such as the Harvard General Inquirer (Stone et al., 1966) or MPQA (Wilson et al., 2005) have hyphenated words but no idioms or MWEs. The lexicons created by early automatic approaches (Turney and Littman, 2003; Hu and Liu, 2004) deal with words but not longer n-grams. Approaches using WordNet (Miller, 1995), like those of Esuli and Sebastiani (2006) or Blair-Goldensohn et al. (2008), will include MWEs but WordNet has low coverage of idioms in our lexicon. Other graph-based approaches using distributionally similar n-grams (Velikovich et al., 2010) can return sentiment for MWEs, but the approach is sensitive to parameter tuning and there has been 2390 no evaluation of the quality"
L18-1408,W16-2502,0,0.0543424,"Missing"
L18-1408,W16-2501,0,0.0349605,"Missing"
L18-1408,J90-1003,0,0.364079,"high quality large benchmark data. Here, we introduce a new dataset, named WORD, composed of 19, 276 Wikipedia concept pairs, manually annotated to determine their level of relatedness. We exploit these data to assess several types of state of the art semantic-relatedness tools, including word and document similarity functions (Mikolov et al., 2013; Gabrilovich and Markovitch, 2007), and link based methods (Witten and Milne, 2008; Ceccarelli et al., 2013), for the task of concept relatedness. Moreover, we suggest several new utilities, that are explicitly designed for this task, such as PMI (Church and Hanks, 1990; Bullinaria and Levy, 2007) measured between concepts based on their statistical co-occurrence in the entire Wikipedia corpus. We further exploit the data for supervised learning of concept-relatedness function, and use our novel utilities along with known state-of-the-art semantic-relatedness methods as features in a Linear Regression (LR) model. The resultant concept-relatedness tool, termed henceforth WORT, clearly outperforms each individual feature. Finally, to demonstrate the versatility of WORD, we suggest a mechanism for automatically generating a disambiguated term-relatedness datase"
L18-1408,W16-2506,0,0.0498797,"Missing"
L18-1408,Q15-1016,0,0.12244,"automatically generating a disambiguated term-relatedness dataset from a Wikipedia concept-relatedness dataset. This mechanism, which does not involve additional human annotation, yields a new term-relatedness dataset containing 37, 309 pairs. This dataset, WORD, and the annotation guidelines are all available in http://www.research.ibm.com/haifa/ dept/vst/debating_data.shtml. 2. Related Work Most existing semantic relatedness datasets are composed of pairs of words (e.g. (Finkelstein et al., 2002; Hill et al., 2014)). Nevertheless, dataets with other element types, such as multi-word terms (Levy et al., 2015b), knowledge base concepts (Ceccarelli et al., 2013) and documents (Lee and Welsh, 2005) are also available. From all term-relatedness datasets, TR9856 (Levy et al., 2015b) is probably the most similar to the current work, mainly due to its large size, and its generation process. The major difference between the two is in the type of elements composing them, Wikipedia concepts vs. terms, and thus in the relatedness task underlying their scores. Although part of the terms in TR9856 can be linked to concepts, the concepts are not available in the data, and were not provided to the annotators. A"
L18-1408,P15-2069,1,0.911348,"automatically generating a disambiguated term-relatedness dataset from a Wikipedia concept-relatedness dataset. This mechanism, which does not involve additional human annotation, yields a new term-relatedness dataset containing 37, 309 pairs. This dataset, WORD, and the annotation guidelines are all available in http://www.research.ibm.com/haifa/ dept/vst/debating_data.shtml. 2. Related Work Most existing semantic relatedness datasets are composed of pairs of words (e.g. (Finkelstein et al., 2002; Hill et al., 2014)). Nevertheless, dataets with other element types, such as multi-word terms (Levy et al., 2015b), knowledge base concepts (Ceccarelli et al., 2013) and documents (Lee and Welsh, 2005) are also available. From all term-relatedness datasets, TR9856 (Levy et al., 2015b) is probably the most similar to the current work, mainly due to its large size, and its generation process. The major difference between the two is in the type of elements composing them, Wikipedia concepts vs. terms, and thus in the relatedness task underlying their scores. Although part of the terms in TR9856 can be linked to concepts, the concepts are not available in the data, and were not provided to the annotators. A"
L18-1408,N15-1058,0,0.0235935,"Missing"
P15-2069,W14-2109,1,0.886757,"Missing"
P15-2069,J90-1003,0,0.484539,"SWT) and 8, 367 were comprised of at least one 421 Term 1 copyright grand theft auto video games sales civil rights rights nation of islam racial Term 2 wipo violent video games violent video games affirmative action public property affirmative action sex discrimination Score 1.0 1.0 5 Results for existing techniques To demonstrate the usability of the new TR9856 data, we present baseline results of commonly used methods that can be exploited to predict term relatedness, including ESA (Gabrilovich and Markovitch, 2007), Word2Vec (W2V) (Mikolov et al., 2013) and first–order positive PMI (PMI) (Church and Hanks, 1990). To handle MWTs, we used summation on the vector representations of W2V and ESA. For PMI, we tokenized each MWT and averaged the PMI of all possible single–word pairs. For all these methods we used the March 2015 Wikipedia dump and a relatively standard configuration of the relevant parameters. In addition, we report results for an ensemble of these methods using 10-fold cross validation. 0.7 0.6 0.5 0.1 0.1 Table 1: Examples of pairs of terms and their associated dataset scores. 5.1 Evaluation measures Previous experiments on WS353 and other datasets reported Spearman Correlation (ρ) between"
P15-2069,D08-1027,0,0.0380941,"Missing"
P15-2069,J15-4004,0,0.121791,"Missing"
P15-2069,P12-1092,0,0.0554762,"://www.research.ibm.com/haifa/ dept/vst/mlta_data.shtml 419 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 419–424, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 3.1 man results were averaged, to obtain a relatedness score for each pair. Other relatively small datasets include (Radinsky et al., 2011; Halawi et al., 2012; Hill et al., 2014). A larger dataset is Stanford’s Contextual Word Similarities dataset, denoted SCWS (Huang et al., 2012) with 2,003 word pairs, where each word appears in the context of a specific sentence. The authors rely on Wordnet (Fellbaum, 1998) for choosing a diverse set of words as well as to enrich the dataset with related pairs. A more recent dataset, denoted MEN (Bruni et al., 2014) consists of 3,000 word pairs, where a specific relatedness measure was used to enrich the data with related pairs. Thus, these two larger datasets are potentially biased in favor of the relatedness algorithm or lexical resource used in their development. TR9856 is much larger and potentially less biased than all these pre"
P15-2069,E14-1057,0,0.0878078,"Missing"
P15-2069,E09-1064,0,\N,Missing
P16-2085,P98-1116,0,0.056679,"Missing"
P16-2085,C14-1141,1,0.815006,"), where applications such as recommender systems synthesize arguments to explain their recommendations, as done for example in (Carenini and Moore, 2006) . These approaches yield good results when applied to specific domains. In an NLG application, there is commonly a specific knowledge base which the system communicates. The form and content of arguments are derived and determined by it and are thus limited to the knowledge therein. Similarly, argument mining works well when an argument-rich and topic-related corpus is available - e.g. (Wyner et al., 2010) - but in general seems to be hard (Levy et al., 2014). Thus, it is interesting and challenging to synthesize arguments in an open domain. To the best of our knowledge, this is the first work that directly attempts to address this task. Modeling of arguments goes back to the ancient Greeks and Aristotle, and more modern work starting perhaps most famously with the Toulmin argument model (Toulmin, 1958). A common element in all such models is the claim (or conclusion) being forwarded by the argument. Thus, a natural first step in synthesizing arguments in a general setting is being able to synthesize claims in such a setting. We suggest here a sim"
P16-2085,D15-1050,1,0.908638,"Missing"
P16-2085,W09-0613,0,\N,Missing
P16-2085,C98-1112,0,\N,Missing
P18-2009,P09-3012,0,0.0759851,"Missing"
P18-2009,P15-1150,0,0.0167195,"pular triplet loss suggested in (Schroff et al., 2015). Finally, we use Adam optimizer with initial learning rate of 0.001. Given a sentence s, Net(s) provides a sentence embedding of dimension 600. 5 5.1 accuracy 0.65 0.615 0.547 0.74 Experiments Reconstructing Article Sections As mentioned, our main objective task is clustering sentences into subtopics. As a preliminary step, we first evaluate our method on the tripletsen test set. We compare the model trained on triplet-sen to two well known methods. The first, mean-vectors, is simply the mean of the GloVe embeddings of the sentence words (Tai et al., 2015), which is considered a strong unsupervised baseline. The second, skip-thoughts (Ryan Kiros, 2015), is among the state-of-the-art unsupervised models for semantic similarity, and the most popular multi-purpose embedding method. We address two versions of skip-thoughts: one is based on the original 4800-dimensional vectors (skipthoughts-cs), and the other, skip-thoughts-SICK, is based on the similarity function learned from the SICK semantic similarity dataset, as described in (Ryan Kiros, 2015). The aim of assessing skipthoughts-SICK is to examine how well a state-ofthe-art semantic similarity"
P18-2009,W00-0405,0,0.143235,"matic relatedness between sentences. 3 Sentence Triplets 1. McDonnell resigned from Martin in 1938 and founded McDonnell Aircraft Corporation in 1939 2. In 1967, McDonnell Aircraft merged with the Douglas Aircraft Company to create McDonnell Douglas 3. Born in Denver, Colorado, McDonnell was raised in Little Rock, Arkansas, and graduated from Little Rock High School in 1917 Table 1: Example of a section-sen triplet from the article ‘James Smith McDonnell’. The first two sentences are from the section ’Career’ and the third is from ‘Early life’ In use-cases such as multi-document summarization(Goldstein et al., 2000), one often needs to organize sentences originating from different documents. Such sentences tend to be standalone sentences, that do not contain the syntactic cues that often exist between adjacent sentences (e.g. co-references, discourse markers etc.). Correspondingly, to focus our weakly labeled data on sentences that are typically stand-alone in nature, we consider only paragraph opening sentences. An essential part of learning using triplets, is the mining of difficult examples, that prevent quick stagnation of the network (Hermans et al., 2017). Since sentences in the same article essent"
P18-2009,N16-1174,0,0.0298141,"sters to the correct number. Finally, we use standard agreement measures, MI, Adjusted MI (AMI) (Vinh et al., 2009), Rand Index (RI) and Adjusted Rand Index (ARI) (Rand, 1971), to quantify the agreement between the ground truth and the clustering results. As exhibited in Table 4, our models significantly outperform the two other methods for both clustering algorithms, where the best performance is achieved by the concatenated representations (triplets-sen-titles), suggesting the two models, Net is composed of a Bi-directional LSTM with hidden size 300 and 0.8 dropout followed by an attention (Yang et al., 2016) layer of size 200. The input to Net are the pre-trained glove word embeddings of 300d trained on 840B tokens (Pennington et al., 2014). For dist and the loss function we use the L1 distance, which we found to yield better results than L2 and cosine-similarity. The selected loss function outperformed the popular triplet loss suggested in (Schroff et al., 2015). Finally, we use Adam optimizer with initial learning rate of 0.001. Given a sentence s, Net(s) provides a sentence embedding of dimension 600. 5 5.1 accuracy 0.65 0.615 0.547 0.74 Experiments Reconstructing Article Sections As mentioned"
P18-2009,W17-5110,1,0.716167,"it Aharonov and Noam Slonim IBM Research, Haifa, Israel {liate,yosimass,alonhal,eladv,ilyashn,ranita,noams}@il.ibm.com Abstract to the related task of clustering sentences that represent paraphrases of the same core statement. Thematic clustering is important for various use cases. For example, in multi-document summarization, one often extracts sentences from multiple documents that have to be organized into meaningful sections and paragraphs. Similarly, within the emerging field of computational argumentation (Lippi and Torroni, 2016), arguments may be found in a widespread set of articles (Levy et al., 2017), which further require thematic organization to generate a compelling argumentative narrative. We approach the problem of thematic clustering by developing a dedicated sentence similarity measure, targeted at a comparative task – Thematic Distance Comparison (TDC): given a pivot sentence, and two other sentences, the task is to determine which of the two sentences is thematically closer to the pivot. By training a deep neural network (DNN) to perform TDC, we are able to learn a thematic similarity measure. Obtaining annotated data for training the DNN is quite demanding. Hence, we exploit the"
P18-2009,S14-2001,0,0.0629249,"Missing"
P18-2009,D14-1162,0,0.0927079,"RI) and Adjusted Rand Index (ARI) (Rand, 1971), to quantify the agreement between the ground truth and the clustering results. As exhibited in Table 4, our models significantly outperform the two other methods for both clustering algorithms, where the best performance is achieved by the concatenated representations (triplets-sen-titles), suggesting the two models, Net is composed of a Bi-directional LSTM with hidden size 300 and 0.8 dropout followed by an attention (Yang et al., 2016) layer of size 200. The input to Net are the pre-trained glove word embeddings of 300d trained on 840B tokens (Pennington et al., 2014). For dist and the loss function we use the L1 distance, which we found to yield better results than L2 and cosine-similarity. The selected loss function outperformed the popular triplet loss suggested in (Schroff et al., 2015). Finally, we use Adam optimizer with initial learning rate of 0.001. Given a sentence s, Net(s) provides a sentence embedding of dimension 600. 5 5.1 accuracy 0.65 0.615 0.547 0.74 Experiments Reconstructing Article Sections As mentioned, our main objective task is clustering sentences into subtopics. As a preliminary step, we first evaluate our method on the tripletsen"
P18-2095,W14-2109,1,0.941912,"Missing"
P18-2095,D15-1255,0,0.037571,"pic-dependent claim detection task, Levy et al. (2017) showed that retrieving sentences with the word that followed by the concept representing the topic, yields candidates that are more likely to contain a claim for that topic than arbitrary sentences which contain the topic concept. 2.2 We present BlendNet, a neural network that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a pre"
P18-2095,N16-1165,0,0.199166,"Missing"
P18-2095,N18-1175,0,0.119584,"a during the training of neural networks. Experiments in the context of topic-dependent evidence detection with two forms of weak labeled data show the advantages of the blending scheme. In addition, we provide a manually annotated data set for the task of topicdependent evidence detection. 1 Introduction In recent years, neural networks have been widely used for natural language understanding tasks. Such networks demand a considerable amount of labeled data for each specific task. However, for many tasks, the process of obtaining high quality labeled data is slow, expensive, and complicated (Habernal et al., 2018). In this work, we propose a method for improving network training when a small amount of labeled data is available. Several works have suggested methods for generating weak labeled data (WLD) whose quality for the task of interest is low, but that can be easily obtained. One approach for gathering WLD is to apply heuristics to a large corpus. For example, Hearst (1992) considered a noun to be the hypernym of another noun if they are connected by the is a pattern in a sentence. 2 2.1 Background WLD and networks In the field of neural networks, WLD has mainly been employed for pre-training netw"
P18-2095,W10-2914,0,0.0575909,"lending Weak and Strong Labeled Data in a Neural Network for Argumentation Mining Eyal Shnarch, Carlos Alzate, Lena Dankin, Martin Gleize, Yufang Hou, Leshem Choshen, Ranit Aharonov, Noam Slonim IBM Research {eyals, lenad, leshem.choshen, ranita, noams}@il.ibm.com {carlos.alzate, martin.gleize, yhou}@ie.ibm.com Abstract Distant supervision is another form of WLD used in various tasks such as relation extraction (Mintz et al., 2009; Surdeanu et al., 2012) and sentiment analysis (Go et al., 2009). Other works use emojis or hashtags as weak labels describing the texts in which they appear (e.g., Davidov et al. (2010) in the context of sarcasm detection). WLD can be freely obtained, however it comes with a price: it is often very noisy. Therefore, systems trained only on WLD are at a serious disadvantage compared to systems trained on high quality labeled data, which we term henceforth strong labeled data (SLD). However, we suggest that the easily accessible WLD is still useful when used alongside SLD, which is naturally limited in size. In this work we propose a method for blending WLD and SLD in the training of neural networks. Focusing on the argumentation mining field, we create and release a data set"
P18-2095,C92-2082,0,0.0884734,"e understanding tasks. Such networks demand a considerable amount of labeled data for each specific task. However, for many tasks, the process of obtaining high quality labeled data is slow, expensive, and complicated (Habernal et al., 2018). In this work, we propose a method for improving network training when a small amount of labeled data is available. Several works have suggested methods for generating weak labeled data (WLD) whose quality for the task of interest is low, but that can be easily obtained. One approach for gathering WLD is to apply heuristics to a large corpus. For example, Hearst (1992) considered a noun to be the hypernym of another noun if they are connected by the is a pattern in a sentence. 2 2.1 Background WLD and networks In the field of neural networks, WLD has mainly been employed for pre-training networks. This was done in related fields such as information retrieval (Dehghani et al., 2017b) and sentiment analysis (Severyn and Moschitti, 2015; Deriu et al., 2017). Contrary to those works, we ex599 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 599–605 c Melbourne, Australia, July 15 - 20, 2018. 2018 Asso"
P18-2095,C14-1141,1,0.882697,"twork that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articles, in which the likelihood to find an evidence is considerably higher than in an arbitrary article from the corpus. In this work, we detect evidence directly supporting or contesting the topic (without an intermediate claim), and we search in the entire corpus, with no need for pre-select"
P18-2095,W17-5110,1,0.94214,"the training process. Most similar to our work, Dehghani et al. (2017a) use WLD and SLD together, for sentiment classification. They train two separate networks, one with WLD only, and another with SLD only. They control the magnitude of the gradient updates to the network trained on WLD, using the scores provided by the network trained on SLD. Differently, we blend the two types of labeled data in a single network. Sentences from the first paragraph are considered as non-argumentative and the rest of the sentences are considered as argumentative. For the topic-dependent claim detection task, Levy et al. (2017) showed that retrieving sentences with the word that followed by the concept representing the topic, yields candidates that are more likely to contain a claim for that topic than arbitrary sentences which contain the topic concept. 2.2 We present BlendNet, a neural network that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Ege"
P18-2095,P17-1002,0,0.0839024,"17) showed that retrieving sentences with the word that followed by the concept representing the topic, yields candidates that are more likely to contain a claim for that topic than arbitrary sentences which contain the topic concept. 2.2 We present BlendNet, a neural network that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articles, in which the lik"
P18-2095,P09-1113,0,0.0508116,"Missing"
P18-2095,D14-1162,0,0.0835419,"s and Schmidhuber, 2005) with an additional attention layer (Yang et al., 2016). The models are all trained with a dropout of 0.85, using a single dropout mask across all timesteps as proposed by Gal and Ghahramani (2016). The cell size in the LSTM layers is 128, and the attention layer is of size 100. We use the Adam method as an optimizer (Kingma and Ba, 2015) with a learning rate of 0.001, and apply gradient clipping with a maximum global norm of 1.0. Words are represented using the 300 dimensional GloVe embeddings learned on 840B Common Crawl tokens and are left untouched during training (Pennington et al., 2014). We note that even though we chose this network architecture, there is nothing in the blending method we propose which is restricted to it, and blending can be easily applied to other networks. 3.2 WLD blending WLD is a pair of disjoint sets, WLDpos and WLDneg . The two sets are constructed such that the probability of finding positive instances in WLDpos is significantly higher than that of finding them in WLDneg . This difference in probabilities is the source of the signal WLD provides. Importantly, the probability in WLDpos can still be rather low. As mentioned in Section 2.1, using WLD t"
P18-2095,N16-1164,0,0.0319874,"task, Levy et al. (2017) showed that retrieving sentences with the word that followed by the concept representing the topic, yields candidates that are more likely to contain a claim for that topic than arbitrary sentences which contain the topic concept. 2.2 We present BlendNet, a neural network that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articl"
P18-2095,D15-1050,1,0.873822,"ed on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articles, in which the likelihood to find an evidence is considerably higher than in an arbitrary article from the corpus. In this work, we detect evidence directly supporting or contesting the topic (without an intermediate claim), and we search in the entire corpus, with no need for pre-selecting a small set of re"
P18-2095,S15-2079,0,0.0471032,"identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articles, in which the likelihood to find an evidence is considerably higher than in an arbitrary article from the corpus. In this work, we detect evidence directly supporting or contesting the topic (without an intermediate claim), and we search in the entire corpus, with no need for pre-selecting a small set of relevant articles. 2.3 3.1 BlendNet Network description Our network is a bi-directional LSTM (Graves and Schmidhuber, 2005) with an additional attention l"
P18-2095,D14-1006,0,0.114061,"argumentative. For the topic-dependent claim detection task, Levy et al. (2017) showed that retrieving sentences with the word that followed by the concept representing the topic, yields candidates that are more likely to contain a claim for that topic than arbitrary sentences which contain the topic concept. 2.2 We present BlendNet, a neural network that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a give"
P18-2095,J17-3005,0,0.0745246,"WLDneg . This difference in probabilities is the source of the signal WLD provides. Importantly, the probability in WLDpos can still be rather low. As mentioned in Section 2.1, using WLD to pretrain neural networks has been proven to be effective. We extend this idea by allowing the use of WLD alongside SLD during the entire training process of the network. Our intuition is that even though WLD signal is noisy, there is potential in SLD and WLD in argumentation mining Publicly available strong labeled data (SLD) for argument mining is usually only a couple of thousand instances in size (e.g., Stab and Gurevych (2017) present one of the largest, with around 6,000 annotated positive instances). Recently, Habernal et al. (2018) have commented about the difficulty to collect valuable SLD from crowd sourcing for such tasks. Several works utilize WLD for argumentation mining; Webis-Debate-16 (Al-Khatib et al., 2016) use the structure of online debates as distant supervision for the task of argument classification. 600 its additional massive amount, and integrating it can improve training when SLD is limited in size. In every epoch (a pass through the entire SLD), the training data is enriched with WLD. However,"
P18-2095,D12-1042,0,0.0394347,"Missing"
P18-2095,E17-1105,0,0.0362311,"ich contain the topic concept. 2.2 We present BlendNet, a neural network that is trained on a blend of WLD and SLD. 3 Argumentation mining Argumentation mining is attracting a lot of attention (Lippi and Torroni, 2016). One line of research focuses on identifying arguments (claims and evidence/premises) within a text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Persing and Ng, 2016; Eger et al., 2017). Another line of work seeks to mine arguments relevant for a given topic or claim, either from a pre-built argument repository where arguments are collected from online debate portals (Wachsmuth et al., 2017), or from unrestricted large scale corpora (Levy et al., 2014; Rinott et al., 2015; Levy et al., 2017). Our work falls into the latter category of corpus wide topic-dependent argumentation mining. Previous work by Rinott et al. (2015) presented the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articles, in which the likelihood to find an evidence is considerably higher than in an arbitrary article from the corpus. In this work, we detect evidence directly supporting or contesting the topic (without an intermediate claim), an"
P18-2095,N16-1174,0,0.0286348,"nted the task of detecting evidence texts that are relevant for claims of a given topic. They search in a preselected set of articles, in which the likelihood to find an evidence is considerably higher than in an arbitrary article from the corpus. In this work, we detect evidence directly supporting or contesting the topic (without an intermediate claim), and we search in the entire corpus, with no need for pre-selecting a small set of relevant articles. 2.3 3.1 BlendNet Network description Our network is a bi-directional LSTM (Graves and Schmidhuber, 2005) with an additional attention layer (Yang et al., 2016). The models are all trained with a dropout of 0.85, using a single dropout mask across all timesteps as proposed by Gal and Ghahramani (2016). The cell size in the LSTM layers is 128, and the attention layer is of size 100. We use the Adam method as an optimizer (Kingma and Ba, 2015) with a learning rate of 0.001, and apply gradient clipping with a maximum global norm of 1.0. Words are represented using the 300 dimensional GloVe embeddings learned on 840B Common Crawl tokens and are left untouched during training (Pennington et al., 2014). We note that even though we chose this network archit"
P19-1093,N19-1053,0,0.0204128,"iveness) arouses great interest in various fields such as essay scoring (Ghosh et al., 2016), persuasive technologies (Fogg, 1998, 2002, 2009), and social networks, where it is deemed a hard problem (Hidey and McKeown, 2018). Naturally, it is also relevant for social sciences, for example in public narrative (Green and Brock, 2000), internet discussions (Tan et al., 2016), and in argumentative process of thought (Burnstein, 2003). In theoretical argumentation studies, the importance of quality (Wachsmuth et al., 2017) and convincingness was emphasized (O’Keefe, 2012; Van Eemeren et al., 2014; Chen et al., 2019), but assessment is still a challenge despite years of study (Weltzer-Ward et al., 2009; Rosenfeld and Kraus, 2016). Traditionally, assessment of arguments convincingness, if addressed at all, relied on relevance, acceptability or sufficiency of arguments (Habernal and Gurevych, 2015; Johnson and Blair, 2006), or on general fallacies (Hamblin, 1971; Tindale, 2007). Recently, some works studied convincingness of full texts, assessing the role of prior beliefs (Durmus and Cardie, 2018) and structure (Wachsmuth et al., 2016). A second important distinction between the data sets is writing level."
P19-1093,N18-1094,0,0.0347512,"tance of quality (Wachsmuth et al., 2017) and convincingness was emphasized (O’Keefe, 2012; Van Eemeren et al., 2014; Chen et al., 2019), but assessment is still a challenge despite years of study (Weltzer-Ward et al., 2009; Rosenfeld and Kraus, 2016). Traditionally, assessment of arguments convincingness, if addressed at all, relied on relevance, acceptability or sufficiency of arguments (Habernal and Gurevych, 2015; Johnson and Blair, 2006), or on general fallacies (Hamblin, 1971; Tindale, 2007). Recently, some works studied convincingness of full texts, assessing the role of prior beliefs (Durmus and Cardie, 2018) and structure (Wachsmuth et al., 2016). A second important distinction between the data sets is writing level. The arguments for UPKConvArg were extracted from two Web debate portals, on which people post free text and in which writing level widely varies (for instance, some posts include ungrammatical texts which require a preprocessing step). Our arguments were retrieved from Wikipedia, a heavily edited corpus which makes them on par in terms of writing level. Overall, the contribution of this new data set is that it emphasizes pairs homogeneity – in terms of argument type, length, and writ"
P19-1093,P16-2089,0,0.0174147,"identification of the more convincing argument may be based not only on argument convincingness, but also on identifying argument type, or even on a shallow feature such as argument length. Indeed, we show a very high performance of the baseline by length over UPKConvArg in §5.1. On the other hand, a data set that includes only evidence poses a more challenging task. In addition, we directly controlled for argument length by building pairs of roughly the same length. 2 Background Convincingness. Convincingness (or persuasiveness) arouses great interest in various fields such as essay scoring (Ghosh et al., 2016), persuasive technologies (Fogg, 1998, 2002, 2009), and social networks, where it is deemed a hard problem (Hidey and McKeown, 2018). Naturally, it is also relevant for social sciences, for example in public narrative (Green and Brock, 2000), internet discussions (Tan et al., 2016), and in argumentative process of thought (Burnstein, 2003). In theoretical argumentation studies, the importance of quality (Wachsmuth et al., 2017) and convincingness was emphasized (O’Keefe, 2012; Van Eemeren et al., 2014; Chen et al., 2019), but assessment is still a challenge despite years of study (Weltzer-Ward"
P19-1093,W14-2109,1,0.929228,"Missing"
P19-1093,D15-1255,0,0.042571,"s, for example in public narrative (Green and Brock, 2000), internet discussions (Tan et al., 2016), and in argumentative process of thought (Burnstein, 2003). In theoretical argumentation studies, the importance of quality (Wachsmuth et al., 2017) and convincingness was emphasized (O’Keefe, 2012; Van Eemeren et al., 2014; Chen et al., 2019), but assessment is still a challenge despite years of study (Weltzer-Ward et al., 2009; Rosenfeld and Kraus, 2016). Traditionally, assessment of arguments convincingness, if addressed at all, relied on relevance, acceptability or sufficiency of arguments (Habernal and Gurevych, 2015; Johnson and Blair, 2006), or on general fallacies (Hamblin, 1971; Tindale, 2007). Recently, some works studied convincingness of full texts, assessing the role of prior beliefs (Durmus and Cardie, 2018) and structure (Wachsmuth et al., 2016). A second important distinction between the data sets is writing level. The arguments for UPKConvArg were extracted from two Web debate portals, on which people post free text and in which writing level widely varies (for instance, some posts include ungrammatical texts which require a preprocessing step). Our arguments were retrieved from Wikipedia, a h"
P19-1093,P16-1150,0,0.100484,"s such a system is intended to enhance decision making. In this work we target the task of assessing argument convincingness, and more specifically, we focus on evidence convincingness – given texts representing evidence for a given debatable topic, identify the more convincing ones. Theoretical works have analyzed the factors that make an argument more convincing (e.g., Boudry et al., 2015). This work is an empirical one in the line of (Persing and Ng, 2017; Tan et al., 2016). To the best of our knowledge, this is the first work on evidence convincingness. Most similar to our work is that of Habernal and Gurevych (2016a) who are the first to directly compare pairs of arguments (previous works compared documents). They released UPKConvArg, the first data set of convincingness, containing argument pairs with a label indicating which one is preferred over the other. In this work we release IBM-EviConv, a data set of evidence pairs which offers a more focused view of the argument convincingness task. As a source of evidence sentences we use the evidence data set released by Shnarch et al. (2018), which contains more than 2,000 evidence sentences over 118 topics. We then sampled more than 8,000 pairs of evidence"
P19-1093,D15-1050,1,0.939749,"Missing"
P19-1093,D16-1129,0,0.127779,"s such a system is intended to enhance decision making. In this work we target the task of assessing argument convincingness, and more specifically, we focus on evidence convincingness – given texts representing evidence for a given debatable topic, identify the more convincing ones. Theoretical works have analyzed the factors that make an argument more convincing (e.g., Boudry et al., 2015). This work is an empirical one in the line of (Persing and Ng, 2017; Tan et al., 2016). To the best of our knowledge, this is the first work on evidence convincingness. Most similar to our work is that of Habernal and Gurevych (2016a) who are the first to directly compare pairs of arguments (previous works compared documents). They released UPKConvArg, the first data set of convincingness, containing argument pairs with a label indicating which one is preferred over the other. In this work we release IBM-EviConv, a data set of evidence pairs which offers a more focused view of the argument convincingness task. As a source of evidence sentences we use the evidence data set released by Shnarch et al. (2018), which contains more than 2,000 evidence sentences over 118 topics. We then sampled more than 8,000 pairs of evidence"
P19-1093,P18-2095,1,0.859296,"e best of our knowledge, this is the first work on evidence convincingness. Most similar to our work is that of Habernal and Gurevych (2016a) who are the first to directly compare pairs of arguments (previous works compared documents). They released UPKConvArg, the first data set of convincingness, containing argument pairs with a label indicating which one is preferred over the other. In this work we release IBM-EviConv, a data set of evidence pairs which offers a more focused view of the argument convincingness task. As a source of evidence sentences we use the evidence data set released by Shnarch et al. (2018), which contains more than 2,000 evidence sentences over 118 topics. We then sampled more than 8,000 pairs of evidence and sent them for convincingness labeling. Machines capable of responding and interacting with humans in helpful ways have become ubiquitous. We now expect them to discuss with us the more delicate questions in our world, and they should do so armed with effective arguments. But what makes an argument more persuasive? What will convince you? In this paper, we present a new data set, IBMEviConv, of pairs of evidence labeled for convincingness, designed to be more challenging th"
P19-1093,C14-1141,1,0.887365,"Missing"
P19-1093,Q18-1026,0,0.242859,"data set differs from UPKConvArg in a few important aspects. While in UPKConvArg the pairs consist of two types or arguments, claims and evi1 For more details and a video of the debate: https://www.research.ibm.com/ artificial-intelligence/project-debater/ live/ First two authors contributed equally. 967 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 967–976 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics data sets, and show that it outperforms the methods suggested by Habernal and Gurevych (2016a) and Simpson and Gurevych (2018) on both sets. With the advancement in argument detection, the research community can now pay more attention to the challenging task of identifying the more convincing arguments. This work continues the line started by Habernal and Gurevych (2016a) by suggesting a focused framing of the task, providing a new data set for it, and presenting a neural network which surpasses state of the art performance. dence, IBM-EviConv pairs are composed solely of evidence. In a follow-up work on UPKConvArg, Habernal and Gurevych (2016b) showed that the most frequent reason by far to prefer one argument over"
P19-1093,C14-1142,0,0.125914,"d with them. When opposing sides debate such questions, each side aims to present the most convincing arguments for its point of view, typically by raising various claims and supporting them with relevant pieces of evidence. Ideally, the arguments by both sides are then carefully compared, as part of the decision process. Automatic methods for this process of argumentation and debating are developed within the field of Computational Argumentation. which focuses on methods for argument detection (Lippi and Torroni, 2016; Levy et al., 2014; Rinott et al., 2015) and revealing argument relations (Stab and Gurevych, 2014, 2017). Recently, IBM introduced Project Debater, the first AI system able to debate humans on complex topics. Project Debater participated in a live debate against a world champion debater, and was ∗ Why is the new data set useful? Our new data set differs from UPKConvArg in a few important aspects. While in UPKConvArg the pairs consist of two types or arguments, claims and evi1 For more details and a video of the debate: https://www.research.ibm.com/ artificial-intelligence/project-debater/ live/ First two authors contributed equally. 967 Proceedings of the 57th Annual Meeting of the Associ"
P19-1093,J17-3005,0,0.172658,"Missing"
P19-1093,C16-1158,0,0.0368759,") and convincingness was emphasized (O’Keefe, 2012; Van Eemeren et al., 2014; Chen et al., 2019), but assessment is still a challenge despite years of study (Weltzer-Ward et al., 2009; Rosenfeld and Kraus, 2016). Traditionally, assessment of arguments convincingness, if addressed at all, relied on relevance, acceptability or sufficiency of arguments (Habernal and Gurevych, 2015; Johnson and Blair, 2006), or on general fallacies (Hamblin, 1971; Tindale, 2007). Recently, some works studied convincingness of full texts, assessing the role of prior beliefs (Durmus and Cardie, 2018) and structure (Wachsmuth et al., 2016). A second important distinction between the data sets is writing level. The arguments for UPKConvArg were extracted from two Web debate portals, on which people post free text and in which writing level widely varies (for instance, some posts include ungrammatical texts which require a preprocessing step). Our arguments were retrieved from Wikipedia, a heavily edited corpus which makes them on par in terms of writing level. Overall, the contribution of this new data set is that it emphasizes pairs homogeneity – in terms of argument type, length, and writing level. We believe that IBM-EviConv"
P19-1093,P17-2039,0,0.104506,"y building pairs of roughly the same length. 2 Background Convincingness. Convincingness (or persuasiveness) arouses great interest in various fields such as essay scoring (Ghosh et al., 2016), persuasive technologies (Fogg, 1998, 2002, 2009), and social networks, where it is deemed a hard problem (Hidey and McKeown, 2018). Naturally, it is also relevant for social sciences, for example in public narrative (Green and Brock, 2000), internet discussions (Tan et al., 2016), and in argumentative process of thought (Burnstein, 2003). In theoretical argumentation studies, the importance of quality (Wachsmuth et al., 2017) and convincingness was emphasized (O’Keefe, 2012; Van Eemeren et al., 2014; Chen et al., 2019), but assessment is still a challenge despite years of study (Weltzer-Ward et al., 2009; Rosenfeld and Kraus, 2016). Traditionally, assessment of arguments convincingness, if addressed at all, relied on relevance, acceptability or sufficiency of arguments (Habernal and Gurevych, 2015; Johnson and Blair, 2006), or on general fallacies (Hamblin, 1971; Tindale, 2007). Recently, some works studied convincingness of full texts, assessing the role of prior beliefs (Durmus and Cardie, 2018) and structure (W"
P19-1093,N18-1202,0,0.0254503,"Missing"
P19-1093,D17-1261,0,0.0681943,"nce to reveal real convincingness signals, beyond the more trivial ones. Finally, UPKConvArg pairs are of the same stance towards the topic, (either both supporting it or both contesting it), and therefore it is aligned with the task of choosing the most convincing arguments of a given side of the debate. In contrast, our data set contains both same stance pairs, as well as cross stance pairs (i.e., one is supporting and the other is contesting the topic). Thus it is aligned with the above mentioned task, but in addition, with the task of choosing which side of the debate was more convincing (Potash and Rumshisky, 2017). Argument convincingness data set. Closer to our work, recent studies aim to assess the convincingness of a single argument, rather than that of a full text. The first data set for this task was published by Habernal and Gurevych (2016a). Their data set, UPKConvArg, consists of approximately 1,000 web mined arguments across 16 different topics, each split into two sets by stance (support or contest the topic). In each such split, all argument pairs are annotated by crowd workers for the preference of one argument over the other. In addiIn addition to the release of a new data set, a second co"
P19-1094,D15-1203,0,0.0207263,"ive evidence for the existence of the target relation (consistent or contrastive expansion) between DC and EC. When applying the classifier to a new pair, we collect up to 500 sentences for that pair, and average the classifier’s predictions for each sentence. WordNet. Whether DC is a hypernym, hyponym, synonym or co-hyponym of EC in WordNet (Miller, 1995) - four binary features. Sentiment. Consistent expansions are expected to have the same sentiment polarity as the DC, whereas opposite polarities may indicate contrastive expansions (e.g., Democracy vs. Dictatorship). Similar to Iyyer et al. (2015), we train a linear SVM classifier on the sentiment lexicon of Hu and Liu (2004), using the word2vec word embeddings computed over the CN corpus as the features and the word polarities as the labels. Word polarity can then be determined by the sign of the classifier’s output score, and the sentiment strength by its magnitude. We take the product of the classifier’s scores for DC and EC as a single sentiment feature. Corpus statistics. Simple corpus-based features are derived from the number of co-occurrences of DC and EC in the same sentence in CN or in the same query in CQ . These features ar"
P19-1094,N18-5005,0,0.0679863,"Missing"
P19-1097,N16-1165,0,0.0660775,"Missing"
P19-1097,P16-2085,1,0.812839,"n (2012). That is, instead of having such data per topic, as is currently the case in AraucariaDB, having such data for commonplace principled arguments facilitates their use over a wide range of topics. Note that for this the stance of the argument w.r.t the CoPA and the motion is important. For the sake of simplicity and brevity we have ignored this issue in this manuscript, but the relevant stance labeling is available in the supplementary material. In the field of computational argumentation, de novo argument synthesis has received relatively little attention. One naive attempt is that of Bilu and Slonim (2016), where claims are generated by pasting together a topic and short predicate. The framework suggested here may provide a richer and more stable basis for argumentative text generation. That is, a CoPA may include structured data which describes its principal theme. Then, when presented with a motion in this CoPA, the system would automatically generate, de novo, argumentative text based on this structured data and the topic. For example, this could be an NLG neural net trained on a large corpus of claims extracted using argument mining for motions in the CoPA. Finally, let us reappraise the ba"
P19-1097,P15-2072,0,0.0209479,"ask of locating recurring patterns in order to argue the motion abstractly is composed of understanding what are the fundamental ’clashes’ in the debate (cf. Sonnreich (2012), ”debating from first principles”), similar to the taxonomy herein. Our approach bears similarities to work in social sciences that attempts to describe different types of information framing, usually in the context of the news media (e.g. Semetko and Valkenburg, 2000). Recurring themes, like Fairness and equality or Crime and punishment, can be identified in the way the news media frames a certain policy issue or event (Card et al., 2015). de Vreese (2005) differentiates between specific and generic frames, characterizing the latter as those that can be applied to a wide range of events and contexts. Similarly, our work aims to categorize commonplace themes and identify their relevance (at a considerably larger scale), in the context of framing a topic that is subject to debate. Our work also has some commonalities with psychological research on ideology (e.g. Altemeyer, 1981; Sidanius and Pratto, 2001; Jost et al., 2003). For example, Everett (2013) lists a 12-item scale to assess conservative ideology - of these, some map to"
P19-1097,P17-1002,0,0.0253537,"osing stance can be made around this theme2 . We show that for most motions there exist relevant arguments within the suggested knowledge base, and that they can be identified automatically with reasonable precision and recall. Moreover, we show that professional human debaters often allude to such arguments when they debate. 2 Related work Previous computational work on argument invention was mainly done within the field of argument mining, where – as the name implies – the focus is on identifying arguments within a given text. Most works (e.g. Stab and Gurevych, 2014; Palau and Moens, 2009; Eger et al., 2017) assume that a relevant text is provided, while some include the task of extracting such text from a large open-domain corpus (e.g. Levy et al., 2014; Rinott et al., 2015; Shnarch et al., 2018; Al-Khatib et al., 2016; Levy et al., 2018). The work here complements such techniques by providing a dataset of arguments whose manual construction facilitates automatic retrieval for topics of interest and ensures quality, validity, style and so on. A somewhat similar approach is suggested in work of Walton and Gordon (2017, 2018), where arguments are constructed from a database (Reed and Rowe, 2004) o"
P19-1097,D18-1078,1,0.604095,"of 0.76 when comparing the majority vote to the full annotation. The full dataset can be found in the supplementary material. 7 When analyzing our data, it will sometimes be interesting to omit these three classes, to ascertain they do not significantly skew the results. 8 https://www.figure-eight.com/ 4.3 CoPA claims in recorded speeches It is natural to ask whether the claims authored for each CoPA are an artificial construct for facilitating motion assignment, or are actual claims, likely to be made by people deliberating these motions. To this end we considered the speeches we recorded in Mirkin et al. (2018). Each such speech is given in the context of a motion, all of which are included in our dataset. For each motion we extracted the CoPAs to which it belongs according to our annotation, yielding 184 speeches with at least one matching CoPA. 7 Figure-Eight annotators were presented with speeches in both audio and written form, alongside the claims from the matching CoPAs. They were asked whether each claim was (i) explicitly made by the speaker, was (ii) implicit in the speech or was (iii) not mentioned at all. A total of 800 (speech, claim) pairs were annotated, with one half of them being cla"
P19-1097,Q14-1025,0,0.0681096,"Missing"
P19-1097,D18-1522,1,0.649669,"t (if t is a multi-word expression the vectors are summed and normalized). This vector is then used as a feature vector for a logistic regression classifier. That is, each CoPA is assigned the classification score of the classifier so trained. As a safeguard mechanism, we also determine an actions blacklist, Bc , for each CoPA c. An action a is in Bc if in the training set no motion with action a is in c. During prediction, if the left-out motion’s action is in Bc , it will not be predicted as belonging to c. By topic, Naive Bayes (NB) and Recurrent Neural Network (RNN): Following the work of Rabinovich et al. (2018) each motion m = (a, t) is associated with a set of retrieved sentences containing the term t. For a given CoPA c, all the sentences associated with motions in c are considered as positive examples, and those of motions not in c as negative examples. For NB, A Naive Bayes classifier is then trained over the unigrams of these sentences, and uses its score for prediction. In addition, the same blacklist safeguard mechanism as for W2V above is used. Similarly, these sentences were used to train an RNN to differentiate between positively-labeled sentences and negatively-labeled ones. See Rabinovic"
W14-2109,C14-1141,1,\N,Missing
W14-2109,D15-1050,1,\N,Missing
W15-0511,W10-3110,0,0.0317455,"can be applied, and applying it using simple rules. Here we consider only one scope, and explore the fluency and plausibility of the resulting statement and its argumentative value. Finally, Ahmed and Lin exemplify their technique on a small set of sentences, whereas here the statistical analysis and learning are done on much larger data. 3 Related work Negation detection has received much attention in NLP. This includes the detection of negated clauses and subclauses, and of negation expressions and of their scope. Methods employed in this detection include conditional random fields (CRFs) (Councill et al., 2010), regular expressions (Chapman et al., 2013), and syntactic rules (Lotan et al., 2013; Mutalik et al., 2001). Negation detection is critical for medical applications, for example, in order to classify whether the text contains the existence or absence of a condition (Chapman et al., 2013). It was 85 Problem definition and associated challenges Similarly to (Aharoni et al., 2014; Levy et al., 2014) we define the following two concepts: • Topic – a short phrase that frames the discussion. • Context Dependent Claim (CDC) – a general, concise statement that directly supports or contests the given"
W15-0511,W09-0613,0,0.0148696,"hu et al., 2014), as negation alters the sentiment of the text in its scope. Despite these results, it is not trivial, in general, to infer the meaning of a negated utterance, or what is in fact negated—it depends on the focus of the original sentence (Blanco and Moldovan, 2011). By contrast, here we deal with negating typically short claims (12 words long on average), where focus and scope are usually relatively easy to infer. Several works have tackled the task of surface realization—the transformation from a logical representation to human-readable text—providing systems such as SimpleNLG (Gatt and Reiter, 2009). However, these earlier works do not provide a principled method of negating existing sentences given as free text statements. To our knowledge, there has been just one work on generating the negations of existing sentences (Ahmed and Lin, 2014). Ahmed and Lin use a set of syntactic rules to phrase all possible negations of a given sentence, according to the possible scopes of negation. Their focus is rather different from ours. First, they are interested in sentences in general, rather than Claims – which, in our corpus, tend to have a typically simple structure. Second, their interest is ma"
W15-0511,C14-1141,1,0.951335,"entation mining is to analyze argumentative structures within a document. Typically, documents in which such structures are abundant, such as from the legal domain (Mochales Palau and Moens, 2011; Bach et al., 2013; Ashley and Walker, 2013; Wyner et al., 2010), are analyzed, and compound argumentative structures, or argumentation schemes, are sought (Walton, 2012). More recently, there is also interest in automatically detecting simple argumentative structures, or the building blocks of such structures, in documents which are not argumentative by nature. For example, in (Aharoni et al., 2014; Levy et al., 2014) it was shown that context-dependent Claims and Evidences (sometimes called Conclusion and Grounds, respectively) are fairly common in Wikipedia articles, and can be detected automatically. In this setting, detection is done within a given context of a pre-specified debatable topic. Then, the objective is to search a given set of documents, and mine Claims and Evidence pertaining to this topic. 84 Proceedings of the 2nd Workshop on Argumentation Mining, pages 84–93, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics One motivation for such context-dependent argume"
W15-0511,N13-1091,0,0.0559555,"Missing"
W15-0511,P14-5010,0,0.00748811,"the negation would be usable. These results suggested that the main challenge for automatic Claim negation would lie in determining usability rather than in generating a grammatically correct negation, which led us to the approach described in the sequel. 4.2 Claim negation: How? The first stage of our algorithm receives a Claim as input, and uses a simple rule–based machinery to generate its candidate negation, aiming for it to be a Negated Claim. Specifically, the algorithm runs as follows: 1. Tokenize the Claim and label the tokens for parts-of-speech using the Stanford Core NLP pipeline (Manning et al., 2014). 2. Find the first token labeled as one of the following: a modal verb, a verb in the present tense, or a verb ending in ”n’t”. We denote this token as T1 . 3. If T1 is followed or preceded by one of several negation strings (e.g., “no”, “not”’), remove this negation and finish. 4. If T1 ends in “n’t”, remove this suffix and finish (so, e.g., “can’t” would be transformed to “can”). 5. If T1 is a modal verb, is a form of the verb “to be” (e.g. “is” or “are”). or is followed by a gerund, then: (a) If T1 is followed by a word composed of a negation prefix (e.g. “un”, “non”) and a WordNet (Miller"
W15-0511,P14-1029,0,0.0469269,"Missing"
W15-0511,W14-2109,1,\N,Missing
W15-0511,W10-3111,0,\N,Missing
W16-2814,D15-1050,1,0.764713,"ion for Computational Linguistics concept, in a fully-automatic fashion. For both settings, our approach is based on Wikipedia categories and lists, which have several advantages: (a) they provide an easy access to large collections of experts, (b) their stance classification is relatively easy, and (c) their hierarchical structure can be exploited. writers, religious figures, politicians, activists, and so on. 3 Applications Expert opinions are highly valuable for making persuasive arguments, and expert evidence (premise) is a commonly used type of argumentation scheme (Walton et al., 2008). Rinott et al. (2015) describe a method for automatic evidence detection in Wikipedia articles. Three common evidence types are explored: Study, Expert, and Anecdotal. The proposed method uses typespecific features for detecting evidence. For instance, in the case of expert evidence, a lexicon of words describing persons and organizations with relevant expertise is used. The process of incorporating expert opinions on a given topic into an argument involves several steps. First, we need to retrieve from our corpus articles that contain expert opinions related to the given topic. Second, the exact boundaries of the"
W16-2814,W06-1639,0,0.0244587,". If an expert E is known to be a supporter or an opponent of some topic T , then the Wikipedia page of E is likely to contain relevant opinions on T . Furthermore, a mention of E can be a useful feature for identifying relevant expert opinions for T in a given article. Finally, perhaps the most important use of the graph for expert evidence is stance classification. Previous work on stance classification has shown that it can be much improved by utilizing external information beyond the text itself. For example, posts by the same author on the same topic are expected to have the same stance (Thomas et al., 2006; Hasan and Ng, 2013). Similarly, as shown in the previous example, external knowledge on expert stance towards a topic can improve stance classification of expert opinions. 4 4.1 Concepts Offline construction of the graph starts with deriving the set of concepts. We started with Wikipedia’s list of controversial issues3 , which contains about 1,000 Wikipedia entries, grouped into several top-level categories. We manually selected a subset of 12 categories , and filtered out the remaining 3 categories.4 One of the authors selected from the remaining list concepts that represent a two-sided deb"
W16-2814,I13-1191,0,0.042,"nown to be a supporter or an opponent of some topic T , then the Wikipedia page of E is likely to contain relevant opinions on T . Furthermore, a mention of E can be a useful feature for identifying relevant expert opinions for T in a given article. Finally, perhaps the most important use of the graph for expert evidence is stance classification. Previous work on stance classification has shown that it can be much improved by utilizing external information beyond the text itself. For example, posts by the same author on the same topic are expected to have the same stance (Thomas et al., 2006; Hasan and Ng, 2013). Similarly, as shown in the previous example, external knowledge on expert stance towards a topic can improve stance classification of expert opinions. 4 4.1 Concepts Offline construction of the graph starts with deriving the set of concepts. We started with Wikipedia’s list of controversial issues3 , which contains about 1,000 Wikipedia entries, grouped into several top-level categories. We manually selected a subset of 12 categories , and filtered out the remaining 3 categories.4 One of the authors selected from the remaining list concepts that represent a two-sided debate (Meaning of life,"
W17-5104,S15-2102,0,0.255375,"Missing"
W17-5104,E17-1024,1,0.922545,"wards its target, choosing their head of state, which implies negative sentiment towards the monarchy, since the targets are contrastive. The topic’s sentiment towards the monarchy is also negative, hence it is a Pro claim. Introduction Debating technologies aim to help humans debate and make better decisions. A core capability for these technologies is the on-demand construction of pro and con arguments for a given controversial topic. Most previous work was aimed at detecting topic-dependent argument components, such as claims and evidence (Levy et al., 2014; Rinott et al., 2015). Recently, Bar-Haim et al. (2017) introduced the related task of claim stance classification. For example, given the topic On-demand argument generation is inherently an open-domain task, so one cannot learn topicspecific features for stance classification from the training data. Furthermore, claims are short sentences, and the number of claims in the training data is relatively small as compared to common sentiment analysis and stance classification benchmarks. Consequently, external knowledge such as sentiment lexicons is crucial for this task. However, the coverage of manually-constructed sentiment lexicons is often incomp"
W17-5104,esuli-sebastiani-2006-sentiwordnet,0,0.0900176,"ar et al., 2015) or by applying classification to groups of arguments linked by citations or agreement/disagreement (Burfoot et al., 2011; Sridhar et al., 2014). However, many features used in previous works were not available for our task. Instead, we leveraged other context information present in Wikipedia articles, and assume sentiment agreement across neighboring text fragments. A number of approaches in the literature can generate sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), many of which rely on graph-based approaches over WordNet (Hu and Liu, 2004; Esuli and Sebastiani, 2006; Blair-Goldensohn et al., 2008) or over a graph of distributionally similar n-grams (Velikovich et al., 2010). Our approach (Section 3) differs in that we leverage larger existing sentiment lexicons, instead of relying on small seed sets. Moreover, we opt for classifying word embeddings instead of graph-based approaches, which are sensitive to parameter settings. More similar recent work includes Amir et al. (2015), who also used manually-created sentiment lexicons (annotated with discrete sentiment levels) and word embeddings to train linear regression models that aim to predict the polarity"
W17-5104,N16-1091,0,0.145446,"Missing"
W17-5104,I13-1191,0,0.0293119,"e second is a combination of the baseline system and an SVM with unigram features, which was the best performer on higher coverage rates. Row 3 is our rerun of the baseline system. The results are close to the EACL ’17 results (row 1) but not identical. This is due to some changes in low-level tools used by the system, such as the wikifier.5 6 Related Work Stance classification has been applied to several different means of argumentation, for example congressional debates (Thomas et al., 2006; Yessenalina et al., 2010) or online discussions (Somasundaran and Wiebe, 2009; Walker et al., 2012b; Hasan and Ng, 2013). Some previous 5 As explained by Bar-Haim et al. (2017), the baseline results (rows 1,3) for each coverage level≥ 0.8 are the same, since they all add the default majority class predictions. 35 # 1 2 3 4 5 6 Configuration Baseline (EACL’17) Baselne+SVM (EACL’17) Baseline (Rerun) +Lexicon Expansion +Local Contextual Features +Neighboring Claims 0.1 0.849 0.784 0.846 0.899 0.935 0.954 0.2 0.847 0.758 0.841 0.867 0.892 0.935 0.3 0.836 0.749 0.823 0.844 0.866 0.882 Accuracy@Coverage 0.4 0.5 0.6 0.7 0.793 0.767 0.740 0.704 0.743 0.730 0.711 0.682 0.787 0.771 0.742 0.706 0.803 0.765 0.749 0.731 0.8"
W17-5104,P09-1026,0,0.118493,"rk, which performed best on lower coverage rates. The second is a combination of the baseline system and an SVM with unigram features, which was the best performer on higher coverage rates. Row 3 is our rerun of the baseline system. The results are close to the EACL ’17 results (row 1) but not identical. This is due to some changes in low-level tools used by the system, such as the wikifier.5 6 Related Work Stance classification has been applied to several different means of argumentation, for example congressional debates (Thomas et al., 2006; Yessenalina et al., 2010) or online discussions (Somasundaran and Wiebe, 2009; Walker et al., 2012b; Hasan and Ng, 2013). Some previous 5 As explained by Bar-Haim et al. (2017), the baseline results (rows 1,3) for each coverage level≥ 0.8 are the same, since they all add the default majority class predictions. 35 # 1 2 3 4 5 6 Configuration Baseline (EACL’17) Baselne+SVM (EACL’17) Baseline (Rerun) +Lexicon Expansion +Local Contextual Features +Neighboring Claims 0.1 0.849 0.784 0.846 0.899 0.935 0.954 0.2 0.847 0.758 0.841 0.867 0.892 0.935 0.3 0.836 0.749 0.823 0.844 0.866 0.882 Accuracy@Coverage 0.4 0.5 0.6 0.7 0.793 0.767 0.740 0.704 0.743 0.730 0.711 0.682 0.787 0."
W17-5104,P97-1023,0,0.11647,"set. work has improved stance classification by using the conversation structure (e.g., discussion reply links) (Walker et al., 2012a; Sridhar et al., 2015) or by applying classification to groups of arguments linked by citations or agreement/disagreement (Burfoot et al., 2011; Sridhar et al., 2014). However, many features used in previous works were not available for our task. Instead, we leveraged other context information present in Wikipedia articles, and assume sentiment agreement across neighboring text fragments. A number of approaches in the literature can generate sentiment lexicons (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), many of which rely on graph-based approaches over WordNet (Hu and Liu, 2004; Esuli and Sebastiani, 2006; Blair-Goldensohn et al., 2008) or over a graph of distributionally similar n-grams (Velikovich et al., 2010). Our approach (Section 3) differs in that we leverage larger existing sentiment lexicons, instead of relying on small seed sets. Moreover, we opt for classifying word embeddings instead of graph-based approaches, which are sensitive to parameter settings. More similar recent work includes Amir et al. (2015), who also used manually-created sentiment lexico"
W17-5104,P15-1012,0,0.0411663,"Missing"
W17-5104,W14-2715,0,0.197949,"Missing"
W17-5104,C14-1141,1,0.895687,"Missing"
W17-5104,W06-1639,0,0.251956,"Missing"
W17-5104,H92-1116,0,0.686011,"Missing"
W17-5104,D15-1050,1,0.913498,"Missing"
W17-5104,N12-1072,0,0.018908,"wer coverage rates. The second is a combination of the baseline system and an SVM with unigram features, which was the best performer on higher coverage rates. Row 3 is our rerun of the baseline system. The results are close to the EACL ’17 results (row 1) but not identical. This is due to some changes in low-level tools used by the system, such as the wikifier.5 6 Related Work Stance classification has been applied to several different means of argumentation, for example congressional debates (Thomas et al., 2006; Yessenalina et al., 2010) or online discussions (Somasundaran and Wiebe, 2009; Walker et al., 2012b; Hasan and Ng, 2013). Some previous 5 As explained by Bar-Haim et al. (2017), the baseline results (rows 1,3) for each coverage level≥ 0.8 are the same, since they all add the default majority class predictions. 35 # 1 2 3 4 5 6 Configuration Baseline (EACL’17) Baselne+SVM (EACL’17) Baseline (Rerun) +Lexicon Expansion +Local Contextual Features +Neighboring Claims 0.1 0.849 0.784 0.846 0.899 0.935 0.954 0.2 0.847 0.758 0.841 0.867 0.892 0.935 0.3 0.836 0.749 0.823 0.844 0.866 0.882 Accuracy@Coverage 0.4 0.5 0.6 0.7 0.793 0.767 0.740 0.704 0.743 0.730 0.711 0.682 0.787 0.771 0.742 0.706 0.803"
W17-5104,D10-1102,0,0.0132989,"rst is the baseline configuration used in this work, which performed best on lower coverage rates. The second is a combination of the baseline system and an SVM with unigram features, which was the best performer on higher coverage rates. Row 3 is our rerun of the baseline system. The results are close to the EACL ’17 results (row 1) but not identical. This is due to some changes in low-level tools used by the system, such as the wikifier.5 6 Related Work Stance classification has been applied to several different means of argumentation, for example congressional debates (Thomas et al., 2006; Yessenalina et al., 2010) or online discussions (Somasundaran and Wiebe, 2009; Walker et al., 2012b; Hasan and Ng, 2013). Some previous 5 As explained by Bar-Haim et al. (2017), the baseline results (rows 1,3) for each coverage level≥ 0.8 are the same, since they all add the default majority class predictions. 35 # 1 2 3 4 5 6 Configuration Baseline (EACL’17) Baselne+SVM (EACL’17) Baseline (Rerun) +Lexicon Expansion +Local Contextual Features +Neighboring Claims 0.1 0.849 0.784 0.846 0.899 0.935 0.954 0.2 0.847 0.758 0.841 0.867 0.892 0.935 0.3 0.836 0.749 0.823 0.844 0.866 0.882 Accuracy@Coverage 0.4 0.5 0.6 0.7 0.79"
W17-5110,W14-2109,1,0.895179,"ell, we avoid this problem. Searching for sentences with the concept Marriage will not return sentences in which the Wikification tool found the concept Same sex marriage. 2 Related Work Context dependent claim detection (i.e. the detection of claims that support/contest a given topic) was first suggested by (Levy et al., 2014). Next, (Lippi and Torroni, 2015) proposed the context independent claim detection task, in which one attempts to detect claims without having the topic as input. Thus, if the texts contain claims for multiple topics, all should be detected. Both works used the data in (Aharoni et al., 2014) for training and testing their models. (Levy et al., 2014) have first described ’that’ as an indicator for sentences containing claims. Other works have identified additional indicators of claims, such as discourse markers, and have used them within a rule-based, rather than a supervised, framework (Eckle-Kohler et al., 2015; Ong et al., 2014; Somasundaran and Wiebe, 2009; Schneider and Wyner, 2012). However, as mentioned, semantic relatedness is not enough; e.g., S4 mentions the MC explicitly, but does not include a claim. To further distinguish such sentences from those containing claims, w"
W17-5110,N16-1165,0,0.183508,"Missing"
W17-5110,D15-1267,0,0.0153368,"2014). Next, (Lippi and Torroni, 2015) proposed the context independent claim detection task, in which one attempts to detect claims without having the topic as input. Thus, if the texts contain claims for multiple topics, all should be detected. Both works used the data in (Aharoni et al., 2014) for training and testing their models. (Levy et al., 2014) have first described ’that’ as an indicator for sentences containing claims. Other works have identified additional indicators of claims, such as discourse markers, and have used them within a rule-based, rather than a supervised, framework (Eckle-Kohler et al., 2015; Ong et al., 2014; Somasundaran and Wiebe, 2009; Schneider and Wyner, 2012). However, as mentioned, semantic relatedness is not enough; e.g., S4 mentions the MC explicitly, but does not include a claim. To further distinguish such sentences from those containing claims, we observe that the token ’that’ is often used as a precursor to a claim; as in S1, S2 and in the sentence “we observe that the token ’that’ is often used as a precursor to a claim.” The usage of ’that’ as a feature was first suggested in (Levy et al., 2014). Thus, we use the presence of ’that’ as an initial weak label, and fu"
W17-5110,C92-2082,0,0.479856,"on (Mintz et al., 2009). In the context of argument mining, (AlKhatib et al., 2016) also used noisy labels to train a classifier, albeit for a different task. They exploited the manually curated idebate.org resource to define – admittedly noisy – labeled data, that were used to train an argument mining classification scheme. In contrast, our approach requires no data curation and relies on a simple linguistic observation of the typical role of ’that’ in argumentative text. Our use of the token ’that’ as a weak label to identify a relevant lexicon, is also reminiscent of the classical work by (Hearst, 1992) who suggested to use lexico-syntactic patterns to identify various lexical relations. However, to the best of our knowledge, the present work is the first to use such a paradigm in the context of argument mining. We demonstrate empirically over Wikipedia, that for sentences satisfying this query, the prior probability to include a relevant claim is enhanced compared to the background distribution. Further80 3 System Description 3.1 will enable designing a query with a relatively high prior for detecting claim–containing sentences. We start with standard pre-processing including tokenization,"
W17-5110,C14-1141,1,0.880508,"greatly enhanced. Next, we employ simple heuristics to rank the sentences, leading to an unsupervised corpus–wide claim detection system, with precision that outperforms previously reported results on the task of claim detection given relevant documents and labeled data. 1 S1 S2 S3 S4 Table 1: Example sentences for the topic ’End affirmative action’: 3 sentences containing claims (in bold), and a non– argumentative sentence which is still relevant to the topic. Previous works on claim detection have assumed the availability of a relatively small set of articles enriched with relevant claims (Levy et al., 2014). Similarly, other argument–mining works have focused on the analysis of a small set of argumentative essays (Stab and Gurevych, 2014). This paradigm has two limitations. First, it relies on a manual, or automatic (Roitman et al., 2016), process to retrieve the relevant set of articles, which is non-trivial and prone to errors. In addition, when considering large corpora, relevant claims may spread across a much wider and diverse set of articles compared to those considered by earlier works. Here, we present a first corpus–wide claim detection framework, that can be directly applied to massive"
W17-5110,P09-1113,0,0.0278038,"sence of ’that’ as an initial weak label, and further identify unigrams enriched in the suffixes of sentences containing ’that’ followed by the MC, compared to sentences containing the MC without a preceding ’that’. This yields a Claim Lexicon (CL), from which we derive a Claim Sentence Query (CSQ) composed of the following ordered triplet: that → MC → CL, i.e., the token ’that’, the MC as identified by a Wikification tool, and a unigram from the CL, in that order. The usage we make in this work of the word ’that’ as an initial weak label is closely related to the idea of distant supervision (Mintz et al., 2009). In the context of argument mining, (AlKhatib et al., 2016) also used noisy labels to train a classifier, albeit for a different task. They exploited the manually curated idebate.org resource to define – admittedly noisy – labeled data, that were used to train an argument mining classification scheme. In contrast, our approach requires no data curation and relies on a simple linguistic observation of the typical role of ’that’ in argumentative text. Our use of the token ’that’ as a weak label to identify a relevant lexicon, is also reminiscent of the classical work by (Hearst, 1992) who sugge"
W17-5110,W14-2104,0,0.0246867,"rroni, 2015) proposed the context independent claim detection task, in which one attempts to detect claims without having the topic as input. Thus, if the texts contain claims for multiple topics, all should be detected. Both works used the data in (Aharoni et al., 2014) for training and testing their models. (Levy et al., 2014) have first described ’that’ as an indicator for sentences containing claims. Other works have identified additional indicators of claims, such as discourse markers, and have used them within a rule-based, rather than a supervised, framework (Eckle-Kohler et al., 2015; Ong et al., 2014; Somasundaran and Wiebe, 2009; Schneider and Wyner, 2012). However, as mentioned, semantic relatedness is not enough; e.g., S4 mentions the MC explicitly, but does not include a claim. To further distinguish such sentences from those containing claims, we observe that the token ’that’ is often used as a precursor to a claim; as in S1, S2 and in the sentence “we observe that the token ’that’ is often used as a precursor to a claim.” The usage of ’that’ as a feature was first suggested in (Levy et al., 2014). Thus, we use the presence of ’that’ as an initial weak label, and further identify uni"
W17-5110,D15-1050,1,0.926014,"Missing"
W17-5110,P09-1026,0,0.0253052,"sed the context independent claim detection task, in which one attempts to detect claims without having the topic as input. Thus, if the texts contain claims for multiple topics, all should be detected. Both works used the data in (Aharoni et al., 2014) for training and testing their models. (Levy et al., 2014) have first described ’that’ as an indicator for sentences containing claims. Other works have identified additional indicators of claims, such as discourse markers, and have used them within a rule-based, rather than a supervised, framework (Eckle-Kohler et al., 2015; Ong et al., 2014; Somasundaran and Wiebe, 2009; Schneider and Wyner, 2012). However, as mentioned, semantic relatedness is not enough; e.g., S4 mentions the MC explicitly, but does not include a claim. To further distinguish such sentences from those containing claims, we observe that the token ’that’ is often used as a precursor to a claim; as in S1, S2 and in the sentence “we observe that the token ’that’ is often used as a precursor to a claim.” The usage of ’that’ as a feature was first suggested in (Levy et al., 2014). Thus, we use the presence of ’that’ as an initial weak label, and further identify unigrams enriched in the suffixes"
W17-5110,D14-1006,0,0.0843955,"on system, with precision that outperforms previously reported results on the task of claim detection given relevant documents and labeled data. 1 S1 S2 S3 S4 Table 1: Example sentences for the topic ’End affirmative action’: 3 sentences containing claims (in bold), and a non– argumentative sentence which is still relevant to the topic. Previous works on claim detection have assumed the availability of a relatively small set of articles enriched with relevant claims (Levy et al., 2014). Similarly, other argument–mining works have focused on the analysis of a small set of argumentative essays (Stab and Gurevych, 2014). This paradigm has two limitations. First, it relies on a manual, or automatic (Roitman et al., 2016), process to retrieve the relevant set of articles, which is non-trivial and prone to errors. In addition, when considering large corpora, relevant claims may spread across a much wider and diverse set of articles compared to those considered by earlier works. Here, we present a first corpus–wide claim detection framework, that can be directly applied to massive corpora, with no need to specify a small set of documents in advance. We exploit the empirical observation that relevant claims are t"
W19-2009,N09-1003,0,0.199891,"Missing"
W19-2009,J15-4004,0,0.206157,"this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can be divided into intrinsic and extrinsic evaluation methods (Schnabel et al., 2015; Chiu et al., 2016; Jastrzebski et al., 2017; Alshargi et al., 2018; Bakarov, 2018). While most datasets report the semantic similarity between words, many datasets actually capture semantic relatedness (Hill et al., 2015; Avraham and Goldberg, 2016), or more complex relations such as analogy or the ability to 1 Our code and data are available at https://github. com/danielhers/interchangeability. 70 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 70–76 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics (a) CBOW (b) SGNS Figure 1: Performance of the CBOW (a) and SGNS (b) algorithms on each benchmark, for each window size, measured by Spearman correlation between the benchmark score and the word embedding cosine similarity. categorize words"
W19-2009,W16-2519,0,0.0141632,"luating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can be divided into intrinsic and extrinsic evaluation methods (Schnabel et al., 2015; Chiu et al., 2016; Jastrzebski et al., 2017; Alshargi et al., 2018; Bakarov, 2018). While most datasets report the semantic similarity between words, many datasets actually capture semantic relatedness (Hill et al., 2015; Avraham and Goldberg, 2016), or more complex relations such as analogy or the ability to 1 Our code and data are available at https://github. com/danielhers/interchangeability. 70 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 70–76 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics (a) CBOW (b) SGNS Figure 1: Performance of the CBOW (a) and SGNS (b) algorithms on each benchmark, for each window size, measured by Spearman correlation between the benchmark score and the word embedding cosine similarity. categorize words based on the distributed repr"
W19-2009,Q15-1016,0,0.338563,"he same POS are syntactically valid in the same contexts. We also investigate the relationship between interchangeability and similarity as judged by commonly-used word similarity benchmarks, and correlate the result with the performance of word embedding models on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case. 1 Noam Slonim Introduction Word embedding algorithms (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015) attempt to capture the semantic space of words in a metric space of real-valued vectors. While it is common knowledge that the hyper-parameters used to train these models affects the semantic properties of the distances arising from them (Bansal et al., 2014; Lin et al., 2015; Goldberg, 2016; Lison and Kutuzov, 2017), and indeed, it has been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText ("
W19-2009,P14-2131,0,0.015266,"dels on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case. 1 Noam Slonim Introduction Word embedding algorithms (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015) attempt to capture the semantic space of words in a metric space of real-valued vectors. While it is common knowledge that the hyper-parameters used to train these models affects the semantic properties of the distances arising from them (Bansal et al., 2014; Lin et al., 2015; Goldberg, 2016; Lison and Kutuzov, 2017), and indeed, it has been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In"
W19-2009,N15-1144,0,0.0206066,"rks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case. 1 Noam Slonim Introduction Word embedding algorithms (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015) attempt to capture the semantic space of words in a metric space of real-valued vectors. While it is common knowledge that the hyper-parameters used to train these models affects the semantic properties of the distances arising from them (Bansal et al., 2014; Lin et al., 2015; Goldberg, 2016; Lison and Kutuzov, 2017), and indeed, it has been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can"
W19-2009,W17-0239,0,0.163183,"e research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case. 1 Noam Slonim Introduction Word embedding algorithms (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015) attempt to capture the semantic space of words in a metric space of real-valued vectors. While it is common knowledge that the hyper-parameters used to train these models affects the semantic properties of the distances arising from them (Bansal et al., 2014; Lin et al., 2015; Goldberg, 2016; Lison and Kutuzov, 2017), and indeed, it has been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can be divided into intrinsic and extrinsic ev"
W19-2009,Q17-1010,0,0.0760353,"attempt to capture the semantic space of words in a metric space of real-valued vectors. While it is common knowledge that the hyper-parameters used to train these models affects the semantic properties of the distances arising from them (Bansal et al., 2014; Lin et al., 2015; Goldberg, 2016; Lison and Kutuzov, 2017), and indeed, it has been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can be divided into intrinsic and extrinsic evaluation methods (Schnabel et al., 2015; Chiu et al., 2016; Jastrzebski et al., 2017; Alshargi et al., 2018; Bakarov, 2018). While most datasets report the semantic similarity between words, many datasets actually capture semantic relatedness (Hill et al., 2015; Avraham and Goldberg, 2016), or more compl"
W19-2009,P12-1015,0,0.0412138,"enchmark score and the word embedding cosine similarity (Levy et al., 2015). We learn word embeddings from English Wikipedia, using a dump from May 1, 2017.2 The data is preprocessed using a publicly available preprocessing script,3 extracting text, removing nonalphanumeric characters, converting digits to text, and lowercasing the text. Benchmarks. We use the following benchmarks: WordSim-353 (Finkelstein et al., 2001) and its partition into WordSim-353-Sim (Agirre et al., 2009) and WordSim-353-Rel (Zesch et al., 2008), SimLex999 (Hill et al., 2015), Rare Words (RW; Luong et al., 2013), MEN (Bruni et al., 2012), MTurk287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), and SimVerb-3500 (Gerz et al., 2016). See Table 1 for the size of each benchmark. 2.2 Evaluation on Benchmarks Results. Figure 1 displays the performance of the CBOW and SGNS algorithms on each benchmark, with window sizes 1 to 15. Apart from a small dip between windows 1 and 2 for CBOW, the performance is either nearly constant, or changes nearly monotonically with window size in each setting. The relative improvement (or deterioration), in percents, with the increase of window size from 2 to 15, are shown in Table 1 (∆win ="
W19-2009,W13-3512,0,0.0489748,"correlation between the benchmark score and the word embedding cosine similarity (Levy et al., 2015). We learn word embeddings from English Wikipedia, using a dump from May 1, 2017.2 The data is preprocessed using a publicly available preprocessing script,3 extracting text, removing nonalphanumeric characters, converting digits to text, and lowercasing the text. Benchmarks. We use the following benchmarks: WordSim-353 (Finkelstein et al., 2001) and its partition into WordSim-353-Sim (Agirre et al., 2009) and WordSim-353-Rel (Zesch et al., 2008), SimLex999 (Hill et al., 2015), Rare Words (RW; Luong et al., 2013), MEN (Bruni et al., 2012), MTurk287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), and SimVerb-3500 (Gerz et al., 2016). See Table 1 for the size of each benchmark. 2.2 Evaluation on Benchmarks Results. Figure 1 displays the performance of the CBOW and SGNS algorithms on each benchmark, with window sizes 1 to 15. Apart from a small dip between windows 1 and 2 for CBOW, the performance is either nearly constant, or changes nearly monotonically with window size in each setting. The relative improvement (or deterioration), in percents, with the increase of window size from 2 to 15, ar"
W19-2009,W16-2501,0,0.0301826,"capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can be divided into intrinsic and extrinsic evaluation methods (Schnabel et al., 2015; Chiu et al., 2016; Jastrzebski et al., 2017; Alshargi et al., 2018; Bakarov, 2018). While most datasets report the semantic similarity between words, many datasets actually capture semantic relatedness (Hill et al., 2015; Avraham and Goldberg, 2016), or more complex relations such as analogy or the ability to 1 Our code and data are available at https://github. com/danielhers/interchangeability. 70 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 70–76 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics (a) CBOW (b) SGNS Figure 1: Performanc"
W19-2009,W04-2404,0,0.0995992,"speech (also known as syntactic category) is determined by syntactic distribution, and conveys information about how a word functions in the sentence (Carnie, 2002). We can generally substitute each word in a sentence with various words that are of the same part of speech, but not words that are of different parts of speech. While the same syntactic function can sometimes be fulfilled by words of various parts of speech or possibly longer phrases (such as adverbs and prepositional phrases, or multi-word expressions), part of speech is nonetheless a very good proxy for syntactic distribution (Mohammad and Pedersen, 2004). Related to our work, Vuli´c et al. (2017) introduced a framework for automatic selection of specific context configurations for word embedding models per part of speech, improving performance on the SimLex999 benchmark. We take a different approach, investigating existing word embedding 3.1 Interchangeability Analysis in Word Similarity Benchmarks While all benchmarks we experiment with assign a score along a scale to each pair (calculated from human scoring), for our experiment we would like to use a binary annotation of whether a pair is related or not. For this purpose, we divide the whol"
W19-2009,D14-1162,0,0.0995089,"ly speaking, words with the same POS are syntactically valid in the same contexts. We also investigate the relationship between interchangeability and similarity as judged by commonly-used word similarity benchmarks, and correlate the result with the performance of word embedding models on these benchmarks. Our results will inform future research and applications in the selection of word embedding model, suggesting a principle for an appropriate selection of the context window size parameter depending on the use-case. 1 Noam Slonim Introduction Word embedding algorithms (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015) attempt to capture the semantic space of words in a metric space of real-valued vectors. While it is common knowledge that the hyper-parameters used to train these models affects the semantic properties of the distances arising from them (Bansal et al., 2014; Lin et al., 2015; Goldberg, 2016; Lison and Kutuzov, 2017), and indeed, it has been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, e"
W19-2009,D16-1235,0,0.0313217,"Missing"
W19-2009,D15-1036,0,0.0218972,"as been shown that they capture many different semantic relations (Yang and Powers, 2006; Agirre et al., 2009), little has been done to quantify the effect of model hyperparameters on output tendencies. Here we begin to answer this question, evaluating fastText (Bojanowski et al., 2017) on benchmarks designed to measure how well a model captures the degree of similarity between words (§2). 2 Word Similarity and Relatedness Many benchmarks have been proposed for the evaluation of unsupervised word representations. In general, they can be divided into intrinsic and extrinsic evaluation methods (Schnabel et al., 2015; Chiu et al., 2016; Jastrzebski et al., 2017; Alshargi et al., 2018; Bakarov, 2018). While most datasets report the semantic similarity between words, many datasets actually capture semantic relatedness (Hill et al., 2015; Avraham and Goldberg, 2016), or more complex relations such as analogy or the ability to 1 Our code and data are available at https://github. com/danielhers/interchangeability. 70 Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP, pages 70–76 c Minneapolis, USA, June 6, 2019. 2019 Association for Computational Linguistics (a) CBOW (b) SGNS F"
W19-2009,K17-1013,0,0.0492965,"Missing"
W19-4414,W13-1703,0,0.096282,"andomly. Lastly, we generate the errors in the sentence and add the gold sentence and error sentence to corresponding output files. 3.2 Nematus We trained 4 neural machine translation systems based on Nematus (Sennrich et al., 2017) Trans1 https://www.gutenberg.org https://cgit.freedesktop.org/ libreoffice/dictionaries/tree/en 2 140 former (Vaswani et al., 2017) implementation. All parameters used are the ones suggested for the 2017 Workshop on Machine Translation 3 . As training data we used all the restricted data, i.e., FCE (Dale and Kilgarriff, 2011), LANG8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998) (upsampled 10 times). Each of the four trained models was regarded as a separate correction method and all systems were combined using our method (§4), this was especially beneficial as ensembling is not yet implemented for the transformer. See §5.4 for comparison of the two ensembling methods over RNN based Nematus. 3.3 Model - randomly replace words with a predefined token, [MASK], and predict the missing word. (2) Next Sentence Prediction - given a pair of sentences A and B, does sentence B follow sentence A. Our general approach for usi"
W19-4414,W11-2838,0,0.0391201,"re all corrections can be applied backwards and pick one of them randomly. Lastly, we generate the errors in the sentence and add the gold sentence and error sentence to corresponding output files. 3.2 Nematus We trained 4 neural machine translation systems based on Nematus (Sennrich et al., 2017) Trans1 https://www.gutenberg.org https://cgit.freedesktop.org/ libreoffice/dictionaries/tree/en 2 140 former (Vaswani et al., 2017) implementation. All parameters used are the ones suggested for the 2017 Workshop on Machine Translation 3 . As training data we used all the restricted data, i.e., FCE (Dale and Kilgarriff, 2011), LANG8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998) (upsampled 10 times). Each of the four trained models was regarded as a separate correction method and all systems were combined using our method (§4), this was especially beneficial as ensembling is not yet implemented for the transformer. See §5.4 for comparison of the two ensembling methods over RNN based Nematus. 3.3 Model - randomly replace words with a predefined token, [MASK], and predict the missing word. (2) Next Sentence Prediction - given a pair of sentences A and B,"
W19-4414,E14-3013,0,0.130811,". To develop a system we trained GEC systems and gathered outputs from black-box systems (§3). One of the most frequent error types is spelling errors, we compared off of the shelf spellcheckers, systems developed for this error type specifically, to a new spellchecker (§3.1), finding that our spellchecker outperforms common spellcheckers on the task of spellchecking. Another system tested was modifications of BERT (Devlin et al., 2018) to correct errors, allowing for less reliance on parallel data and more generalizability across domains (§3.4). Lastly, we tested generating synthetic errors (Felice and Yuan, 2014) as a way to replace data in an unsupervised scenario. While finding that mimicking the error distribution and generating errors on the same domain is better, we did not eventually participate in the low-resource track. The field of Grammatical Error Correction (GEC) has produced various systems to deal with focused phenomena or general text editing. We propose an automatic way to combine black-box systems. Our method automatically detects the strength of a system or the combination of several systems per error type, improving precision and recall while optimizing F score directly. We show con"
W19-4414,W18-0529,0,0.0162989,"ll systems were combined using our method (§4), this was especially beneficial as ensembling is not yet implemented for the transformer. See §5.4 for comparison of the two ensembling methods over RNN based Nematus. 3.3 Model - randomly replace words with a predefined token, [MASK], and predict the missing word. (2) Next Sentence Prediction - given a pair of sentences A and B, does sentence B follow sentence A. Our general approach for using BERT to solve the GEC task is by iteratively querying BERT as a black box language model, reminding former use of language models (Dahlmeier and Ng, 2012; Bryant and Briscoe, 2018). To detect missing words we add [MASK] between every two words, if BERT suggests a word with high confidence, we conclude that this word is missing in this gap. To detect unnecessary words, we replace words with the [MASK] token and if all the suggestions returned from BERT have a low probability, we conclude that the masked word was unnecessary. For replacing words, we perform the same procedure by replacing each word with [MASK] and checking if BERT returns a different word with high probability. The described process produces many undesired replacements/deletions due to BERT’s versatile na"
W19-4414,W14-1702,0,0.0403516,"Missing"
W19-4414,W19-4406,0,0.283213,"cision and recall while optimizing F score directly. We show consistent improvement over the best standalone system in all the configurations tested. This approach also outperforms average ensembling of different RNN models with random initializations. In addition, we analyze the use of BERT for GEC - reporting promising results on this end. We also present a spellchecker created for this task which outperforms standard spellcheckers tested on the task of spellchecking. This paper describes a system submission to Building Educational Applications 2019 Shared Task: Grammatical Error Correction(Bryant et al., 2019). Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F0.5 by 3.7 points over the best result reported. 1 Introduction Unlike other generation tasks (e.g. Machine Translation and Text Summarization), Grammatical Error Correction (GEC) contains separable outputs, edits that could be extracted from sentences, categorized (Bryant et al., 2017) and evaluated separately (Choshen and Abend, 2018a). Throughout the years different approaches were considered, some focused on spec"
W19-4414,P17-1074,0,0.405298,"ecking. This paper describes a system submission to Building Educational Applications 2019 Shared Task: Grammatical Error Correction(Bryant et al., 2019). Combining the output of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F0.5 by 3.7 points over the best result reported. 1 Introduction Unlike other generation tasks (e.g. Machine Translation and Text Summarization), Grammatical Error Correction (GEC) contains separable outputs, edits that could be extracted from sentences, categorized (Bryant et al., 2017) and evaluated separately (Choshen and Abend, 2018a). Throughout the years different approaches were considered, some focused on specific error types (Rozovskaya et al., 2014) and others adjusted systems from other tasks (Zhao et al., 2019). While ∗ Contributed equally 139 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 139–148 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 3 Data Systems 3.1 2.1 Preprocessing Many tools are available for spelling correction. Yet, with a few heuristics we managed"
W19-4414,N18-2046,0,0.163799,", the latter often have high recall and differ in what they correct. To benefit from both worlds, pipelines (Rozovskaya and Roth, 2016) and rescoring hybrids (Grundkiewicz and Junczys-Dowmunt, 2018) were introduced. Another suggested method for combining is average ensembling (Junczys-Dowmunt et al., 2018), used when several end to end neural networks are trained. As single systems tend to have low recall (Choshen and Abend, 2018b), pipelining systems may propagate errors and may not benefit from more than one system per error. Rescoring reduces recall and may not be useful with many systems (Grundkiewicz and Junczys-Dowmunt, 2018). We propose a new method for combining systems (§4) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box. We show our system outperforms average ensembling, has benefits even when combining a single system with itself, and produces the new state of the art by combining several existing systems (§5). To develop a system we trained GEC systems and gathered outputs from black-box systems (§3). One of the most frequent error types is spelling errors, we compared off of the shelf spellcheckers, systems developed for this error type specifically, to"
W19-4414,P18-1127,1,0.838994,"oam Slonim IBM Research AI {yoavka,katz,leshem.choshen}@il.ibm.com {edo.cohen,naftali.liberman,assaf.toledo}@ibm.com {amir.menczel,noams}@il.ibm.com Abstract the first receive high precision, the latter often have high recall and differ in what they correct. To benefit from both worlds, pipelines (Rozovskaya and Roth, 2016) and rescoring hybrids (Grundkiewicz and Junczys-Dowmunt, 2018) were introduced. Another suggested method for combining is average ensembling (Junczys-Dowmunt et al., 2018), used when several end to end neural networks are trained. As single systems tend to have low recall (Choshen and Abend, 2018b), pipelining systems may propagate errors and may not benefit from more than one system per error. Rescoring reduces recall and may not be useful with many systems (Grundkiewicz and Junczys-Dowmunt, 2018). We propose a new method for combining systems (§4) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box. We show our system outperforms average ensembling, has benefits even when combining a single system with itself, and produces the new state of the art by combining several existing systems (§5). To develop a system we trained GEC systems"
W19-4414,W16-2703,0,0.0266436,"pts 141 The outputs of Seq2Seq models, differing in training parameters, can be merged using an ensemble approach, where the predictions of the models for each possible word in the sequence are used to compute a merged prediction. It was shown that even an ensemble of models trained with the same hyperparameters but with different instances of random initialization can yield benefit (Junczys-Dowmunt et al., 2018). The idea of automatically combining multiple system outputs is not new to other fields and was successfully used in the Named Entity Recognition (NER) and Entity linking (EL) tasks. Jiang et al. (2016) evaluated multiple NER systems and based on these results, manually selected a rule for combining the two best systems, building a hybrid system that outperformed the standalone systems. Ruiz and Poibeau (2015) used the precision calculated on a training corpus to calculate a weighted vote for each EL output on unseen data. Dlugolinsk`y et al. (2013) used decision tree classifier to identify which output to accept. They used a feature set based on the overall text, NE surface form, the NE type and the overlap between different outputs. In GEC, combining was also proposed but was ad-hoc rather"
W19-4414,P18-1059,1,0.814366,"oam Slonim IBM Research AI {yoavka,katz,leshem.choshen}@il.ibm.com {edo.cohen,naftali.liberman,assaf.toledo}@ibm.com {amir.menczel,noams}@il.ibm.com Abstract the first receive high precision, the latter often have high recall and differ in what they correct. To benefit from both worlds, pipelines (Rozovskaya and Roth, 2016) and rescoring hybrids (Grundkiewicz and Junczys-Dowmunt, 2018) were introduced. Another suggested method for combining is average ensembling (Junczys-Dowmunt et al., 2018), used when several end to end neural networks are trained. As single systems tend to have low recall (Choshen and Abend, 2018b), pipelining systems may propagate errors and may not benefit from more than one system per error. Rescoring reduces recall and may not be useful with many systems (Grundkiewicz and Junczys-Dowmunt, 2018). We propose a new method for combining systems (§4) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box. We show our system outperforms average ensembling, has benefits even when combining a single system with itself, and produces the new state of the art by combining several existing systems (§5). To develop a system we trained GEC systems"
W19-4414,N18-1055,0,0.491809,"Grammatical Error Corrections Yoav Kantor∗ Yoav Katz∗ Leshem Choshen∗ Edo Cohen-Karlik Naftali Liberman Assaf Toledo Amir Menczel Noam Slonim IBM Research AI {yoavka,katz,leshem.choshen}@il.ibm.com {edo.cohen,naftali.liberman,assaf.toledo}@ibm.com {amir.menczel,noams}@il.ibm.com Abstract the first receive high precision, the latter often have high recall and differ in what they correct. To benefit from both worlds, pipelines (Rozovskaya and Roth, 2016) and rescoring hybrids (Grundkiewicz and Junczys-Dowmunt, 2018) were introduced. Another suggested method for combining is average ensembling (Junczys-Dowmunt et al., 2018), used when several end to end neural networks are trained. As single systems tend to have low recall (Choshen and Abend, 2018b), pipelining systems may propagate errors and may not benefit from more than one system per error. Rescoring reduces recall and may not be useful with many systems (Grundkiewicz and Junczys-Dowmunt, 2018). We propose a new method for combining systems (§4) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box. We show our system outperforms average ensembling, has benefits even when combining a single system with itself,"
W19-4414,N18-2020,1,0.841637,"oam Slonim IBM Research AI {yoavka,katz,leshem.choshen}@il.ibm.com {edo.cohen,naftali.liberman,assaf.toledo}@ibm.com {amir.menczel,noams}@il.ibm.com Abstract the first receive high precision, the latter often have high recall and differ in what they correct. To benefit from both worlds, pipelines (Rozovskaya and Roth, 2016) and rescoring hybrids (Grundkiewicz and Junczys-Dowmunt, 2018) were introduced. Another suggested method for combining is average ensembling (Junczys-Dowmunt et al., 2018), used when several end to end neural networks are trained. As single systems tend to have low recall (Choshen and Abend, 2018b), pipelining systems may propagate errors and may not benefit from more than one system per error. Rescoring reduces recall and may not be useful with many systems (Grundkiewicz and Junczys-Dowmunt, 2018). We propose a new method for combining systems (§4) that can combine many systems and relies solely on their output, i.e., it uses systems as a black-box. We show our system outperforms average ensembling, has benefits even when combining a single system with itself, and produces the new state of the art by combining several existing systems (§5). To develop a system we trained GEC systems"
W19-4414,D12-1052,0,0.0204759,"correction method and all systems were combined using our method (§4), this was especially beneficial as ensembling is not yet implemented for the transformer. See §5.4 for comparison of the two ensembling methods over RNN based Nematus. 3.3 Model - randomly replace words with a predefined token, [MASK], and predict the missing word. (2) Next Sentence Prediction - given a pair of sentences A and B, does sentence B follow sentence A. Our general approach for using BERT to solve the GEC task is by iteratively querying BERT as a black box language model, reminding former use of language models (Dahlmeier and Ng, 2012; Bryant and Briscoe, 2018). To detect missing words we add [MASK] between every two words, if BERT suggests a word with high confidence, we conclude that this word is missing in this gap. To detect unnecessary words, we replace words with the [MASK] token and if all the suggestions returned from BERT have a low probability, we conclude that the masked word was unnecessary. For replacing words, we perform the same procedure by replacing each word with [MASK] and checking if BERT returns a different word with high probability. The described process produces many undesired replacements/deletions"
W19-4414,P07-2045,0,0.00936158,"Missing"
W19-4414,I11-1017,0,0.131903,"ackwards and pick one of them randomly. Lastly, we generate the errors in the sentence and add the gold sentence and error sentence to corresponding output files. 3.2 Nematus We trained 4 neural machine translation systems based on Nematus (Sennrich et al., 2017) Trans1 https://www.gutenberg.org https://cgit.freedesktop.org/ libreoffice/dictionaries/tree/en 2 140 former (Vaswani et al., 2017) implementation. All parameters used are the ones suggested for the 2017 Workshop on Machine Translation 3 . As training data we used all the restricted data, i.e., FCE (Dale and Kilgarriff, 2011), LANG8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013) and W&I+LOCNESS (Bryant et al., 2019; Granger, 1998) (upsampled 10 times). Each of the four trained models was regarded as a separate correction method and all systems were combined using our method (§4), this was especially beneficial as ensembling is not yet implemented for the transformer. See §5.4 for comparison of the two ensembling methods over RNN based Nematus. 3.3 Model - randomly replace words with a predefined token, [MASK], and predict the missing word. (2) Next Sentence Prediction - given a pair of sentences A and B, does sentence B follow sentence"
W19-4414,E17-2037,0,0.217512,"Missing"
W19-4414,P11-1093,0,0.035337,"beau (2015) used the precision calculated on a training corpus to calculate a weighted vote for each EL output on unseen data. Dlugolinsk`y et al. (2013) used decision tree classifier to identify which output to accept. They used a feature set based on the overall text, NE surface form, the NE type and the overlap between different outputs. In GEC, combining was also proposed but was ad-hoc rather than automatic and general. Combining was done by either piping (Rozovskaya and Roth, 2016), where each system receives the output of the last system, or correction of specific phenomena per system (Rozovskaya and Roth, 2011), or more involved methods tailored to the systems used (Grundkiewicz and Junczys-Dowmunt, 2018). This required manual adjustments and refinements for every set of systems. Evaluating by a corpus level measure such as F score renders combining systems difficult. Systems developed towards F0.5 tend to reduce recall improving precision (Choshen and Abend, 2018b), while avoiding catastrophic errors (Choshen and Abend, 2018c) this behaviour might reduce the flexibility of the combination. It is possible to tune systems to other goals (e.g. recall) (Grundkiewicz and Junczys-Dowmunt, 2018) and thus"
W19-4414,P16-1208,0,0.0138273,"ly selected a rule for combining the two best systems, building a hybrid system that outperformed the standalone systems. Ruiz and Poibeau (2015) used the precision calculated on a training corpus to calculate a weighted vote for each EL output on unseen data. Dlugolinsk`y et al. (2013) used decision tree classifier to identify which output to accept. They used a feature set based on the overall text, NE surface form, the NE type and the overlap between different outputs. In GEC, combining was also proposed but was ad-hoc rather than automatic and general. Combining was done by either piping (Rozovskaya and Roth, 2016), where each system receives the output of the last system, or correction of specific phenomena per system (Rozovskaya and Roth, 2011), or more involved methods tailored to the systems used (Grundkiewicz and Junczys-Dowmunt, 2018). This required manual adjustments and refinements for every set of systems. Evaluating by a corpus level measure such as F score renders combining systems difficult. Systems developed towards F0.5 tend to reduce recall improving precision (Choshen and Abend, 2018b), while avoiding catastrophic errors (Choshen and Abend, 2018c) this behaviour might reduce the flexibil"
W19-4414,E14-1038,0,0.11559,"tput of top BEA 2019 shared task systems using our approach, currently holds the highest reported score in the open phase of the BEA 2019 shared task, improving F0.5 by 3.7 points over the best result reported. 1 Introduction Unlike other generation tasks (e.g. Machine Translation and Text Summarization), Grammatical Error Correction (GEC) contains separable outputs, edits that could be extracted from sentences, categorized (Bryant et al., 2017) and evaluated separately (Choshen and Abend, 2018a). Throughout the years different approaches were considered, some focused on specific error types (Rozovskaya et al., 2014) and others adjusted systems from other tasks (Zhao et al., 2019). While ∗ Contributed equally 139 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 139–148 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 2 3 Data Systems 3.1 2.1 Preprocessing Many tools are available for spelling correction. Yet, with a few heuristics we managed to get a comparatively high result. As by Errant (Bryant et al., 2017), our spellchecker receives a better F0.5 score of spelling (type R:SPELL) than other leading open-source"
W19-4414,S15-1025,0,0.0299594,"te a merged prediction. It was shown that even an ensemble of models trained with the same hyperparameters but with different instances of random initialization can yield benefit (Junczys-Dowmunt et al., 2018). The idea of automatically combining multiple system outputs is not new to other fields and was successfully used in the Named Entity Recognition (NER) and Entity linking (EL) tasks. Jiang et al. (2016) evaluated multiple NER systems and based on these results, manually selected a rule for combining the two best systems, building a hybrid system that outperformed the standalone systems. Ruiz and Poibeau (2015) used the precision calculated on a training corpus to calculate a weighted vote for each EL output on unseen data. Dlugolinsk`y et al. (2013) used decision tree classifier to identify which output to accept. They used a feature set based on the overall text, NE surface form, the NE type and the overlap between different outputs. In GEC, combining was also proposed but was ad-hoc rather than automatic and general. Combining was done by either piping (Rozovskaya and Roth, 2016), where each system receives the output of the last system, or correction of specific phenomena per system (Rozovskaya"
W19-4414,E17-3017,0,0.0782181,"Missing"
W19-4414,N19-1014,0,\N,Missing
W19-4507,E17-1024,1,0.849226,"ts from a large corpus of news articles containing billions of sentences. Given a controversial topic, several queries are applied, retrieving sentences which potentially contain claims that are relevant to the topic. Query results are then ranked by a neural-model trained to detect sentences containing claims (similarly to Levy et al. (2017, 2018)5 ). Top-ranked sentences are passed to a boundary detection component, responsible for finding the exact span of each claim within each sentence (Levy et al., 2014). Lastly, the stance of each claim towards the topic is detected using the method of Bar-Haim et al. (2017). Used models are tuned towards precision, aimed at obtaining a set of coherent, grammatically–correct claims from the opponent side, which can then be directly quoted in a live debate. Prior to claim matching, mined claims are filtered, aiming to focus on those with a higher chance of obtaining a successful match. This included removing claims containing: (i) more than 10 tokens, since longer claims are less concise and may contain more than a single idea; (ii) named entities (found with Stanford NER (Finkel et al., 2005)), other than the topic itself, assuming they are too specific; (iii) un"
W19-4507,C14-1141,1,0.960173,"s for future work. All collected data is freely available for research. 1 Introduction Project Debater1 is a system designed to engage in a full live debate with expert human debaters. One of the major challenges in such a debate is listening to a several-minute long speech delivered by your opponent, identifying the main arguments, and rebutting them with effective persuasive counter arguments. This work focuses on the former, namely, automatically identifying arguments mentioned in opponent speeches. One of the fundamental capabilities developed in Debater is the automatic mining of claims (Levy et al., 2014) – general, concise statements that directly support or contest a given topic – from a large text corpus. It allows Debater to present high-quality content supporting its side within its 2 Related Work (Mirkin et al., 2018b) recently presented a dataset similar to the one we collected in the context of Machine Listening Comprehension (MLC) over argumentative content. Instead of using mined claims, they extracted lists of potential arguments from iDebate3 , a manually curated high-quality database containing arguments for controversial ∗ 2 * These authors equally contributed to this work. www.r"
W19-4507,W17-5110,1,0.792075,"es the suggested mined–claims based rebuttal generation pipeline. Following is a brief description of the existing components which perform claim mining. The rest of this work focuses on the subsequent component which identifies mentioned claims in speeches. Processing starts from a large corpus of news articles containing billions of sentences. Given a controversial topic, several queries are applied, retrieving sentences which potentially contain claims that are relevant to the topic. Query results are then ranked by a neural-model trained to detect sentences containing claims (similarly to Levy et al. (2017, 2018)5 ). Top-ranked sentences are passed to a boundary detection component, responsible for finding the exact span of each claim within each sentence (Levy et al., 2014). Lastly, the stance of each claim towards the topic is detected using the method of Bar-Haim et al. (2017). Used models are tuned towards precision, aimed at obtaining a set of coherent, grammatically–correct claims from the opponent side, which can then be directly quoted in a live debate. Prior to claim matching, mined claims are filtered, aiming to focus on those with a higher chance of obtaining a successful match. This"
W19-4507,L18-1037,1,0.547241,"Missing"
W19-4507,P05-1045,0,0.0133763,"tance of each claim towards the topic is detected using the method of Bar-Haim et al. (2017). Used models are tuned towards precision, aimed at obtaining a set of coherent, grammatically–correct claims from the opponent side, which can then be directly quoted in a live debate. Prior to claim matching, mined claims are filtered, aiming to focus on those with a higher chance of obtaining a successful match. This included removing claims containing: (i) more than 10 tokens, since longer claims are less concise and may contain more than a single idea; (ii) named entities (found with Stanford NER (Finkel et al., 2005)), other than the topic itself, assuming they are too specific; (iii) unresolved demonstratives, which may hint to an incoherent sentence or an error in boundary detection. The released dataset includes all output from these components, as well as a complete labeling indicating which texts are erroneously predicted to be claims, and what is the correct stance of all valid claims. The percentage of mined texts which are both labeled as claims and have a correctly Data Motions As in Mirkin et al. (2018b), we manually curated a list of 200 controversial topics referred to as motions, as in formal"
W19-4507,D18-1078,1,0.767011,"Missing"
W19-4507,P16-1150,0,0.038107,"Missing"
W19-4507,Q14-1025,0,0.0284223,"Missing"
W19-4507,J17-1004,0,0.0372426,"Missing"
W19-4507,P13-1045,0,0.0193808,"Missing"
W19-4507,J17-3005,0,0.113309,"Missing"
W19-4507,C18-1176,1,\N,Missing
